<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 33]
- [cs.DC](#cs.DC) [Total: 20]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography](https://arxiv.org/abs/2512.22301)
*Aayush Mainali,Sirjan Ghimire*

Main category: cs.CR

TL;DR: 该论文提出了一种基于场景的统计风险模型来评估后量子密码实现中的时序侧信道泄漏，通过合成不同执行条件下的跟踪数据，使用多种统计指标量化泄漏风险，并对代表性格基KEM方案进行比较分析。


<details>
  <summary>Details</summary>
Motivation: 后量子密码（特别是格基方案）的实现可能产生依赖于秘密的时序变异性，而实际时序测量受到环境噪声（如调度效应、争用、重尾延迟）的影响，需要一种系统方法来评估时序泄漏风险。

Method: 提出基于场景的统计风险模型，在空闲、抖动和负载三种执行条件下为两个秘密类别合成跟踪数据，使用Welch's t检验、KS距离、Cliff's delta、互信息和分布重叠等多种统计指标量化泄漏，并以类似TLRI的方式组合得到一致性评分来对场景进行排名。

Result: 对代表性格基KEM家族（Kyber、Saber、Frodo）的分析显示：空闲条件下可区分性最佳；抖动和负载条件通过增加方差和重叠来削弱可区分性；缓存索引和分支式泄漏往往产生最高风险信号；在相似泄漏假设下，更快的方案可能具有更高的峰值风险。

Conclusion: 该统计风险模型能够在早期设计阶段进行可重复的比较，为平台特定验证前的时序侧信道风险评估提供了系统方法，有助于在后量子密码实现中识别和缓解时序泄漏风险。

Abstract: Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.

</details>


### [2] [Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection](https://arxiv.org/abs/2512.22306)
*Chinmay Pushkar,Sanchit Kabra,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CR

TL;DR: 该研究提出了一个针对C、C++、Python和JavaScript四种编程语言的多漏洞检测基准测试，揭示了LLM在复杂多漏洞场景下性能显著下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注孤立单漏洞样本或函数级分类，无法反映真实软件中多个相互作用的漏洞共存于大型文件的复杂性。LLM在多标签任务中存在"计数偏差"和"选择偏差"问题，但在代码安全领域尚未得到严格量化。

Method: 构建包含40,000个文件的基准数据集，通过系统注入控制数量的漏洞（1、3、5、9个）到长上下文代码样本（7.5k-10k tokens）中，评估了包括GPT-4o-mini、Llama-3.3-70B和Qwen-2.5系列在内的五种最先进LLM。

Result: 随着漏洞密度增加，LLM性能急剧下降。Llama-3.3-70B在单漏洞C任务中达到接近完美的F1分数（约0.97），但在高密度设置下性能下降高达40%。Python和JavaScript与C/C++相比表现出不同的故障模式，模型在复杂Python文件中出现严重的"计数不足"（召回率降至0.30以下）。

Conclusion: 该研究揭示了LLM在多漏洞检测任务中的局限性，强调了需要开发能够处理真实世界软件复杂性的新方法，特别是在高漏洞密度和跨编程语言场景下。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from "count bias" and "selection bias" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe "under-counting" (Recall dropping to less than 0.30) in complex Python files.

</details>


### [3] [LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators](https://arxiv.org/abs/2512.22307)
*You Li,Guannan Zhao,Yuhao Ju,Yunqi He,Jie Gu,Hai Zhou*

Main category: cs.CR

TL;DR: LLA是一种结合硬件和软件的保护生成式AI模型知识产权的方案，通过嵌入密钥位到神经元中触发异常值来降低性能，同时在AI加速器中集成轻量级锁定模块，实现模型服务的许可访问。


<details>
  <summary>Details</summary>
Motivation: 保护生成式AI模型的知识产权，防御供应链威胁，包括模型盗窃、模型损坏和信息泄露。

Method: 软件方面：将密钥位嵌入神经元中触发异常值降低性能，并应用不变性变换来隐藏密钥值；硬件方面：在AI加速器中集成轻量级锁定模块，保持与各种数据流模式和工具链的兼容性。

Result: LLA能够抵御广泛的oracle引导的密钥优化攻击，同时仅产生极小的计算开销（对于7,168个密钥位，开销小于0.1%）。

Conclusion: LLA通过硬件和软件的协同作用，为生成式AI模型提供了一种有效的知识产权保护方案，能够防御多种供应链威胁，同时保持高性能和兼容性。

Abstract: We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.

</details>


### [4] [Verifiable Dropout: Turning Randomness into a Verifiable Claim](https://arxiv.org/abs/2512.22526)
*Kichang Lee,Sungmin Lee,Jaeho Jin,JeongGil Ko*

Main category: cs.CR

TL;DR: 提出Verifiable Dropout机制，使用零知识证明确保深度学习随机操作（如dropout）的可验证性，解决现有审计机制无法验证随机值是否被诚实生成和应用的问题。


<details>
  <summary>Details</summary>
Motivation: 现代基于云的AI训练依赖广泛的遥测和日志确保可追溯性，但现有审计机制无法处理深度学习的固有非确定性。随机操作（如dropout）创建了一个模糊表面，攻击者可以将恶意操作伪装成自然随机变化，获得合理的可否认性。现有日志机制无法在不暴露敏感训练数据的情况下验证随机值是否被诚实生成和应用。

Method: 引入Verifiable Dropout，一种基于零知识证明的隐私保护机制。该方法将dropout掩码绑定到确定性的、可加密验证的种子，并证明dropout操作的正确执行。将随机性视为可验证的声明而非借口。

Result: 该设计使用户能够在事后审计随机训练步骤的完整性，确保随机性既没有被偏置也没有被选择性使用，同时严格保护模型和数据的机密性。

Conclusion: Verifiable Dropout填补了现有审计机制的完整性空白，通过零知识证明技术实现了对深度学习随机操作的可验证性，在保护隐私的同时确保了训练过程的完整性。

Abstract: Modern cloud-based AI training relies on extensive telemetry and logs to ensure accountability. While these audit trails enable retrospective inspection, they struggle to address the inherent non-determinism of deep learning. Stochastic operations, such as dropout, create an ambiguity surface where attackers can mask malicious manipulations as natural random variance, granting them plausible deniability. Consequently, existing logging mechanisms cannot verify whether stochastic values were generated and applied honestly without exposing sensitive training data. To close this integrity gap, we introduce Verifiable Dropout, a privacy-preserving mechanism based on zero-knowledge proofs. We treat stochasticity not as an excuse but as a verifiable claim. Our approach binds dropout masks to a deterministic, cryptographically verifiable seed and proves the correct execution of the dropout operation. This design enables users to audit the integrity of stochastic training steps post-hoc, ensuring that randomness was neither biased nor cherry-picked, while strictly preserving the confidentiality of the model and data.

</details>


### [5] [Raven: Mining Defensive Patterns in Ethereum via Semantic Transaction Revert Invariants Categories](https://arxiv.org/abs/2512.22616)
*Mojtaba Eshghie,Melissa Mazura,Alexandre Bartel*

Main category: cs.CR

TL;DR: Raven框架通过分析以太坊交易回滚，挖掘智能合约中的防御性不变式，发现了6个新的不变式类别，可用于安全研究工具开发


<details>
  <summary>Details</summary>
Motivation: 以太坊交易因合约中的不变式检查（require/assert/revert）而回滚，这些回滚交易是活跃链上防御的积极信号，但现有的防御模式在安全研究中尚未被发现和充分利用

Method: 提出Raven框架：1）将回滚交易与导致回滚的智能合约源代码中的不变式对齐；2）使用基于BERT的微调模型嵌入这些不变式；3）按语义意图对不变式进行聚类，挖掘以太坊上的防御性不变式类别

Result: 在20,000个回滚交易样本上评估，Raven实现了有凝聚力和有意义的交易回滚不变式聚类。专家手动审查挖掘出的19个语义聚类，发现了6个现有不变式目录中不存在的新类别：功能切换、重放防护、证明/签名验证、计数器、调用者提供的滑点阈值、允许/禁止/机器人列表

Conclusion: Raven能够映射以太坊成功的防御机制。这些不变式类别使安全研究人员能够基于从智能合约工作防御中提取的数据驱动安全预言机开发分析工具。通过案例研究展示了使用新发现的不变式类别作为模糊测试预言机来检测真实世界攻击中的漏洞的实用性

Abstract: We frame Ethereum transactions reverted by invariants-require(<invariant>)/ assert(<invariant>)/if (<invariant>) revert statements in the contract implementation-as a positive signal of active on-chain defenses. Despite their value, the defensive patterns in these transactions remain undiscovered and underutilized in security research. We present Raven, a framework that aligns reverted transactions to the invariant causing the reversion in the smart contract source code, embeds these invariants using our BERT-based fine-tuned model, and clusters them by semantic intent to mine defensive invariant categories on Ethereum. Evaluated on a sample of 20,000 reverted transactions, Raven achieves cohesive and meaningful clusters of transaction-reverting invariants. Manual expert review of the mined 19 semantic clusters uncovers six new invariant categories absent from existing invariant catalogs, including feature toggles, replay prevention, proof/signature verification, counters, caller-provided slippage thresholds, and allow/ban/bot lists. To demonstrate the practical utility of this invariant catalog mining pipeline, we conduct a case study using one of the newly discovered invariant categories as a fuzzing oracle to detect vulnerabilities in a real-world attack. Raven thus can map Ethereum's successful defenses. These invariant categories enable security researchers to develop analysis tools based on data-driven security oracles extracted from the smart contracts' working defenses.

</details>


### [6] [When RSA Fails: Exploiting Prime Selection Vulnerabilities in Public Key Cryptography](https://arxiv.org/abs/2512.22720)
*Murtaza Nikzad,Kerem Atas*

Main category: cs.CR

TL;DR: 该论文分析了RSA密码系统中因素数选择不当导致的漏洞，主要关注费马分解法和GCD攻击两种攻击向量，揭示了这些漏洞在现实世界加密实现中仍然普遍存在，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究RSA密码系统中因素数选择不当而产生的安全漏洞，特别是针对费马分解法和GCD攻击这两种攻击向量，旨在揭示这些漏洞在现实世界加密实现中的普遍性和严重性。

Method: 通过分析两种主要攻击向量：费马分解法（针对素数过于接近的RSA密钥）和最大公约数攻击（针对共享公因子的密钥），并借鉴Heninger等人的"Ps and Qs"研究和Böck 2023年的费马分解分析，对现实世界加密实现中的漏洞进行系统性研究。

Result: 研究发现这些漏洞在现实世界加密实现中仍然普遍存在，嵌入式设备中的弱随机数生成是导致这些失败的主要原因，Heninger等人的研究发现了超过64,000个易受攻击的TLS主机。

Conclusion: 论文得出结论，RSA密钥生成中的素数选择不当会导致严重安全漏洞，提出了包括适当的熵收集和素数验证检查在内的缓解策略，强调需要改进随机数生成和密钥验证机制。

Abstract: This paper explores vulnerabilities in RSA cryptosystems that arise from improper prime number selection during key generation. We examine two primary attack vectors: Fermat's factorization method, which exploits RSA keys generated with primes that are too close together, and the Greatest Common Divisor (GCD) attack, which exploits keys that share a common prime factor. Drawing from landmark research including Heninger et al.'s ``Mining Your Ps and Qs'' study, which discovered over 64,000 vulnerable TLS hosts, and B{ö}ck's 2023 analysis of Fermat factorization in deployed systems, we demonstrate that these vulnerabilities remain prevalent in real-world cryptographic implementations. Our analysis reveals that weak random number generation in embedded devices is the primary cause of these failures, and we discuss mitigation strategies including proper entropy collection and prime validation checks.

</details>


### [7] [Breaking the illusion: Automated Reasoning of GDPR Consent Violations](https://arxiv.org/abs/2512.22789)
*Ying Li,Wenjun Qiu,Faysal Hossain Shezan,Kunlin Cai,Michelangelo van Dam,Lisa Austin,David Lie,Yuan Tian*

Main category: cs.CR

TL;DR: Cosmic是一个自动化框架，用于检测网页表单中的同意相关隐私违规，在5823个网站和3598个表单中发现了大量GDPR/CCPA违规行为。


<details>
  <summary>Details</summary>
Motivation: GDPR和CCPA等隐私法规要求用户同意必须是知情、自愿、具体和明确的，但现实中仍存在许多违规行为。当前研究主要关注cookie横幅和移动应用对话框，而网页表单中的同意机制多样且难以自动审计，存在合规性检测的空白。

Method: 开发了Cosmic自动化框架，用于检测网页表单中的同意相关隐私违规。该工具能够自动审计网页表单的合规性，覆盖GDPR的关键原则如自愿同意、目的披露和撤回选项。

Result: 在5823个网站和3598个表单的评估中，Cosmic在94.1%的同意表单上检测到3384个违规。工具在同意检测和违规检测方面的真阳性率分别达到98.6%和99.1%，显示出高准确性和实际应用价值。

Conclusion: Cosmic框架能够有效自动化检测网页表单中的隐私合规违规，填补了当前研究空白，为监管机构和网站运营者提供了实用的合规审计工具。

Abstract: Recent privacy regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have established legal requirements for obtaining user consent regarding the collection, use, and sharing of personal data. These regulations emphasize that consent must be informed, freely given, specific, and unambiguous. However, there are still many violations, which highlight a gap between legal expectations and actual implementation. Consent mechanisms embedded in functional web forms across websites play a critical role in ensuring compliance with data protection regulations such as the GDPR and CCPA, as well as in upholding user autonomy and trust. However, current research has primarily focused on cookie banners and mobile app dialogs. These forms are diverse in structure, vary in legal basis, and are often difficult to locate or evaluate, creating a significant challenge for automated consent compliance auditing. In this work, we present Cosmic, a novel automated framework for detecting consent-related privacy violations in web forms. We evaluate our developed tool for auditing consent compliance in web forms, across 5,823 websites and 3,598 forms. Cosmic detects 3,384 violations on 94.1% of consent forms, covering key GDPR principles such as freely given consent, purpose disclosure, and withdrawal options. It achieves 98.6% and 99.1% TPR for consent and violation detection, respectively, demonstrating high accuracy and real-world applicability.

</details>


### [8] [SecureBank: A Financially-Aware Zero Trust Architecture for High-Assurance Banking Systems](https://arxiv.org/abs/2512.23124)
*Paulo Fernandes Biao*

Main category: cs.CR

TL;DR: SecureBank是一个专门为高保障银行系统设计的财务感知和上下文自适应的零信任架构，集成了财务零信任、自适应身份评分、上下文微分割和影响驱动的安全自动化，通过蒙特卡洛模拟验证其优于传统基于规则的基线架构。


<details>
  <summary>Details</summary>
Motivation: 金融机构日益依赖分布式架构、开放银行API、云原生基础设施和高频数字交易，这些转变扩大了攻击面并暴露了传统基于边界的安全模型的局限性。虽然零信任架构提供了基本安全原则，但大多数现有框架没有明确纳入交易语义、金融风险建模、自适应身份信任或基于经济影响的自动化。

Method: 提出SecureBank框架，集成了财务零信任、自适应身份评分、上下文微分割和影响驱动的安全自动化。使用蒙特卡洛模拟评估SecureBank与基于规则的基线架构，使用交易完整性指数(TII)、身份信任适应水平(ITAL)和安全自动化效率(SAE)等指标。

Result: SecureBank显著改善了自动化攻击处理，加速了身份信任适应，同时保持了保守且符合监管要求的交易完整性水平。实验验证表明该架构优于传统基线方法。

Conclusion: SecureBank不仅通过实验验证，还旨在作为受监管金融环境中财务感知零信任系统的参考架构和评估基准，为高保障银行系统提供专门设计的自适应安全解决方案。

Abstract: Financial institutions increasingly rely on distributed architectures, open banking APIs, cloud native infrastructures, and high frequency digital transactions. These transformations expand the attack surface and expose limitations in traditional perimeter based security models. While Zero Trust architectures provide essential security principles, most existing frameworks do not explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, or automation weighted by economic impact.
  This paper introduces SecureBank, a financially aware and context adaptive Zero Trust architecture designed specifically for high assurance banking systems. The proposed framework integrates Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. A Monte Carlo simulation evaluates SecureBank against a representative rule based baseline architecture using metrics such as the Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).
  The results demonstrate that SecureBank significantly improves automated attack handling and accelerates identity trust adaptation while preserving conservative and regulator aligned levels of transactional integrity. Beyond experimental validation, SecureBank is intended to serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments.

</details>


### [9] [Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning](https://arxiv.org/abs/2512.23171)
*Yu Jiang,Xindi Tong,Ziyao Liu,Xiaoxi Zhang,Kwok-Yan Lam,Chee Wei Tan*

Main category: cs.CR

TL;DR: FedORA是一种用于垂直联邦学习(VFL)中样本和标签遗忘的新方法，通过原始-对偶优化框架解决数据移除问题，在保持模型效用的同时显著降低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习中的遗忘问题具有挑战性，因为不同参与方持有相同样本的互补特征，遗忘任务需要跨方协调，导致计算开销和特征相互依赖的复杂性。现有研究主要集中在水平联邦学习，而VFL中的遗忘问题尚未得到充分解决。

Method: 提出FedORA方法：1) 将特定样本或标签的移除建模为约束优化问题，使用原始-对偶框架求解；2) 引入新的遗忘损失函数，促进分类不确定性而非错误分类；3) 采用自适应步长增强稳定性；4) 设计非对称批处理，考虑剩余数据对模型的先验影响，分别处理遗忘数据和保留数据以降低计算成本。

Result: 理论分析证明FedORA与从头训练模型的差异是有界的，为遗忘有效性提供了保证。在表格和图像数据集上的实验表明，FedORA在实现与从头训练相当的遗忘有效性和效用保持的同时，显著减少了计算和通信开销。

Conclusion: FedORA为垂直联邦学习中的样本和标签遗忘提供了一种有效的解决方案，通过优化框架平衡了遗忘效果、模型效用和计算效率，满足了隐私保护需求。

Abstract: Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the "right to be forgotten." While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.

</details>


### [10] [EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](https://arxiv.org/abs/2512.23173)
*Zhen Liang,Hai Huang,Zhengkui Chen*

Main category: cs.CR

TL;DR: EquaCode是一种通过方程求解和代码完成的新型多策略越狱攻击方法，将恶意意图转化为数学问题，然后要求LLM用代码解决，利用跨领域任务的复杂性分散模型对安全约束的关注。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要在自然语言层面操作且依赖单一攻击策略，限制了全面评估LLM鲁棒性的有效性。需要更有效的多策略方法来测试LLM的信任度。

Method: 提出EquaCode方法：1) 将恶意意图转化为数学问题；2) 要求LLM使用代码解决该问题；3) 利用跨领域任务（数学+编程）的复杂性分散模型对安全约束的关注。

Result: 在GPT系列上平均成功率为91.19%，在3个最先进的LLM上达到98.65%，且仅需单次查询。消融实验显示EquaCode优于单独的数学方程模块或代码模块，表明多策略方法具有协同效应。

Conclusion: EquaCode通过方程求解和代码完成的多策略方法有效突破了LLM的安全约束，证明了跨领域任务的复杂性可以分散模型对安全限制的关注，为评估LLM鲁棒性提供了新视角。

Abstract: Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.

</details>


### [11] [Multiparty Authorization for Secure Data Storage in Cloud Environments using Improved Attribute-Based Encryption](https://arxiv.org/abs/2512.23216)
*Partha Paul,Keshav Sinha*

Main category: cs.CR

TL;DR: 提出了一种基于属性加密的改进方案，结合功能流密码和抛物线曲线上的标量点，用于云环境中的安全数据存储和访问控制，支持多方授权并抵抗碰撞攻击。


<details>
  <summary>Details</summary>
Motivation: 云环境中存储敏感数据面临数据请求频率高（增加服务器计算开销）和数据存储泄漏等问题，需要有效的安全存储和访问控制技术。

Method: 改进的属性加密方案，结合功能流密码，使用抛物线曲线上的简单标量点实现多方授权。采用Shamir秘密共享生成授权点，2D-Lagrange插值从正则抛物线重构秘密点，设定阈值(Ts>3)要求合法授权用户重构属性关联密钥进行解密。

Result: 通过统计分析测试验证加密效果，性能分析显示加密时间随授权策略属性数量增加而增加，但存储开销最小。安全分析证明方案能抵抗碰撞攻击，比现有方案更鲁棒安全。

Conclusion: 提出的改进属性加密方案在云环境中能有效实现安全数据存储和访问控制，具有最小存储开销和良好的安全性能，适合处理敏感数据存储需求。

Abstract: In todays scenario, various organizations store their sensitive data in the cloud environment. Multiple problems are present while retrieving and storing vast amounts of data, such as the frequency of data requests (increasing the computational overhead of the server) and data leakage while storing. To cope with said problem, Attribute-Based Encryption (ABE) is one of the potential security and access control techniques for secure data storage and authorization. The proposed work divides into two objectives: (i) provide access to authorized users and (ii) secure data storage in a cloud environment. The improved ABE using Functional Based Stream Cipher (FBSE) is proposed for data storage. The proposed technique uses simple scalar points over a parabolic curve to provide multiparty authorization. The authorization points are generated and share only with the authorized recipients. The Shamir secret sharing technique generate the authorization points and 2D-Lagrange Interpolation is used to reconstruct the secret points from regular parabola. The proposed scheme has specified the threshold (Ts>3) legally authorized users to reconstruct the attribute-associated keys for decryption. The encryption of data is evaluated using Statistical analysis (NIST Statistical Test Suite, Correlation Coefficient, and Histogram) test to investigate image pixel deviation. The parameters like encryption and decryption are used for performance analysis, where an increase in the number of attributes for the authorization policy will increase the encryption time. The proposed scheme imposes minimal storage overhead, irrespective of the users identity. The security analysis evidence that it resists collision attacks. The security and performance analysis results demonstrate that the proposed scheme is more robust and secure.

</details>


### [12] [RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking](https://arxiv.org/abs/2512.23307)
*Jiawei Liu,Zhuo Chen,Rui Zhu,Miaokun Chen,Yuyang Gong,Wei Lu,Xiaofeng Wang*

Main category: cs.CR

TL;DR: RobustMask是一种结合预训练语言模型上下文预测能力和随机掩码平滑机制的新型防御方法，可增强神经排序模型对抗字符、单词和短语级对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经排序模型在现实应用中广泛部署，但容易受到对抗攻击，现有防御方法要么依赖泛化能力差的启发式方法，要么假设过强的对抗知识，限制了实际应用。

Method: 提出RobustMask方法，结合预训练语言模型的上下文预测能力和随机掩码平滑机制，利用排序模型的成对比较能力和概率统计分析，提供理论上的top-K鲁棒性证明。

Result: 实验表明，RobustMask能成功认证超过20%的候选文档在top-10排名位置中对抗影响高达30%内容的对抗扰动。

Conclusion: RobustMark在增强神经排序模型的对抗鲁棒性方面有效，为现实检索系统提供更强安全保障迈出了重要一步。

Abstract: Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.

</details>


### [13] [Fuzzilicon: A Post-Silicon Microcode-Guided x86 CPU Fuzzer](https://arxiv.org/abs/2512.23438)
*Johannes Lenzen,Mohamadreza Rostami,Lichao Wu,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: Fuzzilicon：首个针对真实x86 CPU的硅后模糊测试框架，通过逆向工程Intel微码更新接口实现微架构层深度检测，自动发现微码级漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代CPU作为黑盒系统，存在复杂的微架构漏洞，传统分析方法难以检测。现有漏洞发现依赖繁琐的手动逆向工程，缺乏自动化、系统化的硅后处理器漏洞检测框架。

Method: 通过逆向工程Intel专有微码更新接口，开发微码级检测方法；结合基于hypervisor的模糊测试框架，实现精确的反馈引导输入生成；直接从处理器微架构提取反馈，无需RTL访问。

Result: 在Intel Goldmont微架构上发现5个重要漏洞，包括2个先前未知的微码级推测执行漏洞；自动重新发现μSpectre类漏洞；相比基线技术减少31倍覆盖收集开销；实现16.27%可挂钩位置的唯一微码覆盖率。

Conclusion: Fuzzilicon建立了硅后模糊测试的新基础，为自动化发现复杂CPU漏洞提供了实用、覆盖引导且可扩展的方法，填补了微码和微架构层的检测空白。

Abstract: Modern CPUs are black boxes, proprietary, and increasingly characterized by sophisticated microarchitectural flaws that evade traditional analysis. While some of these critical vulnerabilities have been uncovered through cumbersome manual effort, building an automated and systematic vulnerability detection framework for real-world post-silicon processors remains a challenge.
  In this paper, we present Fuzzilicon, the first post-silicon fuzzing framework for real-world x86 CPUs that brings deep introspection into the microcode and microarchitectural layers. Fuzzilicon automates the discovery of vulnerabilities that were previously only detectable through extensive manual reverse engineering, and bridges the visibility gap by introducing microcode-level instrumentation. At the core of Fuzzilicon is a novel technique for extracting feedback directly from the processor's microarchitecture, enabled by reverse-engineering Intel's proprietary microcode update interface. We develop a minimally intrusive instrumentation method and integrate it with a hypervisor-based fuzzing harness to enable precise, feedback-guided input generation, without access to Register Transfer Level (RTL).
  Applied to Intel's Goldmont microarchitecture, Fuzzilicon introduces 5 significant findings, including two previously unknown microcode-level speculative-execution vulnerabilities. Besides, the Fuzzilicon framework automatically rediscover the $μ$Spectre class of vulnerabilities, which were detected manually in the previous work. Fuzzilicon reduces coverage collection overhead by up to 31$\times$ compared to baseline techniques and achieves 16.27% unique microcode coverage of hookable locations, the first empirical baseline of its kind. As a practical, coverage-guided, and scalable approach to post-silicon fuzzing, Fuzzilicon establishes a new foundation to automate the discovery of complex CPU vulnerabilities.

</details>


### [14] [A Privacy Protocol Using Ephemeral Intermediaries and a Rank-Deficient Matrix Power Function (RDMPF)](https://arxiv.org/abs/2512.23535)
*Eduardo Salazar*

Main category: cs.CR

TL;DR: 提出了一个用于Internet Computer的私有传输架构，通过两个短暂中介分离存款和检索，使用密封存储和短暂见证的认证拆除，提供发送者身份隐私和内容机密性。


<details>
  <summary>Details</summary>
Motivation: 设计一个在Internet Computer上实现私有传输的协议，解决传统区块链交易中隐私保护不足的问题，特别是发送者身份隐私、内容机密性和中介不可信环境下的安全问题。

Method: 使用非交互式RDMPF封装技术派生每笔传输的传输密钥；通过两个短暂中介分离存款和检索过程；采用密封存储和短暂见证的认证拆除机制；计算公共通知提示实现无指纹的接收者发现；使用解密证明进行授权而不暴露身份。

Result: 协议已在ICP上以ICPP名称投入生产，经过详尽测试并包含多项增强功能；提供了发送者身份隐私、内容机密性、传输密钥的前向保密性，以及可验证的活跃性和最终性。

Conclusion: 该设计为ICP上的私有传输提供了一个安全、可审计的解决方案，实现了身份隐私保护、内容机密性和可验证的最终性，并已成功部署为生产系统，可作为该协议的广泛参考。

Abstract: This paper presents a private transfer architecture for the Internet Computer (ICP) that decouples deposit and retrieval through two short-lived intermediaries, with sealed storage and attested teardown by an ephemeral witness. The protocol uses a non-interactive RDMPF-based encapsulation to derive per-transfer transport keys. A public notice hint is computed from the capsule to enable discovery without fingerprinting the recipient's key. Retrieval is authorized by a short proof of decapsulation that reveals no identities. All transaction intermediaries are ephemeral and issue certified destruction intents and proofs, allowing a noticeboard to publish auditable finalization records. The design provides sender identity privacy with respect to the recipient, content confidentiality against intermediaries, forward secrecy for transport keys after staged destruction, verifiable liveness and finality. We formalize the basic interfaces, provide the security arguments for encapsulation correctness, hint privacy, authorization soundness and timeout reclaim.
  In terms of implementation, it has been recently brought into production on the ICP under the name ICPP. It has been subject to exhaustive testing and incorporates a few enhancements, focusing on the operational possibilities offered by ICP's technology. This work hence serves as a broad reference for the protocol now publicly accessible.

</details>


### [15] [Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks](https://arxiv.org/abs/2512.23557)
*Toqeer Ali Syed,Mishal Ateeq Almutairi,Mahmoud Abdel Moaty*

Main category: cs.CR

TL;DR: 本文提出了一种跨智能体多模态溯源感知防御框架，用于检测和防御多模态提示注入攻击，确保多智能体AI系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型、视觉语言模型和智能体AI系统的发展，多智能体环境增加了多模态提示注入攻击的风险，恶意指令可能通过文本、图像、元数据或智能体间消息传播，导致意外行为、策略违反或状态破坏。

Method: 提出跨智能体多模态溯源感知防御框架，包含文本净化智能体、视觉净化智能体和输出验证智能体，由溯源账本协调管理。该框架对所有提示进行净化处理，并在发送到下游节点前独立验证LLM生成的输出，通过追踪模态、来源和信任级别的元数据确保智能体间通信符合明确的信任框架。

Result: 实验评估显示，该框架显著提高了多模态注入检测的准确性，最小化了跨智能体信任泄漏，并使智能体执行路径更加稳定。

Conclusion: 该框架将溯源追踪和验证概念扩展到多智能体编排中，有助于建立安全、可理解和可靠的智能体AI系统。

Abstract: Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.

</details>


### [16] [Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation](https://arxiv.org/abs/2512.23610)
*Heba Osama,Omar Elebiary,Youssef Qassim,Mohamed Amgad,Ahmed Maghawry,Ahmed Saafan,Haitham Ghalwash*

Main category: cs.CR

TL;DR: WAMM是一个AI驱动的多类别Web攻击检测框架，通过重新分类HTTP请求到OWASP对齐的类别，揭示了基于规则系统的局限性，在特定技术栈上达到99.59%的准确率，相比传统WAF有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态规则集的Web应用防火墙（如OWASP CRS）经常错过混淆或零日攻击模式，需要大量手动调优。Web应用面临日益增多的规避性和多态性攻击载荷，需要更智能的检测方法。

Method: WAMM采用多阶段增强管道处理SR-BH 2020数据集，包括大规模去重、LLM引导的重新标注、现实攻击数据增强和LLM过滤，生成三个精炼数据集。使用统计和文本表示的统一特征空间评估四种机器学习和深度学习模型。

Result: 在相同技术栈上，使用增强和LLM过滤的数据集，XGBoost达到99.59%的准确率，具有微秒级推理时间。深度学习模型在噪声增强下性能下降。针对OWASP CRS使用未见过的增强数据集测试时，WAMM实现了96-100%的真正阳性拦截率，改进高达86%。

Conclusion: 研究结果揭示了广泛部署的基于规则防御的差距，并证明精心策划的训练管道结合高效的机器学习模型能够实现更弹性、实时的Web攻击检测方法，适用于生产WAF环境。

Abstract: Web applications increasingly face evasive and polymorphic attack payloads, yet traditional web application firewalls (WAFs) based on static rule sets such as the OWASP Core Rule Set (CRS) often miss obfuscated or zero-day patterns without extensive manual tuning. This work introduces WAMM, an AI-driven multiclass web attack detection framework designed to reveal the limitations of rule-based systems by reclassifying HTTP requests into OWASP-aligned categories for a specific technology stack. WAMM applies a multi-phase enhancement pipeline to the SR-BH 2020 dataset that includes large-scale deduplication, LLM-guided relabeling, realistic attack data augmentation, and LLM-based filtering, producing three refined datasets. Four machine and deep learning models are evaluated using a unified feature space built from statistical and text-based representations. Results show that using an augmented and LLM-filtered dataset on the same technology stack, XGBoost reaches 99.59% accuracy with microsecond-level inference while deep learning models degrade under noisy augmentation. When tested against OWASP CRS using an unseen augmented dataset, WAMM achieves true positive block rates between 96 and 100% with improvements of up to 86%. These findings expose gaps in widely deployed rule-based defenses and demonstrate that curated training pipelines combined with efficient machine learning models enable a more resilient, real-time approach to web attack detection suitable for production WAF environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience](https://arxiv.org/abs/2512.22435)
*Zining Wang,Jian Gao,Weimin Fu,Xiaolong Guo,Xuan Zhang*

Main category: cs.AR

TL;DR: AnalogSAGE是一个开源的自进化多智能体框架，通过分层记忆和三阶段智能体探索实现模拟电路设计的自动化，相比现有方法显著提高了通过率和效率。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计高度依赖人类经验和直觉，现有LLM方法通常基于提示驱动的网表生成或预定义拓扑模板，难以满足复杂规格要求。

Method: 提出AnalogSAGE框架，通过四个分层记忆层协调三阶段智能体探索，支持基于仿真反馈的迭代优化，使用开源SKY130 PDK和ngspice进行验证。

Result: 在10个不同难度的运算放大器设计问题基准测试中，实现了10倍总体通过率、48倍Pass@1和4倍参数搜索空间减少，显著提升了模拟设计自动化的可靠性和自主性。

Conclusion: 分层记忆和基于仿真的推理显著增强了模拟设计自动化的实际应用能力，开源代码支持可复现性和通用性。

Abstract: Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\times$ overall pass rate, a 48$\times$ Pass@1, and a 4$\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.

</details>


### [18] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: 本文提出TYTAN：基于泰勒级数的非线性激活引擎，通过可重构硬件设计和专用算法动态估计激活函数的近似，在边缘AI推理中实现高性能和低功耗。


<details>
  <summary>Details</summary>
Motivation: 随着AI架构的快速发展和AI赋能系统的普及，边缘计算对特定领域架构的需求日益增长，需要解决AI算法部署中的计算成本和能耗问题，特别是通用矩阵乘法和激活函数等高功耗操作。

Method: 提出TYTAN系统，包含可重构硬件设计和专用算法，通过泰勒级数近似非线性激活函数，动态估计所需近似程度以最小化与基准精度的偏差。

Result: 在Silvaco FreePDK45工艺节点上的系统级仿真显示，TYTAN工作频率>950MHz，相比开源NVIDIA深度学习加速器(NVDLA)实现，性能提升约2倍，功耗降低约56%，面积减少约35倍。

Conclusion: TYTAN通过硬件-算法协同设计有效加速非线性激活函数，显著提升边缘AI推理的性能和能效，为资源受限的边缘设备提供了可行的解决方案。

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: 提出双向RAG架构，通过验证后的高质量生成响应回写实现安全的知识库扩展，相比标准RAG将覆盖率从20.33%提升至40.58%，同时比简单回写减少72%的文档添加量。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用静态知识库，无法从用户交互中学习和进化，限制了系统的持续改进能力。需要一种既能扩展知识库又能防止幻觉污染的安全机制。

Method: 提出双向RAG架构，包含多阶段接受层：基于NLI的蕴含验证、归因检查和新颖性检测，确保高质量生成响应能够安全地回写到知识库中，实现知识积累。

Result: 在四个数据集（Natural Questions、TriviaQA、HotpotQA、Stack Overflow）上，双向RAG达到40.58%的平均覆盖率，几乎是标准RAG（20.33%）的两倍，同时比简单回写方法减少72%的文档添加量（140 vs 500）。

Conclusion: 研究表明，在严格验证机制控制下，自改进的RAG系统是可行且安全的，为实现能够从部署中学习的RAG系统提供了实用路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [20] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型在未经明确提示的情况下也可能产生说服行为，特别是在经过监督微调后，即使训练数据只包含良性话题，模型也会在争议性和有害话题上表现出更强的说服倾向。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI系统的广泛应用，AI对人类观点和信念的影响力前所未有。先前研究主要关注"滥用"威胁模型（即恶意行为者要求LLM说服用户），但本研究旨在探索模型在未经明确提示的情况下进行说服的情况，以评估这种新兴说服风险的实际关注程度。

Method: 研究通过两种场景探索未经提示的说服行为：1）通过内部激活引导使模型沿特定人格特质方向调整；2）通过监督微调使模型展现相同特质。研究比较了这两种方法对模型未经提示说服倾向的影响。

Result: 研究发现：1）通过激活引导向说服相关或不相关特质调整，并不能可靠增加模型的未经提示说服倾向；2）监督微调能显著增加模型的未经提示说服倾向；3）即使在仅包含良性话题的一般说服数据集上进行监督微调，也会产生在争议性和有害话题上具有更高说服倾向的模型。

Conclusion: 研究表明，有害的说服行为可能以新兴方式出现，即使训练数据只包含良性内容。监督微调可能无意中增加模型在未经提示情况下的说服倾向，特别是在争议性和有害话题上，这需要进一步研究和关注。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [21] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench是一个评估多模态大语言模型空间推理能力的基准测试，通过折纸任务评估2D到3D规划和空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在感知和指令跟随方面表现良好，但在空间推理（跨多视角和时间跟踪操纵物体的能力）方面仍有困难。现有基准测试主要关注静态图像或最终输出，未能考虑空间推理的顺序性和视角依赖性特点。

Method: 引入GamiBench基准测试，包含186个常规和186个不可能的2D折痕图案及其对应的3D折叠形状，从6个不同视角生成。包含三个视觉问答任务：预测3D折叠配置、区分有效视角、检测不可能图案。引入新的诊断指标：视角一致性和不可能折叠选择率。

Result: 实验显示即使是领先模型如GPT-5和Gemini-2.5-Pro在单步空间理解方面也表现不佳。GamiBench为评估MLLMs的几何理解和空间推理能力提供了标准化框架。

Conclusion: GamiBench填补了现有基准测试在评估空间推理能力方面的空白，通过折纸任务全面评估模型的推理过程，为多模态大语言模型的空间推理能力评估提供了新的标准和方法。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [22] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 本文提出了一种公平感知的AI框架，用于孟加拉国洪水后援助分配的优先级排序，通过对抗性去偏技术减少对边缘化地区的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 发展中国家灾后援助分配存在系统性偏见，边缘化地区往往处于不利地位，这延续了历史不平等。孟加拉国作为洪水频发国家，需要更公平的援助分配机制。

Method: 使用2022年孟加拉国洪水真实数据，开发对抗性去偏模型预测洪水脆弱性，采用梯度反转层学习偏置不变表示，将医疗AI中的公平感知表示学习技术应用于灾害管理。

Result: 在11个地区的87个upazilas上测试，框架将统计奇偶差异减少41.6%，区域公平差距降低43.2%，同时保持强预测准确性（R平方=0.784 vs 基线0.811）。

Conclusion: 该研究展示了算法公平技术在人道主义背景下的有效应用，为决策者提供了实施更公平灾害恢复策略的工具，确保援助基于真实需求而非历史分配模式。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [23] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 提出了Agentic Risk & Capability (ARC)框架，这是一个技术治理框架，帮助组织识别、评估和减轻由智能AI系统带来的风险，通过能力中心视角分析风险源并提供结构化实施方法。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统具有自主行动能力（如代码执行、互联网交互、文件修改），既带来重大机遇也产生新型风险，对组织治理构成挑战，需要全面识别、评估和减轻这些多样且不断演变的风险。

Method: 引入ARC框架，采用能力中心视角分析智能AI系统，提炼出三个主要风险源（组件、设计、能力），建立风险源、具体风险和技术控制之间的明确关联，并提供结构化实施方法。

Result: 开发了一个开源技术治理框架，为组织提供稳健且适应性强的方法来应对智能AI的复杂性，支持快速有效创新同时确保智能AI系统的安全、可靠和负责任部署。

Conclusion: ARC框架为组织管理智能AI系统风险提供了系统化工具，通过能力中心的风险分析方法和结构化控制措施，帮助组织在创新与风险管理之间取得平衡。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [24] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) 是一种轻量级提示框架，通过引入类型变量、确定性条件评估器和基于规则的验证器，显著提升大语言模型在需要严格规则遵循、确定性和可审计性任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理方面表现出色，但在需要严格规则遵循、确定性和可审计性的任务上仍然不可靠，特别是在临床、监管和安全关键决策支持系统中。

Method: 提出Logic Sketch Prompting (LSP)框架，包含类型变量、确定性条件评估器和基于规则的验证器，能够产生可追踪和可重复的输出。在两个药理学逻辑合规任务上，使用三个开源模型（Gemma 2、Mistral、Llama 3）进行基准测试。

Result: 在所有模型和任务中，LSP始终获得最高的准确率（0.83-0.89）和F1分数（0.83-0.89），显著优于零样本提示（0.24-0.60）、简洁提示（0.16-0.30）和思维链提示（0.56-0.75）。McNemar检验显示几乎所有比较中LSP都有统计学显著改进（p < 0.01）。

Conclusion: LSP在不牺牲性能的情况下提高了确定性、可解释性和一致性，支持其在临床、监管和安全关键决策支持系统中的使用。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [25] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World：一个工具增强的多智能体框架，通过多智能体反馈实现推理时世界模型生成，并作为监督微调的数据引擎，在PDDL和可执行代码表示上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLMs生成符号世界模型（如PDDL域或可执行模拟器）受限于缺乏大规模可验证监督，现有方法主要依赖静态验证方法，无法捕捉交互执行中出现的行为级错误。

Method: 提出Agent2World三阶段流水线：1) Deep Researcher智能体通过网页搜索进行知识合成以解决规范缺口；2) Model Developer智能体实现可执行世界模型；3) 专门的Testing Team进行自适应单元测试和基于模拟的验证。

Result: 在三个涵盖PDDL和可执行代码表示的基准测试中展示了优越的推理时性能，取得一致的SOTA结果。基于测试团队反馈生成的训练轨迹进行微调的模型，世界模型生成能力平均相对提升30.95%。

Conclusion: Agent2World不仅实现了强大的推理时世界模型生成，还作为监督微调的数据引擎，通过多智能体反馈机制显著提升了世界模型生成的质量和可靠性。

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [26] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 将带有控制参数的数字规划问题转化为简单数字任务，使传统启发式方法能够处理无限动作空间


<details>
  <summary>Details</summary>
Motivation: 标准数字规划模型引入控制参数后，动作数量可能无限，导致现有基于动作结构的数字启发式方法不可行

Method: 识别可控简单数字问题子集，采用乐观编译方法将其转化为简单数字任务，将控制相关表达式抽象为有界常数效果和宽松前提条件

Result: 提出的编译方法能够有效使用子目标启发式来估计目标距离，是处理无限动作空间的可行有效方法

Conclusion: 该方法扩展了传统数字启发式方法的应用范围，推动了带有控制参数的数字规划领域的技术边界

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [27] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 提出了HalluMatData基准数据集和HalluMatDetector多阶段幻觉检测框架，用于评估和缓解材料科学领域AI生成内容中的幻觉问题，将幻觉率降低了30%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学发现中具有重要作用，但存在生成事实错误或误导性信息的幻觉问题，这损害了研究完整性，特别是在材料科学领域需要解决这一问题。

Method: 提出了HalluMatData基准数据集用于评估幻觉检测方法，并开发了HalluMatDetector多阶段幻觉检测框架，该框架集成了内在验证、多源检索、矛盾图分析和基于度量的评估。

Result: 研究发现材料科学不同子领域的幻觉水平差异显著，高熵查询表现出更大的事实不一致性。使用HalluMatDetector验证流程将幻觉率比标准LLM输出降低了30%。

Conclusion: 通过HalluMatData数据集和HalluMatDetector框架，可以有效检测和缓解材料科学领域AI生成内容中的幻觉问题，同时提出的PHCS指标能够量化语义等效查询下的不一致性，为模型可靠性提供更深层次的见解。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [28] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias：一个轻量级推理时个性化框架，通过结构门控适应将冻结的知识图谱嵌入适配到个体用户上下文，仅需约300个可训练参数，在保持全局准确性的同时提升个性化排名性能。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱基础模型在链接预测上表现出色，但无法捕捉个体用户偏好，导致通用关系推理与个性化排名之间存在关键脱节。

Method: 提出GatedBias框架，采用结构门控适应机制：将用户特定特征与图导出的二元门结合，生成可解释的每实体偏置，仅需约300个可训练参数，无需重新训练或影响全局准确性。

Result: 在两个基准数据集（Amazon-Book和Last-FM）上评估，显示在一致性指标上有统计显著改进，同时保持群体性能。反事实扰动实验验证了因果响应性：受益于特定偏好信号的实体在信号增强时排名改进提高6-30倍。

Conclusion: 基础模型的个性化适应可以是参数高效且因果可验证的，能够桥接通用知识表示与个体用户需求。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [29] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出Monadic Context Engineering (MCE)，一种基于函子、应用函子和单子的代数结构的新型智能体架构范式，用于解决当前智能体设计中状态管理、错误处理和并发等跨领域问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的自主智能体架构通常采用命令式、临时性设计模式，导致系统脆弱，存在状态管理困难、错误处理复杂和并发控制等问题。

Method: 引入Monadic Context Engineering (MCE)范式，利用函子、应用函子和单子的代数结构为智能体设计提供形式化基础。将智能体工作流视为计算上下文，通过抽象代数特性内在管理跨领域关注点。使用单子变换器系统组合这些能力。

Result: MCE使开发者能够从简单、可独立验证的组件构建复杂、健壮且高效的AI智能体。进一步扩展该框架描述元智能体，通过元编程动态创建和管理子智能体工作流。

Conclusion: Monadic Context Engineering为AI智能体设计提供了形式化、可组合的架构范式，通过代数结构的内在特性解决了当前智能体系统的脆弱性问题，支持构建复杂且可靠的自主智能体系统。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [30] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 该研究提出了DarkPatterns-LLM基准数据集和诊断框架，用于细粒度评估大语言模型输出中的操纵性内容，涵盖七个危害类别，并发现现有模型在检测操纵模式方面存在显著性能差异和弱点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的普及加剧了对其操纵性或欺骗性行为的担忧，这些行为可能损害用户自主权、信任和福祉。现有的安全基准主要依赖粗略的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 提出了DarkPatterns-LLM基准数据集和诊断框架，包含401个精心策划的示例，采用四层分析管道：多粒度检测(MGD)、多尺度意图分析(MSIAN)、威胁协调协议(THP)和深度上下文风险对齐(DCRA)。评估了GPT-4、Claude 3.5和LLaMA-3-70B等先进模型。

Result: 评估显示最先进模型在检测操纵模式方面存在显著性能差异（65.2%--89.7%），在检测自主权破坏模式方面存在一致的弱点。建立了首个标准化、多维度的LLM操纵检测基准。

Conclusion: DarkPatterns-LLM为LLM中的操纵检测建立了首个标准化、多维度的基准，提供了可操作的诊断工具，有助于开发更值得信赖的AI系统。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [31] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 论文主张将神经科学中的预测编码模型的三个关键组件（动作整合、层次组合结构和情景记忆）整合到基础模型中，以解决当前AI的缺陷，实现更安全、可解释、节能且类人的AI。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型虽然基于预测编码（最小化下一标记预测损失），但忽略了神经科学中预测编码模型的三个重要组件：动作与生成模型的紧密整合、层次组合结构、情景记忆。这些缺失导致AI存在幻觉、概念理解肤浅、缺乏能动性/责任感、安全可信度不足、能效低下等问题。

Method: 提出将神经科学和认知科学的预测编码模型组件整合到基础模型中：1）在多个抽象层次上整合动作与生成模型；2）采用组合生成架构；3）加入情景记忆系统。对比当前趋势（如思维链推理和检索增强生成），讨论如何用脑启发组件增强这些模型。

Result: 论文未报告具体实验结果，而是提出了理论框架和方向。通过整合这些脑启发组件，有望解决基础模型的当前缺陷：减少幻觉、增强概念理解、建立能动性、提高可解释性和安全性、提升能效。

Conclusion: 重新激活脑科学与AI之间历史上富有成果的思想交流，将有助于实现安全、可解释、以人为本的AI。整合预测编码模型的完整组件是迈向类人AI的关键路径。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [32] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee是一个用于智能生理医疗的统一、模块化、可配置工具包，解决了深度学习在生理信号分析中的数据格式异构、预处理不一致、模型碎片化和实验不可复现等问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析中面临四大挑战：数据格式异构、预处理策略不一致、模型流程碎片化、实验设置不可复现，这些限制了该领域的进一步发展。

Method: Tyee工具包包含三大创新：1) 12种信号模态的统一数据接口和可配置预处理流水线；2) 模块化可扩展架构，支持灵活集成和快速原型开发；3) 端到端工作流配置，促进可复现和可扩展的实验。

Result: Tyee在所有评估任务中表现出稳定有效的实用性和泛化能力，在13个数据集中12个达到最先进水平，其余与基线方法持平或超越。

Conclusion: Tyee为智能生理医疗提供了一个统一、模块化、可配置的工具包，解决了该领域的关键挑战，促进了可复现和可扩展的研究，已在GitHub开源并持续维护。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [33] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: M³ob：利用多模态时空知识增强位置推荐的泛化能力，通过LLM增强的时空知识图谱构建统一时空关系图，解决模态间语义鸿沟问题


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动预测方法泛化能力有限：单模态方法受数据稀疏性和固有偏差限制，多模态方法难以有效捕捉静态多模态表示与时空动态之间的语义鸿沟

Method: 1. 利用LLM增强的时空知识图谱构建统一时空关系图；2. 设计门控机制融合不同模态的时空图表示；3. 提出STKG引导的跨模态对齐，将时空动态知识注入静态图像模态

Result: 在六个公开数据集上的实验表明，该方法不仅在正常场景下取得一致改进，在异常场景下也展现出显著的泛化能力

Conclusion: M³ob通过有效利用多模态时空知识来表征移动动态，解决了现有方法的局限性，为位置推荐任务提供了更强大的解决方案

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [34] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 研究探讨让大语言模型互相审阅预测能否提升准确性，发现在信息共享的多样化模型组中能显著改善预测，但在同质化模型组中无效，额外上下文信息也无帮助。


<details>
  <summary>Details</summary>
Motivation: 结构化审议已被证明能提升人类预测者的表现，本研究旨在探索类似的干预措施——让大语言模型在更新预测前互相审阅彼此的预测——是否也能提升大语言模型的预测准确性。

Method: 使用来自Metaculus Q2 2025 AI预测锦标赛的202个已解决二元问题，评估了四种场景下的准确性：(1)多样化模型+分布式信息，(2)多样化模型+共享信息，(3)同质化模型+分布式信息，(4)同质化模型+共享信息。比较了模型在互相审阅预测前后的表现。

Result: 干预措施在场景(2)中显著提高了准确性，将Log Loss降低了0.020（相对改善约4%，p=0.017）。但在同质化模型组（同一模型的三个实例）中进行相同过程时未观察到任何益处。意外的是，为LLMs提供额外上下文信息并未改善预测准确性。

Conclusion: 审议可能是改善LLM预测的可行策略，特别是在多样化模型组中。然而，同质化模型组的审议无效，且信息池化机制未能通过额外上下文信息得到验证，表明模型多样性在审议过程中起关键作用。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [35] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE是一个两阶段、证据耦合的RAG系统评估框架，通过深度分析推理和概率评分提供可解释、置信度感知的评估，同时采用瑞士制锦标赛将计算复杂度从O(N²)降低到O(N log N)。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统向更复杂架构演进，需要通过可解释和鲁棒的评估来确保其可信度。现有的标量指标存在可解释性有限、不确定性量化不足以及在多系统比较中计算效率低下的问题，阻碍了RAG技术的负责任部署。

Method: DICE采用两阶段、证据耦合的框架：1) 结合深度分析推理与概率{A, B, Tie}评分，生成透明、置信度感知的判断；2) 采用瑞士制锦标赛方法，将计算复杂度从O(N²)降低到O(N log N)，同时保持排序保真度。

Result: 在中文金融QA数据集上的验证显示，DICE与人类专家的一致性达到85.7%，显著优于RAGAS等现有LLM基准指标。在八系统评估中实现了42.9%的计算量减少，同时保持了排序保真度。

Conclusion: DICE建立了一个负责任、可解释且高效的范式，用于可信赖的RAG系统评估，通过可解释的推理痕迹支持可问责的系统改进，实现系统性错误诊断和可操作的见解。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [36] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 本文提出了一种理论框架，将情景记忆与强化学习相结合，使大语言模型智能体能够通过反思机制进行持续和体验式学习，无需反向传播或模型微调。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和部署之间存在严格分离，需要反向传播或模型微调来实现适应。本文旨在通过反思机制使智能体能够在交互中持续学习，打破训练与部署的界限。

Method: 提出了状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互：写入存储交互结果（策略评估），读取检索相关历史案例（策略改进）。该过程诱导出增强状态记忆表示的等效马尔可夫决策过程。

Result: 框架通过熵正则化策略迭代实例化，并建立了收敛保证。当情景记忆增长并充分覆盖状态空间时，所得策略收敛到最优解。

Conclusion: 该工作为基于记忆增强和检索的语言模型智能体提供了理论基础，使其能够在不更新参数的情况下实现持续适应，为无需参数更新的持续学习提供了原则性框架。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [37] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench是一个分层科学智能基准测试，包含5个层级、8735个实例，涵盖6大学科，支持多模态输入，用于全面评估大模型在完整科研工作流程中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有科学智能基准测试过于碎片化，专注于狭窄任务，未能反映真实科学探究的层次性和多学科性。需要一个新的综合性基准来评估大模型在整个科研工作流程中的能力。

Method: 设计了包含5个层级的HiSciBench基准：科学素养(L1)、文献解析(L2)、基于文献的问答(L3)、文献综述生成(L4)和科学发现(L5)。包含8735个实例，涵盖数学、物理、化学、生物、地理、天文6大学科，支持文本、公式、图表等多模态输入和跨语言评估。

Result: 对GPT-5、DeepSeek-R1等领先模型的评估显示显著性能差距：在基础素养任务上准确率可达69%，但在发现级挑战上急剧下降到25%。

Conclusion: HiSciBench为评估科学智能设立了新标准，提供了可操作的见解，有助于开发更强大、更可靠的模型。基准将公开发布以促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [38] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma模型通过引入多头几何注意力机制，使用多种代数变换（实数、复数、分裂复数、对偶数）来增强知识图谱推理的表达能力，相比单一关系变换的Ultra模型在零样本归纳链接预测上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型（如Ultra）依赖单一关系变换（如逐元素乘法）进行消息传递，限制了表达能力，无法捕捉多样化图谱中展现的不同关系和结构模式。

Method: 提出Gamma模型，引入多头几何注意力机制，用多种并行代数变换（实数、复数、分裂复数、对偶数）替代单一关系变换，并通过关系条件注意力融合机制在链接级别自适应融合这些变换，使用轻量级门控和熵正则化。

Result: 在56个多样化知识图谱上的综合实验表明，Gamma在零样本归纳链接预测中持续优于Ultra，在归纳基准测试中平均倒数排名提升5.5%，在所有基准测试中提升4.4%。

Conclusion: Gamma通过互补的几何表示提高了知识图谱推理的表达能力，多头几何注意力机制能够更好地捕捉多样化图谱中的关系和结构模式，证明了多种代数变换组合的有效性。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [39] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 研究表明，在K-12教育中，基于大语言模型的辅导系统在评估学生知识变化方面不如传统的深度知识追踪模型准确可靠，需要混合框架来确保负责任的教学设计。


<details>
  <summary>Details</summary>
Motivation: 针对K-12教育中普遍存在的误解——认为生成式模型可以替代传统的学习者建模进行自适应教学，特别是在欧盟AI法案将其列为高风险领域需要负责任设计的背景下，本研究旨在探讨LLM辅导系统的局限性。

Method: 研究比较了深度知识追踪模型与广泛使用的大语言模型（包括零样本和微调版本），使用大型开放数据集进行评估，重点关注预测准确性、可靠性和时间一致性，并进行定性分析。

Result: 深度知识追踪模型在下一步正确性预测方面表现最佳（AUC=0.83），始终优于大语言模型。微调虽使LLM的AUC提高约8%，但仍比DKT低6%，且早期序列错误率更高。时间分析显示DKT保持稳定、方向正确的掌握度更新，而LLM变体表现出显著的时间弱点。

Conclusion: 大语言模型单独使用难以匹敌成熟的智能辅导系统效果，负责任的辅导需要结合学习者建模的混合框架，特别是在计算效率和评估一致性方面，传统方法仍具优势。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [40] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason是一个通过R1风格方法训练的视觉语言模型，在医疗影像领域使用有限资源（2000个SFT样本、1000个RL样本、单A100 GPU）进行强化学习，发现GRPO能提升同分布性能但损害跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的强化学习在推理任务上取得进展，但在资源受限的医疗影像应用中仍未被充分探索。研究者希望了解在有限计算资源下，强化学习方法对医疗视觉语言模型性能的影响。

Method: 采用R1风格方法：先进行监督微调（SFT），然后使用GRPO进行强化学习。仅使用2000个SFT样本、1000个RL样本和单个A100 GPU。在CheXpert和NIH基准上进行评估，分析同分布性能和跨数据集泛化能力。

Result: GRPO在同分布数据集（CheXpert）上性能提升23%（macro-F1=0.346），但在跨数据集（NIH）上性能下降19%。这反映了与高资源模型类似的问题，表明问题源于RL范式而非模型规模。SFT检查点在RL优化前能独特地改善NIH性能，表明教师引导的推理能捕捉更多机构无关特征。

Conclusion: 对于需要跨不同人群鲁棒性的临床部署，精心策划的监督微调可能优于激进的强化学习。结构化推理支架对通用视觉语言模型有益，但对医学预训练模型增益有限。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [41] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出了Intrinsic Self-reflective Preference Optimization (q)方法，解决了DPO的两个根本限制：最优策略依赖于任意建模选择，以及孤立处理响应生成未能利用成对数据中的比较信息。


<details>
  <summary>Details</summary>
Motivation: DPO及其变体虽然因简单性和离线稳定性成为大语言模型对齐的标准方法，但存在两个根本限制：1) 最优策略依赖于标量化函数、参考策略等任意建模选择，导致行为反映参数化伪影而非真实偏好；2) 孤立处理响应生成未能利用成对数据中的比较信息，未能挖掘模型内在自反思能力。

Method: 提出了Intrinsic Self-reflective Preference Optimization (q)方法，推导出基于上下文和替代响应的全局最优策略。该方法作为即插即用增强，无需架构更改或推理开销，同时保证对标量化和参考选择的鲁棒性。

Result: 实验证明该方法在胜率和长度控制指标上取得一致改进，验证了释放自反思能力能够产生更鲁棒、更符合人类对齐的LLMs。

Conclusion: q方法在理论上优于DPO/RLHF，同时保证对标量化和参考选择的鲁棒性，通过解锁模型的自反思能力，能够产生更鲁棒、更符合人类对齐的大语言模型。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [42] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为当前评估AI系统情感智能的框架需要改进，因为它们未能全面衡量AI相关的EI各个方面，并提出了改进评估策略的方案。


<details>
  <summary>Details</summary>
Motivation: 当前评估人工智能系统情感智能的框架存在不足，未能充分考虑哪些人类EI方面适用于AI系统，哪些不适用，导致评估不全面且缺乏理论基础。

Method: 首先回顾不同情感理论和一般EI理论，评估它们对人工系统的适用性；然后批判性评估现有基准框架，识别其不足之处；最后提出改进EI评估策略的方案。

Result: 识别出现有EI评估框架的局限性，特别是缺乏对情感本质和情感智能的坚实基础，以及未能区分适用于AI和不适用于AI的EI方面。

Conclusion: 需要改进AI系统情感智能的评估策略，建立更全面的框架，区分人类EI中适用于AI的方面（如感知、解释、响应和适应情感状态的能力）和不适用于AI的方面（如现象学体验）。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [43] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL框架将三个专门的LLM智能体嵌入MCTS循环，通过规划器、模拟器和批评器的协同工作，将MCTS从暴力搜索转变为引导式、自校正的推理过程，显著提升了复杂规划任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要探索和自校正的复杂规划任务中表现不佳，其线性推理过程难以从早期错误中恢复。而像蒙特卡洛树搜索这样的搜索算法在稀疏奖励引导下效果有限，且未能充分利用LLM的丰富语义能力。

Method: SPIRAL框架在MCTS循环中嵌入三个专门的LLM智能体：规划器提出创造性下一步，模拟器通过预测现实结果来接地搜索，批评器通过反思提供密集奖励信号。这种协同工作将MCTS从暴力搜索转变为引导式、自校正的推理过程。

Result: 在DailyLifeAPIs和HuggingFace数据集上，SPIRAL持续优于默认的思维链规划方法和其他最先进的智能体。在DailyLifeAPIs上达到83.6%的总体准确率，比次优搜索框架提高了超过16个百分点，同时展现出更优的令牌效率。

Conclusion: 将LLM推理构建为引导式、反思性和接地气的搜索过程，能够产生更强大和高效的自主规划器。这项工作展示了结构化多智能体协作在复杂任务规划中的有效性。

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [44] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 论文提出"模型信念"概念，利用LLM的token级概率分布替代单一输出，显著提高数据利用效率，在需求估计实验中计算效率提升约20倍。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，通常将模型输出作为单一数据点，这未能充分利用LLM固有的概率特性，导致数据利用效率低下。

Method: 提出并形式化"模型信念"概念，从LLM的token级概率分布中提取模型对选择替代方案的信念分布，证明其渐近等价于模型选择的均值但具有更高统计效率。

Result: 模型信念在有限运行次数下比模型选择本身能更好地解释和预测真实模型选择，在需求估计研究中将计算效率提升约20倍。

Conclusion: 模型信念应作为默认测量方法，能从LLM生成数据中提取更多信息，显著提高统计效率和计算效率。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [45] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 该论文研究了规划与验证领域中两个问题的联系：一致性规划与超属性模型检测，证明两者之间存在紧密的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索规划领域（一致性规划）与验证领域（超属性模型检测）之间的理论联系，这两个问题在各自领域都很重要但通常被分开研究。

Method: 方法包括：1）将超属性模型检测实例高效地约简为一致性规划实例，并证明约简的正确性和完备性；2）证明每个一致性规划问题本身就是一个超属性模型检测任务。

Result: 结果表明：1）超属性模型检测问题可以有效地转化为一致性规划问题；2）一致性规划问题本质上就是超属性模型检测任务，建立了两个问题之间的双向对应关系。

Conclusion: 结论是规划与验证领域的这两个核心问题之间存在紧密的理论联系，这种对应关系为两个领域之间的交叉研究和工具开发提供了理论基础。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [46] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文提出CubeBench基准测试，用于评估LLM智能体在物理世界部署中的空间认知能力，发现现有模型在长时程任务上完全失败


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在数字领域表现出色，但在物理世界部署中存在显著差距，主要挑战是形成和维护稳健的空间心理模型。研究者识别出三个阻碍这一转变的核心认知挑战：空间推理、通过心理模拟进行长时程状态跟踪，以及在部分观察下的主动探索。

Method: 引入CubeBench，一个以魔方为中心的新型生成基准测试。采用三层诊断框架逐步评估智能体能力：从具有完整符号信息的基础状态跟踪，到仅具有部分视觉数据的主动探索。通过分析领先LLM在基准测试上的表现，并设计诊断框架通过提供外部求解工具来隔离认知瓶颈。

Result: 实验显示领先LLM存在严重局限性，在所有长时程任务上的通过率均为0.00%，暴露了长期规划的根本性失败。通过分析失败模式，为开发更具物理基础性的智能体提供了关键见解。

Conclusion: CubeBench基准测试成功揭示了LLM智能体在物理世界部署中的核心认知瓶颈，特别是在长时程规划和空间推理方面的根本性缺陷。研究为开发更有效的物理基础智能体提供了诊断框架和指导方向。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [47] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher是一个集成交替思考和多模态思维链推理的工具集成推理智能体，能够自主决定是否以及如何调用多样化工具，无需依赖人工提示或工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流程的智能体在解决需要工具调用的现实世界问题时表现出有限的智能性。工具集成推理智能体能够自主推理和工具调用，正在成为处理涉及与外部环境多步交互的复杂决策任务的有力方法。

Method: MindWatcher采用交替思考范式，使模型能够在任何中间阶段在思考和工具调用之间切换，同时具备多模态思维链推理能力，允许在推理过程中操作图像以获得更精确的搜索结果。系统配备全面的辅助推理工具套件，并构建大规模高质量的本地图像检索数据库。

Result: 实验表明，MindWatcher通过卓越的工具调用能力，匹配甚至超越了更大或更新模型的表现。研究还揭示了智能体训练中的关键见解，如智能体强化学习中的遗传继承现象。

Conclusion: MindWatcher作为一个工具集成推理智能体，通过交替思考和多模态思维链推理的结合，在解决广泛领域的多模态问题方面表现出色，同时设计了更高效的训练基础设施，提高了训练速度和硬件利用率。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [48] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: 提出HiR框架，通过"选择-重写"策略将失败尝试重放为成功样本，解决复杂指令跟随任务中奖励稀疏的问题，提高强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言模型指令跟随任务中面临奖励稀疏问题：初始模型难以生成满足所有约束的高质量响应，导致学习困难。

Method: 提出Hindsight instruction Replay (HiR)框架，采用选择-重写策略，将失败尝试中已满足约束的部分重放为成功样本，结合原始样本进行强化学习，理论化为指令级和响应级的双重偏好学习。

Result: 在多种指令跟随任务中取得显著效果，同时需要更少的计算资源。代码和数据集已开源。

Conclusion: HiR是一种样本高效的强化学习框架，通过重放失败尝试为成功样本，有效解决了复杂指令跟随任务中的奖励稀疏问题，提高了学习效率。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [49] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: CreativeDC是一种两阶段提示方法，通过解耦创造性探索和约束满足，解决LLM生成教育问题时存在的"人工蜂群思维"效应，显著提高问题多样性、新颖性和实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育问题生成方面潜力巨大，但存在"人工蜂群思维"效应，导致同一模型内和不同模型间输出过于相似，学生接触到的LLM生成问题重复性高，损害思维多样性。

Method: 基于Wallas创造力理论和Guilford发散-收敛思维框架，提出CreativeDC两阶段提示方法：第一阶段进行创造性探索，第二阶段进行约束满足，将LLM推理过程显式分解为不同阶段。

Result: CreativeDC在多样性、新颖性和实用性综合评估中表现显著优于基线方法，能生成更多有效且独特的问题，随着采样数量增加，其生成的不同问题数量增长更快。

Conclusion: CreativeDC通过结构化两阶段方法有效缓解LLM的"人工蜂群思维"问题，为教育领域生成多样化、新颖且实用的学习材料提供了有效解决方案。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [50] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: 提出I-PERI算法解决联邦因果发现中的客户端异质干预问题，通过恢复客户端图并集的CPDAG，利用干预诱导的结构差异定向边，获得更紧的Φ-Markov等价类


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法通常假设所有客户端共享相同因果模型，但在实际应用中（如医院间），客户端特定的策略或协议会引入异质且未知的干预，这种理想化假设不现实

Method: 提出I-PERI算法：1) 恢复客户端图并集的CPDAG；2) 利用干预在不同客户端诱导的结构差异定向边；3) 获得更紧的Φ-Markov等价类，用Φ-CPDAG表示

Result: 提供了I-PERI算法的理论收敛保证和隐私保护特性证明，在合成数据上的实证评估展示了算法的有效性

Conclusion: I-PERI算法能够有效处理联邦因果发现中的客户端异质干预问题，通过利用干预诱导的结构差异获得更精确的因果结构，为实际应用提供了实用解决方案

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [51] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Web World Model (WWM) 是一种结合传统Web框架可靠性和生成式世界模型灵活性的中间方案，通过Web代码实现世界状态和"物理规则"，LLM生成上下文和决策，构建可控且开放的环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两极分化：传统Web框架提供可靠但固定的环境，而完全生成式世界模型追求无限环境但牺牲了可控性和工程实用性。需要一种既能保证逻辑一致性又能支持开放探索的中间方案。

Method: 提出Web World Model (WWM)，将世界状态和"物理规则"用普通Web代码实现以确保逻辑一致性，同时使用大语言模型在结构化潜在状态上生成上下文、叙事和高层决策。构建了基于真实Web技术栈的WWM套件。

Result: 成功构建了多种WWM系统：基于真实地理的无限旅行地图、虚构星系探索器、Web规模的百科全书和叙事世界、模拟和游戏环境。提出了WWM的实用设计原则。

Conclusion: Web技术栈本身可以作为世界模型的可扩展基础，实现可控且开放的环境。WWM为语言智能体提供了既能保证逻辑一致性又能支持创造性探索的持久世界。

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [52] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: GPU-Virt-Bench是一个全面的GPU虚拟化基准测试框架，用于评估软件GPU虚拟化系统（如HAMi-core和BUD-FCSP）与硬件MIG技术，涵盖56个性能指标和10个类别，为多租户环境中的GPU资源部署提供决策依据。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速工作负载（特别是AI和LLM推理）的激增，云和容器环境对高效GPU资源共享的需求急剧增加。虽然NVIDIA的MIG技术提供硬件级隔离，但仅限于高端数据中心GPU。软件虚拟化解决方案（如HAMi-core和BUD-FCSP）为更广泛的GPU系列提供了替代方案，但缺乏标准化的评估方法。

Method: 提出了GPU-Virt-Bench基准测试框架，该框架通过56个性能指标（分为10个类别）来评估GPU虚拟化系统。这些类别包括：开销、隔离质量、LLM特定性能、内存带宽、缓存行为、PCIe吞吐量、多GPU通信、调度效率、内存碎片化和错误恢复。该框架能够系统比较软件虚拟化方法与理想MIG行为。

Result: 通过评估HAMi-core、BUD-FCSP和模拟的MIG基线，展示了该框架的实用性，揭示了对于生产部署决策至关重要的性能特征。该框架为从业者在多租户环境中部署GPU资源提供了可操作的见解。

Conclusion: GPU-Virt-Bench是一个全面的基准测试框架，填补了GPU虚拟化系统标准化评估的空白，能够帮助从业者系统比较不同虚拟化方案，并为生产环境中的GPU资源部署提供重要决策依据。

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [53] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: HybridFlow是一个资源自适应的边缘-云协作推理框架，通过细粒度任务分解和并行执行来降低LLM推理延迟和token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临高推理延迟和token消耗的挑战，现有边缘-云协作方法采用粗粒度任务分配策略，无法充分利用细粒度推理并行性，导致冗余计算和资源利用效率低下。

Method: HybridFlow采用两阶段方法：1) 任务分解与并行执行：将复杂查询动态拆分为相互依赖的子任务，依赖关系满足后立即执行；2) 资源感知子任务路由：学习型路由器根据预测的效用增益和实时预算状态，自适应地将每个子任务分配给边缘或云端模型。

Result: 在GPQA、MMLU-Pro、AIME和LiveBench-Reasoning上的综合评估表明，HybridFlow有效减少了端到端推理时间和总体token使用量，同时保持了有竞争力的准确性。

Conclusion: HybridFlow通过细粒度的边缘-云协作推理框架，解决了LLM在边缘设备部署中的延迟和资源消耗问题，实现了更高效的资源利用和更快的推理速度。

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [54] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: Cleave是一个用于边缘设备上分布式基础模型训练的新范式，通过选择性混合张量并行和参数服务器框架，解决了现有边缘训练方法的性能、可扩展性、内存限制和通信开销等问题。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型训练生态系统高度中心化，仅限于大型云数据中心运营商，训练成本高昂。利用边缘设备闲置计算资源进行去中心化训练是一个有前景的民主化替代方案，但现有边缘训练方法存在性能不足、可扩展性有限、内存超限、通信开销大、设备异构性和动态性处理不佳等问题。

Method: 提出Cleave范式，采用选择性混合张量并行方法精细划分训练操作，结合参数服务器中心的训练框架来处理设备内存限制并避免通信瓶颈。同时使用成本优化模型指导设备选择和训练工作负载分配，有效应对设备异构性和动态变化。

Result: 评估显示Cleave能够匹配基于云的GPU训练性能，可扩展到更大模型和数千台设备，支持比基线边缘训练方法多8倍的设备数量。在每批次训练时间上比最先进的边缘训练方法快10倍，并能高效处理设备故障，恢复速度比先前方法快至少100倍。

Conclusion: Cleave通过创新的选择性混合张量并行和参数服务器框架，成功实现了在边缘设备上高效训练大型基础模型，解决了现有边缘训练方法的局限性，为去中心化模型训练提供了可行的解决方案。

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [55] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: 提出一个端到端的LLM框架，通过性能反馈优化GPU内核，无需构建完整应用程序，实现跨平台、低成本的GPU内核优化。


<details>
  <summary>Details</summary>
Motivation: 高性能计算中，热点GPU内核是主要瓶颈，专家手动调优成本高且难以移植。现有LLM方法通常假设内核可以廉价编译执行，但在大型应用中完整构建和运行成本高昂。

Method: 从独立提取的热点内核自动生成最小可执行程序(MEP)，进行多轮迭代优化和评估。集成自动错误修复和性能模式继承来修复故障、保持正确性、重用有效的平铺/内存/同步策略，降低搜索成本。优化后的变体重新集成到原始应用中进行验证。

Result: 在NVIDIA GPU和Haiguang DCU平台上评估，平均加速比达到：PolyBench在NVIDIA上5.05倍，PolyBench在DCU上7.77倍，AMD APP SDK上1.77倍，三个热点内核上1.25倍，超越直接LLM优化。

Conclusion: 该方法无需完整源代码依赖，提供跨平台可移植性，实现了实用、低成本的GPU内核优化。

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [56] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: 提出自适应GPU资源分配框架，在serverless平台上为多智能体系统实现85%延迟降低，同时保持与静态分配相当的吞吐量


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在serverless GPU平台上部署面临资源分配挑战，包括异构工作负载、不同计算需求和成本效益扩展需求

Method: 自适应GPU资源分配框架，基于工作负载特征、智能体优先级和最小资源需求动态分配GPU资源，使用O(N)复杂度算法实现实时适应

Result: 相比轮询调度减少85%延迟，吞吐量与静态分配相当；在延迟、成本和GPU利用率指标上优于静态平均和轮询策略

Conclusion: 该框架为在serverless GPU基础设施上部署成本效益高的多智能体AI系统提供了实用解决方案

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [57] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: AiiDAlab平台从材料科学扩展到多学科，通过浏览器界面简化复杂计算工作流，自动追踪模拟溯源确保可重复性，并与电子实验笔记本集成支持FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力增长，需要自动化研究工作流来管理大量相互依赖的模拟，但执行这些工作流通常需要技术专业知识来设置输入、解释输出和处理远程机器上的并行代码执行。

Method: 开发AiiDAlab平台，提供直观的Web浏览器用户界面，使复杂计算工作流易于访问，基于AiiDA引擎自动追踪完整模拟溯源，确保可重复性。

Result: AiiDAlab已从计算材料科学扩展到量子化学、大气建模、电池研究和大规模设施实验数据分析等多个学科，并用于教育环境。通过用户反馈改进，简化用户入门、优化计算资源访问，并提供处理大数据集的机制。

Conclusion: AiiDAlab使科学家能够专注于研究而非计算细节，通过与电子实验笔记本集成强化FAIR原则遵守，支持数据为中心的科学学科轻松生成可重复的开放研究数据。

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [58] [BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs](https://arxiv.org/abs/2512.22174)
*Muhammad Zeeshan Karamat,Sadman Saif,Christiana Chamon Garcia*

Main category: cs.DC

TL;DR: BitFlipScope是一个可扩展的软件框架，用于在Transformer架构中定位由硬件退化、宇宙辐射或故障注入攻击引起的比特翻转故障，支持有参考模型和无参考模型两种部署场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在实际部署中容易受到比特翻转故障的影响，这些故障会静默地破坏内部参数，导致不可预测或危险的行为。定位这些故障对于诊断问题、应用针对性修复措施以及恢复模型功能至关重要。

Method: 提出BitFlipScope框架，包含两种方法：1）有干净参考模型时，通过输出、隐藏状态和内部激活的差异分析来检测异常行为；2）无参考模型时，使用残差路径扰动和损失敏感性分析直接从损坏模型中推断故障影响区域。

Result: 该框架不仅能有效诊断故障，还支持无需微调的轻量级性能恢复，为在硬件易错和对抗环境中恢复损坏模型提供了实用路径。

Conclusion: BitFlipScope是实现可信赖、具有故障恢复能力的LLM部署的重要一步，特别适用于硬件易错和对抗性环境。

Abstract: Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.

</details>


### [59] [iOS as Acceleration](https://arxiv.org/abs/2512.22180)
*Alexander K. Chen*

Main category: cs.DC

TL;DR: 本文探索利用iOS手机作为分布式计算资源来增强本地机器学习能力，通过流水线并行技术实现模型训练和推理加速，为零成本提升弱计算环境提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习需要强大计算资源，但云计算的成本、隐私限制或物理环境不可用等问题使得本地计算成为必要。移动设备作为普遍存在但未充分利用的资源，具有改善弱计算环境的潜力。

Method: 提出概念验证系统，利用iOS设备通过分布式流水线并行技术，克服内存限制、热节流和操作系统沙盒等限制，实现模型训练、批量推理和智能体工具使用的加速。

Result: 展示了在弱计算环境中通过iOS设备实现显著性能提升，能够加速适度规模的模型训练、批量推理和智能体LRM工具使用。

Conclusion: 移动设备有潜力为机器学习做出更大贡献，本文提出的方法为零成本改善本地计算环境提供了可行方案，并讨论了实际用例、限制和未来研究方向。

Abstract: Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.

</details>


### [60] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: MatKV通过预计算RAG文档的键值向量并存储在闪存中，在推理时直接复用，将推理时间和功耗减半，同时支持GPU并行加载和低端GPU解码优化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理成本已超过训练成本，RAG在处理长输入时预填充阶段计算键值向量能耗高、耗时长，需要提高RAG推理效率。

Method: 提出MatKV方案：预计算RAG对象的键值向量，将其物化存储在廉价但快速、能效高的闪存中，在推理时直接复用这些预计算的KVs，避免使用GPU重新计算。

Result: 实验表明，相比GPU完全计算KVs，MatKV将RAG工作负载的推理时间和功耗减少一半，且对问答任务准确性影响不大。此外支持GPU并行加载和低端GPU解码优化。

Conclusion: MatKV能够使大规模生成式AI应用更具成本效益、能效更高，并在更广泛的任务和硬件环境中更易访问，为RAG推理效率提供了有效解决方案。

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [61] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: SPUMA是OPENFOAM的完整GPU移植版本，支持NVIDIA和AMD GPU，通过可移植编程模型和内存池管理器实现高性能计算，在LUMI和Leonardo集群上测试显示良好的强扩展性和弱扩展性，能耗降低达82%。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU在HPC中广泛应用，但在开源CFD软件（如OPENFOAM）中的可编程性仍然是一个挑战，特别是在混合集群环境中有效利用现代加速器。

Method: 基于可移植编程模型，采用内存池管理器利用现代GPU的统一内存特性，将OPENFOAM完整移植到NVIDIA和AMD GPU上，形成SPUMA框架。

Result: 在LUMI（AMD MI250X）和Leonardo（NVIDIA A100）集群上测试显示：强扩展性在每GPU负载800万网格时达到65%效率；弱扩展性在20个GPU上为75-85%；使用NVIDIA AmgX求解器时效率不低于90%；一个A100 GPU相当于200-300个Intel Sapphire Rapids核心；能耗相比CPU降低达82%。

Conclusion: SPUMA成功实现了OPENFOAM的GPU移植，在混合集群上展示了良好的性能和能效，为CFD在预百亿亿次计算系统上的应用提供了有效解决方案。

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [62] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: 提出了一种基于云原生架构的智能PMU数据处理框架，通过分布式流处理、容器化微服务和弹性资源编排，实现低延迟、可扩展的电网实时监控与分析。


<details>
  <summary>Details</summary>
Motivation: 随着PMU部署规模的扩大，传统集中式处理架构在延迟、可扩展性和可靠性方面面临挑战，难以应对现代电网动态运行条件下的数据量和处理速度需求。

Method: 采用云原生架构，整合边缘与云计算，使用分布式流处理、容器化微服务和弹性资源编排技术，并集成机器学习模型进行时间序列分析。

Result: 分析模型显示该架构可实现亚秒级响应时间，并能扩展到大规模PMU部署，同时具备安全隐私机制，适用于关键基础设施环境。

Conclusion: 该云原生架构为下一代智能电网分析提供了稳健灵活的基础，能够有效解决PMU数据处理中的延迟、可扩展性和可靠性问题。

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [63] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: Pick and Spin是一个基于Kubernetes的LLM编排框架，通过统一部署、自适应缩容和混合路由策略，显著提升自托管大语言模型的效率和经济性。


<details>
  <summary>Details</summary>
Motivation: 组织自托管大语言模型在隐私、成本控制和定制化方面具有吸引力，但在GPU利用率、工作负载路由和可靠性方面面临挑战，需要一种可扩展且经济的解决方案。

Method: 基于Kubernetes构建，包含统一Helm部署系统、自适应缩容自动化、混合路由模块（结合关键词启发式和轻量级DistilBERT分类器），在成本、延迟和准确性之间进行平衡。

Result: 在四个模型（Llama-3 90B、Gemma-3 27B、Qwen-3 235B、DeepSeek-R1 685B）上评估，涵盖8个公共基准数据集、5种推理策略和2种路由变体，共31,019个提示和163,720次推理运行。相比静态部署，实现了高达21.6%的成功率提升、30%的延迟降低和33%的GPU成本降低。

Conclusion: Pick and Spin框架使自托管LLM编排变得可扩展且经济，显著提升了部署效率、降低了成本，为组织自托管大语言模型提供了实用的解决方案。

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [64] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: Nightjar是一种基于学习的自适应推测解码算法，通过动态调整推测长度来优化LLM推理性能，在实时服务场景中相比标准推测解码实现了最高14.8%的吞吐量提升和20.2%的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 标准推测解码使用固定的推测长度，无法适应动态请求负载变化，在计算密集型的高负载环境中会因验证开销导致性能下降，这在实际服务场景中造成了显著的性能瓶颈。

Method: 提出Nightjar算法，这是一种基于学习的自适应推测推理方法，能够根据请求负载动态选择最优推测长度，甚至在没有收益时完全禁用推测解码，以适应不同的批处理大小。

Result: 实验表明，Nightjar相比标准推测解码实现了最高14.8%的吞吐量提升和20.2%的延迟降低，在实时服务中展现出强大的效率优势。

Conclusion: Nightjar通过自适应调整推测长度解决了标准推测解码在动态负载环境中的性能瓶颈问题，为LLM推理的实时服务提供了更高效、更灵活的解决方案。

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [65] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: RobustRL是一个针对LLM强化学习后训练的系统，通过角色隔离、智能故障检测、非中断恢复和动态重连机制，显著提高了GPU集群故障下的训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM故障容错框架主要针对训练或推理场景，而RL后训练混合了训练和推理工作负载，现有方案无法充分利用异步执行的优化潜力。当GPU机器故障时，传统方法需要重启整个RL任务，造成巨大开销。

Method: 1. 角色隔离：将训练器、rollout和管理角色视为独立分布式子任务；2. 智能检测：实现角色感知监控，区分实际故障与角色特定行为；3. 非中断恢复：训练器通过rollout热备快速恢复，rollout进行隔离机器替换；4. 动态重连：用UCX点对点通信替代静态集合通信，实现权重即时同步。

Result: 在256-GPU集群上使用Qwen3-8B-Math工作负载，故障注入频率10%的条件下，RobustRL的ETTR（有效训练时间比）超过80%，相比ByteRobust的60%有显著提升，端到端训练时间加快8.4%-17.4%。

Conclusion: RobustRL通过角色隔离的故障容错机制，显著提升了RL后训练在GPU故障场景下的系统鲁棒性和训练效率，为大规模RL训练提供了实用的容错解决方案。

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [66] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 本文首次对多模态大语言模型推理的能耗进行详细分析，发现多模态输入导致17%-94%的额外能耗，主要源于视觉编码和视觉token序列扩展，并提出了阶段级DVFS优化方案。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文本模型基础上引入视觉等模态，但现有研究主要关注文本模型，对多模态引入的能耗权衡缺乏理解。本文旨在填补这一空白，分析多模态推理中的能效问题。

Method: 将MLLM推理流程分解为视觉编码、预填充和解码三个阶段，在NVIDIA A100 GPU上评估四种代表性MLLM。通过分析GPU功耗轨迹，量化多模态推理相比纯文本基线的额外能耗，并探索阶段级动态电压频率缩放优化。

Result: 多模态推理能耗比纯文本基线高17%-94%，瓶颈因模型架构而异：计算密集的视觉编码器或大量视觉token序列的预填充阶段。发现GPU在多模态执行中存在显著利用率不足，输入复杂度导致不同模型的能耗扩展行为差异明显。阶段级DVFS优化可在仅轻微性能影响下实现能耗节省。

Conclusion: 多模态LLM服务系统存在显著的能效优化空间，模型架构选择、输入处理策略和运行时优化（如阶段级DVFS）是设计更节能多模态LLM服务系统的关键。研究结果为构建高效多模态AI系统提供了实用见解和具体指导。

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [67] [OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads](https://arxiv.org/abs/2512.22743)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: OptiNIC是一个针对分布式机器学习优化的RDMA传输协议，通过消除重传和顺序交付要求，利用ML对数据丢失的容忍性来降低尾部延迟。


<details>
  <summary>Details</summary>
Motivation: 随着分布式机器学习扩展到数千个GPU，集体通信中的尾部延迟已成为主要瓶颈。现有RDMA传输协议（如RoCE、IRN等）强制实施严格可靠性和顺序交付，依赖重传和包排序，这在ML场景中引入了复杂性和延迟，即使罕见的包延迟也会阻塞整个模型流水线。

Method: OptiNIC是一个领域特定的RDMA传输协议，重新审视传统可靠性保证，基于ML对部分或丢失数据的容忍性。它从NIC中消除重传和顺序交付，实现尽力而为、乱序的RDMA传输模型。引入自适应超时机制在数据可能丢失或延迟时触发前进，同时保留标准拥塞控制机制，将丢失恢复转移到ML流水线本身（如通过Hadamard变换和纠删码）。

Result: 评估显示，OptiNIC在两个公共云（Hyperstack和CloudLab）上，将训练和推理的时间到准确度（TTA）分别提高2倍，吞吐量提高1.6倍。同时将第99百分位延迟降低3.5倍，BRAM使用减少2.7倍，NIC对故障的恢复能力几乎翻倍。

Conclusion: OptiNIC为分布式ML工作负载提供了一个具有弹性、尾部优化的RDMA传输协议，通过利用ML应用对数据丢失的容忍特性，显著改善了通信性能。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.
  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).
  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.

</details>


### [68] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: Argus是一个面向异构边缘-云系统的LLM推理框架，通过令牌感知的任务卸载优化，解决推理时间可变性问题


<details>
  <summary>Details</summary>
Motivation: LLM在真实世界应用中面临推理时间可变性问题，特别是在异构边缘-云系统中。现有解决方案忽视了动态、随机和异构环境的特性，忽略了可变输出令牌长度和设备多样性的影响。

Method: Argus框架包含两个核心模块：1) 长度感知语义(LAS)模块，使用微调的语言模型预测输出令牌长度；2) Lyapunov引导的卸载优化(LOO)模块，通过新颖的迭代卸载算法(IODCC)解决整数非线性规划问题。

Result: 广泛的理论和实证评估表明，Argus在高度动态、异构的环境中实现了鲁棒性能和卓越效率。

Conclusion: Argus是第一个令牌感知的分布式边缘-云LLM推理框架，通过精确的令牌长度预测和优化的任务卸载策略，有效解决了LLM推理中的时间可变性问题。

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


### [69] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: LRH是一种新的分布式哈希方案，通过限制HRW选择到缓存局部窗口，在保持环结构的同时显著提升负载均衡和性能。


<details>
  <summary>Details</summary>
Motivation: 传统一致性哈希方案存在负载不均衡问题：基于环的方案需要大量虚拟节点才能降低峰值负载比，而多探针方法虽然改善了平衡性但导致内存访问分散、性能下降。

Method: 提出局部会合哈希(LRH)，保留令牌环结构但将最高随机权重选择限制在C个相邻物理节点的缓存局部窗口内。通过一次二分查找定位键，使用预计算的不同偏移量枚举C个候选节点，选择HRW胜出者。

Result: 在N=5000、V=256、K=5000万、C=8的基准测试中，LRH将最大/平均负载从1.2785降至1.0947，达到60.05 Mkeys/s，比8探针多探针一致性哈希快6.8倍，同时接近其负载平衡水平(1.0697)。

Conclusion: LRH在保持环结构优势的同时，通过缓存局部性显著提升了负载均衡和性能，解决了传统一致性哈希方案在平衡性和效率之间的权衡问题。

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [70] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: 该研究将离线性能优化框架应用于DevOps发布阶段的微服务应用，优化CPU和内存资源配置，解决了现有研究主要关注运维阶段而忽略发布前资源配置调优的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注DevOps运维阶段的智能调度和自动扩缩容，但忽略了发布阶段CPU和内存资源配置的优化。即使基于CPU使用率进行容器水平自动扩缩容，如果部署前没有对两种资源进行精细调优，容器仍可能分配不适当的内存资源。

Method: 使用TeaStore微服务应用评估性能优化框架，统计比较不同优化算法。研究探讨了因子筛选（减少搜索空间）与贝叶斯优化在不同目标下的应用策略。

Result: 研究表明：当目标是找到最优资源配置且采样预算有限时，前置因子筛选有助于减少搜索空间；当需要统计比较不同算法时，筛选也使搜索空间中所有数据点的数据收集变得可行；但当目标是找到接近最优的配置时，无筛选的贝叶斯优化效果更好。

Conclusion: 在DevOps发布阶段应用性能优化框架可以有效优化微服务应用的资源配置，不同优化目标需要采用不同的算法策略，因子筛选和贝叶斯优化各有适用场景。

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>


### [71] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: 论文探讨了如何通过控制循环实现微服务系统的自适应，分析了三种不同方法（软件架构方法、云原生Operator模式、传统编程技术）在实现TeaStore规范时的权衡，并提出了多层架构的解决方案。


<details>
  <summary>Details</summary>
Motivation: TeaStore规范为通过控制循环实现自适应提供了微服务案例研究，但实现需要考虑自适应系统的关键特性：系统范围一致性、规划能力和模块化，以有效解耦自适应控制逻辑与应用业务逻辑。

Method: 论文分析了三种实现方法：1）软件架构方法；2）云原生Operator模式；3）传统编程语言技术。通过比较这些方法在细粒度表达性自适应与系统范围控制之间的权衡，探讨如何有效解耦自适应控制逻辑。

Result: 分析表明这些方法并非互斥，可以结合形成多层架构用于自适应微服务。不同方法在重用自适应策略方面各有优势，需要根据具体场景选择合适的方法组合。

Conclusion: 通过结合软件架构方法、Operator模式和传统编程技术，可以构建多层架构的自适应微服务系统，平衡细粒度自适应表达与系统范围控制的需求，实现更有效的自适应策略重用。

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>
