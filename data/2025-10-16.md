<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 本文主张将时间序列分析重新构想为基于大语言模型的推理任务，强调因果结构和可解释性，而非仅依赖数值回归能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的时间序列分析需要超越表面趋势，揭示驱动因素。传统方法在政策变化、人类行为适应和突发事件等动态环境中存在局限，而现有LLM方法大多忽视其深层推理潜力。

Method: 将时间序列分析重新定义为推理任务，利用LLMs的多模态输入整合能力，重点关注因果结构和可解释性分析。

Result: 该方法使时间序列分析更接近人类对齐的理解，能够在复杂现实环境中提供透明和上下文感知的洞察。

Conclusion: 将LLMs作为推理工具而非单纯回归工具，能够显著提升时间序列分析的深度和实用性，特别是在动态变化的现实场景中。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [2] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: Pxplore是一个个性化学习路径规划框架，通过强化学习训练和LLM驱动的教育架构，将抽象学习目标转化为可计算信号，生成连贯、个性化和目标驱动的学习路径。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的个性化学习方法缺乏目标对齐规划机制，无法有效将个体学习目标转化为具体的学习路径。

Method: 设计结构化学习者状态模型和自动奖励函数，结合监督微调(SFT)和组相对策略优化(GRPO)训练策略，并在真实学习平台中部署。

Result: 大量实验验证了Pxplore在生成连贯、个性化和目标驱动学习路径方面的有效性。

Conclusion: Pxplore框架成功解决了目标对齐的个性化学习路径规划问题，为未来研究提供了代码和数据集支持。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [3] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: 本文提出了一种基于效用的分析模型，用于自动驾驶车辆的环境感知系统。该模型包含自定义数据集采集、基于YOLOv8s的目标检测模块，以及从训练模型性能值中测量感知服务效用的模块。实验验证了该模型在确定自动驾驶车辆正确感知方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶环境感知对智能交通至关重要，需要开发能够准确感知道路多目标并预测驾驶员感知的深度学习模型，以控制车辆运动。

Method: 提出基于效用的分析模型，包括：采集包含摩托车手、三轮车等独特对象的自定义数据集；使用YOLOv8s进行目标检测；从训练模型性能值中测量感知服务效用的模块。

Result: 实验结果显示三个性能最佳的YOLOv8s实例：SGD-based (mAP@0.5=0.832)、Adam-based (0.810)和AdamW-based (0.822)。虽然SGD模型的整体mAP最高，但AdamW模型在类别级别性能更优（汽车：0.921，摩托车手：0.899，卡车：0.793等）。

Conclusion: 提出的感知函数能够为自动驾驶车辆找到正确的感知，鼓励使用该感知模型来评估学习模型的效用并确定适合自动驾驶车辆的感知。

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [4] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型在机制性因果推理方面的能力，通过隐式因果链发现任务，分析模型如何生成连接给定因果对的中间因果步骤。研究发现LLMs在生成因果步骤的数量和粒度上存在差异，主要依赖关联模式匹配而非真正的因果推理。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何理解因果关系的机制，特别是它们能否识别和生成连接因果对的中间因果步骤，以评估其机制性因果推理能力。

Method: 使用诊断评估框架，让9个LLMs为气候变化论证研究中的因果对生成所有可能的中间因果步骤，分析生成的因果链结构。

Result: LLMs在生成因果步骤的数量和粒度上表现不一，虽然生成的因果链在逻辑上具有一致性和完整性，但其判断主要基于关联模式匹配而非真正的因果推理。

Conclusion: 研究为推进隐式机制性因果推理在论证环境中的未来工作奠定了坚实基础，包括基线因果链发现方法、诊断评估见解和带因果链的基准数据集。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [5] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 提出一种用于形式化法律案例推理的分类器模态逻辑，结合时间维度和法院层级来解决先例冲突


<details>
  <summary>Details</summary>
Motivation: 基于逻辑的模型可用于构建机器学习分类器的验证工具，这些分类器在法律领域通过案例推理预测新案件结果

Method: 引入分类器模态逻辑，整合案件时间维度和法院系统层级，为法律案例推理提供形式化框架

Result: 开发了能够处理先例冲突的逻辑系统，通过时间顺序和法院层级关系来协调不一致的先例

Conclusion: 该逻辑框架为验证法律机器学习分类器提供了形式化基础，能够更好地模拟法律案例推理过程

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [6] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: Hard2Verify是一个人工标注的步骤级验证基准，用于评估前沿LLM在数学证明中的步骤验证能力，发现开源验证器普遍落后于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 在IMO 2025等数学竞赛中，LLM需要生成不仅正确而且充分支持的证明步骤，因此需要强大的步骤级验证器来训练LLM推理系统。

Method: 构建了Hard2Verify基准，包含500多小时人工标注的步骤级验证数据，评估了29个生成式批评器和过程奖励模型在挑战性数学问题上的表现。

Result: 除了少数表现优异者外，开源验证器普遍落后于闭源模型，研究还分析了步骤级验证性能差的原因、验证器计算规模的影响以及自验证和验证-生成动态等基本问题。

Conclusion: Hard2Verify基准为评估前沿步骤级验证器提供了严格标准，揭示了当前验证器在数学证明步骤验证方面的局限性，为改进LLM推理系统提供了重要见解。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 本文提出了一种针对大语言模型的计算和内存成本优化方法，通过输入分解、渐进分解算法和专用加速器架构，显著降低了分解延迟并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和内存成本持续增长，已超过1T参数规模。现有模型分解方法主要关注权重分解，但运行时分解延迟往往超过分解带来的收益，需要更高效的分解方法。

Method: 采用渐进分解算法（Lanczos算法），设计协同加速器架构，引入计算复制方法将操作转向计算密集型区域，开发输出形状保持计算方案消除连续层的分解成本，采用多通道分解方法分别处理异常通道。

Result: 实现了6.2倍的加速，与A100 GPU相比端到端延迟改善了22%，模型质量损失较小（如在AI2推理挑战任务上仅3%）。

Conclusion: 通过适当的分解算法选择和硬件支持，输入分解可以显著有益，D-com加速器在保持模型质量的同时显著提升了性能。

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Dodoor是一种高效的随机化去中心化调度器，用于现代数据中心的任务调度。它基于批量更新的缓存服务器信息进行调度决策，减少了通信开销，在101节点异构集群上相比传统方法减少了55-66%的调度消息，提升了吞吐量和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心需要高效的任务调度器，但现有去中心化调度器依赖实时探测远程服务器，导致通信开销大。Dodoor旨在通过基于缓存信息的调度决策来减少通信成本。

Method: Dodoor利用带权重的球-箱模型和批量设置，基于批量更新的缓存服务器信息进行调度决策。它使用新颖的负载评分来衡量服务器负载，捕捉服务器与任务之间的反亲和性，而不是简单的待处理任务计数。

Result: 在101节点异构集群上的评估显示：Dodoor在两个工作负载上分别减少了55-66%的调度消息；吞吐量分别提升了33.2%和21.5%；平均完成时间延迟分别降低了12.1%和7.2%；尾部延迟分别改善了21.9%和24.6%。

Conclusion: Dodoor通过基于缓存信息的调度方法和新颖的负载评分机制，显著减少了通信开销，同时提高了调度性能，证明了其在现代数据中心任务调度中的有效性。

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [9] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 提出了一种新的集成分布式系统框架FDIRS，通过使用异构分布式数据库技术来提高系统性能、响应速度和可靠性，解决了之前框架的一些问题。


<details>
  <summary>Details</summary>
Motivation: 现有集成分布式系统框架在性能、效率和可靠性方面存在不足，需要开发新的框架来提升系统整体表现。

Method: 分析现有集成系统及其演进过程，简要介绍ERPSD和ERPDRT框架，然后详细阐述新的FDIRS框架，并使用异构分布式数据库技术来提高性能。

Result: 仿真结果显示，FDIRS框架在性能、响应速度和效率方面优于现有框架，成功提高了集成系统的可靠性和性能。

Conclusion: FDIRS框架通过采用异构分布式数据库技术，有效提升了集成分布式系统的性能、效率和可靠性，解决了之前框架的若干问题。

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [10] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 本文提出了首个分布式内存并行缩减算法，用于解决最大权重独立集问题，实现了在大规模图上的高效处理，相比现有顺序算法获得了显著加速。


<details>
  <summary>Details</summary>
Motivation: 最大权重独立集问题是一个重要的NP难优化问题，现有算法主要使用数据缩减规则来求解，但缺乏分布式并行实现，无法处理超大规模图。

Method: 开发了分布式内存并行缩减算法，包括分布式reduce-and-greedy和reduce-and-peel启发式算法，支持异步处理。

Result: 在1024个处理器上实验显示良好可扩展性，reduce-and-peel方法相比顺序算法平均加速33倍，reduce-and-greedy方法平均加速50倍，能够处理超过10亿顶点和170亿边的图。

Conclusion: 分布式并行缩减算法能够有效处理超大规模图的最大权重独立集问题，在保持解质量的同时实现显著性能提升。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [11] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: ARES是一个基于长度预测的自适应解码重调度系统，通过LLM原生预测方法准确预测剩余生成长度，在解码阶段实现动态负载均衡，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中输出长度变化导致解码阶段工作负载严重不平衡，特别是在长输出推理任务中。现有系统采用静态预填充到解码调度，在变化的解码工作负载下经常导致SLO违规和OOM故障。

Method: 提出轻量级连续LLM原生预测方法，利用LLM隐藏状态建模剩余生成长度；在解码阶段采用重调度解决方案，集成当前和预测工作负载的动态平衡机制。

Result: 预测方法将MAE降低49.42%，预测器参数减少93.28%；重调度机制将P99 TPOT降低74.77%，吞吐量提升最高达2.24倍。

Conclusion: ARES系统通过自适应解码重调度和精确的长度预测，有效解决了LLM推理中的工作负载不平衡问题，显著提升了系统性能和可靠性。

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [12] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST是一个联邦推理资源调度工具包，在分布式高性能计算集群上提供推理即服务，支持研究人员通过OpenAI兼容API在私有安全环境中运行并行推理工作负载。


<details>
  <summary>Details</summary>
Motivation: 解决科学工作流中对私有、安全、可扩展AI推理日益增长的需求，让研究人员能够在本地基础设施上生成数十亿token，而无需依赖商业云基础设施。

Method: 利用Globus Auth和Globus Compute，通过集群无关的API将请求分发到联邦集群，支持多种推理后端（如vLLM），自动扩展资源，维护"热"节点以实现低延迟执行。

Result: 提供了一个框架，使研究人员能够在现有HPC基础设施上以云服务方式访问多样化AI模型（如大语言模型），支持高吞吐量批处理和交互模式。

Conclusion: FIRST成功实现了在分布式HPC集群上提供私有、安全、可扩展的AI推理服务，满足了科学工作流对大规模AI推理的需求。

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice](https://arxiv.org/abs/2510.12812)
*Aleksandar Petrov,Pierre Fernandez,Tomáš Souček,Hady Elsahar*

Main category: cs.CR

TL;DR: 该论文分析了当前深度学习图像水印方法的容量限制，建立了在PSNR和线性鲁棒性约束下的理论容量上限，发现理论容量远高于现有方法。作者训练了ChunkySeal模型，将容量提升4倍至1024位，同时保持图像质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习图像水印技术快速发展，但当前鲁棒方法的容量仍限制在几百位。这种进展停滞引发了对图像水印基本极限的疑问，需要探索理论容量与实际性能之间的差距。

Method: 建立了在PSNR和线性鲁棒性约束下的图像消息携带容量上限分析，并训练了ChunkySeal模型作为VideoSeal的扩展版本，通过架构创新和训练策略提升容量。

Result: 理论分析表明图像水印的理论容量比现有方法高出几个数量级。实验验证了理论容量与实际性能之间的差距持续存在。ChunkySeal模型成功将容量提升4倍至1024位，同时保持图像质量和鲁棒性。

Conclusion: 现代方法尚未达到水印容量的饱和点，在架构创新和训练策略方面仍存在显著改进空间，理论分析为未来研究方向提供了指导。

Abstract: Despite rapid progress in deep learning-based image watermarking, the
capacity of current robust methods remains limited to the scale of only a few
hundred bits. Such plateauing progress raises the question: How far are we from
the fundamental limits of image watermarking? To this end, we present an
analysis that establishes upper bounds on the message-carrying capacity of
images under PSNR and linear robustness constraints. Our results indicate
theoretical capacities are orders of magnitude larger than what current models
achieve. Our experiments show this gap between theoretical and empirical
performance persists, even in minimal, easily analysable setups. This suggests
a fundamental problem. As proof that larger capacities are indeed possible, we
train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4
times to 1024 bits, all while preserving image quality and robustness. These
findings demonstrate modern methods have not yet saturated watermarking
capacity, and that significant opportunities for architectural innovation and
training strategies remain.

</details>


### [14] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: SimKey是一种语义密钥模块，通过将密钥生成与先前上下文的意义绑定来增强水印鲁棒性，解决了现有LLM水印方法对表面编辑脆弱和可能被恶意利用的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本的快速传播，区分真实人类写作和机器输出变得越来越困难。现有水印方法存在两个问题：(i) 水印对简单的表面编辑（如改写或重新排序）很脆弱；(ii) 攻击者可以附加不相关的有害文本来继承水印，给模型所有者带来声誉风险。

Method: 引入SimKey语义密钥模块，使用语义嵌入的局部敏感哈希来确保改写文本产生相同的水印密钥，而不相关或语义转移的文本产生不同的密钥。SimKey与最先进的水印方案集成。

Result: SimKey提高了水印对改写和翻译的鲁棒性，同时防止有害内容被错误归因。

Conclusion: 语义感知密钥生成是实用且可扩展的水印方向。

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [15] [From base cases to backdoors: An Empirical Study of Unnatural Crypto-API Misuse](https://arxiv.org/abs/2510.13102)
*Victor Olaiya,Adwait Nadkarni*

Main category: cs.CR

TL;DR: 该论文通过大规模研究分析了密码API的非自然使用模式，开发了复杂度指标对140,431个API调用进行分层，并手动分析了5,704个代表性调用，建立了非自然误用的详细分类法。


<details>
  <summary>Details</summary>
Motivation: 现有工具只能检测基本的密码API误用，无法识别非平凡变体。需要了解开发者如何在实际环境中使用和误用密码API，特别是非自然使用模式，以指导工具设计。

Method: 开发直观的复杂度指标对140,431个密码API调用进行分层，从20,508个Android应用中采样5,704个代表性调用，通过手动逆向工程、开发最小示例和探索原生代码进行定性分析。

Result: 研究建立了两个详细的非自然密码API误用分类法，发现了17个关键发现，包括高度不寻常的误用、规避代码，以及流行工具无法处理轻微非传统使用的问题。

Conclusion: 研究结果为未来检测非自然密码API误用的工作提供了四个关键启示，强调了需要改进工具以处理更复杂的误用模式。

Abstract: Tools focused on cryptographic API misuse often detect the most basic
expressions of the vulnerable use, and are unable to detect non-trivial
variants. The question of whether tools should be designed to detect such
variants can only be answered if we know how developers use and misuse
cryptographic APIs in the wild, and in particular, what the unnatural usage of
such APIs looks like. This paper presents the first large-scale study that
characterizes unnatural crypto-API usage through a qualitative analysis of
5,704 representative API invocations. We develop an intuitive complexity metric
to stratify 140,431 crypto-API invocations obtained from 20,508 Android
applications, allowing us to sample 5,704 invocations that are representative
of all strata, with each stratum consisting of invocations with similar
complexity/naturalness. We qualitatively analyze the 5,704 sampled invocations
using manual reverse engineering, through an in-depth investigation that
involves the development of minimal examples and exploration of native code.
Our study results in two detailed taxonomies of unnatural crypto-API misuse,
along with 17 key findings that show the presence of highly unusual misuse,
evasive code, and the inability of popular tools to reason about even mildly
unconventional usage. Our findings lead to four key takeaways that inform
future work focused on detecting unnatural crypto-API misuse.

</details>


### [16] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: 本文提出了首个可撤销后门攻击范式，通过双层优化方法设计触发器，使其在攻击成功后能通过遗忘机制主动彻底移除，解决了传统后门攻击存在的持久性痕迹问题。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略在利用模型遗忘机制增强隐蔽性的同时，仍会留下可能被静态分析检测到的持久痕迹，这构成了持续的安全风险。

Method: 将可撤销后门攻击中的触发器优化建模为双层优化问题：通过模拟后门注入和遗忘过程，优化触发器生成器以实现高攻击成功率，同时确保后门能通过遗忘轻松擦除。采用确定性划分中毒和遗忘样本减少采样方差，并应用投影冲突梯度技术解决剩余梯度冲突。

Result: 在CIFAR-10和ImageNet上的实验表明，该方法在保持与最先进后门攻击相当的攻击成功率的同时，能够通过遗忘有效移除后门行为。

Conclusion: 这项工作为后门攻击研究开辟了新方向，并为机器学习系统的安全性提出了新的挑战。

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [17] [Towards Trusted Service Monitoring: Verifiable Service Level Agreements](https://arxiv.org/abs/2510.13370)
*Fernando Castillo,Eduardo Brito,Sebastian Werner,Pille Pullonen-Raudvere,Jonathan Heiss*

Main category: cs.CR

TL;DR: 提出一个基于可信硬件和零知识证明的SLA监控框架，通过密码学保证服务级别协议违规声明的可验证性，实现无需信任的自动化合规验证。


<details>
  <summary>Details</summary>
Motivation: 解决服务导向环境中SLA监控存在的信任冲突问题，当提供商自行报告指标时存在少报违规的动机，需要建立密码学基础来确保服务生态系统的真正可信度。

Method: 将机器可读的SLA条款转换为可验证谓词，在可信执行环境中监控，收集时间戳遥测数据并组织成Merkle树，生成签名证明，使用零知识证明聚合服务级别指标来评估合规性。

Result: 原型系统展示了对每小时超过100万个事件的线性扩展能力，单个违规声明的证明生成和验证时间接近常数，支持大规模服务监控。

Conclusion: 该框架通过密码学保证实现了三个安全属性：完整性、真实性和有效性，为服务监控中的自动化合规验证提供了无需信任的SLA执行机制。

Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments
suffers from inherent trust conflicts when providers self-report metrics,
creating incentives to underreport violations. We introduce a framework for
generating verifiable SLA violation claims through trusted hardware monitors
and zero-knowledge proofs, establishing cryptographic foundations for genuine
trustworthiness in service ecosystems. Our approach starts with
machine-readable SLA clauses converted into verifiable predicates and monitored
within Trusted Execution Environments. These monitors collect timestamped
telemetry, organize measurements into Merkle trees, and produce signed
attestations. Zero-knowledge proofs aggregate Service-Level Indicators to
evaluate compliance, generating cryptographic proofs verifiable by
stakeholders, arbitrators, or insurers in disputes, without accessing
underlying data. This ensures three security properties: integrity,
authenticity, and validity. Our prototype demonstrates linear scaling up to
over 1 million events per hour for measurements with near constant-time proof
generation and verification for single violation claims, enabling trustless SLA
enforcement through cryptographic guarantees for automated compliance
verification in service monitoring.

</details>


### [18] [Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers](https://arxiv.org/abs/2510.13462)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Haoyu Gao,Zhendong Zhao,Yilong Chen*

Main category: cs.CR

TL;DR: BadSwitch是一种针对MoE架构LLM的新型后门攻击框架，通过利用专家路由偏好，将恶意触发器嵌入到具有强任务亲和力的专家路由路径中，实现精确且隐蔽的模型操控。


<details>
  <summary>Details</summary>
Motivation: MoE架构中的稀疏路由机制由于专家专业化而表现出任务偏好，这引入了一个未被充分探索的后门攻击漏洞。本研究旨在探索在MoE-based LLM中注入后门的可行性和有效性。

Method: 提出BadSwitch框架，集成任务耦合动态触发器优化和敏感度引导的Top-S专家追踪机制。在预训练期间联合优化触发器嵌入，同时识别S个最敏感专家，随后将Top-K门控机制限制在这些目标专家上。

Result: 在三种主流MoE架构上的综合评估显示，BadSwitch能高效劫持预训练模型，成功率高达100%，同时在所有基线中保持最高的干净准确率。在AGNews数据集上达到94.07%攻击成功率和87.18%干净准确率，对文本级和模型级防御机制表现出强韧性。

Conclusion: BadSwitch暴露了MoE系统的安全风险，专家激活模式分析揭示了MoE漏洞的基本见解，这项工作将有助于推进AI安全发展。

Abstract: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this sparse routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based LLMs by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.

</details>


### [19] [How Blind and Low-Vision Users Manage Their Passwords](https://arxiv.org/abs/2510.13538)
*Alexander Ponticello,Filipo Sharevski,Simon Anell,Katharina Krombholz*

Main category: cs.CR

TL;DR: 该研究调查了盲人和低视力用户如何使用密码管理器，发现虽然所有参与者都使用密码管理器，但主要是为了存储和检索密码的便利性，而非安全优势。密码管理器未能满足BLV用户对自主权的需求，导致他们采用不安全的密码实践。


<details>
  <summary>Details</summary>
Motivation: 研究盲人和低视力用户在密码管理方面的挑战，以及密码管理器如何帮助他们解决这些问题。现有研究表明用户存在安全担忧导致不安全实践，但缺乏对BLV用户特定需求的研究。

Method: 采用定性访谈研究方法，对33名盲人和低视力参与者进行访谈研究。

Result: 所有参与者都使用密码管理器，主要出于存储和检索密码的便利性。安全优势（生成强随机密码）因缺乏实际可访问性而被避免。密码管理器未能满足BLV用户对自主权的需求，导致不安全实践，如重复使用可预测密码或在盲文中书写重要凭据。

Conclusion: 需要改进密码管理器的实际可访问性和可用性，以建立信任和安全实践，同时维护BLV用户的自主权。

Abstract: Managing passwords securely and conveniently is still an open problem for
many users. Existing research has examined users' password management
strategies and identified pain points, such as security concerns, leading to
insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle
this problem and how password managers can assist them. This paper presents the
results of a qualitative interview study with N = 33 BLV participants. We found
that all participants utilize password managers to some extent, which they
perceive as fairly accessible. However, the adoption is mainly driven by the
convenience of storing and retrieving passwords. The security advantages -
generating strong, random passwords - were avoided mainly due to the absence of
practical accessibility. Password managers do not adhere to BLV users'
underlying needs for agency, which stem from experiences with inaccessible
software and vendors who deprioritize accessibility issues. Underutilization of
password managers leads BLV users to adopt insecure practices, such as reusing
predictable passwords or resorting to 'security through obscurity' by writing
important credentials in braille. We conclude our analysis by discussing the
need to implement practical accessibility and usability improvements for
password managers as a way of establishing trust and secure practices while
maintaining BLV users' agency.

</details>
