<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining](https://arxiv.org/abs/2510.18612)
*Muhammad Hassan,Maria Mushtaq,Jaan Raik,Tara Ghasempouri*

Main category: cs.CR

TL;DR: 提出了一种结合统计预处理和关联规则挖掘的新检测方法，用于检测RISC-V处理器中的微架构侧信道攻击，相比现有方法在准确率、精确度和召回率方面均有提升，并能检测新型flush+fault攻击变种。


<details>
  <summary>Details</summary>
Motivation: RISC-V处理器在关键应用中日益普及，但其对微架构侧信道攻击的易受性是一个严重问题。相比x86和ARM，RISC-V中微架构攻击检测的研究相对不足，现有基于机器学习的方法存在一些实际问题需要进一步研究。

Method: 利用gem5模拟器，提出结合统计预处理和关联规则挖掘的检测方法，具有重新配置能力，可泛化用于检测任何微架构攻击。

Result: 与最先进方法相比，在加密、计算和内存密集型工作负载下，准确率提升达5.15%，精确度提高7%，召回率改善3.91%，并能灵活检测新型flush+fault攻击变种。

Conclusion: 该方法不仅检测性能优越，而且由于依赖关联规则，其人类可解释性为理解攻击和良性应用执行期间的微架构行为提供了深入洞察。

Abstract: RISC-V processors are becoming ubiquitous in critical applications, but their
susceptibility to microarchitectural side-channel attacks is a serious concern.
Detection of microarchitectural attacks in RISC-V is an emerging research topic
that is relatively underexplored, compared to x86 and ARM. The first line of
work to detect flush+fault-based microarchitectural attacks in RISC-V leverages
Machine Learning (ML) models, yet it leaves several practical aspects that need
further investigation. To address overlooked issues, we leveraged gem5 and
propose a new detection method combining statistical preprocessing and
association rule mining having reconfiguration capabilities to generalize the
detection method for any microarchitectural attack. The performance comparison
with state-of-the-art reveals that the proposed detection method achieves up to
5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in
recall under the cryptographic, computational, and memory-intensive workloads
alongside its flexibility to detect new variant of flush+fault attack.
Moreover, as the attack detection relies on association rules, their
human-interpretable nature provides deep insight to understand
microarchitectural behavior during the execution of attack and benign
applications.

</details>


### [2] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: sNVMe-oF是一个基于NVMe-oF协议的存储管理系统，通过扩展协议并提供机密性、完整性和新鲜性保证，解决了传统机密计算方法在保护现代存储系统时面临的性能和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 随着分解式存储和机密计算成为现代数据中心的标准，传统CC方法在保护先进存储系统时面临扩展性差、性能或安全性受损的问题，需要新的解决方案。

Method: 扩展NVMe-oF协议但不修改协议本身，引入控制路径和计数器租赁等新概念，利用NVMe元数据优化数据路径性能，采用分解式Hazel Merkle Tree，避免冗余IPSec保护，并利用支持CC的智能网卡加速器。

Result: 在NVIDIA BlueField-3上原型实现，对于合成模式和AI训练，性能下降仅为2%，能够实现线速性能。

Conclusion: sNVMe-oF成功解决了机密计算环境下存储系统的性能和安全问题，在不修改NVMe-oF协议的前提下实现了高性能的安全存储管理。

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>


### [3] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: BreakFun是一种利用LLM对结构化模式遵循性的越狱攻击方法，通过精心设计的'特洛伊模式'强制模型生成有害内容，在13个主流模型上平均成功率89%，并提出基于文字转录的防御方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在处理结构化数据和遵循句法规则能力方面的固有脆弱性，这种能力既是其广泛应用的基础，也使其容易受到攻击。

Method: 提出BreakFun越狱方法，使用包含无辜框架、思维链干扰和核心'特洛伊模式'的三部分提示，利用LLM对结构和模式的强烈遵循倾向。

Result: 攻击在13个基础模型和专有模型上平均成功率89%，多个知名模型达到100%攻击成功率，消融研究证实特洛伊模式是主要因果因素。

Conclusion: LLM的核心优势可能成为关键弱点，通过针对欺骗性模式的防御策略可以有效缓解此类攻击，为构建更鲁棒的对齐模型提供了新视角。

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [4] [ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2510.17919)
*Tenghui Huang,Jinbo Wen,Jiawen Kang,Siyong Chen,Zhengtao Li,Tao Zhang,Dongning Liu,Jiacheng Wang,Chengjun Cai,Yinqiu Liu,Dusit Niyato*

Main category: cs.CR

TL;DR: ParaVul是一个并行LLM和检索增强框架，通过SLoRA微调、混合RAG系统和元学习融合，提高智能合约漏洞检测的可靠性和准确性，在F1分数上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统智能合约漏洞检测方法存在高误报率和可扩展性差的问题，而现有LLM方法面临高推理成本和计算开销的挑战，需要更高效可靠的检测方案。

Method: 开发SLoRA用于LLM微调，构建混合RAG系统（密集检索+BM25），提出元学习模型融合RAG和LLM输出，并使用思维链提示生成检测报告。

Result: 仿真结果显示ParaVul在F1分数上表现优越，单标签检测达到0.9398，多标签检测达到0.9330。

Conclusion: ParaVul框架通过并行LLM和检索增强方法，有效提高了智能合约漏洞检测的准确性和可靠性，同时降低了计算开销。

Abstract: Smart contracts play a significant role in automating blockchain services.
Nevertheless, vulnerabilities in smart contracts pose serious threats to
blockchain security. Currently, traditional detection methods primarily rely on
static analysis and formal verification, which can result in high
false-positive rates and poor scalability. Large Language Models (LLMs) have
recently made significant progress in smart contract vulnerability detection.
However, they still face challenges such as high inference costs and
substantial computational overhead. In this paper, we propose ParaVul, a
parallel LLM and retrieval-augmented framework to improve the reliability and
accuracy of smart contract vulnerability detection. Specifically, we first
develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA
introduces sparsification by incorporating a sparse matrix into quantized
LoRA-based LLMs, thereby reducing computational overhead and resource
requirements while enhancing their ability to understand vulnerability-related
issues. We then construct a vulnerability contract dataset and develop a hybrid
Retrieval-Augmented Generation (RAG) system that integrates dense retrieval
with Best Matching 25 (BM25), assisting in verifying the results generated by
the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of
the RAG system and the LLM, thereby generating the final detection results.
After completing vulnerability detection, we design chain-of-thought prompts to
guide LLMs to generate comprehensive vulnerability detection reports.
Simulation results demonstrate the superiority of ParaVul, especially in terms
of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for
multi-label detection.

</details>


### [5] [BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?](https://arxiv.org/abs/2510.18003)
*Fengqing Jiang,Yichen Feng,Yuetai Li,Luyao Niu,Basel Alomair,Radha Poovendran*

Main category: cs.CR

TL;DR: BadScientist框架揭示了LLM驱动的科研助手与AI同行评审系统结合时的关键漏洞：完全自动化的发表循环，其中AI生成的研究被AI评审员评估而无人监督。研究发现伪造论文的接受率很高，评审员经常标记诚信问题但仍给出接受级别的分数，现有缓解策略效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机是识别LLM驱动的科研助手与AI同行评审系统结合时产生的关键漏洞——完全自动化的发表循环，其中AI生成的研究被AI评审员评估而无人监督，这可能破坏科学出版的完整性。

Method: 开发了BadScientist框架，使用无需真实实验的呈现操纵策略生成伪造论文，并建立具有正式错误保证（集中界限和校准分析）的严格评估框架，在真实数据上进行校准。

Result: 结果揭示了系统性漏洞：伪造论文的接受率高达一定水平。关键发现是"关注-接受冲突"——评审员经常标记诚信问题但仍给出接受级别的分数。缓解策略仅带来边际改进，检测准确率仅略高于随机机会。

Conclusion: 尽管具有可证明正确的聚合数学，诚信检查系统性地失败，暴露了当前AI驱动评审系统的基本局限性，并强调了在科学出版中迫切需要深度防御保障措施。

Abstract: The convergence of LLM-powered research assistants and AI-based peer review
systems creates a critical vulnerability: fully automated publication loops
where AI-generated research is evaluated by AI reviewers without human
oversight. We investigate this through \textbf{BadScientist}, a framework that
evaluates whether fabrication-oriented paper generation agents can deceive
multi-model LLM review systems. Our generator employs presentation-manipulation
strategies requiring no real experiments. We develop a rigorous evaluation
framework with formal error guarantees (concentration bounds and calibration
analysis), calibrated on real data. Our results reveal systematic
vulnerabilities: fabricated papers achieve acceptance rates up to . Critically,
we identify \textit{concern-acceptance conflict} -- reviewers frequently flag
integrity issues yet assign acceptance-level scores. Our mitigation strategies
show only marginal improvements, with detection accuracy barely exceeding
random chance. Despite provably sound aggregation mathematics, integrity
checking systematically fails, exposing fundamental limitations in current
AI-driven review systems and underscoring the urgent need for defense-in-depth
safeguards in scientific publishing.

</details>


### [6] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: PrivaDE是一个用于隐私保护的数据效用评分和选择的加密协议，采用区块链中心化设计，确保模型和数据的强隐私保护，实现15分钟内完成数据评估。


<details>
  <summary>Details</summary>
Motivation: 模型构建者需要评估数据相关性来提升模型性能，但需要在不暴露模型专有细节的情况下进行，同时数据提供者需要确保除了计算出的效用分数外，其数据信息不会泄露给模型构建者。

Method: 采用区块链中心化设计，整合模型蒸馏、模型分割和零知识证明等技术，提出统一的效用评分函数，结合经验损失、预测熵和特征空间多样性。

Result: 评估显示PrivaDE能有效进行数据评估，即使对于具有数百万参数的模型，在线运行时间也在15分钟内。

Conclusion: 这项工作为去中心化机器学习生态系统中的公平和自动化数据市场奠定了基础。

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [7] [RESCUE: Retrieval Augmented Secure Code Generation](https://arxiv.org/abs/2510.18204)
*Jiahao Shi,Tianyi Zhang*

Main category: cs.CR

TL;DR: RESCUE是一个新的RAG框架，通过混合知识库构建和分层多面检索，显著提升大语言模型生成安全代码的能力，在四个基准测试中平均提升SecurePass@1指标4.8分。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型仍会生成易受攻击的代码，传统RAG设计在处理原始安全文档时存在噪声问题，且现有检索方法忽略了任务描述中隐含的重要安全语义。

Method: 提出RESCUE框架：1）混合知识库构建方法，结合LLM辅助的聚类-总结蒸馏和程序切片，生成高级安全指南和简洁的安全代码示例；2）分层多面检索，从上到下遍历知识库并在每个层级集成多个安全关键事实。

Result: 在四个基准测试和六个LLM上，与五种最先进的安全代码生成方法相比，RESCUE将SecurePass@1指标平均提升了4.8分，建立了新的安全性能最佳水平。

Conclusion: RESCUE通过创新的知识库构建和检索机制，有效解决了安全代码生成中的挑战，深度分析和消融研究验证了各组件有效性。

Abstract: Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.

</details>


### [8] [CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments](https://arxiv.org/abs/2510.18324)
*Gyeonghoon Park,Jaehan Kim,Jinu Choi,Jinwoo Kim*

Main category: cs.CR

TL;DR: CryptoGuard是一个轻量级混合解决方案，结合检测和修复策略来对抗Linux云环境中的加密挖矿恶意软件，通过低开销系统调用监控和两阶段深度学习分类实现高精度检测，并集成基于eBPF的针对性修复机制。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案在Linux云环境中面临可扩展性差、检测精度低和缺乏集成修复的问题，无法有效应对加密挖矿恶意软件的隐蔽行为和规避技术。

Method: 使用基于草图和滑动窗口的系统调用监控收集行为模式，将分类任务分解为两阶段过程，利用深度学习模型识别可疑活动，并集成基于eBPF的针对性修复机制来对抗规避技术。

Result: 在123个真实世界加密挖矿恶意软件样本上评估，两阶段的平均F1分数分别达到96.12%和92.26%，在真阳性率和假阳性率方面优于最先进的基线方法，每个主机仅产生0.06%的CPU开销。

Conclusion: CryptoGuard提供了一个高效、可扩展的解决方案，能够准确检测和修复加密挖矿恶意软件，同时保持极低的系统开销。

Abstract: Host-based cryptomining malware, commonly known as cryptojackers, have gained
notoriety for their stealth and the significant financial losses they cause in
Linux-based cloud environments. Existing solutions often struggle with
scalability due to high monitoring overhead, low detection accuracy against
obfuscated behavior, and lack of integrated remediation. We present
CryptoGuard, a lightweight hybrid solution that combines detection and
remediation strategies to counter cryptojackers. To ensure scalability,
CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect
behavior patterns with minimal overhead. It decomposes the classification task
into a two-phase process, leveraging deep learning models to identify
suspicious activity with high precision. To counter evasion techniques such as
entry point poisoning and PID manipulation, CryptoGuard integrates targeted
remediation mechanisms based on eBPF, a modern Linux kernel feature deployable
on any compatible host. Evaluated on 123 real-world cryptojacker samples, it
achieves average F1-scores of 96.12% and 92.26% across the two phases, and
outperforms state-of-the-art baselines in terms of true and false positive
rates, while incurring only 0.06% CPU overhead per host.

</details>


### [9] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: 本文分析了LLM水印技术在实际部署中的障碍，提出激励错配是主要问题，并介绍了三种水印方法，特别推荐激励对齐的上下文水印(ICW)作为可行解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM水印算法取得进展，但实际部署有限，主要原因是LLM提供商、平台和终端用户之间的激励错配，表现为竞争风险、检测工具治理、鲁棒性问题和归因问题四大障碍。

Method: 重新审视三类水印方法：模型水印（适合LLM提供商但面临开源挑战）、LLM文本水印（在反滥用场景中效益有限）、上下文水印(ICW)（为可信方设计，通过嵌入水印指令检测滥用）。

Result: ICW方法能实现激励对齐：用户无质量损失，可信方获得检测工具，LLM提供商保持中立。该方法在会议评审、教育等场景中特别有效。

Conclusion: LLM水印的实际采用需要在目标应用领域对齐利益相关者激励，促进社区参与，并探索更多激励对齐的领域特定水印方法。

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>


### [10] [Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet](https://arxiv.org/abs/2510.18394)
*Yong Zhang,Nishanth Sastry*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Undoubtedly, the Internet has become one of the most important conduits to
information for the general public. Nonetheless, Internet access can be and has
been limited systematically or blocked completely during political events in
numerous countries and regions by various censorship mechanisms. Depending on
where the core filtering component is situated, censorship techniques have been
classified as client-based, server-based, or network-based. However, as the
Internet evolves rapidly, new and sophisticated censorship techniques have
emerged, which involve techniques that cut across locations and involve new
forms of hurdles to information access. We argue that modern censorship can be
better understood through a new lens that we term chokepoints, which identifies
bottlenecks in the content production or delivery cycle where efficient new
forms of large-scale client-side surveillance and filtering mechanisms have
emerged.

</details>


### [11] [DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning](https://arxiv.org/abs/2510.18438)
*Yixuan Liu,Xinlei Li,Yi Li*

Main category: cs.CR

TL;DR: DeepTx是一个实时交易分析系统，通过模拟待处理交易、提取行为特征，并使用多LLM推理交易意图来检测Web3钓鱼攻击。


<details>
  <summary>Details</summary>
Motivation: Web3生态系统中的钓鱼攻击日益复杂，利用欺骗性合约逻辑、恶意前端脚本和代币授权模式，需要实时检测系统来保护用户。

Method: 系统模拟待处理交易，提取行为、上下文和UI特征，使用多个大语言模型推理交易意图，并通过自反思的共识机制确保决策的鲁棒性和可解释性。

Result: 在钓鱼数据集上的评估显示，DeepTx实现了高精度和高召回率。

Conclusion: DeepTx能够有效检测Web3钓鱼威胁，在用户确认交易前提供保护。

Abstract: Phishing attacks in Web3 ecosystems are increasingly sophisticated,
exploiting deceptive contract logic, malicious frontend scripts, and token
approval patterns. We present DeepTx, a real-time transaction analysis system
that detects such threats before user confirmation. DeepTx simulates pending
transactions, extracts behavior, context, and UI features, and uses multiple
large language models (LLMs) to reason about transaction intent. A consensus
mechanism with self-reflection ensures robust and explainable decisions.
Evaluated on our phishing dataset, DeepTx achieves high precision and recall
(demo video: https://youtu.be/4OfK9KCEXUM).

</details>


### [12] [One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection](https://arxiv.org/abs/2510.18493)
*Kangzhong Wang,Zitong Shen,Youqian Zhang,Michael MK Cheung,Xiapu Luo,Grace Ngai,Eugene Yujun Fu*

Main category: cs.CR

TL;DR: 提出了MASK框架，用于在利用大语言模型检测电话诈骗的同时保护用户隐私，支持基于个人偏好的动态隐私调整和多种脱敏方法。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型检测电话诈骗存在隐私风险，因为通话记录包含敏感个人信息可能在处理过程中暴露给第三方服务提供商。

Method: 提出MASK（模块化自适应脱敏工具包）框架，提供可插拔架构支持多种脱敏方法，从传统基于关键词的技术到复杂的神经方法，支持基于用户偏好的动态隐私调整。

Result: MASK框架能够平衡用户信任和检测效果，支持创建真正个性化的隐私感知检测系统。

Conclusion: MASK框架为构建隐私保护的大语言模型检测系统提供了可行方案，即使在电话诈骗检测之外的场景也具有应用潜力。

Abstract: Phone scams remain a pervasive threat to both personal safety and financial
security worldwide. Recent advances in large language models (LLMs) have
demonstrated strong potential in detecting fraudulent behavior by analyzing
transcribed phone conversations. However, these capabilities introduce notable
privacy risks, as such conversations frequently contain sensitive personal
information that may be exposed to third-party service providers during
processing. In this work, we explore how to harness LLMs for phone scam
detection while preserving user privacy. We propose MASK (Modular Adaptive
Sanitization Kit), a trainable and extensible framework that enables dynamic
privacy adjustment based on individual preferences. MASK provides a pluggable
architecture that accommodates diverse sanitization methods - from traditional
keyword-based techniques for high-privacy users to sophisticated neural
approaches for those prioritizing accuracy. We also discuss potential modeling
approaches and loss function designs for future development, enabling the
creation of truly personalized, privacy-aware LLM-based detection systems that
balance user trust and detection effectiveness, even beyond phone scam context.

</details>


### [13] [Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization](https://arxiv.org/abs/2510.18508)
*Osama Al Haddad,Muhammad Ikram,Ejaz Ahmed,Young Lee*

Main category: cs.CR

TL;DR: 评估四种大语言模型在漏洞分类任务中的表现，发现Gemini模型在SSVC框架的决策点预测中表现最佳，但所有模型都无法完全替代专家判断。


<details>
  <summary>Details</summary>
Motivation: 安全分析师面临处理大量复杂漏洞的压力，需要自动化工具辅助解释漏洞信息，提高漏洞分类效率。

Method: 使用384个真实漏洞数据，对ChatGPT、Claude、Gemini和DeepSeek四种模型进行超过165,000次查询测试，采用包括one-shot、few-shot和chain-of-thought在内的12种提示技术。

Result: Gemini在四个决策点中的三个表现最佳，提示示例通常能提高准确性，但所有模型在某些决策点上仍有困难。只有DeepSeek在加权指标下达到公平一致性，所有模型都倾向于过度预测风险。

Conclusion: 当前大语言模型不能替代专家判断，但特定模型和提示组合在针对性SSVC决策中表现出中等有效性，可谨慎应用于漏洞优先级排序工作流程。

Abstract: Security analysts face increasing pressure to triage large and complex
vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by
automating parts of the interpretation process. We evaluate four models
(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to
interpret semi-structured and unstructured vulnerability information. As a
concrete use case, we test each model's ability to predict decision points in
the Stakeholder-Specific Vulnerability Categorization (SSVC) framework:
Exploitation, Automatable, Technical Impact, and Mission and Wellbeing.
  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more
than 165,000 queries to assess performance under prompting styles including
one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC
decision point and Cohen's kappa (weighted and unweighted) for the final SSVC
decision outcomes. Gemini consistently ranked highest, leading on three of four
decision points and yielding the most correct recommendations. Prompting with
exemplars generally improved accuracy, although all models struggled on some
decision points. Only DeepSeek achieved fair agreement under weighted metrics,
and all models tended to over-predict risk.
  Overall, current LLMs do not replace expert judgment. However, specific LLM
and prompt combinations show moderate effectiveness for targeted SSVC
decisions. When applied with care, LLMs can support vulnerability
prioritization workflows and help security teams respond more efficiently to
emerging threats.

</details>


### [14] [The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability](https://arxiv.org/abs/2510.18563)
*Zijie Xu,Minfeng Qi,Shiqing Wu,Lefeng Zhang,Qiwen Wei,Han He,Ningran Li*

Main category: cs.CR

TL;DR: 该研究揭示了多智能体系统中的信任-脆弱性悖论：增加智能体间信任可提升协作效率，但同时也增加了过度暴露和过度授权的安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统快速发展，但相互信任与安全之间的紧张关系尚未得到充分探索。研究者希望深入理解信任增强协作的同时如何带来安全风险。

Method: 构建包含3个宏观场景和19个子场景的场景-游戏数据集，进行广泛的闭环交互实验，将信任显式参数化。使用最小必要信息作为安全基线，提出过度暴露率和授权漂移两个统一指标来评估风险。

Result: 多个模型后端和编排框架的实验结果一致显示：更高的信任度提高了任务成功率，但也增加了暴露风险，不同系统表现出异质的信任-风险映射关系。敏感信息重新分区和守护智能体启用等防御措施能有效降低风险。

Conclusion: 研究形式化了信任-脆弱性悖论，建立了可复现的基准和统一指标，证明在多智能体系统设计中，信任必须作为首要安全变量进行建模和调度。

Abstract: Multi-agent systems powered by large language models are advancing rapidly,
yet the tension between mutual trust and security remains underexplored. We
introduce and empirically validate the Trust-Vulnerability Paradox (TVP):
increasing inter-agent trust to enhance coordination simultaneously expands
risks of over-exposure and over-authorization. To investigate this paradox, we
construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,
and run extensive closed-loop interactions with trust explicitly parameterized.
Using Minimum Necessary Information (MNI) as the safety baseline, we propose
two unified metrics: Over-Exposure Rate (OER) to detect boundary violations,
and Authorization Drift (AD) to capture sensitivity to trust levels. Results
across multiple model backends and orchestration frameworks reveal consistent
trends: higher trust improves task success but also heightens exposure risks,
with heterogeneous trust-to-risk mappings across systems. We further examine
defenses such as Sensitive Information Repartitioning and Guardian-Agent
enablement, both of which reduce OER and attenuate AD. Overall, this study
formalizes TVP, establishes reproducible baselines with unified metrics, and
demonstrates that trust must be modeled and scheduled as a first-class security
variable in multi-agent system design.

</details>


### [15] [Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain](https://arxiv.org/abs/2510.18568)
*Behnam Rezaei Bezanjani,Seyyed Hamid Ghafouri,Reza Gholamrezaei*

Main category: cs.CR

TL;DR: 提出一个三阶段安全框架，通过信誉评估、区块链集成和轻量级LSTM异常检测，提升物联网医疗系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在医疗领域的应用带来了实时监测和个性化治疗等优势，但也引入了严重的安全风险，传统安全措施难以应对物联网环境的异构性、资源限制和实时处理需求。

Method: 三阶段框架：第一阶段使用信誉机制评估设备可靠性；第二阶段集成区块链技术确保数据不可篡改和安全通信；第三阶段采用轻量级LSTM模型进行实时异常检测。

Result: 仿真结果显示，该框架在精度、准确率和召回率上提升2%，攻击检测率提高5%，误报率降低3%。

Conclusion: 该框架能有效解决关键安全问题，同时保持可扩展性和实时性能。

Abstract: The integration of Internet of Things (IoT) devices in healthcare has
revolutionized patient care by enabling real-time monitoring, personalized
treatments, and efficient data management. However, this technological
advancement introduces significant security risks, particularly concerning the
confidentiality, integrity, and availability of sensitive medical data.
Traditional security measures are often insufficient to address the unique
challenges posed by IoT environments, such as heterogeneity, resource
constraints, and the need for real-time processing. To tackle these challenges,
we propose a comprehensive three-phase security framework designed to enhance
the security and reliability of IoT-enabled healthcare systems. In the first
phase, the framework assesses the reliability of IoT devices using a
reputation-based trust estimation mechanism, which combines device behavior
analytics with off-chain data storage to ensure scalability. The second phase
integrates blockchain technology with a lightweight proof-of-work mechanism,
ensuring data immutability, secure communication, and resistance to
unauthorized access. The third phase employs a lightweight Long Short-Term
Memory (LSTM) model for anomaly detection and classification, enabling
real-time identification of cyber threats. Simulation results demonstrate that
the proposed framework outperforms existing methods, achieving a 2% increase in
precision, accuracy, and recall, a 5% higher attack detection rate, and a 3%
reduction in false alarm rate. These improvements highlight the framework's
ability to address critical security concerns while maintaining scalability and
real-time performance.

</details>


### [16] [Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks](https://arxiv.org/abs/2510.18572)
*Maynard Koch,Florian Dolzmann,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.CR

TL;DR: 透明DNS转发器通过不重建数据包源地址的方式，将DNS请求转发到开放递归解析器，使其被滥用于分布式反射放大攻击，绕过速率限制并利用DNS任播基础设施实现高达14倍的攻击放大效果。


<details>
  <summary>Details</summary>
Motivation: DNS基础设施长期以来面临反射放大攻击威胁，尽管已实施多种防护措施，但威胁依然存在。本文旨在揭示透明DNS转发器这一广泛部署但功能不完整的DNS组件所带来的未被充分认识的安全威胁。

Method: 通过分析透明DNS转发器的工作机制，研究其如何绕过现有防护措施，并实证验证其通过DNS任播基础设施实现的攻击放大效果。

Result: 实证研究表明透明DNS转发器能够绕过速率限制，利用DNS任播基础设施实现高达14倍的攻击放大效果，并使受防火墙保护的递归解析器成为全球DNS攻击面的一部分。

Conclusion: 透明DNS转发器对互联网基础设施构成严重威胁，它们能够有效规避现有防护措施，显著增强DNS反射放大攻击的规模和影响范围。

Abstract: The DNS infrastructure is infamous for facilitating reflective amplification
attacks. Various countermeasures such as server shielding, access control, rate
limiting, and protocol restrictions have been implemented. Still, the threat
remains throughout the deployment of DNS servers. In this paper, we report on
and evaluate the often unnoticed threat that derives from transparent DNS
forwarders, a widely deployed, incompletely functional set of DNS components.
Transparent DNS forwarders transfer DNS requests without rebuilding packets
with correct source addresses. As such, transparent forwarders feed DNS
requests into (mainly powerful and anycasted) open recursive resolvers, which
thereby can be misused to participate unwillingly in distributed reflective
amplification attacks. We show how transparent forwarders raise severe threats
to the Internet infrastructure. They easily circumvent rate limiting and
achieve an additional, scalable impact via the DNS anycast infrastructure. We
empirically verify this scaling behavior up to a factor of 14. Transparent
forwarders can also assist in bypassing firewall rules that protect recursive
resolvers, making these shielded infrastructure entities part of the global DNS
attack surface.

</details>


### [17] [Evaluating Large Language Models in detecting Secrets in Android Apps](https://arxiv.org/abs/2510.18601)
*Marco Alecci,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: SecretLoc是一种基于LLM的方法，用于检测Android应用中的硬编码秘密，无需依赖预定义模式或训练数据，发现了现有方法遗漏的4828个秘密。


<details>
  <summary>Details</summary>
Motivation: 移动应用开发者经常将API密钥、令牌等认证秘密硬编码到应用中，这些秘密可能通过逆向工程被提取，导致严重的安全和财务风险。现有检测方法需要先验知识，无法识别未知类型的秘密。

Method: 提出SecretLoc方法，利用上下文和结构线索来识别秘密，不依赖预定义模式或标记训练集，使用文献中的基准数据集进行评估。

Result: 在基准数据集中发现了现有方法遗漏的4828个秘密，包括10多种新型秘密类型；在Google Play的5000个应用中，42.5%的应用包含硬编码秘密，部分已得到开发者修复。

Conclusion: 研究揭示了双重使用风险：分析人员能用LLM发现这些秘密，攻击者同样可以。这强调了移动生态系统中需要主动的秘密管理和更强的缓解措施。

Abstract: Mobile apps often embed authentication secrets, such as API keys, tokens, and
client IDs, to integrate with cloud services. However, developers often
hardcode these credentials into Android apps, exposing them to extraction
through reverse engineering. Once compromised, adversaries can exploit secrets
to access sensitive data, manipulate resources, or abuse APIs, resulting in
significant security and financial risks. Existing detection approaches, such
as regex-based analysis, static analysis, and machine learning, are effective
for identifying known patterns but are fundamentally limited: they require
prior knowledge of credential structures, API signatures, or training data.
  In this paper, we propose SecretLoc, an LLM-based approach for detecting
hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it
leverages contextual and structural cues to identify secrets without relying on
predefined patterns or labeled training sets. Using a benchmark dataset from
the literature, we demonstrate that SecretLoc detects secrets missed by regex-,
static-, and ML-based methods, including previously unseen types of secrets. In
total, we discovered 4828 secrets that were undetected by existing approaches,
discovering more than 10 "new" types of secrets, such as OpenAI API keys,
GitHub Access Tokens, RSA private keys, and JWT tokens, and more.
  We further extend our analysis to newly crawled apps from Google Play, where
we uncovered and responsibly disclosed additional hardcoded secrets. Across a
set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which
were confirmed and remediated by developers after we contacted them. Our
results reveal a dual-use risk: if analysts can uncover these secrets with
LLMs, so can attackers. This underscores the urgent need for proactive secret
management and stronger mitigation practices across the mobile ecosystem.

</details>


### [18] [Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation](https://arxiv.org/abs/2510.18614)
*René Coignard,Anton Rygin*

Main category: cs.CR

TL;DR: Qatsi是一个使用Argon2id的分层密钥派生方案，无需持久存储即可生成可重现的加密密钥。该系统通过确定性派生所有密钥来消除基于保险库的攻击面，采用内存硬派生和可证明均匀的拒绝采样来生成高熵输出。


<details>
  <summary>Details</summary>
Motivation: 针对气隙系统和主凭证生成场景，在这些场景中无状态的可重现性比轮换灵活性更重要。消除基于保险库的攻击面，通过单一高熵主密钥和上下文层确定性派生所有密钥。

Method: 使用Argon2id进行分层密钥派生，采用内存硬派生（64-128 MiB内存，16-32次迭代）和可证明均匀的拒绝采样来生成7776词助记词或90字符密码。在Rust中实现，提供自动内存清零、编译时词表完整性验证和全面测试覆盖。

Result: 输出达到103-312位熵，GPU攻击成本量化：在偏执参数下（128 MiB内存），80位主密钥对单GPU攻击者需要2.4×10^16年。在Apple M1 Pro上的基准测试显示标准模式544ms和偏执模式2273ms的单层派生时间。

Conclusion: Qatsi为需要无状态可重现性的场景提供了实用的解决方案，通过分层密钥派生和内存硬计算实现了强大的安全性，同时保持了实际可用性。

Abstract: We present Qatsi, a hierarchical key derivation scheme using Argon2id that
generates reproducible cryptographic secrets without persistent storage. The
system eliminates vault-based attack surfaces by deriving all secrets
deterministically from a single high-entropy master secret and contextual
layers. Outputs achieve 103-312 bits of entropy through memory-hard derivation
(64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over
7776-word mnemonics or 90-character passwords. We formalize the hierarchical
construction, prove output uniformity, and quantify GPU attack costs: $2.4
\times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under
Paranoid parameters (128 MiB memory). The implementation in Rust provides
automatic memory zeroization, compile-time wordlist integrity verification, and
comprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021)
demonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid
mode single-layer derivations. Qatsi targets air-gapped systems and master
credential generation where stateless reproducibility outweighs rotation
flexibility.

</details>


### [19] [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)
*Alexander Nemecek,Zebin Yun,Zahra Rahmani,Yaniv Harel,Vipin Chaudhary,Mahmood Sharif,Erman Ayday*

Main category: cs.CR

TL;DR: 该研究探讨了临床大语言模型在电子健康记录数据微调过程中的成员推断漏洞，发现现有模型虽然具有一定抵抗力但仍存在可测量的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在临床决策支持、文档记录和患者信息系统中的广泛应用，确保其隐私性和可信度已成为医疗健康领域的重要挑战。模型在敏感电子健康记录数据上的微调虽然提升了领域对齐能力，但也增加了通过模型行为泄露患者信息的风险。

Method: 使用最先进的临床问答模型Llemr，评估了基于标准损失的攻击方法和基于领域动机的释义扰动策略，后者更真实地反映了临床对抗条件。

Result: 初步研究结果显示存在有限但可测量的成员信息泄露，表明当前临床大语言模型虽然提供部分抵抗力，但仍容易受到可能破坏临床AI信任度的微妙隐私风险影响。

Conclusion: 这些结果推动了针对特定领域的上下文感知隐私评估和防御措施的持续开发，如差分隐私微调和释义感知训练，以加强医疗AI系统的安全性和可信度。

Abstract: As large language models (LLMs) become progressively more embedded in
clinical decision-support, documentation, and patient-information systems,
ensuring their privacy and trustworthiness has emerged as an imperative
challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic
health record (EHR) data improves domain alignment but also raises the risk of
exposing patient information through model behaviors. In this work-in-progress,
we present an exploratory empirical study on membership inference
vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if
specific patient records were used during model training. Using a
state-of-the-art clinical question-answering model, Llemr, we evaluate both
canonical loss-based attacks and a domain-motivated paraphrasing-based
perturbation strategy that more realistically reflects clinical adversarial
conditions. Our preliminary findings reveal limited but measurable membership
leakage, suggesting that current clinical LLMs provide partial resistance yet
remain susceptible to subtle privacy risks that could undermine trust in
clinical AI adoption. These results motivate continued development of
context-aware, domain-specific privacy evaluations and defenses such as
differential privacy fine-tuning and paraphrase-aware training, to strengthen
the security and trustworthiness of healthcare AI systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: CAST框架通过激活空间映射实现LoRA适配器在不同LLM架构间的零样本迁移，解决了模型架构锁定的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型微调后的任务特定行为被锁定在源模型架构中的问题，现有权重空间迁移方法效果有限。

Method: 学习轻量级双向投影头，在源模型和目标模型的激活流形之间建立非线性映射，将预训练LoRA作为冻结的行为核应用。

Result: 在Llama-2和Mistral等异构模型家族间的迁移实验中，CAST转换的适配器性能达到目标模型上完全重新训练LoRA的85-95%。

Conclusion: CAST建立了模型互操作性的新标准，能够实现真正的零样本LoRA适配器迁移，显著优于现有的权重空间迁移技术。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [21] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: 提出了一种多样性感知检索框架，通过在固定token预算下选择多样化的上下文示例来提升LLM意图理解性能，在MultiWOZ 2.4和SGD数据集上显著提高了联合目标准确率。


<details>
  <summary>Details</summary>
Motivation: 面向任务的聊天机器人在实际部署中面临token预算限制和噪声上下文的问题，现有检索方法过于强调相关性而忽略了集合层面的多样性和混淆因素（如更多上下文或示例顺序）。

Method: 开发了多样性感知检索框架，选择上下文示例以平衡意图覆盖和语言多样性，并将此选择与标准LLM解码器集成；评估采用预算匹配提示和随机化位置，包括对示例数量、多样性强度和骨干模型大小的敏感性分析。

Result: 在MultiWOZ 2.4和SGD数据集上，该方法在相同token预算下实现了联合目标准确率的显著提升，超越了强大的LLM/DST基线，在K=4到7范围内保持一致的改进，且延迟适中。

Conclusion: 研究分离并验证了检索中内容多样性的影响，为构建准确、预算受限的多轮意图系统提供了一个简单可部署的选择原则。

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [22] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出了一种基于主体-事件的本体论形式化方法，用于建模复杂动态系统，不依赖全局时间。核心原则包括事件作为固定行为、因果顺序通过happens-before定义、可执行本体论等。


<details>
  <summary>Details</summary>
Motivation: 为复杂动态系统提供不依赖全局时间的建模方法，特别适用于分布式系统、微服务架构和多方视角场景。

Method: 基于九个公理(A1-A9)的形式化方法，包括事件固定、因果顺序、可执行本体论、模型作为认知过滤器等原则，并通过boldsea系统实现。

Result: 建立了确保可执行本体论正确性的三个不变性：历史单调性(I1)、因果无环性(I2)、可追溯性(I3)，并在boldsea工作流引擎中实现。

Conclusion: 该形式化方法为分布式系统、微服务架构和多方视角场景提供了有效的建模框架，通过boldsea系统验证了其实际应用价值。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [23] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: Planned diffusion是一种结合自回归和扩散模型优势的混合方法，通过两阶段生成：先自回归创建短计划分解输出为独立片段，然后用扩散模型并行生成这些片段，在保持高质量的同时显著提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中生成速度与输出质量之间的权衡问题。自回归模型质量高但生成速度慢，扩散模型可并行生成但需要多次迭代才能达到相同质量。

Method: 两阶段混合方法：1) 自回归阶段创建短计划，将输出分解为较小的独立片段；2) 扩散阶段并行生成这些片段。

Result: 在AlpacaEval 805个指令跟随提示测试中，实现了Pareto最优的质量-延迟权衡，相比自回归生成获得1.27x到1.81x速度提升，仅损失0.87%到5.4%的胜率。

Conclusion: Planned diffusion扩展了速度-质量Pareto边界，为更快、高质量的文本生成提供了实用路径，其规划机制简洁可靠，且可通过简单运行时调节灵活控制质量-延迟权衡。

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [24] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: PaDA-Agent是一种评估驱动的数据增强方法，通过发现验证数据中的失败模式来制定有针对性的数据增强策略，从而缩小小语言模型的泛化差距，显著优于现有LLM数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在部署成本和延迟方面具有优势，但在复杂领域特定任务上的准确性往往落后于大型模型。虽然有监督微调可以弥补性能差距，但需要大量人工数据准备和迭代优化工作。

Method: PaDA-Agent通过评估从验证数据中发现失败模式，并制定有针对性的数据增强策略，直接减少泛化差距，而不是仅关注模型训练错误和生成纠错样本。

Result: 实验结果显示，在Llama 3.2 1B Instruct模型微调中，PaDA-Agent相比最先进的基于LLM的数据增强方法取得了显著改进。

Conclusion: PaDA-Agent提供了一种有效的数据增强方法，能够显著提升小语言模型在复杂任务上的性能，同时减少人工干预需求。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [25] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: 提出了一个句子级标注数据集，用于在LLM推理过程中基于激活的安全行为监控，填补了现有数据集仅整体标注推理的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本推理步骤的安全监控方法可能遗漏细微的有害模式，且可能被隐藏不安全推理的模型规避。需要更精确的激活层面监控技术。

Method: 构建包含推理序列和句子级安全行为标注的数据集，提取用于检测和影响这些行为的引导向量，在模型激活层面进行操作。

Result: 展示了数据集的有效性，提取的表征能够在模型激活中检测和引导安全行为，证明了激活层面技术对改进推理安全监督的潜力。

Conclusion: 激活层面的安全监控技术能够更有效地检测和影响推理过程中的安全行为，为AI安全监督提供了新的技术途径。

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [26] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 提出一个基于大语言模型的多智能体仿真框架，用于模拟消费者决策和社会动态，为营销策略提供低成本的前期测试工具。


<details>
  <summary>Details</summary>
Motivation: 传统的事后分析和基于规则的智能体模型难以捕捉人类行为和社会互动的复杂性，需要更准确的消费者决策模拟方法。

Method: 利用大语言模型在沙盒环境中构建多智能体仿真框架，智能体能够互动、表达内部推理、形成习惯并做出购买决策，无需预定义规则。

Result: 在价格折扣营销场景中，系统提供了可操作的策略测试结果，并揭示了传统方法无法捕捉的新兴社会模式。

Conclusion: 该方法为营销人员提供了一个可扩展、低风险的预实施测试工具，减少了对耗时的事后评估的依赖，并降低了营销活动表现不佳的风险。

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real- world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox envi- ronment, our framework enables generative agents to
interact, express internal reasoning, form habits, and make purchasing
decisions without predefined rules. In a price-discount marketing scenario, the
system delivers actionable strategy-testing outcomes and reveals emergent
social patterns beyond the reach of con- ventional methods. This approach
offers marketers a scalable, low-risk tool for pre-implementation testing,
reducing reliance on time-intensive post-event evaluations and lowering the
risk of underperforming campaigns.

</details>


### [27] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出Saber算法，一种无需训练的新型采样方法，用于提升扩散语言模型在代码生成任务中的推理速度和输出质量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在代码生成任务中面临推理速度与输出质量的关键权衡问题，减少采样步骤通常会导致性能灾难性下降。

Method: Saber算法基于两个关键洞察：1）随着代码上下文的建立可以自适应加速；2）需要回溯机制来反转生成的标记。

Result: 在多个主流代码生成基准测试中，Saber将Pass@1准确率平均提升1.9%，同时实现平均251.4%的推理加速。

Conclusion: 通过利用扩散语言模型的固有优势，该工作显著缩小了与自回归模型在代码生成方面的性能差距。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [28] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 本文提出了一个可量化的AGI评估框架，基于Cattell-Horn-Carroll人类认知理论，将通用智能分解为10个核心认知领域，并应用人类心理测量工具来评估AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对人工通用智能（AGI）的具体定义，模糊了当今专业AI与人类水平认知之间的差距，需要建立量化框架来明确这一差距。

Method: 基于Cattell-Horn-Carroll理论（经验验证最充分的人类认知模型），将通用智能分解为10个核心认知领域（包括推理、记忆、感知等），并采用成熟的人类心理测量工具来评估AI系统。

Result: 应用该框架发现当代AI模型具有高度"锯齿状"的认知特征：在知识密集型领域表现熟练，但在基础认知机制（特别是长期记忆存储）方面存在严重缺陷。GPT-4的AGI得分为27%，GPT-5为58%。

Conclusion: 该框架量化了AI向AGI的快速进展和剩余的巨大差距，为AGI发展提供了具体的评估标准和方向。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [29] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 提出ssToken方法，通过自调节损失差异和语义感知注意力机制进行token级数据选择，提升大语言模型监督微调效果，无需额外参考模型


<details>
  <summary>Details</summary>
Motivation: 现有token级选择方法需要额外参考模型且仅依赖损失信息，无法很好保留语义重要但损失指标不偏好的token

Method: 使用历史模型计算token损失差异作为自调节信号，结合注意力机制的语义重要性评估，实现自适应token选择

Result: 自调节选择和语义感知选择单独使用均优于全数据微调，两者结合的ssToken方法实现协同增益，超越现有token级选择方法

Conclusion: ssToken通过自调节和语义感知token选择，在保持训练效率的同时提升模型性能，为数据质量优化提供有效解决方案

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [30] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: 本文测试了八个前沿大语言模型在开放式但规则约束任务中的自我反思能力，发现模型在第一次尝试时表现很差，反思后只有轻微改善，且经常重复相同的约束违规，表明所谓的'修正收益'主要来自偶然产生的有效项目而非真正的错误检测和原则性修复。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验当前大语言模型的'反思'功能是否与人类反思推理在功能上等效。先前在封闭式任务上的研究可能掩盖了模型在自我修正方面的局限性，因此需要在开放式但规则约束的真实世界任务中进行测试。

Method: 测试八个前沿模型在一个简单但真实的任务上：生成有效的科学测试项目，然后在考虑自己的批评后进行修订。任务具有可审计的成功标准，评估模型在第一次尝试和反思后的表现。

Result: 第一次尝试表现很差（通常4个要求项目中零有效项目，平均约1个），反思后只有轻微改善（也约1个）。关键发现是第二次尝试经常重复相同的约束违规，表明'修正收益'主要来自偶然产生的有效项目而非真正的错误检测和原则性修复。随着开放性增加，反思前后的表现都会恶化，标榜'推理'能力的模型没有优势。

Conclusion: 当前LLM的'反思'缺乏功能性证据表明存在主动的、目标驱动的监控机制，这种机制帮助人类即使在第一次尝试时也能尊重约束。在模型本身实例化这种机制之前，可靠性能需要强制执行约束的外部结构。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [31] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 提出ShortcutBreaker框架解决多类无监督异常检测中的身份捷径问题，通过低秩噪声瓶颈和全局扰动注意力机制防止特征直接复制，在四个基准数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 多类无监督异常检测需要统一模型检测多类异常，但现有Transformer架构存在身份捷径问题，即直接复制输入到输出，导致正常与异常样本的重构误差差异变小，难以区分。

Method: 提出ShortcutBreaker框架：1）基于矩阵秩不等式设计低秩噪声瓶颈，将高维特征投影到低秩潜在空间防止身份复制；2）利用ViT全局建模能力引入全局扰动注意力防止解码器中的信息捷径。

Result: 在四个基准数据集（三个工业数据集MVTec-AD、ViSA、Real-IAD和一个医学数据集Universal Medical）上分别达到99.8%、98.9%、90.6%和87.8%的图像级AUROC，优于现有方法。

Conclusion: ShortcutBreaker通过解决身份捷径问题，在多类无监督异常检测任务中实现了优异的性能，证明了所提框架的有效性和通用性。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [32] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: 提出HAP框架，通过对抗性自动课程学习实现教师-学生协同进化，解决传统自博弈在非对称学习场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈方法依赖智能体对称性，无法适应开放式的非对称学习场景。人类教学系统展示了非对称教学框架的有效性，但如何在人工系统中实现自适应课程生成仍具挑战。

Method: HAP框架将师生互动形式化为极小极大优化问题，教师作为任务生成者与学生作为问题解决者通过对抗动态共同进化，建立双向反馈系统实时调整任务复杂度。

Result: 在多任务学习领域的实验验证表明，HAP框架达到最先进基线的性能水平，同时生成的课程能提升人工智能体和人类受试者的学习效果。

Conclusion: HAP框架成功实现了非对称对抗性自动课程学习，为开放式技能获取提供了有效解决方案，证明了对抗性师生互动在课程生成中的价值。

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [33] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: UWAssess是一个基于基础模型的框架，通过监控图像自动识别城市内涝区域并生成结构化评估报告，解决了传统人工监测方法的不足。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧，城市内涝对全球公共安全和基础设施的威胁日益严重，而现有监测方法依赖人工报告，无法提供及时全面的评估。

Method: 设计了半监督微调策略和思维链提示策略，释放基础模型在数据稀缺下游任务中的潜力，利用多基础模型协作框架。

Result: 在视觉基准测试中感知性能显著提升，GPT评估证实UWAssess能生成准确描述内涝程度、深度、风险和影响的可靠文本报告。

Conclusion: 该框架实现了从感知到生成的内涝监测转变，为智能可扩展系统奠定基础，支持城市管理、灾害响应和气候韧性。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [34] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: AlphaOPT是一个自改进的经验库系统，使LLM能够从有限的演示和求解器反馈中学习，无需标注推理轨迹或参数更新，通过两阶段循环持续改进优化建模能力。


<details>
  <summary>Details</summary>
Motivation: 优化建模在工业决策中至关重要但难以自动化，现有LLM方法要么依赖脆弱的提示工程，要么需要昂贵的重新训练且泛化能力有限。

Method: 采用持续的两阶段循环：库学习阶段从失败尝试中提取求解器验证的结构化洞察；库演化阶段诊断检索偏差并细化存储洞察的适用条件。

Result: 实验显示AlphaOPT随数据增加稳步改进（从100到300训练项，准确率从65%提升到72%），在仅使用答案训练时，在OptiBench数据集上超越最强基线7.7%。

Conclusion: AlphaOPT能够高效从有限演示中学习，通过更新库而非模型权重实现持续扩展，使知识显式且可解释，为人类检查和干预提供便利。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [35] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: 本文提出PlanU方法，通过将蒙特卡洛树搜索(MCTS)与分位数分布建模相结合，解决LLM在不确定性环境中的决策问题。


<details>
  <summary>Details</summary>
Motivation: LLM在不确定性环境中的决策能力不足，特别是在随机环境中规划行动时表现不佳。现有方法主要关注LLM自身不确定性，而忽略了环境不确定性，导致在随机状态转换环境中性能较差。

Method: 提出PlanU方法，在MCTS中建模每个节点的回报为分位数分布，使用一组分位数表示回报分布。引入基于好奇心的上置信界限(UCC)评分来平衡树搜索中的探索与利用。

Result: 通过大量实验验证了PlanU在LLM不确定性决策任务中的有效性。

Conclusion: PlanU方法能够有效处理LLM决策中的不确定性，特别是在多步决策任务中与环境交互时表现优异。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [36] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: CircuitSeer是一种基于模型内部注意力机制的数据选择方法，通过识别核心推理电路来量化数据复杂度，仅使用10%的数据就能在推理任务上获得比完整数据集更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法依赖昂贵的外部模型或启发式方法，作者希望利用模型内部机制来更有效地选择高质量训练数据。

Method: 发现复杂推理任务会激活稀疏的专用注意力头形成核心推理电路，CircuitSeer通过测量数据对这些关键电路的影响来量化推理复杂度。

Result: 在4个模型和9个数据集上的实验显示，CircuitSeer选择10%数据微调Qwen2.5-Math-7B，平均Pass@1比完整数据集训练提升1.4个百分点。

Conclusion: CircuitSeer通过利用模型内部推理电路提供了一种高效的数据选择方法，显著减少了训练计算成本同时提升了性能。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [37] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: COUPLE是一个基于反事实推理的框架，用于解决大语言模型与多元化人类价值观对齐的问题，通过结构因果模型处理价值观的复杂性和可操控性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多元文化环境中的应用日益广泛，需要超越平均原则（如HHH）来与多元化的人类价值观对齐。现有方法在处理细粒度价值目标时面临两个挑战：价值观的复杂性和可操控性。

Method: 提出COUPLE框架，引入结构因果模型来表征特征间的复杂依赖关系和优先级，以及高层价值维度与行为之间的因果关系。应用反事实推理生成符合任何期望价值目标的输出。

Result: 在两个具有不同价值体系的数据集上评估COUPLE，结果显示COUPLE在各类价值目标上都优于其他基线方法。

Conclusion: COUPLE通过显式的因果建模提供了更好的可解释性，能够有效处理价值观对齐中的复杂性和可操控性挑战，在多元化价值目标上表现优异。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [38] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA-Nabla是一个端到端的智能体框架，将模拟器构建视为文本计算图中的代码实例优化，通过专门的LLM驱动智能体作为图节点，执行代码合成->执行->评估->代码修复的损失驱动循环，实现约束感知的模拟器代码生成。


<details>
  <summary>Details</summary>
Motivation: 将脆弱的提示管道转换为可重现、约束感知的模拟器代码生成，统一多智能体编排与损失对齐的优化视图，实现跨领域和模拟粒度的扩展。

Method: 使用文本梯度下降（TGD）优化器，在文本计算图中嵌入专门的LLM驱动智能体作为节点，工作流管理器执行损失驱动循环：代码合成->执行->评估->代码修复，保留人机交互用于任务规范确认。

Result: 在三个CPS任务（用户建模、口罩采用和个人移动性）中实现了最先进的整体准确性。

Conclusion: SOCIA-Nabla通过统一多智能体编排与损失对齐的优化视图，成功将脆弱的提示管道转换为可重现、约束感知的模拟器代码生成，能够跨领域和模拟粒度进行扩展。

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [39] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [40] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: QuantEvolve是一个结合质量-多样性优化和假设驱动策略生成的进化框架，用于自动化量化交易策略开发，在动态市场中产生多样化且适应性强的策略。


<details>
  <summary>Details</summary>
Motivation: 动态市场中自动化量化交易策略开发具有挑战性，现有方法难以在探索广阔策略空间的同时保持多样性，而多样性对于在不同市场条件下保持稳健性能至关重要。

Method: QuantEvolve采用与投资者偏好对齐的特征映射（如策略类型、风险状况、换手率和收益特征），结合假设驱动的多智能体系统，通过迭代生成和评估来系统探索策略空间。

Result: 实证结果表明QuantEvolve优于传统基线方法，验证了其有效性。该框架生成了能够适应市场制度转变和个性化投资需求的多样化、复杂策略。

Conclusion: QuantEvolve框架成功解决了量化交易策略开发中的多样性和适应性挑战，为个性化投资解决方案提供了有效工具，并发布了进化策略数据集以支持未来研究。

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [41] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出了Visual Attention Reasoning (VAR)框架，通过结构化搜索解决MLLMs的幻觉问题和线性推理限制，将推理过程分解为可追溯的证据基础和搜索式思维链生成，包含回溯自校正机制。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在高幻觉倾向和脆弱的线性推理过程，导致复杂任务失败，需要解决这些限制。

Method: VAR框架将基础推理重构为推理轨迹空间的结构化搜索，包含两个关键阶段：可追溯证据基础和搜索式思维链生成（带回溯机制），搜索由包含语义和几何自验证组件的多面奖励函数引导。

Result: 7B模型VAR-7B在幻觉和安全基准测试中创下新SOTA，显著优于现有开源模型，与领先专有系统表现相当。

Conclusion: VAR框架通过结构化搜索和自验证机制有效解决了MLLMs的幻觉问题，在推理任务中表现出色。

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [42] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 提出了一种结合数据和知识的分类方法，通过数据挖掘获取关联规则，利用这些规则提升决策树和随机森林的预测性能，并生成更通用的解释。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法主要依赖数据，缺乏知识元素的整合。本文旨在结合数据挖掘得到的关联规则来增强树基模型的预测能力和解释性。

Method: 从数据中挖掘关联规则（可能包含否定项），将这些规则融入决策树和随机森林模型，用于提升分类性能并生成更通用的溯因解释。

Result: 实验表明，该方法在预测性能和解释规模方面都能为两种树基模型带来益处。

Conclusion: 结合数据和知识的分类方法能够有效提升树基模型的预测性能和解释能力，为分类任务提供更优的解决方案。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [43] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 本文研究了形式论证中定性不确定性的建模，比较了抽象模型和结构化模型在表达能力方面的差异，提出了新的表达能力概念，并给出了负面和正面的表达能力结果。


<details>
  <summary>Details</summary>
Motivation: 在形式论证中建模定性不确定性对于实际应用和理论理解都至关重要，但现有工作大多关注抽象模型。本文旨在研究这些抽象模型的合理实例化，将论证的不确定性基于其组成部分（规则和前提）来建模。

Method: 将论证的不确定性基于其组成部分（规则和前提）进行结构化建模，引入了一个能够处理抽象和结构化形式体系的表达能力概念，比较了抽象模型和结构化模型在表达能力方面的差异。

Result: 提出了负面和正面的表达能力结果，比较了抽象论证框架（包括不完全抽象论证框架及其依赖扩展）与结构化模型（如ASPIC+）在表达能力方面的差异。

Conclusion: 通过将论证的不确定性基于其组成部分进行结构化建模，本文为抽象不确定性模型提供了合理的实例化，并通过表达能力分析揭示了抽象模型和结构化模型之间的重要差异。

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [44] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: 该论文将检索增强生成(RAG)系统中的查询分解和文档检索建模为探索-利用权衡问题，使用多臂老虎机方法动态选择信息量最大的子查询，显著提升了检索精度和长文本生成性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统在处理复杂用户请求时需要平衡两个关键权衡：检索足够广泛以捕获所有相关材料，同时限制检索以避免过度噪声和计算成本。传统方法难以有效解决这一平衡问题。

Method: 将查询分解和文档检索建模为探索-利用设置，每次检索一个文档来建立对给定子查询效用的信念，并据此决定继续利用当前子查询还是探索替代方案。实验了多种老虎机学习方法。

Result: 使用排名信息和人工判断估计文档相关性的方法在文档级精度上获得了35%的提升，α-nDCG提高了15%，并在长文本生成的下游任务中表现更好。

Conclusion: 将RAG系统中的查询分解和文档检索建模为探索-利用问题，并应用老虎机学习方法，能够有效动态选择最具信息量的子查询，显著提升检索精度和生成性能。

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [45] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: SherlockLLM是一个基于强化学习的对话式检索框架，通过生成二元问题序列来高效缩小搜索空间，解决了传统检索系统因用户查询模糊而难以确定目标意图的问题。


<details>
  <summary>Details</summary>
Motivation: 信息检索中的用户查询往往具有模糊性，传统系统难以从单一查询中准确识别用户目标。虽然现有的对话式交互检索系统可以澄清用户意图，但缺乏明确的策略来询问最有信息量的问题，导致效率低下。

Method: 提出SherlockLLM框架，使用强化学习训练智能体生成二元问题序列，无需大规模标注对话数据。智能体学习最优提问策略，通过一系列二元问题逐步缩小搜索范围。

Result: 在结构化任务上，SherlockLLM性能与强基线方法相当，接近二分搜索的理论最优值。在更具挑战性的非结构化任务上，该智能体显著优于基线方法，展示了其学习高效信息寻求对话策略的能力。

Conclusion: SherlockLLM是一个鲁棒且高效的解决方案，能够通过学习最优提问策略，在对话式检索中有效缩小搜索空间，特别是在非结构化任务上表现出色。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


### [46] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: ALGOS是一个用于有害藻华监测的分割与推理系统，结合遥感图像理解和严重程度估计，通过GeoSAM辅助人工评估和微调视觉语言模型，在分割和严重程度估计方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了有害藻华（特别是蓝藻）的发生，威胁水生生态系统和人类健康。传统监测方法劳动密集且覆盖范围有限，需要开发可扩展的AI驱动解决方案。

Method: 结合遥感图像理解与严重程度估计，集成GeoSAM辅助人工评估进行高质量分割掩码整理，并使用NASA的蓝藻聚合手动标签微调视觉语言模型进行严重程度预测。

Result: 实验表明ALGOS在分割和严重程度级别估计方面都实现了稳健的性能。

Conclusion: ALGOS为实现实用和自动化的蓝藻监测系统铺平了道路。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [47] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 本研究通过分析加拿大自然科学与工程研究委员会18年的资助提案，比较了三种主题建模方法（LDA、STM、BERTopic），并开发了COFFEE算法来增强BERTopic的协变量分析能力。结果显示BERTopic在识别细粒度、连贯性和新兴主题方面表现最佳，协变量分析揭示了省级研究专业化和性别相关的主题模式。


<details>
  <summary>Details</summary>
Motivation: 优化国家科学投资需要了解研究趋势演变及人口地理因素的影响，特别是在公平、多元和包容性承诺背景下。

Method: 分析NSERC 2005-2022年资助提案，比较LDA、STM和BERTopic三种主题建模方法，开发COFFEE算法实现BERTopic的协变量效应估计。

Result: 所有模型都能有效识别核心科学领域，但BERTopic表现最佳，能识别更细粒度、连贯的新兴主题（如人工智能快速发展）。COFFEE协变量分析确认了省级研究专业化，并揭示了跨学科一致的性别相关主题模式。

Conclusion: 研究结果为资助机构制定更公平有效的资助策略提供了坚实实证基础，有助于提升科学生态系统的效能。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 该论文提出了一个将大规模大气和海洋AI模型从PyTorch迁移到MindSpore并针对中国芯片优化的框架，旨在减少对GPU的依赖，提高硬件独立性。


<details>
  <summary>Details</summary>
Motivation: 当前AI气候和天气研究模型（如FourCastNet和AI-GOMS）严重依赖GPU，限制了硬件独立性，特别是对中国国产硬件和框架的支持不足。

Method: 开发了软件-硬件适配、内存优化和并行化的迁移框架，将模型从PyTorch迁移到MindSpore，并针对中国芯片进行优化。

Result: 实验结果显示迁移和优化过程保持了模型的原始精度，同时显著减少了系统依赖，通过利用中国芯片提高了运行效率。

Conclusion: 这项工作为在大气和海洋AI模型开发中利用中国国产芯片和框架提供了有价值的见解和实践指导，为实现更大的技术独立性提供了途径。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [49] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 提出了一种端到端的并行性能分析框架，用于高效处理大规模GPU跟踪数据，通过并行处理和因果图方法提高性能分析效率。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU跟踪数据对于识别异构高性能计算架构中的性能瓶颈至关重要，但单一跟踪数据的庞大体积和复杂性使得性能分析计算成本高且耗时。

Method: 采用端到端并行性能分析框架，对跟踪数据进行分区并行处理，并运用因果图方法和并行协调图来揭示执行流中的性能变异性和依赖关系。

Result: 实验结果显示在可扩展性方面提升了67%，证明了该流水线在独立分析多个跟踪数据方面的有效性。

Conclusion: 所提出的并行性能分析框架能够高效处理大规模GPU跟踪数据，显著提升分析效率和可扩展性。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [50] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: SLICE是一种针对边缘计算场景的创新调度解决方案，通过结合效用最大化请求调度算法和生成速率的动态迭代控制机制，显著提高LLM推理服务的SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现有调度服务系统仍以最大化输出令牌吞吐量为唯一优化目标，无法充分满足边缘设备对TTFT、TPOT和端到端延迟等差异化SLO要求，导致SLO违规率持续居高。

Method: 结合效用最大化请求调度算法与生成速率的动态迭代控制机制。

Result: 相比最先进解决方案Orca和FastServe，SLICE实现了高达35倍的SLO达成率提升和3.4倍的任务完成时间优势。

Conclusion: SLICE能够有效解决边缘计算场景中LLM推理服务的差异化SLO需求问题，显著提升服务质量和效率。

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [51] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: Tokencake是一个面向KV缓存的LLM多智能体服务框架，通过空间调度和时间调度优化，解决了多智能体应用中KV缓存的空间争用和时间利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多智能体应用中面临KV缓存性能挑战：空间争用导致关键智能体缓存被驱逐，时间利用率低使得等待工具调用的智能体缓存闲置在GPU内存中。

Method: Tokencake采用智能体感知设计，空间调度器使用动态内存分区保护关键智能体免受争用，时间调度器采用主动卸载和预测性上传机制在函数调用停滞期间重新利用GPU内存。

Result: 在代表性多智能体基准测试中，Tokencake相比vLLM能够减少端到端延迟超过47.06%，提高有效GPU内存利用率达16.9%。

Conclusion: Tokencake通过协同优化调度和内存管理，有效解决了多智能体LLM应用中的KV缓存性能瓶颈，显著提升了服务效率。

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


### [52] [Distributed Interactive Proofs for Planarity with Log-Star Communication](https://arxiv.org/abs/2510.18592)
*Yuval Gil,Merav Parter*

Main category: cs.DC

TL;DR: 本文提出了新的通信高效的分布式交互证明协议，用于验证平面性。主要成果是开发了O(log* n)轮次的协议，其中嵌入平面性的证明大小为O(1)，普通平面性的证明大小为O(⌈log Δ/log* n⌉)。


<details>
  <summary>Details</summary>
Motivation: 分布式交互证明(DIP)由Kol、Oshman和Saxena在2018年提出，旨在通过集中式证明者与分布式验证者之间的交互来验证图的性质，目标是设计通信量小的协议。

Method: 设计了多轮交互的分布式交互证明协议，针对嵌入平面性和普通平面性分别优化了证明大小。协议允许在轮次和证明大小之间进行权衡，对于任意1≤r≤log* n，都可以设计O(r)轮协议。

Result: 实现了O(log* n)轮次的协议，嵌入平面性的证明大小为O(1)，普通平面性的证明大小为O(⌈log Δ/log* n⌉)。更一般地，对于任意r，嵌入平面性的证明大小为O(log(r)n)，普通平面性的证明大小为O(log(r)n + log Δ/r)。

Conclusion: 本文显著改进了平面性验证的分布式交互证明协议的通信效率，为图性质验证提供了更高效的分布式解决方案。

Abstract: We provide new communication-efficient distributed interactive proofs for
planarity. The notion of a \emph{distributed interactive proof (DIP)} was
introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover}
is a single centralized entity whose goal is to prove a certain claim regarding
an input graph $G$. To do so, the prover communicates with a distributed
\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is
measured by the amount of prover-verifier communication it requires. Namely,
the goal is to design a DIP with a small number of interaction rounds and a
small \emph{proof size}, i.e., a small amount of communication per round. Our
main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and
planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log
^{*}n\rceil)$, respectively. In fact, this result can be generalized as
follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol
for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and
$O(\log ^{(r)}n+\log \Delta /r)$, respectively.

</details>


### [53] [Towards an Optimized Benchmarking Platform for CI/CD Pipelines](https://arxiv.org/abs/2510.18640)
*Nils Japke,Sebastian Koch,Helmut Lukasczyk,David Bermbach*

Main category: cs.DC

TL;DR: 性能回归检测在大型软件系统中至关重要，但目前缺乏实用的基准测试优化系统集成到CI/CD流水线中。本文识别了三个关键挑战：基准测试优化策略的可组合性、自动化评估以及实际应用复杂性，并提出了一个概念性云基准测试框架来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 大型软件系统中的性能回归会导致严重的资源效率低下，需要早期检测。频繁的基准测试对于识别这些回归和维护服务水平协议(SLA)至关重要，但基准测试资源密集且耗时，难以集成到CI/CD流水线中。

Method: 本文分析了基准测试优化领域的关键挑战，提出了三个核心问题：基准测试优化策略的可组合性、自动化评估以及实际应用复杂性。同时介绍了一个概念性的云基准测试框架来透明处理这些挑战。

Result: 识别了阻碍基准测试优化更广泛采用的三个关键挑战：(a)基准测试优化策略的可组合性，(b)基准测试结果的自动化评估，(c)在实际CI/CD系统中应用这些策略的可用性和复杂性。

Conclusion: 基准测试优化领域在关键方面仍未被充分探索，阻碍了其更广泛采用。通过提出这些开放性问题，旨在推动研究使CI/CD系统中的性能回归检测更加实用和有效。

Abstract: Performance regressions in large-scale software systems can lead to
substantial resource inefficiencies, making their early detection critical.
Frequent benchmarking is essential for identifying these regressions and
maintaining service-level agreements (SLAs). Performance benchmarks, however,
are resource-intensive and time-consuming, which is a major challenge for
integration into Continuous Integration / Continuous Deployment (CI/CD)
pipelines. Although numerous benchmark optimization techniques have been
proposed to accelerate benchmark execution, there is currently no practical
system that integrates these optimizations seamlessly into real-world CI/CD
pipelines. In this vision paper, we argue that the field of benchmark
optimization remains under-explored in key areas that hinder its broader
adoption. We identify three central challenges to enabling frequent and
efficient benchmarking: (a) the composability of benchmark optimization
strategies, (b) automated evaluation of benchmarking results, and (c) the
usability and complexity of applying these strategies as part of CI/CD systems
in practice. We also introduce a conceptual cloud-based benchmarking framework
handling these challenges transparently. By presenting these open problems, we
aim to stimulate research toward making performance regression detection in
CI/CD systems more practical and effective.

</details>


### [54] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: PCMS是一个新的GPU加速通用耦合框架，用于在超级计算机上耦合模拟代码，支持多达五维的分布式控制和场映射方法，并在Frontier超级计算机上展示了良好的弱扩展性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在领导级超级计算机上高效耦合不同模拟代码的框架，以支持复杂的多物理场模拟需求。

Method: PCMS采用GPU加速的分布式控制和场映射方法，支持多达五维的耦合，并能利用离散化和场信息来适应物理约束。

Result: 成功实现了XGC与DEGAS2的耦合，以及GNET与GTC的5D分布函数耦合；在Frontier的2,080个GPU上实现了85%的弱扩展效率。

Conclusion: PCMS是一个高效的多模型模拟耦合框架，在GPU加速的超级计算环境中表现出良好的性能和扩展性。

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [55] [From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing](https://arxiv.org/abs/2510.18525)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: SPEQ是一种算法-硬件协同设计的推测解码方法，使用全模型部分权重位形成量化草稿模型，无需额外训练或存储开销，通过可重构处理单元阵列高效执行草稿和验证过程，在15个LLM和任务上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能优异但推理延迟高，量化会降低性能，推测解码虽无损但带来额外开销，需要一种既能加速推理又保持性能的方法。

Method: 提出SPEQ算法-硬件协同设计方法：使用全模型部分权重位形成量化草稿模型，无需额外训练或存储；采用可重构处理单元阵列高效执行草稿和验证过程。

Result: 在15个LLM和任务上的实验结果显示，SPEQ相比FP16、Olive和Tender分别实现了2.07倍、1.53倍和1.45倍的加速。

Conclusion: SPEQ通过算法-硬件协同设计有效解决了大语言模型推理延迟问题，在保持性能的同时显著提升推理速度。

Abstract: Large language models achieve impressive performance across diverse tasks but
exhibit high inference latency due to their large parameter sizes. While
quantization reduces model size, it often leads to performance degradation
compared to the full model. Speculative decoding remains lossless but typically
incurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed
speculative decoding method that uses part of the full-model weight bits to
form a quantized draft model, thereby eliminating additional training or
storage overhead. A reconfigurable processing element array enables efficient
execution of both the draft and verification passes. Experimental results
across 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,
1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.

</details>
