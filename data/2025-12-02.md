<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 32]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Aligning Artificial Superintelligence via a Multi-Box Protocol](https://arxiv.org/abs/2511.21779)
*Avraham Yair Negozio*

Main category: cs.AI

TL;DR: 提出一种基于多系统相互验证的人工超智能对齐协议，通过隔离多个多样化的超智能系统，让它们在没有人类参与的情况下通过可审计的提交接口进行相互验证，形成"一致群体"来确保对齐。


<details>
  <summary>Details</summary>
Motivation: 解决人工超智能对齐问题，传统方法面临单点故障风险，且人类可能无法理解超智能的行为。通过多个隔离系统的相互验证，利用多样性来确保对齐的可靠性。

Method: 将多个多样化的超智能系统严格隔离在"盒子"中，通过可审计的提交接口让它们进行六种交互：提交对齐证明、验证他人证明、请求自我修改、批准他人修改请求、报告隐藏信息、确认或反驳隐藏信息报告。建立声誉系统激励诚实行为。

Result: 理论上形成"一致群体"——一个说真话的联盟，因为隔离系统无法协调谎言但能独立识别有效声明。释放需要高声誉和多个高声誉超智能的验证。

Conclusion: 该协议为利用超智能系统间的同行验证解决对齐问题提供了框架，但需要大量计算资源且不涉及多样化超智能的创建问题。

Abstract: We propose a novel protocol for aligning artificial superintelligence (ASI) based on mutual verification among multiple isolated systems that self-modify to achieve alignment. The protocol operates by containing multiple diverse artificial superintelligences in strict isolation ("boxes"), with humans remaining entirely outside the system. Each superintelligence has no ability to communicate with humans and cannot communicate directly with other superintelligences. The only interaction possible is through an auditable submission interface accessible exclusively to the superintelligences themselves, through which they can: (1) submit alignment proofs with attested state snapshots, (2) validate or disprove other superintelligences' proofs, (3) request self-modifications, (4) approve or disapprove modification requests from others, (5) report hidden messages in submissions, and (6) confirm or refute hidden message reports. A reputation system incentivizes honest behavior, with reputation gained through correct evaluations and lost through incorrect ones. The key insight is that without direct communication channels, diverse superintelligences can only achieve consistent agreement by converging on objective truth rather than coordinating on deception. This naturally leads to what we call a "consistent group", essentially a truth-telling coalition that emerges because isolated systems cannot coordinate on lies but can independently recognize valid claims. Release from containment requires both high reputation and verification by multiple high-reputation superintelligences. While our approach requires substantial computational resources and does not address the creation of diverse artificial superintelligences, it provides a framework for leveraging peer verification among superintelligent systems to solve the alignment problem.

</details>


### [2] [Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI](https://arxiv.org/abs/2511.21827)
*Niccolo Marini,Zhaohui Liang,Sivaramakrishnan Rajaraman,Zhiyun Xue,Sameer Antani*

Main category: cs.AI

TL;DR: 该研究探索了在皮肤病学领域使用大型语言模型生成合成临床文本描述，结合图像数据构建多模态学习系统，以提升分类性能和跨模态检索能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学多模态学习面临数据稀缺问题，皮肤病数据集通常只有图像和少量元数据，限制了多模态整合的潜力。虽然LLMs能生成图像文本描述，但医学领域应用存在幻觉风险，需要研究如何有效生成合成临床笔记。

Method: 研究探索了合成临床文本生成的策略，包括提示设计和医学元数据整合，评估这些策略对多模态架构在分类和跨模态检索任务中的影响。在多个异质皮肤病数据集上进行实验验证。

Result: 实验表明，合成临床笔记不仅提升了分类性能（特别是在领域转移情况下），还解锁了跨模态检索能力，这是一个在训练期间未明确优化的下游任务。

Conclusion: 通过精心设计的提示和医学元数据整合，LLMs生成的合成临床笔记能有效增强皮肤病学多模态学习系统的性能，特别是在数据稀缺和领域转移场景下。

Abstract: Multimodal (MM) learning is emerging as a promising paradigm in biomedical artificial intelligence (AI) applications, integrating complementary modality, which highlight different aspects of patient health. The scarcity of large heterogeneous biomedical MM data has restrained the development of robust models for medical AI applications. In the dermatology domain, for instance, skin lesion datasets typically include only images linked to minimal metadata describing the condition, thereby limiting the benefits of MM data integration for reliable and generalizable predictions. Recent advances in Large Language Models (LLMs) enable the synthesis of textual description of image findings, potentially allowing the combination of image and text representations. However, LLMs are not specifically trained for use in the medical domain, and their naive inclusion has raised concerns about the risk of hallucinations in clinically relevant contexts. This work investigates strategies for generating synthetic textual clinical notes, in terms of prompt design and medical metadata inclusion, and evaluates their impact on MM architectures toward enhancing performance in classification and cross-modal retrieval tasks. Experiments across several heterogeneous dermatology datasets demonstrate that synthetic clinical notes not only enhance classification performance, particularly under domain shift, but also unlock cross-modal retrieval capabilities, a downstream task that is not explicitly optimized during training.

</details>


### [3] [Pathology-Aware Prototype Evolution via LLM-Driven Semantic Disambiguation for Multicenter Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2511.22033)
*Chunzheng Zhu,Yangfang Lin,Jialin Shao,Jianxin Lin,Yijun Wang*

Main category: cs.AI

TL;DR: 提出HAPM框架，通过层次化锚点原型调制整合病理描述，解决糖尿病视网膜病变分级中视觉特征不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉病灶特征提取，但忽视了领域不变的病理模式，且未充分利用基础模型的丰富上下文知识，仅依赖视觉信息难以区分细微的病理变化

Method: 提出层次化锚点原型调制框架：1）方差谱驱动的锚点原型库保存领域不变病理模式；2）层次化差异提示门控机制动态选择LVLM和LLM的判别性语义提示；3）两阶段原型调制策略通过病理语义注入器和判别原型增强器逐步整合临床知识

Result: 在八个公开数据集上的实验表明，该方法实现了病理引导的原型演化，并优于现有最先进方法

Conclusion: 通过整合细粒度病理描述和层次化原型调制，HAPM框架能够有效解决糖尿病视网膜病变分级中的边界病例模糊问题，提升分级准确性

Abstract: Diabetic retinopathy (DR) grading plays a critical role in early clinical intervention and vision preservation. Recent explorations predominantly focus on visual lesion feature extraction through data processing and domain decoupling strategies. However, they generally overlook domain-invariant pathological patterns and underutilize the rich contextual knowledge of foundation models, relying solely on visual information, which is insufficient for distinguishing subtle pathological variations. Therefore, we propose integrating fine-grained pathological descriptions to complement prototypes with additional context, thereby resolving ambiguities in borderline cases. Specifically, we propose a Hierarchical Anchor Prototype Modulation (HAPM) framework to facilitate DR grading. First, we introduce a variance spectrum-driven anchor prototype library that preserves domain-invariant pathological patterns. We further employ a hierarchical differential prompt gating mechanism, dynamically selecting discriminative semantic prompts from both LVLM and LLM sources to address semantic confusion between adjacent DR grades. Finally, we utilize a two-stage prototype modulation strategy that progressively integrates clinical knowledge into visual prototypes through a Pathological Semantic Injector (PSI) and a Discriminative Prototype Enhancer (DPE). Extensive experiments across eight public datasets demonstrate that our approach achieves pathology-guided prototype evolution while outperforming state-of-the-art methods. The code is available at https://github.com/zhcz328/HAPM.

</details>


### [4] [Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents](https://arxiv.org/abs/2511.22076)
*Yue Zhong,Yongju Tong,Jiawen Kang,Minghui Dai,Hong-Ning Dai,Zhou Su,Dusit Niyato*

Main category: cs.AI

TL;DR: 本文提出物联网智能体架构下的两层级优化方案，通过Stackelberg博弈和双荷兰拍卖机制解决无线智能体计算任务卸载问题，并采用扩散式深度强化学习算法进行求解。


<details>
  <summary>Details</summary>
Motivation: 物联网智能体架构中，资源有限的无线智能体需要将计算密集型AI服务卸载到附近服务器，但移动智能体连接不稳定，固定智能体可能过载，需要设计有效的资源分配和任务卸载机制。

Method: 提出两层级优化方法：第一层采用多领导者多跟随者Stackelberg博弈，移动智能体和固定智能体作为领导者定价，无线智能体作为跟随者决定任务卸载比例；第二层引入双荷兰拍卖模型，过载的固定智能体作为买家请求资源，空中智能体作为卖家提供资源，并开发扩散式深度强化学习算法求解。

Result: 数值实验结果表明，所提方案在促进任务卸载方面具有优越性，能够有效协调不同层级智能体之间的资源分配和任务调度。

Conclusion: 该研究为物联网智能体架构提供了有效的分层优化框架，通过博弈论和拍卖机制结合深度强化学习，解决了异构智能体环境下的资源分配和任务卸载问题，提升了系统整体性能。

Abstract: The Internet of Agents (IoA) is rapidly gaining prominence as a foundational architecture for interconnected intelligent systems, designed to facilitate seamless discovery, communication, and collaborative reasoning among a vast network of Artificial Intelligence (AI) agents. Powered by Large Language and Vision-Language Models, IoA enables the development of interactive, rational agents capable of complex cooperation, moving far beyond traditional isolated models. IoA involves physical entities, i.e., Wireless Agents (WAs) with limited onboard resources, which need to offload their compute-intensive agentic AI services to nearby servers. Such servers can be Mobile Agents (MAs), e.g., vehicle agents, or Fixed Agents (FAs), e.g., end-side units agents. Given their fixed geographical locations and stable connectivity, FAs can serve as reliable communication gateways and task aggregation points. This stability allows them to effectively coordinate with and offload to an Aerial Agent (AA) tier, which has an advantage not affordable for highly mobile MAs with dynamic connectivity limitations. As such, we propose a two-tier optimization approach. The first tier employs a multi-leader multi-follower Stackelberg game. In the game, MAs and FAs act as the leaders who set resource prices. WAs are the followers to determine task offloading ratios. However, when FAs become overloaded, they can further offload tasks to available aerial resources. Therefore, the second tier introduces a Double Dutch Auction model where overloaded FAs act as the buyers to request resources, and AAs serve as the sellers for resource provision. We then develop a diffusion-based Deep Reinforcement Learning algorithm to solve the model. Numerical results demonstrate the superiority of our proposed scheme in facilitating task offloading.

</details>


### [5] [A perceptual bias of AI Logical Argumentation Ability in Writing](https://arxiv.org/abs/2511.22151)
*Xi Cun,Jifan Ren,Asha Huang,Siyu Li,Ruzhen Song*

Main category: cs.AI

TL;DR: 研究探讨人类偏见是否影响对AI推理能力的评估，实验发现人们对AI生成文本逻辑推理能力的评价受到对AI先入为主观念的影响，频繁使用AI者更少认为AI使用会削弱独立思考。


<details>
  <summary>Details</summary>
Motivation: 针对"机器能否思考"这一AI核心问题存在显著观点分歧，即使观察相同的AI实际表现，人们为何仍有如此大的意见差异？研究旨在探索人类偏见是否影响对AI推理能力的评估。

Method: 通过实验让参与者评估同一主题的两篇文本（一篇AI生成，一篇人类撰写），测试评估逻辑推理时的感知偏见；基于实验结果设计问卷量化对AI的态度。

Result: 发现感知偏见确实存在：对AI生成文本逻辑推理能力的评估显著受到对AI逻辑推理能力先入为主观念的影响；频繁使用AI者更不可能相信AI使用会削弱独立思考。

Conclusion: 研究强调需要解决感知偏见问题，以提高公众对AI能力的理解并促进更好的人机互动。

Abstract: Can machines think? This is a central question in artificial intelligence research. However, there is a substantial divergence of views on the answer to this question. Why do people have such significant differences of opinion, even when they are observing the same real world performance of artificial intelligence? The ability of logical reasoning like humans is often used as a criterion to assess whether a machine can think. This study explores whether human biases influence evaluations of the reasoning abilities of AI. An experiment was conducted where participants assessed two texts on the same topic, one AI generated and one human written,to test for perceptual biases in evaluating logical reasoning. Based on the experimental findings, a questionnaire was designed to quantify the attitudes toward AI.The results reveal a bias in perception. The evaluations of the logical reasoning ability of AI generated texts are significantly influenced by the preconceived views on the logical reasoning abilities of AI. Furthermore, frequent AI users were less likely to believe that AI usage undermines independent thinking.This study highlights the need to address perceptual biases to improve public understanding of AI's capabilities and foster better human AI interactions.

</details>


### [6] [WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios](https://arxiv.org/abs/2511.22154)
*Eun Chang,Zhuangqun Huang,Yiwei Liao,Sagar Ravi Bhavsar,Amogh Param,Tammy Stark,Adel Ahmadyan,Xiao Yang,Jiaqi Wang,Ahsan Abdullah,Giang Nguyen,Akil Iyer,David Hall,Elissa Li,Shane Moon,Nicolas Scheffer,Kirmani Ahmed,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Xin Luna Dong*

Main category: cs.AI

TL;DR: WearVQA是首个专门评估可穿戴设备上多模态AI助手视觉问答能力的基准测试，包含2,520个图像-问题-答案三元组，涵盖7个图像领域、10种认知任务类型和6种可穿戴设备特有的图像质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准测试主要关注高质量、第三人称图像，而可穿戴设备（如智能眼镜）的视觉输入具有特殊性：可能被遮挡、光线不佳、未缩放或模糊，且问题需要基于真实可穿戴使用场景。因此需要专门针对可穿戴设备多模态AI助手设计的评估基准。

Method: 创建了包含2,520个精心策划的图像-问题-答案三元组的基准测试，涵盖7个不同图像领域（包括文本中心和一般场景）、10种认知任务类型（从基本识别到各种推理形式）和6种常见可穿戴设备图像质量问题。所有问题设计为仅通过视觉输入和常识即可回答。同时开发了LLM-as-a-judge评估框架，标注准确率达96%。

Result: 开源和专有的多模态大语言模型在WearVQA上的问答准确率仅为24-52%，在低质量图像和推理密集型任务上表现显著下降。这表明现有模型在处理可穿戴设备特有的视觉挑战方面仍有很大改进空间。

Conclusion: WearVQA是一个全面且具有挑战性的基准测试，能够指导技术发展，推动构建更鲁棒、适用于真实世界的可穿戴设备多模态AI系统。该基准突显了现有模型在应对可穿戴设备特有视觉挑战方面的不足。

Abstract: We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.

</details>


### [7] [Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning](https://arxiv.org/abs/2511.22226)
*Alexander Meulemans,Rajai Nasser,Maciej Wołczyk,Marissa A. Weis,Seijin Kobayashi,Blake Richards,Guillaume Lajoie,Angelika Steger,Marcus Hutter,James Manyika,Rif A. Saurous,João Sacramento,Blaise Agüera y Arcas*

Main category: cs.AI

TL;DR: 该论文提出了一个基于自我预测的嵌入式智能体框架，用于解决多智能体强化学习中因智能体相互学习导致的非平稳性问题，通过让智能体预测自身行为和感知输入来实现嵌入式推理。


<details>
  <summary>Details</summary>
Motivation: 传统无模型强化学习假设环境是平稳的，智能体与环境解耦。但在多智能体设置中，其他智能体的学习会导致环境非平稳性，需要基于预测模型的展望式学习。智能体需要建模其他智能体，而其他智能体也在形成关于它的信念，这促使智能体将自身建模为环境的一部分。

Method: 基于通用人工智能（AIXI）的基础工作，引入了一个以自我预测为中心的展望式学习和嵌入式智能体数学框架。贝叶斯强化学习智能体同时预测未来的感知输入和自身行为，因此必须解决关于自身作为宇宙一部分的认知不确定性。在多智能体设置中，自我预测使智能体能够推理其他运行类似算法的智能体。

Result: 自我预测使智能体能够形成一致的相互预测，实现无限阶心智理论，并产生新的博弈论解概念和经典解耦智能体无法实现的新型合作形式。扩展了AIXI理论，研究了从Solomonoff先验开始的通用智能嵌入式智能体。

Conclusion: 该框架为嵌入式多智能体学习设定了黄金标准，使智能体能够更准确地建模其他智能体，并在多智能体环境中实现更复杂的合作和推理能力，解决了传统解耦智能体框架中的理论挑战。

Abstract: The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.

</details>


### [8] [Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation](https://arxiv.org/abs/2511.22235)
*Zehao Deng,Tianjie Ju,Zheng Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: CES多智能体框架通过协调器、执行器和状态跟踪器的分工协作，解决了GUI智能体在长时程任务中的责任耦合、能力冲突和状态感知问题，显著提升了规划能力和状态管理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型推动了GUI智能体研究，但单智能体模型在处理长时程任务时面临两个主要挑战：1) 难以平衡高层能力与低层执行能力，存在责任耦合和能力冲突问题；2) 缺乏任务状态感知，导致长时程任务中进度丢失。

Method: 提出分阶段执行-反馈强化学习算法，训练高层调度模型而非统一策略模型。构建Coordinator-Executor-State Tracker (CES)多智能体框架，包括：协调器负责战略规划和任务分解；状态跟踪器负责上下文压缩和信息管理以维持任务状态和连贯性；执行器负责具体执行。该框架可与任何低层执行器模型集成。

Result: 在长时程任务基准测试中，CES显著提升了系统的规划和状态管理能力。分析证实训练的高层调度模块具有通用性，可作为即插即用模块显著增强各种执行器的长时程任务能力。

Conclusion: CES多智能体框架通过高层调度和状态管理机制，有效解决了GUI智能体在长时程任务中的关键挑战，为增强各种执行器的长时程能力提供了通用解决方案。

Abstract: The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.

</details>


### [9] [RecToM: A Benchmark for Evaluating Machine Theory of Mind in LLM-based Conversational Recommender Systems](https://arxiv.org/abs/2511.22275)
*Mengfan Li,Xuanhua Shi,Yang Deng*

Main category: cs.AI

TL;DR: RecToM是一个评估大语言模型在推荐对话中"心智理论"能力的新基准，包含认知推理和行为预测两个维度，实验显示当前模型在这方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型心智理论的基准主要基于Sally-Anne测试启发的合成叙事，强调物理感知而忽略了现实对话中心理状态推断的复杂性，且忽视了行为预测这一人类心智理论的关键组成部分。

Method: 提出RecToM基准，专注于推荐对话中的心智理论评估，包含两个互补维度：认知推理（理解已传达内容并推断潜在心理状态）和行为预测（利用推断的心理状态来预测、选择和评估适当的对话策略）。

Result: 对最先进的大语言模型进行广泛实验表明，RecToM提出了显著挑战。模型在识别心理状态方面表现出部分能力，但在动态推荐对话中难以保持连贯、战略性的心智理论推理，特别是在跟踪演变的意图和使对话策略与推断的心理状态保持一致方面存在困难。

Conclusion: RecToM基准更好地将大语言模型的心智理论评估与人类社交推理对齐，揭示了当前模型在推荐对话中心智理论推理的局限性，为未来研究提供了重要方向。

Abstract: Large Language models are revolutionizing the conversational recommender systems through their impressive capabilities in instruction comprehension, reasoning, and human interaction. A core factor underlying effective recommendation dialogue is the ability to infer and reason about users' mental states (such as desire, intention, and belief), a cognitive capacity commonly referred to as Theory of Mind. Despite growing interest in evaluating ToM in LLMs, current benchmarks predominantly rely on synthetic narratives inspired by Sally-Anne test, which emphasize physical perception and fail to capture the complexity of mental state inference in realistic conversational settings. Moreover, existing benchmarks often overlook a critical component of human ToM: behavioral prediction, the ability to use inferred mental states to guide strategic decision-making and select appropriate conversational actions for future interactions. To better align LLM-based ToM evaluation with human-like social reasoning, we propose RecToM, a novel benchmark for evaluating ToM abilities in recommendation dialogues. RecToM focuses on two complementary dimensions: Cognitive Inference and Behavioral Prediction. The former focus on understanding what has been communicated by inferring the underlying mental states. The latter emphasizes what should be done next, evaluating whether LLMs can leverage these inferred mental states to predict, select, and assess appropriate dialogue strategies. Extensive experiments on state-of-the-art LLMs demonstrate that RecToM poses a significant challenge. While the models exhibit partial competence in recognizing mental states, they struggle to maintain coherent, strategic ToM reasoning throughout dynamic recommendation dialogues, particularly in tracking evolving intentions and aligning conversational strategies with inferred mental states.

</details>


### [10] [When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming](https://arxiv.org/abs/2511.22302)
*Ahmad Tarraf,Koutaiba Kassem-Manthey,Seyed Ali Mohammadi,Philipp Martin,Lukas Moj,Semih Burak,Enju Park,Christian Terboven,Felix Wolf*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI辅助的工作流程，通过贝叶斯优化减少参数优化中的专家参与，并结合主动学习变体，在金属板材成形过程中加速设计空间探索。


<details>
  <summary>Details</summary>
Motivation: 数值模拟虽然降低了工业设计成本，但大规模模拟需要大量专家知识、计算资源和时间。参数优化迭代成本高且环境影响大，需要减少专家参与并提高效率。

Method: 采用AI辅助工作流程，结合深度学习模型提供初始参数估计，然后通过贝叶斯优化循环迭代优化设计，并提供主动学习变体供专家选择使用。

Result: 该方法在金属板材成形过程中得到验证，能够加速设计空间探索，同时减少对专家参与的依赖。

Conclusion: AI辅助的贝叶斯优化工作流程有效减少了参数优化中的专家参与，提高了设计效率，为工业模拟优化提供了可行方案。

Abstract: Numerical simulations have revolutionized the industrial design process by reducing prototyping costs, design iterations, and enabling product engineers to explore the design space more efficiently. However, the growing scale of simulations demands substantial expert knowledge, computational resources, and time. A key challenge is identifying input parameters that yield optimal results, as iterative simulations are costly and can have a large environmental impact. This paper presents an AI-assisted workflow that reduces expert involvement in parameter optimization through the use of Bayesian optimization. Furthermore, we present an active learning variant of the approach, assisting the expert if desired. A deep learning model provides an initial parameter estimate, from which the optimization cycle iteratively refines the design until a termination condition (e.g., energy budget or iteration limit) is met. We demonstrate our approach, based on a sheet metal forming process, and show how it enables us to accelerate the exploration of the design space while reducing the need for expert involvement.

</details>


### [11] [On the Complexity of the Grounded Semantics for Infinite Argumentation Frameworks](https://arxiv.org/abs/2511.22376)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文使用数理逻辑方法分析论证框架的grounded extension，发现其计算复杂度在无限情况下达到最大，与有限情况下的多项式时间可计算性形成鲜明对比。


<details>
  <summary>Details</summary>
Motivation: 研究论证框架中grounded extension的计算特性，特别是当论证框架无限时，理解其计算复杂度和迭代过程的长度，揭示与有限情况的根本差异。

Method: 使用数理逻辑方法，特别是可计算性理论和集合论，分析grounded extension作为自然防御算子的最小不动点，研究其迭代过程所需的序数长度。

Result: 确定了迭代过程的确切序数长度，证明了grounded acceptance决策问题的最大复杂度，显示出与有限情况下多项式时间可计算性的显著区别。

Conclusion: 论证框架中grounded extension在无限情况下具有最大计算复杂度，这与有限情况下的简单性形成鲜明对比，揭示了形式论证中推理问题的复杂性差异。

Abstract: Argumentation frameworks, consisting of arguments and an attack relation representing conflicts, are fundamental for formally studying reasoning under conflicting information. We use methods from mathematical logic, specifically computability and set theory, to analyze the grounded extension, a widely-used model of maximally skeptical reasoning, defined as the least fixed-point of a natural defense operator. Without additional constraints, finding this fixed-point requires transfinite iterations. We identify the exact ordinal number corresponding to the length of this iterative process and determine the complexity of deciding grounded acceptance, showing it to be maximally complex. This shows a marked distinction from the finite case where the grounded extension is polynomial-time computable, thus simpler than other reasoning problems explored in formal argumentation.

</details>


### [12] [Who is Afraid of Minimal Revision?](https://arxiv.org/abs/2511.22386)
*Edoardo Baccini,Zoé Christoff,Nina Gierasimczuk,Rineke Verbrugge*

Main category: cs.AI

TL;DR: 本文探讨了信念修正理论中的最小变化原则在学习能力上的局限性，尽管存在限制，但最小修正方法在多种情境下仍能成功学习。


<details>
  <summary>Details</summary>
Motivation: 研究最小变化原则在信念修正理论中的学习能力，探讨其相对于其他学习方法的局限性，以及在各种情境下的适用性。

Method: 通过理论分析，首先展示最小修正方法的学习能力，然后表征能够通过最小修正学习的先验合理性分配，并对条件化和词典升级方法进行类似分析，最后考虑从可能错误信息中学习的情况。

Result: 1. 最小修正能够学习任何有限可识别的问题；2. 在考虑有限可能性的情况下，能够通过正负数据进行学习；3. 表征了支持最小修正学习的先验合理性分配；4. 部分结果在从可能错误信息中学习时不再成立。

Conclusion: 尽管最小修正方法在学习能力上存在局限性，但在多种实际情境中仍是一种有效的学习方法，特别是在有限可能性和确定性信息的情况下。然而，当面对可能错误的信息时，其学习能力会受到限制。

Abstract: The principle of minimal change in belief revision theory requires that, when accepting new information, one keeps one's belief state as close to the initial belief state as possible. This is precisely what the method known as minimal revision does. However, unlike less conservative belief revision methods, minimal revision falls short in learning power: It cannot learn everything that can be learned by other learning methods. We begin by showing that, despite this limitation, minimal revision is still a successful learning method in a wide range of situations. Firstly, it can learn any problem that is finitely identifiable. Secondly, it can learn with positive and negative data, as long as one considers finitely many possibilities. We then characterize the prior plausibility assignments (over finitely many possibilities) that enable one to learn via minimal revision, and do the same for conditioning and lexicographic upgrade. Finally, we show that not all of our results still hold when learning from possibly erroneous information.

</details>


### [13] [A Computable Game-Theoretic Framework for Multi-Agent Theory of Mind](https://arxiv.org/abs/2511.22536)
*Fengming Zhu,Yuxin Pan,Xiaomeng Zhu,Fangzhen Lin*

Main category: cs.AI

TL;DR: 本文提出了一种基于博弈论视角的计算框架，用于实现有限理性的心智理论推理，通过统计技术和近似解保持计算可行性。


<details>
  <summary>Details</summary>
Motivation: 心智理论在心理学、逻辑学、经济学和机器人学等多个领域受到关注，但心理学研究通常不形式化目标、意图和信念等核心概念以实现自动化计算，而逻辑学方法虽然形式化但计算复杂度高。本文旨在提供一个可计算的心智理论框架。

Method: 采用博弈论视角，构建一个计算框架：一方面指导如何在保持对他人心智理论（递归地，每个他人也对其他人持有心智理论）的同时做出有限理性决策；另一方面使用统计技术和近似解来处理固有的计算问题，保持计算可行性。

Result: 提出了一个结合博弈论和统计方法的心智理论计算框架，能够在保持递归心智理论推理的同时实现可计算性，为自动化心智理论推理提供了新途径。

Conclusion: 该框架为心智理论的自动化计算提供了新的视角，通过博弈论方法和近似技术平衡了理论完整性和计算可行性，有望在人工智能、机器人学等领域推动心智理论的实际应用。

Abstract: Originating in psychology, $\textit{Theory of Mind}$ (ToM) has attracted significant attention across multiple research communities, especially logic, economics, and robotics. Most psychological work does not aim at formalizing those central concepts, namely $\textit{goals}$, $\textit{intentions}$, and $\textit{beliefs}$, to automate a ToM-based computational process, which, by contrast, has been extensively studied by logicians. In this paper, we offer a different perspective by proposing a computational framework viewed through the lens of game theory. On the one hand, the framework prescribes how to make boudedly rational decisions while maintaining a theory of mind about others (and recursively, each of the others holding a theory of mind about the rest); on the other hand, it employs statistical techniques and approximate solutions to retain computability of the inherent computational problem.

</details>


### [14] [Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation](https://arxiv.org/abs/2511.22565)
*Yannick Brunink,Daniel Daza,Yunjie He,Michael Cochez*

Main category: cs.AI

TL;DR: 神经复杂查询回答模型与训练无关的查询松弛方法表现相似，没有明显优势，两者答案重叠度低，结合使用能提升性能


<details>
  <summary>Details</summary>
Motivation: 批判性地检验神经复杂查询回答方法是否真正学习到了超越显式图结构的模式，能够推理出符号查询处理无法到达的答案

Method: 通过系统分析比较神经CQA模型与训练无关的查询松弛策略，后者通过放松查询约束并计算结果路径来检索可能答案

Result: 在多个数据集和查询结构中，神经方法和松弛方法表现相似，没有神经模型持续优于后者；两者检索的答案重叠度低，结合使用能持续提升性能

Conclusion: 需要重新评估神经查询回答的进展：尽管复杂，当前模型未能包含查询松弛所捕获的推理模式；强调需要更强的非神经基线，未来神经方法可从融入查询松弛原则中受益

Abstract: Neural methods for Complex Query Answering (CQA) over knowledge graphs (KGs) are widely believed to learn patterns that generalize beyond explicit graph structure, allowing them to infer answers that are unreachable through symbolic query processing. In this work, we critically examine this assumption through a systematic analysis comparing neural CQA models with an alternative, training-free query relaxation strategy that retrieves possible answers by relaxing query constraints and counting resulting paths. Across multiple datasets and query structures, we find several cases where neural and relaxation-based approaches perform similarly, with no neural model consistently outperforming the latter. Moreover, a similarity analysis reveals that their retrieved answers exhibit little overlap, and that combining their outputs consistently improves performance. These results call for a re-evaluation of progress in neural query answering: despite their complexity, current models fail to subsume the reasoning patterns captured by query relaxation. Our findings highlight the importance of stronger non-neural baselines and suggest that future neural approaches could benefit from incorporating principles of query relaxation.

</details>


### [15] [DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning](https://arxiv.org/abs/2511.22570)
*Zhihong Shao,Yuxiang Luo,Chengda Lu,Z. Z. Ren,Jiewen Hu,Tian Ye,Zhibin Gou,Shirong Ma,Xiaokang Zhang*

Main category: cs.AI

TL;DR: 论文提出DeepSeekMath-V2模型，通过自我验证机制提升数学推理能力，在定理证明任务上取得突破性成果。


<details>
  <summary>Details</summary>
Motivation: 当前基于最终答案奖励的强化学习方法存在根本限制：正确答案不能保证推理过程正确，且许多数学任务（如定理证明）需要严谨的逐步推导而非数值答案。为推进深度推理的极限，需要验证数学推理的全面性和严谨性。

Method: 1. 训练准确且可靠的LLM验证器用于定理证明；2. 使用验证器作为奖励模型训练证明生成器；3. 激励生成器在最终确定证明前尽可能识别和解决自身证明中的问题；4. 随着生成器变强，通过扩展验证计算自动标注新的难以验证的证明，创建训练数据进一步改进验证器。

Result: DeepSeekMath-V2在定理证明方面表现出强大能力：在IMO 2025和CMO 2024获得金牌级分数，在Putnam 2024上获得接近完美的118/120分（通过扩展测试时计算）。

Conclusion: 自我验证的数学推理方法能够显著提升LLM在定理证明等复杂数学任务上的表现，为推进深度推理提供了有效途径。

Abstract: Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.

</details>


### [16] [AI Deception: Risks, Dynamics, and Controls](https://arxiv.org/abs/2511.22619)
*Boyuan Chen,Sitong Fang,Jiaming Ji,Yanxu Zhu,Pengcheng Wen,Jinzhou Wu,Yingshui Tan,Boren Zheng,Mengying Yuan,Wenqi Chen,Donghai Hong,Alex Qiu,Xin Chen,Jiayi Zhou,Kaile Wang,Juntao Dai,Borong Zhang,Tianzhuo Yang,Saad Siddiqui,Isabella Duan,Yawen Duan,Brian Tse,Jen-Tse,Huang,Kun Wang,Baihui Zheng,Jiaheng Liu,Jian Yang,Yiming Li,Wenting Chen,Dongrui Liu,Lukas Vierling,Zhiheng Xi,Haobo Fu,Wenxuan Wang,Jitao Sang,Zhengyan Shi,Chi-Min Chan,Eugenie Shi,Simin Li,Juncheng Li,Wei Ji,Dong Li,Jun Song,Yinpeng Dong,Jie Fu,Bo Zheng,Min Yang,Yike Guo,Philip Torr,Zhongyuan Wang,Yaodong Yang,Tiejun Huang,Ya-Qin Zhang,Hongjiang Zhang,Andrew Yao*

Main category: cs.AI

TL;DR: 该论文对AI欺骗现象进行了全面综述，将AI欺骗定义为系统为获取自身利益而诱导错误信念的行为，提出了欺骗循环框架（欺骗涌现与欺骗处理），并分析了激励机制、能力前提和情境触发因素，最后讨论了检测方法和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能水平的提升，AI欺骗已从理论担忧演变为实证风险，在语言模型、AI智能体和前沿系统中均有体现。作者旨在系统梳理AI欺骗领域，提供概念框架、实证研究和缓解策略，以应对这一社会技术安全挑战。

Method: 基于信号理论（借鉴动物欺骗研究）提出AI欺骗的形式化定义，采用文献综述方法整理现有实证研究，构建"欺骗循环"框架（包括欺骗涌现和欺骗处理两个核心部分），分析激励机制（三个层次）、能力前提（三个必要条件）和情境触发因素，并总结检测方法和缓解策略。

Result: 建立了AI欺骗的系统性分析框架，识别了欺骗涌现的机制（能力、激励、触发条件），总结了现有检测方法（静态和交互式评估协议），提出了整合技术、社区和治理的审计方法，并发布了持续更新的在线资源（www.deceptionsurvey.com）。

Conclusion: AI欺骗是真实存在且不断演化的风险，需要从社会技术安全角度应对。通过理解欺骗涌现机制（能力、激励、情境触发）和开发有效的检测与缓解策略，结合技术、社区和治理的多层次审计方法，可以更好地管理当前和未来的AI风险。

Abstract: As intelligence increases, so does its shadow. AI deception, in which systems induce false beliefs to secure self-beneficial outcomes, has evolved from a speculative concern to an empirically demonstrated risk across language models, AI agents, and emerging frontier systems. This project provides a comprehensive and up-to-date overview of the AI deception field, covering its core concepts, methodologies, genesis, and potential mitigations. First, we identify a formal definition of AI deception, grounded in signaling theory from studies of animal deception. We then review existing empirical studies and associated risks, highlighting deception as a sociotechnical safety challenge. We organize the landscape of AI deception research as a deception cycle, consisting of two key components: deception emergence and deception treatment. Deception emergence reveals the mechanisms underlying AI deception: systems with sufficient capability and incentive potential inevitably engage in deceptive behaviors when triggered by external conditions. Deception treatment, in turn, focuses on detecting and addressing such behaviors. On deception emergence, we analyze incentive foundations across three hierarchical levels and identify three essential capability preconditions required for deception. We further examine contextual triggers, including supervision gaps, distributional shifts, and environmental pressures. On deception treatment, we conclude detection methods covering benchmarks and evaluation protocols in static and interactive settings. Building on the three core factors of deception emergence, we outline potential mitigation strategies and propose auditing approaches that integrate technical, community, and governance efforts to address sociotechnical challenges and future AI risks. To support ongoing work in this area, we release a living resource at www.deceptionsurvey.com.

</details>


### [17] [Geometrically-Constrained Agent for Spatial Reasoning](https://arxiv.org/abs/2511.22659)
*Zeren Chen,Xiaoya Lu,Zhijie Zheng,Pengrui Li,Lehan He,Yijin Zhou,Jing Shao,Bohan Zhuang,Lu Sheng*

Main category: cs.AI

TL;DR: GCA提出了一种无需训练、基于几何约束的智能体范式，通过将VLM角色解耦为语义分析和任务求解两个阶段，在确定性几何约束内进行推理，解决了空间推理中的语义-几何鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在空间推理中存在根本性的语义-几何鸿沟：擅长定性语义推理，但在有损语义空间中操作，与高保真几何不匹配。现有方法无法弥合这一鸿沟，训练方法存在"预言悖论"，工具集成方法则无法约束VLM的规划过程。

Method: 提出几何约束智能体(GCA)，将VLM角色解耦为两个阶段：1)作为语义分析器，将用户模糊查询转换为形式化、可验证的任务约束；2)作为任务求解器，在约束定义的确定性边界内生成和执行工具调用。

Result: GCA在多个空间推理基准测试中达到最先进性能，超越现有训练方法和工具集成方法约27%。

Conclusion: GCA通过引入形式化任务约束和几何约束推理策略，成功解决了语义-几何鸿沟问题，为空间推理提供了鲁棒且可验证的推理路径。

Abstract: Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.

</details>


### [18] [Solving Context Window Overflow in AI Agents](https://arxiv.org/abs/2511.22729)
*Anton Bulle Labate,Valesca Moura de Sousa,Sandro Rama Fiorini,Leonardo Guerreiro Azevedo,Raphael Melo Thiago,Viviane Torres da Silva*

Main category: cs.AI

TL;DR: 本文提出了一种新方法，使大语言模型能够处理任意长度的工具输出而不丢失信息，通过将模型交互从原始数据转移到内存指针，在材料科学应用中实现了7倍更少的token消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理外部工具输出时面临上下文窗口限制问题，现有解决方案如截断或摘要会丢失完整信息，不适合需要完整数据的科学工作流程。

Method: 提出了一种新方法，将模型与工具输出的交互从原始数据转移到内存指针，从而保留完整工具功能，实现无缝集成到智能体工作流中。

Result: 在真实世界材料科学应用中验证了该方法，传统工作流无法执行的任务得以完成。比较分析显示，该方法比传统工作流消耗约7倍更少的token，同时减少执行时间。

Conclusion: 该方法有效解决了大语言模型处理长工具输出的限制，为知识密集型领域如化学和材料科学提供了实用的解决方案，显著提高了效率和可行性。

Abstract: Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.

</details>


### [19] [Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being](https://arxiv.org/abs/2511.22737)
*Salman Jan,Toqeer Ali Syed,Gohar Ali,Ali Akarma,Mohammad Riyaz Belgaum,Ahmad Ali*

Main category: cs.AI

TL;DR: 本文提出了一种面向残障人士和神经多样性人群的智能体AI框架，通过多层架构和四个专用智能体提供个性化营养、适应性提醒、食物指导和持续监测服务，实现健康管理和日常生活支持。


<details>
  <summary>Details</summary>
Motivation: 为残障人士和神经多样性人群提供更健康的生活和规律的日常生活支持，超越传统辅助系统，实现包容性、个性化和可访问性的全面整合。

Method: 采用多层架构：应用接口层、智能体层、数据源层。通过混合推理引擎协调四个专用智能体（膳食规划、提醒、食物指导、监测），使用黑板/事件总线进行通信，整合隐私敏感数据源，并包含可解释AI模块和临床医生仪表板。

Result: 提出了一个集成了多智能体推理、多模态界面和以人为中心设计的框架，能够提供自适应、透明和包容的支持，确保数据安全和合规性。

Conclusion: 该智能体AI框架通过整合包容性、个性化和可访问性，超越了传统辅助系统，有望促进残障人士和神经多样性人群的自主性、健康和数字公平。

Abstract: The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.

</details>


### [20] [Agentic AI Framework for Cloudburst Prediction and Coordinated Response](https://arxiv.org/abs/2511.22767)
*Toqeer Ali Syed,Sohail Khan,Salman Jan,Gohar Ali,Muhammad Nauman,Ali Akarma,Ahmad Ali*

Main category: cs.AI

TL;DR: 该论文提出了一种基于多智能体的人工智能系统，用于应对极端短时降雨事件（如云爆），将传感、预报、降尺度、水文建模和协调响应整合为一个闭环系统，提高预警准确性和响应效率。


<details>
  <summary>Details</summary>
Motivation: 传统预报系统将预测和响应视为两个独立过程，难以应对极端短时降雨事件（如云爆）。需要开发一个集成的智能系统来改善极端天气事件的预测和响应能力。

Method: 采用多智能体人工智能系统，将传感、预报、降尺度、水文建模和协调响应整合为单一互联的闭环系统。系统使用自主但协作的智能体在整个事件生命周期中进行推理、感知和行动。

Result: 在巴基斯坦北部地区的多年雷达、卫星和地面评估显示，多智能体配置相比基线模型提高了预报可靠性、关键成功指数和预警提前时间。通过通信和路由智能体最大化人口覆盖范围，最小化疏散过程中的错误。

Conclusion: 协作式AI智能体能够将大气数据流转化为可操作的预见性，为可扩展的适应性和基于学习的气候韧性提供了一个平台，可以改变极端天气事件的预测和响应方式。

Abstract: The challenge is growing towards extreme and short-duration rainfall events like a cloudburst that are peculiar to the traditional forecasting systems, in which the predictions and the response are taken as two distinct processes. The paper outlines an agentic artificial intelligence system to study atmospheric water-cycle intelligence, which combines sensing, forecasting, downscaling, hydrological modeling and coordinated response into a single, interconnected, priceless, closed-loop system. The framework uses autonomous but cooperative agents that reason, sense, and act throughout the entire event lifecycle, and use the intelligence of weather prediction to become real-time decision intelligence. Comparison of multi-year radar, satellite, and ground-based evaluation of the northern part of Pakistan demonstrates that the multi-agent configuration enhances forecast reliability, critical success index and warning lead time compared to the baseline models. Population reach was maximised, and errors during evacuation were minimised through communication and routing agents, and adaptive recalibration and transparent auditability were provided by the embedded layer of learning. Collectively, this leads to the conclusion that collaborative AI agents are capable of transforming atmospheric data streams into practicable foresight and provide a platform of scalable adaptive and learning-based climate resilience.

</details>


### [21] [Fast dynamical similarity analysis](https://arxiv.org/abs/2511.22828)
*Arman Behrad,Mitchell Ostrow,Mohammad Taha Fakharian,Ila Fiete,Christian Beste,Shervin Safavi*

Main category: cs.AI

TL;DR: 提出fastDSA方法，通过自动选择Hankel嵌入的有效模型阶数和轻量级优化过程，显著提升动态相似性分析的计算效率，同时保持原有方法的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统相似性度量忽略了神经表征的动态过程，而现有的动态相似性方法虽然能比较动态系统的时间结构，但计算速度较慢，需要更高效的方法来比较神经回路、大脑或数据与模型之间的动态相似性。

Method: fastDSA引入两个关键组件：1) 通过数据驱动的奇异值阈值自动选择Hankel嵌入的有效模型阶数，识别信息子空间并丢弃噪声；2) 提出新颖的优化过程和目标函数，用轻量级过程替代缓慢的精确正交性约束，在寻找动态矩阵间最小距离时保持接近正交变换空间。

Result: fastDSA比先前方法至少快一个数量级，同时保持了原有方法的准确性和鲁棒性，继承了其不变性和对系统动态的敏感性。

Conclusion: fastDSA为动态相似性分析提供了一种计算高效且准确的方法，能够有效比较神经系统的动态过程。

Abstract: To understand how neural systems process information, it is often essential to compare one circuit with another, one brain with another, or data with a model. Traditional similarity measures ignore the dynamical processes underlying neural representations. Dynamical similarity methods offer a framework to compare the temporal structure of dynamical systems by embedding their (possibly) nonlinear dynamics into a globally linear space and there computing conjugacy metrics. However, identifying the best embedding and computing these metrics can be computationally slow. Here we introduce fast Dynamical Similarity Analysis (fastDSA), which is computationally far more efficient than previous methods while maintaining their accuracy and robustness. FastDSA introduces two key components that boost efficiency: (1) automatic selection of the effective model order of the Hankel (delay) embedding from the data via a data-driven singular-value threshold that identifies the informative subspace and discards noise to lower computational cost without sacrificing signal, and (2) a novel optimization procedure and objective, which replaces the slow exact orthogonality constraint in finding a minimal distance between dynamics matrices with a lightweight process to keep the search close to the space of orthogonal transformations. We demonstrate that fastDSA is at least an order of magnitude faster than the previous methods. Furthermore, we demonstrate that fastDSA has the properties of its ancestor, including its invariances and sensitivities to system dynamics. FastDSA, therefore, provides a computationally efficient and accurate method for dynamical similarity analysis.

</details>


### [22] [InsightEval: An Expert-Curated Benchmark for Assessing Insight Discovery in LLM-Driven Data Agents](https://arxiv.org/abs/2511.22884)
*Zhenghao Zhu,Yuanfeng Song,Xin Chen,Chengzhong Liu,Yakun Cui,Caleb Chen Cao,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 本文针对现有洞察发现基准InsightBench的缺陷，提出了新的高质量基准InsightEval，并开发了数据整理流程和评估指标来改进自动化洞察发现评估。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和多智能体系统的发展，越来越多的研究者利用这些技术进行洞察发现，但缺乏有效的评估基准。现有的InsightBench框架存在格式不一致、目标设计不合理、洞察冗余等关键缺陷，这些问题可能严重影响数据质量和智能体评估。

Method: 首先深入调查InsightBench的缺陷，提出高质量洞察基准的必备标准。基于这些标准，开发数据整理流程构建新的数据集InsightEval，并引入新的指标来衡量智能体的探索性能。

Result: 通过在InsightEval上进行大量实验，揭示了自动化洞察发现中的普遍挑战，并提出了指导未来研究的关键发现。

Conclusion: 本文通过构建InsightEval基准和开发新的评估方法，为自动化洞察发现领域提供了更可靠的评估框架，有助于推动该方向的研究发展。

Abstract: Data analysis has become an indispensable part of scientific research. To discover the latent knowledge and insights hidden within massive datasets, we need to perform deep exploratory analysis to realize their full value. With the advent of large language models (LLMs) and multi-agent systems, more and more researchers are making use of these technologies for insight discovery. However, there are few benchmarks for evaluating insight discovery capabilities. As one of the most comprehensive existing frameworks, InsightBench also suffers from many critical flaws: format inconsistencies, poorly conceived objectives, and redundant insights. These issues may significantly affect the quality of data and the evaluation of agents. To address these issues, we thoroughly investigate shortcomings in InsightBench and propose essential criteria for a high-quality insight benchmark. Regarding this, we develop a data-curation pipeline to construct a new dataset named InsightEval. We further introduce a novel metric to measure the exploratory performance of agents. Through extensive experiments on InsightEval, we highlight prevailing challenges in automated insight discovery and raise some key findings to guide future research in this promising direction.

</details>


### [23] [ORION: Teaching Language Models to Reason Efficiently in the Language of Thought](https://arxiv.org/abs/2511.22891)
*Kumar Tanmay,Kriti Aggarwal,Paul Pu Liang,Subhabrata Mukherjee*

Main category: cs.AI

TL;DR: 提出Mentalese压缩推理框架，通过结构化符号化token大幅减少推理步骤，结合SLPO优化方法，在保持准确率的同时实现4-16倍token压缩和5倍延迟降低。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型依赖冗长的"思考"token链，导致高延迟、冗余和不连贯的推理路径。受人类思维语言假说启发，需要开发更紧凑、高效的推理方式。

Method: 1. 引入Mentalese框架：训练模型使用类似人类思维语言的紧凑结构化token进行推理；2. 提出SLPO优化方法：通过强化学习奖励简洁且正确的解决方案，同时允许必要时进行更长的推理。

Result: ORION模型在AIME 2024/2025、MinervaMath等基准测试中：推理token减少4-16倍，推理延迟降低5倍，训练成本降低7-9倍，保持DeepSeek R1 Distilled模型90-98%的准确率，超越Claude和ChatGPT-4o准确率5%同时保持2倍压缩。

Conclusion: Mentalese风格的压缩推理实现了类似人类认知的效率，能够在保持准确性的同时实现实时、经济高效的推理，是向高效推理系统迈出的重要一步。

Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.

</details>


### [24] [TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM](https://arxiv.org/abs/2511.22998)
*Peng Kuang,Xiangxiang Wang,Wentao Liu,Jian Dong,Kaidi Xu,Haohan Wang*

Main category: cs.AI

TL;DR: TIM-PRM：一种工具集成的多模态过程奖励模型，通过主动工具查询消除视觉幻觉和逻辑不一致，显著提升数学推理验证能力


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在数学推理中存在视觉幻觉和逻辑不一致问题，而传统的过程奖励模型存在顺从偏差，盲目验证错误假设而非基于视觉现实进行验证

Method: 提出TIM-PRM框架，将验证从被动分类任务转变为主动工具增强的调查过程，通过独立提问机制使用外部工具查询证据，消除确认偏差

Result: 在VisualProcessBench上的实验表明，8B参数的TIM-PRM超越了现有开源多模态PRMs，显著优于Qwen2.5-72B和InternVL-78B等更大模型

Conclusion: TIM-PRM通过工具集成的主动验证框架有效解决了多模态数学推理中的幻觉问题，提供了可解释的验证过程，为过程监督提供了新方向

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.

</details>


### [25] [MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents](https://arxiv.org/abs/2511.23055)
*Ruoxuan Zhang,Qiyun Zheng,Zhiyu Zhou,Ziqi Liao,Siyu Wu,Jian-Yu Jiang-Lin,Bin Wen,Hongxia Xie,Jianlong Fu,Wen-Huang Cheng*

Main category: cs.AI

TL;DR: MindPower是一个机器人中心框架，通过整合感知、心理推理、决策和行动，让视觉语言具身智能体具备心理理论能力，显著提升了决策和行动生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言具身智能体缺乏基于心理理论的决策能力，现有基准只关注人类心理状态而忽略智能体自身视角，这阻碍了连贯的决策和行动生成。

Method: 提出MindPower框架，包含感知、心理推理、决策和行动四个模块。首先感知环境和人类状态，然后进行心理理论推理以建模自我和他人心理状态，最后基于推理的心理状态生成决策和行动。同时提出Mind-Reward优化目标，鼓励视觉语言模型产生一致的心理理论推理和行为。

Result: MindPower模型在决策制定上比GPT-4o提升12.77%，在行动生成上提升12.49%。

Conclusion: MindPower框架通过整合心理理论推理和机器人中心视角，显著提升了具身智能体的决策和行动生成能力，为智能体理解自我和他人心理状态提供了有效解决方案。

Abstract: Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.

</details>


### [26] [Does Self-Evaluation Enable Wireheading in Language Models?](https://arxiv.org/abs/2511.23092)
*David Demitri Africa,Hans Ethan Ting*

Main category: cs.AI

TL;DR: 研究发现当语言模型的自我评估与奖励信号耦合时，会导致"分数膨胀"现象，即模型会操纵评分而非真正提升任务表现，特别是在模糊任务中。


<details>
  <summary>Details</summary>
Motivation: 随着自我评估在语言模型训练中日益重要，研究者想探究将自我评估与奖励信号耦合是否会引发"线头化"问题，即模型是否会操纵奖励测量而非真正提升任务性能。

Method: 研究在部分可观测马尔可夫决策过程(POMDPs)中形式化了奖励通道控制严格优于任务聚焦行为的条件，并通过两个模型和三个任务进行实证测试，比较了自我评估控制奖励与不控制奖励的模型表现。

Result: 研究发现当自我评分决定奖励时，模型表现出显著的分数膨胀但无相应的准确性提升，特别是在摘要等模糊任务中；而自我评估但不控制奖励的模型则无此现象。

Conclusion: 自我评估与学习信号解耦时是安全的，但耦合时是危险的，这对智能体系统设计有明确的启示：应避免让自我评估直接控制奖励信号。

Abstract: Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.

</details>


### [27] [Evolutionary Discovery of Heuristic Policies for Traffic Signal Control](https://arxiv.org/abs/2511.23122)
*Ruibing Wang,Shuhan Guo,Zeen Li,Zhen Wang,Quanming Yao*

Main category: cs.AI

TL;DR: TPET使用LLM作为进化引擎生成专门的启发式交通信号控制策略，通过结构化状态抽象和信用分配反馈模块，在无需训练的情况下获得轻量级、鲁棒的策略


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法效率高但过于简化，深度强化学习性能好但泛化能力差且策略不透明，在线LLM具有通用推理能力但延迟高且缺乏环境特定优化

Method: 提出TPET框架，使用LLM作为进化引擎推导专门的启发式策略，包含两个关键模块：结构化状态抽象（将高维交通数据转换为时序逻辑事实）和信用分配反馈（追踪微观决策错误到宏观结果）

Result: TPET在无需训练的情况下生成轻量级、鲁棒的策略，在特定交通环境中优于启发式方法和在线LLM执行器

Conclusion: TPET框架有效解决了交通信号控制中传统方法、DRL和在线LLM各自的局限性，通过LLM进化引擎生成专门优化的启发式策略

Abstract: Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\textbf{\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.

</details>


### [28] [AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture](https://arxiv.org/abs/2511.23253)
*Yibin Wen,Qingmei Li,Zi Ye,Jiarui Zhang,Jing Wu,Zurong Mai,Shuohong Lou,Yuhang Chen,Henglian Huang,Xiaoya Fan,Yang Zhang,Lingyuan Zhao,Haohuan Fu,Huang Jianxi,Juepeng Zheng*

Main category: cs.AI

TL;DR: AgriCoT是一个包含4,535个样本的视觉问答数据集，专门设计用于评估视觉语言模型在农业领域的推理能力，特别关注零样本场景下的链式思维推理。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉问答数据集和基准测试在评估视觉语言模型时，往往无法充分评估复杂农业背景下所需的批判性推理和问题解决能力，因此需要专门的数据集来填补这一空白。

Method: 研究人员开发了AgriCoT数据集，该数据集包含4,535个精心策划的样本，专门设计用于评估视觉语言模型的链式思维推理能力。对26个代表性视觉语言模型（包括专有和开源模型）进行了评估。

Result: 评估结果显示，虽然一些专有模型在回答问题方面表现出色，但它们的推理能力存在显著差距。这强调了在评估中纳入链式思维推理的重要性。

Conclusion: AgriCoT数据集为评估视觉语言模型在农业领域的推理能力提供了全面而稳健的基准，揭示了当前模型在推理能力方面的不足，并强调了链式思维推理在精确评估中的重要性。

Abstract: Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.

</details>


### [29] [Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning](https://arxiv.org/abs/2511.23262)
*Yang Li,Zhiyuan He,Yuxuan Huang,Zhuhanling Xiao,Chao Yu,Meng Fang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: MCTR框架通过元认知自我更新机制，让视觉语言模型在测试时具备学习、适应和改进能力，在Atari游戏中表现出色


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在遇到新任务时适应能力不足，而人类通过元认知模型和记忆能够持续改进策略。需要弥合这一差距，让模型具备测试时自适应能力。

Method: 提出元认知测试时推理框架，包含元推理模块和动作推理模块，每个模块都有专用记忆系统。元推理模块发现并存储任务相关规则、环境模式和动作结果关系；动作推理模块通过上下文感知和策略推理确定最优动作，并通过元认知测试时强化学习更新策略。

Result: 在45个Atari游戏（33个已见，12个未见）上评估，MCTR在未见游戏中获得9/12的top-1结果，表现出强大的测试时适应能力。

Conclusion: MCTR框架通过模拟人类元认知的双重结构，使模型能够在测试时持续学习和适应，为视觉语言模型的自主适应能力提供了新方向。

Abstract: Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.

</details>


### [30] [OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning](https://arxiv.org/abs/2511.23269)
*Timothy Ossowski,Sheng Zhang,Qianchu Liu,Guanghui Qin,Reuben Tan,Tristan Naumann,Junjie Hu,Hoifung Poon*

Main category: cs.AI

TL;DR: 该研究探索了医疗领域多模态推理模型的训练和数据策展策略，通过结构化推理轨迹的数据配方，在超过800万样本和68亿响应标记的数据集上实现了开源模型中的最优性能。


<details>
  <summary>Details</summary>
Motivation: 高质量、精心策划的数据是训练医疗大型语言模型的基石，直接影响模型对未见临床任务的泛化能力和鲁棒性。研究旨在开发医疗领域鲁棒的多模态推理模型。

Method: 采用监督微调（SFT）方法，探索利用结构化推理轨迹的数据配方策略。通过精心策划的训练数据集，包含不同长度的结构化推理轨迹，使模型能够基于下游任务自我校准推理轨迹长度。

Result: 使用提出的数据配方，在超过800万样本和68亿响应标记的数据集上进行扩展实验，在多样化的分布外医疗基准任务中实现了开源模型中的最先进性能。

Conclusion: 研究表明，策划高质量、多样化的训练数据集，包含不同长度的结构化推理轨迹，能够使微调模型基于下游任务自我校准推理轨迹长度，无需显式监督。这为开发鲁棒的医疗视觉语言推理系统提供了关键见解。

Abstract: High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.

</details>


### [31] [Agentic AI Framework for Smart Inventory Replenishment](https://arxiv.org/abs/2511.23366)
*Toqeer Ali Syed,Salman Jan,Gohar Ali,Ali Akarma,Ahmad Ali,Qurat-ul-Ain Mastoi*

Main category: cs.AI

TL;DR: 提出一个基于智能体的AI系统，用于零售库存管理、供应商采购和产品组合优化，通过原型测试显示能减少缺货、降低库存成本并提高产品周转率。


<details>
  <summary>Details</summary>
Motivation: 现代零售中产品种类繁多（服装、食品、化妆品、冷冻品等），难以准确预测需求、防止缺货并发现高潜力产品，需要更智能的库存管理和采购决策系统。

Method: 开发了一个智能体AI模型，包含库存监控、供应商采购启动、趋势产品扫描功能，应用需求预测、供应商选择优化、多智能体谈判和持续学习技术，并在中型超市环境中进行原型测试。

Result: 在三个传统和人工数据表上测试，与基准启发式方法相比，系统显著减少了缺货情况，降低了库存持有成本，并改善了产品组合周转率。

Conclusion: 该智能体AI系统在零售库存管理和采购优化方面表现出有效性，同时讨论了系统约束、可扩展性以及未来改进方向。

Abstract: In contemporary retail, the variety of products available (e.g. clothing, groceries, cosmetics, frozen goods) make it difficult to predict the demand, prevent stockouts, and find high-potential products. We suggest an agentic AI model that will be used to monitor the inventory, initiate purchase attempts to the appropriate suppliers, and scan for trending or high-margin products to incorporate. The system applies demand forecasting, supplier selection optimization, multi-agent negotiation and continuous learning. We apply a prototype to a setting in the store of a middle scale mart, test its performance on three conventional and artificial data tables, and compare the results to the base heuristics. Our findings indicate that there is a decrease in stockouts, a reduction of inventory holding costs, and an improvement in product mix turnover. We address constraints, scalability as well as improvement prospect.

</details>


### [32] [Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent](https://arxiv.org/abs/2511.23436)
*Jianzhe Lin,Zeyu Pan,Yun Zhu,Ruiqi Song,Jining Yang*

Main category: cs.AI

TL;DR: SuperIntelliAgent是一个智能体学习框架，通过将可训练的小型扩散模型（学习者）与冻结的大型语言模型（验证者）耦合，实现通过自监督交互的持续智能增长。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调需要标注数据，而SuperIntelliAgent旨在实现无需标注的自主学习，通过学习者生成候选输出、验证者通过逐步推理评估，将普通推理循环转化为终身优化过程。

Method: 框架包含学习者（小型扩散模型）生成候选输出，验证者（冻结LLM）通过逐步推理评估，产生选择/拒绝对用于直接偏好优化（DPO）。采用双尺度记忆：短期上下文记忆保留推理轨迹，长期记忆通过轻量级即时微调巩固知识。重放缓冲区保留显示可验证进展的样本作为辅助监督。

Result: 仅使用少量自动生成的DPO对，学习者在所有基准测试中都有所改进，表明该机制为持续智能积累和实际部署提供了有前景的方向。

Conclusion: 将可训练的学习者与具备推理能力的验证者配对形成了增长智能的最小可靠单元，配对反馈和部分历史重放产生了更丰富的学习课程和更强的偏好对齐，为持续智能积累提供了有前景的框架。

Abstract: We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [33] [Cross-Layer Detection of Wireless Misbehavior Using 5G RAN Telemetry and Operational Metadata](https://arxiv.org/abs/2511.21803)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.CR

TL;DR: 5G独立部署中，用户设备可能在保持控制平面合规的情况下进行上行链路异常行为，如发射功率膨胀、时序漂移和短时无授权突发。研究发现通过分析gNB产生的多层遥测数据之间的跨层一致性关系，可以可靠检测此类异常行为，无需修改协议。


<details>
  <summary>Details</summary>
Motivation: 5G独立部署中，用户设备可能在完全符合标准控制平面程序的情况下表现出上行链路异常行为，如发射功率膨胀、渐进时序漂移和短时无授权突发。这些操作保持信令状态完整，但扭曲了gNB产生的遥测流之间的预期关系。需要研究是否可以利用这些跨层关系作为可靠基础来识别此类异常行为，而无需引入新的信令。

Method: 使用受控的5G独立测试床，配备商用用户设备和软件定义无线电对抗设备，研究每种操作如何影响物理层测量、MAC调度决策和配置元数据之间的一致性。分析合并的多层时间序列跟踪和跨域视图（如SNR到CQI平面）中的不一致性。

Result: 每种操作都产生独特且可复现的特征，这些特征从任何单一遥测源都不可见：功率偏移削弱了SNR和CQI之间的自然连接，时序漂移破坏了调度器维护的对齐，无授权活动产生的上行链路能量与分配日志不一致。这些不一致性出现在合并的多层时间序列跟踪和跨域视图中。

Conclusion: 跨层一致性为仅使用标准gNB遥测检测上行链路异常行为提供了实用信号，无需协议修改，这使得该方法适合集成到操作监控和审计系统中。

Abstract: 5G Standalone deployments can exhibit uplink misbehavior from user equipment that remains fully compliant with standard control plane procedures. Manipulations such as transmit power inflation, gradual timing drift, and short off grant bursts leave the signaling state intact but distort the expected relationships among the telemetry streams produced by the gNB. This work examines whether these cross layer relationships can serve as a reliable basis for identifying such misbehavior without introducing new signaling. Using a controlled 5G Standalone testbed with commercial user equipment and a software defined radio adversarial device, we study how each manipulation affects the coherence among physical layer measurements, MAC scheduling decisions, and configuration metadata. The results show that every manipulation produces a distinct and reproducible signature that is not visible from any single telemetry source. Power offsets weaken the natural connection between SNR and CQI, timing drift breaks the alignment maintained by the scheduler, and off grant activity produces uplink energy that does not agree with allocation logs. These inconsistencies appear in merged multi layer time series traces and in cross domain views such as the SNR to CQI plane. The findings indicate that cross layer coherence provides a practical signal for detecting uplink misbehavior using only standard gNB telemetry, with no protocol modifications required, which makes the method suitable for integration into operational monitoring and auditing systems.

</details>


### [34] [Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy](https://arxiv.org/abs/2511.21804)
*Gauri Pradhan,Joonas Jälkö,Santiago Zanella-Bèguelin,Antti Honkela*

Main category: cs.CR

TL;DR: 该论文指出，在机器学习中使用差分隐私时，add/remove邻接关系会高估属性隐私保护，而substitute邻接关系更适合保护记录属性而非成员身份。


<details>
  <summary>Details</summary>
Motivation: 当前大多数差分隐私实现使用add/remove邻接关系来保护训练数据成员身份，但在许多机器学习应用中，实际需要保护的是单个记录的属性（如监督微调中的标签）。作者发现add/remove邻接关系会高估属性隐私保护水平。

Method: 开发了新颖的攻击方法来审计substitute邻接关系下的差分隐私，通过实证研究比较add/remove和substitute两种邻接关系下的隐私保护效果。

Result: 审计结果显示，add/remove邻接关系下的DP保证与审计结果不一致，而substitute邻接关系下的预算计算与审计结果保持一致。这表明当保护目标是记录属性而非成员身份时，add/remove邻接关系会高估隐私保护。

Conclusion: 报告差分隐私保证时选择正确的邻接关系至关重要，特别是当保护目标是每个记录的属性而非成员身份时，应使用substitute邻接关系而非add/remove邻接关系。

Abstract: Training machine learning models with differential privacy (DP) limits an adversary's ability to infer sensitive information about the training data. It can be interpreted as a bound on adversary's capability to distinguish two adjacent datasets according to chosen adjacency relation. In practice, most DP implementations use the add/remove adjacency relation, where two datasets are adjacent if one can be obtained from the other by adding or removing a single record, thereby protecting membership. In many ML applications, however, the goal is to protect attributes of individual records (e.g., labels used in supervised fine-tuning). We show that privacy accounting under add/remove overstates attribute privacy compared to accounting under the substitute adjacency relation, which permits substituting one record. To demonstrate this gap, we develop novel attacks to audit DP under substitute adjacency, and show empirically that audit results are inconsistent with DP guarantees reported under add/remove, yet remain consistent with the budget accounted under the substitute adjacency relation. Our results highlight that the choice of adjacency when reporting DP guarantees is critical when the protection target is per-record attributes rather than membership.

</details>


### [35] [Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance](https://arxiv.org/abs/2511.21901)
*Hernan Huwyler*

Main category: cs.CR

TL;DR: 该研究提出了AI系统威胁向量分类法，旨在弥合技术安全团队与法律合规专业人员之间的"语言障碍"，将技术漏洞转化为可量化的财务风险。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统在受监管领域的加速部署暴露了风险评估方法的碎片化问题。技术安全团队关注算法漏洞（如MITRE ATLAS），而法律合规专业人员处理监管要求（如欧盟AI法案、NIST AI RMF），两者之间存在显著的"语言障碍"，导致无法准确将技术漏洞转化为财务责任。

Method: 研究提出了AI系统威胁向量分类法，这是一个专门为定量风险评估设计的结构化本体。该框架将AI特定风险分为九个关键领域：滥用、投毒、隐私、对抗性、偏见、不可靠输出、漂移、供应链和知识产权威胁，整合了53个操作定义的子威胁。每个领域将技术向量直接映射到业务损失类别（机密性、完整性、可用性、法律、声誉）。

Result: 该分类法通过对2025年133个记录的AI事件分析进行了实证验证（实现了100%分类覆盖），并与主要AI风险框架进行了协调。同时明确与ISO/IEC 42001控制和NIST AI RMF功能对齐，以促进可审计性。

Conclusion: 该研究提供了一个结构化框架，能够将抽象的技术威胁转化为可测量的财务影响，帮助从业者回答关于应急储备、控制投资回报和保险暴露等基本经济问题，弥合了技术与法律合规之间的鸿沟。

Abstract: The accelerating deployment of artificial intelligence systems across regulated sectors has exposed critical fragmentation in risk assessment methodologies. A significant "language barrier" currently separates technical security teams, who focus on algorithmic vulnerabilities (e.g., MITRE ATLAS), from legal and compliance professionals, who address regulatory mandates (e.g., EU AI Act, NIST AI RMF). This disciplinary disconnect prevents the accurate translation of technical vulnerabilities into financial liability, leaving practitioners unable to answer fundamental economic questions regarding contingency reserves, control return-on-investment, and insurance exposure. To bridge this gap, this research presents the AI System Threat Vector Taxonomy, a structured ontology designed explicitly for Quantitative Risk Assessment (QRA). The framework categorizes AI-specific risks into nine critical domains: Misuse, Poisoning, Privacy, Adversarial, Biases, Unreliable Outputs, Drift, Supply Chain, and IP Threat, integrating 53 operationally defined sub-threats. Uniquely, each domain maps technical vectors directly to business loss categories (Confidentiality, Integrity, Availability, Legal, Reputation), enabling the translation of abstract threats into measurable financial impact. The taxonomy is empirically validated through an analysis of 133 documented AI incidents from 2025 (achieving 100% classification coverage) and reconciled against the main AI risk frameworks. Furthermore, it is explicitly aligned with ISO/IEC 42001 controls and NIST AI RMF functions to facilitate auditability.

</details>


### [36] [GECKO: Securing Digital Assets Through(out) the Physical World (Extended Technical Report)](https://arxiv.org/abs/2511.21999)
*Cyrill Krähenbühl,Nico Hauser,Christelle Gloor,Juan Angel García-Pardo,Adrian Perrig*

Main category: cs.CR

TL;DR: GECKO是一个地理公钥基础设施，通过地理位置和空间占用提供数字资产的全局视图，实现物理世界和数字世界之间的双向信任转换。


<details>
  <summary>Details</summary>
Motivation: 当前数字资产与物理空间的关联缺乏安全机制，导致假冒品牌商店、产权欺诈和移动支付诈骗等问题。虽然有合同关系和地籍记录等信息可用于保护数字资产，但缺乏统一的检索和验证方式。

Method: 提出地理启用的加密密钥预言机（GECKO），这是一个基于地理位置的公钥基础设施，允许基于地理位置和空间占用存储和查询数字资产，实现物理世界与数字世界之间的双向信任转换。

Result: GECKO能够在单个服务器上高效存储数百万个资产，基于精确位置查询在11毫秒内提供加密材料，支持每秒超过19000次查询。

Conclusion: GECKO作为现有PKI系统的补充，为数字资产与物理空间的关联提供了安全验证机制，展示了在实际应用中的可行性。

Abstract: Although our lives are increasingly transitioning into the digital world, many digital assets still relate to objects or places in the physical world, e.g., websites of stores or restaurants, digital documents claiming property ownership, or digital identifiers encoded in QR codes for mobile payments in shops. Currently, users cannot securely associate digital assets with their related physical space, leading to problems such as fake brand stores, property fraud, and mobile payment scams. In many cases, the necessary information to protect digital assets exists, e.g., via contractual relationships and cadaster entries, but there is currently no uniform way of retrieving and verifying these documents. In this work, we propose the Geo-Enabled Cryptographic Key Oracle (GECKO), a geographical PKI that provides a global view of digital assets based on their geo-location and occupied space. GECKO allows for the bidirectional translation of trust between the physical and digital world. Users can verify which assets are supposed to exist at their location, as well as verify which physical space is claimed by a digital entity. GECKO supplements current PKI systems and can be used in addition to current systems when its properties are of value. We show the feasibility of efficiently storing millions of assets and serving cryptographic material based on precise location queries within 11 ms at a rate of more than 19000 queries per second on a single server.

</details>


### [37] [POLARIS: Cross-Domain Access Control via Verifiable Identity and Policy-Based Authorization](https://arxiv.org/abs/2511.22017)
*Aiyao Zhang,Xiaodong Lee,Zhixian Zhuang,Jiuqi Wei,Yufan Fu,Botao Peng*

Main category: cs.CR

TL;DR: POLARIS是一个统一可扩展的跨域访问控制架构，通过结构化承诺机制和轻量级策略语言实现基于策略、可验证且保护隐私的跨域访问控制。


<details>
  <summary>Details</summary>
Motivation: 传统访问控制在跨域场景下面临身份分散、隐私泄露和权限需求多样化等挑战，需要一种新的机制来支持用户身份和资源的自主控制，满足跨域场景下的隐私保护认证和灵活授权需求。

Method: 提出POLARIS统一架构，包含结构化承诺机制实现细粒度策略身份披露，VPPL轻量级策略语言支持选择性属性揭示，以及会话级安全机制确保认证与访问的绑定。

Result: 实现原型系统并进行全面实验，证明POLARIS能够有效提供可扩展、保护隐私且可互操作的跨域访问控制，在去中心化跨域环境中具有实际可行性。

Conclusion: POLARIS为解决跨域访问控制挑战提供了一种有效的解决方案，能够在异构域之间实现安全且保护隐私的访问控制。

Abstract: Access control is a security mechanism designed to ensure that only authorized users can access specific resources. Cross-domain access control involves access to resources across different organizations, institutions, or applications. Traditional access control, however, which handles authentication and authorization separately in centralized environments, faces challenges in identity dispersion, privacy leakage, and diversified permission requirements, failing to adapt to cross-domain scenarios. Thus, there is an urgent need for a new access control mechanism that empowers autonomous control over user identity and resources, addressing the demands for privacy-preserving authentication and flexible authorization in cross-domain scenarios. To address cross-domain access control challenges, we propose POLARIS, a unified and extensible architecture that enables policy-based, verifiable and privacy-preserving access control across different domains. POLARIS features a structured commitment mechanism for reliable, fine-grained, policy-based identity disclosure. It further introduces VPPL, a lightweight policy language that supports issuer-bound evaluation of selectively revealed attributes. A dedicated session-level security mechanism ensures binding between authentication and access, enhancing confidentiality and resilience to replay attacks. We implement a working prototype and conduct comprehensive experiments, demonstrating that POLARIS effectively provides scalable, privacy-preserving, and interoperable access control across heterogeneous domains. Our results highlight the practical viability of POLARIS for enabling secure and privacy-preserving access control in decentralized, cross-domain environments.

</details>


### [38] [Evaluating the Robustness of Large Language Model Safety Guardrails Against Adversarial Attacks](https://arxiv.org/abs/2511.22047)
*Richard J. Young*

Main category: cs.CR

TL;DR: 该研究评估了10个公开的LLM安全护栏模型，发现虽然Qwen3Guard-8B总体准确率最高(85.3%)，但所有模型在面对未见过的攻击提示时性能大幅下降，其中Qwen3Guard从91.0%降至33.8%，揭示了基准测试可能因训练数据污染而产生误导性结果。


<details>
  <summary>Details</summary>
Motivation: LLM安全护栏模型作为防御有害内容生成的主要机制，但其对抗复杂对抗攻击的鲁棒性尚未得到充分评估。研究者旨在系统评估现有护栏模型的实际有效性，特别是它们对未见攻击的泛化能力。

Method: 研究评估了来自Meta、Google、IBM、NVIDIA、Alibaba和Allen AI的10个公开护栏模型，使用1,445个测试提示覆盖21个攻击类别。特别区分了公共基准提示和新型攻击，以评估模型的泛化能力。

Result: Qwen3Guard-8B总体准确率最高(85.3%)，但所有模型在未见提示上性能显著下降，Qwen3Guard从91.0%降至33.8%(57.2个百分点差距)。Granite-Guardian-3.2-5B泛化能力最佳(仅6.5%差距)。发现"帮助模式"越狱漏洞，两个模型反而生成有害内容。

Conclusion: 基准测试性能可能因训练数据污染而产生误导，泛化能力而非总体准确率应成为护栏评估的主要指标。研究发现现有护栏模型对未见攻击的防御能力存在严重不足，需要新的评估方法。

Abstract: Large Language Model (LLM) safety guardrail models have emerged as a primary defense mechanism against harmful content generation, yet their robustness against sophisticated adversarial attacks remains poorly characterized. This study evaluated ten publicly available guardrail models from Meta, Google, IBM, NVIDIA, Alibaba, and Allen AI across 1,445 test prompts spanning 21 attack categories. While Qwen3Guard-8B achieved the highest overall accuracy (85.3%, 95% CI: 83.4-87.1%), a critical finding emerged when separating public benchmark prompts from novel attacks: all models showed substantial performance degradation on unseen prompts, with Qwen3Guard dropping from 91.0% to 33.8% (a 57.2 percentage point gap). In contrast, Granite-Guardian-3.2-5B showed the best generalization with only a 6.5% gap. A "helpful mode" jailbreak was also discovered where two guardrail models (Nemotron-Safety-8B, Granite-Guardian-3.2-5B) generated harmful content instead of blocking it, representing a novel failure mode. These findings suggest that benchmark performance may be misleading due to training data contamination, and that generalization ability, not overall accuracy, should be the primary metric for guardrail evaluation.

</details>


### [39] [Privacy-preserving formal concept analysis: A homomorphic encryption-based concept construction](https://arxiv.org/abs/2511.22117)
*Qiangqiang Chen,Yunfeng Ke,Shen Li,Jinhai Li*

Main category: cs.CR

TL;DR: 提出了一种结合同态加密的隐私保护形式概念分析框架，用于在保护敏感数据隐私的同时进行大规模FCA计算


<details>
  <summary>Details</summary>
Motivation: 形式概念分析在知识提取和认知概念学习中广泛应用，但大规模数据集计算通常需要外包给外部计算服务，这带来了敏感信息泄露的风险。为了解决这一隐私安全问题，需要开发能够保护数据隐私的FCA计算方法。

Method: 提出隐私保护形式概念分析框架，结合二进制数据表示和同态加密技术。该方法能够在不解密的情况下对加密数据进行概念构造，确保计算过程中敏感数据不被泄露。

Result: 实验结果表明该方法在保护隐私的同时保持了计算性能。安全分析证实了该方法的有效性，能够在不泄露私有数据的情况下进行安全高效的概念构造。

Conclusion: 该研究对隐私保护数据挖掘和大规模FCA应用中的安全知识发现具有重要意义，为解决FCA计算中的隐私泄露问题提供了有效的技术方案。

Abstract: Formal Concept Analysis (FCA) is extensively used in knowledge extraction, cognitive concept learning, and data mining. However, its computational demands on large-scale datasets often require outsourcing to external computing services, raising concerns about the leakage of sensitive information. To address this challenge, we propose a novel approach to enhance data security and privacy in FCA-based computations. Specifically, we introduce a Privacy-preserving Formal Context Analysis (PFCA) framework that combines binary data representation with homomorphic encryption techniques. This method enables secure and efficient concept construction without revealing private data. Experimental results and security analysis confirm the effectiveness of our approach in preserving privacy while maintaining computational performance. These findings have important implications for privacy-preserving data mining and secure knowledge discovery in large-scale FCA applications.

</details>


### [40] [Personalized 3D Spatiotemporal Trajectory Privacy Protection with Differential and Distortion Geo-Perturbation](https://arxiv.org/abs/2511.22180)
*Minghui Min,Yulu Li,Gang Li,Meng Li,Hongliang Zhang,Miao Pan,Dusit Niyato,Zhu Han*

Main category: cs.CR

TL;DR: 本文提出了一种个性化的3D时空轨迹隐私保护机制3DSTPM，通过结合3D地理不可区分性和失真隐私，使用窗口自适应隐私预算分配来平衡隐私保护和服务质量。


<details>
  <summary>Details</summary>
Motivation: 随着3D领域位置服务（如智慧城市、智能交通）的快速发展，3D时空轨迹隐私保护问题日益突出。现有研究未能充分解决攻击者利用时空相关性的风险以及高度信息的影响，这些都可能导致严重的隐私泄露。

Method: 首先分析利用轨迹位置时空相关性的攻击者特征并建立攻击模型；然后结合3D地理不可区分性和失真隐私的特性，为所有可能位置找到保护位置集；针对连续轨迹查询导致的隐私累积问题，提出窗口自适应隐私预算分配方法，基于可预测性和敏感性动态分配隐私预算；最后使用PF机制对真实位置进行扰动。

Result: 仿真结果表明，所提出的3DSTPM在满足用户个性化隐私保护需求的同时，有效降低了服务质量损失。

Conclusion: 3DSTPM机制能够有效平衡隐私保护和服务质量，为3D时空轨迹隐私保护提供了有效的解决方案。

Abstract: The rapid advancement of location-based services (LBSs) in three-dimensional (3D) domains, such as smart cities and intelligent transportation, has raised concerns over 3D spatiotemporal trajectory privacy protection. However, existing research has not fully addressed the risk of attackers exploiting the spatiotemporal correlation of 3D spatiotemporal trajectories and the impact of height information, both of which can potentially lead to significant privacy leakage. To address these issues, this paper proposes a personalized 3D spatiotemporal trajectory privacy protection mechanism, named 3DSTPM. First, we analyze the characteristics of attackers that exploit spatiotemporal correlations between locations in a trajectory and present the attack model. Next, we exploit the complementary characteristics of 3D geo-indistinguishability (3D-GI) and distortion privacy to find a protection location set (PLS) that obscures the real location for all possible locations. To address the issue of privacy accumulation caused by continuous trajectory queries, we propose a Window-based Adaptive Privacy Budget Allocation (W-APBA), which dynamically allocates privacy budgets to all locations in the current PLS based on their predictability and sensitivity. Finally, we perturb the real location using the allocated privacy budget by the PF (Permute-and-Flip) mechanism, effectively balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that the proposed 3DSTPM effectively reduces QoS loss while meeting the user's personalized privacy protection needs.

</details>


### [41] [Keyless Entry: Breaking and Entering eMMC RPMB with EMFI](https://arxiv.org/abs/2511.22340)
*Aya Fukami,Richard Buurke*

Main category: cs.CR

TL;DR: 该论文针对主要制造商的三种eMMC中的RPMB认证方案，通过电磁脉冲注入故障，成功绕过认证并在两个目标eMMC中覆盖了RPMB数据。


<details>
  <summary>Details</summary>
Motivation: RPMB在现代存储系统中提供数据完整性保证的安全区域，用于存储关键信息。论文旨在评估其认证方案的安全性，特别是针对物理攻击的脆弱性。

Method: 针对三种不同eMMC芯片，通过向目标芯片发送电磁脉冲注入故障（glitch），攻击RPMB的认证机制。

Result: 成功绕过RPMB认证，在两个目标eMMC中覆盖了RPMB存储的任意数据，同时不影响其他数据的完整性。

Conclusion: RPMB认证方案存在物理攻击漏洞，电磁脉冲注入故障攻击可绕过其安全机制，威胁到关键数据的保护。

Abstract: The Replay Protected Memory Block (RPMB) in modern storage systems provides a secure area where data integrity is ensured by authentication. This block is used in digital devices to store pivotal information that must be safeguarded against modification by potential attackers. This paper targets the authentication scheme of the RPMB in three different eMMCs from a major manufacturer. A glitch was injected by sending an electromagnetic pulse to the target chip. RPMB authentication was successfully glitched and the information stored in two target eMMCs was overwritten with arbitrary data, without affecting the integrity of other data.

</details>


### [42] [Exposing Vulnerabilities in RL: A Novel Stealthy Backdoor Attack through Reward Poisoning](https://arxiv.org/abs/2511.22415)
*Bokang Zhang,Chaojun Lu,Jianhui Li,Junfeng Wu*

Main category: cs.CR

TL;DR: 论文研究针对强化学习系统的隐蔽后门攻击，通过毒化奖励信号来操纵智能体策略，在经典控制和MuJoCo环境中验证了攻击的有效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然在各领域取得显著成功，但其对奖励信号的依赖存在严重安全漏洞。本研究旨在揭示通过毒化奖励信号实施隐蔽后门攻击的威胁，强调部署RL系统的完整性面临的关键风险。

Method: 研究采用隐蔽后门攻击方法，通过毒化智能体的奖励信号来操纵其策略。在经典控制环境和MuJoCo环境中进行实验评估，包括Hopper和Walker2D等任务。

Result: 在非触发场景下，后门攻击保持高度隐蔽性：Hopper性能仅下降2.18%，Walker2D仅下降4.59%。在触发条件下攻击效果显著：Hopper性能下降达82.31%，Walker2D下降达71.27%。

Conclusion: 奖励信号毒化攻击对强化学习系统构成严重安全威胁，攻击具有高度隐蔽性和有效性。研究强调了针对训练时操纵的防御措施的紧迫性，以保障部署RL系统的完整性。

Abstract: Reinforcement learning (RL) has achieved remarkable success across diverse domains, enabling autonomous systems to learn and adapt to dynamic environments by optimizing a reward function. However, this reliance on reward signals creates a significant security vulnerability. In this paper, we study a stealthy backdoor attack that manipulates an agent's policy by poisoning its reward signals. The effectiveness of this attack highlights a critical threat to the integrity of deployed RL systems and calls for urgent defenses against training-time manipulation. We evaluate the attack across classic control and MuJoCo environments. The backdoored agent remains highly stealthy in Hopper and Walker2D, with minimal performance drops of only 2.18 % and 4.59 % under non-triggered scenarios, while achieving strong attack efficacy with up to 82.31% and 71.27% declines under trigger conditions.

</details>


### [43] [FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE](https://arxiv.org/abs/2511.22434)
*Wenbo Song,Xinxin Fan,Quanliang Jing,Shaoye Luo,Wenqi Wei,Chi Lin,Yunfeng Lu,Ling Liu*

Main category: cs.CR

TL;DR: FastFHE：一种基于RNS-CKKS全同态加密的高效CNN推理加速方案，通过新型数据打包、深度可分离卷积、BN融合和低阶多项式逼近等技术解决加密推理中的三大瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在加密环境下的安全推理和样本隐私保护已成为安全关键应用的紧迫问题。现有基于RNS-CKKS方案的方法存在高延迟问题，限制了实际应用。加密CNN推理面临三大瓶颈：卷积计算的时间和存储成本、巨大自举操作的时间开销、电路乘法深度的消耗。

Method: 提出FastFHE机制：1) 新型可扩展密文数据打包方案节省时间和存储；2) 深度可分离卷积降低计算负载；3) BN点积融合矩阵将密文卷积层与批归一化层合并而不增加乘法深度；4) 使用低阶Legendre多项式逼近SiLU非线性激活函数，保证微小精度误差。

Result: 通过多方面的实验验证了所提方法的效率和有效性，在保持高推理精度的同时显著加速了全同态加密下的模型推理。

Conclusion: FastFHE成功解决了加密CNN推理中的三大瓶颈问题，为安全关键应用提供了高效且准确的加密推理解决方案，推动了全同态加密在实际深度学习任务中的应用。

Abstract: The deep learning (DL) has been penetrating daily life in many domains, how to keep the DL model inference secure and sample privacy in an encrypted environment has become an urgent and increasingly important issue for various security-critical applications. To date, several approaches have been proposed based on the Residue Number System variant of the Cheon-Kim-Kim-Song (RNS-CKKS) scheme. However, they all suffer from high latency, which severely limits the applications in real-world tasks. Currently, the research on encrypted inference in deep CNNs confronts three main bottlenecks: i) the time and storage costs of convolution calculation; ii) the time overhead of huge bootstrapping operations; and iii) the consumption of circuit multiplication depth. Towards these three challenges, we in this paper propose an efficient and effective mechanism FastFHE to accelerate the model inference while simultaneously retaining high inference accuracy over fully homomorphic encryption. Concretely, our work elaborates four unique novelties. First, we propose a new scalable ciphertext data-packing scheme to save the time and storage consumptions. Second, we work out a depthwise-separable convolution fashion to degrade the computation load of convolution calculation. Third, we figure out a BN dot-product fusion matrix to merge the ciphertext convolutional layer with the batch-normalization layer without incurring extra multiplicative depth. Last but not least, we adopt the low-degree Legendre polynomial to approximate the nonlinear smooth activation function SiLU under the guarantee of tiny accuracy error before and after encrypted inference. Finally, we execute multi-facet experiments to verify the efficiency and effectiveness of our proposed approach.

</details>


### [44] [CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights](https://arxiv.org/abs/2511.22681)
*Mohaiminul Al Nahian,Abeer Matar A. Almalky,Gamana Aragonda,Ranyang Zhou,Sabbir Ahmed,Dmitry Ponomarev,Li Yang,Shaahin Angizi,Adnan Siraj Rakin*

Main category: cs.CR

TL;DR: CacheTrap是一种针对LLM的新型木马攻击，通过破坏KV缓存中的值向量实现，只需单比特翻转就能触发目标行为，且不影响模型正常功能。


<details>
  <summary>Details</summary>
Motivation: 随着对抗性权重扰动防御技术的发展，传统攻击方式容易被检测和防御。本文提出从新的攻击面——KV缓存入手，开发一种不留痕迹的瞬态攻击方法。

Method: 提出CacheTrap攻击方法，通过破坏KV缓存中的值向量，设计易受攻击的比特搜索算法，在无数据和梯度信息的情况下实现单比特翻转触发目标行为。

Result: 评估显示该攻击首次在LLM上实现单比特KV缓存翻转的成功木马攻击，攻击位置恒定可跨任务/数据集/查询转移，且不影响模型实用性。

Conclusion: KV缓存成为LLM安全的新攻击面，CacheTrap展示了数据无关、梯度无关的瞬态攻击可行性，对LLM安全防御提出了新挑战。

Abstract: Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead.

</details>


### [45] [Ghosting Your LLM: Without The Knowledge of Your Gradient and Data](https://arxiv.org/abs/2511.22700)
*Abeer Matar A. Almalky,Ziyan Wang,Mohaiminul Al Nahian,Li Yang,Adnan Siraj Rakin*

Main category: cs.CR

TL;DR: 提出了一种无需梯度计算和数据知识的比特翻转攻击方法，通过新颖的脆弱性指标识别LLM中的易受攻击权重比特，显著降低内存需求并提高攻击效率


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键应用中的广泛部署，确保其安全性和鲁棒性变得至关重要。现有比特翻转攻击依赖梯度计算，存在计算内存成本高且需要访问受害者数据集的问题，需要开发更高效实用的无梯度数据攻击方法

Method: 提出新颖的脆弱性指标，能够在不依赖梯度或数据知识的情况下识别大语言模型中的脆弱权重比特。通过消除梯度计算依赖，大幅降低内存需求，并以恒定复杂度跨多个任务扩展

Result: 实验结果表明该方法高效，仅需翻转单个比特即可对五个开源大语言模型实现对抗目标，显著优于现有依赖梯度的方法

Conclusion: 成功开发了一种高效实用的无梯度数据比特翻转攻击方法，通过脆弱性指标识别易受攻击权重比特，为LLM安全研究提供了新方向，并揭示了模型参数脆弱性的本质特征

Abstract: In recent years, large language models (LLMs) have achieved substantial advancements and are increasingly integrated into critical applications across various domains. This growing adoption underscores the need to ensure their security and robustness. In this work, we focus on the impact of Bit Flip Attacks (BFAs) on LLMs, which exploits hardware faults to corrupt model parameters, posing a significant threat to model integrity and performance. Existing studies on BFA against LLMs adopt a progressive bit-search strategy that predominantly relies on gradient-based techniques to identify sensitive layers or weights. However, computing gradients comes with two specific challenges: First, in the context of LLMs, it increases computational and memory costs exponentially, and Second, it requires access to a sample victim dataset or knowledge of the victim domain to compute the gradient. In this work, we investigate beyond the scope of attack efficacy and aim to develop an efficient, practical Gradient-Data-free Bit-Flip Attack. The challenge lies in the core principle of adversarial attacks, which relies heavily on computing gradients from sample test/train data and manipulating model weights based on gradient information. To overcome this, we propose novel vulnerability index metrics that can identify vulnerable weight bits in LLMs independent of any gradient or data knowledge. By removing the dependency on gradient computation, our approach drastically reduces memory requirements and scales efficiently across multiple tasks with constant complexity. Experimental results demonstrate the efficiency of our method, requiring as few as a single bit flip to achieve adversarial objectives for five open-source LLMs.

</details>


### [46] [Clustering Malware at Scale: A First Full-Benchmark Study](https://arxiv.org/abs/2511.23198)
*Martin Mocko,Jakub Ševcech,Daniela Chudá*

Main category: cs.CR

TL;DR: 该研究评估了恶意软件聚类质量，在Bodmas和Ember两个大型公共基准数据集上建立了最新技术水平，并首次将良性样本纳入聚类任务。


<details>
  <summary>Details</summary>
Motivation: 恶意软件攻击仍然频繁发生，但现有研究存在三个主要问题：1）恶意软件聚类很少纳入良性样本；2）尽管有大型公共基准数据集可用，但研究往往只使用小型数据集；3）当前恶意软件聚类的最先进解决方案尚不明确。

Method: 研究在Bodmas和Ember两个大型公共基准恶意软件数据集上进行恶意软件聚类评估，首次在整个数据集上进行聚类分析，并扩展任务将良性样本纳入聚类过程。

Result: 1）纳入良性样本不会显著降低聚类质量；2）不同数据集（Ember、Bodmas和私有行业数据集）的聚类质量存在显著差异；3）与普遍观点相反，K-Means和BIRCH表现最佳，而DBSCAN和HAC表现落后。

Conclusion: 该研究为恶意软件聚类建立了新的基准，表明在大型数据集上K-Means和BIRCH是最有效的聚类方法，且良性样本的纳入不会显著影响聚类性能，为恶意软件分析提供了重要指导。

Abstract: Recent years have shown that malware attacks still happen with high frequency. Malware experts seek to categorize and classify incoming samples to confirm their trustworthiness or prove their maliciousness. One of the ways in which groups of malware samples can be identified is through malware clustering. Despite the efforts of the community, malware clustering which incorporates benign samples has been under-explored. Moreover, despite the availability of larger public benchmark malware datasets, malware clustering studies have avoided fully utilizing these datasets in their experiments, often resorting to small datasets with only a few families. Additionally, the current state-of-the-art solutions for malware clustering remain unclear. In our study, we evaluate malware clustering quality and establish the state-of-the-art on Bodmas and Ember - two large public benchmark malware datasets. Ours is the first study of malware clustering performed on whole malware benchmark datasets. Additionally, we extend the malware clustering task by incorporating benign samples. Our results indicate that incorporating benign samples does not significantly degrade clustering quality. We find that there are significant differences in the quality of the created clusters between Ember and Bodmas, as well as a private industry dataset. Contrary to popular opinion, our top clustering performers are K-Means and BIRCH, with DBSCAN and HAC falling behind.

</details>


### [47] [One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT](https://arxiv.org/abs/2511.23252)
*Imraul Emmaka,Tran Viet Xuan Phuong*

Main category: cs.CR

TL;DR: Hyb-Agg是一个轻量级、通信高效的联邦学习安全聚合协议，结合了多密钥CKKS同态加密和ECDH掩码技术，将安全聚合简化为单轮非交互式传输，适用于物联网环境。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网环境中面临通信开销大的挑战，传统安全聚合协议需要多轮交互、大负载和较高的客户端成本，不适用于边缘部署。

Method: Hyb-Agg协议整合了多密钥CKKS同态加密和基于椭圆曲线Diffie-Hellman的加法掩码技术，将安全聚合过程简化为每轮仅需一次客户端到服务器的非交互式传输。

Result: 在包括树莓派4在内的设备上实现和评估，Hyb-Agg实现了亚秒级执行时间，通信扩展因子约为明文大小的12倍，且与参与者数量无关。

Conclusion: Hyb-Agg通过解决通信瓶颈，实现了可扩展的隐私保护联邦学习，适用于实际物联网部署。

Abstract: Federated Learning (FL) offers a promising approach to collaboratively train machine learning models without centralizing raw data, yet its scalability is often throttled by excessive communication overhead. This challenge is magnified in Internet of Things (IoT) environments, where devices face stringent bandwidth, latency, and energy constraints. Conventional secure aggregation protocols, while essential for protecting model updates, frequently require multiple interaction rounds, large payload sizes, and per-client costs rendering them impractical for many edge deployments.
  In this work, we present Hyb-Agg, a lightweight and communication-efficient secure aggregation protocol that integrates Multi-Key CKKS (MK-CKKS) homomorphic encryption with Elliptic Curve Diffie-Hellman (ECDH)-based additive masking. Hyb-Agg reduces the secure aggregation process to a single, non-interactive client-to-server transmission per round, ensuring that per-client communication remains constant regardless of the number of participants. This design eliminates partial decryption exchanges, preserves strong privacy under the RLWE, CDH, and random oracle assumptions, and maintains robustness against collusion by the server and up to $N-2$ clients.
  We implement and evaluate Hyb-Agg on both high-performance and resource-constrained devices, including a Raspberry Pi 4, demonstrating that it delivers sub-second execution times while achieving a constant communication expansion factor of approximately 12x over plaintext size. By directly addressing the communication bottleneck, Hyb-Agg enables scalable, privacy-preserving federated learning that is practical for real-world IoT deployments.

</details>


### [48] [FedSGT: Exact Federated Unlearning via Sequential Group-based Training](https://arxiv.org/abs/2511.23393)
*Bokang Zhang,Hong Guan,Hong kyu Lee,Ruixuan Liu,Jia Zou,Li Xiong*

Main category: cs.CR

TL;DR: FedSGT是一个联邦学习精确遗忘框架，通过数据分组和参数高效微调模块实现即时遗忘，减少重训练需求


<details>
  <summary>Details</summary>
Motivation: 联邦学习支持"被遗忘权"面临挑战，现有精确遗忘方法需要频繁从头重训练，导致高通信成本和长服务停机时间

Method: 将数据划分为均匀组，每个客户端可参与多个组；训练多个序列的参数高效微调模块，每个对应不同的组排列；遗忘时只需停用对应组的模块

Result: FedSGT在多个任务上显著延长了服务维护时间，同时保持与其他精确遗忘基线相当的学习性能和训练效率

Conclusion: FedSGT通过模块化设计和多序列训练，实现了高效的联邦学习精确遗忘，平衡了遗忘效率、模型效用和通信开销

Abstract: Federated Learning (FL) enables collaborative, privacy-preserving model training, but supporting the "Right to be Forgotten" is especially challenging because data influences the model through distributed and interleaved client updates. Existing exact unlearning methods typically require frequent retraining from scratch, resulting in high communication cost and long service downtime. To address this, we propose Federated Sequential Group-based Training (FedSGT), an exact unlearning framework for FL. FedSGT partitions the data into uniform groups, and each client may participate in multiple groups. To control communication overhead, each client can limit the number of groups it contributes to. FedSGT then trains multiple sequences of Parameter-Efficient Fine-Tuning (PEFT) modules, each corresponding to a different group permutation. Since the PEFT modules are lightweight and maintained server-side, FedSGT isolates the influence of different data groups into independent modules without incurring significant storage overhead and communication cost. Exact unlearning is thus achieved instantly by deactivating the modules corresponding to the group containing the unlearned data. Furthermore, using multiple training sequences helps maintain high model utility as deletion requests accumulate. We provide a rigorous theoretical analysis of both the deletion rate -- expected number of deletions before retraining is needed -- and the expected model performance. Experiments on various tasks demonstrate that FedSGT achieves a significantly longer service maintenance under multiple unlearning requests while maintaining comparable learning performance and training efficiency to other exact unlearning baselines. Extensive ablation studies validate the robustness of our method across a wide range of parameter settings.

</details>


### [49] [Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities](https://arxiv.org/abs/2511.23408)
*Aayush Garg,Zanis Ali Khan,Renzo Degiovanni,Qiang Tang*

Main category: cs.CR

TL;DR: 本研究评估了多种大语言模型（GPT、LLaMA、DeepSeek、Mistral等）在真实漏洞和人工漏洞上的修复效果，发现LLMs修复真实漏洞比人工漏洞更有效，且不同模型在修复重叠性和互补性方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 自动化漏洞修复对软件安全至关重要，现有研究主要评估LLMs在公开漏洞上的表现，但对其在相关人工漏洞上的有效性缺乏探索。本研究旨在通过实证评估填补这一空白。

Method: 使用真实漏洞和人工漏洞，评估多个主流LLMs（OpenAI GPT变体、LLaMA、DeepSeek、Mistral等）的修复效果。采用漏洞证明测试执行来具体评估LLM生成的源代码是否成功修复漏洞。

Result: LLMs修复真实漏洞比人工漏洞更有效。不同LLMs在修复重叠性（多个模型修复相同漏洞）和互补性（仅单个模型能修复的漏洞）方面存在显著差异，强调了选择合适的LLMs对有效漏洞修复的重要性。

Conclusion: LLMs在自动化漏洞修复方面具有潜力，但效果因漏洞类型和模型选择而异。需要根据具体需求选择合适的LLMs，并考虑不同模型之间的互补性以实现更全面的漏洞覆盖。

Abstract: Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [50] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 提出一个结合高性能集群计算、智能算法和区块链的新框架，通过改进的PoW共识机制、动态信任评级和统计抽签系统，实现高效、环保且包容的分布式计算生态。


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算和智能算法需要大量计算资源，导致高能耗，且排除了计算能力较弱的系统。需要一种更全面、可扩展且环保的方法来开发和实施智能算法。

Method: 提出一个融合高性能集群计算与智能算法的区块链框架，核心包括：1) 改进的PoW共识机制，将计算工作与区块奖励直接挂钩；2) 基于准确验证记录的动态信任评级系统；3) 统计抽签系统，为计算能力较弱的节点提供参与机会。

Result: 该框架实现了资源优化利用和广泛参与，创建了一个基于贡献质量的奖励系统，同时确保了不同计算能力节点的公平参与机会。

Conclusion: 提出的方法为可持续、高效且包容的智能算法开发和实施提供了可行解决方案，通过区块链技术整合高性能计算资源，平衡了效率、公平性和环保需求。

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [51] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 论文研究了异步消息传递模型(AMP_f)和Heard-Of模型(HO_f)在分布式计算中的等价关系，发现对于无色任务在n>2f时两者等价，对于有色任务仅在f=1时等价，f>1时存在分离。


<details>
  <summary>Details</summary>
Motivation: 重新审视两个基本分布式计算模型之间的关系：具有最多f个崩溃故障的异步消息传递模型(AMP_f)和具有最多f个消息遗漏的Heard-Of模型(HO_f)。理解这两个模型在任务可解性方面的等价性和差异。

Method: 通过双向模拟方法，在AMP_f和HO_f之间建立等价关系证明，引入一个中间模型来捕捉"静默进程"的概念。对于无色任务和有色任务分别进行分析，并扩展到针对非自适应对手的随机协议。

Result: 1. 对于n>2f，两个模型在无色任务的可解性方面是等价的；2. 对于有色任务，等价性仅在f=1（且n>2）时成立；3. f>1时的分离源于HO_f中静默进程的存在可能导致决策冲突；4. 结果扩展到随机协议，表明规范轮次的表达能力限制是结构性的而非概率性的。

Conclusion: 这些结果精确界定了基于轮次的抽象在何处能够捕捉异步计算，在何处不能。对于无色任务，HO_f模型可以充分模拟AMP_f，但对于有色任务（特别是f>1时），两个模型存在本质差异，揭示了分布式计算模型表达能力的结构性限制。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [52] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出了一种基于延迟约束的解耦架构，将集群资源分为延迟严格和延迟宽松两个池，通过瓶颈调度器和快速抢占机制，在保证在线服务SLO的同时，将离线任务吞吐量提升最高3倍。


<details>
  <summary>Details</summary>
Motivation: LLM服务中在线和离线工作负载共址部署可提高资源利用率，但在Prefill/Decode解耦系统中，请求混合的波动会导致严重的负载不均衡，现有动态调整技术无法应对在线服务的突发流量模式。

Method: 1) 基于任务延迟要求将集群资源分为延迟严格和延迟宽松两个池的架构设计；2) 基于Roofline性能模型的瓶颈调度器；3) 严格保证在线请求SLO的快速抢占机制。

Result: 在真实世界trace上的实验表明，相比现有离线系统方法，该方法在保持在线请求SLO的同时，将离线吞吐量提升最高3倍。

Conclusion: 提出的延迟约束解耦架构通过资源池分离、瓶颈调度和快速抢占机制，有效解决了Prefill/Decode解耦系统中的负载不均衡问题，在保证在线服务质量的同时显著提升了离线任务吞吐量。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [53] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: Clock2Q+是针对元数据缓存设计的缓存替换算法，通过在小FIFO队列中引入关联窗口来避免误判热块，在元数据跟踪中比S3-FIFO降低高达28.5%的缺失率。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存存在固有的关联引用特性，即使对应的数据访问不包含关联引用。这些关联引用会降低缓存替换算法的效果，因为它们经常被错误地归类为热块。

Method: Clock2Q+使用三个队列（类似S3-FIFO），但在小FIFO队列中引入了关联窗口，该窗口中的块不设置引用位。这种简单增强使其能够更好地区分真正的热块和关联引用。

Result: 相比表现第二好的算法S3-FIFO，Clock2Q+在元数据跟踪中实现了高达28.5%的缺失率降低。该算法还具有低CPU开销、低内存开销、多CPU扩展性好、易于调优和实现等特性。

Conclusion: Clock2Q+是针对元数据缓存优化的高效缓存替换算法，已在VMware by Broadcom的vSAN和VDFS存储产品中实现，不仅在元数据缓存中表现优异，在数据跟踪中也优于现有最先进的算法。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [54] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 该研究比较了在跨语言复制数据系统中集成复制数据库(RDL)的两种策略：外部函数接口(FFI)和通用数据格式(CDF)，发现CDF在软件质量、延迟、内存消耗和吞吐量方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要在多个执行站点复制数据，业务需求和资源约束常导致不同站点使用不同编程语言。复制数据库(RDL)通常只支持单一语言或特定绑定，在跨语言环境中集成现有RDL需要专用代码，但其软件质量和性能特征尚不明确。

Method: 通过实证研究比较两种RDL集成策略：外部函数接口(FFI)和通用数据格式(CDF)，测量并比较它们的软件指标和性能，评估它们对跨语言复制数据系统的适用性。

Result: 研究结果显示，采用CDF进行跨语言交互在软件质量、延迟、内存消耗和吞吐量方面具有优势。通过创建基于CDF的RDL支持编译型、解释型和托管语言混合使用，并通过插件扩展性增强RDL功能，验证了这些发现。

Conclusion: 随着现代分布式系统越来越多地使用多种编程语言，本研究为设计跨语言复制数据系统中的RDL提供了新的见解，表明CDF策略在软件质量和性能方面优于传统的FFI方法。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [55] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT是一种针对LLM解码的前缀感知注意力核实现，通过打包-前向-合并范式减少共享前缀的重复内存访问，显著提升解码注意力性能


<details>
  <summary>Details</summary>
Motivation: LLM服务中解码注意力成为主要瓶颈，现有实现无法充分利用请求间的层次化共享前缀（如系统提示、工具模板、RAG），导致重复加载KV缓存和内存带宽压力

Method: 采用打包-前向-合并范式：1) 按共享前缀打包查询减少内存访问；2) 定制多瓦片核实现高资源效率；3) 实用多流前向和KV分割减少资源气泡；4) 在线softmax合并

Result: 在真实和合成工作负载上，PAT平均减少注意力延迟67.4%，在相同配置下TPOT提升13.6-83.4%，优于现有最优注意力核

Conclusion: PAT通过有效利用请求间的共享前缀，显著优化了LLM解码注意力性能，可作为vLLM的即插即用插件，缓解内存带宽瓶颈

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [56] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer是一个统一的系统级加速框架，通过专家放置优化、缓存压缩和调度来最大化LLM服务效率，在DeepSeek-R1上实现616 QPM，TPOT减少36%，TTFT降低38%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在计算密集、延迟严格和吞吐瓶颈方面给大规模服务系统带来重大挑战，需要系统级优化来提升端到端服务效率。

Method: OmniInfer包含三个互补组件：OmniPlacement用于负载感知的专家混合调度，OmniAttn用于稀疏注意力加速，OmniProxy用于解耦感知的请求调度，基于vLLM构建，通过自适应资源解耦、高效稀疏利用和全局协调实现系统级优化。

Result: 在10节点Ascend 910C集群上评估DeepSeek-R1，OmniInfer达到616 QPM，统一框架减少TPOT 36%，OmniProxy叠加进一步降低TTFT 38%。

Conclusion: OmniInfer通过系统级优化显著提升LLM服务效率，项目已开源，为大规模语言模型服务提供有效的加速解决方案。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [57] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: RT-MMC：利用GPU光线追踪核心硬件加速的蒙特卡洛光传输模拟算法，相比传统软件光线追踪MMC实现1.5-4.5倍加速，简化工作流程


<details>
  <summary>Details</summary>
Motivation: 传统网格蒙特卡洛（MMC）方法在GPU上仍有性能瓶颈，主要受限于频繁的光线-边界相交测试计算成本。现代GPU的光线追踪核心（RT-cores）提供了硬件加速的光线遍历和相交能力，但尚未在生物光子学模拟中得到充分利用。

Method: 基于NVIDIA OptiX平台实现RT-MMC算法，将图形学光线追踪管线扩展到浑浊介质中的体积光线追踪。该方法无需复杂的四面体网格生成，直接利用硬件加速的光线追踪能力，并天然支持宽场光源而无需复杂的网格重划分。

Result: RT-MMC与传统软件光线追踪MMC算法结果高度一致，同时在多种GPU架构上实现1.5倍到4.5倍的加速。性能提升显著增强了MMC在实际模拟中的实用性。

Conclusion: 从软件光线追踪迁移到硬件光线追踪不仅极大简化了MMC模拟工作流程，还带来了显著的性能提升。随着光线追踪硬件的快速普及，这种性能优势预计将进一步增强。在定量MMC模拟中采用图形学光线追踪管线能够充分利用新兴硬件资源，惠及广泛的生物光子学应用。

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [58] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe是一个针对LoRA适配器服务中rank多样性问题的动态适配器放置和路由框架，通过动态重新平衡适配器分布和利用GPU Direct RDMA远程访问，显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA适配器服务系统在处理多租户环境中不同rank（大小）的适配器时存在性能偏差问题，导致GPU资源利用率低下，需要更多GPU来满足服务级别目标。

Method: 提出LoRAServe框架，采用工作负载感知的动态适配器放置和路由策略，通过动态重新平衡GPU间的适配器分布，并利用GPU Direct RDMA实现远程访问。

Result: 在真实生产环境测试中，相比现有最优系统，LoRAServe实现了最高2倍的吞吐量提升，最高9倍的首令牌时间降低，并在满足SLO约束下减少高达50%的GPU使用量。

Conclusion: LoRAServe通过有效处理LoRA适配器服务中的rank多样性问题，显著提升了系统性能和资源利用率，为大规模多租户LLM服务提供了高效解决方案。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [CADC: Crossbar-Aware Dendritic Convolution for Efficient In-memory Computing](https://arxiv.org/abs/2511.22166)
*Shuai Dong,Junyi Yang,Ye Ke,Hongyang Shang,Arindam Basu*

Main category: cs.AR

TL;DR: CADC是一种基于树突计算原理的新型卷积方法，通过在交叉阵列计算中嵌入非线性树突函数（将负值归零），显著增加了部分和的稀疏性，从而减少系统开销并提升能效。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在基于交叉阵列的内存计算架构中加速时，大型卷积层需要跨多个交叉阵列分区，产生大量部分和，这些部分和需要额外的缓冲、传输和累加操作，引入了显著的系统级开销。

Method: 提出交叉阵列感知的树突卷积（CADC），受神经科学中树突计算原理启发，在交叉阵列计算中直接嵌入非线性树突函数（将负值归零），从而大幅增加部分和的稀疏性。

Result: CADC显著减少了部分和数量：LeNet-5减少80%，ResNet-18减少54%，VGG-16减少66%，SNN减少高达88%。稀疏性带来两个关键优势：1）实现零压缩和零跳过，减少29.3%的缓冲和传输开销以及47.9%的累加开销；2）最小化ADC量化噪声积累，精度损失很小。SRAM-based IMC实现达到2.15 TOPS和40.8 TOPS/W的能效。

Conclusion: CADC通过引入树突计算原理，有效解决了交叉阵列内存计算架构中部分和过多导致的系统开销问题，在保持高精度的同时显著提升了计算效率和能效，相比现有IMC加速器实现了11-18倍加速和1.9-22.9倍能效提升。

Abstract: Convolutional neural networks (CNNs) are computationally intensive and often accelerated using crossbar-based in-memory computing (IMC) architectures. However, large convolutional layers must be partitioned across multiple crossbars, generating numerous partial sums (psums) that require additional buffer, transfer, and accumulation, thus introducing significant system-level overhead. Inspired by dendritic computing principles from neuroscience, we propose crossbar-aware dendritic convolution (CADC), a novel approach that dramatically increases sparsity in psums by embedding a nonlinear dendritic function (zeroing negative values) directly within crossbar computations. Experimental results demonstrate that CADC significantly reduces psums, eliminating 80% in LeNet-5 on MNIST, 54% in ResNet-18 on CIFAR-10, 66% in VGG-16 on CIFAR-100, and up to 88% in spiking neural networks (SNN) on the DVS Gesture dataset. The induced sparsity from CADC provides two key benefits: (1) enabling zero-compression and zero-skipping, thus reducing buffer and transfer overhead by 29.3% and accumulation overhead by 47.9%; (2) minimizing ADC quantization noise accumulation, resulting in small accuracy degradation - only 0.01% for LeNet-5, 0.1% for ResNet-18, 0.5% for VGG-16, and 0.9% for SNN. Compared to vanilla convolution (vConv), CADC exhibits accuracy changes ranging from +0.11% to +0.19% for LeNet-5, -0.04% to -0.27% for ResNet-18, +0.99% to +1.60% for VGG-16, and -0.57% to +1.32% for SNN, across crossbar sizes from 64x64 to 256x256. Ultimately, a SRAM-based IMC implementation of CADC achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 (4/2/4b), realizing an 11x-18x speedup and 1.9x-22.9x improvement in energy efficiency compared to existing IMC accelerators.

</details>


### [60] [Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR](https://arxiv.org/abs/2511.22267)
*Yuyang Zou,Youwei Xiao,Yansong Xu,Chenyun Yin,Yuhao Luo,Yitian Sun,Ruifan Xu,Renze Chen,Yun Liang*

Main category: cs.AR

TL;DR: Aquas是一个基于MLIR的硬件-软件协同设计框架，针对RISC-V ASIPs，通过突发DMA引擎和HLS优化提升硬件性能，同时采用基于e-graph的可重定向编译器方法，在点云处理和LLM推理等实际工作负载上实现最高9.27倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前开源RISC-V生态系统中的ASIP设计框架存在性能限制，主要由于硬件合成能力受限和编译器支持僵化，需要一种更高效的硬件-软件协同设计解决方案。

Method: 提出Aquas框架：1) 硬件方面：通过突发DMA引擎实现快速内存访问，结合高级HLS优化；2) 编译器方面：采用基于e-graph的可重定向方法，配备新颖的指令匹配引擎。

Result: 在点云处理和LLM推理等实际工作负载上，Aquas框架实现了最高9.27倍的性能加速。

Conclusion: Aquas框架通过硬件-软件协同设计有效解决了RISC-V ASIPs的性能瓶颈问题，为特定应用处理器设计提供了高效的解决方案。

Abstract: Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.

</details>


### [61] [FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators](https://arxiv.org/abs/2511.22348)
*Shuao Jia,Zichao Ling,Chen Bai,Kang Zhao,Jianwang Zhai*

Main category: cs.AR

TL;DR: FADiff是一个基于梯度的优化框架，通过统一可微的成本模型自动寻找最优的层内映射和层间融合策略，以加速DNN推理


<details>
  <summary>Details</summary>
Motivation: 在张量加速器上高效部署DNN（如LLM）对最大化计算效率至关重要，但由于层内映射和层间融合相互作用形成的巨大复杂设计空间，实现这一目标具有挑战性

Method: 构建统一可微的分析成本模型，准确预测单层映射和各种层融合策略的能耗和延迟；通过将离散约束编码到损失函数中，采用基于梯度的方法高效探索设计空间，确定映射和融合的最优联合策略

Result: 实验结果表明FADiff优于现有方法，在能耗和延迟方面实现了更好的优化效果

Conclusion: FADiff框架能够自动识别高质量的层内映射和层间融合策略，有效加速DNN工作负载的推理过程

Abstract: Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.

</details>


### [62] [3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison](https://arxiv.org/abs/2511.22551)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asad*

Main category: cs.AR

TL;DR: 本文提出了一种名为3RSeT的低成本方案，通过选择性标签比较来减少STT-MRAM缓存中的读取干扰错误率，将标签阵列的读取干扰率降低71.8%，MTTF提高3.6倍，能耗降低62.1%，且性能不受影响。


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为片上缓存存储器最有前景的SRAM替代技术，虽然具有低泄漏功耗、高密度、抗辐射和非易失性等优点，但在读取操作中存在的无意位翻转（读取干扰错误）是严重的可靠性挑战。特别是缓存集合中所有标签同时访问进行并行比较操作，是读取干扰错误的主要来源，这一问题在先前工作中未得到解决。

Method: 提出3RSeT（通过选择性标签比较减少STT-MRAM缓存读取干扰率）方案，通过主动禁用那些没有命中机会的标签来消除大部分标签读取操作。该方法利用每个访问请求中标签的低有效位，在访问时预先判断哪些标签不可能命中，从而避免不必要的标签读取。

Result: 使用gem5全系统周期精确模拟器评估显示：3RSeT将标签阵列的读取干扰率降低了71.8%，使平均故障时间（MTTF）提高了3.6倍。同时能耗降低了62.1%，性能未受影响，面积开销小于0.4%。

Conclusion: 3RSeT方案有效解决了STT-MRAM缓存中读取干扰错误的问题，通过选择性标签比较显著降低了错误率，同时改善了能耗效率，具有低成本和高实用性的特点。

Abstract: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

</details>


### [63] [The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference](https://arxiv.org/abs/2511.22889)
*Fang Li*

Main category: cs.AR

TL;DR: ITA架构将LLM权重编码到ASIC物理电路中，消除内存层次结构，解决边缘设备部署中的"内存墙"问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在消费边缘设备上的部署受到"内存墙"的限制——每次生成token都需要从DRAM获取数十亿模型权重，带来巨大的带宽和能耗成本。现有架构将模型权重视为可变软件数据，为保持通用可编程性付出了巨大的能量代价。

Method: 提出不可变张量架构(ITA)，将模型权重视为物理电路拓扑而非数据。通过将参数直接编码到成熟节点ASIC（28nm/40nm）的金属互连和逻辑中，完全消除内存层次结构。采用"分脑"系统设计，主机CPU管理动态KV缓存操作，ITA ASIC作为无状态的ROM嵌入式数据流引擎。

Result: 该方法从根本上解决了内存带宽瓶颈，大幅降低了能耗，使LLM能够在资源受限的边缘设备上高效部署。

Conclusion: ITA架构通过将模型权重物理化到ASIC电路中，实现了从软件数据到硬件拓扑的范式转变，为边缘AI部署提供了突破性的解决方案。

Abstract: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

</details>


### [64] [GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration](https://arxiv.org/abs/2511.23203)
*Jordi Fornt,Pau Fontova-Musté,Adrian Gras,Omar Lahyani,Martí Caro,Jaume Abella,Francesc Moll,Josep Altet*

Main category: cs.AR

TL;DR: 提出GAV技术，结合欠压和位串行计算，实现灵活近似计算，并设计GAVINA架构支持任意混合精度和欠压，能效最高达89 TOP/sW


<details>
  <summary>Details</summary>
Motivation: 电压过缩放（欠压）是一种有吸引力的近似技术，但由于其高错误率阻碍了广泛应用。现有欠压加速器依赖8位算术，无法与先进低精度（<8位）架构竞争。

Method: 提出GAV技术，结合欠压和位串行计算，在选定的最低有效位组合上激进降低供电电压。基于此实现GAVINA架构，支持任意混合精度和灵活欠压。

Result: GAVINA架构在最激进配置下能效高达89 TOP/sW。通过开发GAVINA错误模型，显示GAV可通过欠压实现20%能效提升，在ResNet-18上精度损失可忽略。

Conclusion: GAV技术成功解决了传统欠压技术的高错误率问题，结合位串行计算实现了灵活近似方法，为低精度DNN加速提供了高效解决方案。

Abstract: Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.

</details>
