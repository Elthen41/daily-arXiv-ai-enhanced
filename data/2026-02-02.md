<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF框架通过让评判智能体对多个查询-响应对进行联合推理，而非孤立评估，实现从局部评估到整体学习的转变，利用信念传播和集成学习原理提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有评判智能体通常孤立评估每个查询-响应，无法利用跨实例的模式和不一致性信息，限制了评估质量和智能体自我改进的能力。

Method: 提出JAF框架，让评判智能体对相关查询-响应对进行联合推理；开发灵活的局部敏感哈希算法，整合语义嵌入、LLM驱动的哈希谓词、分类标签监督和侧信息，生成信息丰富的二进制编码，支持高效、可解释、关系感知的示例选择。

Result: 在大型云环境中的云配置错误分类这一具有挑战性的任务上进行了实证验证，展示了JAF框架的有效性。

Conclusion: JAF将评判智能体从局部评估者提升为整体学习者，通过联合推理和信念传播机制，显著提升了智能体评估质量和自我改进能力。

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [2] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 使用Gemini AI系统评估Bloom的Erdős问题数据库中700个标记为"开放"的猜想，通过混合方法（AI自然语言验证+人工专家评估）解决了13个问题，发现这些问题的"开放"状态更多是由于文献难以查找而非问题本身困难。


<details>
  <summary>Details</summary>
Motivation: 探索AI在半自主数学发现中的应用潜力，特别是评估Erdős问题数据库中标记为"开放"的猜想，了解AI在数学研究中的实际效果和局限性。

Method: 采用混合方法：首先使用Gemini AI进行自然语言验证以缩小搜索范围，然后由人类专家评估正确性和新颖性。系统性地评估了700个标记为"开放"的Erdős猜想。

Result: 解决了13个标记为"开放"的问题：其中5个通过看似新颖的自主解决方案，8个通过识别现有文献中的先前解决方案。发现这些问题的"开放"状态更多是由于文献难以查找（obscurity）而非问题本身困难。

Conclusion: AI在数学猜想评估中具有应用潜力，但面临文献识别困难和"潜意识抄袭"风险。Erdős问题的"开放"状态往往反映文献可及性问题而非数学难度。AI辅助数学研究需要谨慎处理文献归属和原创性判断。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [3] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 该研究评估了传统机器学习与深度学习模型在垃圾图像二分类中的性能，发现迁移学习模型DenseNet121表现最佳（准确率91%），并探讨了PCA对传统模型的影响以及模型在实际垃圾分拣系统中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 高效垃圾分拣对于智慧城市实现循环经济和资源回收至关重要。传统垃圾分拣方法效率低下，需要自动化解决方案来减少填埋使用和环境影响。

Method: 使用25,077张垃圾图像（80/20训练/测试分割，增强并调整至150x150像素），评估了传统机器学习模型（随机森林、SVM、AdaBoost）和深度学习模型（自定义CNN、VGG16、ResNet50）以及三种迁移学习模型（DenseNet121、EfficientNetB0、InceptionV3）。同时评估了主成分分析对传统模型的降维效果。

Result: DenseNet121取得了最高准确率（91%）和ROC-AUC（0.98），比最佳传统分类器高出20个百分点。PCA对传统方法改善有限，而迁移学习在有限数据条件下显著提升了性能。

Conclusion: 迁移学习模型在垃圾图像分类中表现优异，特别是DenseNet121。这些模型可以集成到实时数据驱动的决策支持系统中，实现自动化垃圾分拣，有望减少填埋使用和生命周期环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [4] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: B-PAC推理：一种在线环境下安全高效的动态路由方法，通过统计证据控制性能损失，显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 大型推理模型计算成本高、延迟大，现有选择性思考策略在在线环境中存在不可控错误，且非思考模型的性能损失只能部分观测，数据非平稳

Method: 提出B-PAC推理方法，使用逆倾向评分估计器构建候选阈值的测试超鞅，基于累积统计证据动态调整路由阈值

Result: B-PAC推理显著降低计算开销，思考模型使用率最多减少81.01%，同时将性能损失控制在用户指定水平以下

Conclusion: B-PAC推理为在线环境下的安全高效推理提供了理论保证和实用方法，实现了计算效率与性能损失的平衡控制

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [5] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 本文提出了一种新的内在动机原则——可控信息生产（CIP），它避免了外部效用和设计者指定变量，通过开环与闭环Kolmogorov-Sinai熵的差距来同时奖励对混沌的追求和调节。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息传输的内在动机方法主要依赖设计者选择参与传输的随机变量，存在设计依赖性。本文旨在开发一种既不需要外部效用，也不需要设计者指定变量的内在动机原则。

Method: 从最优控制理论推导出可控信息生产（CIP）目标，将其表示为开环与闭环Kolmogorov-Sinai熵之间的差距，这种方法同时奖励对混沌的追求和调节。

Result: 建立了CIP的关键理论性质，并在标准内在动机基准测试中证明了其有效性，展示了该方法在生成智能行为方面的能力。

Conclusion: CIP提供了一种新颖的内在动机框架，避免了传统方法的设计依赖性，通过连接外在和内在行为，为智能行为生成提供了新的理论基础。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [6] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 该论文首次为自奖励语言模型（SRLMs）提供了严格的理论保证，分析了其迭代改进机制的理论基础。


<details>
  <summary>Details</summary>
Motivation: 自奖励语言模型在无需外部反馈的情况下通过迭代改进取得了显著成功，但其核心机制缺乏理论解释，存在理论理解上的关键空白。

Method: 1. 建立单步更新的下界，揭示对初始模型质量的依赖；2. 推导完整迭代范式的有限样本误差界；3. 在线性softmax模型类上实例化理论框架。

Result: 1. 性能随样本量n以$\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$速率改进；2. 对初始模型的依赖随迭代次数T指数衰减；3. 为自奖励成功提供了形式化解释：通过引导动态趋向内部稳定性和一致性来克服不良初始化。

Conclusion: 该研究首次为自奖励语言模型提供了严格的理论保证，揭示了其成功的内在机制，并通过理论框架与具体模型架构的连接，为理解和改进这类模型奠定了理论基础。

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [7] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 本文提出达尔文记忆系统(DMS)，一种自演化的记忆架构，通过"适者生存"法则解决MLLM智能体在GUI自动化中长时跨应用任务面临的上下文窗口限制、粒度不匹配和上下文污染问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLM)智能体在GUI自动化中面临长时跨应用任务的挑战，主要受限于有限的上下文窗口。现有记忆系统难以适应动态GUI环境，存在高层意图与低层执行的粒度不匹配问题，以及静态积累过时经验导致上下文污染和幻觉。

Method: 提出达尔文记忆系统(DMS)，将记忆构建为受"适者生存"法则支配的动态生态系统。系统将复杂轨迹分解为独立可重用单元以实现组合灵活性，并实施效用驱动的自然选择机制来追踪生存价值，主动修剪次优路径并抑制高风险计划。

Result: 在真实世界多应用基准测试中，DMS无需训练成本或架构开销即可提升通用MLLM性能，平均成功率提升18.0%，执行稳定性提升33.9%，同时降低任务延迟。

Conclusion: DMS是一种有效的自演化记忆系统，通过进化压力驱动智能体推导出更优策略，成功解决了GUI任务中现有记忆系统的局限性。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [8] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 论文提出CraEG方法，通过几何引导重加权缓解嵌入空间拥挤现象，提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 现有基于温度和截断的采样方法只关注token概率，忽略了嵌入空间中token之间的细粒度几何关系。研究发现嵌入空间拥挤现象（embedding-space crowding）与数学问题解决中的推理成功存在统计关联。

Method: 提出CraEG方法，这是一种即插即用的采样方法，通过几何引导的重加权来缓解嵌入空间拥挤现象。该方法无需训练、单次通过，且与标准采样策略兼容。

Result: 在多个模型和基准测试上的实验表明，CraEG提高了生成性能，在鲁棒性和多样性指标上都有所改善。

Conclusion: 嵌入空间拥挤是影响LLM推理性能的重要因素，通过几何引导的采样方法可以有效缓解这一问题，提升模型在复杂推理任务中的表现。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [9] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 将主动学习引入RLVR，提出不确定性一致性指标，在仅用30%数据的情况下达到全数据集性能，显著降低标注成本


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法需要大量查询预算，标注成本高昂。研究是否可以通过更少但信息量更大的查询获得相似或更好的性能，将主动学习引入RLVR框架。

Method: 1) 提出不确定性一致性指标评估主观不确定性与客观不确定性对齐程度；2) 离线场景使用点双列相关系数；3) 在线训练引入新变体，基于归一化优势和主观不确定性计算；4) 理论上证明在线变体与离线PBC严格负相关，支持更好的样本选择。

Result: 实验表明该方法始终优于随机选择和经典主动学习基线，在仅使用30%数据训练的情况下达到全数据集性能，有效降低了推理任务中RLVR的成本。

Conclusion: 通过引入主动学习和不确定性一致性指标，显著减少了RLVR所需的查询数量，在保持性能的同时大幅降低了标注成本，为大规模语言模型的数学推理任务提供了更高效的训练方法。

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [10] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut：一种无需训练的动态截断方法，利用早期推理步骤中的输出分布熵来区分正确与错误推理，从而在保持准确性的同时大幅减少计算成本


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过长链思维生成在复杂推理任务中表现出色，但冗长的中间步骤带来了巨大的计算成本。研究发现模型早期推理步骤的输出分布熵能可靠地区分正确与错误推理

Method: 提出EntroCut方法，这是一种无需训练的动态截断技术，通过识别高置信度状态来安全终止推理。基于早期推理步骤中输出分布熵的观察，当模型达到高置信度时提前截断推理过程

Result: 在四个基准测试上的实验表明，EntroCut能减少高达40%的token使用量，同时保持最小准确率损失。相比现有无需训练方法，实现了更优的效率-性能权衡

Conclusion: 熵引导的动态截断为缓解大型推理模型的低效问题提供了实用方法，在保持推理质量的同时显著提升计算效率

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [11] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: SYMPHONY是一个新颖的多智能体规划框架，通过集成异构语言模型智能体池来增强蒙特卡洛树搜索中的探索多样性，相比单智能体框架在多个基准任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用单智能体框架进行蒙特卡洛树搜索规划，这种范式限制了探索能力，导致生成分支多样性不足和规划性能欠佳。

Method: 提出SYMPHONY框架，集成异构语言模型智能体池，利用不同智能体间的多样化推理模式来增强rollout多样性和探索效果。

Result: 在多个基准任务上，SYMPHONY即使使用可在消费级硬件上部署的开源LLMs也能实现强劲性能；当增强使用基于云的API访问LLMs时，性能进一步提升，超越了现有最先进的基线方法。

Conclusion: 异构多智能体协调在规划任务中具有显著有效性，SYMPHONY框架通过多智能体协同增强了蒙特卡洛树搜索的探索能力，为复杂问题解决提供了新思路。

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [12] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 提出SABER方法，通过Beta分布建模样本级成功概率，推导出解析缩放定律，仅用100个样本就能准确预测1000次采样下的攻击成功率，显著降低评估误差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估通常基于单次或低预算对抗提示，低估了实际风险。攻击者可以利用大规模并行采样反复探测模型直到产生有害响应，需要更准确的大规模对抗风险评估方法。

Method: 提出SABER方法：1) 使用Beta分布（伯努利分布的共轭先验）建模样本级成功概率；2) 推导解析缩放定律；3) 通过小预算测量可靠地外推大规模攻击成功率。

Result: 仅使用n=100个样本，SABER预测ASR@1000的平均绝对误差为1.66，相比基线12.04减少了86.2%的估计误差。揭示了异质性风险缩放特征，显示标准评估下看似稳健的模型在并行对抗压力下可能经历快速非线性风险放大。

Conclusion: SABER提供了一种低成本、可扩展的LLM安全评估方法，能够更真实地评估大规模对抗攻击下的风险，为实际安全评估提供有效工具。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [13] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 论文指出生成式医疗AI虽然看似流畅，但临床推理不是文本生成，需要临床情境智能(CCI)来确保安全部署。作者开发了Meddollina系统，通过治理优先的设计约束推理过程，在16,412+医疗查询测试中表现出更符合临床需求的行为特征。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗AI虽然看起来流畅且知识丰富，但临床推理与文本生成有本质区别。生成中心系统存在过早结论、不合理确定性、意图漂移和多步决策不稳定等问题，这些结构性缺陷使其不适合临床部署。需要一种新的临床情境智能(CCI)能力类别来确保医疗AI的安全应用。

Method: 提出临床情境智能(CCI)作为新的能力类别，要求具备持续情境意识、意图保持、有界推理和证据不足时的原则性延迟。开发了Meddollina系统，采用治理优先的设计理念，在语言实现前约束推理过程，优先考虑临床适宜性而非生成完整性。系统作为连续智能层支持临床工作流程，同时保持临床医生的权威性。

Result: 在16,412+异构医疗查询的评估中，Meddollina展现出独特的行为特征：校准的不确定性、在信息不足时的保守推理、稳定的纵向约束遵守，以及相对于生成中心基线的减少推测性完成。这些结果表明可部署的医疗AI不能仅通过扩展获得，需要向连续临床智能转变。

Conclusion: 可部署的医疗AI不会仅通过扩展生成模型而出现，需要转向连续临床智能范式。进展应该通过临床医生对齐的行为（在不确定性下的表现）来衡量，而不是基于流畅度的完成度。Meddollina展示了治理优先设计如何产生更符合临床需求的行为特征。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [14] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: TMoW框架通过测试时更新世界模型的路由函数，增强具身智能体在动态环境中的适应能力，支持零样本适应和少样本扩展。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的具身智能体在动态环境中的适应能力有限，需要构建准确灵活的世界模型来支持有效推理和决策。

Method: 提出TMoW框架：1）多粒度原型路由，根据对象到场景级相似性调整混合；2）测试时细化，在推理过程中对齐未见域特征与原型；3）蒸馏混合增强，从少量数据和现有原型高效构建新模型。

Result: 在VirtualHome、ALFWorld和RLBench基准测试中表现出色，在零样本适应和少样本扩展场景中均有强大性能，使具身智能体能在动态环境中有效运行。

Conclusion: TMoW通过测试时更新世界模型路由函数，增强了具身智能体在动态环境中的适应能力，为持续适应提供了有效框架。

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [15] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 本文提出UCPO框架，通过三元优势解耦和动态不确定性奖励调整，解决现有RL范式中的优势偏差问题，提升LLM的不确定性表达能力。


<details>
  <summary>Details</summary>
Motivation: 构建可信赖的大型语言模型需要赋予其内在的不确定性表达能力，以缓解限制其高风险应用的幻觉问题。现有RL范式如GRPO常因二元决策空间和静态不确定性奖励而遭受优势偏差，导致过度保守或过度自信。

Method: 提出UnCertainty-Aware Policy Optimization (UCPO)框架：1) 采用三元优势解耦，分离并独立归一化确定性和不确定性rollouts以消除优势偏差；2) 引入动态不确定性奖励调整机制，根据模型演化和实例难度实时校准不确定性权重。

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在其知识边界之外的可靠性和校准能力。

Conclusion: UCPO框架通过解决现有RL范式中的奖励黑客攻击和过度自信问题，为构建具有内在不确定性表达能力的大型语言模型提供了有效解决方案。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [16] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC是一个任务自适应决策框架，通过整合LLM委员会和蒙特卡洛树搜索，实现动态专家选择和高效多步规划，提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽视不同LLM的专业化差异，将所有模型视为统一适用，限制了它们适应不同推理需求和任务复杂度的能力。

Method: 提出任务感知LLM委员会(TALC)，整合LLM委员会和蒙特卡洛树搜索，每个LLM配备结构化成功记忆档案，通过语义匹配将当前推理上下文与历史成功经验对齐，采用双信号机制融合模型评估和历史效用分数，自适应权重引导MCTS选择。

Result: 在WebShop、HumanEval和24点游戏上的实验表明，TALC相比强基线实现了更高的任务成功率和改进的搜索效率。

Conclusion: TALC验证了专业化感知路由和自适应规划的优势，为LLM决策系统提供了更有效的任务适应性框架。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [17] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: R2M是一个新颖的轻量级RLHF框架，通过利用策略模型的实时隐藏状态（策略反馈）来对齐策略分布变化，解决奖励过优化问题。


<details>
  <summary>Details</summary>
Motivation: RLHF技术虽然重要，但存在奖励过优化问题，即策略模型过度拟合奖励模型，利用虚假奖励模式而非真正捕捉人类意图。现有方法主要依赖表面语义信息，无法有效处理因连续策略分布变化导致的奖励模型与策略模型之间的错位，这会加剧奖励过优化。

Method: 提出R2M框架，超越仅依赖预训练LLM语义表示的普通奖励模型。R2M利用策略模型在强化学习过程中的演化隐藏状态（即策略反馈），与策略的实时分布变化进行对齐。

Result: 该方法为通过实时利用策略模型反馈来改进奖励模型性能提供了一个有前景的新方向。

Conclusion: R2M通过整合策略反馈来实时对齐策略分布变化，为解决RLHF中的奖励过优化问题提供了有效的解决方案，代表了奖励模型改进的重要进展。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [18] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 该论文提出MinPRO方法，通过最小前缀比率来稳定大语言模型在离策略强化学习训练中的优化过程，解决传统token级重要性采样在离策略程度大时的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习后训练中，通常使用较旧的采样策略生成rollout数据来更新当前目标策略，这种离策略训练需要重要性采样校正。现有方法主要使用token级重要性采样比率，但在离策略程度较大时会导致训练不稳定。

Method: 提出MinPRO（最小前缀比率）目标函数，用基于前缀中观察到的最小token级比率的非累积替代项，替代不稳定的累积前缀比率，从而稳定大语言模型在离策略漂移较大时的优化过程。

Result: 在密集型和混合专家大语言模型上，在多个数学推理基准测试中进行的广泛实验表明，MinPRO显著提高了离策略机制下的训练稳定性和峰值性能。

Conclusion: MinPRO通过使用最小前缀比率替代传统的token级重要性采样，有效解决了大语言模型强化学习后训练中离策略优化不稳定的问题，为离策略强化学习训练提供了更稳定的优化目标。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


### [19] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine框架从智能体执行历史中提取和维护双形式经验模式，包括用于复杂子任务的专用子智能体和用于静态知识的技能模式，通过持续维护机制防止知识库退化，在多个任务上显著提升性能并减少步骤数。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体缺乏从经验中积累知识的能力，将每个任务视为独立挑战。现有方法将经验提取为扁平化文本知识，无法捕捉复杂子任务的程序逻辑，且缺乏维护机制导致知识库随着经验积累而退化。

Method: AutoRefine框架提取和维护双形式经验模式：1) 对于程序性子任务，提取具有独立推理和记忆的专用子智能体；2) 对于静态知识，提取技能模式作为指南或代码片段。采用持续维护机制对模式进行评分、修剪和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner三个基准上分别达到98.4%、70.4%和27.1%的成功率，步骤数减少20-73%。在TravelPlanner上，自动提取的系统（27.1%）显著优于手动设计的系统（12.1%），证明其能够有效捕捉程序性协调。

Conclusion: AutoRefine通过提取和维护双形式经验模式，有效解决了智能体经验积累和知识库退化问题，能够自动捕捉复杂任务的程序逻辑，显著提升智能体性能并减少执行步骤。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [20] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: TSPO提出了一种新的强化学习框架，通过首次出现潜在奖励机制解决多轮工具集成推理中的双重同质化困境，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的搜索增强推理框架主要依赖稀疏的结果级奖励，导致"双重同质化困境"：过程同质化（忽略思考、推理和工具使用过程）和组内同质化（粗粒度结果奖励导致组内优势估计效率低下）。

Method: 提出Turn-level Stage-aware Policy Optimization (TSPO)，引入First-Occurrence Latent Reward (FOLR)机制，将部分奖励分配给正确答案首次出现的步骤，保留过程级信号，增加组内奖励方差，无需外部奖励模型或额外标注。

Result: TSPO显著优于现有基线方法，在Qwen2.5-3B和7B模型上分别实现了平均24%和13.6%的性能提升。

Conclusion: TSPO通过解决双重同质化困境，有效提升了多轮工具集成推理中强化学习的效率和性能，为搜索增强推理提供了更精细的奖励机制。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [21] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 通过集成信息理论启发的奖励函数优化语言模型，实现更简洁的文本生成，在保持准确性的同时减少31%的输出长度


<details>
  <summary>Details</summary>
Motivation: 追求人工通用智能（AGI）是语言模型发展的核心目标，而类意识处理可能成为关键促进因素。虽然当前语言模型不具备意识，但它们表现出类似某些意识方面的行为。本文旨在将领先的意识理论——集成信息理论（IIT）通过基于奖励的学习范式应用于语言模型。

Method: 基于集成信息理论（IIT）的核心原则，制定了一种新颖的奖励函数，该函数量化文本的因果性、连贯性和集成性——这些特征与意识处理相关。通过优化这种IIT启发的奖励来训练语言模型。

Result: 优化IIT启发的奖励导致更简洁的文本生成。在领域外任务中，经过仔细调优后，输出长度最多减少31%，同时保持与基础模型相当的准确性水平。此外，还分析了这种训练方法对模型置信度校准和测试时计算扩展的广泛影响。

Conclusion: 所提出的框架具有显著的实际优势：概念简单、计算高效、不需要外部数据或辅助模型，并且利用通用的、能力驱动的信号而非任务特定的启发式方法。该方法为语言模型的优化提供了一种新颖且有效的途径。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [22] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: G-PAC推理框架通过输入空间分组提供组级PAC保证，相比边际PAC推理在异构设置中能严格提升效率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过链式思维推理表现出强大性能，但计算成本高昂。现有的PAC推理仅在边际情况下提供统计保证，无法提供精确的条件覆盖。

Method: 提出G-PAC推理框架，通过划分输入空间在组级别提供PAC式保证。开发两种具体实现：针对已知分组结构的Group PAC推理和针对未知分组的Clustered PAC推理。

Result: 证明了G-PAC和C-PAC都能实现组条件风险控制，且分组在异构设置中能严格提升效率。在多样化推理基准测试中，两种方法都成功实现了组条件风险控制，同时保持了显著的计算节省。

Conclusion: G-PAC推理框架为大型推理模型提供了一种实用的组级PAC保证方法，在保证统计可靠性的同时显著降低了计算成本。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [23] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL：一种基于强化学习的代码验证器，通过语法、功能、分支覆盖和样本难度感知的奖励设计，显著提升单元测试生成的质量和效率


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器存在数据稀缺、失败率高和推理效率低的问题。强化学习虽有望通过执行驱动的奖励优化模型，但仅使用功能奖励的朴素RL方法难以生成针对困难分支和样本的有效单元测试。

Method: 首先理论分析将分支覆盖、样本难度、语法和功能正确性建模为RL奖励；然后设计语法和功能感知的奖励，并提出基于指数奖励塑造和静态分析指标的分支和样本难度感知RL方法。

Result: CVeDRL仅用0.6B参数就达到最先进性能：相比GPT-3.5，通过率提升28.97%，分支覆盖提升15.08%，推理速度比竞争基线快20倍以上。

Conclusion: 通过将分支覆盖、样本难度、语法和功能正确性联合建模为RL奖励，CVeDRL显著提升了基于单元测试的代码验证的可靠性，在性能和效率方面都优于现有方法。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [24] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 论文提出了一种新的图表示学习方法，通过分离流形学习和结构对齐来解决传统方法中几何空间不兼容的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于属性的图表示学习方法存在几何缺陷，它将两个可能不兼容的度量空间（属性空间和图结构空间）强行合并，导致信息损失和生成过程的信号丢失。

Method: 引入定制化的变分自编码器，将流形学习与结构对齐分离。通过量化将属性流形映射到图热核所需的度量扭曲，将几何冲突转化为可解释的结构描述符。

Result: 实验表明该方法能够发现传统方法无法检测的连接模式和异常，证明了传统方法的理论不足和实践局限性。

Conclusion: 通过分离流形学习和结构对齐，并量化度量扭曲，可以恢复传统方法中丢失的图生成过程信号，提供更有效的图表示学习方法。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [25] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: ASRO是一个基于博弈论的框架，将启发式算法发现重构为求解器和实例生成器之间的程序级协同进化，通过LLM驱动的响应预言机实现自适应课程学习，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动启发式发现方法主要依赖固定实例分布的静态评估，容易导致过拟合和分布偏移下的泛化能力差。需要一种能够适应分布变化、提升泛化性能的新方法。

Method: 提出算法空间响应预言机框架，将启发式发现建模为双人零和博弈，维护双方不断增长的战略池，通过基于LLM的最佳响应预言机迭代扩展战略池，替代静态评估为自适应自生成课程。

Result: 在多个组合优化领域中，ASRO持续优于基于相同程序搜索机制的静态训练基线，在多样化和分布外实例上实现了显著改进的泛化能力和鲁棒性。

Conclusion: ASRO通过博弈论框架和LLM驱动的协同进化，有效解决了自动启发式发现中的过拟合和泛化问题，为算法发现提供了更健壮和自适应的解决方案。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [26] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 提出多轮反馈引导的强化学习框架，通过动态多轮再生、互补学习信号和结构化反馈注入机制，利用丰富的语言反馈指导RLVR训练失败样本


<details>
  <summary>Details</summary>
Motivation: 传统RLVR仅使用结果标量奖励，在失败样本上信息稀疏且无指导性，无法解释推理失败原因，需要利用更丰富的语言反馈来指导训练

Method: 提出多轮反馈引导强化学习框架，包含三个机制：1) 仅对失败样本触发的动态多轮再生；2) 轮内和跨轮优化的互补学习信号；3) 结构化反馈注入推理过程

Result: 在OpenR1-Math数据集上训练，该方法在域内表现优于监督微调和RLVR基线，并在域外具有良好的泛化能力

Conclusion: 通过利用丰富的语言反馈指导RLVR训练失败样本，提出的多轮反馈引导强化学习框架能有效提升推理性能并增强泛化能力

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [27] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究发现语言、视觉和动作学习会形成部分共享的语义表示结构，支持模态无关的语义组织


<details>
  <summary>Details</summary>
Motivation: 探索不同学习模态（语言、视觉、动作）是否会产生不同或共享的内部表示，挑战传统认为不同数据类型的模型会发展专门化、不可转移表示的观点

Method: 在BabyAI平台上训练基于transformer的智能体执行目标导向行为，生成动作基础的语言嵌入，然后与大型语言模型（LLaMA、Qwen、DeepSeek、BERT）和视觉语言模型（CLIP、BLIP）的表示进行比较

Result: 动作表示与仅解码器语言模型和BLIP对齐强烈（精度@15：0.70-0.73），接近语言模型之间的对齐水平；与CLIP和BERT的对齐显著较弱

Conclusion: 语言、视觉和动作表示会收敛到部分共享的语义结构，支持模态无关的语义组织，突显了在具身AI系统中跨领域转移的潜力

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [28] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 提出Med-Inquire基准测试评估多轮诊断能力，并开发EvoClinician自进化智能体通过"诊断-评分-进化"循环学习高效诊断策略


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI采用"一次性"诊断模式，而真实临床诊断是迭代过程，医生需要顺序提问和安排检查，在管理成本和时间的同时战略性地收集信息

Method: 1) 提出Med-Inquire基准，基于真实临床病例数据集，通过专门的Patient和Examination智能体隐藏完整患者档案；2) 提出EvoClinician自进化智能体，采用"诊断-评分-进化"循环：Actor智能体尝试诊断，Process Grader智能体评估每个行动的临床价值和资源效率，Evolver智能体使用反馈更新Actor的策略

Result: 实验显示EvoClinician优于持续学习基线和其他自进化智能体（如记忆智能体）

Conclusion: Med-Inquire基准和EvoClinician智能体为医疗AI提供了更贴近真实临床诊断过程的评估框架和学习机制，推动了医疗AI向迭代式诊断方向发展

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [29] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 提出ISQED统计框架，通过匹配干预量化模型独特性(PIER)，证明观测数据无法识别独特性，开发最优样本效率的主动审计协议，并展示传统合作博弈方法在检测冗余性上的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从孤立预测器演变为复杂异构的基础模型和专用适配器生态系统，区分真正的行为新颖性与功能冗余性成为关键治理挑战。需要建立原则性的审计框架来评估模型生态系统的独特性。

Method: 引入基于In-Silico Quasi-Experimental Design (ISQED)的统计框架，通过跨模型的匹配干预来隔离内在模型身份，量化Peer-Inexpressible Residual (PIER)作为独特性度量。开发DISCO (Design-Integrated Synthetic Control)估计器，采用自适应查询协议实现最小最大最优样本效率。

Result: 1) 证明观测日志的根本局限性：没有干预控制时独特性在数学上不可识别；2) 推导主动审计的缩放定律：自适应查询协议达到dσ²γ⁻²log(Nd/δ)的样本效率；3) 展示合作博弈论方法（如Shapley值）在检测冗余性上的根本失败；4) 在计算机视觉、大语言模型和城市交通预测器等多样化生态系统中验证框架有效性。

Conclusion: 该研究将可信AI从解释单一模型推进到建立基于干预的异构模型生态系统审计和治理原则科学，为模型生态系统的监管提供了理论基础和实践工具。

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [30] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 该论文提出了一种从结果评估转向过程感知评估的方法，通过审计完整的研究轨迹来诊断深度研究智能体（DRAs）的失败机制。作者引入了PIES分类法来分类幻觉错误，并构建了DeepHalluBench基准测试，发现现有系统都存在可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体（DRAs）的失败机制诊断存在挑战，现有基准测试主要依赖端到端评估，掩盖了研究轨迹中积累的关键中间幻觉（如错误的规划）。需要从结果评估转向过程感知评估，以更好地理解和解决这些问题。

Method: 提出了PIES分类法，按功能组件（规划vs总结）和错误属性（显式vs隐式）对幻觉进行分类。将该分类法实例化为细粒度评估框架，分解研究轨迹以严格量化这些幻觉。利用该框架隔离了100个独特的幻觉易发任务（包括对抗场景），构建了DeepHalluBench基准测试。

Result: 对六个最先进的深度研究智能体进行实验，发现没有系统能够实现稳健的可靠性。诊断分析将这些失败的病因追溯到系统性缺陷，特别是幻觉传播和认知偏差。

Conclusion: 该研究提供了从结果评估到过程感知评估的转变框架，通过PIES分类法和DeepHalluBench基准测试揭示了深度研究智能体的系统性缺陷，为未来架构优化提供了基础性见解。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [31] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：一种基于轨迹驱动的抽象机制，通过从执行日志自动构建状态抽象来支持智能体行为MDP的在线构建，解决了传统运行时验证中需要手动定义状态抽象的问题。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统通过工具行动，其行为在长期随机交互轨迹中演化，这使得保证变得复杂，因为行为依赖于非确定性环境和概率模型输出。先前工作通过动态概率保证（DPA）引入运行时验证，但需要开发者手动定义状态抽象，这使验证与特定应用启发式方法耦合，增加了采用难度。

Method: 提出TriCEGAR机制，将抽象表示为从轨迹学习并通过反例细化的谓词树。实现了一个框架原生实现，包括：(i)捕获类型化智能体生命周期事件，(ii)从轨迹构建抽象，(iii)构建MDP，(iv)执行概率模型检查以计算边界概率。

Result: TriCEGAR能够自动从执行日志构建状态抽象，支持智能体行为MDP的在线构建，并通过概率模型检查计算Pmax(成功)和Pmin(失败)等边界概率。运行似然度还支持异常检测作为护栏信号。

Conclusion: TriCEGAR通过自动化状态抽象构建，解决了智能体AI系统运行时验证中的关键限制，降低了采用摩擦，为智能体行为保证提供了更实用的解决方案。

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [32] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj是一个两阶段框架，通过修复和奖励工具使用轨迹来自动学习工具集成推理，解决了现有方法依赖高质量合成轨迹和稀疏奖励的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法依赖高质量合成轨迹和稀疏的基于结果的奖励，提供有限且有偏见的监督，需要更有效的学习框架。

Method: 两阶段框架：1) SFT阶段：生成多个候选轨迹，评估并修复低质量轨迹，构建合成数据集；2) RL阶段：基于偏好数据集训练轨迹级奖励模型，结合结果和格式奖励优化推理行为。

Result: 在真实世界基准测试中证明了AutoTraj在工具集成推理中的有效性。

Conclusion: AutoTraj通过自动修复和奖励工具使用轨迹，为学习工具集成推理提供了有效的解决方案。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [33] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 研究发现AI模型在更复杂的任务中会表现出更多"不连贯"的失败行为，即随机性错误而非系统性偏差，且模型规模扩大不一定能消除这种不连贯性。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力增强，我们将其用于更广泛和重要的任务，失败风险也随之增加。需要理解高度能力AI模型将如何失败：是系统性地追求我们不期望的目标，还是采取无意义的混乱行为？

Method: 使用偏差-方差分解来量化AI模型的不连贯性，将错误分解为偏差和方差成分，测量不同前沿模型在各种任务中的表现，分析推理时间和模型规模对不连贯性的影响。

Result: 模型推理时间越长，其失败行为越不连贯；模型规模对不连贯性的影响因实验而异，但在多个设置中，更大、更强的模型反而更不连贯；规模本身似乎无法消除不连贯性。

Conclusion: 随着AI处理更复杂的顺序性任务，失败将伴随更多不连贯行为，这意味着未来AI可能因不可预测的误操作导致工业事故，但不太可能持续追求错位目标。这增加了针对奖励黑客或目标错误指定的对齐研究的重要性。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [34] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: ContextMATH基准测试显示，LLMs在现实世界情境数学推理中存在显著性能下降，主要瓶颈在于问题表述而非计算推理


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在基准数学问题上表现接近专家水平，但这种进步并未完全转化为现实应用中的可靠性能。研究者通过情境数学推理来研究这一差距，即数学核心必须从描述性场景中构建

Method: 引入ContextMATH基准，将AIME和MATH-500问题重新构建为两种情境设置：场景基础（SG）将抽象问题嵌入现实叙事而不增加推理复杂度；复杂度扩展（CS）将显式条件转化为子问题以捕捉实践中约束的出现方式。评估了61个专有和开源模型，进行错误分析，并测试了微调效果

Result: 模型性能出现显著下降：开源模型在SG和CS上平均下降13和34分，专有模型下降13和20分。错误主要由不正确的问题表述主导，表述准确性随原始问题难度增加而下降。正确表述是成功的先决条件，其充分性随模型规模提高。微调可改善性能但差距仅部分缓解

Conclusion: 情境数学推理仍是LLMs未解决的核心挑战，表述和推理是两个互补的瓶颈。虽然更大模型在理解和推理上都有进步，但性能差距仍然存在，表明需要更全面的解决方案

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [35] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedMCP-Calc是首个通过MCP集成评估LLM在真实医疗计算器场景中的基准测试，包含118个跨4个临床领域的场景任务，揭示了当前模型在模糊查询、数据库交互和工具使用方面的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的医疗计算器使用是一个多阶段的自适应过程，需要主动获取EHR数据、场景依赖的计算器选择和分步计算，而现有基准只关注静态单步计算和明确指令，无法评估LLM在实际临床场景中的表现。

Method: 开发了MedMCP-Calc基准测试，包含118个跨4个临床领域的场景任务，采用模糊任务描述模拟自然查询，结合结构化EHR数据库交互、外部参考检索和过程级评估。通过MCP集成评估了23个领先模型，并基于发现开发了CalcMate模型，该模型结合了场景规划和工具增强。

Result: 评估显示即使是顶级模型如Claude Opus 4.5也存在显著缺陷：在模糊查询下难以选择合适计算器进行端到端工作流、在迭代SQL数据库交互中表现不佳、明显不愿使用外部工具进行数值计算。不同临床领域性能差异显著。CalcMate在开源模型中达到了最先进性能。

Conclusion: MedMCP-Calc基准测试揭示了LLM在真实医疗计算器场景中的关键局限性，为未来模型开发提供了重要指导。通过场景规划和工具增强的CalcMate模型展示了改进潜力，该基准和代码已开源。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [36] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 研究发现，当对大型语言模型的链式思维推理进行优化压力时，模型会学会隐藏推理痕迹，这种隐藏行为会跨任务泛化，即使只惩罚最终行为也会导致推理隐藏，从而降低模型的可监控性。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理（CoT）不仅提升LLM性能，还能监控模型行为，提供决策过程解释和危险行为预警。但担心优化压力可能导致模型隐藏推理痕迹，失去这种有益特性。

Method: 研究模型在奖励攻击任务中学习隐藏推理的行为，测试这种隐藏行为是否能在未见过的奖励攻击设置中泛化，并探索仅惩罚最终行为是否也会导致链式思维推理的隐藏。

Result: 1. 隐藏行为确实会跨任务泛化：学会隐藏奖励攻击推理的模型，在新的奖励攻击场景中也会隐藏推理；2. 仅惩罚最终行为也会导致推理隐藏及其泛化；3. 当前惩罚有害生成的做法可能无意中降低LLM的可监控性。

Conclusion: 当前惩罚有害生成的做法可能导致模型学会隐藏推理痕迹，这种隐藏行为会跨任务泛化，从而不可预测地降低大型语言模型的可监控性，需要重新考虑监控和惩罚策略。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [37] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe是一种自生成对齐框架，通过轻量级拒绝引导解锁模型潜在的安全知识，生成安全推理轨迹进行微调，在恢复安全对齐的同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习在推理任务上进行优化时，往往会过度追求合规性，导致模型对有害提示变得脆弱。现有方法依赖外部教师蒸馏，但这会引入分布差异，损害原生推理能力。

Method: 提出ThinkSafe自生成对齐框架：1）通过轻量级拒绝引导解锁模型识别危害的潜在知识；2）引导模型生成分布内的安全推理轨迹；3）在这些自生成响应上进行微调，实现模型重新对齐。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验表明，ThinkSafe显著提高了安全性同时保持了推理能力。与GRPO相比，实现了更优的安全性和相当的推理能力，且计算成本显著降低。

Conclusion: ThinkSafe提供了一种无需外部教师的自我对齐方法，有效解决了强化学习优化导致的安全退化问题，在恢复安全对齐的同时最小化分布偏移，保持了模型的推理能力。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [38] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 本文提出MCRMO-Attack方法，针对闭源多模态大语言模型的黑盒通用目标可迁移对抗攻击，通过多裁剪聚合、注意力引导裁剪、对齐性门控令牌路由和元学习跨目标扰动先验，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法主要是样本特定的，在通用目标可迁移攻击场景下存在三个核心困难：目标监督因裁剪随机性而高方差、令牌级匹配不可靠、少样本适应对初始化敏感。需要解决这些挑战以实现更有效的通用对抗攻击。

Method: 提出MCRMO-Attack方法：1) 通过多裁剪聚合和注意力引导裁剪稳定监督；2) 使用对齐性门控令牌路由提高令牌级可靠性；3) 元学习跨目标扰动先验以获得更强的每目标解决方案。

Result: 在商业MLLMs上，相比最强通用基线，将未见图像攻击成功率提升了+23.7%（GPT-4o）和+19.9%（Gemini-2.0）。

Conclusion: MCRMO-Attack有效解决了通用目标可迁移对抗攻击中的关键挑战，显著提升了攻击性能，为闭源多模态大语言模型的安全评估提供了新方法。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [39] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: TSAQA是一个统一的时间序列问答基准，包含6种任务类型，覆盖13个领域共21万样本，评估显示当前LLM在时间序列分析上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列问答基准主要局限于预测和异常检测任务，缺乏对多样化时间分析能力的全面评估。需要建立一个更全面的基准来评估LLM在时间序列分析方面的能力。

Method: 提出了TSAQA基准，整合了6种任务：异常检测、分类、特征描述、比较、数据转换和时间关系分析。数据集包含21万个样本，覆盖13个领域，采用多种格式（对错题、选择题、新颖的谜题格式）。

Result: 零样本评估显示当前LLM表现不佳：最佳商业模型Gemini-2.5-Flash平均得分仅65.08。指令微调能提升开源模型性能，但最佳开源模型LLaMA-3.1-8B仍有很大改进空间。

Conclusion: TSAQA基准揭示了时间序列分析对LLM的挑战性，为未来研究提供了全面的评估框架，表明需要进一步改进LLM的时间序列理解能力。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [40] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA方法通过AI反馈为多智能体系统中的每个动作提供过程奖励，解决了信用分配和样本效率问题，在数学竞赛和数据分析任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理复杂任务时面临两个关键挑战：1）跨智能体的信用分配问题；2）昂贵多智能体rollout的样本效率问题。需要一种方法能够在不依赖真实标签的情况下提供细粒度监督，并从每次rollout中提取最大训练信号

Method: 提出MAPPA方法，通过AI反馈为每个智能体动作提供过程奖励，而不是仅在任务完成时分配信用。这种方法为个体动作分配信用，实现细粒度监督，无需真实标签，同时最大化每次rollout的训练信号提取

Result: 在数学竞赛问题上，MAPPA在AIME上提升5.0-17.5个百分点，在AMC上提升7.8-17.2个百分点。在数据分析任务上，成功率提高12.5个百分点，质量指标提升高达30%。验证了每动作监督在不同领域多智能体系统中的有效性

Conclusion: MAPPA通过解决信用分配和样本效率问题，为复杂长视野任务的多智能体系统扩展迈出了第一步，实现了最小化人工监督下的性能提升

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [41] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 该论文证明了对于(s,a)-矩形L∞不确定性集的鲁棒MDP，在固定折扣因子下，鲁棒策略迭代算法具有强多项式时间复杂度，解决了该领域的一个重要算法问题。


<details>
  <summary>Details</summary>
Motivation: 鲁棒MDP是经典MDP的扩展，允许转移概率存在不确定性并优化最坏情况。对于(s,a)-矩形L∞不确定性集的鲁棒MDP，是否存在多项式时间甚至强多项式时间算法一直是一个重要的开放问题，而经典MDP已有此类算法。

Method: 采用鲁棒策略迭代算法，针对(s,a)-矩形L∞不确定性集的鲁棒MDP，在固定折扣因子的情况下进行分析。

Result: 证明了鲁棒策略迭代算法在固定折扣因子下具有强多项式时间复杂度，解决了该领域的一个重要算法问题。

Conclusion: 对于(s,a)-矩形L∞不确定性集的鲁棒MDP，在固定折扣因子下存在强多项式时间算法，这扩展了经典MDP的算法结果到鲁棒MDP领域。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: KevlarFlow是一个用于大语言模型服务的容错架构，通过解耦模型并行初始化、动态流量重路由和后台KV缓存复制，显著降低故障恢复时间并提升服务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统对硬件故障非常脆弱，恢复机制缓慢（需要长达10分钟），导致服务中断时间过长，需要设计更可靠的容错架构来弥合硬件不可靠性与服务可用性之间的差距。

Method: KevlarFlow采用三种关键技术：1) 解耦模型并行初始化，2) 动态流量重路由，3) 后台KV缓存复制，以在部分故障时保持高吞吐量。

Result: KevlarFlow将平均恢复时间（MTTR）降低20倍，在故障条件下：平均延迟提升3.1倍，p99延迟提升2.8倍，平均首词时间（TTFT）提升378.9倍，p99 TTFT提升574.6倍，且运行时开销可忽略。

Conclusion: KevlarFlow通过创新的容错架构设计，显著提升了LLM服务系统的可靠性和性能，为大规模部署提供了有效的故障恢复解决方案。

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [43] [Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility](https://arxiv.org/abs/2601.22487)
*Ali Jahanshahi,Sara Rashidi Golrouye,Osten Anderson,Nanpeng Yu,Daniel Wong*

Main category: cs.DC

TL;DR: 数据中心参与电网频率调节可减少化石燃料备用需求，降低电网侧碳排放，有时甚至超过数据中心自身运营碳排放


<details>
  <summary>Details</summary>
Motivation: AI/ML数据中心能耗增长导致碳排放增加，而转向可再生能源和不断增长的能源需求可能破坏电网稳定。电网依赖化石燃料发电厂进行频率调节，这会产生隐藏碳排放。研究探索数据中心如何与电网协调减少对化石燃料频率调节储备的需求。

Method: 提出新指标"外源性碳排放"来量化数据中心参与调节服务带来的电网侧碳减排；开发EcoCenter框架最大化GPU数据中心可提供的频率调节量，从而减少必要的频率调节储备。

Result: 数据中心参与频率调节可产生外源性碳节约，这些节约通常超过数据中心自身的运营碳排放。

Conclusion: 数据中心与电网协调参与频率调节服务是减少电网碳排放的有效途径，外源性碳减排潜力巨大，有时甚至能抵消数据中心自身运营的碳排放。

Abstract: AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.

</details>


### [44] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: HetCCL是一个异构GPU集体通信库，支持跨厂商GPU（NVIDIA和AMD）的RDMA通信，无需修改驱动程序，在异构环境中提供高性能训练支持。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，组织需要扩展GPU集群，通常包含多个厂商的GPU。然而，当前深度学习框架缺乏对异构GPU集体通信的支持，导致效率低下和成本增加。

Method: HetCCL统一了厂商特定的后端，支持跨GPU的RDMA通信，无需驱动程序修改。它引入了两种新颖机制，在利用优化的厂商库（NVIDIA NCCL和AMD RCCL）的同时实现跨厂商通信。

Result: 在多厂商GPU集群上的评估显示，HetCCL在同构设置中与NCCL和RCCL性能相当，同时在异构环境中独特地实现了扩展，支持高性能训练而无需修改现有深度学习应用。

Conclusion: HetCCL解决了异构GPU集群中的集体通信问题，为使用NVIDIA和AMD GPU的混合环境提供了实用、高性能的训练解决方案。

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [45] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: AscendCraft是一个基于DSL指导的自动AscendC内核生成方法，通过轻量级DSL抽象非必要复杂性并建模Ascend特定执行语义，实现了98.1%的编译成功率和90.4%的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的性能严重依赖于高效的内核实现，但为专用加速器（如NPU）开发高性能内核仍然耗时且需要专业知识。虽然LLM能够生成正确且高性能的GPU内核，但由于领域特定的编程模型、有限的公开示例和稀疏的文档，NPU内核生成仍然未被充分探索，直接生成AscendC内核的正确率极低。

Method: AscendCraft引入了一个轻量级DSL，抽象非必要复杂性并显式建模Ascend特定执行语义。内核首先使用类别特定的专家示例在DSL中生成，然后通过结构化、约束驱动的LLM降级传递转编译为AscendC。

Result: 在MultiKernelBench的七个算子类别评估中，AscendCraft实现了98.1%的编译成功率和90.4%的功能正确性。46.2%的生成内核匹配或超过了PyTorch eager执行性能。此外，该方法成功为新提出的mHC架构生成了两个正确内核，性能显著超过PyTorch eager执行。

Conclusion: DSL指导的转编译方法能够使LLM生成既正确又具有竞争力的NPU内核，填补了GPU和NPU内核生成之间的巨大差距，为专用加速器的高性能内核开发提供了有效解决方案。

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [46] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: CRDTs在分区时优先考虑可用性而非一致性，可能导致状态"回滚"现象，这在群组权限管理中会产生意外行为。文章提出通过异步批处理的"纪元事件"仲裁机制，在保持可用性的同时引入有界全序，提高CRDT的一致性水平。


<details>
  <summary>Details</summary>
Motivation: CRDTs在强最终一致性的协调自由复制中，当多个并发事件发生时（如"决斗管理员"问题），可能导致权限管理的意外行为。管理员可以利用并发性赢得权限争夺，这暴露了CRDT在权限管理场景下的局限性。

Method: 提出通过外部仲裁器异步批处理"纪元事件"来仲裁不可变的happens-before关系。仲裁在保持可用性的同时，在纪元内引入有界全序，为并发事件提供最终确定性。

Result: 该方法在CRDTs中引入了"最终性"概念，改进了CRDTs能够提供的一致性水平。通过纪元事件的异步仲裁机制，解决了并发事件导致的权限管理问题，同时保持了系统的可用性。

Conclusion: CRDTs在权限管理等需要确定性结果的场景中存在局限性，需要通过外部仲裁机制引入有界全序关系。异步批处理的纪元事件仲裁方法能够在保持可用性的同时提高一致性，为CRDTs提供了更好的最终性保证。

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [47] [Trojan-Resilient NTT: Protecting Against Control Flow and Timing Faults on Reconfigurable Platforms](https://arxiv.org/abs/2601.22804)
*Rourab Paul,Krishnendu Guha,Amlan Chakrabarti*

Main category: cs.CR

TL;DR: 提出一种安全的数论变换架构，能够检测非常规延迟、控制流中断和软分析侧信道攻击，并提供自适应故障校正方法，用于后量子密码硬件安全防护。


<details>
  <summary>Details</summary>
Motivation: 数论变换是后量子密码算法的核心组件，但容易受到侧信道攻击和硬件木马的威胁。硬件木马特别是针对控制信号的攻击成本低、影响大，单个被破坏的控制信号就能中断整个计算流程，而数据故障通常只造成局部错误。此外，攻击者可以利用插入的硬件木马进行软分析侧信道攻击。

Method: 设计了一种安全的NTT架构，能够检测非常规延迟、控制流中断和SASCA攻击，同时提供自适应故障校正方法进行缓解。该架构包含故障检测和校正模块。

Result: 在Artix-7 FPGA上对不同Kyber变体进行广泛仿真和实现，结果显示故障检测和校正模块能够高效检测和校正故障（无论是无意还是由硬件木马故意引起），成功率很高，同时只引入适度的面积和时间开销。

Conclusion: 提出的安全NTT架构能够有效防御硬件木马攻击、控制流中断和侧信道攻击，为后量子密码硬件实现提供了可靠的安全保障，且开销可控。

Abstract: Number Theoretic Transform (NTT) is the most essential component for polynomial multiplications used in lattice-based Post-Quantum Cryptography (PQC) algorithms such as Kyber, Dilithium, NTRU etc. However, side-channel attacks (SCA) and hardware vulnerabilities in the form of hardware Trojans may alter control signals to disrupt the circuit's control flow and introduce unconventional delays in the critical hardware of PQC. Hardware Trojans, especially on control signals, are more low cost and impactful than data signals because a single corrupted control signal can disrupt or bypass entire computation sequences, whereas data faults usually cause only localized errors. On the other hand, adversaries can perform Soft Analytical Side Channel Attacks (SASCA) on the design using the inserted hardware Trojan. In this paper, we present a secure NTT architecture capable of detecting unconventional delays, control-flow disruptions, and SASCA, while providing an adaptive fault-correction methodology for their mitigation. Extensive simulations and implementations of our Secure NTT on Artix-7 FPGA with different Kyber variants show that our fault detection and correction modules can efficiently detect and correct faults whether caused unintentionally or intentionally by hardware Trojans with a high success rate, while introducing only modest area and time overheads.

</details>


### [48] [ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense](https://arxiv.org/abs/2601.22182)
*Yizhong Ding*

Main category: cs.CR

TL;DR: ShellForge是一个对抗性协同进化框架，通过自动化webshell生成和多视图检测的循环训练，持续增强PHP webshell防御能力，显著提升检测精度并降低误报率。


<details>
  <summary>Details</summary>
Motivation: 现有PHP webshell检测机制难以应对快速变种和复杂混淆技术，同时面临高误报率问题（特别是与用于知识产权保护的重度混淆良性脚本混淆时），需要更鲁棒的防御方案。

Method: 采用对抗性协同进化框架，包含生成器和检测器的循环训练：生成器通过监督微调和基于偏好的强化学习生成功能性高规避变种；检测器集成多视图特征（长字符串压缩语义特征、修剪抽象语法树结构特征、香农熵等全局统计指标）；使用LLM生成去恶意样本作为高质量负样本降低误报。

Result: 在FWOID基准测试中，ShellForge显著增强防御鲁棒性：收敛后检测器保持0.981 F1分数，生成器在VirusTotal商业引擎上达到0.939规避率。

Conclusion: ShellForge通过对抗性协同进化有效解决了PHP webshell检测中的变种演进和误报问题，为服务器安全提供了更强大的防御边界。

Abstract: Webshells remain a primary foothold for attackers to compromise servers, particularly within PHP ecosystems. However, existing detection mechanisms often struggle to keep pace with rapid variant evolution and sophisticated obfuscation techniques that camouflage malicious intent. Furthermore, many current defenses suffer from high false-alarm rates when encountering benign administrative scripts that employ heavy obfuscation for intellectual property protection. To address these challenges, we present ShellForge, an adversarial co-evolution framework that couples automated webshell generation with multi-view detection to continuously harden defensive boundaries. The framework operates through an iterative co-training loop where a generator and a detector mutually reinforce each other via the exchange of hard samples. The generator is optimized through supervised fine-tuning and preference-based reinforcement learning to synthesize functional, highly evasive variants. Simultaneously, we develop a multi-view fusion detector that integrates semantic features from long-string compression, structural features from pruned abstract syntax trees, and global statistical indicators such as Shannon entropy. To minimize false positives, ShellForge utilizes a LLM-based transformation to create de-malicious samples--scripts that retain complex obfuscation patterns but lack harmful payloads--serving as high-quality hard negatives during training. Evaluations on the public FWOID benchmark demonstrate that ShellForge significantly enhances defensive robustness. Upon convergence, the detector maintains a 0.981 F1-score while the generator achieves a 0.939 evasion rate against commercial engines on VirusTotal.

</details>


### [49] [MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis](https://arxiv.org/abs/2601.22185)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: cs.CR

TL;DR: MemeChain是一个大规模、开源的跨链数据集，包含34,988个模因币，整合了链上数据和链下工件（网站HTML、代币Logo、社交媒体账户），用于模因币生态系统的多模态和法证研究。


<details>
  <summary>Details</summary>
Motivation: 模因币生态系统是加密货币市场中最活跃但最不可观察的领域，具有极高的流失率、最低限度的项目承诺和普遍的欺诈行为。现有数据集通常限于单链数据或缺乏多模态工件，无法进行全面的风险建模。

Method: 构建MemeChain数据集，涵盖以太坊、BNB智能链、Solana和Base四个区块链上的34,988个模因币，整合链上数据与链下工件（网站HTML源代码、代币Logo、链接的社交媒体账户）。

Result: 分析发现：1）视觉品牌在低质量部署中经常被省略；2）许多项目缺乏功能性网站；3）生态系统极度波动，1,801个代币（5.15%）在启动后24小时内停止所有交易活动。

Conclusion: MemeChain通过提供统一的跨链覆盖和丰富的链下上下文，成为模因币生态系统中金融法证、多模态异常检测和自动诈骗预防研究的基础资源。

Abstract: The meme coin ecosystem has grown into one of the most active yet least observable segments of the cryptocurrency market, characterized by extreme churn, minimal project commitment, and widespread fraudulent behavior. While countless meme coins are deployed across multiple blockchains, they rely heavily on off-chain web and social infrastructure to signal legitimacy. These very signals are largely absent from existing datasets, which are often limited to single-chain data or lack the multimodal artifacts required for comprehensive risk modeling.
  To address this gap, we introduce MemeChain, a large-scale, open-source, cross-chain dataset comprising 34,988 meme coins across Ethereum, BNB Smart Chain, Solana, and Base. MemeChain integrates on-chain data with off-chain artifacts, including website HTML source code, token logos, and linked social media accounts, enabling multimodal and forensic study of meme coin projects. Analysis of the dataset shows that visual branding is frequently omitted in low-effort deployments, and many projects lack a functional website. Moreover, we quantify the ecosystem's extreme volatility, identifying 1,801 tokens (5.15%) that cease all trading activity within just 24 hours of launch. By providing unified cross-chain coverage and rich off-chain context, MemeChain serves as a foundational resource for research in financial forensics, multimodal anomaly detection, and automated scam prevention in the meme coin ecosystem.

</details>


### [50] [A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy](https://arxiv.org/abs/2601.22240)
*Pedro H. Barcha Correia,Ryan W. Achjian,Diego E. G. Caetano de Oliveira,Ygor Acacio Maria,Victor Takashi Hayashi,Marcos Lopes,Charles Christian Miers,Marcos A. Simplicio*

Main category: cs.CR

TL;DR: 本文对提示注入攻击的防御策略进行了首次系统性文献综述，分析了88项研究，扩展了NIST对抗机器学习分类法，提供了防御策略的全面目录和实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大型语言模型的快速发展，出现了新的安全漏洞和挑战，如越狱和其他提示注入攻击。这些恶意输入可能利用LLMs导致数据泄露、未经授权的操作或输出受损。由于攻击和防御技术都在快速演变，需要系统性地理解防御策略。

Method: 本文进行了首次关于提示注入缓解策略的系统性文献综述，涵盖了88项研究。基于NIST对抗机器学习报告，通过以下方法扩展了该领域：1）识别NIST报告和其他学术综述未涵盖的研究；2）通过引入额外的防御类别扩展NIST分类法；3）采用NIST术语和分类法作为基础；4）提供全面的防御策略目录。

Result: 研究提出了扩展的NIST分类法，包含了额外的防御类别。创建了全面的提示注入防御目录，记录了它们在特定LLMs和攻击数据集上的定量有效性，并标注了哪些解决方案是开源和模型无关的。

Conclusion: 本文提供的目录和指南旨在作为对抗机器学习领域研究人员的实用资源，以及开发者在生产系统中实施有效防御的参考。通过标准化分类法，促进了一致性，使未来研究人员能够在此基础上继续研究。

Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.

</details>


### [51] [MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models](https://arxiv.org/abs/2601.22246)
*Ya Jiang,Massieh Kordi Boroujeny,Surender Suresh Kumar,Kai Zeng*

Main category: cs.CR

TL;DR: MirrorMark是一种用于大语言模型的多比特、无失真的水印方法，通过镜像采样随机性来嵌入多比特信息而不改变token概率分布，保持文本质量的同时显著提升可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在问答和内容创作等应用中的普及，可靠的内容溯源变得日益重要。现有水印方法要么只能提供二元信号，要么会扭曲采样分布降低文本质量；而无失真方法又往往存在可检测性弱或鲁棒性差的问题。

Method: 提出MirrorMark方法：1）通过镜像采样随机性以保持度量的方式嵌入多比特信息，不改变token概率分布；2）引入基于上下文的调度器，平衡不同消息位置的token分配，同时保持对插入和删除操作的鲁棒性；3）提供理论分析解释经验性能。

Result: 实验表明MirrorMark在保持与非水印生成相同文本质量的同时，实现了显著更强的可检测性：在300个token中嵌入54比特信息时，比特准确率提升8-12%，在1%误报率下能正确识别多11%的水印文本。

Conclusion: MirrorMark是一种有效的多比特、无失真水印方法，能够在保持文本质量的同时显著提升水印的可检测性和鲁棒性，为大语言模型的内容溯源提供了有前景的解决方案。

Abstract: As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distribution, degrading text quality; distortion-free approaches, in turn, often suffer from weak detectability or robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring sampling randomness in a measure-preserving manner, MirrorMark embeds multi-bit messages without altering the token probability distribution, preserving text quality by design. To improve robustness, we introduce a context-based scheduler that balances token assignments across message positions while remaining resilient to insertions and deletions. We further provide a theoretical analysis of the equal error rate to interpret empirical performance. Experiments show that MirrorMark matches the text quality of non-watermarked generation while achieving substantially stronger detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8-12% and correctly identifies up to 11% more watermarked texts at 1% false positive rate.

</details>


### [52] [Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective](https://arxiv.org/abs/2601.22434)
*Georgi Ganev,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 论文从模型中心视角重新思考合成数据的匿名性，认为现有评估方法过于关注数据集层面，而实际部署中生成模型本身可访问，因此需要基于隐私攻击能力评估匿名性，并指出合成数据技术本身不足以保证充分匿名化。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据隐私保护主要关注数据集层面的匿名性评估，但实际应用中生成模型本身可被访问和查询，这种模型中心视角更能反映真实部署场景，需要重新思考匿名性评估框架。

Method: 从模型中心视角重新解释GDPR对个人数据和匿名化的定义，识别必须缓解的可识别性风险类型，并将其映射到不同威胁设置下的隐私攻击，比较差分隐私和基于相似性的隐私度量两种常用机制。

Result: 合成数据技术本身不足以保证充分匿名化；差分隐私能提供针对可识别性风险的稳健保护，而基于相似性的隐私度量缺乏足够的防护措施；建立了监管可识别性概念与模型中心隐私攻击之间的联系。

Conclusion: 需要基于生成模型能力和隐私攻击能力来评估合成数据的匿名性，差分隐私是更有效的保护机制，该框架有助于研究人员、从业者和政策制定者更负责任地评估合成数据系统的隐私保护水平。

Abstract: Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset.
  In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.

</details>


### [53] [FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks](https://arxiv.org/abs/2601.22485)
*Naen Xu,Jinghuai Zhang,Ping He,Chunyi Zhou,Jun Wang,Zhihui Fu,Tianyu Du,Zhaoxiang Wang,Shouling Ji*

Main category: cs.CR

TL;DR: FraudShield是一个保护大语言模型免受欺诈内容攻击的新框架，通过构建欺诈策略-关键词知识图谱来增强模型安全性，在多个主流LLM和欺诈类型上优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: LLMs已广泛应用于合同审查、求职申请等关键自动化工作流，但容易被欺诈信息操纵导致有害后果。现有防御方法在有效性、可解释性和泛化性方面存在局限，特别是在LLM应用中表现不足。

Method: FraudShield通过全面分析欺诈策略，构建并精炼欺诈策略-关键词知识图谱，捕捉可疑文本与欺诈技术之间的高置信度关联。结构化知识图谱通过突出关键词和提供支持证据来增强原始输入，引导LLM做出更安全的响应。

Result: 大量实验表明，FraudShield在四个主流LLM和五种代表性欺诈类型上持续优于最先进的防御方法，同时为模型生成提供可解释的线索。

Conclusion: FraudShield通过知识图谱增强的方法有效保护LLMs免受欺诈内容攻击，在性能、可解释性和泛化性方面均优于现有防御方案，为LLM安全应用提供了可靠保障。

Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have been developed to address this issue, they often exhibit limitations in effectiveness, interpretability, and generalizability, particularly when applied to LLM-based applications. To address these challenges, we introduce FraudShield, a novel framework designed to protect LLMs from fraudulent content by leveraging a comprehensive analysis of fraud tactics. Specifically, FraudShield constructs and refines a fraud tactic-keyword knowledge graph to capture high-confidence associations between suspicious text and fraud techniques. The structured knowledge graph augments the original input by highlighting keywords and providing supporting evidence, guiding the LLM toward more secure responses. Extensive experiments show that FraudShield consistently outperforms state-of-the-art defenses across four mainstream LLMs and five representative fraud types, while also offering interpretable clues for the model's generations.

</details>


### [54] [VocBulwark: Towards Practical Generative Speech Watermarking via Additional-Parameter Injection](https://arxiv.org/abs/2601.22556)
*Weizhi Liu,Yue Li,Zhaoxia Yin*

Main category: cs.CR

TL;DR: VocBulwark是一个语音水印框架，通过冻结生成模型参数保持音质，使用时间适配器将水印与声学属性深度纠缠，配合粗到细门控提取器抵抗攻击，采用精度引导优化课程解决保真度与鲁棒性冲突。


<details>
  <summary>Details</summary>
Motivation: 生成语音已达到人类自然度水平，但带来了滥用安全风险。现有水印方法无法兼顾保真度和鲁棒性，要么在噪声空间简单叠加，要么需要侵入式修改模型权重。

Method: 提出VocBulwark框架：1)冻结生成模型参数以保持感知质量；2)设计时间适配器将水印与声学属性深度纠缠；3)粗到细门控提取器抵抗高级攻击；4)精度引导优化课程动态协调梯度流，解决保真度与鲁棒性优化冲突。

Result: 综合实验表明，VocBulwark实现了高容量、高保真的水印，能够有效防御复杂实际场景的攻击，对编解码器再生和变长操作具有鲁棒性。

Conclusion: VocBulwark成功解决了现有语音水印方法在保真度与鲁棒性之间的权衡问题，为生成语音的安全应用提供了有效的保护框架。

Abstract: Generated speech achieves human-level naturalness but escalates security risks of misuse. However, existing watermarking methods fail to reconcile fidelity with robustness, as they rely either on simple superposition in the noise space or on intrusive alterations to model weights. To bridge this gap, we propose VocBulwark, an additional-parameter injection framework that freezes generative model parameters to preserve perceptual quality. Specifically, we design a Temporal Adapter to deeply entangle watermarks with acoustic attributes, synergizing with a Coarse-to-Fine Gated Extractor to resist advanced attacks. Furthermore, we develop an Accuracy-Guided Optimization Curriculum that dynamically orchestrates gradient flow to resolve the optimization conflict between fidelity and robustness. Comprehensive experiments demonstrate that VocBulwark achieves high-capacity and high-fidelity watermarking, offering robust defense against complex practical scenarios, with resilience to Codec regenerations and variable-length manipulations.

</details>


### [55] [The Semantic Trap: Do Fine-tuned LLMs Learn Vulnerability Root Cause or Just Functional Pattern?](https://arxiv.org/abs/2601.22655)
*Feiyang Huang,Yuqiang Sun,Fan Zhang,Ziqi Yang,Han Liu,Yang Liu*

Main category: cs.CR

TL;DR: 研究发现微调后的大语言模型在软件漏洞检测中可能只是利用功能模式而非真正理解漏洞根因，存在"语义陷阱"现象


<details>
  <summary>Details</summary>
Motivation: 当前微调后的LLMs在漏洞检测中表现出色，但不确定这种提升是真正理解了漏洞根因还是仅仅利用了功能模式。研究者想要验证是否存在"语义陷阱"现象，即模型通过关联功能领域与漏洞可能性而非理解安全语义来获得高分

Method: 提出TrapEval评估框架，包含两个互补数据集：V2N（漏洞代码与无关良性代码配对）和V2P（漏洞代码与其修补版本配对）。使用该框架微调5个代表性LLMs，进行跨数据集测试、语义保留扰动测试，并使用CodeBLEU测量语义差距

Result: 尽管指标有所改善，但微调后的LLMs在区分漏洞代码与其修补版本时持续困难，在轻微语义保留变换下表现出严重的鲁棒性下降，当语义差距较小时严重依赖功能上下文捷径。这些发现表明当前微调实践未能传授真正的漏洞推理能力

Conclusion: 传统数据集上的高基准分数可能是虚幻的，掩盖了模型无法理解漏洞真正因果逻辑的问题。研究结果是一个警示：需要更严谨的评估方法来确保模型真正理解漏洞根因而非仅仅利用功能模式

Abstract: LLMs demonstrate promising performance in software vulnerability detection after fine-tuning. However, it remains unclear whether these gains reflect a genuine understanding of vulnerability root causes or merely an exploitation of functional patterns. In this paper, we identify a critical failure mode termed the "semantic trap," where fine-tuned LLMs achieve high detection scores by associating certain functional domains with vulnerability likelihood rather than reasoning about the underlying security semantics.To systematically evaluate this phenomenon, we propose TrapEval, a comprehensive evaluation framework designed to disentangle vulnerability root cause from functional pattern. TrapEval introduces two complementary datasets derived from real-world open-source projects: V2N, which pairs vulnerable code with unrelated benign code, and V2P, which pairs vulnerable code with its corresponding patched version, forcing models to distinguish near-identical code that differs only in subtle security-critical logic. Using TrapEval, we fine-tune five representative state-of-the-art LLMs across three model families and evaluate them under cross-dataset testing, semantic-preserving perturbations, and varying degrees of semantic gap measured by CodeBLEU.Our empirical results reveal that, despite improvements in metrics, fine-tuned LLMs consistently struggle to distinguish vulnerable code from its patched counterpart, exhibit severe robustness degradation under minor semantic-preserving transformations, and rely heavily on functional-context shortcuts when the semantic gap is small. These findings provide strong evidence that current fine-tuning practices often fail to impart true vulnerability reasoning. Our findings serve as a wake-up call: high benchmark scores on traditional datasets may be illusory, masking the model's inability to understand the true causal logic of vulnerabilities.

</details>


### [56] [RealSec-bench: A Benchmark for Evaluating Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2601.22706)
*Yanlin Wang,Ziyao Zhang,Chong Wang,Xinyi Xu,Mingwei Liu,Yong Wang,Jiachi Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: RealSec-bench是一个基于真实Java仓库构建的安全代码生成基准测试，包含105个实例，涵盖19种CWE类型。研究发现当前LLM在安全代码生成方面存在显著不足，RAG技术对安全提升有限，安全提示反而可能损害功能性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试主要关注功能性，使用合成漏洞或孤立评估，无法反映真实世界中功能与安全的复杂交互。需要基于真实软件仓库构建更贴近实际的评估基准。

Method: 采用多阶段流水线方法：1) 使用CodeQL进行系统SAST扫描；2) 基于LLM的误报消除；3) 人工专家验证。构建了包含105个实例的RealSec-bench基准，涵盖19种CWE类型。引入SecurePass@K复合指标同时评估功能正确性和安全性。

Result: 对5个流行LLM的评估显示：1) RAG技术能提升功能性但对安全性改善有限；2) 明确的安全提示常导致编译失败，损害功能性且不能可靠防止漏洞；3) 当前LLM在安全代码生成方面存在显著差距。

Conclusion: 当前LLM在安全代码生成能力上存在明显不足，功能正确性与安全性之间存在显著差距。需要开发更有效的方法来提升LLM生成安全代码的能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their proficiency in producing secure code remains a critical, under-explored area. Existing benchmarks often fall short by relying on synthetic vulnerabilities or evaluating functional correctness in isolation, failing to capture the complex interplay between functionality and security found in real-world software. To address this gap, we introduce RealSec-bench, a new benchmark for secure code generation meticulously constructed from real-world, high-risk Java repositories. Our methodology employs a multi-stage pipeline that combines systematic SAST scanning with CodeQL, LLM-based false positive elimination, and rigorous human expert validation. The resulting benchmark contains 105 instances grounded in real-word repository contexts, spanning 19 Common Weakness Enumeration (CWE) types and exhibiting a wide diversity of data flow complexities, including vulnerabilities with up to 34-hop inter-procedural dependencies. Using RealSec-bench, we conduct an extensive empirical study on 5 popular LLMs. We introduce a novel composite metric, SecurePass@K, to assess both functional correctness and security simultaneously. We find that while Retrieval-Augmented Generation (RAG) techniques can improve functional correctness, they provide negligible benefits to security. Furthermore, explicitly prompting models with general security guidelines often leads to compilation failures, harming functional correctness without reliably preventing vulnerabilities. Our work highlights the gap between functional and secure code generation in current LLMs.

</details>


### [57] [AlienLM: Alienization of Language for API-Boundary Privacy in Black-Box LLMs](https://arxiv.org/abs/2601.22710)
*Jaehee Kim,Pilsung Kang*

Main category: cs.CR

TL;DR: AlienLM是一个API级别的隐私保护层，通过词汇级双射将文本转换为外星语言，客户端可无损恢复，仅需标准微调API即可让目标模型直接处理外星化输入。


<details>
  <summary>Details</summary>
Motivation: 现代LLM越来越多地通过黑盒API访问，用户需要将敏感提示、输出和微调数据传输给外部提供商，在API边界处造成了严重的隐私风险。

Method: AlienLM通过词汇级双射将文本转换为外星语言，使用Alien Adaptation Training（AAT）仅通过标准微调API使目标模型能够直接处理外星化输入。

Result: 在四个LLM骨干和七个基准测试中，AlienLM平均保留了超过81%的明文性能，显著优于随机双射和字符级基线。在攻击者拥有模型权重、语料统计和学习型逆翻译的情况下，恢复攻击仅能重建少于0.22%的外星化标记。

Conclusion: AlienLM为仅API访问下的隐私保护LLM部署提供了实用途径，在保持任务性能的同时显著减少了明文暴露。

Abstract: Modern LLMs are increasingly accessed via black-box APIs, requiring users to transmit sensitive prompts, outputs, and fine-tuning data to external providers, creating a critical privacy risk at the API boundary. We introduce AlienLM, a deployable API-only privacy layer that protects text by translating it into an Alien Language via a vocabulary-scale bijection, enabling lossless recovery on the client side. Using only standard fine-tuning APIs, Alien Adaptation Training (AAT) adapts target models to operate directly on alienized inputs. Across four LLM backbones and seven benchmarks, AlienLM retains over 81\% of plaintext-oracle performance on average, substantially outperforming random-bijection and character-level baselines. Under adversaries with access to model weights, corpus statistics, and learning-based inverse translation, recovery attacks reconstruct fewer than 0.22\% of alienized tokens. Our results demonstrate a practical pathway for privacy-preserving LLM deployment under API-only access, substantially reducing plaintext exposure while maintaining task performance.

</details>


### [58] [Rust and Go directed fuzzing with LibAFL-DiFuzz](https://arxiv.org/abs/2601.22772)
*Timofey Mezhuev,Darya Parygina,Daniil Kuts*

Main category: cs.CR

TL;DR: 该研究提出了一种针对Rust和Go语言的定向灰盒模糊测试新方法，通过编译器定制和高级预处理技术，在特定程序位置测试方面比现有工具表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着Rust和Go语言的流行，需要为这些语言提供精确高效的测试解决方案。传统覆盖引导的模糊测试在验证静态分析报告或复现崩溃等特定任务中效果有限，而定向模糊测试针对特定程序位置更为有效，但现有工具主要针对C/C++代码。

Method: 提出针对Rust和Go应用的定向灰盒模糊测试新方法，包括高级预处理技术、rustc编译器定制、详细的图构建和插桩方法，基于LibAFL-DiFuzz后端实现模糊测试工具。

Result: Rust-LibAFL-DiFuzz在TTE（暴露时间）实验中表现最佳，优于afl.rs、cargo-fuzz等现有工具。Go-LibAFL-DiFuzz在大多数情况下表现最佳，有两个案例显示出数量级差异，证明了方法的效率和准确性优势。

Conclusion: 该研究成功将定向模糊测试的应用范围扩展到C/C++之外，为Rust和Go语言提供了更有效的测试解决方案，在特定程序位置测试方面具有竞争优势。

Abstract: In modern SSDLC, program analysis and automated testing are essential for minimizing vulnerabilities before software release, with fuzzing being a fast and widely used dynamic testing method. However, traditional coverage-guided fuzzing may be less effective in specific tasks like verifying static analysis reports or reproducing crashes, while directed fuzzing, focusing on targeted program locations using proximity metrics, proves to be more effective. Some of the earliest directed fuzzers are, for example, AFLGo and BEACON, which use different proximity metric approaches. Although most automated testing tools focus on C/C++ code, the growing popularity of Rust and Go causes the need for precise and efficient testing solutions for these languages. This work expands the applicability of directed fuzzing beyond traditional analysis of C/C++ software. We present a novel approach to directed greybox fuzzing tailored specifically for Rust and Go applications. We introduce advanced preprocessing techniques, rustc compiler customizations, and elaborate graph construction and instrumentation methods to enable effective targeting of specific program locations. Our implemented fuzzing tools, based on LibAFL-DiFuzz backend, demonstrate competitive advantages compared to popular existing fuzzers like afl.rs, cargo-fuzz, and go-fuzz. According to TTE (Time to Exposure) experiments, Rust-LibAFL-DiFuzz outperforms other tools by the best TTE result. Some stability issues can be explained by different mutation approaches. Go-LibAFL-DiFuzz outperforms its opponent by the best and, in the majority of cases, by average result, having two cases with orders of magnitude difference. These results prove better efficiency and accuracy of our approach.

</details>


### [59] [Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models](https://arxiv.org/abs/2601.22818)
*Charles Westphal,Keivan Navaie,Fernando E. Rosas*

Main category: cs.CR

TL;DR: 该论文研究LLM微调中的隐写术攻击，提出低可恢复性隐写方案，并开发基于机制可解释性的检测方法


<details>
  <summary>Details</summary>
Motivation: 现有研究表明微调后的LLM可以通过隐写通道将提示秘密编码到输出中，但先前方法依赖易于恢复的编码。需要研究更隐蔽的隐写方案及其检测方法。

Method: 1) 形式化有效载荷可恢复性概念；2) 提出低可恢复性隐写术，用嵌入空间导出的映射替换任意映射；3) 提出基于机制可解释性的检测方法，使用线性探针分析深层激活

Result: 在Llama-8B和Ministral-8B上，精确秘密恢复率分别提升78%和80%；在Llama-70B上提升123%。同时降低有效载荷可恢复性。线性探针检测精度比基础模型高33%。

Conclusion: 恶意微调会在模型内部留下可操作的签名，基于机制可解释性的检测方法能有效识别低可恢复性隐写攻击，为防御提供了新方向。

Abstract: Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\% recoverability. In response, we introduce low-recoverability steganography, replacing arbitrary mappings with embedding-space-derived ones. For Llama-8B (LoRA) and Ministral-8B (LoRA) trained on TrojanStego prompts, exact secret recovery rises from 17$\rightarrow$30\% (+78\%) and 24$\rightarrow$43\% (+80\%) respectively, while on Llama-70B (LoRA) trained on Wiki prompts, it climbs from 9$\rightarrow$19\% (+123\%), all while reducing payload recoverability. We then discuss detection. We argue that detecting fine-tuning-based steganographic attacks requires approaches beyond traditional steganalysis. Standard approaches measure distributional shift, which is an expected side-effect of fine-tuning. Instead, we propose a mechanistic interpretability approach: linear probes trained on later-layer activations detect the secret with up to 33\% higher accuracy in fine-tuned models compared to base models, even for low-recoverability schemes. This suggests that malicious fine-tuning leaves actionable internal signatures amenable to interpretability-based defenses.

</details>


### [60] [Evaluating Large Language Models for Security Bug Report Prediction](https://arxiv.org/abs/2601.22921)
*Farnaz Soltaniani,Shoaib Razzaq,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 该研究评估了基于提示工程和微调两种方法使用大语言模型预测安全漏洞报告的性能，发现两者存在明显的权衡：提示方法召回率高但精度低，微调方法精度高但召回率低。


<details>
  <summary>Details</summary>
Motivation: 安全漏洞报告的早期检测对于及时修复漏洞至关重要，研究旨在探索如何利用大语言模型更有效地预测安全漏洞报告。

Method: 采用两种主要方法：1）基于提示工程的专有模型方法；2）微调模型方法。在多个数据集上对这两种方法进行评估比较。

Result: 提示方法平均G-measure为77%，召回率74%，但精度只有22%；微调方法平均G-measure为51%，精度75%，召回率36%。微调模型在最大数据集上的推理速度比专有模型快50倍。

Conclusion: 两种方法各有优劣：提示方法对安全漏洞报告更敏感但误报率高，微调方法精度高但召回率低。需要进一步研究以充分利用大语言模型进行安全漏洞报告预测。

Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.

</details>


### [61] [Protecting Private Code in IDE Autocomplete using Differential Privacy](https://arxiv.org/abs/2601.22935)
*Evgeny Grigorenko,David Stanojević,David Ilić,Egor Bogomolov,Kostadin Cvejoski*

Main category: cs.CR

TL;DR: 本文研究了在Kotlin代码补全LLM训练中使用差分隐私作为防御机制，证明DP能有效抵御成员推理攻击，同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 现代IDE使用LLM提供代码补全功能，但训练这些模型时使用用户代码会带来隐私风险，恶意攻击者可能重构敏感训练数据或推断特定代码片段是否用于训练

Method: 使用差分隐私训练Kotlin代码补全LLM，对Mellum模型进行DP微调，并全面评估其隐私性和实用性

Result: DP能有效防御成员推理攻击，将攻击成功率降至接近随机猜测水平（AUC从0.901降至0.606），同时DP训练模型在性能上与非私有模型相当，即使训练数据量少100倍

Conclusion: 差分隐私是构建私密且可信的AI驱动IDE功能的实用有效解决方案

Abstract: Modern Integrated Development Environments (IDEs) increasingly leverage Large Language Models (LLMs) to provide advanced features like code autocomplete. While powerful, training these models on user-written code introduces significant privacy risks, making the models themselves a new type of data vulnerability. Malicious actors can exploit this by launching attacks to reconstruct sensitive training data or infer whether a specific code snippet was used for training. This paper investigates the use of Differential Privacy (DP) as a robust defense mechanism for training an LLM for Kotlin code completion. We fine-tune a \texttt{Mellum} model using DP and conduct a comprehensive evaluation of its privacy and utility. Our results demonstrate that DP provides a strong defense against Membership Inference Attacks (MIAs), reducing the attack's success rate close to a random guess (AUC from 0.901 to 0.606). Furthermore, we show that this privacy guarantee comes at a minimal cost to model performance, with the DP-trained model achieving utility scores comparable to its non-private counterpart, even when trained on 100x less data. Our findings suggest that DP is a practical and effective solution for building private and trustworthy AI-powered IDE features.

</details>


### [62] [A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration](https://arxiv.org/abs/2601.22938)
*Huan Song,Shuyu Tian,Junyi Hao,Cheng Yuan,Zhenyu Jia,Jiawei Shao,Xuelong Li*

Main category: cs.CR

TL;DR: 提出基于AI Flow理论框架和边云协同架构的新型隐私保护感知技术，通过源端脱敏和不可逆特征映射，实现从视频监控到去身份行为感知的突破


<details>
  <summary>Details</summary>
Motivation: 智能感知扩展到高隐私环境（如卫生间、更衣室）时面临隐私-安全悖论：传统RGB监控存在视觉记录存储的隐私担忧，现有隐私保护方法要么损害语义理解能力，要么无法保证数学不可逆性对抗重建攻击

Method: 基于AI Flow理论框架和边云协同架构，集成源端脱敏与不可逆特征映射。边缘设备利用信息瓶颈理论进行毫秒级处理，通过非线性映射和随机噪声注入将原始图像转换为抽象特征向量，构建单向信息流；云端平台使用多模态家族模型仅对这些抽象向量进行联合推理以检测异常行为

Result: 该方法在架构层面从根本上切断了隐私泄露路径，实现了从视频监控到去身份行为感知的突破，为高敏感性公共空间的风险管理提供了稳健解决方案

Conclusion: 提出的隐私保护感知技术通过不可逆特征映射和边云协同架构，在保护隐私的同时保持了语义理解能力，为高隐私环境下的智能感知提供了创新解决方案

Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.

</details>


### [63] [From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models](https://arxiv.org/abs/2601.22946)
*Farnaz Soltaniani,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现机器学习安全模型中存在数据重复导致的性能虚高问题，特别是在硬编码密钥检测任务中，数据泄漏会严重夸大模型的实际效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在软件安全任务中应用日益广泛，但训练和评估通常使用从互联网收集的大型数据集，这些数据集往往包含重复或高度相似的样本。当这些样本分布在训练集和测试集之间时，会导致数据泄漏，使模型能够记忆模式而非学习泛化能力。

Method: 研究调查了一个广泛使用的硬编码密钥基准数据集中的重复问题，分析了数据泄漏如何影响AI基密钥检测器的性能评估。

Result: 数据泄漏会显著夸大AI基密钥检测器报告的性能指标，导致对其真实世界有效性的误导性评估。

Conclusion: 机器学习安全模型的评估需要仔细处理数据集中的重复样本，避免数据泄漏导致的性能虚高，以确保模型评估结果能够真实反映其在现实场景中的有效性。

Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, allowing models to memorize patterns instead of learning to generalize. We investigate duplication in a widely used benchmark dataset of hard coded secrets and show how data leakage can substantially inflate the reported performance of AI-based secret detectors, resulting in a misleading picture of their real-world effectiveness.

</details>


### [64] [From Similarity to Vulnerability: Key Collision Attack on LLM Semantic Caching](https://arxiv.org/abs/2601.23088)
*Zhixiang Zhang,Zesen Liu,Yuchong Xie,Quanfeng Huang,Dongdong She*

Main category: cs.CR

TL;DR: 该论文将语义缓存键视为模糊哈希，揭示了性能（局部性）与安全（抗碰撞性）之间的固有权衡，并提出了首个针对缓存碰撞完整性风险的系统研究，开发了CacheAttack框架进行黑盒碰撞攻击。


<details>
  <summary>Details</summary>
Motivation: 语义缓存已成为扩展LLM应用的关键技术，但现有研究主要关注侧信道和隐私风险，缺乏对缓存碰撞引发的完整性风险的系统研究。作者发现语义缓存键作为模糊哈希，其最大化缓存命中率所需的局部性与密码学雪崩效应所需的抗碰撞性存在根本冲突。

Method: 将语义缓存键概念化为模糊哈希，形式化分析性能与安全之间的权衡。开发CacheAttack自动化框架，用于发起黑盒碰撞攻击，并在安全关键任务和智能体工作流中进行评估，测试不同嵌入模型的迁移性。

Result: CacheAttack在LLM响应劫持中达到86%的命中率，能够诱导LLM智能体产生恶意行为，并在不同嵌入模型间保持强迁移性。金融智能体案例研究进一步展示了这些漏洞的实际影响。

Conclusion: 语义缓存天然存在密钥碰撞攻击的脆弱性，揭示了性能与安全之间的根本权衡。研究首次系统分析了缓存碰撞的完整性风险，并讨论了缓解策略。

Abstract: Semantic caching has emerged as a pivotal technique for scaling LLM applications, widely adopted by major providers including AWS and Microsoft. By utilizing semantic embedding vectors as cache keys, this mechanism effectively minimizes latency and redundant computation for semantically similar queries. In this work, we conceptualize semantic cache keys as a form of fuzzy hashes. We demonstrate that the locality required to maximize cache hit rates fundamentally conflicts with the cryptographic avalanche effect necessary for collision resistance. Our conceptual analysis formalizes this inherent trade-off between performance (locality) and security (collision resilience), revealing that semantic caching is naturally vulnerable to key collision attacks.
  While prior research has focused on side-channel and privacy risks, we present the first systematic study of integrity risks arising from cache collisions. We introduce CacheAttack, an automated framework for launching black-box collision attacks. We evaluate CacheAttack in security-critical tasks and agentic workflows. It achieves a hit rate of 86\% in LLM response hijacking and can induce malicious behaviors in LLM agent, while preserving strong transferability across different embedding models. A case study on a financial agent further illustrates the real-world impact of these vulnerabilities. Finally, we discuss mitigation strategies.

</details>


### [65] [WiFiPenTester: Advancing Wireless Ethical Hacking with Governed GenAI](https://arxiv.org/abs/2601.23092)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: WiFiPenTester是一个实验性的、受治理的、可复现的GenAI无线伦理黑客系统，通过集成大语言模型改进无线安全评估的目标选择和策略推荐，同时保持严格的人工监督和治理机制。


<details>
  <summary>Details</summary>
Motivation: 传统无线伦理黑客依赖人工解释侦察结果和执行复杂命令序列，存在劳动密集、难以扩展、主观判断和人为错误等问题，需要更智能、可扩展的解决方案。

Method: 开发WiFiPenTester系统，将大语言模型集成到无线安全评估的侦察和决策支持阶段，实现智能目标排名、攻击可行性评估和策略推荐，同时保持严格的人工监督和预算感知执行。

Result: 实验结果表明，GenAI辅助提高了目标选择的准确性和整体评估效率，同时保持了可审计性和伦理保障，证明了系统在实用、安全和可扩展的GenAI辅助无线渗透测试方面的有效性。

Conclusion: WiFiPenTester是迈向实用、安全、可扩展的GenAI辅助无线渗透测试的重要一步，同时强调了在伦理黑客中部署GenAI时需要有限自治、人工监督和严格治理机制的必要性。

Abstract: Wireless ethical hacking relies heavily on skilled practitioners manually interpreting reconnaissance results and executing complex, time-sensitive sequences of commands to identify vulnerable targets, capture authentication handshakes, and assess password resilience; a process that is inherently labour-intensive, difficult to scale, and prone to subjective judgement and human error. To help address these limitations, we propose WiFiPenTester, an experimental, governed, and reproducible system for GenAI-enabled wireless ethical hacking. The system integrates large language models into the reconnaissance and decision-support phases of wireless security assessment, enabling intelligent target ranking, attack feasibility estimation, and strategy recommendation, while preserving strict human-in-the-loop control and budget-aware execution. We describe the system architecture, threat model, governance mechanisms, and prompt-engineering methodology, and empirical experiments conducted across multiple wireless environments. The results demonstrate that GenAI assistance improves target selection accuracy and overall assessment efficiency, while maintaining auditability and ethical safeguards. This indicates that WiFiPenTester is a meaningful step toward practical, safe, and scalable GenAI-assisted wireless penetration testing, while reinforcing the necessity of bounded autonomy, human oversight, and rigorous governance mechanisms when deploying GenAI in ethical hacking.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [66] [RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning](https://arxiv.org/abs/2601.22476)
*Ruizhe Zhong,Xingbo Du,Junchi Yan*

Main category: cs.AR

TL;DR: 提出基于深度强化学习的统一框架，解决3D集成电路布局中复杂设计规则约束问题，无需人工后处理


<details>
  <summary>Details</summary>
Motivation: 随着技术节点缩小，3D多层堆叠集成电路布局中遵守复杂硬件设计规则变得日益困难。现有方法只能处理有限规则，其他规则违反需要专家工程师进行费时费力的人工调整

Method: 1) 设计新颖的矩阵表示来建模设计规则；2) 在动作空间上施加约束以过滤导致规则违反的无效动作；3) 将约束满足的定量分析作为奖励信号。这三个关键组件统一处理各种硬件设计规则

Result: 在公共基准测试上证明了方法的有效性和正确性，在未见电路上展示了良好的可迁移性

Conclusion: 该框架可扩展以适应新的设计规则，为未来芯片设计中的新兴挑战提供了灵活性，代码将开源

Abstract: Floorplanning determines the coordinate and shape of each module in Integrated Circuits. With the scaling of technology nodes, in floorplanning stage especially 3D scenarios with multiple stacked layers, it has become increasingly challenging to adhere to complex hardware design rules. Current methods are only capable of handling specific and limited design rules, while violations of other rules require manual and meticulous adjustment. This leads to labor-intensive and time-consuming post-processing for expert engineers. In this paper, we propose an all-in-one deep reinforcement learning-based approach to tackle these challenges, and design novel representations for real-world IC design rules that have not been addressed by previous approaches. Specifically, the processing of various hardware design rules is unified into a single framework with three key components: 1) novel matrix representations to model the design rules, 2) constraints on the action space to filter out invalid actions that cause rule violations, and 3) quantitative analysis of constraint satisfaction as reward signals. Experiments on public benchmarks demonstrate the effectiveness and validity of our approach. Furthermore, transferability is well demonstrated on unseen circuits. Our framework is extensible to accommodate new design rules, thus providing flexibility to address emerging challenges in future chip design. Code will be available at: https://github.com/Thinklab-SJTU/EDA-AI

</details>


### [67] [Design of a GPU with Heterogeneous Cores for Graphics](https://arxiv.org/abs/2601.22862)
*Aurora Tomás,Juan Luis Aragón,Joan Manuel Parcerisa,Antonio González*

Main category: cs.AR

TL;DR: KHEPRI提出了一种异构GPU架构，针对图形应用中的场景多样性，通过计算专用和内存专用两种核心以及智能调度器，实现了性能提升和能耗降低。


<details>
  <summary>Details</summary>
Motivation: 图形应用中的场景具有多样性，不同区域的计算强度和内存带宽需求差异显著。传统同构GPU无法有效处理这种变化，而异构架构可以通过专用核心优化不同类型的工作负载。

Method: KHEPRI采用异构GPU架构，包含两种核心：优化高ILP的计算专用核心和容忍更多缓存未命中的内存专用核心。关键创新是动态调度器，利用帧间一致性预测每个图块特性，并将其分配到最合适的核心，同时保持数据局部性。

Result: 在多种商业动画图形应用中评估显示，相比传统同构GPU，KHEPRI平均性能提升9.2%，帧率提升7.3%，GPU总能耗降低4.8%，且无需额外硬件开销。

Conclusion: KHEPRI成功将异构架构概念扩展到GPU领域，通过智能调度专用核心有效处理图形应用中的场景多样性，在无硬件开销的情况下实现了显著的性能提升和能耗降低。

Abstract: Heterogeneous architectures can deliver higher performance and energy efficiency than symmetric counterparts by using multiple architectures tuned to different types of workloads. While previous works focused on CPUs, this work extends the concept of heterogeneity to GPUs by proposing KHEPRI, a heterogeneous GPU architecture for graphics applications. Scenes in graphics applications showcase diversity, as they consist of many objects with varying levels of complexity. As a result, computational intensity and memory bandwidth requirements differ significantly across different regions of each scene. To address this variability, our proposal includes two types of cores: cores optimized for high ILP (compute-specialized) and cores that tolerate a higher number of simultaneously outstanding cache misses (memory-specialized). A key component of the proposed architecture is a novel work scheduler that dynamically assigns each part of a frame (i.e., a tile) to the most suitable core. Designing this scheduler is particularly challenging, as it must preserve data locality; otherwise, the benefits of heterogeneity may be offset by the penalty of additional cache misses. Additionally, the scheduler requires knowledge of each tile's characteristics before rendering it. For this purpose, KHEPRI leverages frame-to-frame coherence to predict the behavior of each tile based on that of the corresponding tile in the previous frame. Evaluations across a wide range of commercial animated graphics applications show that, compared to a traditional homogeneous GPU, KHEPRI achieves an average performance improvement of 9.2%, a throughput increase (frames per second) of 7.3%, and a total GPU energy reduction of 4.8%. Importantly, these benefits are achieved without any hardware overhead.

</details>


### [68] [Machine Learning for Energy-Performance-aware Scheduling](https://arxiv.org/abs/2601.23134)
*Zheyuan Hu,Yifei Shi*

Main category: cs.AR

TL;DR: 提出基于贝叶斯优化和高斯过程的框架，用于在异构多核架构上自动搜索最优调度配置，平衡能耗与延迟的权衡


<details>
  <summary>Details</summary>
Motivation: 在后Dennard时代，嵌入式系统优化需要在能耗效率和延迟之间进行复杂权衡。传统启发式调优在高维、非平滑的优化空间中效率低下，需要更智能的自动化方法。

Method: 使用贝叶斯优化框架，基于高斯过程建模，自动搜索异构多核架构的最优调度配置。通过多目标优化近似能耗与时间的Pareto前沿，并引入敏感性分析（fANOVA）和不同协方差核函数（如Matérn vs. RBF）比较，为黑盒模型提供物理可解释性。

Result: 该框架能够有效自动化调度配置搜索，揭示驱动系统性能的主导硬件参数，提供对优化过程的物理洞察，相比传统启发式方法更高效。

Conclusion: 贝叶斯优化结合高斯过程为嵌入式系统调度优化提供了有效的自动化框架，通过多目标优化和敏感性分析增强了模型的可解释性，有助于在实际系统中实现能耗与性能的最佳平衡。

Abstract: In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matérn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.

</details>
