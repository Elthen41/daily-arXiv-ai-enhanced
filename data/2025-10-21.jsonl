{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats."}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally."}
{"id": "2510.15959", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.15959", "abs": "https://arxiv.org/abs/2510.15959", "authors": ["Isabelle Hupont", "Marisa Ponti", "Sven Schade"], "title": "Exploring the Potential of Citiverses for Regulatory Learning", "comment": "26 pages", "summary": "Citiverses hold the potential to support regulatory learning by offering\nimmersive, virtual environments for experimenting with policy scenarios and\ntechnologies. This paper proposes a science-for-policy agenda to explore the\npotential of citiverses as experimentation spaces for regulatory learning,\ngrounded in a consultation with a high-level panel of experts, including\npolicymakers from the European Commission, national government science advisers\nand leading researchers in digital regulation and virtual worlds. It identifies\nkey research areas, including scalability, real-time feedback, complexity\nmodelling, cross-border collaboration, risk reduction, citizen participation,\nethical considerations and the integration of emerging technologies. In\naddition, the paper analyses a set of experimental topics, spanning\ntransportation, urban planning and the environment/climate crisis, that could\nbe tested in citiverse platforms to advance regulatory learning in these areas.\nThe proposed work is designed to inform future research for policy and\nemphasizes a responsible approach to developing and using citiverses. It\nprioritizes careful consideration of the ethical, economic, ecological and\nsocial dimensions of different regulations. The paper also explores essential\npreliminary steps necessary for integrating citiverses into the broader\necosystems of experimentation spaces, including test beds, living labs and\nregulatory sandboxes"}
{"id": "2510.15953", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15953", "abs": "https://arxiv.org/abs/2510.15953", "authors": ["Sisir Doppalapudi"], "title": "Hierarchical Multi-Modal Threat Intelligence Fusion Without Aligned Data: A Practical Framework for Real-World Security Operations", "comment": null, "summary": "Multi-modal threat detection faces a fundamental challenge that involves\nsecurity tools operating in isolation, and this creates streams of network,\nemail, and system data with no natural alignment or correlation. We present\nHierarchical Multi-Modal Threat Intelligence Fusion (HM-TIF), a framework\nexplicitly designed for this realistic scenario where naturally aligned\nmulti-modal attack data does not exist. Unlike prior work that assumes or\ncreates artificial alignment, we develop principled methods for correlating\nindependent security data streams while maintaining operational validity. Our\narchitecture employs hierarchical cross-attention with dynamic weighting that\nadapts to data availability and threat context, coupled with a novel temporal\ncorrelation protocol that preserves statistical independence. Evaluation on\nUNSW-NB15, CSE-CIC-IDS2018, and CICBell-DNS2021 datasets demonstrates that\nHM-TIF achieves 88.7% accuracy with a critical 32% reduction in false positive\nrates, even without true multi-modal training data. The framework maintains\nrobustness when modalities are missing, making it immediately deployable in\nreal security operations where data streams frequently have gaps. Our\ncontributions include: (i) the first multi-modal security framework explicitly\ndesigned for non-aligned data, (ii) a temporal correlation protocol that avoids\ncommon data leakage pitfalls, (iii) empirical validation that multi-modal\nfusion provides operational benefits even without perfect alignment, and (iv)\npractical deployment guidelines for security teams facing heterogeneous,\nuncoordinated data sources. Index Terms: multi-modal learning, threat\nintelligence, non-aligned data, operational security, cross-attention\nmechanisms, practical deployment"}
{"id": "2510.15966", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15966", "abs": "https://arxiv.org/abs/2510.15966", "authors": ["Shian Jia", "Ziyang Huang", "Xinbo Wang", "Haofei Zhang", "Mingli Song"], "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency", "comment": null, "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks\nadaptability to diverse tasks and overlooks the constructive and task-oriented\nrole of AI agent memory. Drawing from Piaget's theory of cognitive development,\nwe propose PISA, a pragmatic, psych-inspired unified memory system that\naddresses these limitations by treating memory as a constructive and adaptive\nprocess. To enable continuous learning and adaptability, PISA introduces a\ntrimodal adaptation mechanism (i.e., schema updation, schema evolution, and\nschema creation) that preserves coherent organization while supporting flexible\nmemory updates. Building on these schema-grounded structures, we further design\na hybrid memory access architecture that seamlessly integrates symbolic\nreasoning with neural retrieval, significantly improving retrieval accuracy and\nefficiency. Our empirical evaluation, conducted on the existing LOCOMO\nbenchmark and our newly proposed AggQA benchmark for data analysis tasks,\nconfirms that PISA sets a new state-of-the-art by significantly enhancing\nadaptability and long-term knowledge retention."}
{"id": "2510.15971", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15971", "abs": "https://arxiv.org/abs/2510.15971", "authors": ["Md. Ifthekhar Hossain", "Kazi Abdullah Al Arafat", "Bryce Shepard", "Kayd Craig", "Imtiaz Parvez"], "title": "A Graph-Attentive LSTM Model for Malicious URL Detection", "comment": "Planned to be submitted", "summary": "Malicious URLs pose significant security risks as they facilitate phishing\nattacks, distribute malware, and empower attackers to deface websites.\nBlacklist detection methods fail to identify new or obfuscated URLs because\nthey depend on pre-existing patterns. This work presents a hybrid deep learning\nmodel named GNN-GAT-LSTM that combines Graph Neural Networks (GNNs) with Graph\nAttention Networks (GATs) and Long Short-Term Memory (LSTM) networks. The\nproposed architecture extracts both the structural and sequential patterns of\nthe features from data. The model transforms URLs into graphs through a process\nwhere characters become nodes that connect through edges. It applies one-hot\nencoding to represent node features. The model received training and testing\ndata from a collection of 651,191 URLs, which were classified into benign,\nphishing, defacement, and malware categories. The preprocessing stage included\nboth feature engineering and data balancing techniques, which addressed the\nclass imbalance issue to enhance model learning. The GNN-GAT-LSTM model\nachieved outstanding performance through its test accuracy of 0.9806 and its\nweighted F1-score of 0.9804. It showed excellent precision and recall\nperformance across most classes, particularly for benign and defacement URLs.\nOverall, the model provides an efficient and scalable system for detecting\nmalicious URLs while demonstrating strong potential for real-world\ncybersecurity applications."}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs."}
{"id": "2510.15973", "categories": ["cs.CR", "cs.AI", "cs.CY", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15973", "abs": "https://arxiv.org/abs/2510.15973", "authors": ["Tiarnaigh Downey-Webb", "Olamide Jogunola", "Oluwaseun Ajao"], "title": "Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts", "comment": "10 pages, 4 pages manuscript submitted to the Language Resources and\n  Evaluation Conference (LREC 2026)", "summary": "This paper presents a systematic security assessment of four prominent Large\nLanguage Models (LLMs) against diverse adversarial attack vectors. We evaluate\nPhi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack\ncategories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),\nand Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs\n1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six\nharm categories. Results demonstrate significant variations in model\nrobustness, with Llama-2 achieving the highest overall security (3.4% average\nattack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%\naverage attack success rate). We identify critical transferability patterns\nwhere GCG and TAP attacks, though ineffective against their target model\n(Llama-2), achieve substantially higher success rates when transferred to other\nmodels (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals\nsignificant differences in vulnerability across harm categories ($p < 0.001$),\nwith malicious use prompts showing the highest attack success rates (10.71%\naverage). Our findings contribute to understanding cross-model security\nvulnerabilities and provide actionable insights for developing targeted defense\nmechanisms"}
{"id": "2510.15980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15980", "abs": "https://arxiv.org/abs/2510.15980", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition", "comment": null, "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy."}
{"id": "2510.15975", "categories": ["cs.CR", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.15975", "abs": "https://arxiv.org/abs/2510.15975", "authors": ["Zaixi Zhang", "Souradip Chakraborty", "Amrit Singh Bedi", "Emilin Mathew", "Varsha Saravanan", "Le Cong", "Alvaro Velasquez", "Sheng Lin-Gibson", "Megan Blewett", "Dan Hendrycs", "Alex John London", "Ellen Zhong", "Ben Raphael", "Jian Ma", "Eric Xing", "Russ Altman", "George Church", "Mengdi Wang"], "title": "Generative AI for Biosciences: Emerging Threats and Roadmap to Biosecurity", "comment": null, "summary": "The rapid adoption of generative artificial intelligence (GenAI) in the\nbiosciences is transforming biotechnology, medicine, and synthetic biology. Yet\nthis advancement is intrinsically linked to new vulnerabilities, as GenAI\nlowers the barrier to misuse and introduces novel biosecurity threats, such as\ngenerating synthetic viral proteins or toxins. These dual-use risks are often\noverlooked, as existing safety guardrails remain fragile and can be\ncircumvented through deceptive prompts or jailbreak techniques. In this\nPerspective, we first outline the current state of GenAI in the biosciences and\nemerging threat vectors ranging from jailbreak attacks and privacy risks to the\ndual-use challenges posed by autonomous AI agents. We then examine urgent gaps\nin regulation and oversight, drawing on insights from 130 expert interviews\nacross academia, government, industry, and policy. A large majority ($\\approx\n76$\\%) expressed concern over AI misuse in biology, and 74\\% called for the\ndevelopment of new governance frameworks. Finally, we explore technical\npathways to mitigation, advocating a multi-layered approach to GenAI safety.\nThese defenses include rigorous data filtering, alignment with ethical\nprinciples during development, and real-time monitoring to block harmful\nrequests. Together, these strategies provide a blueprint for embedding security\nthroughout the GenAI lifecycle. As GenAI becomes integrated into the\nbiosciences, safeguarding this frontier requires an immediate commitment to\nboth adaptive governance and secure-by-design technologies."}
{"id": "2510.15872", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15872", "abs": "https://arxiv.org/abs/2510.15872", "authors": ["Yun-Da Tsai", "Chang-Yu Chao", "Liang-Yeh Shen", "Tsung-Han Lin", "Haoyu Yang", "Mark Ho", "Yi-Chen Lu", "Wen-Hao Liu", "Shou-De Lin", "Haoxing Ren"], "title": "Multimodal Chip Physical Design Engineer Assistant", "comment": null, "summary": "Modern chip physical design relies heavily on Electronic Design Automation\n(EDA) tools, which often struggle to provide interpretable feedback or\nactionable guidance for improving routing congestion. In this work, we\nintroduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this\ngap by not only predicting congestion but also delivering human-interpretable\ndesign suggestions. Our method combines automated feature generation through\nMLLM-guided genetic prompting with an interpretable preference learning\nframework that models congestion-relevant tradeoffs across visual, tabular, and\ntextual inputs. We compile these insights into a \"Design Suggestion Deck\" that\nsurfaces the most influential layout features and proposes targeted\noptimizations. Experiments on the CircuitNet benchmark demonstrate that our\napproach outperforms existing models on both accuracy and explainability.\nAdditionally, our design suggestion guidance case study and qualitative\nanalyses confirm that the learned preferences align with real-world design\nprinciples and are actionable for engineers. This work highlights the potential\nof MLLMs as interactive assistants for interpretable and context-aware physical\ndesign optimization."}
{"id": "2510.16284", "categories": ["cs.DC", "cs.MS", "cs.NA", "math.NA", "stat.CO", "65Y05, 65C60, 62F40", "F.2.2; G.3; D.1.3"], "pdf": "https://arxiv.org/pdf/2510.16284", "abs": "https://arxiv.org/abs/2510.16284", "authors": ["Di Zhang"], "title": "Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI", "comment": "6 pages", "summary": "Bootstrapping is a powerful statistical resampling technique for estimating\nthe sampling distribution of an estimator. However, its computational cost\nbecomes prohibitive for large datasets or a high number of resamples. This\npaper presents a theoretical analysis and design of parallel bootstrapping\nalgorithms using the Message Passing Interface (MPI). We address two key\nchallenges: high communication overhead and memory constraints in distributed\nenvironments. We propose two novel strategies: 1) Local Statistic Aggregation,\nwhich drastically reduces communication by transmitting sufficient statistics\ninstead of full resampled datasets, and 2) Synchronized Pseudo-Random Number\nGeneration, which enables distributed resampling when the entire dataset cannot\nbe stored on a single process. We develop analytical models for communication\nand computation complexity, comparing our methods against naive baseline\napproaches. Our analysis demonstrates that the proposed methods offer\nsignificant reductions in communication volume and memory usage, facilitating\nscalable parallel bootstrapping on large-scale systems."}
{"id": "2510.15981", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.15981", "abs": "https://arxiv.org/abs/2510.15981", "authors": ["Rafael Cabral", "Tuan Manh Do", "Xuejun Yu", "Wai Ming Tai", "Zijin Feng", "Xin Shen"], "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization", "comment": null, "summary": "Proof autoformalization, the task of translating natural language theorems\nand proofs into machine-verifiable code, is a critical step for integrating\nlarge language models into rigorous mathematical workflows. Current approaches\nfocus on producing executable code, but they frequently fail to preserve the\nsemantic meaning and logical structure of the original human-written argument.\nTo address this, we introduce ProofFlow, a novel pipeline that treats\nstructural fidelity as a primary objective. ProofFlow first constructs a\ndirected acyclic graph (DAG) to map the logical dependencies between proof\nsteps. Then, it employs a novel lemma-based approach to systematically\nformalize each step as an intermediate lemma, preserving the logical structure\nof the original argument. To facilitate evaluation, we present a new benchmark\nof 184 undergraduate-level problems, manually annotated with step-by-step\nsolutions and logical dependency graphs, and introduce ProofScore, a new\ncomposite metric to evaluate syntactic correctness, semantic faithfulness, and\nstructural fidelity. Experimental results show our pipeline sets a new\nstate-of-the-art for autoformalization, achieving a ProofScore of 0.545,\nsubstantially exceeding baselines like full-proof formalization (0.123), which\nprocesses the entire proof at once, and step-proof formalization (0.072), which\nhandles each step independently. Our pipeline, benchmark, and score metric are\nopen-sourced to encourage further progress at\nhttps://github.com/Huawei-AI4Math/ProofFlow."}
{"id": "2510.15976", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15976", "abs": "https://arxiv.org/abs/2510.15976", "authors": ["Chenrui Wang", "Junyi Shu", "Billy Chiu", "Yu Li", "Saleh Alharbi", "Min Zhang", "Jing Li"], "title": "Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization", "comment": "28 pages, 11 figures, NeurIPS 2025 Poster", "summary": "The rapid development of LLMs has raised concerns about their potential\nmisuse, leading to various watermarking schemes that typically offer high\ndetectability. However, existing watermarking techniques often face trade-off\nbetween watermark detectability and generated text quality. In this paper, we\nintroduce Learning to Watermark (LTW), a novel selective watermarking framework\nthat leverages multi-objective optimization to effectively balance these\ncompeting goals. LTW features a lightweight network that adaptively decides\nwhen to apply the watermark by analyzing sentence embeddings, token entropy,\nand current watermarking ratio. Training of the network involves two\nspecifically constructed loss functions that guide the model toward\nPareto-optimal solutions, thereby harmonizing watermark detectability and text\nquality. By integrating LTW with two baseline watermarking methods, our\nexperimental evaluations demonstrate that LTW significantly enhances text\nquality without compromising detectability. Our selective watermarking approach\noffers a new perspective for designing watermarks for LLMs and a way to\npreserve high text quality for watermarks. The code is publicly available at:\nhttps://github.com/fattyray/learning-to-watermark"}
{"id": "2510.15878", "categories": ["cs.AR", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15878", "abs": "https://arxiv.org/abs/2510.15878", "authors": ["David A. Roberts"], "title": "Putting the Context back into Memory", "comment": null, "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."}
{"id": "2510.16415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16415", "abs": "https://arxiv.org/abs/2510.16415", "authors": ["Rizhen Hu", "Yutong He", "Ran Yan", "Mou Sun", "Binghang Yuan", "Kun Yuan"], "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization", "comment": "NeurIPS 2025 poster", "summary": "As distributed optimization scales to meet the demands of Large Language\nModel (LLM) training, hardware failures become increasingly non-negligible.\nExisting fault-tolerant training methods often introduce significant\ncomputational or memory overhead, demanding additional resources. To address\nthis challenge, we propose Memory- and Computation-efficient Fault-tolerant\nOptimization (MeCeFO), a novel algorithm that ensures robust training with\nminimal overhead. When a computing node fails, MeCeFO seamlessly transfers its\ntraining task to a neighboring node while employing memory- and\ncomputation-efficient algorithmic optimizations to minimize the extra workload\nimposed on the neighboring node handling both tasks. MeCeFO leverages three key\nalgorithmic designs: (i) Skip-connection, which drops the multi-head attention\n(MHA) module during backpropagation for memory- and computation-efficient\napproximation; (ii) Recomputation, which reduces activation memory in\nfeedforward networks (FFNs); and (iii) Low-rank gradient approximation,\nenabling efficient estimation of FFN weight matrix gradients. Theoretically,\nMeCeFO matches the convergence rate of conventional distributed training, with\na rate of $\\mathcal{O}(1/\\sqrt{nT})$, where n is the data parallelism size and\nT is the number of iterations. Empirically, MeCeFO maintains robust performance\nunder high failure rates, incurring only a 4.18% drop in throughput,\ndemonstrating 5.0$\\times$ to 6.7$\\times$ greater resilience than previous SOTA\napproaches. Codes are available at https://github.com/pkumelon/MeCeFO."}
{"id": "2510.15983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15983", "abs": "https://arxiv.org/abs/2510.15983", "authors": ["Sarah Rebecca Ondraszek", "Jörg Waitelonis", "Katja Keller", "Claudia Niessner", "Anna M. Jacyszyn", "Harald Sack"], "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science", "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th\n  International Workshop on Scientific Knowledge: Representation, Discovery,\n  and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th\n  International Semantic Web Conference, ISWC 2025. To be published in CEUR\n  proceedings", "summary": "An essential component for evaluating and comparing physical and cognitive\ncapabilities between populations is the testing of various factors related to\nhuman performance. As a core part of sports science research, testing motor\nperformance enables the analysis of the physical health of different\ndemographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe\nInstitute of Technology, is an infrastructure for publishing and archiving\nresearch data in sports science, particularly in the field of motor performance\nresearch. In this paper, we present our vision for creating a knowledge graph\nfrom MO|RE data. With an ontology rooted in the Basic Formal Ontology, our\napproach centers on formally representing the interrelation of plan\nspecifications, specific processes, and related measurements. Our goal is to\ntransform how motor performance data are modeled and shared across studies,\nmaking it standardized and machine-understandable. The idea presented here is\ndeveloped within the Leibniz Science Campus ``Digital Transformation of\nResearch'' (DiTraRe)."}
{"id": "2510.15989", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15989", "abs": "https://arxiv.org/abs/2510.15989", "authors": ["Keshav Sood", "Sanjay Selvaraj", "Youyang Qu"], "title": "Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies", "comment": null, "summary": "The use of Immersive Technologies has shown its potential to revolutionize\nmany sectors such as health, entertainment, education, and industrial sectors.\nImmersive technologies such as Virtual Reality (VR), Augmented reality (AR),\nand Mixed Reality (MR) have redefined user interaction through real-time\nbiometric and behavioral tracking. Although Immersive Technologies (XR)\nessentially need the collection of the biometric data which acts as a baseline\nto create immersive experience, however, this ongoing feedback information\n(includes biometrics) poses critical privacy concerns due to the sensitive\nnature of the data collected. A comprehensive review of recent literature\nexplored the technical dimensions of related problem; however, they largely\noverlook the challenge particularly the intricacies of real-time biometric data\nfiltering within head-mounted display system. Motivated from this, in this\nwork, we propose a novel privacy-preserving system architecture that identifies\nand filters biometric signals (within the VR headset) in real-time before\ntransmission or storage. Implemented as a modular Unity Software-development\nKit (SDK) compatible with major immersive platforms, our solution (named\nMeta-Guardian) employs machine learning models for signal classification and a\nfiltering mechanism to block sensitive data. This framework aims to enable\ndevelopers to embed privacy-by-design principles into immersive experiences on\nvarious headsets and applications."}
{"id": "2510.15880", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15880", "abs": "https://arxiv.org/abs/2510.15880", "authors": ["Philip Emma", "Eren Kurshan"], "title": "Opportunities and Challenges for 3D Systems and Their Design", "comment": "IEEE Design and Computers", "summary": "Although it is not a new concept, 3D integration increasingly receives\nwidespread interest and focus as lithographic scaling becomes more challenging,\nand as the ability to make miniature vias greatly improves. Like Moores law, 3D\nintegration improves density. With improvements in packaging density, however,\ncome the challenges associated with its inherently higher power density. And\nthough it acts somewhat as a scaling accelerator, the vertical integration also\nposes new challenges to design and manufacturing technologies. The placement of\ncircuits, vias, and macros in the planes of a 3D stack must be co-designed\nacross layers (or must conform to new standards) so that, when assembled, they\nhave correct spatial correspondence. Each layer, although perhaps being a mere\nfunctional slice through a system (and we can slice the system in many\ndifferent ways), must be independently testable so that we can systematically\ntest and diagnose subsystems before and after final assembly. When those layers\nare assembled, they must come together in a way that enables a sensible yield\nand facilitates testing the finished product. To make the most of 3D\nintegration, we should articulate the leverages of 3D systems (other\nresearchers offer a more complete treatment elsewhere). Then we can enumerate\nand elucidate many of the new challenges posed by the design, assembly, and\ntest of 3D systems."}
{"id": "2510.16418", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16418", "abs": "https://arxiv.org/abs/2510.16418", "authors": ["Jian Ma", "Xinchen Lyu", "Jun Jiang", "Longhao Zou", "Chenshan Ren", "Qimei Cui", "Xiaofeng Tao"], "title": "FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference", "comment": null, "summary": "Collaborative large language model (LLM) inference enables real-time,\nprivacy-preserving AI services on resource-constrained edge devices by\npartitioning computational workloads between client devices and edge servers.\nHowever, this paradigm is severely hindered by communication bottlenecks caused\nby the transmission of high-dimensional intermediate activations, exacerbated\nby the autoregressive decoding structure of LLMs, where bandwidth consumption\nscales linearly with output length. Existing activation compression methods\nstruggle to simultaneously achieve high compression ratios, low reconstruction\nerror, and computational efficiency. This paper proposes FourierCompress, a\nnovel, layer-aware activation compression framework that exploits the\nfrequency-domain sparsity of LLM activations. We rigorously demonstrate that\nactivations from the first Transformer layer exhibit strong smoothness and\nenergy concentration in the low-frequency domain, making them highly amenable\nto near-lossless compression via the Fast Fourier Transform (FFT).\nFourierCompress transforms activations into the frequency domain, retains only\na compact block of low-frequency coefficients, and reconstructs the signal at\nthe server using conjugate symmetry, enabling seamless hardware acceleration on\nDSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10\ncommonsense reasoning datasets demonstrate that FourierCompress preserves\nperformance remarkably close to the uncompressed baseline, outperforming Top-k,\nQR, and SVD. FourierCompress bridges the gap between communication efficiency\n(an average 7.6x reduction in activation size), near-lossless inference (less\nthan 0.3% average accuracy loss), and significantly faster compression\n(achieving over 32x reduction in compression time compared to Top-k via\nhardware acceleration) for edge-device LLM inference."}
{"id": "2510.16001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16001", "abs": "https://arxiv.org/abs/2510.16001", "authors": ["Ruolan Cheng", "Yong Deng", "Enrique Herrera-Viedma"], "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets", "comment": null, "summary": "Random permutation set (RPS) is a new formalism for reasoning with\nuncertainty involving order information. Measuring the conflict between two\npieces of evidence represented by permutation mass functions remains an urgent\nresearch topic in order-structured uncertain information fusion. In this paper,\na detailed analysis of conflicts in RPS is carried out from two different\nperspectives: random finite set (RFS) and Dempster-Shafer theory (DST).\nStarting from the observation of permutations, we first define an inconsistency\nmeasure between permutations inspired by the rank-biased overlap(RBO) measure\nand further propose a non-overlap-based conflict measure method for RPSs. This\npaper regards RPS theory (RPST) as an extension of DST. The order information\nnewly added in focal sets indicates qualitative propensity, characterized by\ntop-ranked elements occupying a more critical position. Some numerical examples\nare used to demonstrate the behavior and properties of the proposed conflict\nmeasure. The proposed method not only has the natural top-weightedness property\nand can effectively measure the conflict between RPSs from the DST view but\nalso provides decision-makers with a flexible selection of weights, parameters,\nand truncated depths."}
{"id": "2510.15994", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15994", "abs": "https://arxiv.org/abs/2510.15994", "authors": ["Dongsen Zhang", "Zekun Li", "Xu Luo", "Xuannan Liu", "Peipei Li", "Wenjun Xu"], "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents", "comment": null, "summary": "The Model Context Protocol (MCP) standardizes how large language model (LLM)\nagents discover, describe, and call external tools. While MCP unlocks broad\ninteroperability, it also enlarges the attack surface by making tools\nfirst-class, composable objects with natural-language metadata, and\nstandardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end\nevaluation suite that systematically measures how well LLM agents resist\nMCP-specific attacks throughout the full tool-use pipeline: task planning, tool\ninvocation, and response handling. MSB contributes: (1) a taxonomy of 12\nattacks including name-collision, preference manipulation, prompt injections\nembedded in tool descriptions, out-of-scope parameter requests,\nuser-impersonating responses, false-error escalation, tool-transfer, retrieval\ninjection, and mixed attacks; (2) an evaluation harness that executes attacks\nby running real tools (both benign and malicious) via MCP rather than\nsimulation; and (3) a robustness metric that quantifies the trade-off between\nsecurity and performance: Net Resilient Performance (NRP). We evaluate nine\npopular LLM agents across 10 domains and 400+ tools, producing 2,000 attack\ninstances. Results reveal the effectiveness of attacks against each stage of\nMCP. Models with stronger performance are more vulnerable to attacks due to\ntheir outstanding tool calling and instruction following capabilities. MSB\nprovides a practical baseline for researchers and practitioners to study,\ncompare, and harden MCP agents."}
{"id": "2510.15882", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15882", "abs": "https://arxiv.org/abs/2510.15882", "authors": ["Ao Shen", "Rui Zhang", "Junping Zhao"], "title": "FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern", "comment": null, "summary": "As large language models (LLMs) continue to scale, multi-node deployment has\nbecome a necessity. Consequently, communication has become a critical\nperformance bottleneck. Current intra-node communication libraries, like NCCL,\ntypically make use of a single interconnect such as NVLink. This approach\ncreates performance ceilings, especially on hardware like the H800 GPU where\nthe primary interconnect's bandwidth can become a bottleneck, and leaves other\nhardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable\nNetwork Interface Cards (NICs) largely idle during intensive workloads. We\npropose FlexLink, the first collective communication framework to the best of\nour knowledge designed to systematically address this by aggregating these\nheterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance\ncommunication fabric. FlexLink employs an effective two-stage adaptive load\nbalancing strategy that dynamically partitions communication traffic across all\navailable links, ensuring that faster interconnects are not throttled by slower\nones. On an 8-GPU H800 server, our design improves the bandwidth of collective\noperators such as AllReduce and AllGather by up to 26% and 27% over the NCCL\nbaseline, respectively. This gain is achieved by offloading 2-22% of the total\ncommunication traffic to the previously underutilized PCIe and RDMA NICs.\nFlexLink provides these improvements as a lossless, drop-in replacement\ncompatible with the NCCL API, ensuring easy adoption."}
{"id": "2510.16497", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16497", "abs": "https://arxiv.org/abs/2510.16497", "authors": ["Pacome Simon Mbonimpa", "Diane Tuyizere", "Azizuddin Ahmed Biyabani", "Ozan K. Tonguz"], "title": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages", "comment": null, "summary": "This paper presents a novel framework for speech transcription and synthesis,\nleveraging edge-cloud parallelism to enhance processing speed and accessibility\nfor Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful\nlanguage processing tools for these widely spoken languages in East African\ncountries with limited technological infrastructure. The framework utilizes the\nWhisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and\ntext-to-speech (TTS) translation. The architecture uses a cascading mechanism\nthat distributes the model inference workload between the edge device and the\ncloud, thereby reducing latency and resource usage, benefiting both ends. On\nthe edge device, our approach achieves a memory usage compression of 9.5% for\nthe SpeechT5 model and 14% for the Whisper model, with a maximum memory usage\nof 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with\na 1 MB/s network bandwidth, the system can process a 270-character text in less\nthan a minute for both speech-to-text and text-to-speech transcription. Using\nreal-world survey data from Kenya, it is shown that the cascaded edge-cloud\narchitecture proposed could easily serve as an excellent platform for STT and\nTTS transcription with good accuracy and response time."}
{"id": "2510.16004", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16004", "abs": "https://arxiv.org/abs/2510.16004", "authors": ["Andreas Radler", "Vincent Seyfried", "Stefan Pirker", "Johannes Brandstetter", "Thomas Lichtenegger"], "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction", "comment": "22 pages, 16 figures", "summary": "Neural surrogates have shown great potential in simulating dynamical systems,\nwhile offering real-time capabilities. We envision Neural Twins as a\nprogression of neural surrogates, aiming to create digital replicas of real\nsystems. A neural twin consumes measurements at test time to update its state,\nthereby enabling context-specific decision-making. A critical property of\nneural twins is their ability to remain on-trajectory, i.e., to stay close to\nthe true system state over time. We introduce Parallel-in-time Neural Twins\n(PAINT), an architecture-agnostic family of methods for modeling dynamical\nsystems from measurements. PAINT trains a generative neural network to model\nthe distribution of states parallel over time. At test time, states are\npredicted from measurements in a sliding window fashion. Our theoretical\nanalysis shows that PAINT is on-trajectory, whereas autoregressive models\ngenerally are not. Empirically, we evaluate our method on a challenging\ntwo-dimensional turbulent fluid dynamics problem. The results demonstrate that\nPAINT stays on-trajectory and predicts system states from sparse measurements\nwith high fidelity. These findings underscore PAINT's potential for developing\nneural twins that stay on-trajectory, enabling more accurate state estimation\nand decision-making."}
{"id": "2510.16005", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16005", "abs": "https://arxiv.org/abs/2510.16005", "authors": ["Giacomo Bertollo", "Naz Bodemir", "Jonah Burgess"], "title": "Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers", "comment": null, "summary": "Analyzing 500 CTF participants, this paper shows that while participants\nreadily bypassed simple AI guardrails using common techniques, layered\nmulti-step defenses still posed significant challenges, offering concrete\ninsights for building safer AI systems."}
{"id": "2510.15884", "categories": ["cs.AR", "cs.MS"], "pdf": "https://arxiv.org/pdf/2510.15884", "abs": "https://arxiv.org/abs/2510.15884", "authors": ["Faizan A Khattak", "Mantas Mikaitis"], "title": "Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I", "comment": "Accepted for IEEE HPEC 2025", "summary": "Numerical features of matrix multiplier hardware units in NVIDIA and AMD data\ncentre GPUs have recently been studied. Features such as rounding,\nnormalisation, and internal precision of the accumulators are of interest. In\nthis paper, we extend the methodology for analysing those features, to\nconsumer-grade NVIDIA GPUs by implementing an architecture-independent test\nscheme for various input and output precision formats. Unlike current\napproaches, the proposed test vector generation method neither performs an\nexhaustive search nor relies on hard-coded {constants that are device-specific,\nyet remains applicable to a wide range of mixed-precision formats. We have\napplied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada\nLovelace architecture) graphics cards and determined numerical features of\nmatrix multipliers for binary16, TensorFloat32, and bfloat16 input floating\npoint formats and binary16 and binary32 IEEE 754 output formats. Our\nmethodology allowed us to determine that} the numerical features of RTX-3060, a\nconsumer-grade GPU, are identical to those of the A100, a data centre GPU. We\ndo not expect our code to require any changes for performing analysis of matrix\nmultipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future\nsuccessors, and any input/output format combination, including the latest 8-bit\nfloating-point formats."}
{"id": "2510.16606", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16606", "abs": "https://arxiv.org/abs/2510.16606", "authors": ["Ertza Warraich", "Ali Imran", "Annus Zulfiqar", "Shay Vargaftik", "Sonia Fahmy", "Muhammad Shahbaz"], "title": "Reimagining RDMA Through the Lens of ML", "comment": "4 pages", "summary": "As distributed machine learning (ML) workloads scale to thousands of GPUs\nconnected by ultra-high-speed inter-connects, tail latency in collective\ncommunication has emerged as a primary bottleneck. Prior RDMA designs, like\nRoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying\non retransmissions and packet sequencing to ensure correctness. While effective\nfor general-purpose workloads, these mechanisms introduce complexity and\nlatency that scale poorly, where even rare packet losses or delays can\nconsistently degrade system performance. We introduce Celeris, a\ndomain-specific RDMA transport that revisits traditional reliability guarantees\nbased on ML's tolerance for lost or partial data. Celeris removes\nretransmissions and in-order delivery from the RDMA NIC, enabling best-effort\ntransport that exploits the robustness of ML workloads. It retains congestion\ncontrol (e.g., DCQCN) and manages communication with software-level mechanisms\nsuch as adaptive timeouts and data prioritization, while shifting loss recovery\nto the ML pipeline (e.g., using the Hadamard Transform). Early results show\nthat Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by\n67%, and nearly doubles NIC resilience to faults -- delivering a resilient,\nscalable transport tailored for ML at cluster scale."}
{"id": "2510.16033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16033", "abs": "https://arxiv.org/abs/2510.16033", "authors": ["Junyu Ren", "Wensheng Gan", "Guangyu Zhang", "Wei Zhong", "Philip S. Yu"], "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis", "comment": "Preprint. 16 figures, 12 tables", "summary": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN"}
{"id": "2510.16024", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16024", "abs": "https://arxiv.org/abs/2510.16024", "authors": ["Abdulrahman Alhaidari", "Balaji Palanisamy", "Prashant Krishnamurthy"], "title": "On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation", "comment": "Published in the 7th Conference on Advances in Financial Technologies\n  (AFT 2025)", "summary": "Billions of dollars are lost every year in DeFi platforms by transactions\nexploiting business logic or accounting vulnerabilities. Existing defenses\nfocus on static code analysis, public mempool screening, attacker contract\ndetection, or trusted off-chain monitors, none of which prevents exploits\nsubmitted through private relays or malicious contracts that execute within the\nsame block. We present the first decentralized, fully on-chain learning\nframework that: (i) performs gas-prohibitive computation on Layer-2 to reduce\ncost, (ii) propagates verified model updates to Layer-1, and (iii) enables\ngas-bounded, low-latency inference inside smart contracts. A novel\nProof-of-Improvement (PoIm) protocol governs the training process and verifies\neach decentralized micro update as a self-verifying training transaction.\nUpdates are accepted by \\textit{PoIm} only if they demonstrably improve at\nleast one core metric (e.g., accuracy, F1-score, precision, or recall) on a\npublic benchmark without degrading any of the other core metrics, while\nadversarial proposals get financially penalized through an adaptable test set\nfor evolving threats. We develop quantization and loop-unrolling techniques\nthat enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs\n(with support for formally verified decision tree inference) within the\nEthereum block gas limit, while remaining bit-exact to their off-chain\ncounterparts, formally proven in Z3. We curate 298 unique real-world exploits\n(2020 - 2025) with 402 exploit transactions across eight EVM chains,\ncollectively responsible for \\$3.74 B in losses."}
{"id": "2510.15885", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.15885", "abs": "https://arxiv.org/abs/2510.15885", "authors": ["Dingcui Yu", "Zonghuan Yan", "Jialin Liu", "Yumiao Zhao", "Yanyun Wang", "Xinghui Duan", "Yina Lv", "Liang Shi"], "title": "ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices", "comment": null, "summary": "To facilitate the understanding and efficient enhancement of software and\nhardware design for consumer-grade zoned flash storage, ConZone is proposed as\nthe first emulator designed to model the resource constraints and architectural\nfeatures typical of such systems. It incorporates essential components commonly\ndeployed in consumer-grade devices, including limited logical to physical\nmapping caches, constrained write buffers, and hybrid flash media management.\nHowever, ConZone cannot be mounted with the file system due to the lack of\nin-place update capability, which is required by the metadata area of F2FS. To\nimprove the usability of the emulator, ConZone+ extends ConZone with support\nfor a block interface. We also provide a script to help the deployment and\nintroduces several enhancements over the original version. Users can explore\nthe internal architecture of consumer-grade zoned flash storage and integrate\ntheir optimizations with system software using ConZone+. We validate the\naccuracy of ConZone+ by comparing a hardware architecture representative of\nconsumer-grade zoned flash storage and comparing it with the state-of-the-art.\nIn addition, we conduct several case studies using ConZone+ to investigate the\ndesign of zoned storage and explore the inadequacies of the current file\nsystem."}
{"id": "2510.16890", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16890", "abs": "https://arxiv.org/abs/2510.16890", "authors": ["Jiří Klepl", "Martin Kruliš", "Matyáš Brabec"], "title": "Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Recent Advances in the Message Passing Interface (EuroMPI 2025),\n  and is available online at https://doi.org/10.1007/978-3-032-07194-1_3", "summary": "Message Passing Interface (MPI) has been a well-established technology in the\ndomain of distributed high-performance computing for several decades. However,\none of its greatest drawbacks is a rather ancient pure-C interface. It lacks\nmany useful features of modern languages (namely C++), like basic type-checking\nor support for generic code design. In this paper, we propose a novel\nabstraction for MPI, which we implemented as an extension of the C++ Noarr\nlibrary. It follows Noarr paradigms (first-class layout and traversal\nabstraction) and offers layout-agnostic design of MPI applications. We also\nimplemented a layout-agnostic distributed GEMM kernel as a case study to\ndemonstrate the usability and syntax of the proposed abstraction. We show that\nthe abstraction achieves performance comparable to the state-of-the-art MPI C++\nbindings while allowing for a more flexible design of distributed applications."}
{"id": "2510.16047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16047", "abs": "https://arxiv.org/abs/2510.16047", "authors": ["Ioan Hedea"], "title": "Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks", "comment": "8 pages 2 column, 11 figures. Bachelor's thesis", "summary": "Modern manufacturing systems must meet hard delivery deadlines while coping\nwith stochastic task durations caused by process noise, equipment variability,\nand human intervention. Traditional deterministic schedules break down when\nreality deviates from nominal plans, triggering costly last-minute repairs.\nThis thesis combines offline constraint-programming (CP) optimisation with\nonline temporal-network execution to create schedules that remain feasible\nunder worst-case uncertainty. First, we build a CP model of the flexible\njob-shop with per-job deadline tasks and insert an optimal buffer $\\Delta^*$ to\nobtain a fully pro-active baseline. We then translate the resulting plan into a\nSimple Temporal Network with Uncertainty (STNU) and verify dynamic\ncontrollability, which guarantees that a real-time dispatcher can retime\nactivities for every bounded duration realisation without violating resource or\ndeadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4\nbenchmark suite show that our hybrid approach eliminates 100\\% of deadline\nviolations observed in state-of-the-art meta-heuristic schedules, while adding\nonly 3--5\\% makespan overhead. Scalability experiments confirm that CP\nsolve-times and STNU checks remain sub-second on medium-size instances. The\nwork demonstrates how temporal-network reasoning can bridge the gap between\nproactive buffering and dynamic robustness, moving industry a step closer to\ntruly digital, self-correcting factories."}
{"id": "2510.16025", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16025", "abs": "https://arxiv.org/abs/2510.16025", "authors": ["Denis Ovichinnikov", "Hemant Kavadia", "Satya Keerti Chand Kudupudi", "Ilya Rempel", "Vineet Chadha", "Marty Franz", "Paul Master", "Craig Gentry", "Darlene Kindler", "Alberto Reyes", "Muthu Annamalai"], "title": "Resource Estimation of CGGI and CKKS scheme workloads on FracTLcore Computing Fabric", "comment": "5 tables, 2 figures, conference style", "summary": "Cornami Mx2 accelerates of Fully Homomorphic Encryption (FHE) applications,\nenabled by breakthrough work [1], which are otherwise compute limited. Our\nprocessor architecture is based on the systolic array of cores with in-memory\ncompute capability and a network on chip (NoC) processor architecture called\nthe \"FracTLcore compute fabric processor\" (Mx2). Here, we describe the work to\nestimate processor resources to compute workload in CGGI (TFHE-rs) or CKKS\nscheme during construction of our compiler backend for this architecture [2].\nThese processors are available for running applications in both the TFHE-rs\nBoolean scheme and CKKS scheme FHE applications."}
{"id": "2510.15887", "categories": ["cs.AR", "C.1.0; B.7.1"], "pdf": "https://arxiv.org/pdf/2510.15887", "abs": "https://arxiv.org/abs/2510.15887", "authors": ["Hyun Woo Kang", "Ji Woong Choi"], "title": "basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I", "comment": "2 pages, 3 figures. Accepted to ISOCC 2025 (submitted 14 Jul. 2025;\n  accepted 8 Aug. 2025). To appear in the Proceedings of ISOCC 2025; oral\n  presentation on 17 Oct. 2025 (conference opens 15 Oct 2025). Camera-ready\n  version. Project repository: https://github.com/RISC-KC/basic_rv32s", "summary": "This paper introduces BASIC_RV32s, an open-source framework providing a\npractical microarchitectural roadmap for the RISC-V RV32I architecture,\naddressing the gap between theoretical knowledge and hardware implementation.\nFollowing the classic Patterson and Hennessy methodology, the design evolves\nfrom a basic single-cycle core to a 5-stage pipelined core design with full\nhazard forwarding, dynamic branch prediction, and exception handling. For\nverification, the final core design is integrated into a System-on-Chip (SoC)\nwith Universal Asynchronous Receiver-Transmitter (UART) communication\nimplemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving\n1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50\nMHz. By releasing all Register-Transfer Level (RTL) source code, signal-level\nlogic block diagrams, and development logs under MIT license on GitHub,\nBASIC_RV32s offers a reproducible instructional pathway for the open-source\nhardware ecosystem."}
{"id": "2510.16896", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16896", "abs": "https://arxiv.org/abs/2510.16896", "authors": ["Yiming Hu"], "title": "FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems", "comment": null, "summary": "Two-Phase Triple Modular Redundancy TMR divides redundancy operations into\ntwo stages, omitting part of the computation during fault-free operation to\nreduce energy consumption. However, it becomes ineffective under permanent\nfaults, limiting its reliability in critical systems. To address this,\nReactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty\ncores, tolerating both transient and permanent faults. Yet, its reliance on\nadditional hardware increases system complexity and reduces fault tolerance\nwhen multiple cores or auxiliary modules fail. This paper proposes an\nintegrated fault-tolerant architecture for interconnected multicore systems. By\nconstructing a stability metric to identify reliable machines and performing\nperiodic diagnostics, the method enables permanent fault isolation and adaptive\ntask scheduling without extra hardware. Experimental results show that it\nreduces task workload by approximately 30% compared to baseline TMR and\nachieves superior fault coverage and isolation accuracy, significantly\nimproving both reliability and energy efficiency."}
{"id": "2510.16095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16095", "abs": "https://arxiv.org/abs/2510.16095", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study", "comment": null, "summary": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for\nexplainable medical Artificial Intelligence (AI) while constrained by data\nscarcity. Although Large Language Models (LLMs) can synthesize medical data,\ntheir clinical reliability remains unverified. This study evaluates the\nreliability of LLM-generated CoTs and investigates prompting strategies to\nenhance their quality. In a blinded comparative study, senior clinicians in\nAssisted Reproductive Technology (ART) evaluated CoTs generated via three\ndistinct strategies: Zero-shot, Random Few-shot (using shallow examples), and\nSelective Few-shot (using diverse, high-quality examples). These expert ratings\nwere compared against evaluations from a state-of-the-art AI model (GPT-4o).\nThe Selective Few-shot strategy significantly outperformed other strategies\nacross all human evaluation metrics (p < .001). Critically, the Random Few-shot\nstrategy offered no significant improvement over the Zero-shot baseline,\ndemonstrating that low-quality examples are as ineffective as no examples. The\nsuccess of the Selective strategy is attributed to two principles:\n\"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\"\n(generalization). Notably, the AI evaluator failed to discern these critical\nperformance differences. The clinical reliability of synthetic CoTs is dictated\nby strategic prompt curation, not the mere presence of examples. We propose a\n\"Dual Principles\" framework as a foundational methodology to generate\ntrustworthy data at scale. This work offers a validated solution to the data\nbottleneck and confirms the indispensable role of human expertise in evaluating\nhigh-stakes clinical AI."}
{"id": "2510.16028", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16028", "abs": "https://arxiv.org/abs/2510.16028", "authors": ["Jianzhu Yao", "Hongxu Su", "Taobo Liao", "Zerui Cheng", "Huan Zhang", "Xuechao Wang", "Pramod Viswanath"], "title": "Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks", "comment": "17 pages, 7 figures", "summary": "Neural networks increasingly run on hardware outside the user's control\n(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about\nwhat actually ran or whether returned outputs faithfully reflect the intended\ninputs. Users lack recourse against service downgrades (model swaps,\nquantization, graph rewrites, or discrepancies like altered ad embeddings).\nVerifying outputs is hard because floating-point(FP) execution on heterogeneous\naccelerators is inherently nondeterministic. Existing approaches are either\nimpractical for real FP neural networks or reintroduce vendor trust. We present\nNAO: a Nondeterministic tolerance Aware Optimistic verification protocol that\naccepts outputs within principled operator-level acceptance regions rather than\nrequiring bitwise equality. NAO combines two error models: (i) sound\nper-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile\nprofiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,\nthreshold-guided dispute game that recursively partitions the computation graph\nuntil one operator remains, where adjudication reduces to a lightweight\ntheoretical-bound check or a small honest-majority vote against empirical\nthresholds. Unchallenged results finalize after a challenge window, without\nrequiring trusted hardware or deterministic kernels. We implement NAO as a\nPyTorch-compatible runtime and a contract layer currently deployed on Ethereum\nHolesky testnet. The runtime instruments graphs, computes per-operator bounds,\nand runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on\nQwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,\nRTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than\ntheoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO\nreconciles scalability with verifiability for real-world heterogeneous ML\ncompute."}
{"id": "2510.15888", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15888", "abs": "https://arxiv.org/abs/2510.15888", "authors": ["Konstantinos Kafousis"], "title": "Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol", "comment": null, "summary": "Hardware Transactional Memory (HTM) allows lock-free programming as easy as\nwith traditional coarse-grain locks or similar, while benefiting from the\nperformance advantages of fine-grained locking. Many HTM implementations have\nbeen proposed, but they have not received widespread adoption because of their\nhigh hardware complexity, their need for additions to the Instruction Set\nArchitecture (ISA), and often for modifications to the cache coherence\nprotocol.\n  We show that HTM can be implemented without adding new instructions -- merely\nby extending the semantics of two existing, Load-Linked and Store-Conditional.\nAlso, our proposed design does not modify or extend standard coherence\nprotocols. We further propose to drastically simplify the implementation of HTM\n-- confined to modifications in the L1 Data Cache only -- by restricting it to\napplications where the write set plus the read set of each transaction do not\nexceed a small number of cache lines. We also propose two alternative\nmechanisms to guarantee forward progress, both based on detecting retrial\nattempts.\n  We simulated our proposed design in Gem5, and we used it to implement several\npopular concurrent data structures, showing that a maximum of eight (8) words\n(cache lines) suffice for the write plus read sets. We provide a detailed\nexplanation of selected implementations, clarifying the intended usage of our\nHTM from a programmer's perspective. We evaluated our HTM under varying\ncontention levels to explore its scalability limits. The results indicate that\nour HTM provides good performance in concurrent data structures when contention\nis spread across multiple nodes: in such cases, the percentage of aborts\nrelative to successful commits is very low. In the atomic fetch-and-increment\nbenchmark for multiple shared counters, the results show that, under\nlow-congestion, our HTM improves performance relative to the TTS lock."}
{"id": "2510.16933", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16933", "abs": "https://arxiv.org/abs/2510.16933", "authors": ["Matyáš Brabec", "Jiří Klepl", "Michal Töpfer", "Martin Kruliš"], "title": "Tutoring LLM into a Better CUDA Optimizer", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Euro-Par 2025: Parallel Processing, Part II, and is available\n  online at https://doi.org/10.1007/978-3-031-99857-7_18", "summary": "Recent leaps in large language models (LLMs) caused a revolution in\nprogramming tools (like GitHub Copilot) that can help with code generation,\ndebugging, and even performance optimization. In this paper, we focus on the\ncapabilities of the most recent reasoning models to generate optimized CUDA\ncode for predefined, well-known tasks. Our objective is to determine which\ntypes of code optimizations and parallel patterns the LLMs can perform by\nthemselves and whether they can be improved by tutoring (providing more\ndetailed hints and guidelines in the prompt). The generated solutions were\nevaluated both automatically (for correctness and speedup) and manually (code\nreviews) to provide a more detailed perspective. We also tried an interactive\napproach where the LLM can fix its previous mistakes within a session. The\nresults indicate that LLMs are quite skilled coders; however, they require\ntutoring to reach optimized solutions provided by parallel computing experts."}
{"id": "2510.16193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16193", "abs": "https://arxiv.org/abs/2510.16193", "authors": ["Elija Perrier"], "title": "Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability", "comment": "Under review", "summary": "Corporate responsibility turns on notions of corporate \\textit{mens rea},\ntraditionally imputed from human agents. Yet these assumptions are under\nchallenge as generative AI increasingly mediates enterprise decision-making.\nBuilding on the theory of extended cognition, we argue that in response\ncorporate knowledge may be redefined as a dynamic capability, measurable by the\nefficiency of its information-access procedures and the validated reliability\nof their outputs. We develop a formal model that captures epistemic states of\ncorporations deploying sophisticated AI or information systems, introducing a\ncontinuous organisational knowledge metric $S_S(\\varphi)$ which integrates a\npipeline's computational cost and its statistically validated error rate. We\nderive a thresholded knowledge predicate $\\mathsf{K}_S$ to impute knowledge and\na firm-wide epistemic capacity index $\\mathcal{K}_{S,t}$ to measure overall\ncapability. We then operationally map these quantitative metrics onto the legal\nstandards of actual knowledge, constructive knowledge, wilful blindness, and\nrecklessness. Our work provides a pathway towards creating measurable and\njusticiable audit artefacts, that render the corporate mind tractable and\naccountable in the algorithmic age."}
{"id": "2510.16037", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16037", "abs": "https://arxiv.org/abs/2510.16037", "authors": ["Peini Cheng", "Amir Bahmani"], "title": "Membership Inference over Diffusion-models-based Synthetic Tabular Data", "comment": null, "summary": "This study investigates the privacy risks associated with diffusion-based\nsynthetic tabular data generation methods, focusing on their susceptibility to\nMembership Inference Attacks (MIAs). We examine two recent models, TabDDPM and\nTabSyn, by developing query-based MIAs based on the step-wise error comparison\nmethod. Our findings reveal that TabDDPM is more vulnerable to these attacks.\nTabSyn exhibits resilience against our attack models. Our work underscores the\nimportance of evaluating the privacy implications of diffusion models and\nencourages further research into robust privacy-preserving mechanisms for\nsynthetic data generation."}
{"id": "2510.15893", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG", "68M10, 68M14", "B.4.3; C.2.4; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.15893", "abs": "https://arxiv.org/abs/2510.15893", "authors": ["Mikhail Bernadskiy", "Peter Carson", "Thomas Graham", "Taylor Groves", "Ho John Lee", "Eric Yeh"], "title": "Accelerating Frontier MoE Training with 3D Integrated Optics", "comment": "12 pages, 11 figures. To be published in Hot Interconnects 2025", "summary": "The unabated growth in AI workload demands is driving the need for concerted\nadvances in compute, memory, and interconnect performance. As traditional\nsemiconductor scaling slows, high-speed interconnects have emerged as the new\nscaling engine, enabling the creation of larger logical GPUs by linking many\nGPUs into a single, low-latency, high-bandwidth compute domain. While initial\nscale-up fabrics leveraged copper interconnects for their power and cost\nadvantages, the maximum reach of passive electrical interconnects\n(approximately 1 meter) effectively limits the scale-up domain to within a\nsingle rack. The advent of 3D-stacked optics and logic offers a transformative,\npower-efficient scale-up solution for connecting hundreds of GPU packages\n(thousands of GPUs) across multiple data center racks. This work explores the\ndesign tradeoffs of scale-up technologies and demonstrates how frontier LLMs\nnecessitate novel photonic solutions to achieve aggressive power and\nperformance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and\nswitches within the scale-up domain when training Frontier Mixture of Experts\n(MoE) models exceeding one trillion parameters. Our results show that the\nsubstantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X\nincrease in scale-up capability. This affords new opportunities for\nmulti-dimensional parallelism within the scale-up domain and results in a 2.7X\nreduction in time-to-train, unlocking unprecedented model scaling."}
{"id": "2510.16946", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16946", "abs": "https://arxiv.org/abs/2510.16946", "authors": ["Erfan Darzi", "Aldo Pareja", "Shreeanant Bharadwaj"], "title": "Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure", "comment": null, "summary": "Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is\ncritical for maintaining performance predictability and resource utilization,\nyet existing monitoring tools lack the granularity for root cause analysis in\nshared computing environments. We introduce an eBPF-based telemetry system that\nprovides unified host-side monitoring of GPU workloads, correlating\neBPF-derived host metrics with GPU-internal events for holistic system\nobservability. The system achieves 81--88\\% diagnostic accuracy, detects spikes\nwithin 5 seconds, and completes root cause analysis in 6--8 seconds, operating\nwith 1.21\\% CPU overhead at 100Hz sampling. Evaluated on distributed learning\nworkloads, the system identifies root causes including NIC contention, PCIe\npressure, and CPU interference, enabling operational debugging for multi-tenant\nGPU infrastructure without requiring cluster-wide instrumentation."}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited."}
{"id": "2510.16044", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16044", "abs": "https://arxiv.org/abs/2510.16044", "authors": ["Zeng Zhang", "Wenjie Yin", "Xiaoqi Li"], "title": "A Novel GPT-Based Framework for Anomaly Detection in System Logs", "comment": null, "summary": "Identification of anomalous events within system logs constitutes a pivotal\nelement within the frame- work of cybersecurity defense strategies. However,\nthis process faces numerous challenges, including the management of substantial\ndata volumes, the distribution of anomalies, and the precision of con-\nventional methods. To address this issue, the present paper puts forward a\nproposal for an intelligent detection method for system logs based on Genera-\ntive Pre-trained Transformers (GPT). The efficacy of this approach is\nattributable to a combination of structured input design and a Focal Loss op-\ntimization strategy, which collectively result in a substantial enhancement of\nthe performance of log anomaly detection. The initial approach involves the\nconversion of raw logs into event ID sequences through the use of the Drain\nparser. Subsequently, the Focal Loss loss function is employed to address the\nissue of class imbalance. The experimental re- sults demonstrate that the\noptimized GPT-2 model significantly outperforms the unoptimized model in a\nrange of key metrics, including precision, recall, and F1 score. In specific\ntasks, comparable or superior performance has been demonstrated to that of the\nGPT-3.5 API."}
{"id": "2510.15897", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15897", "abs": "https://arxiv.org/abs/2510.15897", "authors": ["Kien Le Trung", "Truong-Son Hy"], "title": "DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms", "comment": null, "summary": "Chip placement, the task of determining optimal positions of circuit modules\non a chip canvas, is a critical step in the VLSI design flow that directly\nimpacts performance, power consumption, and routability. Traditional methods\nrely on analytical optimization or reinforcement learning, which struggle with\nhard placement constraints or require expensive online training for each new\ncircuit design. To address these limitations, we introduce DiffPlace, a\nframework that formulates chip placement as a conditional denoising diffusion\nprocess, enabling transferable placement policies that generalize to unseen\ncircuit netlists without retraining. DiffPlace leverages the generative\ncapabilities of diffusion models to efficiently explore the vast space of\nplacement while conditioning on circuit connectivity and relative quality\nmetrics to identify optimal solutions globally. Our approach combines\nenergy-guided sampling with constrained manifold diffusion to ensure placement\nlegality, achieving extremely low overlap across all experimental scenarios.\nOur method bridges the gap between optimization-based and learning-based\napproaches, offering a practical path toward automated, high-quality chip\nplacement for modern VLSI design. Our source code is publicly available at:\nhttps://github.com/HySonLab/DiffPlace/"}
{"id": "2510.17158", "categories": ["cs.DC", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17158", "abs": "https://arxiv.org/abs/2510.17158", "authors": ["Daniel Nichols", "Konstantinos Parasyris", "Charles Jekel", "Abhinav Bhatele", "Harshitha Menon"], "title": "Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization", "comment": null, "summary": "Language models are now prevalent in software engineering with many\ndevelopers using them to automate tasks and accelerate their development. While\nlanguage models have been tremendous at accomplishing complex software\nengineering tasks, there are still many areas where they fail to deliver\ndesirable results, for instance code performance related tasks. Tasks like\noptimization depend on many complex data from the environment, hardware, etc.\nthat are not directly represented in source code. Recent efforts have seen\nlarge improvements in general code modeling tasks using chain-of-thought style\nreasoning, but these models still fail to comprehend how the environment\ninteracts with code performance. In this paper we propose a methodology to\ntrain language models that can interact with performance tools during their\nreasoning process. We then demonstrate how this methodology can be used to\ntrain a state-of-the-art GPU kernel optimization model."}
{"id": "2510.16206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16206", "abs": "https://arxiv.org/abs/2510.16206", "authors": ["Alex Zhavoronkov", "Dominika Wilczok", "Roman Yampolskiy"], "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI", "comment": null, "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory.To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful."}
{"id": "2510.16054", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16054", "abs": "https://arxiv.org/abs/2510.16054", "authors": ["Zheng Hui", "Yijiang River Dong", "Sanhanat Sivapiromrat", "Ehsan Shareghi", "Nigel Collier"], "title": "PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation", "comment": null, "summary": "When users submit queries to Large Language Models (LLMs), their prompts can\noften contain sensitive data, forcing a difficult choice: Send the query to a\npowerful proprietary LLM providers to achieving state-of-the-art performance\nand risk data exposure, or relying on smaller, local models guarantees data\nprivacy but often results in a degradation of task performance. Prior\napproaches have relied on static pipelines that use LLM rewriting, which\nshatters linguistic coherence and indiscriminately removes privacy-sensitive\ninformation, including task-critical content. We reformulate this challenge\n(Privacy-Conscious Delegation) as a sequential decision-making problem and\nintroduce a novel reinforcement learning (RL) framework called PrivacyPAD to\nsolve it. Our framework trains an agent to dynamically route text chunks,\nlearning a policy that optimally balances the trade-off between privacy leakage\nand task performance. It implicitly distinguishes between replaceable\nPersonally Identifiable Information (PII) (which it shields locally) and\ntask-critical PII (which it strategically sends to the remote model for maximal\nutility). To validate our approach in complex scenarios, we also introduce a\nnew medical dataset with high PII density. Our framework achieves a new\nstate-of-the-art on the privacy-utility frontier, demonstrating the necessity\nof learned, adaptive policies for deploying LLMs in sensitive environments."}
{"id": "2510.15899", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15899", "abs": "https://arxiv.org/abs/2510.15899", "authors": ["Kiran Thorat", "Jiahui Zhao", "Yaotian Liu", "Amit Hasan", "Hongwu Peng", "Xi Xie", "Bin Lei", "Caiwen Ding"], "title": "LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are gaining prominence in various fields, thanks\nto their ability to generate high- quality content from human instructions.\nThis paper delves into the field of chip design using LLMs, specifically in\nPower- Performance-Area (PPA) optimization and the generation of accurate\nVerilog codes for circuit designs. We introduce a novel framework VeriPPA\ndesigned to optimize PPA and generate Verilog code using LLMs. Our method\nincludes a two-stage process where the first stage focuses on improving the\nfunctional and syntactic correctness of the generated Verilog codes, while the\nsecond stage focuses on optimizing the Verilog codes to meet PPA constraints of\ncircuit designs, a crucial element of chip design. Our framework achieves an\n81.37% success rate in syntactic correctness and 62.06% in functional\ncorrectness for code genera- tion, outperforming current state-of-the-art\n(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework\nachieves 99.56% syntactic correctness and 43.79% functional correctness, also\nsurpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%\nfor functional correctness. Furthermore, Our framework able to optimize the PPA\nof the designs. These results highlight the potential of LLMs in handling\ncomplex technical areas and indicate an encouraging development in the\nautomation of chip design processes."}
{"id": "2510.17639", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.17639", "abs": "https://arxiv.org/abs/2510.17639", "authors": ["Alkida Balliu", "Sebastian Brandt", "Ole Gabsdil", "Dennis Olivetti", "Jukka Suomela"], "title": "On the Universality of Round Elimination Fixed Points", "comment": null, "summary": "Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC\n2020] has drawn attention to the following open question: are round elimination\nfixed points a universal technique for proving lower bounds? That is, given a\nlocally checkable problem $\\Pi$ that requires at least $\\Omega(\\log n)$ rounds\nin the deterministic LOCAL model, can we always find a relaxation $\\Pi'$ of\n$\\Pi$ that is a nontrivial fixed point for the round elimination technique [see\nSTOC 2016, PODC 2019]? If yes, then a key part of distributed computational\ncomplexity would be also decidable.\n  The key obstacle so far has been a certain family of homomorphism problems\n[ITCS 2022], which require $\\Omega(\\log n)$ rounds, but the only known proof is\nbased on Marks' technique [J.AMS 2016].\n  We develop a new technique for constructing round elimination lower bounds\nsystematically. Using so-called tripotent inputs we show that the\naforementioned homomorphism problems indeed admit a lower bound proof that is\nbased on round elimination fixed points. Hence we eliminate the only known\nobstacle for the universality of round elimination.\n  Yet we also present a new obstacle: we show that there are some problems with\ninputs that require $\\Omega(\\log n)$ rounds, yet there is no proof that is\nbased on relaxations to nontrivial round elimination fixed points. Hence round\nelimination cannot be a universal technique for problems with inputs (but it\nmight be universal for problems without inputs).\n  We also prove the first fully general lower bound theorem that is applicable\nto any problem, with or without inputs, that is a fixed point in round\nelimination. Prior results of this form were only able to handle certain very\nrestricted inputs."}
{"id": "2510.16234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16234", "abs": "https://arxiv.org/abs/2510.16234", "authors": ["Hanane Nour Moussa", "Patrick Queiroz Da Silva", "Daniel Adu-Ampratwum", "Alyson East", "Zitong Lu", "Nikki Puccetti", "Mingyi Xue", "Huan Sun", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "comment": null, "summary": "As AI tools become increasingly common for research ideation, robust\nevaluation is critical to ensure the validity and usefulness of generated\nideas. We introduce ScholarEval, a retrieval augmented evaluation framework\nthat assesses research ideas based on two fundamental criteria: soundness - the\nempirical validity of proposed methods based on existing literature, and\ncontribution - the degree of advancement made by the idea across different\ndimensions relative to prior research. To evaluate ScholarEval, we introduce\nScholarIdeas, the first expert-annotated dataset of multi-domain research ideas\nand reviews, comprised of 117 ideas across four disciplines: artificial\nintelligence, neuroscience, biochemistry, and ecology. Our evaluation shows\nthat ScholarEval achieves significantly higher coverage of points mentioned in\nthe human expert annotated rubrics in ScholarIdeas compared to all baselines.\nFurthermore, ScholarEval is consistently preferred over our strongest baseline\no4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,\nin terms of evaluation actionability, depth, and evidence support. Our\nlarge-scale user study also shows that ScholarEval significantly outperforms\ndeep research in literature engagement, idea refinement, and usefulness. We\nopenly release our code, dataset, and ScholarEval tool for the community to use\nand build on."}
{"id": "2510.16067", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16067", "abs": "https://arxiv.org/abs/2510.16067", "authors": ["Saurabh Deochake", "Ryan Murphy", "Jeremiah Gearheart"], "title": "A Multi-Cloud Framework for Zero-Trust Workload Authentication", "comment": "Cyber Security Experimentation and Test (CSET) at the Annual Computer\n  Security Applications Conference (ACSAC) 2025", "summary": "Static, long-lived credentials for workload authentication create untenable\nsecurity risks that violate Zero-Trust principles. This paper presents a\nmulti-cloud framework using Workload Identity Federation (WIF) and OpenID\nConnect (OIDC) for secretless authentication. Our approach uses\ncryptographically-verified, ephemeral tokens, allowing workloads to\nauthenticate without persistent private keys and mitigating credential theft.\nWe validate this framework in an enterprise-scale Kubernetes environment, which\nsignificantly reduces the attack surface. The model offers a unified solution\nto manage workload identities across disparate clouds, enabling future\nimplementation of robust, attribute-based access control."}
{"id": "2510.15902", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15902", "abs": "https://arxiv.org/abs/2510.15902", "authors": ["Shuhang Zhang", "Jelena Radulovic", "Thorsten Dworzak"], "title": "Fully Automated Verification Framework for Configurable IPs: From Requirements to Results", "comment": "DVCon Europe 2025", "summary": "The increasing competition in the semiconductor industry has created\nsignificant pressure to reduce chip prices while maintaining quality and\nreliability. Functional verification, particularly for configurable IPs, is a\nmajor contributor to development costs due to its complexity and\nresource-intensive nature. To address this, we propose a fully automated\nframework for requirements driven functional verification. The framework\nautomates key processes, including vPlan generation, testbench creation,\nregression execution, and reporting in a requirements management tool,\ndrastically reducing verification effort. This approach accelerates development\ncycles, minimizes human error, and enhances coverage, offering a scalable and\nefficient solution to the challenges of verifying configurable IPs."}
{"id": "2510.15882", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15882", "abs": "https://arxiv.org/abs/2510.15882", "authors": ["Ao Shen", "Rui Zhang", "Junping Zhao"], "title": "FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern", "comment": null, "summary": "As large language models (LLMs) continue to scale, multi-node deployment has\nbecome a necessity. Consequently, communication has become a critical\nperformance bottleneck. Current intra-node communication libraries, like NCCL,\ntypically make use of a single interconnect such as NVLink. This approach\ncreates performance ceilings, especially on hardware like the H800 GPU where\nthe primary interconnect's bandwidth can become a bottleneck, and leaves other\nhardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable\nNetwork Interface Cards (NICs) largely idle during intensive workloads. We\npropose FlexLink, the first collective communication framework to the best of\nour knowledge designed to systematically address this by aggregating these\nheterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance\ncommunication fabric. FlexLink employs an effective two-stage adaptive load\nbalancing strategy that dynamically partitions communication traffic across all\navailable links, ensuring that faster interconnects are not throttled by slower\nones. On an 8-GPU H800 server, our design improves the bandwidth of collective\noperators such as AllReduce and AllGather by up to 26% and 27% over the NCCL\nbaseline, respectively. This gain is achieved by offloading 2-22% of the total\ncommunication traffic to the previously underutilized PCIe and RDMA NICs.\nFlexLink provides these improvements as a lossless, drop-in replacement\ncompatible with the NCCL API, ensuring easy adoption."}
{"id": "2510.16259", "categories": ["cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.16259", "abs": "https://arxiv.org/abs/2510.16259", "authors": ["Zhehao Zhang", "Weijie Xu", "Shixian Cui", "Chandan K. Reddy"], "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense", "comment": "29 pages, 9 tables, 4 figures", "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems."}
{"id": "2510.16078", "categories": ["cs.CR", "cs.AI", "cs.CV", "68T10, 68T45, 94A60", "I.4.8; I.5.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.16078", "abs": "https://arxiv.org/abs/2510.16078", "authors": ["Abdelilah Ganmati", "Karim Afdel", "Lahcen Koutti"], "title": "ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates", "comment": "~14 pages, 6 figures, 6 tables. Source uses elsarticle class; all\n  figures included as PNG/PDF. Primary: cs.CV", "summary": "We present a practical match-on-card design for face verification in which\ncompact 64/128-bit templates are produced off-card by PCA-ITQ and compared\non-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and\n14443-4 command APDUs with fixed-length payloads and decision-only status words\n(no score leakage), together with a minimal per-identity EEPROM map. Using real\nbinary codes from a CelebA working set (55 identities, 412 images), we (i)\nderive operating thresholds from ROC/DET, (ii) replay enroll->verify\ntransactions at those thresholds, and (iii) bound end-to-end time by pure link\nlatency plus a small constant on-card budget. Even at the slowest contact rate\n(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at\n38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,\nwhile 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted\nsymbol-level parity over empirically unstable bits) is latency-negligible.\nOverall, short binary templates, fixed-payload decision-only APDUs, and\nconstant-time matching satisfy ISO/IEC transport constraints with wide timing\nmargin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset\nevaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and\non-card microbenchmarks as next steps."}
{"id": "2510.15904", "categories": ["cs.AR", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15904", "abs": "https://arxiv.org/abs/2510.15904", "authors": ["Subhradip Chakraborty", "Ankur Singh", "Xuming Chen", "Gourav Datta", "Akhilesh R. Jaiswal"], "title": "NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme", "comment": "11 pages", "summary": "The rapid growth of deep neural network (DNN) workloads has significantly\nincreased the demand for large-capacity on-chip SRAM in machine learning (ML)\napplications, with SRAM arrays now occupying a substantial fraction of the\ntotal die area. To address the dual challenges of storage density and\ncomputation efficiency, this paper proposes an NVM-in-Cache architecture that\nintegrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell,\nforming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory\n(PIM) mode, which performs massively parallel multiply-and-accumulate (MAC)\noperations directly on cache power lines while preserving stored cache data. By\nexploiting the intrinsic properties of the 6T-2R structure, the architecture\nachieves additional storage capability, high computational throughput without\nany bit-cell area overhead. Circuit- and array-level simulations in\nGlobalFoundries 22nm FDSOI technology demonstrate that the proposed design\nachieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel\noperations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18\nneural network, achieving an accuracy of 91.27%. These results highlight the\npotential of the NVM-in-Cache approach to serve as a scalable, energy-efficient\ncomputing method by re-purposing existing 6T SRAM cache architecture for\nnext-generation AI accelerators and general purpose processors."}
{"id": "2510.15893", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG", "68M10, 68M14", "B.4.3; C.2.4; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.15893", "abs": "https://arxiv.org/abs/2510.15893", "authors": ["Mikhail Bernadskiy", "Peter Carson", "Thomas Graham", "Taylor Groves", "Ho John Lee", "Eric Yeh"], "title": "Accelerating Frontier MoE Training with 3D Integrated Optics", "comment": "12 pages, 11 figures. To be published in Hot Interconnects 2025", "summary": "The unabated growth in AI workload demands is driving the need for concerted\nadvances in compute, memory, and interconnect performance. As traditional\nsemiconductor scaling slows, high-speed interconnects have emerged as the new\nscaling engine, enabling the creation of larger logical GPUs by linking many\nGPUs into a single, low-latency, high-bandwidth compute domain. While initial\nscale-up fabrics leveraged copper interconnects for their power and cost\nadvantages, the maximum reach of passive electrical interconnects\n(approximately 1 meter) effectively limits the scale-up domain to within a\nsingle rack. The advent of 3D-stacked optics and logic offers a transformative,\npower-efficient scale-up solution for connecting hundreds of GPU packages\n(thousands of GPUs) across multiple data center racks. This work explores the\ndesign tradeoffs of scale-up technologies and demonstrates how frontier LLMs\nnecessitate novel photonic solutions to achieve aggressive power and\nperformance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and\nswitches within the scale-up domain when training Frontier Mixture of Experts\n(MoE) models exceeding one trillion parameters. Our results show that the\nsubstantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X\nincrease in scale-up capability. This affords new opportunities for\nmulti-dimensional parallelism within the scale-up domain and results in a 2.7X\nreduction in time-to-train, unlocking unprecedented model scaling."}
{"id": "2510.16276", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16276", "abs": "https://arxiv.org/abs/2510.16276", "authors": ["Song Bian", "Minghao Yan", "Anand Jayarajan", "Gennady Pekhimenko", "Shivaram Venkataraman"], "title": "What Limits Agentic Systems Efficiency?", "comment": "27 pages, 15 figures", "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."}
{"id": "2510.16087", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16087", "abs": "https://arxiv.org/abs/2510.16087", "authors": ["Sabbir M Saleh", "Nazim Madhavji", "John Steinbacher"], "title": "Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments", "comment": "8 pages, 5 figures, conference", "summary": "Security is becoming a pivotal point in cloud platforms. Several divisions,\nsuch as business organisations, health care, government, etc., have experienced\ncyber-attacks on their infrastructures. This research focuses on security\nissues within Continuous Integration and Deployment (CI/CD) pipelines in a\ncloud platform as a reaction to recent cyber breaches. This research proposes a\nblockchain-based solution to enhance CI/CD pipeline security. This research\naims to develop a framework that leverages blockchain's distributed ledger\ntechnology and tamper-resistant features to improve CI/CD pipeline security.\nThe goal is to emphasise secure software deployment by integrating threat\nmodelling frameworks and adherence to coding standards. It also aims to employ\ntools to automate security testing to detect publicly disclosed vulnerabilities\nand flaws, such as an outdated version of Java Spring Framework, a JavaScript\nlibrary from an unverified source, or a database library that allows SQL\ninjection attacks in the deployed software through the framework."}
{"id": "2510.15906", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15906", "abs": "https://arxiv.org/abs/2510.15906", "authors": ["Yunsheng Bai", "Ghaith Bany Hamad", "Chia-Tung Ho", "Syed Suhaib", "Haoxing Ren"], "title": "FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures", "comment": null, "summary": "Debugging formal verification (FV) failures represents one of the most\ntime-consuming bottlenecks in modern hardware design workflows. When properties\nfail, engineers must manually trace through complex counter-examples spanning\nmultiple cycles, analyze waveforms, and cross-reference design specifications\nto identify root causes - a process that can consume hours or days per bug.\nExisting solutions are largely limited to manual waveform viewers or simple\nautomated tools that cannot reason about the complex interplay between design\nintent and implementation logic. We present FVDebug, an intelligent system that\nautomates root-cause analysis by combining multiple data sources - waveforms,\nRTL code, design specifications - to transform failure traces into actionable\ninsights. Our approach features a novel pipeline: (1) Causal Graph Synthesis\nthat structures failure traces into directed acyclic graphs, (2) Graph Scanner\nusing batched Large Language Model (LLM) analysis with for-and-against\nprompting to identify suspicious nodes, and (3) Insight Rover leveraging\nagentic narrative exploration to generate high-level causal explanations.\nFVDebug further provides concrete RTL fixes through its Fix Generator.\nEvaluated on open benchmarks, FVDebug attains high hypothesis quality and\nstrong Pass@k fix rates. We further report results on two proprietary,\nproduction-scale FV counterexamples. These results demonstrate FVDebug's\napplicability from academic benchmarks to industrial designs."}
{"id": "2510.15917", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15917", "abs": "https://arxiv.org/abs/2510.15917", "authors": ["Shai Bergman", "Won Wook Song", "Lukas Cavigelli", "Konstantin Berestizshevsky", "Ke Zhou", "Ji Zhang"], "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding", "comment": null, "summary": "Existing storage systems lack visibility into workload intent, limiting their\nability to adapt to the semantics of modern, large-scale data-intensive\napplications. This disconnect leads to brittle heuristics and fragmented,\nsiloed optimizations. To address these limitations, we propose Intent-Driven\nStorage Systems (IDSS), a vision for a new paradigm where large language models\n(LLMs) infer workload and system intent from unstructured signals to guide\nadaptive and cross-layer parameter reconfiguration. IDSS provides holistic\nreasoning for competing demands, synthesizing safe and efficient decisions\nwithin policy guardrails. We present four design principles for integrating\nLLMs into storage control loops and propose a corresponding system\narchitecture. Initial results on FileBench workloads show that IDSS can improve\nIOPS by up to 2.45X by interpreting intent and generating actionable\nconfigurations for storage components such as caching and prefetching. These\nfindings suggest that, when constrained by guardrails and embedded within\nstructured workflows, LLMs can function as high-level semantic optimizers,\nbridging the gap between application goals and low-level system control. IDSS\npoints toward a future in which storage systems are increasingly adaptive,\nautonomous, and aligned with dynamic workload demands."}
{"id": "2510.16302", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16302", "abs": "https://arxiv.org/abs/2510.16302", "authors": ["Changhao Wang", "Yanfang Liu", "Xinxin Fan", "Anzhi Zhou", "Lao Tian", "Yunfeng Lu"], "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA", "comment": "13 pages, 5 figures", "summary": "Multi-hop reasoning for question answering (QA) plays a critical role in\nretrieval-augmented generation (RAG) for modern large language models (LLMs).\nThe accurate answer can be obtained through retrieving relational structure of\nentities from knowledge graph (KG). Regarding the inherent relation-dependency\nand reasoning pattern, multi-hop reasoning can be in general classified into\ntwo categories: i) parallel fact-verification multi-hop reasoning question,\ni.e., requiring simultaneous verifications of multiple independent\nsub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding\nsequential multi-step inference with intermediate conclusions serving as\nessential premises for subsequent reasoning. Currently, the multi-hop reasoning\napproaches singly employ one of two techniques: LLM response-based fact\nverification and KG path-based chain construction. Nevertheless, the former\nexcels at parallel fact-verification but underperforms on chained reasoning\ntasks, while the latter demonstrates proficiency in chained multi-hop reasoning\nbut suffers from redundant path retrieval when handling parallel\nfact-verification reasoning. These limitations deteriorate the efficiency and\naccuracy for multi-hop QA tasks. To address this challenge, we propose a novel\ndual-track KG verification and reasoning framework DTKG, which is inspired by\nthe Dual Process Theory in cognitive science. Specifically, DTKG comprises two\nmain stages: the Classification Stage and the Branch Processing Stage."}
{"id": "2510.16122", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16122", "abs": "https://arxiv.org/abs/2510.16122", "authors": ["Owais Makroo", "Siva Rajesh Kasa", "Sumegh Roychowdhury", "Karan Gupta", "Nikhil Pattisapu", "Santhosh Kasa", "Sumit Negi"], "title": "The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers", "comment": null, "summary": "Membership Inference Attacks (MIAs) pose a critical privacy threat by\nenabling adversaries to determine whether a specific sample was included in a\nmodel's training dataset. Despite extensive research on MIAs, systematic\ncomparisons between generative and discriminative classifiers remain limited.\nThis work addresses this gap by first providing theoretical motivation for why\ngenerative classifiers exhibit heightened susceptibility to MIAs, then\nvalidating these insights through comprehensive empirical evaluation. Our study\nencompasses discriminative, generative, and pseudo-generative text classifiers\nacross varying training data volumes, evaluated on nine benchmark datasets.\nEmploying a diverse array of MIA strategies, we consistently demonstrate that\nfully generative classifiers which explicitly model the joint likelihood\n$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe\nthat the canonical inference approach commonly used in generative classifiers\nsignificantly amplifies this privacy risk. These findings reveal a fundamental\nutility-privacy trade-off inherent in classifier design, underscoring the\ncritical need for caution when deploying generative classifiers in\nprivacy-sensitive applications. Our results motivate future research directions\nin developing privacy-preserving generative classifiers that can maintain\nutility while mitigating membership inference vulnerabilities."}
{"id": "2510.15907", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15907", "abs": "https://arxiv.org/abs/2510.15907", "authors": ["Era Thaqi", "Dennis Eigner", "Arman Ferdowsi", "Ulrich Schmid"], "title": "Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions", "comment": null, "summary": "We propose a novel approach to symbolic timing analysis for digital\nintegrated circuits based on recently developed analytic delay formulas for\n2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a\nfixed order of the transitions of all input and internal signals of a circuit,\nour framework computes closed-form analytic delay expressions for all the\ninternal signal transition times that depend on (i) the symbolic transition\ntimes of the relevant input signals and (ii) the model parameters of the\nrelevant gates. The resulting formulas facilitate per-transition timing\nanalysis without any simulation, by instantiating the symbolic input transition\ntimes and the gate parameters. More importantly, however, they also enable an\n\\emph{analytic} study of the dependencies of certain timing properties on input\nsignals and gate parameters. For instance, differentiating a symbolic delay\nexpression with respect to a gate parameter or input transition time enables\nsensitivity analysis. As a proof of concept, we implement our approach using\nthe computer algebra system SageMath and apply it to the NOR-gate version of\nthe c17 slack benchmark circuit."}
{"id": "2510.15927", "categories": ["cs.AR", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15927", "abs": "https://arxiv.org/abs/2510.15927", "authors": ["Krystian Chmielewski", "Jarosław Ławnicki", "Uladzislau Lukyanau", "Tadeusz Kobus", "Maciej Maciejewski"], "title": "UPMEM Unleashed: Software Secrets for Speed", "comment": null, "summary": "Developing kernels for Processing-In-Memory (PIM) platforms poses unique\nchallenges in data management and parallel programming on limited processing\nunits. Although software development kits (SDKs) for PIM, such as the UPMEM\nSDK, provide essential tools, these emerging platforms still leave significant\nroom for performance optimization. In this paper, we reveal surprising\ninefficiencies in UPMEM software stack and play with non-standard programming\ntechniques. By making simple modifications to the assembly generated by the\nUPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x\nin integer multiplication, depending on the data type. We also demonstrate that\nbit-serial processing of low precision data is a viable option for UPMEM: in\nINT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup\nover the baseline. Minor API extensions for PIM allocation that account for the\nnon-uniform memory access (NUMA) architecture of the server further improve the\nconsistency and throughput of host-PIM data transfers by up to 2.9x. Finally,\nwe show that, when the matrix is preloaded into PIM, our optimized kernels\noutperform a dual-socket CPU server by over 3x for INT8 generalized\nmatrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized\nINT8 GEMV kernel outperforms the baseline 3.5x."}
{"id": "2510.16309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16309", "abs": "https://arxiv.org/abs/2510.16309", "authors": ["Crystal Su"], "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier", "comment": "Accepted to the Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2026) Workshop", "summary": "Large language models (LLMs) often produce fluent reasoning steps while\nviolating simple mathematical or logical constraints. We introduce MedRule-KG,\na compact typed knowledge graph coupled with a symbolic verifier, designed to\nenforce mathematically interpretable rules in reasoning tasks. MedRule-KG\nencodes entities, relations, and three domain-inspired rules, while the\nverifier checks predictions and applies minimal corrections to guarantee\nconsistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG\nimproves exact match (EM) from 0.767 to 0.900, and adding the verifier yields\n1.000 EM while eliminating rule violations entirely. We demonstrate how\nMedRule-KG provides a general scaffold for safe mathematical reasoning, discuss\nablations, and release code and data to encourage reproducibility."}
{"id": "2510.16128", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16128", "abs": "https://arxiv.org/abs/2510.16128", "authors": ["Kate Glazko", "Jennifer Mankoff"], "title": "Prompt injections as a tool for preserving identity in GAI image descriptions", "comment": "Accepted as a poster to Soups 2025", "summary": "Generative AI risks such as bias and lack of representation impact people who\ndo not interact directly with GAI systems, but whose content does: indirect\nusers. Several approaches to mitigating harms to indirect users have been\ndescribed, but most require top down or external intervention. An emerging\nstrategy, prompt injections, provides an empowering alternative: indirect users\ncan mitigate harm against them, from within their own content. Our approach\nproposes prompt injections not as a malicious attack vector, but as a tool for\ncontent/image owner resistance. In this poster, we demonstrate one case study\nof prompt injections for empowering an indirect user, by retaining an image\nowner's gender and disabled identity when an image is described by GAI."}
{"id": "2510.15908", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15908", "abs": "https://arxiv.org/abs/2510.15908", "authors": ["Hana Chitsaz", "Johnson Umeike", "Amirmahdi Namjoo", "Babak N. Safa", "Bahar Asgari"], "title": "Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations", "comment": null, "summary": "Finite element simulations are essential in biomechanics, enabling detailed\nmodeling of tissues and organs. However, architectural inefficiencies in\ncurrent hardware and software stacks limit performance and scalability,\nespecially for iterative tasks like material parameter identification. As a\nresult, workflows often sacrifice fidelity for tractability. Reconfigurable\nhardware, such as FPGAs, offers a promising path to domain-specific\nacceleration without the cost of ASICs, but its potential in biomechanics\nremains underexplored. This paper presents Belenos, a comprehensive workload\ncharacterization of finite element biomechanics using FEBio, a widely adopted\nsimulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal\nthat smaller workloads experience moderate front-end stalls, typically around\n13.1%, whereas larger workloads are dominated by significant back-end\nbottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.\nComplementary gem5 sensitivity studies identify optimal hardware configurations\nfor Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,\nmemory, or branch predictor settings can degrade performance by up to 37.1%.\nThese findings underscore the need for architecture-aware co-design to\nefficiently support biomechanical simulation workloads."}
{"id": "2510.16024", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16024", "abs": "https://arxiv.org/abs/2510.16024", "authors": ["Abdulrahman Alhaidari", "Balaji Palanisamy", "Prashant Krishnamurthy"], "title": "On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation", "comment": "Published in the 7th Conference on Advances in Financial Technologies\n  (AFT 2025)", "summary": "Billions of dollars are lost every year in DeFi platforms by transactions\nexploiting business logic or accounting vulnerabilities. Existing defenses\nfocus on static code analysis, public mempool screening, attacker contract\ndetection, or trusted off-chain monitors, none of which prevents exploits\nsubmitted through private relays or malicious contracts that execute within the\nsame block. We present the first decentralized, fully on-chain learning\nframework that: (i) performs gas-prohibitive computation on Layer-2 to reduce\ncost, (ii) propagates verified model updates to Layer-1, and (iii) enables\ngas-bounded, low-latency inference inside smart contracts. A novel\nProof-of-Improvement (PoIm) protocol governs the training process and verifies\neach decentralized micro update as a self-verifying training transaction.\nUpdates are accepted by \\textit{PoIm} only if they demonstrably improve at\nleast one core metric (e.g., accuracy, F1-score, precision, or recall) on a\npublic benchmark without degrading any of the other core metrics, while\nadversarial proposals get financially penalized through an adaptable test set\nfor evolving threats. We develop quantization and loop-unrolling techniques\nthat enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs\n(with support for formally verified decision tree inference) within the\nEthereum block gas limit, while remaining bit-exact to their off-chain\ncounterparts, formally proven in Z3. We curate 298 unique real-world exploits\n(2020 - 2025) with 402 exploit transactions across eight EVM chains,\ncollectively responsible for \\$3.74 B in losses."}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept."}
{"id": "2510.16168", "categories": ["cs.CR", "C.2.0; C.2.2; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.16168", "abs": "https://arxiv.org/abs/2510.16168", "authors": ["Ahmed Fouad Kadhim Koysha", "Aytug Boyaci", "Rafet Akdeniz"], "title": "WebRTC Metadata and IP Leakage in Modern Browsers: A Cross-Platform Measurement Study", "comment": "14 pages, 7 figures. This preprint is under review at a Taylor &\n  Francis journal", "summary": "Web Real-Time Communication (WebRTC) enables real-time peer-to-peer\ncommunication, but its Interactive Connectivity Establishment (ICE) process can\nunintentionally expose internal and public IP addresses as metadata. This paper\npresents a cross-platform measurement study of WebRTC metadata leakage using\ncurrent (2025) builds of Chrome, Brave, Firefox, and Tor on desktop and mobile\nplatforms. Experiments were conducted across semi-trusted Wi-Fi and untrusted\nmobile carrier networks. Results show that Chrome remains the most\nleakage-prone, disclosing LAN or Carrier-Grade NAT (CGNAT) addresses on mobile\nand metadata on desktop; Brave avoids direct IP leaks but exposes\nsession-stable mDNS identifiers; Firefox provides strong protection on desktop\nbut leaks internal IPs on Android; and Tor consistently prevents all forms of\nleakage. We introduce a structured threat model for semi-trusted environments\nand evaluate the limitations of mDNS obfuscation. Finally, we propose layered\nmitigation strategies combining browser defaults, institutional safeguards, and\nuser controls. Findings demonstrate that while direct LAN leakage is declining,\nemerging vectors such as mDNS and CGNAT create persistent privacy risks\nrequiring protocol-level redesign and policy action."}
{"id": "2510.15910", "categories": ["cs.AR", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.15910", "abs": "https://arxiv.org/abs/2510.15910", "authors": ["Marvin Fuchs", "Lukas Scheller", "Timo Muscheid", "Oliver Sander", "Luis E. Ardila-Perez"], "title": "SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs", "comment": "26 pages, single-column, 13 figures, 2 tables", "summary": "Modern heterogeneous System-on-Chip (SoC) devices integrate advanced\ncomponents into a single package, offering powerful capabilities while also\nintroducing significant complexity. To manage these sophisticated devices,\nfirmware and software developers need powerful development tools. However, as\nthese tools become increasingly complex, they often lack adequate support,\nresulting in a steep learning curve and challenging troubleshooting. To address\nthis, this work introduces System-on-Chip blocks (SoCks), a flexible and\nexpandable build framework that reduces complexity by partitioning the SoC\nimage into high-level units called blocks. SoCks builds each firmware and\nsoftware block in an encapsulated way, independently from other components of\nthe image, thereby reducing dependencies to a minimum. While some information\nexchange between the blocks is unavoidable to ensure seamless runtime\nintegration, this interaction is standardized via interfaces. A small number of\ndependencies and well-defined interfaces simplify the reuse of existing block\nimplementations and facilitate seamless substitution between versions-for\ninstance, when choosing root file systems for the embedded Linux operating\nsystem. Additionally, this approach facilitates the establishment of a\ndecentralized and partially automated development flow through Continuous\nIntegration and Continuous Delivery (CI/CD). Measurement results demonstrate\nthat SoCks can build a complete SoC image up to three times faster than\nestablished tools."}
{"id": "2510.16067", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16067", "abs": "https://arxiv.org/abs/2510.16067", "authors": ["Saurabh Deochake", "Ryan Murphy", "Jeremiah Gearheart"], "title": "A Multi-Cloud Framework for Zero-Trust Workload Authentication", "comment": "Cyber Security Experimentation and Test (CSET) at the Annual Computer\n  Security Applications Conference (ACSAC) 2025", "summary": "Static, long-lived credentials for workload authentication create untenable\nsecurity risks that violate Zero-Trust principles. This paper presents a\nmulti-cloud framework using Workload Identity Federation (WIF) and OpenID\nConnect (OIDC) for secretless authentication. Our approach uses\ncryptographically-verified, ephemeral tokens, allowing workloads to\nauthenticate without persistent private keys and mitigating credential theft.\nWe validate this framework in an enterprise-scale Kubernetes environment, which\nsignificantly reduces the attack surface. The model offers a unified solution\nto manage workload identities across disparate clouds, enabling future\nimplementation of robust, attribute-based access control."}
{"id": "2510.16368", "categories": ["cs.AI", "cs.HC", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.16368", "abs": "https://arxiv.org/abs/2510.16368", "authors": ["Ali Shirali"], "title": "The Burden of Interactive Alignment with Inconsistent Preferences", "comment": "Published as a conference paper at NeurIPS 2025", "summary": "From media platforms to chatbots, algorithms shape how people interact,\nlearn, and discover information. Such interactions between users and an\nalgorithm often unfold over multiple steps, during which strategic users can\nguide the algorithm to better align with their true interests by selectively\nengaging with content. However, users frequently exhibit inconsistent\npreferences: they may spend considerable time on content that offers little\nlong-term value, inadvertently signaling that such content is desirable.\nFocusing on the user side, this raises a key question: what does it take for\nsuch users to align the algorithm with their true interests?\n  To investigate these dynamics, we model the user's decision process as split\nbetween a rational system 2 that decides whether to engage and an impulsive\nsystem 1 that determines how long engagement lasts. We then study a\nmulti-leader, single-follower extensive Stackelberg game, where users,\nspecifically system 2, lead by committing to engagement strategies and the\nalgorithm best-responds based on observed interactions. We define the burden of\nalignment as the minimum horizon over which users must optimize to effectively\nsteer the algorithm. We show that a critical horizon exists: users who are\nsufficiently foresighted can achieve alignment, while those who are not are\ninstead aligned to the algorithm's objective. This critical horizon can be\nlong, imposing a substantial burden. However, even a small, costly signal\n(e.g., an extra click) can significantly reduce it. Overall, our framework\nexplains how users with inconsistent preferences can align an engagement-driven\nalgorithm with their interests in a Stackelberg equilibrium, highlighting both\nthe challenges and potential remedies for achieving alignment."}
{"id": "2510.16219", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16219", "abs": "https://arxiv.org/abs/2510.16219", "authors": ["Yang Feng", "Xudong Pan"], "title": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection", "comment": null, "summary": "Malicious agents pose significant threats to the reliability and\ndecision-making capabilities of Multi-Agent Systems (MAS) powered by Large\nLanguage Models (LLMs). Existing defenses often fall short due to reactive\ndesigns or centralized architectures which may introduce single points of\nfailure. To address these challenges, we propose SentinelNet, the first\ndecentralized framework for proactively detecting and mitigating malicious\nbehaviors in multi-agent collaboration. SentinelNet equips each agent with a\ncredit-based detector trained via contrastive learning on augmented adversarial\ndebate trajectories, enabling autonomous evaluation of message credibility and\ndynamic neighbor ranking via bottom-k elimination to suppress malicious\ncommunications. To overcome the scarcity of attack data, it generates\nadversarial trajectories simulating diverse threats, ensuring robust training.\nExperiments on MAS benchmarks show SentinelNet achieves near-perfect detection\nof malicious agents, close to 100% within two debate rounds, and recovers 95%\nof system accuracy from compromised baselines. By exhibiting strong\ngeneralizability across domains and attack patterns, SentinelNet establishes a\nnovel paradigm for safeguarding collaborative MAS."}
{"id": "2510.15914", "categories": ["cs.AR", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.15914", "abs": "https://arxiv.org/abs/2510.15914", "authors": ["Jiayu Zhao", "Song Chen"], "title": "VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts", "comment": "9 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ngenerating Verilog code from natural language descriptions. However, Verilog\ncode inherently encodes structural information of hardware circuits.\nEffectively leveraging this structural information to enhance the functional\nand syntactic correctness of LLM-generated Verilog code remains a significant\nchallenge. To address this challenge, we propose VeriGRAG , a novel framework\nthat extracts structural graph embeddings from Verilog code using graph neural\nnetworks (GNNs). A multimodal retriever then selects the graph embeddings most\nrelevant to the given generation task, which are aligned with the code modality\nthrough the VeriFormer module to generate structure-aware soft prompts. Our\nexperiments demonstrate that VeriGRAG substantially improves the correctness of\nVerilog code generation, achieving state-of-the-art or superior performance\nacross both VerilogEval and RTLLM benchmarks."}
{"id": "2510.16087", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16087", "abs": "https://arxiv.org/abs/2510.16087", "authors": ["Sabbir M Saleh", "Nazim Madhavji", "John Steinbacher"], "title": "Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments", "comment": "8 pages, 5 figures, conference", "summary": "Security is becoming a pivotal point in cloud platforms. Several divisions,\nsuch as business organisations, health care, government, etc., have experienced\ncyber-attacks on their infrastructures. This research focuses on security\nissues within Continuous Integration and Deployment (CI/CD) pipelines in a\ncloud platform as a reaction to recent cyber breaches. This research proposes a\nblockchain-based solution to enhance CI/CD pipeline security. This research\naims to develop a framework that leverages blockchain's distributed ledger\ntechnology and tamper-resistant features to improve CI/CD pipeline security.\nThe goal is to emphasise secure software deployment by integrating threat\nmodelling frameworks and adherence to coding standards. It also aims to employ\ntools to automate security testing to detect publicly disclosed vulnerabilities\nand flaws, such as an outdated version of Java Spring Framework, a JavaScript\nlibrary from an unverified source, or a database library that allows SQL\ninjection attacks in the deployed software through the framework."}
{"id": "2510.16374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16374", "abs": "https://arxiv.org/abs/2510.16374", "authors": ["Nick Oh"], "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs", "comment": "Presented at the Workshop on the Application of LLM Explainability to\n  Reasoning and Planning at COLM 2025 (non-archival)", "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms:\nMonitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and\nSELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack\nmechanisms to verify whether selected strategies succeed; while Generate-Verify\napproaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan\net al., 2023) iteratively refine outputs but commence generation blindly\nwithout task assessment. This separation creates inefficiencies -- strategies\nfail without feedback, and refinement occurs without strategic grounding. We\naddress this gap by implementing Flavell's cognitive monitoring model (1979)\nfrom the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),\noperationalising it as a three-phase iterative system. On GSM8K, preliminary\nresults show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for\nSelf-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%\nincreased inference cost. These initial findings suggest upfront monitoring\nproduces higher-quality initial solutions that reduce refinement needs, though\nevaluation beyond arithmetic reasoning is needed to establish generalisability."}
{"id": "2510.16229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16229", "abs": "https://arxiv.org/abs/2510.16229", "authors": ["Vienna Li", "Justin Villa", "Dan Diessner", "Jayson Clifford", "Laxima Niure Kandel"], "title": "C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations", "comment": null, "summary": "GPS spoofing poses a growing threat to aviation by falsifying satellite\nsignals and misleading aircraft navigation systems. This paper demonstrates a\nproof-of-concept spoofing detection strategy based on analyzing satellite\nCarrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static\nantenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite\nsimulator, C/N$_0$ data is collected under three antenna orientations flat,\nbanked right, and banked left) in both real-sky (non-spoofed) and spoofed\nenvironments. Our findings reveal that under non-spoofed signals, C/N$_0$\nvalues fluctuate naturally with orientation, reflecting true geometric\ndependencies. However, spoofed signals demonstrate a distinct pattern: the flat\norientation, which directly faces the spoofing antenna, consistently yielded\nthe highest C/N$_0$ values, while both banked orientations showed reduced\nC/N$_0$ due to misalignment with the spoofing source. These findings suggest\nthat simple maneuvers such as brief banking to induce C/N$_0$ variations can\nprovide early cues of GPS spoofing for general aviation and UAV systems."}
{"id": "2510.15917", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15917", "abs": "https://arxiv.org/abs/2510.15917", "authors": ["Shai Bergman", "Won Wook Song", "Lukas Cavigelli", "Konstantin Berestizshevsky", "Ke Zhou", "Ji Zhang"], "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding", "comment": null, "summary": "Existing storage systems lack visibility into workload intent, limiting their\nability to adapt to the semantics of modern, large-scale data-intensive\napplications. This disconnect leads to brittle heuristics and fragmented,\nsiloed optimizations. To address these limitations, we propose Intent-Driven\nStorage Systems (IDSS), a vision for a new paradigm where large language models\n(LLMs) infer workload and system intent from unstructured signals to guide\nadaptive and cross-layer parameter reconfiguration. IDSS provides holistic\nreasoning for competing demands, synthesizing safe and efficient decisions\nwithin policy guardrails. We present four design principles for integrating\nLLMs into storage control loops and propose a corresponding system\narchitecture. Initial results on FileBench workloads show that IDSS can improve\nIOPS by up to 2.45X by interpreting intent and generating actionable\nconfigurations for storage components such as caching and prefetching. These\nfindings suggest that, when constrained by guardrails and embedded within\nstructured workflows, LLMs can function as high-level semantic optimizers,\nbridging the gap between application goals and low-level system control. IDSS\npoints toward a future in which storage systems are increasingly adaptive,\nautonomous, and aligned with dynamic workload demands."}
{"id": "2510.16382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16382", "abs": "https://arxiv.org/abs/2510.16382", "authors": ["Ze Tao", "Jian Zhang", "Haowei Li", "Xianshuai Li", "Yifei Peng", "Xiyao Liu", "Senzhang Wang", "Chao Liu", "Sheng Ren", "Shichao Zhang"], "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization", "comment": null, "summary": "This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a\nnovel causal framework inspired by human intelligence, designed to overcome the\nlimitations of conventional domain generalization models. Unlike approaches\nthat rely on statistics to capture data-label dependencies and learn\ndistortion-invariant representations, HSCM replicates the hierarchical\nprocessing and multi-level learning of human vision systems, focusing on\nmodeling fine-grained causal mechanisms. By disentangling and reweighting key\nimage attributes such as color, texture, and shape, HSCM enhances\ngeneralization across diverse domains, ensuring robust performance and\ninterpretability. Leveraging the flexibility and adaptability of human\nintelligence, our approach enables more effective transfer and learning in\ndynamic, complex environments. Through both theoretical and empirical\nevaluations, we demonstrate that HSCM outperforms existing domain\ngeneralization models, providing a more principled method for capturing causal\nrelationships and improving model robustness. The code is available at\nhttps://github.com/lambett/HSCM."}
{"id": "2510.16251", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16251", "abs": "https://arxiv.org/abs/2510.16251", "authors": ["Changyu Zhao", "Yohan Beugin", "Jean-Charles Noirot Ferrand", "Quinn Burke", "Guancheng Li", "Patrick McDaniel"], "title": "LibIHT: A Hardware-Based Approach to Efficient and Evasion-Resistant Dynamic Binary Analysis", "comment": "Accepted in Proceedings of the 2025 Workshop on Software\n  Understanding and Reverse Engineering (SURE'25), October 13-17, 2025, Taipei,\n  Taiwan", "summary": "Dynamic program analysis is invaluable for malware detection, debugging, and\nperformance profiling. However, software-based instrumentation incurs high\noverhead and can be evaded by anti-analysis techniques. In this paper, we\npropose LibIHT, a hardware-assisted tracing framework that leverages on-CPU\nbranch tracing features (Intel Last Branch Record and Branch Trace Store) to\nefficiently capture program control-flow with minimal performance impact. Our\napproach reconstructs control-flow graphs (CFGs) by collecting hardware\ngenerated branch execution data in the kernel, preserving program behavior\nagainst evasive malware. We implement LibIHT as an OS kernel module and\nuser-space library, and evaluate it on both benign benchmark programs and\nadversarial anti-instrumentation samples. Our results indicate that LibIHT\nreduces runtime overhead by over 150x compared to Intel Pin (7x vs 1,053x\nslowdowns), while achieving high fidelity in CFG reconstruction (capturing over\n99% of execution basic blocks and edges). Although this hardware-assisted\napproach sacrifices the richer semantic detail available from full software\ninstrumentation by capturing only branch addresses, this trade-off is\nacceptable for many applications where performance and low detectability are\nparamount. Our findings show that hardware-based tracing captures control flow\ninformation significantly faster, reduces detection risk and performs dynamic\nanalysis with minimal interference."}
{"id": "2510.15926", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15926", "abs": "https://arxiv.org/abs/2510.15926", "authors": ["Ye Qiao", "Zhiheng Chen", "Yifan Zhang", "Yian Wang", "Sitao Huang"], "title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs", "comment": null, "summary": "With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs."}
{"id": "2510.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16392", "abs": "https://arxiv.org/abs/2510.16392", "authors": ["Ao Tian", "Yunfeng Lu", "Xinxin Fan", "Changhao Wang", "Lanzhi Zhou", "Yeyao Zhang", "Yanfang Liu"], "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile", "comment": "11 pages,3 figures", "summary": "Personalized and continuous interactions are the key to enhancing user\nexperience in today's large language model (LLM)-based conversational systems,\nhowever, the finite context windows and static parametric memory make it\ndifficult to model the cross-session long-term user states and behavioral\nconsistency. Currently, the existing solutions to this predicament, such as\nretrieval-augmented generation (RAG) and explicit memory systems, primarily\nfocus on fact-level storage and retrieval, lacking the capability to distill\nlatent preferences and deep traits from the multi-turn dialogues, which limits\nthe long-term and effective user modeling, directly leading to the personalized\ninteractions remaining shallow, and hindering the cross-session continuity. To\nrealize the long-term memory and behavioral consistency for Language Agents in\nLLM era, we propose a self-evolving memory framework RGMem, inspired by the\nideology of classic renormalization group (RG) in physics, this framework\nenables to organize the dialogue history in multiple scales: it first extracts\nsemantics and user insights from episodic fragments, then through hierarchical\ncoarse-graining and rescaling operations, progressively forms a\ndynamically-evolved user profile. The core innovation of our work lies in\nmodeling memory evolution as a multi-scale process of information compression\nand emergence, which accomplishes the high-level and accurate user profiles\nfrom noisy and microscopic-level interactions."}
{"id": "2510.16255", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16255", "abs": "https://arxiv.org/abs/2510.16255", "authors": ["Sarah Egler", "John Schulman", "Nicholas Carlini"], "title": "Detecting Adversarial Fine-tuning with Auditing Agents", "comment": null, "summary": "Large Language Model (LLM) providers expose fine-tuning APIs that let end\nusers fine-tune their frontier LLMs. Unfortunately, it has been shown that an\nadversary with fine-tuning access to an LLM can bypass safeguards. Particularly\nconcerning, such attacks may avoid detection with datasets that are only\nimplicitly harmful. Our work studies robust detection mechanisms for\nadversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning\nauditing agent and show it can detect harmful fine-tuning prior to model\ndeployment. We provide our auditing agent with access to the fine-tuning\ndataset, as well as the fine-tuned and pre-fine-tuned models, and request the\nagent assigns a risk score for the fine-tuning job. We evaluate our detection\napproach on a diverse set of eight strong fine-tuning attacks from the\nliterature, along with five benign fine-tuned models, totaling over 1400\nindependent audits. These attacks are undetectable with basic content\nmoderation on the dataset, highlighting the challenge of the task. With the\nbest set of affordances, our auditing agent achieves a 56.2% detection rate of\nadversarial fine-tuning at a 1% false positive rate. Most promising, the\nauditor is able to detect covert cipher attacks that evade safety evaluations\nand content moderation of the dataset. While benign fine-tuning with\nunintentional subtle safety degradation remains a challenge, we establish a\nbaseline configuration for further work in this area. We release our auditing\nagent at https://github.com/safety-research/finetuning-auditor."}
{"id": "2510.15927", "categories": ["cs.AR", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15927", "abs": "https://arxiv.org/abs/2510.15927", "authors": ["Krystian Chmielewski", "Jarosław Ławnicki", "Uladzislau Lukyanau", "Tadeusz Kobus", "Maciej Maciejewski"], "title": "UPMEM Unleashed: Software Secrets for Speed", "comment": null, "summary": "Developing kernels for Processing-In-Memory (PIM) platforms poses unique\nchallenges in data management and parallel programming on limited processing\nunits. Although software development kits (SDKs) for PIM, such as the UPMEM\nSDK, provide essential tools, these emerging platforms still leave significant\nroom for performance optimization. In this paper, we reveal surprising\ninefficiencies in UPMEM software stack and play with non-standard programming\ntechniques. By making simple modifications to the assembly generated by the\nUPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x\nin integer multiplication, depending on the data type. We also demonstrate that\nbit-serial processing of low precision data is a viable option for UPMEM: in\nINT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup\nover the baseline. Minor API extensions for PIM allocation that account for the\nnon-uniform memory access (NUMA) architecture of the server further improve the\nconsistency and throughput of host-PIM data transfers by up to 2.9x. Finally,\nwe show that, when the matrix is preloaded into PIM, our optimized kernels\noutperform a dual-socket CPU server by over 3x for INT8 generalized\nmatrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized\nINT8 GEMV kernel outperforms the baseline 3.5x."}
{"id": "2510.16466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16466", "abs": "https://arxiv.org/abs/2510.16466", "authors": ["Siddhartha Krothapalli", "Tridib Kumar Das", "Praveen Kumar", "Naveen Suravarpu", "Pratik Narang"], "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights", "comment": "11 pages, 1 figure, 4 tables", "summary": "As customer feedback becomes increasingly central to strategic growth, the\nability to derive actionable insights from unstructured reviews is essential.\nWhile traditional AI-driven systems excel at predicting user preferences, far\nless work has focused on transforming customer reviews into prescriptive,\nbusiness-facing recommendations. This paper introduces ReviewSense, a novel\nprescriptive decision support framework that leverages advanced large language\nmodels (LLMs) to transform customer reviews into targeted, actionable business\nrecommendations. By identifying key trends, recurring issues, and specific\nconcerns within customer sentiments, ReviewSense extends beyond\npreference-based systems to provide businesses with deeper insights for\nsustaining growth and enhancing customer loyalty. The novelty of this work lies\nin integrating clustering, LLM adaptation, and expert-driven evaluation into a\nunified, business-facing pipeline. Preliminary manual evaluations indicate\nstrong alignment between the model's recommendations and business objectives,\nhighlighting its potential for driving data-informed decision-making. This\nframework offers a new perspective on AI-driven sentiment analysis,\ndemonstrating its value in refining business strategies and maximizing the\nimpact of customer feedback."}
{"id": "2510.16331", "categories": ["cs.CR", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.16331", "abs": "https://arxiv.org/abs/2510.16331", "authors": ["Fatemeh Jafarian Dehkordi", "Elahe Vedadi", "Alireza Feizbakhsh", "Yasaman Keshtkarjahromi", "Hulya Seferoglu"], "title": "Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation", "comment": null, "summary": "Striking a balance between protecting data privacy and enabling collaborative\ncomputation is a critical challenge for distributed machine learning. While\nprivacy-preserving techniques for federated learning have been extensively\ndeveloped, methods for scenarios involving bitwise operations, such as\ntree-based vertical federated learning (VFL), are still underexplored.\nTraditional mechanisms, including Shamir's secret sharing and multi-party\ncomputation (MPC), are not optimized for bitwise operations over binary data,\nparticularly in settings where each participant holds a different part of the\nbinary vector. This paper addresses the limitations of existing methods by\nproposing a novel binary multi-party computation (BiMPC) framework. The BiMPC\nmechanism facilitates privacy-preserving bitwise operations, with a particular\nfocus on dot product computations of binary vectors, ensuring the privacy of\neach individual bit. The core of BiMPC is a novel approach called Dot Product\nvia Modular Addition (DoMA), which uses regular and modular additions for\nefficient binary dot product calculation. To ensure privacy, BiMPC uses random\nmasking in a higher field for linear computations and a three-party oblivious\ntransfer (triot) protocol for non-linear binary operations. The privacy\nguarantees of the BiMPC framework are rigorously analyzed, demonstrating its\nefficiency and scalability in distributed settings."}
{"id": "2510.15930", "categories": ["cs.AR", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.15930", "abs": "https://arxiv.org/abs/2510.15930", "authors": ["Philippe Magalhães", "Virginie Fresse", "Benoît Suffran", "Olivier Alata"], "title": "Implémentation Efficiente de Fonctions de Convolution sur FPGA à l'Aide de Blocs Paramétrables et d'Approximations Polynomiales", "comment": "in French language, XXXe Colloque Francophone de Traitement du Signal\n  et des Images (GRETSI), Aug 2025, Strabourg, France", "summary": "Implementing convolutional neural networks (CNNs) on field-programmable gate\narrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower\nlatency, greater power efficiency and greater flexibility. However, this\ndevelopment remains complex due to the hardware knowledge required and the long\nsynthesis, placement and routing stages, which slow down design cycles and\nprevent rapid exploration of network configurations, making resource\noptimisation under severe constraints particularly challenging. This paper\nproposes a library of configurable convolution Blocks designed to optimize FPGA\nimplementation and adapt to available resources. It also presents a\nmethodological framework for developing mathematical models that predict FPGA\nresources utilization. The approach is validated by analyzing the correlation\nbetween the parameters, followed by error metrics. The results show that the\ndesigned blocks enable adaptation of convolution layers to hardware\nconstraints, and that the models accurately predict resource consumption,\nproviding a useful tool for FPGA selection and optimized CNN deployment."}
{"id": "2510.16476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16476", "abs": "https://arxiv.org/abs/2510.16476", "authors": ["Xiaozhe Li", "Xinyu Fang", "Shengyuan Ding", "Linyang Li", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems", "comment": null, "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities, with\nmodels like OpenAI's O-series and DeepSeek R1 excelling at tasks such as\nmathematics, coding, logic, and puzzles through Reinforcement Learning with\nVerifiable Rewards (RLVR). However, their ability to solve more complex\noptimization problems - particularly NP-hard tasks - remains underexplored. To\nbridge this gap, we propose NP-ENGINE, the first comprehensive framework for\ntraining and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks\nacross five domains, each equipped with (i) a controllable instance generator,\n(ii) a rule-based verifier, and (iii) a heuristic solver that provides\napproximate optimal solutions as ground truth. This\ngenerator-verifier-heuristic pipeline enables scalable and verifiable RLVR\ntraining under hierarchical difficulties. We also introduce NP-BENCH, a\nbenchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'\nability to tackle NP-hard level reasoning problems, focusing not only on\nfeasibility but also on solution quality. Additionally, we present\nQWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on\nQwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and\nachieves SOTA performance with the same model size. Beyond in-domain tasks, we\ndemonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain\n(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),\nas well as non-reasoning tasks such as instruction following. We also observe a\nscaling trend: increasing task diversity improves OOD generalization. These\nfindings suggest that task-rich RLVR training is a promising direction for\nadvancing LLM's reasoning ability, revealing new insights into the scaling laws\nof RLVR."}
{"id": "2510.16367", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16367", "abs": "https://arxiv.org/abs/2510.16367", "authors": ["Shuai Li", "Kejiang Chen", "Jun Jiang", "Jie Zhang", "Qiyi Yao", "Kai Zeng", "Weiming Zhang", "Nenghai Yu"], "title": "EditMark: Watermarking Large Language Models based on Model Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, but\ntheir training requires extensive data and computational resources, rendering\nthem valuable digital assets. Therefore, it is essential to watermark LLMs to\nprotect their copyright and trace unauthorized use or resale. Existing methods\nfor watermarking LLMs primarily rely on training LLMs with a watermarked\ndataset, which entails burdensome training costs and negatively impacts the\nLLM's performance. In addition, their watermarked texts are not logical or\nnatural, thereby reducing the stealthiness of the watermark. To address these\nissues, we propose EditMark, the first watermarking method that leverages model\nediting to embed a training-free, stealthy, and performance-lossless watermark\nfor LLMs. We observe that some questions have multiple correct answers.\nTherefore, we assign each answer a unique watermark and update the weights of\nLLMs to generate corresponding questions and answers through the model editing\ntechnique. In addition, we refine the model editing technique to align with the\nrequirements of watermark embedding. Specifically, we introduce an adaptive\nmulti-round stable editing strategy, coupled with the injection of a noise\nmatrix, to improve both the effectiveness and robustness of the watermark\nembedding. Extensive experiments indicate that EditMark can embed 32-bit\nwatermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a\nwatermark extraction success rate of 100%, which demonstrates its effectiveness\nand efficiency. External experiments further demonstrate that EditMark has\nfidelity, stealthiness, and a certain degree of robustness against common\nattacks."}
{"id": "2510.16040", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16040", "abs": "https://arxiv.org/abs/2510.16040", "authors": ["Tianhua Xia", "Sai Qian Zhang"], "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing", "comment": null, "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions."}
{"id": "2510.16533", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16533", "abs": "https://arxiv.org/abs/2510.16533", "authors": ["Eilene Tomkins-Flanagan", "Connor Hanley", "Mary A. Kelly"], "title": "Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination", "comment": null, "summary": "We present a typed computer language, Doug, in which all typed programs may\nbe proved to halt in polynomial time, encoded in a vector-symbolic architecture\n(VSA). Doug is just an encoding of the light linear functional programming\nlanguage (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are\nencoded using a slot-value encoding scheme based on holographic declarative\nmemory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the\nLisp VSA defined by (Flanagan, 2024). Doug allows for some points on the\nembedding space of a neural network to be interpreted as types, where the types\nof nearby points are similar both in structure and content. Types in Doug are\ntherefore learnable by a neural network. Following (Chollet, 2019), (Card,\n1983), and (Newell, 1981), we view skill as the application of a procedure, or\nprogram of action, that causes a goal to be satisfied. Skill acquisition may\ntherefore be expressed as program synthesis. Using Doug, we hope to describe a\nform of learning of skilled behaviour that follows a human-like pace of skill\nacquisition (i.e., substantially faster than brute force; Heathcote, 2000),\nexceeding the efficiency of all currently existing approaches (Kaplan, 2020;\nJones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling\nhuman mental representations, as they must actually exist in the brain, and\nthose representations' acquisition, as they are actually learned."}
{"id": "2510.16461", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16461", "abs": "https://arxiv.org/abs/2510.16461", "authors": ["Minjae Seo", "Jaehan Kim", "Eduard Marin", "Myoungsung You", "Taejune Park", "Seungsoo Lee", "Seungwon Shin", "Jinwoo Kim"], "title": "Heimdallr: Fingerprinting SD-WAN Control-Plane Architecture via Encrypted Control Traffic", "comment": "14 pages, 14 figures", "summary": "Software-defined wide area network (SD-WAN) has emerged as a new paradigm for\nsteering a large-scale network flexibly by adopting distributed\nsoftware-defined network (SDN) controllers. The key to building a logically\ncentralized but physically distributed control-plane is running diverse cluster\nmanagement protocols to achieve consistency through an exchange of control\ntraffic. Meanwhile, we observe that the control traffic exposes unique\ntime-series patterns and directional relationships due to the operational\nstructure even though the traffic is encrypted, and this pattern can disclose\nconfidential information such as control-plane topology and protocol\ndependencies, which can be exploited for severe attacks. With this insight, we\npropose a new SD-WAN fingerprinting system, called Heimdallr. It analyzes\nperiodical and operational patterns of SD-WAN cluster management protocols and\nthe context of flow directions from the collected control traffic utilizing a\ndeep learning-based approach, so that it can classify the cluster management\nprotocols automatically from miscellaneous control traffic datasets. Our\nevaluation, which is performed in a realistic SD-WAN environment consisting of\ngeographically distant three campus networks and one enterprise network shows\nthat Heimdallr can classify SD-WAN control traffic with $\\geq$ 93%, identify\nindividual protocols with $\\geq$ 80% macro F-1 scores, and finally can infer\ncontrol-plane topology with $\\geq$ 70% similarity."}
{"id": "2510.16487", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.16487", "abs": "https://arxiv.org/abs/2510.16487", "authors": ["Giovanni Agosta", "Stefano Cherubin", "Derek Christ", "Francesco Conti", "Asbjørn Djupdal", "Matthias Jung", "Georgios Keramidas", "Roberto Passerone", "Paolo Rech", "Elisa Ricci", "Philippe Velha", "Flavio Vella", "Kasim Sinan Yildirim", "Nils Wilbert"], "title": "Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project", "comment": null, "summary": "ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,\nin particular, optoelectronic, volatile and non-volatile processing-in-memory,\nand neuromorphic, to tackle the power, efficiency, and scalability bottlenecks\nof AI with an emphasis on defense use cases (e.g., autonomous vehicles,\nsurveillance drones, maritime and space platforms). In this paper, we present\nthe system architecture and software stack that ARCHYTAS will develop to\nintegrate and support those accelerators, as well as the simulation software\nneeded for early prototyping of the full system and its components."}
{"id": "2510.16555", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16555", "abs": "https://arxiv.org/abs/2510.16555", "authors": ["Qiongyan Wang", "Xingchen Zou", "Yutian Jiang", "Haomin Wen", "Jiaheng Wei", "Qingsong Wen", "Yuxuan Liang"], "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence", "comment": null, "summary": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence."}
{"id": "2510.16544", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16544", "abs": "https://arxiv.org/abs/2510.16544", "authors": ["Weijie Chen", "Shan Tang", "Yulin Tang", "Xiapu Luo", "Yinqian Zhang", "Weizhong Qiang"], "title": "$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching", "comment": "Accepted for publication in the 58th IEEE/ACM International Symposium\n  on Microarchitecture (MICRO '25). This is the author's version of the paper", "summary": "Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)\nthat continues to pose a significant threat to various systems. However, we\nfind that conventional load-based attacks are becoming highly ineffective on\nthe most recent architectures such as Intel Alder and Raptor Lake. In this\npaper, we present $\\rho$Hammer, a new Rowhammer framework that systematically\novercomes three core challenges impeding attacks on these new architectures.\nFirst, we design an efficient and generic DRAM address mapping\nreverse-engineering method that uses selective pairwise measurements and\nstructured deduction, enabling recovery of complex mappings within seconds on\nthe latest memory controllers. Second, to break through the activation rate\nbottleneck of load-based hammering, we introduce a novel prefetch-based\nhammering paradigm that leverages the asynchronous nature of x86 prefetch\ninstructions and is further enhanced by multi-bank parallelism to maximize\nthroughput. Third, recognizing that speculative execution causes more severe\ndisorder issues for prefetching, which cannot be simply mitigated by memory\nbarriers, we develop a counter-speculation hammering technique using\ncontrol-flow obfuscation and optimized NOP-based pseudo-barriers to maintain\nprefetch order with minimal overhead. Evaluations across four latest Intel\narchitectures demonstrate $\\rho$Hammer's breakthrough effectiveness: it induces\nup to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes\nand has a 112x higher flip rate than the load-based hammering baselines on\nComet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on\nthe latest Raptor Lake architecture, where baselines completely fail, achieving\nstable flip rates of 2,291/min and fast end-to-end exploitation."}
{"id": "2510.16622", "categories": ["cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16622", "abs": "https://arxiv.org/abs/2510.16622", "authors": ["Kazi Ababil Azam", "Hasan Masum", "Masfiqur Rahaman", "A. B. M. Alim Al Islam"], "title": "Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization", "comment": "10 pages, Submitted to IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS)", "summary": "The vehicular density in urbanizing cities of developing countries such as\nDhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road\nexperiences. Traffic signaling is a key component in effective traffic\nmanagement for such situations, but the advancements in intelligent traffic\nsignaling have been exclusive to developed countries with structured traffic.\nThe non-lane-based, heterogeneous traffic of Dhaka City requires a contextual\napproach. This study focuses on the development of an intelligent traffic\nsignaling system feasible in the context of developing countries such as\nBangladesh. We propose a pipeline leveraging Real Time Streaming Protocol\n(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of\nthe art YOLO-based object detection model trained on the Non-lane-based and\nHeterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous\ntraffic. A multi-objective optimization algorithm, NSGA-II, then generates\noptimized signal timings, minimizing waiting time while maximizing vehicle\nthroughput. We test our implementation in a five-road intersection at Palashi,\nDhaka, demonstrating the potential to significantly improve traffic management\nin similar situations. The developed testbed paves the way for more contextual\nand effective Intelligent Traffic Signaling (ITS) solutions for developing\nareas with complicated traffic dynamics such as Dhaka City."}
{"id": "2510.16559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16559", "abs": "https://arxiv.org/abs/2510.16559", "authors": ["Tian Xia", "Tianrun Gao", "Wenhao Deng", "Long Wei", "Xiaowei Qian", "Yixian Jiang", "Chenglei Yu", "Tailin Wu"], "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction", "comment": "33 pages, 10 figures", "summary": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/."}
{"id": "2510.16558", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16558", "abs": "https://arxiv.org/abs/2510.16558", "authors": ["Xiaofan Li", "Xing Gao"], "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem", "comment": null, "summary": "The Model Context Protocol (MCP) is an emerging open standard that enables\nAI-powered applications to interact with external tools through structured\nmetadata. A rapidly growing ecosystem has formed around MCP, including a wide\nrange of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP\nregistries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),\nand thousands of community-contributed MCP servers. Although the MCP ecosystem\nis gaining traction, there has been little systematic study of its architecture\nand associated security risks. In this paper, we present the first\ncomprehensive security analysis of the MCP ecosystem. We decompose MCP\necosystem into three core components: hosts, registries, and servers, and study\nthe interactions and trust relationships among them. Users search for servers\non registries and configure them in the host, which translates LLM-generated\noutput into external tool invocations provided by the servers and executes\nthem. Our qualitative analysis reveals that hosts lack output verification\nmechanisms for LLM-generated outputs, enabling malicious servers to manipulate\nmodel behavior and induce a variety of security threats, including but not\nlimited to sensitive data exfiltration. We uncover a wide range of\nvulnerabilities that enable attackers to hijack servers, due to the lack of a\nvetted server submission process in registries. To support our analysis, we\ncollect and analyze a dataset of 67,057 servers from six public registries. Our\nquantitative analysis demonstrates that a substantial number of servers can be\nhijacked by attackers. Finally, we propose practical defense strategies for MCP\nhosts, registries, and users. We responsibly disclosed our findings to affected\nhosts and registries."}
{"id": "2510.17251", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17251", "abs": "https://arxiv.org/abs/2510.17251", "authors": ["Chengxi Li", "Yang Sun", "Lei Chen", "Yiwen Wang", "Mingxuan Yuan", "Evangeline F. Y. Young"], "title": "SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding", "comment": null, "summary": "This paper proposes smaRTLy: a new optimization technique for multiplexers in\nRegister-Transfer Level (RTL) logic synthesis. Multiplexer trees are very\ncommon in RTL designs, and traditional tools like Yosys optimize them by\ntraversing the tree and monitoring control port values. However, this method\ndoes not fully exploit the intrinsic logical relationships among signals or the\npotential for structural optimization. To address these limitations, we develop\ninnovative strategies to remove redundant multiplexer trees and restructure the\nremaining ones, significantly reducing the overall gate count. We evaluate\nsmaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%\nreduction in AIG area compared to Yosys. We also evaluate smaRTLy on an\nindustrial benchmark in the scale of millions of gates, results show that\nsmaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate\nthe effectiveness of our logic inferencing and structural rebuilding techniques\nin enhancing the RTL optimization process, leading to more efficient hardware\ndesigns."}
{"id": "2510.16572", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16572", "abs": "https://arxiv.org/abs/2510.16572", "authors": ["Ayush Chopra", "Aman Sharma", "Feroz Ahmad", "Luca Muscariello", "Vijoy Pandey", "Ramesh Raskar"], "title": "Ripple Effect Protocol: Coordinating Agent Populations", "comment": null, "summary": "Modern AI agents can exchange messages using protocols such as A2A and ACP,\nyet these mechanisms emphasize communication over coordination. As agent\npopulations grow, this limitation produces brittle collective behavior, where\nindividually smart agents converge on poor group outcomes. We introduce the\nRipple Effect Protocol (REP), a coordination protocol in which agents share not\nonly their decisions but also lightweight sensitivities - signals expressing\nhow their choices would change if key environmental variables shifted. These\nsensitivities ripple through local networks, enabling groups to align faster\nand more stably than with agent-centric communication alone. We formalize REP's\nprotocol specification, separating required message schemas from optional\naggregation rules, and evaluate it across scenarios with varying incentives and\nnetwork topologies. Benchmarks across three domains: (i) supply chain cascades\n(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),\nand (iii) sustainable resource allocation (Fishbanks) show that REP improves\ncoordination accuracy and efficiency over A2A by 41 to 100%, while flexibly\nhandling multimodal sensitivity signals from LLMs. By making coordination a\nprotocol-level capability, REP provides scalable infrastructure for the\nemerging Internet of Agents"}
{"id": "2510.16581", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16581", "abs": "https://arxiv.org/abs/2510.16581", "authors": ["Xinfeng Li", "Shengyuan Pang", "Jialin Wu", "Jiangyi Deng", "Huanlong Zhong", "Yanjiao Chen", "Jie Zhang", "Wenyuan Xu"], "title": "Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries", "comment": "14 pages, 18 figures, 7 tables", "summary": "Text-to-image (T2I) models, though exhibiting remarkable creativity in image\ngeneration, can be exploited to produce unsafe images. Existing safety\nmeasures, e.g., content moderation or model alignment, fail in the presence of\nwhite-box adversaries who know and can adjust model parameters, e.g., by\nfine-tuning. This paper presents a novel defensive framework, named Patronus,\nwhich equips T2I models with holistic protection to defend against white-box\nadversaries. Specifically, we design an internal moderator that decodes unsafe\ninput features into zero vectors while ensuring the decoding performance of\nbenign input features. Furthermore, we strengthen the model alignment with a\ncarefully designed non-fine-tunable learning mechanism, ensuring the T2I model\nwill not be compromised by malicious fine-tuning. We conduct extensive\nexperiments to validate the intactness of the performance on safe content\ngeneration and the effectiveness of rejecting unsafe content generation.\nResults also confirm the resilience of Patronus against various fine-tuning\nattacks by white-box adversaries."}
{"id": "2510.16582", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16582", "abs": "https://arxiv.org/abs/2510.16582", "authors": ["Junchi Yu", "Yujie Liu", "Jindong Gu", "Philip Torr", "Dongzhan Zhou"], "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances\nlarge language models (LLMs) by providing structured and interpretable external\nknowledge. However, existing KG-based RAG methods struggle to retrieve accurate\nand diverse information from text-rich KGs for complex real-world queries.\nProcess Reward Models (PRMs) offer a way to align the retrieval process of\nKG-based RAG with query-specific knowledge requirements, but they heavily rely\non process-level supervision signals that are expensive and hard to obtain on\nKGs. To address this challenge, we propose GraphFlow, a framework that\nefficiently retrieves accurate and diverse knowledge required for real-world\nqueries from text-rich KGs. GraphFlow employs a transition-based flow matching\nobjective to jointly optimize a retrieval policy and a flow estimator. The flow\nestimator factorizes the reward of the retrieval outcome into the intermediate\nretrieval states. Such reward factorization guides the retrieval policy to\nretrieve candidates from KGs in proportion to their reward. This allows\nGraphFlow to explore high-quality regions of KGs that yield diverse and\nrelevant results. We evaluate GraphFlow on the STaRK benchmark, which includes\nreal-world queries from multiple domains over text-rich KGs. GraphFlow\noutperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit\nrate and recall. It also shows strong generalization to unseen KGs,\ndemonstrating its effectiveness and robustness."}
{"id": "2510.16593", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16593", "abs": "https://arxiv.org/abs/2510.16593", "authors": ["Khandaker Akramul Haque", "Katherine R. Davis"], "title": "DESTinE Block: Private Blockchain Based Data Storage Framework for Power System", "comment": null, "summary": "This paper presents DESTinE Block, a blockchain-based data storage framework\ndesigned for power systems and optimized for resource-constrained environments,\nincluding grid-edge devices such as single-board computers. The proposed\narchitecture leverages the InterPlanetary File System (IPFS) for storing large\nfiles while maintaining secure and traceable metadata on a custom blockchain\nnamed DESTinE Block. The metadata, comprising the IPFS Content Identifier\n(CID), uploader identity, administrator verification, and timestamp; is\nimmutably recorded on-chain to ensure authenticity and integrity. DESTinE Block\nadopts a dual-blockchain abstraction, where the blockchain remains unaware of\nthe IPFS storage layer to enhance security and limit the exposure of sensitive\nfile data. The consensus mechanism is based on Proof of Authority (PoA), where\nboth an administrator and an uploader with distinct cryptographic key pairs are\nrequired to create a block collaboratively. Each block contains verified\nsignatures of both parties and is designed to be computationally efficient,\nenabling deployment on devices like the Raspberry Pi 5. The framework was\ntested on both an x86-based device and an ARM64-based Raspberry Pi,\ndemonstrating its potential for secure, decentralized logging and measurement\nstorage in smart grid applications. Moreover, DESTinE Block is compared with a\nsimilar framework based on Multichain. The results indicate that DESTinE Block\nprovides a promising solution for tamper-evident data retention in distributed\npower system infrastructure while maintaining minimal hardware requirements."}
{"id": "2510.16601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16601", "abs": "https://arxiv.org/abs/2510.16601", "authors": ["Tianxing Wu", "Shutong Zhu", "Jingting Wang", "Ning Xu", "Guilin Qi", "Haofen Wang"], "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning", "comment": "13 pages, accepted by NeurIPS 2025 (spotlight)", "summary": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence\nscore to provide more precise knowledge representations. Recently, since\nreal-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)\ncompletion attracts more attention, aiming to complete missing triples and\nconfidences. Current studies attempt to learn UKG embeddings to solve this\nproblem, but they neglect the extremely imbalanced distributions of triple\nconfidences. This causes that the learnt embeddings are insufficient to\nhigh-quality UKG completion. Thus, in this paper, to address the above issue,\nwe propose a new semi-supervised Confidence Distribution Learning (ssCDL)\nmethod for UKG completion, where each triple confidence is transformed into a\nconfidence distribution to introduce more supervision information of different\nconfidences to reinforce the embedding learning process. ssCDL iteratively\nlearns UKG embedding by relational learning on labeled data (i.e., existing\ntriples with confidences) and unlabeled data with pseudo labels (i.e., unseen\ntriples with the generated confidences), which are predicted by meta-learning\nto augment the training data and rebalance the distribution of triple\nconfidences. Experiments on two UKG datasets demonstrate that ssCDL\nconsistently outperforms state-of-the-art baselines in different evaluation\nmetrics."}
{"id": "2510.16610", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16610", "abs": "https://arxiv.org/abs/2510.16610", "authors": ["Bruno Lourenço", "Pedro Adão", "João F. Ferreira", "Mario Monteiro Marques", "Cátia Vaz"], "title": "Structuring Security: A Survey of Cybersecurity Ontologies, Semantic Log Processing, and LLMs Application", "comment": null, "summary": "This survey investigates how ontologies, semantic log processing, and Large\nLanguage Models (LLMs) enhance cybersecurity. Ontologies structure domain\nknowledge, enabling interoperability, data integration, and advanced threat\nanalysis. Security logs, though critical, are often unstructured and complex.\nTo address this, automated construction of Knowledge Graphs (KGs) from raw logs\nis emerging as a key strategy for organizing and reasoning over security data.\nLLMs enrich this process by providing contextual understanding and extracting\ninsights from unstructured content. This work aligns with European Union (EU)\nefforts such as NIS 2 and the Cybersecurity Taxonomy, highlighting challenges\nand opportunities in intelligent ontology-driven cyber defense."}
{"id": "2510.16614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16614", "abs": "https://arxiv.org/abs/2510.16614", "authors": ["Xuan Zhang", "Ruixiao Li", "Zhijian Zhou", "Long Li", "Yulei Qin", "Ke Li", "Xing Sun", "Xiaoyu Tan", "Chao Qu", "Yuan Qi"], "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards", "comment": null, "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning."}
{"id": "2510.16637", "categories": ["cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16637", "abs": "https://arxiv.org/abs/2510.16637", "authors": ["Alireza Heshmati", "Saman Soleimani Roudi", "Sajjad Amini", "Shahrokh Ghaemmaghami", "Farokh Marvasti"], "title": "A Versatile Framework for Designing Group-Sparse Adversarial Attacks", "comment": null, "summary": "Existing adversarial attacks often neglect perturbation sparsity, limiting\ntheir ability to model structural changes and to explain how deep neural\nnetworks (DNNs) process meaningful input patterns. We propose ATOS (Attack\nThrough Overlapping Sparsity), a differentiable optimization framework that\ngenerates structured, sparse adversarial perturbations in element-wise,\npixel-wise, and group-wise forms. For white-box attacks on image classifiers,\nwe introduce the Overlapping Smoothed L0 (OSL0) function, which promotes\nconvergence to a stationary point while encouraging sparse, structured\nperturbations. By grouping channels and adjacent pixels, ATOS improves\ninterpretability and helps identify robust versus non-robust features. We\napproximate the L-infinity gradient using the logarithm of the sum of\nexponential absolute values to tightly control perturbation magnitude. On\nCIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing\nsignificantly sparser and more structurally coherent perturbations than prior\nmethods. The structured group-wise attack highlights critical regions from the\nnetwork's perspective, providing counterfactual explanations by replacing\nclass-defining regions with robust features from the target class."}
{"id": "2510.16658", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16658", "abs": "https://arxiv.org/abs/2510.16658", "authors": ["Shihao Yang", "Xiying Huang", "Danilo Bernardo", "Jun-En Ding", "Andrew Michael", "Jingmei Yang", "Patrick Kwan", "Ashish Raj", "Feng Liu"], "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review", "comment": null, "summary": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided."}
{"id": "2510.16706", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16706", "abs": "https://arxiv.org/abs/2510.16706", "authors": ["Hongjie Zhang", "Zhiqi Zhao", "Hanzhou Wu", "Zhihua Xia", "Athanasios V. Vasilakos"], "title": "Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models", "comment": null, "summary": "Feature embedding has become a cornerstone technology for processing\nhigh-dimensional and complex data, which results in that Embedding as a Service\n(EaaS) models have been widely deployed in the cloud. To protect the\nintellectual property of EaaS models, existing methods apply digital\nwatermarking to inject specific backdoor triggers into EaaS models by modifying\ntraining samples or network parameters. However, these methods inevitably\nproduce detectable patterns through semantic analysis and exhibit\nsusceptibility to geometric transformations including rotation, scaling, and\ntranslation (RST). To address this problem, we propose a fingerprinting\nframework for EaaS models, rather than merely refining existing watermarking\ntechniques. Different from watermarking techniques, the proposed method\nestablishes EaaS model ownership through geometric analysis of embedding\nspace's topological structure, rather than relying on the modified training\nsamples or triggers. The key innovation lies in modeling the victim and\nsuspicious embeddings as point clouds, allowing us to perform robust spatial\nalignment and similarity measurement, which inherently resists RST attacks.\nExperimental results evaluated on visual and textual embedding tasks verify the\nsuperiority and applicability. This research reveals inherent characteristics\nof EaaS models and provides a promising solution for ownership verification of\nEaaS models under the black-box scenario."}
{"id": "2510.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16701", "abs": "https://arxiv.org/abs/2510.16701", "authors": ["Ni Zhang", "Zhiguang Cao", "Jianan Zhou", "Cong Zhang", "Yew-Soon Ong"], "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems", "comment": null, "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks."}
{"id": "2510.16716", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16716", "abs": "https://arxiv.org/abs/2510.16716", "authors": ["Asmita Mohanty", "Gezheng Kang", "Lei Gao", "Murali Annavaram"], "title": "DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance across\ndiverse tasks, but fine-tuning them typically relies on cloud-based,\ncentralized infrastructures. This requires data owners to upload potentially\nsensitive data to external servers, raising serious privacy concerns. An\nalternative approach is to fine-tune LLMs directly on edge devices using local\ndata; however, this introduces a new challenge: the model owner must transfer\nproprietary models to the edge, which risks intellectual property (IP) leakage.\nTo address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning\nframework that enables privacy-preserving knowledge distillation on the edge.\nIn DistilLock, a proprietary foundation model is executed within a trusted\nexecution environment (TEE) enclave on the data owner's device, acting as a\nsecure black-box teacher. This setup preserves both data privacy and model IP\nby preventing direct access to model internals. Furthermore, DistilLock employs\na model obfuscation mechanism to offload obfuscated weights to untrusted\naccelerators for efficient knowledge distillation without compromising\nsecurity. We demonstrate that DistilLock prevents unauthorized knowledge\ndistillation processes and model-stealing attacks while maintaining high\ncomputational efficiency, but offering a secure and practical solution for\nedge-based LLM personalization."}
{"id": "2510.16720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16720", "abs": "https://arxiv.org/abs/2510.16720", "authors": ["Jitao Sang", "Jinlin Xiao", "Jiarun Han", "Jilin Chen", "Xiaoyi Chen", "Shuyu Wei", "Yongjie Sun", "Yuhang Wang"], "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI", "comment": null, "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience."}
{"id": "2510.16744", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16744", "abs": "https://arxiv.org/abs/2510.16744", "authors": ["Srinivas Vivek"], "title": "Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022", "comment": "9 pages", "summary": "Ride-Hailing Services (RHS) match a ride request initiated by a rider with a\nsuitable driver responding to the ride request. A Privacy-Preserving RHS\n(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'\nand drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie\net al. proposed a PP-RHS. In this work, we demonstrate a passive attack on\ntheir PP-RHS protocol. Our attack allows the SP to completely recover the\nlocations of the rider as well as that of the responding drivers in every ride\nrequest. Further, our attack is very efficient as it is independent of the\nsecurity parameter."}
{"id": "2510.16724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16724", "abs": "https://arxiv.org/abs/2510.16724", "authors": ["Minhua Lin", "Zongyu Wu", "Zhichao Xu", "Hui Liu", "Xianfeng Tang", "Qi He", "Charu Aggarwal", "Hui Liu", "Xiang Zhang", "Suhang Wang"], "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications", "comment": "38 pages, 4 figures, 7 tables", "summary": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers."}
{"id": "2510.16794", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16794", "abs": "https://arxiv.org/abs/2510.16794", "authors": ["Jie Zhang", "Meng Ding", "Yang Liu", "Jue Hong", "Florian Tramèr"], "title": "Black-box Optimization of LLM Outputs by Asking for Directions", "comment": null, "summary": "We present a novel approach for attacking black-box large language models\n(LLMs) by exploiting their ability to express confidence in natural language.\nExisting black-box attacks require either access to continuous model outputs\nlike logits or confidence scores (which are rarely available in practice), or\nrely on proxy signals from other models. Instead, we demonstrate how to prompt\nLLMs to express their internal confidence in a way that is sufficiently\ncalibrated to enable effective adversarial optimization. We apply our general\nmethod to three attack scenarios: adversarial examples for vision-LLMs,\njailbreaks and prompt injections. Our attacks successfully generate malicious\ninputs against systems that only expose textual outputs, thereby dramatically\nexpanding the attack surface for deployed LLMs. We further find that better and\nlarger models exhibit superior calibration when expressing confidence, creating\na concerning security paradox where model capability improvements directly\nenhance vulnerability. Our code is available at this\n[link](https://github.com/zj-jayzhang/black_box_llm_optimization)."}
{"id": "2510.16742", "categories": ["cs.AI", "cs.MA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16742", "abs": "https://arxiv.org/abs/2510.16742", "authors": ["Paul Saves", "Pramudita Satria Palar", "Muhammad Daffa Robani", "Nicolas Verstaevel", "Moncef Garouani", "Julien Aligon", "Benoit Gaudou", "Koji Shimoyama", "Joseph Morlier"], "title": "Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration", "comment": null, "summary": "Complex systems are increasingly explored through simulation-driven\nengineering workflows that combine physics-based and empirical models with\noptimization and analytics. Despite their power, these workflows face two\ncentral obstacles: (1) high computational cost, since accurate exploration\nrequires many expensive simulator runs; and (2) limited transparency and\nreliability when decisions rely on opaque blackbox components. We propose a\nworkflow that addresses both challenges by training lightweight emulators on\ncompact designs of experiments that (i) provide fast, low-latency\napproximations of expensive simulators, (ii) enable rigorous uncertainty\nquantification, and (iii) are adapted for global and local Explainable\nArtificial Intelligence (XAI) analyses. This workflow unifies every\nsimulation-based complex-system analysis tool, ranging from engineering design\nto agent-based models for socio-environmental understanding. In this paper, we\nproposea comparative methodology and practical recommendations for using\nsurrogate-based explainability tools within the proposed workflow. The\nmethodology supports continuous and categorical inputs, combines global-effect\nand uncertainty analyses with local attribution, and evaluates the consistency\nof explanations across surrogate models, thereby diagnosing surrogate adequacy\nand guiding further data collection or model refinement. We demonstrate the\napproach on two contrasting case studies: a multidisciplinary design analysis\nof a hybrid-electric aircraft and an agent-based model of urban segregation.\nResults show that the surrogate model and XAI coupling enables large-scale\nexploration in seconds, uncovers nonlinear interactions and emergent behaviors,\nidentifies key design and policy levers, and signals regions where surrogates\nrequire more data or alternative architectures."}
{"id": "2510.16830", "categories": ["cs.CR", "cs.CL", "68T07, 94A60, 68Q25", "I.2.6; G.1.6; E.3; C.2.4"], "pdf": "https://arxiv.org/pdf/2510.16830", "abs": "https://arxiv.org/abs/2510.16830", "authors": ["Hasan Akgul", "Daniel Borg", "Arta Berisha", "Amina Rahimova", "Andrej Novak", "Mila Petrov"], "title": "Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy", "comment": "20 pages, 10 figures", "summary": "Large language models are often adapted through parameter efficient fine\ntuning, but current release practices provide weak assurances about what data\nwere used and how updates were computed. We present Verifiable Fine Tuning, a\nprotocol and system that produces succinct zero knowledge proofs that a\nreleased model was obtained from a public initialization under a declared\ntraining program and an auditable dataset commitment. The approach combines\nfive elements. First, commitments that bind data sources, preprocessing,\nlicenses, and per epoch quota counters to a manifest. Second, a verifiable\nsampler that supports public replayable and private index hiding batch\nselection. Third, update circuits restricted to parameter efficient fine tuning\nthat enforce AdamW style optimizer semantics and proof friendly approximations\nwith explicit error budgets. Fourth, recursive aggregation that folds per step\nproofs into per epoch and end to end certificates with millisecond\nverification. Fifth, provenance binding and optional trusted execution property\ncards that attest code identity and constants. On English and bilingual\ninstruction mixtures, the method maintains utility within tight budgets while\nachieving practical proof performance. Policy quotas are enforced with zero\nviolations, and private sampling windows show no measurable index leakage.\nFederated experiments demonstrate that the system composes with probabilistic\naudits and bandwidth constraints. These results indicate that end to end\nverifiable fine tuning is feasible today for real parameter efficient\npipelines, closing a critical trust gap for regulated and decentralized\ndeployments."}
{"id": "2510.16753", "categories": ["cs.AI", "68T30", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16753", "abs": "https://arxiv.org/abs/2510.16753", "authors": ["Wei Huang", "Peining Li", "Meiyu Liang", "Xu Hou", "Junping Du", "Yingxia Shao", "Guanhua Ye", "Wu Liu", "Kangkang Lu", "Yang Yu"], "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion", "comment": "11 pages, 4 figures", "summary": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion."}
{"id": "2510.16835", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16835", "abs": "https://arxiv.org/abs/2510.16835", "authors": ["Hongpeng Bai", "Minhong Dong", "Yao Zhang", "Shunzhe Zhao", "Haobo Zhang", "Lingyue Li", "Yude Bai", "Guangquan Xu"], "title": "ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research", "comment": null, "summary": "The rapidly evolving Android malware ecosystem demands high-quality,\nreal-time datasets as a foundation for effective detection and defense. With\nthe widespread adoption of mobile devices across industrial systems, they have\nbecome a critical yet often overlooked attack surface in industrial\ncybersecurity. However, mainstream datasets widely used in academia and\nindustry (e.g., Drebin) exhibit significant limitations: on one hand, their\nheavy reliance on VirusTotal's multi-engine aggregation results introduces\nsubstantial label noise; on the other hand, outdated samples reduce their\ntemporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer\nfrom suboptimal aggregation strategies, further compounding labeling errors and\npropagating inaccuracies throughout the research community."}
{"id": "2510.16756", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16756", "abs": "https://arxiv.org/abs/2510.16756", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Chao Zhang"], "title": "End-to-end Listen, Look, Speak and Act", "comment": "22 pages, 8 figures", "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance."}
{"id": "2510.16871", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16871", "abs": "https://arxiv.org/abs/2510.16871", "authors": ["Anirban Chakraborty", "Nimish Mishra", "Sayandeep Saha", "Sarani Bhattacharya", "Debdeep Mukhopadhyay"], "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy", "comment": null, "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."}
{"id": "2510.16769", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16769", "abs": "https://arxiv.org/abs/2510.16769", "authors": ["Shuo Han", "Yukun Cao", "Zezhong Ding", "Zengyi Gao", "S Kevin Zhou", "Xike Xie"], "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities."}
{"id": "2510.16873", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16873", "abs": "https://arxiv.org/abs/2510.16873", "authors": ["Jacob Leiken", "Sunoo Park"], "title": "On the Credibility of Deniable Communication in Court", "comment": null, "summary": "Over time, cryptographically deniable systems have come to be associated in\ncomputer-science literature with the idea of \"denying\" evidence in court -\nspecifically, with the ability to convincingly forge evidence in courtroom\nscenarios and an inability to authenticate evidence in such contexts.\nEvidentiary processes in courts, however, have been developed over centuries to\naccount for the reality that evidence has always been forgeable, and relies on\nfactors outside of cryptographic models to seek the truth \"as well as possible\"\nwhile acknowledging that all evidence is imperfect. We argue that deniability\ndoes not and need not change this paradigm.\n  Our analysis highlights a gap between technical deniability notions and their\napplication to the real world. There will always be factors outside a\ncryptographic model that influence perceptions of a message's authenticity, in\nrealistic situations. We propose the broader concept of credibility to capture\nthese factors. The credibility of a system is determined by (1) a threshold of\nquality that a forgery must pass to be \"believable\" as an original\ncommunication, which varies based on sociotechnical context and threat model,\n(2) the ease of creating a forgery that passes this threshold, which is also\ncontext- and threat-model-dependent, and (3) default system retention policy\nand retention settings. All three aspects are important for designing secure\ncommunication systems for real-world threat models, and some aspects of (2) and\n(3) may be incorporated directly into technical system design. We hope that our\nmodel of credibility will facilitate system design and deployment that\naddresses threats that are not and cannot be captured by purely technical\ndefinitions and existing cryptographic models, and support more nuanced\ndiscourse on the strengths and limitations of cryptographic guarantees within\nspecific legal and sociotechnical contexts."}
{"id": "2510.16802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16802", "abs": "https://arxiv.org/abs/2510.16802", "authors": ["Chao Li", "Yuru Wang"], "title": "Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation", "comment": "14 pages", "summary": "Traditional knowledge graphs are constrained by fixed ontologies that\norganize concepts within rigid hierarchical structures. The root cause lies in\ntreating domains as implicit context rather than as explicit, reasoning-level\ncomponents. To overcome these limitations, we propose the Domain-Contextualized\nConcept Graph (CDC), a novel knowledge modeling framework that elevates domains\nto first-class elements of conceptual representation. CDC adopts a C-D-C triple\nstructure - <Concept, Relation@Domain, Concept'> - where domain specifications\nserve as dynamic classification dimensions defined on demand. Grounded in a\ncognitive-linguistic isomorphic mapping principle, CDC operationalizes how\nhumans understand concepts through contextual frames. We formalize more than\ntwenty standardized relation predicates (structural, logical, cross-domain, and\ntemporal) and implement CDC in Prolog for full inference capability. Case\nstudies in education, enterprise knowledge systems, and technical documentation\ndemonstrate that CDC enables context-aware reasoning, cross-domain analogy, and\npersonalized knowledge modeling - capabilities unattainable under traditional\nontology-based frameworks."}
{"id": "2510.16923", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16923", "abs": "https://arxiv.org/abs/2510.16923", "authors": ["Mansi Phute", "Matthew Hull", "Haoran Wang", "Alec Helbling", "ShengYun Peng", "Willian Lunardi", "Martin Andreoni", "Wenke Lee", "Polo Chau"], "title": "UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks", "comment": null, "summary": "Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks."}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science."}
{"id": "2510.16959", "categories": ["cs.CR", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.16959", "abs": "https://arxiv.org/abs/2510.16959", "authors": ["Surendra Ghentiyala"], "title": "Efficient derandomization of differentially private counting queries", "comment": "Accepted to SOSA'26", "summary": "Differential privacy for the 2020 census required an estimated 90 terabytes\nof randomness [GL20], an amount which may be prohibitively expensive or\nentirely infeasible to generate. Motivated by these practical concerns, [CSV25]\ninitiated the study of the randomness complexity of differential privacy, and\nin particular, the randomness complexity of $d$ counting queries. This is the\ntask of outputting the number of entries in a dataset that satisfy predicates\n$\\mathcal{P}_1, \\dots, \\mathcal{P}_d$ respectively. They showed the rather\nsurprising fact that though any reasonably accurate,\n$\\varepsilon$-differentially private mechanism for one counting query requires\n$1-O(\\varepsilon)$ bits of randomness in expectation, there exists a fairly\naccurate mechanism for $d$ counting queries which requires only $O(\\log d)$\nbits of randomness in expectation.\n  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a\ncombinatorial object known as rounding schemes. Here, we give a polynomial time\nmechanism which achieves nearly the same randomness complexity versus accuracy\ntradeoff as that of [CSV25]. Our construction is based on the following simple\nobservation: after a randomized shift of the answer to each counting query, the\nanswer to many counting queries remains the same regardless of whether we add\nnoise to that coordinate or not. This allows us to forgo the step of adding\nnoise to the result of many counting queries. Our mechanism does not make use\nof rounding schemes. Therefore, it provides a different -- and, in our opinion,\nclearer -- insight into the origins of the randomness savings that can be\nobtained by batching $d$ counting queries. Therefore, it provides a different\n-- and, in our opinion, clearer -- insight into the origins of the randomness\nsavings that can be obtained by batching $d$ counting queries."}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io."}
{"id": "2510.17000", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17000", "abs": "https://arxiv.org/abs/2510.17000", "authors": ["Masahiro Kaneko", "Timothy Baldwin"], "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs", "comment": "NeurIPS 2025 (spotlight)", "summary": "Adversarial attacks by malicious users that threaten the safety of large\nlanguage models (LLMs) can be viewed as attempts to infer a target property $T$\nthat is unknown when an instruction is issued, and becomes knowable only after\nthe model's reply is observed. Examples of target properties $T$ include the\nbinary flag that triggers an LLM's harmful response or rejection, and the\ndegree to which information deleted by unlearning can be restored, both\nelicited via adversarial instructions. The LLM reveals an \\emph{observable\nsignal} $Z$ that potentially leaks hints for attacking through a response\ncontaining answer tokens, thinking process tokens, or logits. Yet the scale of\ninformation leaked remains anecdotal, leaving auditors without principled\nguidance and defenders blind to the transparency--risk trade-off. We fill this\ngap with an information-theoretic framework that computes how much information\ncan be safely disclosed, and enables auditors to gauge how close their methods\ncome to the fundamental limit. Treating the mutual information $I(Z;T)$ between\nthe observation $Z$ and the target property $T$ as the leaked bits per query,\nwe show that achieving error $\\varepsilon$ requires at least\n$\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak\nrate and only logarithmically with the desired accuracy. Thus, even a modest\nincrease in disclosure collapses the attack cost from quadratic to logarithmic\nin terms of the desired accuracy. Experiments on seven LLMs across\nsystem-prompt leakage, jailbreak, and relearning attacks corroborate the\ntheory: exposing answer tokens alone requires about a thousand queries; adding\nlogits cuts this to about a hundred; and revealing the full thinking process\ntrims it to a few dozen. Our results provide the first principled yardstick for\nbalancing transparency and security when deploying LLMs."}
{"id": "2510.16956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16956", "abs": "https://arxiv.org/abs/2510.16956", "authors": ["Mark Towers", "Yali Du", "Christopher Freeman", "Timothy J. Norman"], "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification", "comment": "Accepted to ECAI 2025 Workshop on Evaluating Explainable AI and\n  Complex Decision-Making, 8 Pages", "summary": "Debugging is a core application of explainable reinforcement learning (XRL)\nalgorithms; however, limited comparative evaluations have been conducted to\nunderstand their relative performance. We propose a novel evaluation\nmethodology to test whether users can identify an agent's goal from an\nexplanation of its decision-making. Utilising the Atari's Ms. Pacman\nenvironment and four XRL algorithms, we find that only one achieved greater\nthan random accuracy for the tested goals and that users were generally\noverconfident in their selections. Further, we find that users' self-reported\nease of identification and understanding for every explanation did not\ncorrelate with their accuracy."}
{"id": "2510.17033", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17033", "abs": "https://arxiv.org/abs/2510.17033", "authors": ["Leixu Huang", "Zedian Shao", "Teodora Baluta"], "title": "Watermark Robustness and Radioactivity May Be at Odds in Federated Learning", "comment": "9 pages, 4 figures (not including citation and appendix) submitted to\n  ICLR 2026", "summary": "Federated learning (FL) enables fine-tuning large language models (LLMs)\nacross distributed data sources. As these sources increasingly include\nLLM-generated text, provenance tracking becomes essential for accountability\nand transparency. We adapt LLM watermarking for data provenance in FL where a\nsubset of clients compute local updates on watermarked data, and the server\naverages all updates into the global LLM. In this setup, watermarks are\nradioactive: the watermark signal remains detectable after fine-tuning with\nhigh confidence. The $p$-value can reach $10^{-24}$ even when as little as\n$6.6\\%$ of data is watermarked. However, the server can act as an active\nadversary that wants to preserve model utility while evading provenance\ntracking. Our observation is that updates induced by watermarked synthetic data\nappear as outliers relative to non-watermark updates. Our adversary thus\napplies strong robust aggregation that can filter these outliers, together with\nthe watermark signal. All evaluated radioactive watermarks are not robust\nagainst such an active filtering server. Our work suggests fundamental\ntrade-offs between radioactivity, robustness, and utility."}
{"id": "2510.16996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16996", "abs": "https://arxiv.org/abs/2510.16996", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "title": "STARK: Strategic Team of Agents for Refining Kernels", "comment": null, "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet\noptimizing them remains a difficult and labor-intensive task due to complex\ninteractions between memory hierarchies, thread scheduling, and\nhardware-specific characteristics. While recent advances in large language\nmodels (LLMs) provide new opportunities for automated code generation, existing\napproaches largely treat LLMs as single-shot generators or naive refinement\ntools, limiting their effectiveness in navigating the irregular kernel\noptimization landscape. We introduce an LLM agentic framework for GPU kernel\noptimization that systematically explores the design space through multi-agent\ncollaboration, grounded instruction, dynamic context management, and strategic\nsearch. This framework mimics the workflow of expert engineers, enabling LLMs\nto reason about hardware trade-offs, incorporate profiling feedback, and refine\nkernels iteratively. We evaluate our approach on KernelBench, a benchmark for\nLLM-based kernel optimization, and demonstrate substantial improvements over\nbaseline agents: our system produces correct solutions where baselines often\nfail, and achieves kernels with up to 16x faster runtime performance. These\nresults highlight the potential of agentic LLM frameworks to advance fully\nautomated, scalable GPU kernel optimization."}
{"id": "2510.17087", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17087", "abs": "https://arxiv.org/abs/2510.17087", "authors": ["Ziqing Zhu"], "title": "Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability", "comment": null, "summary": "Virtual power plants (VPPs) are becoming a cornerstone of future grids,\naggregating distributed PV, wind, storage, and flexible loads for market\nparticipation and real-time balancing. As operations move to minute-- and\nsecond--level feedback, communication security shifts from a compliance item to\nan operational constraint: latency, reliability, and confidentiality jointly\ndetermine whether dispatch, protection, and settlement signals arrive on time.\nConventional PKI and key-rotation schemes struggle with cross-domain,\nhigh-frequency messaging and face long-term quantum threats. Quantum key\ndistribution (QKD) offers information-theoretic key freshness, but its key\nyield is scarce and stochastic, often misaligned with bursty VPP traffic. This\npaper proposes a key-aware priority and quota framework that treats quantum\nkeys as first-class scheduling resources. The design combines (i)\nforecast-driven long-term quotas and short-term tokens, (ii) key-aware\ndeficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and\n(iv) graceful degradation via encryption-mode switching and controlled\ndown-sampling for non-critical traffic. A drift-plus-penalty analysis\nestablishes strong stability under average supply--demand balance with\nquantifiable bounds on backlog and tail latency, providing interpretable\noperating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus\nVPP systems and evaluate normal, degraded, and outage regimes with\nindustry-consistent message classes and TTLs. Against FIFO, fixed-priority, and\nstatic-quota baselines, the proposed scheme consistently reduces tail delay and\npassive timeouts for critical messages, improves per-bit key utility, and\nenhances power-tracking reliability during key scarcity and regime switches."}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications."}
{"id": "2510.17098", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17098", "abs": "https://arxiv.org/abs/2510.17098", "authors": ["Elias Hossain", "Swayamjit Saha", "Somshubhra Roy", "Ravi Prasad"], "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models", "comment": null, "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."}
{"id": "2510.17064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17064", "abs": "https://arxiv.org/abs/2510.17064", "authors": ["Rongbin Li", "Wenbo Chen", "Zhao Li", "Rodrigo Munoz-Castaneda", "Jinbo Li", "Neha S. Maurya", "Arnav Solanki", "Huan He", "Hanwen Xing", "Meaghan Ramlakhan", "Zachary Wise", "Zhuhao Wu", "Hua Xu", "Michael Hawrylycz", "W. Jim Zheng"], "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation", "comment": "22 pages, 6 figures, 2 tables", "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse\ncell types and their transcriptomic signatures. However, annotating these\nsignatures-especially those involving poorly characterized genes-remains a\nmajor challenge. Traditional methods, such as Gene Set Enrichment Analysis\n(GSEA), depend on well-curated annotations and often perform poorly in these\ncontexts. Large Language Models (LLMs) offer a promising alternative but\nstruggle to represent complex biological knowledge within structured\nontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:\nhttps://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that\nintegrates free-text descriptions with ontology labels to enable more accurate\nand robust gene set annotation. By incorporating retrieval-augmented generation\n(RAG), we developed a robust agentic workflow that refines predictions using\nrelevant PubMed literature, reducing hallucinations and enhancing\ninterpretability. Using this workflow, we achieved correct annotations for 77%\nof mouse gene sets among their top predictions. Applying this approach, we\nannotated 5,322 brain cell clusters from the comprehensive mouse brain cell\natlas generated by the BRAIN Initiative Cell Census Network, enabling novel\ninsights into brain cell function by identifying region-specific gene\nco-expression patterns and inferring functional roles of gene ensembles.\nBRAINCELL-AID also identifies Basal Ganglia-related cell types with\nneurologically meaningful descriptions. Hence, we create a valuable resource to\nsupport community-driven cell type annotation."}
{"id": "2510.17175", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17175", "abs": "https://arxiv.org/abs/2510.17175", "authors": ["Muhammad Wahid Akram", "Keshav Sood", "Muneeb Ul Hassan"], "title": "QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR", "comment": "13 pages, 11 figures, and 7 tables", "summary": "Globally, individuals and organizations employ Quick Response (QR) codes for\nswift and convenient communication. Leveraging this, cybercriminals embed\nfalsify and misleading information in QR codes to launch various phishing\nattacks which termed as Quishing. Many former studies have introduced defensive\napproaches to preclude Quishing such as by classifying the embedded content of\nQR codes and then label the QR codes accordingly, whereas other studies\nclassify them using visual features (i.e., deep features, histogram density\nanalysis features). However, these approaches mainly rely on black-box\ntechniques which do not clearly provide interpretability and transparency to\nfully comprehend and reproduce the intrinsic decision process; therefore,\nhaving certain obvious limitations includes the approaches' trust,\naccountability, issues in bias detection, and many more. We proposed QR\\\"iS,\nthe pioneer method to classify QR codes through the comprehensive structural\nanalysis of a QR code which helps to identify phishing QR codes beforehand. Our\nclassification method is clearly transparent which makes it reproducible,\nscalable, and easy to comprehend. First, we generated QR codes dataset (i.e.\n400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike\nblack-box models, we developed a simple algorithm to extract 24 structural\nfeatures from layout patterns present in QR codes. Later, we train the machine\nlearning models on the harvested features and obtained accuracy of up to\n83.18%. To further evaluate the effectiveness of our approach, we perform the\ncomparative analysis of proposed method with relevant contemporary studies.\nLastly, for real-world deployment and validation, we developed a mobile app\nwhich assures the feasibility of the proposed solution in real-world scenarios\nwhich eventually strengthen the applicability of the study."}
{"id": "2510.17108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17108", "abs": "https://arxiv.org/abs/2510.17108", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices, will be\n  submitted to AAAI-2026 workshop", "summary": "Despite advances in financial AI, the automation of evidence-based reasoning\nremains unresolved in corporate credit assessment, where qualitative\nnon-financial indicators exert decisive influence on loan repayment outcomes\nyet resist formalization. Existing approaches focus predominantly on numerical\nprediction and provide limited support for the interpretive judgments required\nin professional loan evaluation. This study develops and evaluates two\noperational large language model (LLM)-based systems designed to generate\nstructured reasoning from non-financial evidence. The first is a\nnon-adversarial single-agent system (NAS) that produces bidirectional analysis\nthrough a single-pass reasoning pipeline. The second is a debate-based\nmulti-agent system (KPD-MADS) that operationalizes adversarial verification\nthrough a ten-step structured interaction protocol grounded in Karl Popper's\ncritical dialogue framework. Both systems were applied to three real corporate\ncases and evaluated by experienced credit risk professionals. Compared to\nmanual expert reporting, both systems achieved substantial productivity gains\n(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The\nKPD-MADS demonstrated superior reasoning quality, receiving higher median\nratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.\n3.0), and usability (62.5 vs. 52.5). These findings show that structured\nmulti-agent interaction can enhance reasoning rigor and interpretability in\nfinancial AI, advancing scalable and defensible automation in corporate credit\nassessment."}
{"id": "2510.17220", "categories": ["cs.CR", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.17220", "abs": "https://arxiv.org/abs/2510.17220", "authors": ["Giulia Giusti"], "title": "Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography", "comment": null, "summary": "The concept of linearity plays a central role in both mathematics and\ncomputer science, with distinct yet complementary meanings. In mathematics,\nlinearity underpins functions and vector spaces, forming the foundation of\nlinear algebra and functional analysis. In computer science, it relates to\nresource-sensitive computation. Linear Logic (LL), for instance, models\nassumptions that must be used exactly once, providing a natural framework for\ntracking computational resources such as time, memory, or data access. This\ndual perspective makes linearity essential to programming languages, type\nsystems, and formal models that express both computational complexity and\ncomposability. Bridging these interpretations enables rigorous yet practical\nmethodologies for analyzing and verifying complex systems.\n  This thesis explores the use of LL to model programming paradigms based on\nlinearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to\nAutomatic Differentiation (AD), modeling linear functions over the reals and\nthe transposition operation. The latter uses LL to express complexity\nconstraints on adversaries in computational cryptography.\n  In AD, two main approaches use linear type systems: a theoretical one\ngrounded in proof theory, and a practical one implemented in JAX, a Python\nlibrary developed by Google for machine learning research. In contrast,\nframeworks like PyTorch and TensorFlow support AD without linear types. ADLL\naims to bridge theory and practice by connecting JAX's type system to LL.\n  In modern cryptography, several calculi aim to model cryptographic proofs\nwithin the computational paradigm. These efforts face a trade-off between\nexpressiveness, to capture reductions, and simplicity, to abstract probability\nand complexity. CryptoBLL addresses this tension by proposing a framework for\nthe automatic analysis of protocols in computational cryptography."}
{"id": "2510.17145", "categories": ["cs.AI", "68T05, 62H30"], "pdf": "https://arxiv.org/pdf/2510.17145", "abs": "https://arxiv.org/abs/2510.17145", "authors": ["Phi-Hung Hoang", "Nam-Thuan Trinh", "Van-Manh Tran", "Thi-Thu-Hong Phan"], "title": "Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion", "comment": "35 pages, 6 figures and 11 tables", "summary": "Accurate assessment of fish freshness remains a major challenge in the food\nindustry, with direct consequences for product quality, market value, and\nconsumer health. Conventional sensory evaluation is inherently subjective,\ninconsistent, and difficult to standardize across contexts, often limited by\nsubtle, species-dependent spoilage cues. To address these limitations, we\npropose a handcrafted feature-based approach that systematically extracts and\nincrementally fuses complementary descriptors, including color statistics,\nhistograms across multiple color spaces, and texture features such as Local\nBinary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish\neye images. Our method captures global chromatic variations from full images\nand localized degradations from ROI segments, fusing each independently to\nevaluate their effectiveness in assessing freshness. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate the approach's\neffectiveness: in a standard train-test setting, a LightGBM classifier achieved\n77.56% accuracy, a 14.35% improvement over the previous deep learning baseline\nof 63.21%. With augmented data, an Artificial Neural Network (ANN) reached\n97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results\ndemonstrate that carefully engineered, handcrafted features, when strategically\nprocessed, yield a robust, interpretable, and reliable solution for automated\nfish freshness assessment, providing valuable insights for practical\napplications in food quality monitoring."}
{"id": "2510.17277", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17277", "abs": "https://arxiv.org/abs/2510.17277", "authors": ["Xinkai Wang", "Beibei Li", "Zerui Shao", "Ao Liu", "Shouling Ji"], "title": "Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks", "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated significant\nutility across diverse real-world applications. But MLLMs remain vulnerable to\njailbreaks, where adversarial inputs can collapse their safety constraints and\ntrigger unethical responses. In this work, we investigate jailbreaks in the\ntext-vision multimodal setting and pioneer the observation that visual\nalignment imposes uneven safety constraints across modalities in MLLMs, thereby\ngiving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a\nblack-box jailbreak method grounded in reinforcement learning. Initially, we\nprobe the model's attention dynamics and latent representation space, assessing\nhow visual inputs reshape cross-modal information flow and diminish the model's\nability to separate harmful from benign inputs, thereby exposing exploitable\nvulnerabilities. On this basis, we systematize them into generalizable and\nreusable operational rules that constitute a structured library of Atomic\nStrategy Primitives, which translate harmful intents into jailbreak inputs\nthrough step-wise transformations. Guided by the primitives, PolyJailbreak\nemploys a multi-agent optimization process that automatically adapts inputs\nagainst the target models. We conduct comprehensive evaluations on a variety of\nopen-source and closed-source MLLMs, demonstrating that PolyJailbreak\noutperforms state-of-the-art baselines."}
{"id": "2510.17146", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17146", "abs": "https://arxiv.org/abs/2510.17146", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation", "comment": "NeurIPS 2025 Workshop of UrbanAI (Oral)", "summary": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a\nsubstantial share of global building energy use, making reliable anomaly\ndetection essential for improving efficiency and reducing emissions. Classical\nrule-based approaches offer explainability but lack adaptability, while deep\nlearning methods provide predictive power at the cost of transparency,\nefficiency, and physical plausibility. Recent attempts to use Large Language\nModels (LLMs) for anomaly detection improve interpretability but largely ignore\nthe physical principles that govern HVAC operations. We present PILLM, a\nPhysics-Informed LLM framework that operates within an evolutionary loop to\nautomatically generate, evaluate, and refine anomaly detection rules. Our\napproach introduces physics-informed reflection and crossover operators that\nembed thermodynamic and control-theoretic constraints, enabling rules that are\nboth adaptive and physically grounded. Experiments on the public Building Fault\nDetection dataset show that PILLM achieves state-of-the-art performance while\nproducing diagnostic rules that are interpretable and actionable, advancing\ntrustworthy and deployable AI for smart building systems."}
{"id": "2510.17284", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17284", "abs": "https://arxiv.org/abs/2510.17284", "authors": ["Jiri Gavenda", "Petr Svenda", "Stanislav Bobon", "Vladimir Sedlacek"], "title": "Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values", "comment": null, "summary": "A coinjoin protocol aims to increase transactional privacy for Bitcoin and\nBitcoin-like blockchains via collaborative transactions, by violating\nassumptions behind common analysis heuristics. Estimating the resulting privacy\ngain is a crucial yet unsolved problem due to a range of influencing factors\nand large computational complexity.\n  We adapt the BlockSci on-chain analysis software to coinjoin transactions,\ndemonstrating a significant (10-50%) average post-mix anonymity set size\ndecrease for all three major designs with a central coordinator: Whirlpool,\nWasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and\nnegligible after one year from a coinjoin creation.\n  Moreover, we design a precise, parallelizable privacy estimation method,\nwhich takes into account coinjoin fees, implementation-specific limitations and\nusers' post-mix behavior. We evaluate our method in detail on a set of emulated\nand real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world\ncoinjoins with hundreds of inputs and outputs. We conclude that despite the\nusers' undesirable post-mix behavior, correctly attributing the coins to their\nowners is still very difficult, even with our improved analysis algorithm."}
{"id": "2510.17149", "categories": ["cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.17149", "abs": "https://arxiv.org/abs/2510.17149", "authors": ["Hongyi Du", "Jiaqi Su", "Jisen Li", "Lijie Ding", "Yingxuan Yang", "Peixuan Han", "Xiangru Tang", "Kunlun Zhu", "Jiaxuan You"], "title": "Which LLM Multi-Agent Protocol to Choose?", "comment": "Under review at ICLR 2026.Code and benchmark artifacts:\n  https://github.com/ulab-uiuc/AgentProtocols", "summary": "As large-scale multi-agent systems evolve, the communication protocol layer\nhas become a critical yet under-evaluated factor shaping performance and\nreliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,\netc.), selection is often intuition-driven and lacks standardized guidance. We\nintroduce ProtocolBench, a benchmark that systematically compares agent\nprotocols along four measurable axes: task success, end-to-end latency, message\nor byte overhead, and robustness under failures. On ProtocolBench, protocol\nchoice significantly influences system behavior. In the Streaming Queue\nscenario, overall completion time varies by up to 36.5% across protocols, and\nmean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,\nresilience also differs consistently across protocols. Beyond evaluation, we\npresent ProtocolRouter, a learnable protocol router that selects per-scenario\n(or per-module) protocols from requirement and runtime signals. ProtocolRouter\nreduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol\nbaseline, and achieves scenario-specific gains such as higher success in GAIA.\nWe also release ProtocolRouterBench to standardize protocol evaluation and\nimprove reliability at scale."}
{"id": "2510.17308", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17308", "abs": "https://arxiv.org/abs/2510.17308", "authors": ["Reo Eriguchi", "Kazumasa Shinagawa"], "title": "Single-Shuffle Full-Open Card-Based Protocols for Any Function", "comment": null, "summary": "A card-based secure computation protocol is a method for $n$ parties to\ncompute a function $f$ on their private inputs $(x_1,\\ldots,x_n)$ using\nphysical playing cards, in such a way that the suits of revealed cards leak no\ninformation beyond the value of $f(x_1,\\ldots,x_n)$. A \\textit{single-shuffle\nfull-open} protocol is a minimal model of card-based secure computation in\nwhich, after the parties place face-down cards representing their inputs, a\nsingle shuffle operation is performed and then all cards are opened to derive\nthe output. Despite the simplicity of this model, the class of functions known\nto admit single-shuffle full-open protocols has been limited to a few small\nexamples. In this work, we prove for the first time that every function admits\na single-shuffle full-open protocol. We present two constructions that offer a\ntrade-off between the number of cards and the complexity of the shuffle\noperation. These feasibility results are derived from a novel connection\nbetween single-shuffle full-open protocols and a cryptographic primitive known\nas \\textit{Private Simultaneous Messages} protocols, which has rarely been\nstudied in the context of card-based cryptography. We also present variants of\nsingle-shuffle protocols in which only a subset of cards are revealed. These\nprotocols reduce the complexity of the shuffle operation compared to existing\nprotocols in the same setting."}
{"id": "2510.17172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17172", "abs": "https://arxiv.org/abs/2510.17172", "authors": ["Shun Huang", "Wenlu Xing", "Shijia Geng", "Hailong Wang", "Guangkun Nie", "Gongzheng Tang", "Chenyang He", "Shenda Hong"], "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients", "comment": null, "summary": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial\ninfarction (AMI) are a major cause of in-hospital death, yet early\nidentification remains a clinical challenge. While traditional risk scores have\nlimited performance, end-to-end deep learning models often lack the\ninterpretability needed for clinical trust. This study aimed to develop a\nhybrid predictive framework that integrates a large-scale electrocardiogram\n(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to\nimprove both accuracy and interpretability. We analyzed 6,634 ECG recordings\nfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder\nmodel was used to extract 150-dimensional diagnostic probability features ,\nwhich were then refined through feature selection to train the XGBoost\nclassifier. Model performance was evaluated using AUC and F1-score , and the\nSHAP method was used for interpretability. The ECGFounder + XGBoost hybrid\nmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC\n0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that\nmodel-identified key features, such as \"premature ventricular complexes\" (risk\npredictor) and \"normal sinus rhythm\" (protective factor), were highly\nconsistent with clinical knowledge. We conclude that this hybrid framework\nprovides a novel paradigm for VT/VF risk prediction by validating the use of\nfoundation model outputs as effective, automated feature engineering for\nbuilding trustworthy, explainable AI-based clinical decision support systems."}
{"id": "2510.17311", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17311", "abs": "https://arxiv.org/abs/2510.17311", "authors": ["Eduard Marin", "Jinwoo Kim", "Alessio Pavoni", "Mauro Conti", "Roberto Di Pietro"], "title": "The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment", "comment": "Accepted at ESORICS 2025", "summary": "Serverless computing has rapidly emerged as a prominent cloud paradigm,\nenabling developers to focus solely on application logic without the burden of\nmanaging servers or underlying infrastructure. Public serverless repositories\nhave become key to accelerating the development of serverless applications.\nHowever, their growing popularity makes them attractive targets for\nadversaries. Despite this, the security posture of these repositories remains\nlargely unexplored, exposing developers and organizations to potential risks.\nIn this paper, we present the first comprehensive analysis of the security\nlandscape of serverless components hosted in public repositories. We analyse\n2,758 serverless components from five widely used public repositories popular\namong developers and enterprises, and 125,936 Infrastructure as Code (IaC)\ntemplates across three widely used IaC frameworks. Our analysis reveals\nsystemic vulnerabilities including outdated software packages, misuse of\nsensitive parameters, exploitable deployment configurations, susceptibility to\ntypo-squatting attacks and opportunities to embed malicious behaviour within\ncompressed serverless components. Finally, we provide practical recommendations\nto mitigate these threats."}
{"id": "2510.17173", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17173", "abs": "https://arxiv.org/abs/2510.17173", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users", "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models", "summary": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure."}
{"id": "2510.17403", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17403", "abs": "https://arxiv.org/abs/2510.17403", "authors": ["Stella N. Arinze", "Patrick U. Okafor", "Onyekachi M. Egwuagu", "Augustine O. Nwajana"], "title": "Process Automation Architecture Using RFID for Transparent Voting Systems", "comment": "7 pages, 5 figures, 1 table", "summary": "This paper presents the development of a process automation architecture\nleveraging Radio Frequency Identification (RFID) technology for secure,\ntransparent and efficient voting systems. The proposed architecture automates\nthe voting workflow through RFID-enabled voter identification, encrypted vote\ncasting, and secure data transmission. Each eligible voter receives a smart\nRFID card containing a uniquely encrypted identifier, which is verified using\nan RC522 reader interfaced with a microcontroller. Upon successful\nverification, the voter interacts with a touchscreen interface to cast a vote,\nwhich is then encrypted using AES-128 and securely stored on a local SD card or\ntransmitted via GSM to a central server. A tamper-proof monitoring mechanism\nrecords each session with time-stamped digital signatures, ensuring\nauditability and data integrity. The architecture is designed to function in\nboth online and offline modes, with an automated batch synchronization\nmechanism that updates vote records once network connectivity is restored.\nSystem testing in simulated environments confirmed 100% voter authentication\naccuracy, minimized latency (average voting time of 11.5 seconds), and\nrobustness against cloning, double voting, and data interception. The\nintegration of real-time monitoring and secure process control modules enables\nelectoral authorities to automate data logging, detect anomalies, and validate\nsystem integrity dynamically. This work demonstrates a scalable,\nautomation-driven solution for voting infrastructure, offering enhanced\ntransparency, resilience, and deployment flexibility, especially in\nenvironments where digital transformation of electoral processes is critically\nneeded."}
{"id": "2510.17211", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17211", "abs": "https://arxiv.org/abs/2510.17211", "authors": ["Tingsong Xiao", "Yao An Lee", "Zelin Xu", "Yupu Zhang", "Zibo Liu", "Yu Huang", "Jiang Bian", "Serena Jingchuan Guo", "Zhe Jiang"], "title": "Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling", "comment": null, "summary": "Disease progression modeling aims to characterize and predict how a patient's\ndisease complications worsen over time based on longitudinal electronic health\nrecords (EHRs). Accurate modeling of disease progression, such as type 2\ndiabetes, can enhance patient sub-phenotyping and inform effective and timely\ninterventions. However, the problem is challenging due to the need to learn\ncontinuous-time dynamics of progression patterns based on irregular-time event\nsamples and patient heterogeneity (\\eg different progression rates and\npathways). Existing mechanistic and data-driven methods either lack\nadaptability to learn from real-world data or fail to capture complex\ncontinuous-time dynamics on progression trajectories. To address these\nlimitations, we propose Temporally Detailed Hypergraph Neural Ordinary\nDifferential Equation (TD-HNODE), which represents disease progression on\nclinically recognized trajectories as a temporally detailed hypergraph and\nlearns the continuous-time progression dynamics via a neural ODE framework.\nTD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the\ninterdependency of disease complication markers within both intra- and\ninter-progression trajectories. Experiments on two real-world clinical datasets\ndemonstrate that TD-HNODE outperforms multiple baselines in modeling the\nprogression of type 2 diabetes and related cardiovascular diseases."}
{"id": "2510.17521", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17521", "abs": "https://arxiv.org/abs/2510.17521", "authors": ["Francesco Balassone", "Víctor Mayoral-Vilches", "Stefan Rass", "Martin Pinzger", "Gaetano Perrone", "Simon Pietro Romano", "Peter Schartner"], "title": "Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs", "comment": null, "summary": "We empirically evaluate whether AI systems are more effective at attacking or\ndefending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution\nframework, we deployed autonomous agents in 23 Attack/Defense CTF\nbattlegrounds. Statistical analysis reveals defensive agents achieve 54.3%\nunconstrained patching success versus 28.3% offensive initial access\n(p=0.0193), but this advantage disappears under operational constraints: when\ndefense requires maintaining availability (23.9%) and preventing all intrusions\n(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy\nanalysis suggests potential patterns in vulnerability exploitation, though\nlimited sample sizes preclude definitive conclusions. This study provides the\nfirst controlled empirical evidence challenging claims of AI attacker\nadvantage, demonstrating that defensive effectiveness critically depends on\nsuccess criteria, a nuance absent from conceptual analyses but essential for\ndeployment. These findings underscore the urgency for defenders to adopt\nopen-source Cybersecurity AI frameworks to maintain security equilibrium\nagainst accelerating offensive automation."}
{"id": "2510.17235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17235", "abs": "https://arxiv.org/abs/2510.17235", "authors": ["Chong Chen", "Ze Liu", "Lingfeng Bao", "Yanlin Wang", "Ting Chen", "Daoyuan Wu", "Jiachi Chen"], "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis", "comment": null, "summary": "The cryptocurrency market offers significant investment opportunities but\nfaces challenges including high volatility and fragmented information. Data\nintegration and analysis are essential for informed investment decisions.\nCurrently, investors use three main approaches: (1) Manual analysis across\nvarious sources, which depends heavily on individual experience and is\ntime-consuming and prone to bias; (2) Data aggregation platforms-limited in\nfunctionality and depth of analysis; (3) Large language model agents-based on\nstatic pretrained models, lacking real-time data integration and multi-step\nreasoning capabilities. To address these limitations, we present Coinvisor, a\nreinforcement learning-based chatbot that provides comprehensive analytical\nsupport for cryptocurrency investment through a multi-agent framework.\nCoinvisor integrates diverse analytical capabilities through specialized tools.\nIts key innovation is a reinforcement learning-based tool selection mechanism\nthat enables multi-step planning and flexible integration of diverse data\nsources. This design supports real-time interaction and adaptive analysis of\ndynamic content, delivering accurate and actionable investment insights. We\nevaluated Coinvisor through automated benchmarks on tool calling accuracy and\nuser studies with 20 cryptocurrency investors using our interface. Results show\nthat Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base\nmodel in tool orchestration. User studies show high satisfaction (4.64/5), with\nparticipants preferring Coinvisor to both general LLMs and existing crypto\nplatforms (4.62/5)."}
{"id": "2510.17552", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17552", "abs": "https://arxiv.org/abs/2510.17552", "authors": ["Persefoni Konteli", "Nikolaos Makris", "Evgenia Niovi Sassalou", "Stylianos A. Kazazis", "Alkinoos Papageorgopoulos", "Stefanos Vasileiadis", "Konstantinos Tsimvrakidis", "Symeon Tsintzos", "Georgios M. Nikolopoulos", "George T. Kanellos"], "title": "Dynamic Switched Quantum Key Distribution Networkwith PUF-based authentication", "comment": null, "summary": "We demonstrate a centrally controlled dynamic switched-QKD network,\nwithintegrated PUF-based dynamic authentication for each QKD link. The\nperformance of the dynamicswitched-QKD network with real-time PUF-based\nauthentication is analyzed."}
{"id": "2510.17309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17309", "abs": "https://arxiv.org/abs/2510.17309", "authors": ["Thorsten Fröhlich", "Tim Schlippe"], "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment", "comment": null, "summary": "The evaluation of academic theses is a cornerstone of higher education,\nensuring rigor and integrity. Traditional methods, though effective, are\ntime-consuming and subject to evaluator variability. This paper presents\nRubiSCoT, an AI-supported framework designed to enhance thesis evaluation from\nproposal to final submission. Using advanced natural language processing\ntechniques, including large language models, retrieval-augmented generation,\nand structured chain-of-thought prompting, RubiSCoT offers a consistent,\nscalable solution. The framework includes preliminary assessments,\nmultidimensional assessments, content extraction, rubric-based scoring, and\ndetailed reporting. We present the design and implementation of RubiSCoT,\ndiscussing its potential to optimize academic assessment processes through\nconsistent, scalable, and transparent evaluation."}
{"id": "2510.17621", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17621", "abs": "https://arxiv.org/abs/2510.17621", "authors": ["Vincenzo Carletti", "Pasquale Foggia", "Carlo Mazzocca", "Giuseppe Parrella", "Mario Vento"], "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models", "comment": null, "summary": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric."}
{"id": "2510.17382", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17382", "abs": "https://arxiv.org/abs/2510.17382", "authors": ["Rishabh Jain", "Keisuke Okumura", "Michael Amir", "Amanda Prorok"], "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding", "comment": null, "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems."}
{"id": "2510.17687", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17687", "abs": "https://arxiv.org/abs/2510.17687", "authors": ["Xu Zhang", "Hao Li", "Zhichao Lu"], "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks", "comment": "14 pages, 8 figures, 2 tables", "summary": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and\nperception capabilities but are increasingly vulnerable to jailbreak attacks.\nWhile existing work focuses on explicit attacks, where malicious content\nresides in a single modality, recent studies reveal implicit attacks, in which\nbenign text and image inputs jointly express unsafe intent. Such joint-modal\nthreats are difficult to detect and remain underexplored, largely due to the\nscarcity of high-quality implicit data. We propose ImpForge, an automated\nred-teaming pipeline that leverages reinforcement learning with tailored reward\nmodules to generate diverse implicit samples across 14 domains. Building on\nthis dataset, we further develop CrossGuard, an intent-aware safeguard\nproviding robust and comprehensive defense against both explicit and implicit\nthreats. Extensive experiments across safe and unsafe benchmarks, implicit and\nexplicit attacks, and multiple out-of-domain settings demonstrate that\nCrossGuard significantly outperforms existing defenses, including advanced\nMLLMs and guardrails, achieving stronger security while maintaining high\nutility. This offers a balanced and practical solution for enhancing MLLM\nrobustness against real-world multimodal threats."}
{"id": "2510.17418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17418", "abs": "https://arxiv.org/abs/2510.17418", "authors": ["Mustafa F. Abdelwahed", "Alice Toniolo", "Joan Espasa", "Ian P. Gent"], "title": "Diverse Planning with Simulators via Linear Temporal Logic", "comment": null, "summary": "Autonomous agents rely on automated planning algorithms to achieve their\nobjectives. Simulation-based planning offers a significant advantage over\ndeclarative models in modelling complex environments. However, relying solely\non a planner that produces a single plan may not be practical, as the generated\nplans may not always satisfy the agent's preferences. To address this\nlimitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner\nexplicitly designed for simulation-based planning problems.\n$\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define\nsemantic diversity criteria, enabling agents to specify what constitutes\nmeaningfully different plans. By integrating these LTL-based diversity models\ndirectly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the\ngeneration of semantically diverse plans, addressing a critical limitation of\nexisting diverse planning approaches that may produce syntactically different\nbut semantically identical solutions. Extensive evaluations on various\nbenchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates\nmore diverse plans compared to a baseline approach. This work establishes the\nfeasibility of semantically-guided diverse planning in simulation-based\nenvironments, paving the way for innovative approaches in realistic,\nnon-symbolic domains where traditional model-based approaches fail."}
{"id": "2510.17759", "categories": ["cs.CR", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17759", "abs": "https://arxiv.org/abs/2510.17759", "authors": ["Qilin Liao", "Anamika Lochab", "Ruqi Zhang"], "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models", "comment": "18 pages, 7 Figures,", "summary": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o."}
{"id": "2510.17450", "categories": ["cs.AI", "H.4.2; I.2.3; I.2.6; I.2.8; I.2.9; J.7"], "pdf": "https://arxiv.org/pdf/2510.17450", "abs": "https://arxiv.org/abs/2510.17450", "authors": ["Johan Schubert", "Farzad Kamrani", "Tove Gustavi"], "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions", "comment": "Presented at the 6th International Workshop on Active Inference,\n  15-17 October 2025, Montreal, Canada", "summary": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects."}
{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats."}
{"id": "2510.17463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17463", "abs": "https://arxiv.org/abs/2510.17463", "authors": ["Cor Steging", "Tadeusz Zbiegień"], "title": "Label Indeterminacy in AI & Law", "comment": "This manuscript has been accepted for presentation as a short paper\n  at the 38th International Conference on Legal Knowledge and Information\n  Systems (JURIX) in Turin, December 9 to 11 of 2025", "summary": "Machine learning is increasingly used in the legal domain, where it typically\noperates retrospectively by treating past case outcomes as ground truth.\nHowever, legal outcomes are often shaped by human interventions that are not\ncaptured in most machine learning approaches. A final decision may result from\na settlement, an appeal, or other procedural actions. This creates label\nindeterminacy: the outcome could have been different if the intervention had or\nhad not taken place. We argue that legal machine learning applications need to\naccount for label indeterminacy. Methods exist that can impute these\nindeterminate labels, but they are all grounded in unverifiable assumptions. In\nthe context of classifying cases from the European Court of Human Rights, we\nshow that the way that labels are constructed during training can significantly\naffect model behaviour. We therefore position label indeterminacy as a relevant\nconcern in AI & Law and demonstrate how it can shape model behaviour."}
{"id": "2510.17590", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG", "I.2.7; H.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.17590", "abs": "https://arxiv.org/abs/2510.17590", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning", "comment": "16 pages, 3 tables, 1 figure", "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce."}
{"id": "2510.17598", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17598", "abs": "https://arxiv.org/abs/2510.17598", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation", "comment": null, "summary": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks."}
{"id": "2510.17614", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17614", "abs": "https://arxiv.org/abs/2510.17614", "authors": ["Praphul Singh", "Corey Barrett", "Sumana Srivasta", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration", "comment": null, "summary": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection."}
{"id": "2510.17638", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17638", "abs": "https://arxiv.org/abs/2510.17638", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena", "comment": "https://www.prophetarena.co/", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears."}
{"id": "2510.17697", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17697", "abs": "https://arxiv.org/abs/2510.17697", "authors": ["Anjie Liu", "Jianhong Wang", "Samuel Kaski", "Jun Wang", "Mengyue Yang"], "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis."}
{"id": "2510.17705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17705", "abs": "https://arxiv.org/abs/2510.17705", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models", "comment": "Accepted by CIKM' 25", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM."}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs."}
{"id": "2510.15872", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15872", "abs": "https://arxiv.org/abs/2510.15872", "authors": ["Yun-Da Tsai", "Chang-Yu Chao", "Liang-Yeh Shen", "Tsung-Han Lin", "Haoyu Yang", "Mark Ho", "Yi-Chen Lu", "Wen-Hao Liu", "Shou-De Lin", "Haoxing Ren"], "title": "Multimodal Chip Physical Design Engineer Assistant", "comment": null, "summary": "Modern chip physical design relies heavily on Electronic Design Automation\n(EDA) tools, which often struggle to provide interpretable feedback or\nactionable guidance for improving routing congestion. In this work, we\nintroduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this\ngap by not only predicting congestion but also delivering human-interpretable\ndesign suggestions. Our method combines automated feature generation through\nMLLM-guided genetic prompting with an interpretable preference learning\nframework that models congestion-relevant tradeoffs across visual, tabular, and\ntextual inputs. We compile these insights into a \"Design Suggestion Deck\" that\nsurfaces the most influential layout features and proposes targeted\noptimizations. Experiments on the CircuitNet benchmark demonstrate that our\napproach outperforms existing models on both accuracy and explainability.\nAdditionally, our design suggestion guidance case study and qualitative\nanalyses confirm that the learned preferences align with real-world design\nprinciples and are actionable for engineers. This work highlights the potential\nof MLLMs as interactive assistants for interpretable and context-aware physical\ndesign optimization."}
{"id": "2510.15882", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15882", "abs": "https://arxiv.org/abs/2510.15882", "authors": ["Ao Shen", "Rui Zhang", "Junping Zhao"], "title": "FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern", "comment": null, "summary": "As large language models (LLMs) continue to scale, multi-node deployment has\nbecome a necessity. Consequently, communication has become a critical\nperformance bottleneck. Current intra-node communication libraries, like NCCL,\ntypically make use of a single interconnect such as NVLink. This approach\ncreates performance ceilings, especially on hardware like the H800 GPU where\nthe primary interconnect's bandwidth can become a bottleneck, and leaves other\nhardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable\nNetwork Interface Cards (NICs) largely idle during intensive workloads. We\npropose FlexLink, the first collective communication framework to the best of\nour knowledge designed to systematically address this by aggregating these\nheterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance\ncommunication fabric. FlexLink employs an effective two-stage adaptive load\nbalancing strategy that dynamically partitions communication traffic across all\navailable links, ensuring that faster interconnects are not throttled by slower\nones. On an 8-GPU H800 server, our design improves the bandwidth of collective\noperators such as AllReduce and AllGather by up to 26% and 27% over the NCCL\nbaseline, respectively. This gain is achieved by offloading 2-22% of the total\ncommunication traffic to the previously underutilized PCIe and RDMA NICs.\nFlexLink provides these improvements as a lossless, drop-in replacement\ncompatible with the NCCL API, ensuring easy adoption."}
{"id": "2510.15893", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG", "68M10, 68M14", "B.4.3; C.2.4; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.15893", "abs": "https://arxiv.org/abs/2510.15893", "authors": ["Mikhail Bernadskiy", "Peter Carson", "Thomas Graham", "Taylor Groves", "Ho John Lee", "Eric Yeh"], "title": "Accelerating Frontier MoE Training with 3D Integrated Optics", "comment": "12 pages, 11 figures. To be published in Hot Interconnects 2025", "summary": "The unabated growth in AI workload demands is driving the need for concerted\nadvances in compute, memory, and interconnect performance. As traditional\nsemiconductor scaling slows, high-speed interconnects have emerged as the new\nscaling engine, enabling the creation of larger logical GPUs by linking many\nGPUs into a single, low-latency, high-bandwidth compute domain. While initial\nscale-up fabrics leveraged copper interconnects for their power and cost\nadvantages, the maximum reach of passive electrical interconnects\n(approximately 1 meter) effectively limits the scale-up domain to within a\nsingle rack. The advent of 3D-stacked optics and logic offers a transformative,\npower-efficient scale-up solution for connecting hundreds of GPU packages\n(thousands of GPUs) across multiple data center racks. This work explores the\ndesign tradeoffs of scale-up technologies and demonstrates how frontier LLMs\nnecessitate novel photonic solutions to achieve aggressive power and\nperformance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and\nswitches within the scale-up domain when training Frontier Mixture of Experts\n(MoE) models exceeding one trillion parameters. Our results show that the\nsubstantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X\nincrease in scale-up capability. This affords new opportunities for\nmulti-dimensional parallelism within the scale-up domain and results in a 2.7X\nreduction in time-to-train, unlocking unprecedented model scaling."}
{"id": "2510.15906", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15906", "abs": "https://arxiv.org/abs/2510.15906", "authors": ["Yunsheng Bai", "Ghaith Bany Hamad", "Chia-Tung Ho", "Syed Suhaib", "Haoxing Ren"], "title": "FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures", "comment": null, "summary": "Debugging formal verification (FV) failures represents one of the most\ntime-consuming bottlenecks in modern hardware design workflows. When properties\nfail, engineers must manually trace through complex counter-examples spanning\nmultiple cycles, analyze waveforms, and cross-reference design specifications\nto identify root causes - a process that can consume hours or days per bug.\nExisting solutions are largely limited to manual waveform viewers or simple\nautomated tools that cannot reason about the complex interplay between design\nintent and implementation logic. We present FVDebug, an intelligent system that\nautomates root-cause analysis by combining multiple data sources - waveforms,\nRTL code, design specifications - to transform failure traces into actionable\ninsights. Our approach features a novel pipeline: (1) Causal Graph Synthesis\nthat structures failure traces into directed acyclic graphs, (2) Graph Scanner\nusing batched Large Language Model (LLM) analysis with for-and-against\nprompting to identify suspicious nodes, and (3) Insight Rover leveraging\nagentic narrative exploration to generate high-level causal explanations.\nFVDebug further provides concrete RTL fixes through its Fix Generator.\nEvaluated on open benchmarks, FVDebug attains high hypothesis quality and\nstrong Pass@k fix rates. We further report results on two proprietary,\nproduction-scale FV counterexamples. These results demonstrate FVDebug's\napplicability from academic benchmarks to industrial designs."}
{"id": "2510.15914", "categories": ["cs.AR", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.15914", "abs": "https://arxiv.org/abs/2510.15914", "authors": ["Jiayu Zhao", "Song Chen"], "title": "VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts", "comment": "9 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ngenerating Verilog code from natural language descriptions. However, Verilog\ncode inherently encodes structural information of hardware circuits.\nEffectively leveraging this structural information to enhance the functional\nand syntactic correctness of LLM-generated Verilog code remains a significant\nchallenge. To address this challenge, we propose VeriGRAG , a novel framework\nthat extracts structural graph embeddings from Verilog code using graph neural\nnetworks (GNNs). A multimodal retriever then selects the graph embeddings most\nrelevant to the given generation task, which are aligned with the code modality\nthrough the VeriFormer module to generate structure-aware soft prompts. Our\nexperiments demonstrate that VeriGRAG substantially improves the correctness of\nVerilog code generation, achieving state-of-the-art or superior performance\nacross both VerilogEval and RTLLM benchmarks."}
{"id": "2510.15917", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15917", "abs": "https://arxiv.org/abs/2510.15917", "authors": ["Shai Bergman", "Won Wook Song", "Lukas Cavigelli", "Konstantin Berestizshevsky", "Ke Zhou", "Ji Zhang"], "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding", "comment": null, "summary": "Existing storage systems lack visibility into workload intent, limiting their\nability to adapt to the semantics of modern, large-scale data-intensive\napplications. This disconnect leads to brittle heuristics and fragmented,\nsiloed optimizations. To address these limitations, we propose Intent-Driven\nStorage Systems (IDSS), a vision for a new paradigm where large language models\n(LLMs) infer workload and system intent from unstructured signals to guide\nadaptive and cross-layer parameter reconfiguration. IDSS provides holistic\nreasoning for competing demands, synthesizing safe and efficient decisions\nwithin policy guardrails. We present four design principles for integrating\nLLMs into storage control loops and propose a corresponding system\narchitecture. Initial results on FileBench workloads show that IDSS can improve\nIOPS by up to 2.45X by interpreting intent and generating actionable\nconfigurations for storage components such as caching and prefetching. These\nfindings suggest that, when constrained by guardrails and embedded within\nstructured workflows, LLMs can function as high-level semantic optimizers,\nbridging the gap between application goals and low-level system control. IDSS\npoints toward a future in which storage systems are increasingly adaptive,\nautonomous, and aligned with dynamic workload demands."}
{"id": "2510.15930", "categories": ["cs.AR", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.15930", "abs": "https://arxiv.org/abs/2510.15930", "authors": ["Philippe Magalhães", "Virginie Fresse", "Benoît Suffran", "Olivier Alata"], "title": "Implémentation Efficiente de Fonctions de Convolution sur FPGA à l'Aide de Blocs Paramétrables et d'Approximations Polynomiales", "comment": "in French language, XXXe Colloque Francophone de Traitement du Signal\n  et des Images (GRETSI), Aug 2025, Strabourg, France", "summary": "Implementing convolutional neural networks (CNNs) on field-programmable gate\narrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower\nlatency, greater power efficiency and greater flexibility. However, this\ndevelopment remains complex due to the hardware knowledge required and the long\nsynthesis, placement and routing stages, which slow down design cycles and\nprevent rapid exploration of network configurations, making resource\noptimisation under severe constraints particularly challenging. This paper\nproposes a library of configurable convolution Blocks designed to optimize FPGA\nimplementation and adapt to available resources. It also presents a\nmethodological framework for developing mathematical models that predict FPGA\nresources utilization. The approach is validated by analyzing the correlation\nbetween the parameters, followed by error metrics. The results show that the\ndesigned blocks enable adaptation of convolution layers to hardware\nconstraints, and that the models accurately predict resource consumption,\nproviding a useful tool for FPGA selection and optimized CNN deployment."}
{"id": "2510.15971", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15971", "abs": "https://arxiv.org/abs/2510.15971", "authors": ["Md. Ifthekhar Hossain", "Kazi Abdullah Al Arafat", "Bryce Shepard", "Kayd Craig", "Imtiaz Parvez"], "title": "A Graph-Attentive LSTM Model for Malicious URL Detection", "comment": "Planned to be submitted", "summary": "Malicious URLs pose significant security risks as they facilitate phishing\nattacks, distribute malware, and empower attackers to deface websites.\nBlacklist detection methods fail to identify new or obfuscated URLs because\nthey depend on pre-existing patterns. This work presents a hybrid deep learning\nmodel named GNN-GAT-LSTM that combines Graph Neural Networks (GNNs) with Graph\nAttention Networks (GATs) and Long Short-Term Memory (LSTM) networks. The\nproposed architecture extracts both the structural and sequential patterns of\nthe features from data. The model transforms URLs into graphs through a process\nwhere characters become nodes that connect through edges. It applies one-hot\nencoding to represent node features. The model received training and testing\ndata from a collection of 651,191 URLs, which were classified into benign,\nphishing, defacement, and malware categories. The preprocessing stage included\nboth feature engineering and data balancing techniques, which addressed the\nclass imbalance issue to enhance model learning. The GNN-GAT-LSTM model\nachieved outstanding performance through its test accuracy of 0.9806 and its\nweighted F1-score of 0.9804. It showed excellent precision and recall\nperformance across most classes, particularly for benign and defacement URLs.\nOverall, the model provides an efficient and scalable system for detecting\nmalicious URLs while demonstrating strong potential for real-world\ncybersecurity applications."}
{"id": "2510.15973", "categories": ["cs.CR", "cs.AI", "cs.CY", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15973", "abs": "https://arxiv.org/abs/2510.15973", "authors": ["Tiarnaigh Downey-Webb", "Olamide Jogunola", "Oluwaseun Ajao"], "title": "Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts", "comment": "10 pages, 4 pages manuscript submitted to the Language Resources and\n  Evaluation Conference (LREC 2026)", "summary": "This paper presents a systematic security assessment of four prominent Large\nLanguage Models (LLMs) against diverse adversarial attack vectors. We evaluate\nPhi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack\ncategories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),\nand Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs\n1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six\nharm categories. Results demonstrate significant variations in model\nrobustness, with Llama-2 achieving the highest overall security (3.4% average\nattack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%\naverage attack success rate). We identify critical transferability patterns\nwhere GCG and TAP attacks, though ineffective against their target model\n(Llama-2), achieve substantially higher success rates when transferred to other\nmodels (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals\nsignificant differences in vulnerability across harm categories ($p < 0.001$),\nwith malicious use prompts showing the highest attack success rates (10.71%\naverage). Our findings contribute to understanding cross-model security\nvulnerabilities and provide actionable insights for developing targeted defense\nmechanisms"}
{"id": "2510.15976", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15976", "abs": "https://arxiv.org/abs/2510.15976", "authors": ["Chenrui Wang", "Junyi Shu", "Billy Chiu", "Yu Li", "Saleh Alharbi", "Min Zhang", "Jing Li"], "title": "Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization", "comment": "28 pages, 11 figures, NeurIPS 2025 Poster", "summary": "The rapid development of LLMs has raised concerns about their potential\nmisuse, leading to various watermarking schemes that typically offer high\ndetectability. However, existing watermarking techniques often face trade-off\nbetween watermark detectability and generated text quality. In this paper, we\nintroduce Learning to Watermark (LTW), a novel selective watermarking framework\nthat leverages multi-objective optimization to effectively balance these\ncompeting goals. LTW features a lightweight network that adaptively decides\nwhen to apply the watermark by analyzing sentence embeddings, token entropy,\nand current watermarking ratio. Training of the network involves two\nspecifically constructed loss functions that guide the model toward\nPareto-optimal solutions, thereby harmonizing watermark detectability and text\nquality. By integrating LTW with two baseline watermarking methods, our\nexperimental evaluations demonstrate that LTW significantly enhances text\nquality without compromising detectability. Our selective watermarking approach\noffers a new perspective for designing watermarks for LLMs and a way to\npreserve high text quality for watermarks. The code is publicly available at:\nhttps://github.com/fattyray/learning-to-watermark"}
{"id": "2510.15994", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15994", "abs": "https://arxiv.org/abs/2510.15994", "authors": ["Dongsen Zhang", "Zekun Li", "Xu Luo", "Xuannan Liu", "Peipei Li", "Wenjun Xu"], "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents", "comment": null, "summary": "The Model Context Protocol (MCP) standardizes how large language model (LLM)\nagents discover, describe, and call external tools. While MCP unlocks broad\ninteroperability, it also enlarges the attack surface by making tools\nfirst-class, composable objects with natural-language metadata, and\nstandardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end\nevaluation suite that systematically measures how well LLM agents resist\nMCP-specific attacks throughout the full tool-use pipeline: task planning, tool\ninvocation, and response handling. MSB contributes: (1) a taxonomy of 12\nattacks including name-collision, preference manipulation, prompt injections\nembedded in tool descriptions, out-of-scope parameter requests,\nuser-impersonating responses, false-error escalation, tool-transfer, retrieval\ninjection, and mixed attacks; (2) an evaluation harness that executes attacks\nby running real tools (both benign and malicious) via MCP rather than\nsimulation; and (3) a robustness metric that quantifies the trade-off between\nsecurity and performance: Net Resilient Performance (NRP). We evaluate nine\npopular LLM agents across 10 domains and 400+ tools, producing 2,000 attack\ninstances. Results reveal the effectiveness of attacks against each stage of\nMCP. Models with stronger performance are more vulnerable to attacks due to\ntheir outstanding tool calling and instruction following capabilities. MSB\nprovides a practical baseline for researchers and practitioners to study,\ncompare, and harden MCP agents."}
{"id": "2510.16005", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16005", "abs": "https://arxiv.org/abs/2510.16005", "authors": ["Giacomo Bertollo", "Naz Bodemir", "Jonah Burgess"], "title": "Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers", "comment": null, "summary": "Analyzing 500 CTF participants, this paper shows that while participants\nreadily bypassed simple AI guardrails using common techniques, layered\nmulti-step defenses still posed significant challenges, offering concrete\ninsights for building safer AI systems."}
{"id": "2510.16024", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16024", "abs": "https://arxiv.org/abs/2510.16024", "authors": ["Abdulrahman Alhaidari", "Balaji Palanisamy", "Prashant Krishnamurthy"], "title": "On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation", "comment": "Published in the 7th Conference on Advances in Financial Technologies\n  (AFT 2025)", "summary": "Billions of dollars are lost every year in DeFi platforms by transactions\nexploiting business logic or accounting vulnerabilities. Existing defenses\nfocus on static code analysis, public mempool screening, attacker contract\ndetection, or trusted off-chain monitors, none of which prevents exploits\nsubmitted through private relays or malicious contracts that execute within the\nsame block. We present the first decentralized, fully on-chain learning\nframework that: (i) performs gas-prohibitive computation on Layer-2 to reduce\ncost, (ii) propagates verified model updates to Layer-1, and (iii) enables\ngas-bounded, low-latency inference inside smart contracts. A novel\nProof-of-Improvement (PoIm) protocol governs the training process and verifies\neach decentralized micro update as a self-verifying training transaction.\nUpdates are accepted by \\textit{PoIm} only if they demonstrably improve at\nleast one core metric (e.g., accuracy, F1-score, precision, or recall) on a\npublic benchmark without degrading any of the other core metrics, while\nadversarial proposals get financially penalized through an adaptable test set\nfor evolving threats. We develop quantization and loop-unrolling techniques\nthat enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs\n(with support for formally verified decision tree inference) within the\nEthereum block gas limit, while remaining bit-exact to their off-chain\ncounterparts, formally proven in Z3. We curate 298 unique real-world exploits\n(2020 - 2025) with 402 exploit transactions across eight EVM chains,\ncollectively responsible for \\$3.74 B in losses."}
{"id": "2510.16028", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16028", "abs": "https://arxiv.org/abs/2510.16028", "authors": ["Jianzhu Yao", "Hongxu Su", "Taobo Liao", "Zerui Cheng", "Huan Zhang", "Xuechao Wang", "Pramod Viswanath"], "title": "Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks", "comment": "17 pages, 7 figures", "summary": "Neural networks increasingly run on hardware outside the user's control\n(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about\nwhat actually ran or whether returned outputs faithfully reflect the intended\ninputs. Users lack recourse against service downgrades (model swaps,\nquantization, graph rewrites, or discrepancies like altered ad embeddings).\nVerifying outputs is hard because floating-point(FP) execution on heterogeneous\naccelerators is inherently nondeterministic. Existing approaches are either\nimpractical for real FP neural networks or reintroduce vendor trust. We present\nNAO: a Nondeterministic tolerance Aware Optimistic verification protocol that\naccepts outputs within principled operator-level acceptance regions rather than\nrequiring bitwise equality. NAO combines two error models: (i) sound\nper-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile\nprofiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,\nthreshold-guided dispute game that recursively partitions the computation graph\nuntil one operator remains, where adjudication reduces to a lightweight\ntheoretical-bound check or a small honest-majority vote against empirical\nthresholds. Unchallenged results finalize after a challenge window, without\nrequiring trusted hardware or deterministic kernels. We implement NAO as a\nPyTorch-compatible runtime and a contract layer currently deployed on Ethereum\nHolesky testnet. The runtime instruments graphs, computes per-operator bounds,\nand runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on\nQwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,\nRTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than\ntheoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO\nreconciles scalability with verifiability for real-world heterogeneous ML\ncompute."}
{"id": "2510.16037", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16037", "abs": "https://arxiv.org/abs/2510.16037", "authors": ["Peini Cheng", "Amir Bahmani"], "title": "Membership Inference over Diffusion-models-based Synthetic Tabular Data", "comment": null, "summary": "This study investigates the privacy risks associated with diffusion-based\nsynthetic tabular data generation methods, focusing on their susceptibility to\nMembership Inference Attacks (MIAs). We examine two recent models, TabDDPM and\nTabSyn, by developing query-based MIAs based on the step-wise error comparison\nmethod. Our findings reveal that TabDDPM is more vulnerable to these attacks.\nTabSyn exhibits resilience against our attack models. Our work underscores the\nimportance of evaluating the privacy implications of diffusion models and\nencourages further research into robust privacy-preserving mechanisms for\nsynthetic data generation."}
{"id": "2510.16040", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16040", "abs": "https://arxiv.org/abs/2510.16040", "authors": ["Tianhua Xia", "Sai Qian Zhang"], "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing", "comment": null, "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions."}
{"id": "2510.16078", "categories": ["cs.CR", "cs.AI", "cs.CV", "68T10, 68T45, 94A60", "I.4.8; I.5.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.16078", "abs": "https://arxiv.org/abs/2510.16078", "authors": ["Abdelilah Ganmati", "Karim Afdel", "Lahcen Koutti"], "title": "ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates", "comment": "~14 pages, 6 figures, 6 tables. Source uses elsarticle class; all\n  figures included as PNG/PDF. Primary: cs.CV", "summary": "We present a practical match-on-card design for face verification in which\ncompact 64/128-bit templates are produced off-card by PCA-ITQ and compared\non-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and\n14443-4 command APDUs with fixed-length payloads and decision-only status words\n(no score leakage), together with a minimal per-identity EEPROM map. Using real\nbinary codes from a CelebA working set (55 identities, 412 images), we (i)\nderive operating thresholds from ROC/DET, (ii) replay enroll->verify\ntransactions at those thresholds, and (iii) bound end-to-end time by pure link\nlatency plus a small constant on-card budget. Even at the slowest contact rate\n(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at\n38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,\nwhile 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted\nsymbol-level parity over empirically unstable bits) is latency-negligible.\nOverall, short binary templates, fixed-payload decision-only APDUs, and\nconstant-time matching satisfy ISO/IEC transport constraints with wide timing\nmargin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset\nevaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and\non-card microbenchmarks as next steps."}
{"id": "2510.16219", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16219", "abs": "https://arxiv.org/abs/2510.16219", "authors": ["Yang Feng", "Xudong Pan"], "title": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection", "comment": null, "summary": "Malicious agents pose significant threats to the reliability and\ndecision-making capabilities of Multi-Agent Systems (MAS) powered by Large\nLanguage Models (LLMs). Existing defenses often fall short due to reactive\ndesigns or centralized architectures which may introduce single points of\nfailure. To address these challenges, we propose SentinelNet, the first\ndecentralized framework for proactively detecting and mitigating malicious\nbehaviors in multi-agent collaboration. SentinelNet equips each agent with a\ncredit-based detector trained via contrastive learning on augmented adversarial\ndebate trajectories, enabling autonomous evaluation of message credibility and\ndynamic neighbor ranking via bottom-k elimination to suppress malicious\ncommunications. To overcome the scarcity of attack data, it generates\nadversarial trajectories simulating diverse threats, ensuring robust training.\nExperiments on MAS benchmarks show SentinelNet achieves near-perfect detection\nof malicious agents, close to 100% within two debate rounds, and recovers 95%\nof system accuracy from compromised baselines. By exhibiting strong\ngeneralizability across domains and attack patterns, SentinelNet establishes a\nnovel paradigm for safeguarding collaborative MAS."}
{"id": "2510.16255", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16255", "abs": "https://arxiv.org/abs/2510.16255", "authors": ["Sarah Egler", "John Schulman", "Nicholas Carlini"], "title": "Detecting Adversarial Fine-tuning with Auditing Agents", "comment": null, "summary": "Large Language Model (LLM) providers expose fine-tuning APIs that let end\nusers fine-tune their frontier LLMs. Unfortunately, it has been shown that an\nadversary with fine-tuning access to an LLM can bypass safeguards. Particularly\nconcerning, such attacks may avoid detection with datasets that are only\nimplicitly harmful. Our work studies robust detection mechanisms for\nadversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning\nauditing agent and show it can detect harmful fine-tuning prior to model\ndeployment. We provide our auditing agent with access to the fine-tuning\ndataset, as well as the fine-tuned and pre-fine-tuned models, and request the\nagent assigns a risk score for the fine-tuning job. We evaluate our detection\napproach on a diverse set of eight strong fine-tuning attacks from the\nliterature, along with five benign fine-tuned models, totaling over 1400\nindependent audits. These attacks are undetectable with basic content\nmoderation on the dataset, highlighting the challenge of the task. With the\nbest set of affordances, our auditing agent achieves a 56.2% detection rate of\nadversarial fine-tuning at a 1% false positive rate. Most promising, the\nauditor is able to detect covert cipher attacks that evade safety evaluations\nand content moderation of the dataset. While benign fine-tuning with\nunintentional subtle safety degradation remains a challenge, we establish a\nbaseline configuration for further work in this area. We release our auditing\nagent at https://github.com/safety-research/finetuning-auditor."}
{"id": "2510.16558", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16558", "abs": "https://arxiv.org/abs/2510.16558", "authors": ["Xiaofan Li", "Xing Gao"], "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem", "comment": null, "summary": "The Model Context Protocol (MCP) is an emerging open standard that enables\nAI-powered applications to interact with external tools through structured\nmetadata. A rapidly growing ecosystem has formed around MCP, including a wide\nrange of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP\nregistries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),\nand thousands of community-contributed MCP servers. Although the MCP ecosystem\nis gaining traction, there has been little systematic study of its architecture\nand associated security risks. In this paper, we present the first\ncomprehensive security analysis of the MCP ecosystem. We decompose MCP\necosystem into three core components: hosts, registries, and servers, and study\nthe interactions and trust relationships among them. Users search for servers\non registries and configure them in the host, which translates LLM-generated\noutput into external tool invocations provided by the servers and executes\nthem. Our qualitative analysis reveals that hosts lack output verification\nmechanisms for LLM-generated outputs, enabling malicious servers to manipulate\nmodel behavior and induce a variety of security threats, including but not\nlimited to sensitive data exfiltration. We uncover a wide range of\nvulnerabilities that enable attackers to hijack servers, due to the lack of a\nvetted server submission process in registries. To support our analysis, we\ncollect and analyze a dataset of 67,057 servers from six public registries. Our\nquantitative analysis demonstrates that a substantial number of servers can be\nhijacked by attackers. Finally, we propose practical defense strategies for MCP\nhosts, registries, and users. We responsibly disclosed our findings to affected\nhosts and registries."}
{"id": "2510.16923", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16923", "abs": "https://arxiv.org/abs/2510.16923", "authors": ["Mansi Phute", "Matthew Hull", "Haoran Wang", "Alec Helbling", "ShengYun Peng", "Willian Lunardi", "Martin Andreoni", "Wenke Lee", "Polo Chau"], "title": "UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks", "comment": null, "summary": "Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks."}
{"id": "2510.16933", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16933", "abs": "https://arxiv.org/abs/2510.16933", "authors": ["Matyáš Brabec", "Jiří Klepl", "Michal Töpfer", "Martin Kruliš"], "title": "Tutoring LLM into a Better CUDA Optimizer", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Euro-Par 2025: Parallel Processing, Part II, and is available\n  online at https://doi.org/10.1007/978-3-031-99857-7_18", "summary": "Recent leaps in large language models (LLMs) caused a revolution in\nprogramming tools (like GitHub Copilot) that can help with code generation,\ndebugging, and even performance optimization. In this paper, we focus on the\ncapabilities of the most recent reasoning models to generate optimized CUDA\ncode for predefined, well-known tasks. Our objective is to determine which\ntypes of code optimizations and parallel patterns the LLMs can perform by\nthemselves and whether they can be improved by tutoring (providing more\ndetailed hints and guidelines in the prompt). The generated solutions were\nevaluated both automatically (for correctness and speedup) and manually (code\nreviews) to provide a more detailed perspective. We also tried an interactive\napproach where the LLM can fix its previous mistakes within a session. The\nresults indicate that LLMs are quite skilled coders; however, they require\ntutoring to reach optimized solutions provided by parallel computing experts."}
{"id": "2510.17098", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17098", "abs": "https://arxiv.org/abs/2510.17098", "authors": ["Elias Hossain", "Swayamjit Saha", "Somshubhra Roy", "Ravi Prasad"], "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models", "comment": null, "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."}
{"id": "2510.17621", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17621", "abs": "https://arxiv.org/abs/2510.17621", "authors": ["Vincenzo Carletti", "Pasquale Foggia", "Carlo Mazzocca", "Giuseppe Parrella", "Mario Vento"], "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models", "comment": null, "summary": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric."}
{"id": "2510.17687", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17687", "abs": "https://arxiv.org/abs/2510.17687", "authors": ["Xu Zhang", "Hao Li", "Zhichao Lu"], "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks", "comment": "14 pages, 8 figures, 2 tables", "summary": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and\nperception capabilities but are increasingly vulnerable to jailbreak attacks.\nWhile existing work focuses on explicit attacks, where malicious content\nresides in a single modality, recent studies reveal implicit attacks, in which\nbenign text and image inputs jointly express unsafe intent. Such joint-modal\nthreats are difficult to detect and remain underexplored, largely due to the\nscarcity of high-quality implicit data. We propose ImpForge, an automated\nred-teaming pipeline that leverages reinforcement learning with tailored reward\nmodules to generate diverse implicit samples across 14 domains. Building on\nthis dataset, we further develop CrossGuard, an intent-aware safeguard\nproviding robust and comprehensive defense against both explicit and implicit\nthreats. Extensive experiments across safe and unsafe benchmarks, implicit and\nexplicit attacks, and multiple out-of-domain settings demonstrate that\nCrossGuard significantly outperforms existing defenses, including advanced\nMLLMs and guardrails, achieving stronger security while maintaining high\nutility. This offers a balanced and practical solution for enhancing MLLM\nrobustness against real-world multimodal threats."}
