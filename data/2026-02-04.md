<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.AI](#cs.AI) [Total: 49]
- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling](https://arxiv.org/abs/2602.03081)
*Mohammadali Khodabandehlou,Jared Coleman,Niranjan Suri,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 研究动态调度中的可控抢占模型，通过选择性重新调度最近任务图来平衡性能指标，相比完全抢占能获得相近收益但保持公平性和低开销


<details>
  <summary>Details</summary>
Motivation: 传统动态调度通常不重新考虑已分配任务，主要关注最小化完成时间，需要研究可控抢占调度来平衡多个性能指标

Method: 提出Last-K抢占模型，选择性重新调度最近的任务图，同时保留早期分配；比较抢占式、非抢占式和部分抢占式策略

Result: 适度抢占能够匹配完全抢占在完成时间和利用率方面的大部分收益，同时保持公平性和低运行时开销

Conclusion: 可控抢占调度在动态任务图调度中是有效的，适度抢占能在性能收益和系统开销之间取得良好平衡

Abstract: Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead.

</details>


### [2] [Exploiting Multi-Core Parallelism in Blockchain Validation and Construction](https://arxiv.org/abs/2602.03444)
*Arivarasan Karmegam,Lucianna Kiffer,Antonio Fernández Anta*

Main category: cs.DC

TL;DR: 区块链验证器利用多核CPU并行处理区块，但需保持确定性执行。本文研究验证器如何在区块构建和执行中利用多核并行而不违反区块链语义，提出了两个优化问题的MILP公式和启发式算法。


<details>
  <summary>Details</summary>
Motivation: 区块链验证器可以通过多核CPU减少区块处理时间，但必须保持确定性执行，同时尊重交易冲突和每个区块的运行时间限制。需要系统研究如何在不违反区块链语义的前提下，在区块构建和执行中利用多核并行性。

Method: 形式化了两个验证器端优化问题：1) 在p个核心上执行已排序区块以最小化完成时间，同时确保与顺序执行等价；2) 在运行时限制B下选择和调度内存池交易子集以最大化验证器奖励。为两者开发了精确的混合整数线性规划(MILP)公式，并提出快速确定性启发式算法。

Result: 使用以太坊主网跟踪数据，包括Solana启发的声明访问基线(Sol)用于有序区块调度和简单奖励贪婪基线(RG)用于区块构建，实证量化了最优性和运行时间之间的权衡。

Conclusion: 本文系统研究了区块链验证器如何利用多核并行性进行区块处理，提出了形式化优化问题和解决方案，为实际区块链系统中的并行处理提供了理论基础和实用算法。

Abstract: Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \(p\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \(B\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.

</details>


### [3] [Recursive Energy Efficient Agreement](https://arxiv.org/abs/2602.03474)
*Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 提出了一种递归的拜占庭容错协议算法，将每个参与者的活跃轮数减少到O(log f)，显著降低了能耗


<details>
  <summary>Details</summary>
Motivation: 传统分布式共识协议中，所有参与者需要全程参与所有轮次，能耗较高。最近的研究提出了"能效共识"概念，旨在通过减少参与者活跃轮数来降低能耗成本

Method: 采用递归算法设计共识协议，其中f<n表示系统中的最大崩溃故障数。算法通过递归结构优化参与者的活跃轮数

Result: 实现了每个参与者仅需O(log f)个活跃轮次，相比传统协议显著降低了能耗成本

Conclusion: 该递归算法为分布式共识提供了能效优化的新方法，在保证协议正确性的同时大幅减少了参与者的能量消耗

Abstract: Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system.

</details>


### [4] [DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs](https://arxiv.org/abs/2602.03495)
*Zeyu Zhu,Gang Li,Peisong Wang,Zitao Mo,Minnan Pei,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.DC

TL;DR: DALI是一个针对本地PC上MoE模型推理的工作负载感知卸载框架，通过动态专家分配、基于残差的预取和工作负载感知缓存替换策略，解决了现有方法在CPU-GPU负载均衡、预取准确性和缓存效率方面的不足。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能显著提升LLM容量而不成比例增加计算量，但参数规模巨大。将MoE专家参数卸载到主机内存并利用CPU和GPU计算是支持资源受限本地PC平台的有前景方向。然而现有方法无法匹配专家工作负载的动态特性，导致三个基本低效问题：静态专家分配导致CPU-GPU负载严重不均衡；现有预取技术无法准确预测高工作负载专家；GPU缓存策略忽视工作负载动态性。

Method: DALI提出三个关键技术：1) 动态专家分配：将分配建模为0-1整数优化问题，运行时使用贪心分配策略高效解决；2) 基于残差的预取：利用层间残差信息准确预测高工作负载专家；3) 工作负载感知缓存替换：利用专家激活的时间相关性提高GPU缓存效率。

Result: 在各种MoE模型和设置下评估，DALI在预填充和解码阶段相比最先进的卸载框架实现了显著加速。

Conclusion: DALI通过工作负载感知的卸载框架有效解决了MoE推理在本地PC上的效率问题，通过动态资源分配、准确预取和智能缓存管理，显著提升了系统性能。

Abstract: Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks.

</details>


### [5] [Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods](https://arxiv.org/abs/2602.03802)
*Grigory Begunov,Alexander Tyurin*

Main category: cs.DC

TL;DR: 同步SGD及其变体m-Synchronous SGD在异构计算场景中近乎最优，与传统认知相反。论文分析了随机计算时间和对抗性部分参与下的同步方法，证明其时间复杂度在许多实际场景中达到最优（至多对数因子）。


<details>
  <summary>Details</summary>
Motivation: 尽管异步优化方法近年来取得显著进展，但现代分布式优化方法仍主要依赖传统同步方法。作者重新审视同步SGD及其鲁棒变体，旨在证明它们在异构计算场景中的优越性，这与传统认知相悖。

Method: 研究同步SGD及其鲁棒变体m-Synchronous SGD，在随机计算时间和对抗性部分参与的工作节点条件下进行理论分析。通过数学证明方法，分析这些同步方法的时间复杂度特性。

Result: 理论证明同步方法在多种异构计算场景中近乎最优，其时间复杂度在许多实际场景中达到最优（至多对数因子）。虽然同步方法不是通用解决方案，但在许多现代异构计算场景中已经足够。

Conclusion: 同步SGD及其变体在异构计算环境中具有出乎意料的优越性能，在许多实际场景中达到近乎最优的时间复杂度，挑战了异步方法必然优于同步方法的传统观念。

Abstract: Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在零样本推荐中的不确定性和公平性问题，提出了包含不确定性评估和人格感知公平性的新评估方法，发现Gemini 1.5 Flash存在系统性不公平，并建立了包含电影和音乐两个领域的数据集基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能够利用广泛的上下文知识进行强大的零样本推荐，但其预测不确定性和嵌入的偏见威胁着推荐系统的可靠性和公平性。需要系统评估这些因素如何影响LLM生成推荐的准确性、一致性和可信度。

Method: 1. 引入包含精心设计指标的基准和数据集，该数据集标注了8个人口统计属性（31个分类值），涵盖电影和音乐两个领域。2. 通过深度案例研究量化预测不确定性（通过熵计算）。3. 分析Gemini 1.5 Flash在不同敏感属性上的系统性不公平。4. 在提示扰动（如拼写错误和多语言输入）下测试公平性。5. 将人格感知公平性整合到RecLLM评估流程中，揭示人格相关的偏见模式。6. 提出新的不确定性感知评估方法。

Result: 1. 发现Google DeepMind的Gemini 1.5 Flash对某些敏感属性表现出系统性不公平，测量到的基于相似性的差距为SNSR 0.1363和SNSV 0.0507。2. 这些不公平在提示扰动下仍然持续存在。3. 揭示了人格个性化与群体公平性之间的权衡。4. 建立了包含不确定性案例研究的经验洞察。5. 创建了人格档案知情的公平性基准，提高了LLM推荐的可解释性和公平性。

Conclusion: 该研究为更安全、更可解释的RecLLM奠定了基础，推动了多模型基准和自适应校准的未来工作，以实现可信赖的部署。提出的不确定性感知评估方法和人格感知公平性基准共同推进了LLM推荐系统的公平性和可解释性。

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [7] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: LLM自我解释的忠实性评估新指标NSG：通过可模拟性增益衡量解释能否帮助预测模型行为，发现自我解释比外部解释更具预测价值，但存在5-15%严重误导性解释。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我解释的忠实性评估方法存在局限，主要依赖对抗性提示或检测推理错误，忽视了解释的预测价值。需要一种更全面、可扩展的忠实性评估指标。

Method: 提出归一化可模拟性增益（NSG）指标，基于"忠实解释应能让观察者学习模型的决策标准并更好预测其行为"的理念。在7,000个反事实样本上评估18个前沿专有和开源模型，涵盖健康、商业和伦理领域。

Result: 自我解释显著提升模型行为预测能力（11-37% NSG增益）；自我解释比外部模型生成的解释更具预测信息，即使外部模型更强；所有模型中5-15%的自我解释存在严重误导性。

Conclusion: 尽管存在缺陷，自我解释确实编码了有助于预测模型行为的信息，展现了自我知识优势，这是外部解释方法无法复制的。NSG为LLM解释忠实性评估提供了新视角。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [8] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS框架通过预算感知规划、模块化构建和对比反思记忆三大支柱，优化了AI研究的自动化，在MLE-Bench上达到开源框架的SOTA性能，并能有效跨搜索路径泛化洞察。


<details>
  <summary>Details</summary>
Motivation: AI研究与一般软件工程不同，涉及计算昂贵的评估（如模型训练）和不透明的性能归因。当前基于LLM的智能体在这方面表现不佳，常生成忽略执行成本和因果关系的整体脚本。

Method: MARS框架基于三大支柱：1) 预算感知规划：使用成本约束的蒙特卡洛树搜索(MCTS)平衡性能与执行成本；2) 模块化构建：采用"设计-分解-实现"管道管理复杂研究仓库；3) 对比反思记忆：通过分析解决方案差异来提取高信号洞察，解决信用分配问题。

Result: MARS在MLE-Bench上实现了开源框架中最先进的性能，在可比设置下与排行榜顶级方法保持竞争力。63%的已使用经验来自跨分支转移，表明智能体能有效跨搜索路径泛化洞察。

Conclusion: MARS框架通过预算感知规划、模块化构建和对比反思记忆，成功优化了AI研究的自动化，解决了计算成本高和性能归因不透明的问题，展现出有效的跨路径知识迁移能力。

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [9] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 本文提出了一种动态混合精度路由框架，用于在长时程决策任务中自适应选择高精度和低精度LLM，以平衡任务成功率和推理成本。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在长时程决策任务中表现良好，但使用大型LLM进行多步交互会产生高昂的推理成本。传统观点认为更高的任务成功率需要使用更大更强的LLM模型，但这会导致成本过高。

Method: 提出动态混合精度路由框架，基于观察到交互步骤的敏感性差异，自适应地在每个决策步骤选择高精度或低精度LLM。路由器通过两阶段管道训练：1) 基于KL散度的监督学习识别精度敏感步骤；2) 使用组相对策略优化(GRPO)进一步提高任务成功率。

Result: 在ALFWorld上的实验表明，该方法在准确率-成本权衡方面相比单精度基线和启发式路由方法有显著提升。

Conclusion: 通过动态混合精度路由框架，可以在保持任务成功率的同时显著降低长时程决策任务的推理成本，为LLM在实际应用中的部署提供了有效的成本优化方案。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [10] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: Cuttlefish是一个统一的全原子大语言模型，通过自适应缩放的结构补丁和几何接地适配器，在保持几何线索的同时解决模态融合瓶颈问题，实现更好的结构推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理生物分子结构时存在两个主要问题：1）通过序列化标记或固定长度连接器压缩结构输入，导致几何基础信息丢失，增加结构幻觉风险；2）模态融合瓶颈既过度压缩又次优分配结构标记，阻碍了通用全原子推理的实现。

Method: Cuttlefish采用两个核心技术：1）缩放感知补丁：通过指令条件门控机制在结构图上生成可变大小的补丁，根据结构复杂性自适应缩放查询标记预算，缓解固定长度连接器瓶颈；2）几何接地适配器：通过跨注意力机制将自适应标记与模态嵌入进行细化，并将生成的模态标记注入LLM，暴露显式几何线索以减少结构幻觉。

Result: 在多样化的全原子基准测试中，Cuttlefish在异构结构接地推理方面实现了卓越性能，代码已在项目仓库中提供。

Conclusion: Cuttlefish通过结合自适应缩放的结构补丁和几何接地适配器，成功解决了现有方法在结构推理中的局限性，实现了语言推理在几何线索中的有效接地，为全原子推理提供了统一且高效的解决方案。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [11] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer是一个基于大语言模型的反射式元优化框架，用于模拟混合信号集成电路的晶体管尺寸优化，通过内外双循环结构实现自适应搜索空间构建和优化协调，在24个电路基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号集成电路设计严重依赖专家知识，晶体管尺寸优化面临非线性行为、高维设计空间和严格性能约束等挑战。现有EDA方法通常将尺寸优化视为静态黑盒优化，导致效率低下且鲁棒性差。虽然大语言模型具备强大推理能力，但不适合AMS尺寸优化的精确数值优化。

Method: 提出AutoSizer框架，采用反射式LLM驱动的元优化方法，统一电路理解、自适应搜索空间构建和优化协调。采用双循环优化框架：内循环负责电路尺寸优化，外循环分析优化动态和约束，根据仿真反馈迭代细化搜索空间。还建立了AMS-SizingBench基准测试集。

Result: AutoSizer在实验中实现了更高的解决方案质量、更快的收敛速度和更高的成功率，在不同电路难度下均优于传统优化方法和现有基于LLM的智能体。AMS-SizingBench包含24个采用SKY130 CMOS技术的多样化AMS电路。

Conclusion: AutoSizer成功地将LLM的推理能力与数值优化相结合，为AMS电路尺寸优化提供了一个有效的元优化框架，能够自适应地构建搜索空间并协调优化过程，显著提升了优化效率和鲁棒性。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [12] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: STEER框架通过进化多样性搜索构建自然语言角色群体，提供单一可解释控制参数来调整决策保守性，解决LLM在有序决策任务中的模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在平均正确性训练下容易出现模式崩溃，在有序决策任务中无法根据上下文约束权衡特异性和敏感性。这在临床分诊等需要可调节风险控制的场景中尤为成问题。

Method: STEER框架通过离线约束质量-多样性搜索构建自然语言角色群体，确保行为覆盖同时强制执行最低安全、推理和稳定性阈值。推理时通过单一可解释控制参数将用户指定的风险百分位数映射到选定角色。

Result: 在两个临床分诊基准测试中，STEER相比基于温度的采样和静态角色集成实现了更广泛的行为覆盖。相比代表性后训练方法，在明确紧急情况下保持更高准确性，同时在模糊决策上提供可比的控制能力。

Conclusion: STEER是一种保持安全性的风险控制范式，能够在不影响领域能力的情况下引导行为，为有序决策任务提供了有效的可调节控制框架。

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [13] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: 该论文提出了BenchAlign方法，通过使用有限的实际性能信息自动更新离线基准测试，使其能更好地预测模型在真实场景中的效用


<details>
  <summary>Details</summary>
Motivation: 当前语言模型基准测试虽然计算高效，但往往无法准确预测模型在实际应用中的真实性能。许多研究发现基准测试与真实效用之间存在差距，需要建立更好的关联

Method: 提出了BenchAlign方法，通过收集模型在部署期间的成对排名信息，结合问题级别的性能数据，学习与偏好对齐的基准测试问题权重，生成新的静态基准测试

Result: 实验表明，对齐后的基准测试能够准确根据人类偏好模型对未见过的模型进行排名，在不同规模模型上均有效，同时保持可解释性

Conclusion: 这项工作为基准测试与实际人类偏好的对齐提供了见解，有望加速模型开发向真实效用的方向推进

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [14] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文研究了思维链推理所需的最小推理token数量，证明了三个典型任务需要Ω(n)个推理token，并通过实验验证了理论下界。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能提升LLM性能，但带来了显著的延迟和计算成本。本文旨在从理论上探究随着输入规模增长，解决问题所需的最小推理token数量，以理解推理时计算的基本瓶颈。

Method: 扩展了有界注意力前缀预言机模型，用于量化解决任务所需的信息流。针对三个典型的BAPO-hard任务（二进制多数、三元组匹配、图可达性），证明了推理token数量的下界，并提供了匹配或接近匹配的上界构造。

Result: 证明了三个任务都需要Ω(n)个推理token（输入规模为n）。实验显示前沿推理模型在这些任务上表现出近似线性的推理token缩放，当限制在较小的推理预算时会失败，这与理论下界一致。

Conclusion: 研究揭示了思维链推理在推理时计算中的基本瓶颈，为分析最优推理长度提供了原则性工具，有助于理解LLM推理效率的极限。

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [15] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: DeltaEvolve：一种动量驱动的进化框架，用结构化语义增量替代完整代码历史，通过捕捉连续节点间修改如何影响性能来更高效地指导进化


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的进化系统（如AlphaEvolve）依赖完整代码历史，存在上下文效率低和进化指导弱的问题。完整代码快照包含冗余实现细节，稀释了核心算法思想，难以提供清晰的进化灵感

Method: 将进化智能体形式化为期望最大化框架：语言模型采样候选程序（E步），系统基于评估反馈更新控制上下文（M步）。提出DeltaEvolve框架，用结构化语义增量替代完整代码历史，捕捉连续节点间修改的"如何"和"为何"影响性能。通过多级数据库和渐进披露机制组织语义增量，减少输入标记

Result: 在多个科学领域的任务上进行实证评估，结果显示该框架能够以更少的标记消耗发现比基于完整代码的进化智能体更好的解决方案

Conclusion: 结构化语义增量比完整代码历史更有效地指导进化过程，DeltaEvolve框架通过动量驱动的方法提高了进化效率，减少了计算资源消耗

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [16] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: UAT-LITE：通过推理时在预训练Transformer分类器中应用蒙特卡洛dropout进行近似贝叶斯推断，使自注意力机制具备不确定性感知能力，从而改善模型校准性能。


<details>
  <summary>Details</summary>
Motivation: 神经NLP模型通常存在校准不足的问题，会对错误预测分配高置信度，这影响了选择性预测和高风险部署。现有的后处理校准方法只调整输出概率而不改变内部计算，而集成和贝叶斯方法虽然能改善不确定性但需要大量训练或存储成本。

Method: 提出UAT-LITE推理时框架，在预训练Transformer分类器中通过蒙特卡洛dropout进行近似贝叶斯推断，使自注意力机制具备不确定性感知能力。通过随机前向传播估计标记级认知不确定性，并用其调制自注意力机制中的上下文化过程，无需修改预训练权重或训练目标。还引入了层间方差分解来诊断预测不确定性如何在Transformer深度中累积。

Result: 在SQuAD 2.0可回答性、MNLI和SST-2数据集上，UAT-LITE相对于微调的BERT-base基线，平均减少了约20%的预期校准误差，同时保持了任务准确性，并改善了选择性预测和分布偏移下的鲁棒性。

Conclusion: UAT-LITE提供了一种轻量级的推理时方法，通过使自注意力机制具备不确定性感知能力，有效改善预训练Transformer分类器的校准性能，而无需修改模型权重或训练过程。

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [17] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: GCR-RL：一种基于序理论的新型强化学习方法，通过将价值函数估计重新构建为学习期望偏序集，利用几何相干正则化提升样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法可以利用几何特性（如对称结构、几何感知数据增强、结构限制）来稳定和加速学习过程。本文从序理论的新视角出发，将价值函数估计重新构建为学习期望偏序集，旨在通过确保支撑学习价值函数的偏序集序列的几何相干性来进一步提升性能。

Method: 提出GCR-RL（几何相干正则化强化学习），通过计算超偏序集细化序列来实现：细化先前步骤中的偏序集，并从时序差分信号中学习额外的序关系。开发了两种新算法：基于Q-learning的算法和基于actor-critic的算法，以高效实现这些超偏序集细化。

Result: 分析了算法的理论性质和收敛速度。在多个任务中进行实证评估，结果显示GCR-RL相比强基线方法在样本效率和稳定性能方面有显著提升。

Conclusion: 通过序理论视角重新构建强化学习问题，提出GCR-RL方法，通过确保偏序集序列的几何相干性，有效提升了强化学习的样本效率和稳定性，为利用几何特性优化强化学习提供了新思路。

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [18] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs在因果推理任务中表现出比人类更规则化的推理策略，较少受到人类特有的偏见影响，但可能在不确定性场景下失效


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在需要因果推理的领域应用增多，需要了解其推理机制是遵循规范因果计算、人类式捷径还是脆弱的模式匹配

Method: 在11个基于碰撞器结构(C₁→E←C₂)的因果判断任务上，将20多个LLMs与匹配的人类基线进行对比，使用可解释模型压缩LLMs的因果判断，并测试其在语义抽象和提示过载下的鲁棒性

Result: LLMs表现出比人类更规则化的推理策略，人类在概率判断中似乎考虑了未提及的潜在因素；大多数LLMs没有表现出人类特有的碰撞器偏见（弱解释消除和马尔可夫违反）；思维链(CoT)提高了许多LLMs的鲁棒性

Conclusion: LLMs的推理策略与人类存在差异，当已知偏见不可取时可以补充人类判断，但其规则化推理在不确定性场景下可能失效，需要表征LLM推理策略以确保安全有效部署

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [19] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: LLMs在训练中获得了序列规划能力，但在推理时表现出短视和不一致的行为。研究提出贝叶斯解释：自生成上下文驱动规划偏移，导致看似受损的规划行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练中获得了序列级规划能力，但在推理时却表现出短视和不一致的规划行为，这种差距需要理论解释。

Method: 提出贝叶斯解释框架，将规划行为基于不断演化的生成上下文。通过两个受控实验验证：随机生成任务展示人类提示下的受限规划以及自生成上下文积累时规划强度的增加；高斯采样任务展示基于自生成序列时初始偏见的减少。

Result: 实验验证了提出的模型：随机生成任务显示随着自生成上下文积累，规划强度增加；高斯采样任务显示基于自生成序列时初始偏见减少。这些结果为LLMs在推理时如何提前规划提供了理论和实证证据。

Conclusion: 研究通过贝叶斯框架解释了LLMs规划能力在训练与推理之间的差距，认为自生成上下文驱动规划偏移是导致看似受损规划行为的原因，为理解LLMs推理时规划机制提供了新视角。

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [20] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 论文综述了可微分社会选择这一新兴范式，将投票规则、机制和聚合过程构建为可从数据中学习的可微分模型，连接了机器学习、经济学和民主理论。


<details>
  <summary>Details</summary>
Motivation: 社会选择已从政治理论和经济学的外围问题转变为现代机器学习系统的核心组成部分。从拍卖、资源分配到联邦学习、参与式治理和大语言模型对齐，机器学习系统越来越多地聚合异质偏好、激励和判断形成集体决策。许多当代机器学习系统已经实现了社会选择机制，但通常是隐式的且缺乏明确的规范审查。

Method: 采用可微分社会选择范式，将投票规则、机制和聚合过程构建为可学习的、可微分的模型，通过数据优化。综合了拍卖、投票、预算分配、流动民主、去中心化聚合和逆向机制学习等领域的工作，展示经典公理和不可能性定理如何重新表现为目标、约束和优化权衡。

Result: 建立了可微分社会选择的理论框架，展示了如何将传统社会选择理论中的规范要求转化为机器学习中的优化问题。通过这一范式，可以设计出既满足理论公理又适应实际数据特征的聚合机制。

Conclusion: 可微分社会选择为机器学习、经济学和民主理论的交叉领域开辟了新的研究方向。论文最后提出了36个开放性问题，定义了一个新的研究议程，旨在推动这一跨学科领域的发展。

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [21] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP框架通过将LLM推理过程外部化为有向无环图，使用模块化概念预测器进行主动蒸馏，提高样本效率和训练稳定性


<details>
  <summary>Details</summary>
Motivation: 当前LLM在判别任务部署中存在推理延迟、计算和API成本问题，传统主动蒸馏方法仅使用最终标签，丢弃了中间推理信号，缺乏对缺失推理的诊断能力

Method: 提出图概念预测器(GCP)框架：1) 将教师模型决策过程外部化为有向无环图；2) 使用模块化概念预测器模拟该图；3) 采用图感知获取策略针对关键推理节点的不确定性和分歧；4) 通过目标子模块重训练实现训练稳定性和效率

Result: 在八个NLP分类基准测试中，GCP在有限标注预算下提升了性能，同时提供了更可解释和可控的训练动态

Conclusion: GCP框架通过显式建模推理过程，实现了更高效、可解释的主动蒸馏，为LLM部署提供了新的解决方案

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [22] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR框架通过相似性引导的教师辅助精炼，将大语言模型的能力蒸馏到超小型模型，解决了现有方法中的过拟合、训练不稳定等问题，在函数调用任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在函数调用中很关键，但模型规模太大限制了广泛应用，需要将能力转移到小模型。现有方法存在过拟合、训练不稳定、多解任务奖励机制不佳、技术难以协同等问题。

Method: STAR框架包含两个核心技术：1) 约束知识蒸馏(CKD)，通过增强top-k前向KL散度来抑制错误预测，确保训练稳定性；2) 相似性引导强化学习(Sim-RL)，引入细粒度、基于相似性的奖励机制。这些技术在一个统一的训练课程中协同工作。

Result: 在具有挑战性的基准测试中，STAR模型在各自规模类别中达到SOTA。特别是0.6B的STAR模型在所有1B以下开源模型中表现最佳，甚至超过了几个更大规模的知名开源模型。

Conclusion: STAR展示了一个将大语言模型能力蒸馏到超小型模型的训练框架，为强大、可访问且高效的人工智能智能体铺平了道路。

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [23] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 提出RC-GRPO方法解决多轮工具调用中奖励稀疏和探索成本高的问题，通过奖励条件化轨迹策略和离散奖励令牌改善组内多样性，在BFCLv4基准上超越基线并超越闭源API模型


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用对大型语言模型具有挑战性，因为奖励稀疏且探索成本高。传统的SFT+GRPO方法在组内奖励变化低时（如组内更多rollout获得全0或全1奖励）会停滞，导致组归一化优势信息不足，更新消失

Method: 提出RC-GRPO（奖励条件化组相对策略优化），将探索视为通过离散奖励令牌的可控引导问题。首先在混合质量轨迹上微调奖励条件化轨迹策略（RCTP），在提示中注入奖励目标特殊令牌（如<|high_reward|>、<|low_reward|>），使模型能够按需生成不同质量的轨迹。然后在RL期间，在每个GRPO组内采样多样奖励令牌，并根据采样令牌条件化rollout以改善组内多样性

Result: 在Berkeley Function Calling Leaderboard v4多轮基准测试中，该方法相比基线获得持续改进的性能，Qwen-2.5-7B-Instruct模型的性能甚至超越了所有闭源API模型

Conclusion: RC-GRPO通过奖励条件化方法有效解决了多轮工具调用中的探索问题，改善了组内多样性，显著提升了模型性能，证明了该方法在复杂任务中的有效性

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [24] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS是一个基于工具驱动的多智能体系统，用于通用时间序列任务，通过分析器-推理器-执行器范式整合视觉推理和潜在重构，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分析方法在整合直观视觉推理和跨任务泛化方面存在局限，缺乏自适应工具使用能力，需要更通用的解决方案。

Method: 基于分析器-推理器-执行器范式，使用视觉语言模型对时间序列图进行视觉推理提取时序结构，在潜在空间重构预测轨迹，三个专门智能体通过共享内存和门控通信协调，路由器选择任务特定工具链执行。

Result: 在多个基准测试中，MAS4TS在广泛的时间序列任务上实现了最先进的性能，同时展现出强大的泛化能力和高效的推理效率。

Conclusion: MAS4TS通过整合视觉推理、潜在重构和多智能体协作，为通用时间序列分析提供了一个有效的工具驱动框架，显著提升了跨任务泛化能力。

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [25] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: 提出KANFIS（Kolmogorov-Arnold神经模糊推理系统），一种紧凑的神经符号架构，通过加法函数分解解决传统ANFIS在高维空间中的规则爆炸问题，实现线性复杂度并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统ANFIS架构存在结构复杂性问题，基于乘积的推理机制在高维空间中会导致规则数量指数级爆炸，限制了其实际应用。

Method: 提出KANFIS架构，采用加法聚合机制替代传统乘积推理，使模型参数和规则复杂度随输入维度线性增长而非指数增长；兼容Type-1和Interval Type-2模糊逻辑系统；使用稀疏掩码机制生成紧凑结构化规则集。

Result: KANFIS在性能上与代表性的神经和神经模糊基线模型具有竞争力，同时保持了内在的可解释性、清晰的规则语义和透明的推理过程。

Conclusion: KANFIS成功解决了传统ANFIS的规则爆炸问题，通过加法分解实现了紧凑的神经符号架构，在保持高性能的同时提供了更好的可解释性和不确定性建模能力。

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [26] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 该研究系统评估了多智能体系统中过程验证的有效性，发现过程级验证并不总能提升性能且方差较大，LLM-as-a-Judge方法表现最佳但仍有挑战


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在推理轨迹上存在高方差，过程验证在一般推理场景中显示出潜力，但其在多智能体系统中的实际效果尚不明确

Method: 提出了MAS-ProVe框架，系统研究了三种验证范式（LLM-as-a-Judge、奖励模型、过程奖励模型），在两个验证粒度（智能体级和迭代级）上评估，考察了五个代表性验证器和四种上下文管理策略，在六个不同的MAS框架和多个推理基准上进行实验

Result: 过程级验证不能一致提升性能且经常表现出高方差；LLM-as-a-Judge方法通常优于基于奖励的方法；训练过的法官优于通用LLM；LLM作为法官与作为单个智能体之间存在小性能差距；验证中存在上下文长度与性能的权衡

Conclusion: 多智能体系统的有效且鲁棒的过程验证仍然是一个开放挑战，需要超越当前范式的进一步进展

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [27] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench是一个用于系统评估LLM智能体安全性的框架，通过领域无关的安全原则和上下文感知的安全标准，在真实部署场景下评估智能体的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有智能体安全评估方法存在局限性：1）依赖针对特定智能体设置的风险导向任务，安全风险空间覆盖有限；2）无法评估智能体在复杂真实部署中长期、交互式任务执行中的安全行为；3）对特定智能体设置的专门化限制了跨不同智能体配置的适应性。

Method: 提出Risky-Bench框架：1）围绕领域无关的安全原则组织评估；2）推导上下文感知的安全标准来界定安全空间；3）在不同威胁假设下通过真实任务执行系统评估安全风险。

Result: 在生活辅助智能体设置中应用Risky-Bench，发现在真实执行条件下最先进的智能体存在重大安全风险。该框架可扩展到其他部署场景，构建环境特定的安全评估。

Conclusion: Risky-Bench提供了一个可扩展的智能体安全评估方法学，能够系统评估LLM智能体在真实世界部署中的安全风险，克服了现有评估方法的局限性。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [28] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: 本文分析了多智能体LLM框架架构设计对系统性能的影响，开发了MAFBench评估套件，发现框架级设计选择可导致延迟增加100倍、规划准确率下降30%、协调成功率从90%降至30%以下。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM框架广泛用于加速基于大语言模型的智能体系统开发，但这些框架采用不同的架构结构来管理智能体交互、信息存储和任务协调。然而，这些架构设计对系统性能的影响尚不明确，而架构选择本身可能导致延迟和吞吐量出现数量级差异，以及准确性和可扩展性的显著变化。

Method: 1) 引入多智能体LLM框架的架构分类法，从基本维度系统比较不同框架；2) 开发MAFBench统一评估套件，将现有基准测试集成到标准化执行流程中；3) 使用MAFBench对多个广泛使用的框架进行受控实证研究。

Result: 研究发现框架级设计选择本身可导致：延迟增加超过100倍；规划准确率下降高达30%；协调成功率从90%以上降至30%以下。这些结果表明架构设计对系统性能有重大影响。

Conclusion: 研究结果转化为具体的架构设计原则和框架选择指导，并指出了有前景的未来研究方向。强调需要联合评估多个能力，并在受控的框架级条件下进行评价以隔离架构效应。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [29] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文扩展了先前关于智能体世界模型的研究，从确定性、完全可观测环境扩展到随机性、部分可观测环境，证明随机智能体也无法避免学习其环境模型


<details>
  <summary>Details</summary>
Motivation: 先前研究证明在确定性、完全可观测环境下，几乎最优的通用智能体必然包含足够的环境知识以进行近似重建。本研究旨在移除这两个限制条件，探索在更现实环境下的智能体世界模型存在性。

Method: 将定理扩展到随机智能体在部分可观测环境中的情况，通过弱化通用性概念来证明更弱能力的智能体也包含世界模型。

Result: 证明了随机智能体无法通过随机化避免学习其环境，并且即使更弱能力的智能体（弱化通用性概念后）也包含其操作环境的世界模型。

Conclusion: 智能体世界模型的存在性具有普遍性，不仅限于确定性、完全可观测环境，随机智能体在部分可观测环境中同样必然学习并包含环境模型。

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [30] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 提出一种通用的缺失模态恢复策略，通过增强扩散模型作为可插拔模块来恢复缺失特征，在零样本评估中优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在模态不完整时性能急剧下降，现有方法存在两个困境：基于提示的方法难以恢复缺失的关键特征且损害模型泛化能力；基于插补的方法缺乏有效指导，容易生成语义无关的噪声

Method: 提出通用缺失模态恢复策略，引入增强扩散模型作为可插拔的中期训练模块，包含两个关键创新：动态模态门控机制自适应利用条件特征引导生成语义一致的特征；跨模态互学习机制桥接双编码器的语义空间实现双向对齐

Result: 在基准数据集上的零样本评估显示，该方法优于现有基线方法。大量实验和消融研究证实该模型是视觉语言模型在缺失模态场景下的鲁棒且可扩展的扩展，确保在不同缺失率和环境下的可靠性

Conclusion: 提出的通用缺失模态恢复策略通过增强扩散模型有效解决了模态不完整问题，在保持视觉语言模型泛化能力的同时恢复精确语义，为缺失模态场景提供了可靠解决方案

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [31] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW是一个统一框架，用于提取、评估和以校准强度控制大语言模型的价值对齐，包含分层价值嵌入空间、价值强度数据库和基于锚点的评估器。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型与人类价值对齐存在挑战：基于偏好的方法难以捕捉深层动机原则，基于价值的方法存在三个缺陷：提取忽略层次结构、评估只检测存在而非校准强度、对可控强度的可操控性理解不足。

Method: 提出VALUEFLOW框架，包含三个组件：1) HIVES分层价值嵌入空间，捕捉理论内和跨理论的价值结构；2) VIDB价值强度数据库，包含基于排序聚合获得强度估计的大规模价值标注文本；3) 基于锚点的评估器，通过与VIDB面板比较对模型输出产生一致的强度评分。

Result: 在10个模型和4个价值理论上的大规模研究发现了可操控性的不对称性和多价值控制的组合规律，建立了评估和控制价值强度的可扩展基础设施。

Conclusion: VALUEFLOW为大语言模型的多元对齐提供了可扩展的基础设施，通过校准强度控制推进了价值对齐的研究。

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [32] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling是一个基于轨迹多样性扩展的代码智能体数据合成框架，通过增加轨迹多样性而非单纯增加数据量来提升性能，解决了现有方法中低质量合成数据和数量扩展收益递减的问题。


<details>
  <summary>Details</summary>
Motivation: 随着代码大语言模型通过模型上下文协议演变为工具交互智能体，其泛化能力受到低质量合成数据和数量扩展收益递减的限制。数量为中心的扩展存在早期瓶颈，未能充分利用轨迹数据。

Method: TDScaling框架包含四个创新：1) 业务集群机制捕捉真实服务逻辑依赖；2) 蓝图驱动的多智能体范式确保轨迹一致性；3) 自适应进化机制使用领域熵、推理模式熵和累积动作复杂度引导合成面向长尾场景；4) 沙盒化代码工具防止内在编码能力的灾难性遗忘。

Result: 在通用工具使用基准（BFCL, tau^2-Bench）和代码智能体任务（RebenchT, CodeCI, BIRD）上的实验表明，TDScaling实现了双赢结果：既提高了工具使用的泛化能力，又增强了内在编码能力。

Conclusion: TDScaling通过轨迹多样性扩展而非数量扩展，在固定训练预算下提供了更好的性能-成本权衡，为代码智能体训练提供了更有效的数据合成方法，计划发布包含30,000+工具集群的完整代码库和合成数据集。

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [33] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: 论文提出TAME框架解决智能体记忆演化中的安全问题，通过双记忆系统分别优化任务执行和安全评估，在保持任务性能的同时提升可信度。


<details>
  <summary>Details</summary>
Motivation: 智能体在测试时通过记忆演化积累经验以增强复杂推理能力，但即使是在良性任务演化过程中，智能体的安全对齐仍然脆弱，这种现象被称为"智能体记忆错误演化"。需要评估和解决这一安全问题。

Method: 提出TAME双记忆演化框架：1) 执行器记忆演化以提升任务性能，通过提炼可泛化的方法论；2) 评估器记忆演化以基于历史反馈完善安全和任务效用的评估。通过记忆过滤、草稿生成、可信度精炼、执行和双轨记忆更新的闭环流程。

Result: 构建Trust-Memevo基准评估良性任务演化中的多维可信度，发现各任务领域和评估设置中可信度普遍下降。TAME框架实验表明能够缓解记忆错误演化，在可信度和任务性能上实现联合提升。

Conclusion: TAME框架通过分离演化执行器记忆和评估器记忆，在保持任务效用的同时保护可信度，为解决智能体记忆演化中的安全问题提供了有效方案。

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [34] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 本文提出需要统一的智能体评估框架来解决当前评估中存在的混杂因素和标准化不足问题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，通用智能体取得了根本性进展，但评估这些智能体面临独特挑战。当前智能体基准测试受到外部因素严重干扰，包括系统提示、工具集配置和环境动态等。现有评估依赖碎片化的研究者特定框架，提示工程差异大，难以将性能提升归因于模型本身。缺乏标准化环境数据导致错误难以追踪、结果不可复现，这种标准化缺失给领域带来了不公平和不透明。

Method: 提出一个旨在标准化智能体评估的提案，建立统一的评估框架。具体方法包括标准化系统提示、工具集配置和环境动态，确保评估过程的一致性和可复现性。

Result: 论文提出了一个标准化智能体评估的提案，但尚未展示具体的实验结果。该提案旨在解决当前评估中的混杂因素问题，为领域提供公平、透明、可复现的评估标准。

Conclusion: 统一的评估框架对于智能体评估的严谨发展至关重要。通过标准化评估流程，可以更准确地评估智能体性能，促进该领域的科学进步。

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [35] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: Accordion-Thinking框架让LLM学会动态调整推理步骤粒度，通过周期性总结思维过程来减少对历史token的依赖，实现3倍推理吞吐量提升且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的长链思维推理方法虽然能显著提升推理能力，但面临KV缓存线性增长和注意力复杂度二次方增长的实践限制，需要更高效的推理机制。

Method: 提出Accordion-Thinking端到端框架，让LLM学习通过动态总结来自我调节推理步骤粒度。采用Fold推理模式周期性总结思维过程并丢弃先前思考，减少对历史token的依赖。使用强化学习进一步激励这种能力。

Result: 训练过程中，高效的Fold模式与详尽Unfold模式之间的准确率差距逐渐缩小并最终消失。模型学会将关键推理信息编码到紧凑总结中，实现推理上下文的有效压缩。在48GB GPU内存配置下实现3倍吞吐量提升且保持准确率。

Conclusion: 通过学习的自我压缩机制，LLM能够以最小的依赖token开销处理复杂推理任务而不影响解决方案质量，同时结构化步骤总结提供了人类可读的推理过程记录。

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [36] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: CSR-Bench是一个评估多模态大语言模型跨模态可靠性的基准测试，通过四种压力测试模式（安全性、过度拒绝、偏见、幻觉）评估模型在需要图像-文本联合理解任务中的表现，发现现有模型存在系统性跨模态对齐差距。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs的安全行为可能由单模态捷径驱动而非真正的联合意图理解，需要评估模型在需要整合图像和文本信息任务中的跨模态可靠性。

Method: 引入CSR-Bench基准，包含四种压力测试交互模式（安全性、过度拒绝、偏见、幻觉），涵盖61种细粒度类型。每个实例都需要图像-文本联合解释，并提供配对纯文本对照以诊断模态引起的行为变化。评估了16个最先进的MLLMs。

Result: 观察到系统性跨模态对齐差距：模型表现出弱安全意识、在干扰下强烈的语言主导性、从纯文本对照到多模态输入的性能持续下降。还观察到减少过度拒绝与保持安全非歧视行为之间的明显权衡，表明某些表面安全增益可能来自拒绝导向的启发式方法而非鲁棒的意图理解。

Conclusion: MLLMs存在跨模态可靠性问题，当前模型的安全行为可能基于单模态捷径而非真正的多模态理解，需要开发更鲁棒的跨模态对齐方法。

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [37] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 本文提出了一种受哲学和语言学启发的计算论证新方法，引入了结构化双极论证框架（SBAFs），允许基于怀疑拒绝论证，并提供语言扩展语义来指定可接受的句子集合。


<details>
  <summary>Details</summary>
Motivation: 现有计算论证方法存在两个局限：1）要求智能体必须接受所有可辩护的论证，而现实中人们可能基于怀疑理性地拒绝某些论证；2）过于关注论证层面，而有时更自然的思考方式是关注哪些具体句子或主张被接受。本文旨在将哲学和语言学中的这些洞见融入计算论证框架。

Method: 首先定义结构化双极论证框架（SBAFs），其中论证由句子组成，包含攻击和支持两种关系。然后为SBAFs提供新的语义学，具有两个特点：1）不同于基于完备性的语义，不强制智能体接受所有可辩护的论证；2）除了论证扩展（可接受的论证集合），还提供语言扩展语义来指定可接受的句子集合。这些语义位于抽象论证的可接受语义和完备语义之间。

Result: 提出的语义学能够表示智能体在辩论中可能持有的合理立场。该方法为现有方法提供了新视角：1）可以指定智能体在何种条件下可以忽略论证间的支持关系（即使用抽象论证的合理性条件）；2）证明演绎支持语义是本文方法的一个特例。

Conclusion: 本文开发了一种更符合人类实际推理模式的计算论证方法，允许基于怀疑的理性拒绝，并提供了句子层面的接受标准。该方法在抽象论证的可接受语义和完备语义之间找到了平衡点，能够更好地模拟真实辩论中的理性立场。

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [38] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora提出了一种平衡抽象与具体细节的谐波记忆表示方法，通过主要抽象索引具体记忆值，并通过线索锚点扩展检索访问，在记忆扩展时保持检索相关性和推理有效性。


<details>
  <summary>Details</summary>
Motivation: 智能体记忆系统需要处理持续增长的信息，同时支持高效、上下文感知的检索。抽象对于扩展记忆至关重要，但通常会牺牲具体细节，而这些细节对于有效推理是必需的。

Method: 引入Memora谐波记忆表示：1）主要抽象索引具体记忆值并将相关更新整合为统一记忆条目；2）线索锚点扩展跨记忆不同方面的检索访问并连接相关记忆；3）检索策略主动利用这些记忆连接来检索超出直接语义相似性的相关信息。

Result: 理论上证明标准检索增强生成（RAG）和基于知识图谱（KG）的记忆系统是Memora框架的特例。在LoCoMo和LongMemEval基准测试中建立了新的最先进水平，在记忆扩展时表现出更好的检索相关性和推理有效性。

Conclusion: Memora通过谐波记忆表示结构性地平衡了抽象与具体性，为智能体记忆系统提供了可扩展且有效的解决方案，在记忆扩展时保持高质量的检索和推理能力。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [39] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: MentalDx Bench是首个基于真实临床电子病历的精神障碍诊断基准，包含712份病历和76种ICD-11疾病。研究发现LLMs在粗分类表现良好但细粒度诊断失败，提出了通过监督轨迹构建和课程强化学习训练的MentalSeek-Dx模型，在14B参数下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在精神健康评估中面临两个主要问题：1) 现有基准缺乏生态效度（真实临床环境）；2) 缺乏细粒度的诊断监督。这严重限制了LLMs在临床实践中的实用性。

Method: 1) 构建MentalDx Bench基准：包含712份去识别化电子健康记录，由认证精神科医生按照ICD-11标准标注，覆盖16个诊断类别和76种疾病；2) 提出MentalSeek-Dx模型：通过监督轨迹构建让模型学习临床推理过程，采用课程式强化学习进行训练。

Result: 评估18个LLMs发现"范式错位"现象：在粗粒度诊断分类上表现良好，但在细粒度疾病级别诊断上系统性失败。MentalSeek-Dx在MentalDx Bench上达到最先进性能，且仅需14B参数。

Conclusion: 该研究填补了LLMs在精神科诊断中的临床实用性空白，提出了首个真实临床环境下的诊断基准和专门化的医疗LLM，为可靠的精神科诊断建立了临床基础框架。

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [40] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: RAI是一种轻量级、无需训练的安全校准框架，通过放大VLM中的不安全信号来恢复LLM式的风险识别能力，有效抵御多模态越狱攻击


<details>
  <summary>Details</summary>
Motivation: 现有VLM安全防御方法依赖安全微调或激进的token操作，成本高且会显著降低模型效用。研究发现LLM本身能识别文本不安全内容，但VLM中视觉输入会稀释风险信号

Method: RAI从语言嵌入构建不安全原型子空间，对选定的高风险视觉token进行针对性调制，在跨模态特征空间中显式激活安全关键信号，同时保持原始token的语义完整性

Result: 在多个越狱攻击和效用基准测试中，RAI显著降低了攻击成功率，同时不损害任务性能

Conclusion: RAI提供了一种轻量级、无需训练的安全校准方法，通过恢复VLM的LLM式风险识别能力，有效平衡安全防御与模型效用

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [41] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: 本文提出了一种直觉模糊偏好冲突分析模型，通过引入直觉模糊集来更精细地描述智能体对问题对的态度，超越了传统三值关系模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于偏好的冲突分析模型仅使用偏好、相反和无差异三种定性关系来描述智能体对问题对的态度，这种粗糙的表示方式严重限制了捕捉冲突本质的能力。

Method: 引入直觉模糊偏好冲突情境概念，建立直觉模糊偏好冲突度量框架，构建三支冲突分析模型来三分智能体对集合、智能体集合和问题集合，并基于相对损失函数计算三支决策阈值。

Result: 提出了同时考虑调整幅度和冲突程度的基于调整机制的可行策略构建算法，并通过示例验证了所提模型的有效性。

Conclusion: 直觉模糊偏好冲突分析模型能够更精细地描述智能体态度，提供更有效的冲突分析工具，为复杂冲突情境下的决策支持提供了新方法。

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [42] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: DiscoverLLM是一个训练大语言模型帮助用户发现和形成意图的框架，通过模拟用户认知状态和意图层次，让模型学会在意图不明确时探索选项，在意图明确时细化实现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理模糊和开放式请求时，通常只是询问用户澄清问题，但用户往往自己也不知道想要什么，需要观察和探索结果来发现意图。现有方法在用户尚未形成意图时效果有限。

Method: 提出DiscoverLLM框架，核心是新颖的用户模拟器，用层次化的意图结构建模认知状态，意图随着模型提供相关选项而逐步具体化。具体化程度作为奖励信号，训练模型优化这一过程。模型学会在意图不明确时发散探索，在意图具体化时收敛细化。

Result: 在创意写作、技术写作和SVG绘图等交互基准测试中，DiscoverLLM实现了超过10%的任务性能提升，同时将对话长度减少了40%。在75名人类参与者的用户研究中，相比基线方法，DiscoverLLM提高了对话满意度和效率。

Conclusion: DiscoverLLM提供了一个通用框架，训练大语言模型帮助用户发现和形成意图，通过建模用户认知状态和意图层次，使模型能够更有效地与用户协作，在意图探索和实现之间取得平衡。

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [43] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: 论文提出将本体编译为工具接口的机制，使LLM能够基于形式化领域知识创建和修改知识图谱，通过语义约束指导生成过程而非事后验证。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将大型语言模型与形式化领域知识相结合，解决LLM在生成结构化知识时缺乏语义约束的问题，减少手动模式设计和提示工程的工作量。

Method: 采用本体到工具的编译机制，将本体规范编译为可执行工具接口；基于The World Avatar框架，使用Model Context Protocol和智能体将本体转换为本体感知工具，迭代应用于从非结构化科学文本中提取、验证和修复结构化知识。

Result: 以金属有机多面体合成文献为例，展示了可执行本体语义如何指导LLM行为，减少手动模式设计和提示工程，建立了将形式化知识嵌入生成系统的通用范式。

Conclusion: 提出了一个将形式化领域知识与LLM耦合的通用范式，通过本体到工具的编译机制实现语义约束的生成，为知识图谱生态系统中的结构化交互提供了可行方案。

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [44] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA是一个用于视觉语言动作模型持续强化学习的框架，通过理论推导和双评论家架构解决稳定性与可塑性权衡问题，在LIBERO基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，终身学习对具身智能体至关重要。持续强化学习是将VLA模型部署到终身机器人场景中的有前景途径，但现有方法在平衡稳定性（保留旧技能）和可塑性（学习新技能）方面面临重大挑战。

Method: 提出CRL-VLA框架，通过理论推导获得统一性能边界，将稳定性-可塑性权衡与目标条件优势幅度和策略散度联系起来。采用非对称调节方法：约束先前任务的优势幅度，同时允许新任务上的受控增长。实现方式是通过具有新颖目标条件价值公式的双评论家架构，其中冻结评论家锚定语义一致性，可训练估计器驱动适应。

Result: 在LIBERO基准测试上的实验表明，CRL-VLA有效协调了这些冲突目标，在抗遗忘和向前适应方面均优于基线方法。

Conclusion: CRL-VLA为VLA模型的持续后训练提供了一个具有严格理论边界的框架，成功解决了终身学习中稳定性与可塑性的权衡问题，为部署VLA模型到终身机器人场景提供了有前景的解决方案。

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [45] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: 研究形式化抽象（移除和聚类）如何影响人类推理性能和认知负荷，使用答案集编程作为形式框架，通过认知实验验证抽象化能提升人类对符号解释的理解


<details>
  <summary>Details</summary>
Motivation: 解释对人类认知至关重要，但AI系统产生的输出往往难以理解。虽然符号AI为可解释性提供了透明基础，但原始逻辑追踪通常带来较高的外在认知负荷。研究者希望探索形式化抽象如何影响人类推理和认知努力

Method: 使用答案集编程（ASP）作为形式框架，定义需要抽象化的无关细节概念，通过认知实验让参与者在不同领域中对刺激进行分类，使用从答案集程序派生的解释，比较聚类细节和移除细节两种抽象方法的效果

Result: 认知实验显示：聚类细节显著提高参与者的理解能力，而移除细节则显著降低认知努力，支持了抽象化能增强以人为中心的符号解释的假设

Conclusion: 形式化抽象（特别是聚类和移除细节）能有效提升人类对符号AI解释的理解并减少认知负荷，为设计更人性化的可解释AI系统提供了实证支持

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [46] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: 论文发现现有LLM路由系统存在"路由崩溃"现象：随着成本预算增加，路由器会系统性地选择最强大但最昂贵的模型，即使更便宜的模型已经足够，导致小模型利用率不足和成本浪费。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由系统存在一个普遍但未被充分探索的失败模式：随着用户成本预算增加，路由器会系统性地默认选择最强大、最昂贵的模型，即使更便宜的模型已经足够。这种现象被称为"路由崩溃"，导致小模型利用率不足，浪费计算资源和金钱成本，违背了路由的核心承诺。

Method: 作者提出了EquiRouter，一种决策感知的路由器，直接学习模型排名而不是预测标量性能分数。这种方法旨在弥合目标-决策不匹配：现有路由器训练预测标量性能分数，而路由决策最终依赖于候选模型之间的离散比较。

Result: 在RouterBench基准测试中，EquiRouter在达到GPT-4级别性能的情况下，相比之前最强的路由器减少了约17%的成本。

Conclusion: 路由崩溃是现有LLM路由系统的一个普遍问题，源于目标-决策不匹配。EquiRouter通过直接学习模型排名来解决这个问题，有效恢复了小模型的作用并缓解了路由崩溃现象，在保持高质量的同时显著降低了成本。

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [47] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: 研究AI使用对人类文化演化的长期影响，发现替代型AI用户（AI-substitute）在个体选择中占优但会减少文化多样性，而互补型AI用户（AI-complement）能维持群体文化多样性，在强群体边界条件下可通过文化群体选择被保留。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的依赖可能降低文化多样性和变异，这已经导致模型性能问题（如模型崩溃和幻觉）。研究旨在探讨AI使用对人类文化演化的长期后果，以及广泛使用AI可能导致"文化崩溃"的条件——即AI生成内容减少人类变异和创新，减缓累积文化演化。

Method: 使用基于主体的建模和演化博弈论，比较两种AI使用类型：互补型（AI-complement）用户寻求建议和指导但仍是最终输出的主要生产者；替代型（AI-substitute）用户提供最小输入，依赖AI产生大部分输出。研究这些使用策略在演化动态下的竞争和传播。

Result: 替代型AI用户在个体层面选择中占优势，尽管会更强地减少文化变异。相比之下，互补型AI用户通过维持探索所需的变异而有益于群体，因此在群体边界强时可通过文化群体选择被青睐。

Conclusion: 研究结果揭示了AI采用对人口层面的长期影响，为缓解这些风险的政策和组织策略提供信息。互补型AI使用在维持文化多样性方面具有群体优势，而替代型使用在个体层面更易传播但可能导致文化崩溃。

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [48] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: LLMs在静态医疗推理任务表现良好，但作为动态医疗世界模型模拟疾病进展和治疗结果时，难以保持一致的病人状态，导致长期模拟误差累积。EHRWorld通过因果序列范式训练，使用真实电子健康记录数据，显著优于基于LLM的基线方法。


<details>
  <summary>Details</summary>
Motivation: 世界模型为干预下的未来状态模拟提供了原则性框架，但在复杂高风险医疗领域实现仍具挑战。尽管LLMs在静态医疗推理任务上表现优异，但能否作为动态医疗世界模型模拟疾病进展和治疗结果尚不明确。

Method: 提出EHRWorld，一种以病人为中心的医疗世界模型，采用因果序列范式训练。同时构建EHRWorld-110K，一个基于真实世界电子健康记录的大规模纵向临床数据集。

Result: EHRWorld显著优于基于LLM的基线方法，实现了更稳定的长期模拟、对临床敏感事件的改进建模，以及更优的推理效率。证明了在因果基础、时间演化的临床数据上训练的必要性。

Conclusion: 可靠的医疗世界建模需要在因果基础、时间演化的临床数据上进行专门训练，而仅依赖医学知识的LLMs无法胜任动态医疗世界模型的任务。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [49] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: 该论文研究了将少样本学习的大语言模型转化为多轮对话智能体时出现的"对话惯性"问题，即模型倾向于模仿自己之前的回答，限制了探索能力。作者提出了基于上下文偏好学习的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为少样本学习者在提供适当示例时表现出色，但在多轮智能体场景中，这种优势变成了问题：模型错误地将自己之前的回答作为少样本示例进行模仿，导致探索能力受限。

Method: 通过注意力分析识别对话惯性现象，发现相同状态下，长上下文生成的动作比短上下文具有更强的惯性。基于此构建无需环境奖励的偏好对，提出上下文偏好学习来校准模型偏好，使其更倾向于低惯性响应而非高惯性响应。同时提供推理时的上下文管理策略来平衡探索与利用。

Result: 在八个智能体环境和一个深度研究场景中的实验结果表明，该框架有效减少了对话惯性并实现了性能提升。

Conclusion: 研究揭示了将少样本LLMs转化为智能体时的内在张力：长上下文虽然丰富了环境反馈以支持利用，但也放大了对话惯性从而削弱探索。提出的上下文偏好学习框架为解决这一矛盾提供了有效途径。

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [50] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm：一种面向任务的动态通信算法，通过策略梯度优化生成适应每轮动态的行为驱动协作拓扑，在动态对手和通信预算下实现更好的任务效果。


<details>
  <summary>Details</summary>
Motivation: 现有多轮LLM多智能体系统通常采用固定的通信拓扑，但在实际应用中智能体角色可能因动态对手、任务进展或时变约束（如通信带宽）而跨轮变化，固定拓扑无法适应这种动态性。

Method: 提出TodyComm算法，通过策略梯度优化生成行为驱动的协作拓扑，使通信结构能够适应每轮的动态变化，从而优化任务效用。

Result: 在五个基准测试上的实验表明，在动态对手和通信预算约束下，TodyComm在任务效果上表现优越，同时保持了令牌效率和可扩展性。

Conclusion: TodyComm通过动态调整通信拓扑有效解决了多轮LLM多智能体系统中角色变化的挑战，为动态环境下的智能体协作提供了更灵活的解决方案。

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [51] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra是一个统一的、框架无关的智能体系统，通过将智能体抽象为(指令、上下文、工具、模型)四元组，实现按需动态创建专用执行器，在复杂任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体系统缺乏对子智能体的动态抽象视图，限制了系统的适应性和灵活性。需要一种统一的、框架无关的智能体抽象方法来提升复杂、长视野任务的自动化能力。

Method: 提出统一的智能体抽象：将任何智能体建模为(指令、上下文、工具、模型)四元组。基于此构建AOrchestra系统，中央编排器在每一步具体化该四元组：策划任务相关上下文、选择工具和模型，并通过即时自动创建智能体来委托执行。

Result: 在三个具有挑战性的基准测试(GAIA、SWE-Bench、Terminal-Bench)中，AOrchestra与Gemini-3-Flash配对时，相比最强基线实现了16.28%的相对改进。系统支持可控的性能-成本权衡，能够接近帕累托效率。

Conclusion: AOrchestra通过统一的智能体抽象和动态编排机制，显著提升了语言智能体在复杂任务中的适应性和性能，同时减少了人工工程工作量，并保持框架无关性。

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [52] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: 研究发现：在多智能体系统中，单纯增加同质智能体数量存在收益递减，而异质性（不同模型、提示词或工具）能持续带来显著性能提升。信息论分析表明系统性能受限于任务内在不确定性而非智能体数量，异质性智能体通过提供互补证据突破这一限制。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统在处理复杂任务方面展现出潜力，但研究发现单纯增加同质智能体数量存在强烈的收益递减现象，而异质性配置却能持续带来性能提升。这引发了一个根本性问题：什么限制了规模扩展？为什么多样性有帮助？

Method: 提出了一个信息论框架，表明多智能体系统性能受限于任务内在不确定性而非智能体数量。推导出架构无关的界限，证明改进取决于系统访问的有效通道数量。引入K*（有效通道计数）来量化无真实标签情况下的有效通道数量。通过实证研究比较同质和异质配置的性能。

Result: 实证研究表明异质配置始终优于同质扩展：2个异质智能体的性能可以匹配甚至超过16个同质智能体。异质性智能体通过提供互补证据，突破了同质智能体因输出高度相关而早期饱和的限制。

Conclusion: 研究结果为通过多样性感知设计构建高效、稳健的多智能体系统提供了原则性指导。异质性（不同模型、提示词或工具）是提升多智能体系统性能的关键因素，而非单纯增加同质智能体数量。

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [53] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 该论文提出了一种基于风险控制的LLM推理预算设置框架，通过上下阈值机制自适应控制计算开销，在保证错误率不超目标风险的前提下最小化计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型支持测试时扩展，准确率随token预算增加而提升，但实际应用中设置token预算和自适应推理阈值存在挑战，需要在风险和准确性之间权衡。传统方法难以平衡计算效率和可靠性。

Method: 提出风险控制框架：1) 上阈值机制在模型置信度高时停止推理（可能输出错误结果）；2) 新颖的参数化下阈值机制提前停止无法解决的实例（可能过早停止）。使用无分布风险控制方法，基于目标风险和验证集优化设置这些停止机制。对于多预算控制标准场景，引入效率损失函数选择计算效率最高的退出机制。

Result: 在多种推理任务和模型上的实证结果表明，该风险控制方法有效实现了计算效率提升，下阈值机制和集成停止机制在遵守用户指定风险目标的同时显著减少了计算开销。

Conclusion: 该研究提出了一种实用的风险控制框架，能够自适应地管理LLM推理的计算预算，在保证错误率不超过目标风险的前提下最小化计算成本，为实际部署中的计算效率与可靠性权衡提供了系统解决方案。

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [54] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench是首个大规模科学插图生成基准，包含3300个高质量文本-插图对；AutoFigure是首个基于长文本自动生成高质量科学插图的智能体框架，通过思考、重组和验证实现结构完整且美观的插图。


<details>
  <summary>Details</summary>
Motivation: 高质量科学插图对于有效传达复杂科技概念至关重要，但其手动制作在学术界和工业界都是公认的瓶颈。现有方法缺乏针对科学插图生成的大规模基准和系统框架。

Method: 提出FigureBench基准，包含3300个高质量科学文本-插图对，涵盖论文、综述、博客和教材等多种来源。提出AutoFigure智能体框架，在最终渲染前进行广泛思考、重组和验证，生成结构完整且美观的科学插图。

Result: 基于FigureBench的高质量数据进行了广泛实验，测试AutoFigure与多种基线方法的性能。结果表明AutoFigure始终优于所有基线方法，能够生成可直接用于发表的科学插图。

Conclusion: FigureBench为科学插图生成提供了首个大规模基准，AutoFigure框架通过智能体方法有效解决了科学插图自动生成的挑战，代码、数据集和演示平台已开源。

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [55] [Position: 3D Gaussian Splatting Watermarking Should Be Scenario-Driven and Threat-Model Explicit](https://arxiv.org/abs/2602.02602)
*Yangfan Deng,Anirudh Nakra,Min Wu*

Main category: cs.CR

TL;DR: 该论文提出需要为3D资产水印制定明确的安全目标和威胁模型，借鉴数字音视频资产保护经验，建立基于场景的安全框架来评估现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着3D内容获取和创建的快速发展，特别是3D高斯泼溅等技术的出现，3D资产的版权保护需求日益增长。由于3D参数化表示易于编辑和传播，未经授权的使用变得更加容易，因此需要有效的知识产权保护机制。

Method: 采用基于场景的安全模型制定方法，将对抗能力形式化，构建参考框架来组织和评估现有水印方法。同时分析了传统的扩频嵌入方案，阐明其优缺点和设计选择与对抗假设的映射关系。

Result: 提出了一个系统化的安全框架，能够清晰地组织现有3D资产水印方法，并揭示特定设计选择与相应对抗假设之间的关系。通过分析传统扩频嵌入方案，展示了该框架的实际应用价值。

Conclusion: 该工作旨在促进3D资产的有效知识产权保护，强调需要借鉴数字音视频保护的经验教训，采用明确的安全目标和现实威胁模型来指导3D水印技术的发展。

Abstract: 3D content acquisition and creation are expanding rapidly in the new era of machine learning and AI. 3D Gaussian Splatting (3DGS) has become a promising high-fidelity and real-time representation for 3D content. Similar to the initial wave of digital audio-visual content at the turn of the millennium, the demand for intellectual property protection is also increasing, since explicit and editable 3D parameterization makes unauthorized use and dissemination easier. In this position paper, we argue that effective progress in watermarking 3D assets requires articulated security objectives and realistic threat models, incorporating the lessons learned from digital audio-visual asset protection over the past decades. To address this gap in security specification and evaluation, we advocate a scenario-driven formulation, in which adversarial capabilities are formalized through a security model. Based on this formulation, we construct a reference framework that organizes existing methods and clarifies how specific design choices map to corresponding adversarial assumptions. Within this framework, we also examine a legacy spread-spectrum embedding scheme, characterizing its advantages and limitations and highlighting the important trade-offs it entails. Overall, this work aims to foster effective intellectual property protection for 3D assets.

</details>


### [56] [ClinConNet: A Blockchain-based Dynamic Consent Management Platform for Clinical Research](https://arxiv.org/abs/2602.02610)
*Montassar Naghmouchi,Maryline Laurent*

Main category: cs.CR

TL;DR: 本文提出ClinConNet平台，基于区块链和动态同意模型，为临床研究提供用户中心化的同意管理系统，解决传统同意管理技术落后、数据孤岛等问题。


<details>
  <summary>Details</summary>
Motivation: 临床研究和医疗中的知情同意是伦理基石，但现有技术基础设施落后。临床医生负责获取和管理知情同意，负担沉重。现有系统缺乏参与者中心化设计，形成数据孤岛，影响患者数据流动和个性化医疗。同意管理工具过时，需要现代化解决方案。

Method: 提出ClinConNet平台，基于区块链和自主权身份系统，采用动态同意模型。平台连接研究人员和参与者，提供用户中心化设计，具备不可链接性、机密性、身份数据所有权等隐私特性，并兼容GDPR等数据保护法规中的被遗忘权。

Result: 通过对抗模型进行详细的隐私和安全分析，并提供概念验证实现。性能测试显示中位端到端同意建立时间低于200毫秒，吞吐量达到250TPS，证明基于区块链的同意管理系统的可行性。

Conclusion: ClinConNet平台通过区块链和动态同意模型，为临床研究提供了现代化、用户中心化的同意管理解决方案，解决了传统系统的局限性，同时满足隐私保护和法规要求。

Abstract: Consent is an ethical cornerstone of clinical research and healthcare in general. Although the ethical principles of consent - providing information, ensuring comprehension, and ensuring voluntariness - are well-defined, the technological infrastructure remains outdated. Clinicians are responsible for obtaining informed consent from research subjects or patients, and for managing it before, during, and after clinical trials or care, which is a burden for them. The voluntary nature of participating in clinical research or undergoing medical treatment implies the need for a participant-centric consent management system. However, this is not reflected in most established systems. Not only do most healthcare information systems not follow a user-centric model, but they also create data silos, which significantly reduce the mobility of patient data between different healthcare institutions and impact personalized medicine. Furthermore, consent management tools are outdated. We propose ClinConNet (Clinical Consent Network), a platform that connects researchers and participants based on clinical research projects. ClinConNet is powered by a dynamic consent model based on blockchain and take advantage of dynamic consent interfaces, as well as blockchain and Self-Sovereign Identity systems. ClinConNet is user-centric and provides important privacy features for patients, such as unlinkability, confidentiality, and ownership of identity data. It is also compatible with the right to be forgotten, as defined in many personal data protection regulations, such as the GDPR. We provide a detailed privacy and security analysis in an adversarial model, as well as a Proof of Concept implementation with detailed performance measures that demonstrate the feasibility of our blockchain-based consent management system with a median end-to-end consent establishment time of under 200ms and a throughput of 250TPS.

</details>


### [57] [TinyGuard:A lightweight Byzantine Defense for Resource-Constrained Federated Learning via Statistical Update Fingerprints](https://arxiv.org/abs/2602.02615)
*Ali Mahdavi,Santa Aghapour,Azadeh Zamanifar,Amirfarhad Farhadi*

Main category: cs.CR

TL;DR: TinyGuard：一种轻量级拜占庭防御机制，通过统计更新指纹在低维空间检测恶意客户端，保持FedAvg收敛性，计算复杂度为O(nd)，适用于大规模联邦学习系统。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒聚合机制通常依赖全维度梯度比较或成对距离计算，计算开销大，限制了在大规模和资源受限的联邦系统中的适用性。

Method: 提出TinyGuard，通过提取紧凑的统计指纹（包括范数统计、层间比率、稀疏性度量和低阶矩）捕获客户端更新的关键行为特性，在低维指纹空间测量统计偏差来识别拜占庭客户端，不修改底层优化过程。

Result: 在MNIST、Fashion-MNIST、ViT-Lite和ViT-Small等数据集上实验表明，TinyGuard在良性设置下保持FedAvg收敛性，在多种拜占庭攻击场景下达到95%准确率；对抗自适应白盒攻击时，攻击者无法同时逃避检测和实现有效投毒；消融研究显示检测精度稳定在0.8。

Conclusion: TinyGuard是一种架构无关的轻量级拜占庭防御框架，特别适用于传统拜占庭防御不切实际的联邦基础模型微调场景，计算效率高且鲁棒性强。

Abstract: Existing Byzantine robust aggregation mechanisms typically rely on fulldimensional gradi ent comparisons or pairwise distance computations, resulting in computational overhead that limits applicability in large scale and resource constrained federated systems. This paper proposes TinyGuard, a lightweight Byzantine defense that augments the standard FedAvg algorithm via statistical update f ingerprinting. Instead of operating directly on high-dimensional gradients, TinyGuard extracts compact statistical fingerprints cap turing key behavioral properties of client updates, including norm statistics, layer-wise ratios, sparsity measures, and low-order mo ments. Byzantine clients are identified by measuring robust sta tistical deviations in this low-dimensional fingerprint space with nd complexity, without modifying the underlying optimization procedure. Extensive experiments on MNIST, Fashion-MNIST, ViT-Lite, and ViT-Small with LoRA adapters demonstrate that TinyGuard pre serves FedAvg convergence in benign settings and achieves up to 95 percent accuracy under multiple Byzantine attack scenarios, including sign-flipping, scaling, noise injection, and label poisoning. Against adaptive white-box adversaries, Pareto frontier analysis across four orders of magnitude confirms that attackers cannot simultaneously evade detection and achieve effective poisoning, features we term statistical handcuffs. Ablation studies validate stable detection precision 0.8 across varying client counts (50-150), threshold parameters and extreme data heterogeneity . The proposed framework is architecture-agnostic and well-suited for federated fine-tuning of foundation models where traditional Byzantine defenses become impractical

</details>


### [58] [Trustworthy Blockchain-based Federated Learning for Electronic Health Records: Securing Participant Identity with Decentralized Identifiers and Verifiable Credentials](https://arxiv.org/abs/2602.02629)
*Rodrigo Tertulino,Ricardo Almeida,Laercio Alencar*

Main category: cs.CR

TL;DR: 本文提出了一种基于区块链的可信联邦学习框架，通过自我主权身份技术确保医疗数据协作的安全性，有效防御Sybil攻击，同时保持临床实用性。


<details>
  <summary>Details</summary>
Motivation: 医疗数字化产生了大量电子健康记录，但严格的隐私法规导致数据孤岛问题。联邦学习虽然能实现协作训练而不共享原始数据，但仍面临投毒攻击和Sybil攻击的威胁。现有基于区块链的方法主要依赖概率性声誉系统，缺乏强大的密码学身份验证机制。

Method: 提出可信区块链联邦学习框架，整合自我主权身份标准，利用去中心化标识符和可验证凭证技术，确保只有经过身份验证的医疗实体才能参与全局模型训练。

Result: 使用MIMIC-IV数据集评估显示：框架成功防御100%的Sybil攻击；实现稳健的预测性能（AUC=0.954，召回率=0.890）；引入的计算开销可忽略不计（<0.12%）；多机构100轮训练的总运营成本约18美元。

Conclusion: 该框架通过将信任锚定在密码学身份验证而非行为模式上，显著降低了安全风险，同时保持了临床实用性，为跨机构医疗数据协作提供了一个安全、可扩展且经济可行的生态系统。

Abstract: The digitization of healthcare has generated massive volumes of Electronic Health Records (EHRs), offering unprecedented opportunities for training Artificial Intelligence (AI) models. However, stringent privacy regulations such as GDPR and HIPAA have created data silos that prevent centralized training. Federated Learning (FL) has emerged as a promising solution that enables collaborative model training without sharing raw patient data. Despite its potential, FL remains vulnerable to poisoning and Sybil attacks, in which malicious participants corrupt the global model or infiltrate the network using fake identities. While recent approaches integrate Blockchain technology for auditability, they predominantly rely on probabilistic reputation systems rather than robust cryptographic identity verification. This paper proposes a Trustworthy Blockchain-based Federated Learning (TBFL) framework integrating Self-Sovereign Identity (SSI) standards. By leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), our architecture ensures only authenticated healthcare entities contribute to the global model. Through comprehensive evaluation using the MIMIC-IV dataset, we demonstrate that anchoring trust in cryptographic identity verification rather than behavioral patterns significantly mitigates security risks while maintaining clinical utility. Our results show the framework successfully neutralizes 100% of Sybil attacks, achieves robust predictive performance (AUC = 0.954, Recall = 0.890), and introduces negligible computational overhead (<0.12%). The approach provides a secure, scalable, and economically viable ecosystem for inter-institutional health data collaboration, with total operational costs of approximately $18 for 100 training rounds across multiple institutions.

</details>


### [59] [On the Feasibility of Hybrid Homomorphic Encryption for Intelligent Transportation Systems](https://arxiv.org/abs/2602.02717)
*Kyle Yates,Abdullah Al Mamun,Mashrur Chowdhury*

Main category: cs.CR

TL;DR: HHE显著降低智能交通系统中的密文大小和通信开销，相比传统HE更适合时延敏感的应用场景。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要强大的隐私保护，但传统同态加密存在密文膨胀和通信开销大的问题，限制了其在时间关键型交通系统中的适用性。

Method: 开发了集成HHE的代表性ITS应用理论模型，并对Rubato HHE方案进行基于参数的评估，估算实际ITS工作负载下的密文大小和通信开销。

Result: HHE相比传统HE实现了数量级的密文大小减少，同时保持密码学安全性，使其在时延受限的ITS通信中更加实用。

Conclusion: 混合同态加密通过结合同态加密方案和对称密码，在保护敏感车辆数据的同时显著降低通信成本，为智能交通系统提供了更实用的隐私保护解决方案。

Abstract: Many Intelligent Transportation Systems (ITS) applications require strong privacy guarantees for both users and their data. Homomorphic encryption (HE) enables computation directly on encrypted messages and thus offers a compelling approach to privacy-preserving data processing in ITS. However, practical HE schemes incur substantial ciphertext expansion and communication overhead, which limits their suitability for time-critical transportation systems. Hybrid homomorphic encryption (HHE) addresses this challenge by combining a homomorphic encryption scheme with a symmetric cipher, enabling efficient encrypted computation while dramatically reducing communication cost. In this paper, we develop theoretical models of representative ITS applications that integrate HHE to protect sensitive vehicular data. We then perform a parameter-based evaluation of the HHE scheme Rubato to estimate ciphertext sizes and communication overhead under realistic ITS workloads. Our results show that HHE achieves orders-of-magnitude reductions in ciphertext size compared with conventional HE while maintaining cryptographic security, making it significantly more practical for latency-constrained ITS communication.

</details>


### [60] [Composition for Pufferfish Privacy](https://arxiv.org/abs/2602.02718)
*Jiamu Bai,Guanlin He,Xin Gu,Daniel Kifer,Kiwan Maeng*

Main category: cs.CR

TL;DR: 本文研究了Pufferfish隐私定义在组合性方面的问题，提出了确保线性组合的必要充分条件，并通过(a,b)-影响曲线将差分隐私机制转化为可组合的Pufferfish算法。


<details>
  <summary>Details</summary>
Motivation: Pufferfish等基于推断/后验的隐私定义为相关数据提供了有吸引力的隐私语义，但由于缺乏组合性而很少在实践中使用。现有算法可能在单次运行时无泄漏，但在多次运行时可能泄露整个数据集。

Method: 提出了确保Pufferfish机制线性组合的必要充分条件，这些条件表现为差分隐私风格的不等式。通过引入(a,b)-影响曲线概念，将现有差分隐私算法转化为可组合的Pufferfish算法。

Result: 证明了实现Pufferfish的可解释语义和组合性需要采用差分隐私机制。为马尔可夫链设计了可组合的Pufferfish算法，性能显著优于先前工作。

Conclusion: 通过将差分隐私机制与Pufferfish框架结合，解决了相关数据隐私保护中的组合性问题，为实际应用提供了可行的解决方案。

Abstract: When creating public data products out of confidential datasets, inferential/posterior-based privacy definitions, such as Pufferfish, provide compelling privacy semantics for data with correlations. However, such privacy definitions are rarely used in practice because they do not always compose. For example, it is possible to design algorithms for these privacy definitions that have no leakage when run once but reveal the entire dataset when run more than once. We prove necessary and sufficient conditions that must be added to ensure linear composition for Pufferfish mechanisms, hence avoiding such privacy collapse. These extra conditions turn out to be differential privacy-style inequalities, indicating that achieving both the interpretable semantics of Pufferfish for correlated data and composition benefits requires adopting differentially private mechanisms to Pufferfish. We show that such translation is possible through a concept called the $(a,b)$-influence curve, and many existing differentially private algorithms can be translated with our framework into a composable Pufferfish algorithm. We illustrate the benefit of our new framework by designing composable Pufferfish algorithms for Markov chains that significantly outperform prior work.

</details>


### [61] [CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability](https://arxiv.org/abs/2602.03012)
*Xianzhen Luo,Jingyuan Zhang,Shiqi Zhou,Rain Huang,Chuan Xiao,Qingfu Zhu,Zhiyuan Ma,Xing Yue,Yang Yue,Wencong Zeng,Wanxiang Che*

Main category: cs.CR

TL;DR: CVE-Factory是一个多智能体框架，能够自动将稀疏的CVE元数据转换为完全可执行的智能体任务，用于评估和改进代码智能体的安全能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵、不可扩展的手动复现，且数据分布过时，需要高质量、可执行的漏洞任务来评估和改进代码智能体的安全能力。

Method: 开发CVE-Factory多智能体框架，自动将CVE元数据转换为可执行任务；构建LiveCVEBench持续更新的基准测试集；合成大规模训练环境；微调Qwen3-32B模型。

Result: CVE-Factory达到95%解决方案正确性和96%环境保真度；在最新现实漏洞上达到66.2%验证成功率；微调后的Qwen3-32B在LiveCVEBench上从5.3%提升到35.8%，超越Claude 4.5 Sonnet。

Conclusion: CVE-Factory能够自动生成专家级质量的漏洞任务，支持构建持续更新的基准测试和大规模训练环境，显著提升代码智能体的安全能力评估和性能。

Abstract: Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .

</details>


### [62] [DF-LoGiT: Data-Free Logic-Gated Backdoor Attacks in Vision Transformers](https://arxiv.org/abs/2602.03040)
*Xiaozuo Shen,Yifei Cai,Rui Ning,Chunsheng Xin,Hongyi Wu*

Main category: cs.CR

TL;DR: DF-LoGiT是一种无需数据的逻辑门控后门攻击方法，通过直接权重编辑在Vision Transformers中植入后门，利用多头架构实现组合触发器，攻击成功率接近100%且不影响正常精度。


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformers的广泛采用，第三方模型库存在供应链风险，攻击者可能在发布的检查点中植入后门。现有ViT后门攻击主要依赖中毒数据训练，而先前无需数据的尝试通常需要合成数据微调或额外模型组件。

Method: 提出DF-LoGiT方法，这是一种真正无需数据的ViT后门攻击，通过直接权重编辑实现。利用ViT原生多头架构实现逻辑门控组合触发器，使后门更加隐蔽有效。

Result: DF-LoGiT在实验中达到接近100%的攻击成功率，对良性精度影响可忽略，并且能够抵抗经典和ViT特定的防御方法。

Conclusion: DF-LoGiT展示了无需数据即可在Vision Transformers中植入高效后门的可行性，揭示了ViT模型供应链安全的新风险。

Abstract: The widespread adoption of Vision Transformers (ViTs) elevates supply-chain risk on third-party model hubs, where an adversary can implant backdoors into released checkpoints. Existing ViT backdoor attacks largely rely on poisoned-data training, while prior data-free attempts typically require synthetic-data fine-tuning or extra model components. This paper introduces Data-Free Logic-Gated Backdoor Attacks (DF-LoGiT), a truly data-free backdoor attack on ViTs via direct weight editing. DF-LoGiT exploits ViT's native multi-head architecture to realize a logic-gated compositional trigger, enabling a stealthy and effective backdoor. We validate its effectiveness through theoretical analysis and extensive experiments, showing that DF-LoGiT achieves near-100% attack success with negligible degradation in benign accuracy and remains robust against representative classical and ViT-specific defenses.

</details>


### [63] [The Trigger in the Haystack: Extracting and Reconstructing LLM Backdoor Triggers](https://arxiv.org/abs/2602.03085)
*Blake Bullwinkel,Giorgio Severi,Keegan Hines,Amanda Minnich,Ram Shankar Siva Kumar,Yonatan Zunger*

Main category: cs.CR

TL;DR: 提出一种实用的扫描器，用于检测因果语言模型中的sleeper agent式后门，无需事先知道触发器或目标行为，仅需推理操作。


<details>
  <summary>Details</summary>
Motivation: 检测模型是否被投毒是AI安全中长期存在的问题，需要实用的方法来识别sleeper agent式后门。

Method: 基于两个关键发现：1) sleeper agent倾向于记忆投毒数据，可通过记忆提取技术泄露后门示例；2) 当输入中存在后门触发器时，被投毒的LLM在输出分布和注意力头上表现出独特模式。开发可扩展的后门扫描方法，无需触发器或目标行为的先验知识，仅需推理操作。

Result: 该方法在多种后门场景、广泛的模型和微调方法中都能成功恢复有效的触发器。

Conclusion: 提出的扫描器可自然集成到更广泛的防御策略中，不会改变模型性能，为检测语言模型后门提供了实用解决方案。

Abstract: Detecting whether a model has been poisoned is a longstanding problem in AI security. In this work, we present a practical scanner for identifying sleeper agent-style backdoors in causal language models. Our approach relies on two key findings: first, sleeper agents tend to memorize poisoning data, making it possible to leak backdoor examples using memory extraction techniques. Second, poisoned LLMs exhibit distinctive patterns in their output distributions and attention heads when backdoor triggers are present in the input. Guided by these observations, we develop a scalable backdoor scanning methodology that assumes no prior knowledge of the trigger or target behavior and requires only inference operations. Our scanner integrates naturally into broader defensive strategies and does not alter model performance. We show that our method recovers working triggers across multiple backdoor scenarios and a broad range of models and fine-tuning methods.

</details>


### [64] [LogicScan: An LLM-driven Framework for Detecting Business Logic Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2602.03271)
*Jiaqi Gao,Zijian Zhang,Yuqiang Sun,Ye Liu,Chengwei Liu,Han Liu,Yi Li,Yang Liu*

Main category: cs.CR

TL;DR: LogicScan是一个基于对比审计的自动化框架，用于检测智能合约中的业务逻辑漏洞。它通过从大规模链上协议中挖掘业务不变量作为参考约束，结合噪声感知逻辑聚合和对比审计来识别缺失或弱执行的不变量，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 业务逻辑漏洞已成为智能合约中最具破坏性但最不被理解的漏洞类别。传统静态分析技术难以捕捉这种高级逻辑，而基于大语言模型的方法由于幻觉和有限验证导致输出不稳定和准确率低。需要一种能够有效检测业务逻辑漏洞的自动化方法。

Method: LogicScan提出对比审计框架，核心洞察是成熟、广泛部署的链上协议隐式编码了经过良好测试和共识驱动的业务不变量。系统从大规模链上合约中挖掘这些不变量，并将其作为参考约束来审计目标合约。引入业务规范语言（BSL）将多样化实现模式规范化为结构化、可验证的逻辑表示，结合噪声感知逻辑聚合与对比审计来识别缺失或弱执行的不变量，同时减轻LLM引起的误报。

Result: 在三个真实世界数据集（DeFiHacks、Web3Bugs和top-200审计合约）上评估，LogicScan达到85.2%的F1分数，显著优于最先进工具，同时在生产级合约上保持低误报率。额外实验表明LogicScan在不同LLM上保持一致的性能，具有成本效益，其误报抑制机制显著提高了鲁棒性。

Conclusion: LogicScan通过从成熟链上协议中挖掘业务不变量作为参考约束，结合规范化的业务规范语言和对比审计方法，有效解决了智能合约业务逻辑漏洞检测的挑战，在准确性和鲁棒性方面显著优于现有方法。

Abstract: Business logic vulnerabilities have become one of the most damaging yet least understood classes of smart contract vulnerabilities. Unlike traditional bugs such as reentrancy or arithmetic errors, these vulnerabilities arise from missing or incorrectly enforced business invariants and are tightly coupled with protocol semantics. Existing static analysis techniques struggle to capture such high-level logic, while recent large language model based approaches often suffer from unstable outputs and low accuracy due to hallucination and limited verification.
  In this paper, we propose LogicScan, an automated contrastive auditing framework for detecting business logic vulnerabilities in smart contracts. The key insight behind LogicScan is that mature, widely deployed on-chain protocols implicitly encode well-tested and consensus-driven business invariants. LogicScan systematically mines these invariants from large-scale on-chain contracts and reuses them as reference constraints to audit target contracts. To achieve this, LogicScan introduces a Business Specification Language (BSL) to normalize diverse implementation patterns into structured, verifiable logic representations. It further combines noise-aware logic aggregation with contrastive auditing to identify missing or weakly enforced invariants while mitigating LLM-induced false positives.
  We evaluate LogicScan on three real-world datasets, including DeFiHacks, Web3Bugs, and a set of top-200 audited contracts. The results show that LogicScan achieves an F1 score of 85.2%, significantly outperforming state-of-the-art tools while maintaining a low false-positive rate on production-grade contracts. Additional experiments demonstrate that LogicScan maintains consistent performance across different LLMs and is cost-effective, and that its false-positive suppression mechanisms substantially improve robustness.

</details>


### [65] [GuardReasoner-Omni: A Reasoning-based Multi-modal Guardrail for Text, Image, and Video](https://arxiv.org/abs/2602.03328)
*Zhenhao Zhu,Yue Liu,Yanpei Guo,Wenjie Qu,Cancan Chen,Yufei He,Yibo Li,Yulin Chen,Tianyi Wu,Huiying Xu,Xinzhong Zhu,Jiaheng Zhang*

Main category: cs.CR

TL;DR: GuardReasoner-Omni是一个基于推理的多模态护栏模型，用于文本、图像和视频内容审核，通过两阶段训练实现卓越性能


<details>
  <summary>Details</summary>
Motivation: 需要开发一个能够同时处理文本、图像和视频三种模态的护栏模型，通过推理能力提升内容审核的准确性和深度

Method: 构建包含14.8万样本的多模态训练语料库，采用两阶段训练范式：1) SFT微调以冷启动模型，获得显式推理能力和结构遵循性；2) 强化学习，引入错误驱动探索奖励，激励模型对困难样本进行深度推理

Result: 发布了2B和4B参数规模的模型套件，在多个护栏基准测试中显著优于现有最先进基线模型，GuardReasoner-Omni (2B)比第二名模型F1分数高出5.3%

Conclusion: GuardReasoner-Omni通过推理驱动的训练方法，在多模态内容审核任务中实现了卓越性能，证明了基于推理的护栏模型的有效性

Abstract: We present GuardReasoner-Omni, a reasoning-based guardrail model designed to moderate text, image, and video data. First, we construct a comprehensive training corpus comprising 148k samples spanning these three modalities. Our training pipeline follows a two-stage paradigm to incentivize the model to deliberate before making decisions: (1) conducting SFT to cold-start the model with explicit reasoning capabilities and structural adherence; and (2) performing RL, incorporating an error-driven exploration reward to incentivize deeper reasoning on hard samples. We release a suite of models scaled at 2B and 4B parameters. Extensive experiments demonstrate that GuardReasoner-Omni achieves superior performance compared to existing state-of-the-art baselines across various guardrail benchmarks. Notably, GuardReasoner-Omni (2B) significantly surpasses the runner-up by 5.3% F1 score.

</details>


### [66] [SEW: Strengthening Robustness of Black-box DNN Watermarking via Specificity Enhancement](https://arxiv.org/abs/2602.03377)
*Huming Qiu,Mi Zhang,Junjie Sun,Peiyi Chen,Xiaohan Zhang,Min Yang*

Main category: cs.CR

TL;DR: 本文提出了一种增强黑盒DNN水印特异性的新方法SEW，通过减少水印与近似密钥的关联来提高对抗移除攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于DNN的泛化特性，提取水印消息的密钥不唯一，这为攻击者提供了更多机会。先进的攻击技术可以逆向工程近似替代原始水印密钥，从而移除水印。因此需要提高水印的特异性来增强对抗移除攻击的能力。

Method: 提出特异性增强水印(SEW)方法，通过减少水印与近似密钥的关联来提高水印特异性。该方法在三个流行的水印基准上进行了广泛评估。

Result: 增强特异性显著提高了对抗移除攻击的鲁棒性。SEW能有效防御六种最先进的移除攻击，同时保持模型可用性和水印验证性能。

Conclusion: 通过提高黑盒DNN水印的特异性，可以显著增强水印对抗移除攻击的鲁棒性，SEW方法为此提供了一种有效的解决方案。

Abstract: To ensure the responsible distribution and use of open-source deep neural networks (DNNs), DNN watermarking has become a crucial technique to trace and verify unauthorized model replication or misuse. In practice, black-box watermarks manifest as specific predictive behaviors for specially crafted samples. However, due to the generalization nature of DNNs, the keys to extracting the watermark message are not unique, which would provide attackers with more opportunities. Advanced attack techniques can reverse-engineer approximate replacements for the original watermark keys, enabling subsequent watermark removal. In this paper, we explore black-box DNN watermarking specificity, which refers to the accuracy of a watermark's response to a key. Using this concept, we introduce Specificity-Enhanced Watermarking (SEW), a new method that improves specificity by reducing the association between the watermark and approximate keys. Through extensive evaluation using three popular watermarking benchmarks, we validate that enhancing specificity significantly contributes to strengthening robustness against removal attacks. SEW effectively defends against six state-of-the-art removal attacks, while maintaining model usability and watermark verification performance.

</details>


### [67] [Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection](https://arxiv.org/abs/2602.03423)
*Alexander Loth,Dominique Conceicao Rosario,Peter Ebinger,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CR

TL;DR: Origin Lens是一个隐私优先的移动框架，通过分层验证架构针对视觉虚假信息，在设备本地执行加密图像来源验证和AI检测


<details>
  <summary>Details</summary>
Motivation: 生成式AI的扩散给信息完整性保障带来挑战，需要将模型治理与终端用户验证连接起来的系统

Method: 采用Rust/Flutter混合架构，在设备本地执行加密图像来源验证和AI检测，整合加密来源、生成模型指纹和可选检索增强验证等多种信号

Result: 开发了一个隐私优先的移动框架，为用户在消费点提供分级置信度指标，能够补充平台级机制

Conclusion: Origin Lens框架符合监管要求（欧盟AI法案、DSA），在验证基础设施中发挥作用，与服务器端检测系统不同，它强调本地处理和隐私保护

Abstract: The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.

</details>


### [68] [Reading Between the Code Lines: On the Use of Self-Admitted Technical Debt for Security Analysis](https://arxiv.org/abs/2602.03470)
*Nicolás E. Díaz Ferreyra,Moritz Mock,Max Kretschmann,Barbara Russo,Mojtaba Shahin,Mansooreh Zahedi,Riccardo Scandariato*

Main category: cs.CR

TL;DR: 研究探讨了安全相关技术债务(SATD)如何补充静态分析工具(SAT)的输出，帮助克服SAT的高误报率和检测盲区问题。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具在安全工程中至关重要，但存在高误报率和漏洞类别覆盖不全的局限性。同时，开发者经常在代码注释中记录安全相关的技术债务(SATD)，但尚不清楚SATD如何在SAT辅助的安全分析中被利用。

Method: 采用混合方法：(1) 使用三种最先进的SAT分析SATD注释的漏洞数据集；(2) 对72名安全从业者进行在线调查。

Result: 所有SAT组合检测到135个安全相关SATD实例中的114个，覆盖24个CWE标识符。手动映射SATD注释发现33个独特CWE类型，其中6个对应SAT常忽略或难以检测的类别（如竞争条件）。调查显示开发者经常结合SAT输出和SATD洞察来更好理解安全弱点的影响和根本原因。

Conclusion: SATD编码的信息可以成为SAT驱动安全分析的有意义补充，同时帮助克服SAT的一些实际缺陷。

Abstract: Static Analysis Tools (SATs) are central to security engineering activities, as they enable early identification of code weaknesses without requiring execution. However, their effectiveness is often limited by high false-positive rates and incomplete coverage of vulnerability classes. At the same time, developers frequently document security-related shortcuts and compromises as Self-Admitted Technical Debt (SATD) in software artifacts, such as code comments. While prior work has recognized SATD as a rich source of security information, it remains unclear whether -and in what ways- it is utilized during SAT-aided security analysis. OBJECTIVE: This work investigates the extent to which security-related SATD complements the output produced by SATs and helps bridge some of their well-known limitations. METHOD: We followed a mixed-methods approach consisting of (i) the analysis of a SATD-annotated vulnerability dataset using three state-of-the-art SATs and (ii) an online survey with 72 security practitioners. RESULTS: The combined use of all SATs flagged 114 of the 135 security-related SATD instances, spanning 24 distinct Common Weakness Enumeration (CWE) identifiers. A manual mapping of the SATD comments revealed 33 unique CWE types, 6 of which correspond to categories that SATs commonly overlook or struggle to detect (e.g., race conditions). Survey responses further suggest that developers frequently pair SAT outputs with SATD insights to better understand the impact and root causes of security weaknesses and to identify suitable fixes. IMPLICATIONS: Our findings show that such SATD-encoded information can be a meaningful complement to SAT-driven security analysis, while helping to overcome some of SATs' practical shortcomings.

</details>


### [69] [Detecting and Explaining Malware Family Evolution Using Rule-Based Drift Analysis](https://arxiv.org/abs/2602.03489)
*Olha Jurečková,Martin Jureček*

Main category: cs.CR

TL;DR: 提出一种可解释的概念漂移检测方法，通过基于规则的分类器生成恶意软件样本的人类可读描述，比较规则集相似度来检测和量化概念漂移，并提供具体特征变化的解释。


<details>
  <summary>Details</summary>
Motivation: 恶意软件持续进化以逃避检测，导致概念漂移问题，降低了静态机器学习模型的有效性。理解和解释这种漂移对于维护稳健可信的恶意软件检测器至关重要。

Method: 使用基于规则的分类器为同一恶意软件家族的原始和进化样本生成人类可读描述，通过相似度函数比较规则集来检测和量化概念漂移，识别具体特征和特征值的变化。

Result: 实验结果表明，该方法不仅能准确检测漂移，还能提供关于进化恶意软件家族行为的可操作见解，支持检测和威胁分析。

Conclusion: 提出的可解释概念漂移检测方法有效解决了恶意软件进化带来的检测挑战，通过提供具体特征变化的解释增强了恶意软件检测器的可信度和实用性。

Abstract: Malware detection and classification into families are critical tasks in cybersecurity, complicated by the continual evolution of malware to evade detection. This evolution introduces concept drift, in which the statistical properties of malware features change over time, reducing the effectiveness of static machine learning models. Understanding and explaining this drift is essential for maintaining robust and trustworthy malware detectors. In this paper, we propose an interpretable approach to concept drift detection. Our method uses a rule-based classifier to generate human-readable descriptions of both original and evolved malware samples belonging to the same malware family. By comparing the resulting rule sets using a similarity function, we can detect and quantify concept drift. Crucially, this comparison also identifies the specific features and feature values that have changed, providing clear explanations of how malware has evolved to bypass detection. Experimental results demonstrate that the proposed method not only accurately detects drift but also provides actionable insights into the behavior of evolving malware families, supporting both detection and threat analysis.

</details>


### [70] [Can Developers rely on LLMs for Secure IaC Development?](https://arxiv.org/abs/2602.03648)
*Ehsan Firouzi,Shardul Bhatt,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究评估了GPT-4o和Gemini 2.0 Flash在安全基础设施即代码开发中的能力，包括安全气味检测和安全代码生成两方面表现


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在协助开发者进行安全基础设施即代码开发方面的实际能力，了解当前模型在安全代码检测和生成方面的表现和局限性

Method: 使用两种提示策略（通用提示和引导提示）在两个数据集（Stack Overflow简化代码片段和GitHub真实项目脚本）上测试模型的安全气味检测能力；通过89个易受攻击的合成场景测试模型的安全代码生成能力

Result: 安全气味检测：在Stack Overflow数据集上，通用提示检测到71%气味，引导提示提升至78%；在GitHub数据集上，通用提示效果较差（超过一半气味未检测到），引导提示检测到67%气味。安全代码生成：初始仅7%生成脚本安全，明确安全指令后GPT提升至17%，Gemini变化不大（8%）

Conclusion: 当前大型语言模型在安全基础设施即代码开发方面的能力有限，需要进一步研究改进模型在协助开发者进行安全代码开发和检测方面的能力

Abstract: We investigated the capabilities of GPT-4o and Gemini 2.0 Flash for secure Infrastructure as Code (IaC) development. For security smell detection, on the Stack Overflow dataset, which primarily contains small, simplified code snippets, the models detected at least 71% of security smells when prompted to analyze code from a security perspective (general prompt). With a guided prompt (adding clear, step-by-step instructions), this increased to 78%.In GitHub repositories, which contain complete, real-world project scripts, a general prompt was less effective, leaving more than half of the smells undetected. However, with the guided prompt, the models uncovered at least 67% of the smells. For secure code generation, we prompted LLMs with 89 vulnerable synthetic scenarios and observed that only 7% of the generated scripts were secure. Adding an explicit instruction to generate secure code increased GPT secure output rate to 17%, while Gemini changed little (8%). These results highlight the need for further research to improve LLMs' capabilities in assisting developers with secure IaC development.

</details>


### [71] [mopri - An Analysis Framework for Unveiling Privacy Violations in Mobile Apps](https://arxiv.org/abs/2602.03671)
*Cornell Ziepel,Stephan Escher,Sebastian Rehms,Stefan Köpsell*

Main category: cs.CR

TL;DR: mopri是一个用于分析移动应用行为的隐私合规性检测框架，通过静态和动态分析方法帮助用户和执法机构验证数据保护合规性。


<details>
  <summary>Details</summary>
Motivation: 移动应用日益普及导致用户参与可能性与隐私保护之间的冲突，现有隐私法规（如GDPR）难以有效检查移动应用合规性，需要工具帮助用户和执法机构验证和执行数据保护。

Method: 提出mopri概念框架，采用模块化管道设计，整合多种分析工具，结合静态分析（提取权限和跟踪库）和动态分析（流量记录和解密）方法，并包含结果增强和报告功能。

Result: 开发的原型系统能有效提取权限和跟踪库，采用稳健的动态流量记录和解密方法，结果增强和报告功能提高了分析结果的清晰度和可用性。

Conclusion: mopri展示了整体模块化隐私分析方法的可行性，强调需要持续适应移动应用生态系统不断演变的挑战，为验证和执行数据保护合规性提供了基础框架。

Abstract: Everyday services of society increasingly rely on mobile applications, resulting in a conflicting situation between the possibility of participation on the one side and user privacy and digital freedom on the other. In order to protect users' rights to informational self-determination, regulatory approaches for the collection and processing of personal data have been developed, such as the EU's GDPR. However, inspecting the compliance of mobile apps with privacy regulations remains difficult. Thus, in order to enable end users and enforcement bodies to verify and enforce data protection compliance, we propose mopri, a conceptual framework designed for analyzing the behavior of mobile apps through a comprehensive, adaptable, and user-centered approach. Recognizing the gaps in existing frameworks, mopri serves as a foundation for integrating various analysis tools into a streamlined, modular pipeline that employs static and dynamic analysis methods. Building on this concept, a prototype has been developed which effectively extracts permissions and tracking libraries while employing robust methods for dynamic traffic recording and decryption. Additionally, it incorporates result enrichment and reporting features that enhance the clarity and usability of the analysis outcomes. The prototype showcases the feasibility of a holistic and modular approach to privacy analysis, emphasizing the importance of continuous adaptation to the evolving challenges presented by the mobile app ecosystem.

</details>
