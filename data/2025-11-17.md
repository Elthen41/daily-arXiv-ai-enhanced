<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity](https://arxiv.org/abs/2511.10760)
*Emad Haque,Pragnya Sudershan Nalla,Jeff Zhang,Sachin S. Sapatnekar,Chaitali Chakrabarti,Yu Cao*

Main category: cs.AR

TL;DR: 本文重新审视了芯片接口设计的可靠性要求，发现在未来2.5D/3D封装技术中，ESD保护和芯片间信号传输可以大幅简化，从而为芯片小型化和可组合性提供新路径。


<details>
  <summary>Details</summary>
Motivation: 传统I/O电路（包括ESD保护和信号传输）在2.5D/3D异构集成中引入显著面积开销，成为制约芯片尺寸减小的主要瓶颈。

Method: 通过寄生参数提取和SPICE仿真，从芯片接口设计角度重新评估可靠性要求。

Result: 研究表明在未来2.5D/3D封装技术中，ESD保护和芯片间信号传输可以大幅简化。

Conclusion: 这种简化为进一步芯片小型化铺平了道路，并提高了微型芯片的可组合性和可重用性。

Abstract: The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.

</details>


### [2] [T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup](https://arxiv.org/abs/2511.11248)
*Jianyu Wei,Qingtao Li,Shijie Cao,Lingxiao Ma,Zixu Hao,Yanyong Zhang,Xiaoyan Hu,Ting Cao*

Main category: cs.AR

TL;DR: 本文提出了一种基于查表的方法来解决NPU上LLM推理性能差的问题，通过两级表结构解量化实现预填充和解码的统一处理，在NPU上分别获得了1.4倍和3.1倍的加速。


<details>
  <summary>Details</summary>
Motivation: 当前NPU在LLM推理中性能不如CPU，主要原因是NPU在除GEMM外的其他计算（如解量化）上表现不佳。现有方法要么将预填充和解码分离到不同硬件，要么在NPU上运行但存在精度损失。

Method: 基于低比特可以编码目标计算到可接受大小的表中的洞察，提出查表方法替代硬件不支持的操作。具体包括：(1) 融合两级表结构解量化；(2) 并发层次引导的平铺；(3) 预填充阶段的三级流水线；(4) 基于查表的解码映射到NPU向量单元。

Result: 相比基线NPU方法，预填充和解码分别获得1.4倍和3.1倍加速，同时节省84%的能耗。

Conclusion: 提出的查表方法有效解决了NPU上LLM推理的性能瓶颈，实现了显著的加速和能效提升，为设备端LLM部署提供了高效解决方案。

Abstract: Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: FengHuang平台是一个解耦的AI基础设施平台，采用多级共享内存架构，结合高速本地内存和集中式远程内存，通过主动张量分页和近内存计算来解决AI推理中的内存和通信扩展限制。


<details>
  <summary>Details</summary>
Motivation: 传统GPU中心架构在AI推理工作负载中面临内存容量、带宽和互连扩展的限制，需要新的基础设施设计来克服这些挑战。

Method: 提出FengHuang平台，采用多级共享内存架构，结合高速本地内存和集中式解耦远程内存，使用主动张量分页和近内存计算技术进行张量操作。

Result: 模拟显示FengHuang可实现高达93%的本地内存容量减少、50%的GPU计算节省，以及比传统GPU扩展快16倍到70倍的GPU间通信速度。在GPT-3、Grok-1和QWEN3-235B等工作负载下，可减少高达50%的GPU使用同时保持终端用户性能。

Conclusion: FengHuang提供了一个可扩展、灵活且经济高效的AI推理基础设施解决方案，作为机架级AI基础设施扩展方案实现了最佳平衡，其开放异构设计消除了供应商锁定并增强了供应链灵活性。

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [4] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: HPCAgentTester是一个基于多智能体大语言模型的框架，用于自动化生成高性能计算软件的单元测试，特别针对OpenMP和MPI并行编程模型。


<details>
  <summary>Details</summary>
Motivation: 传统测试方法难以处理高性能计算中的并行性、复杂算法和硬件多样性，特别是在处理非确定性行为和同步问题时存在不足。

Method: 采用多智能体协作架构，包含配方智能体和测试智能体，通过迭代式批判循环生成和优化测试用例，专门针对并行执行结构、复杂通信模式和层次化并行性。

Result: HPCAgentTester能够生成可编译且功能正确的OpenMP和MPI原语测试，有效识别传统方法常遗漏的细微错误，显著提高了测试编译率和正确性。

Conclusion: 相比独立大语言模型，HPCAgentTester提供了更强大和可扩展的解决方案，能够更好地确保并行软件系统的可靠性。

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: Co-EPG是一个自迭代训练框架，通过规划和接地模型的协同进化来解决GUI任务自动化中的挑战，在无需外部数据的情况下仅通过三次迭代就超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体方法存在两个基本限制：(1) 跨模型协同利用不足；(2) 过度依赖合成数据生成但利用不足。为了解决这些问题，需要开发能够有效整合规划和接地能力的协同进化框架。

Method: 提出Co-EPG自迭代训练框架，建立正反馈循环：规划模型通过GRPO在接地奖励指导下探索策略，生成多样化数据优化接地模型；优化后的接地模型为后续规划模型训练提供更有效的奖励，实现持续改进。

Result: 在Multimodal-Mind2Web和AndroidControl基准测试中，仅通过三次迭代就超越了现有最先进方法，无需外部数据。智能体在每次迭代中持续改进，展现出强大的自我增强能力。

Conclusion: 这项工作为GUI智能体建立了一种新颖的训练范式，从孤立优化转向集成、自驱动的协同进化方法。

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [6] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文研究多目标决策中的帕累托剪枝问题，通过多赢家投票框架分析现有质量度量方法，提出新的定向覆盖度量，并分析计算复杂性边界。


<details>
  <summary>Details</summary>
Motivation: 多目标决策问题中，决策者需要从帕累托最优解集中选择代表性子集，现有质量度量方法存在不直观行为，需要更好的度量方法和计算复杂性分析。

Method: 将帕累托剪枝问题重新构建为多赢家投票问题，进行公理化分析，提出新的定向覆盖度量方法，并分析不同质量度量的计算复杂性边界。

Result: 发现现有质量度量存在不直观行为，提出的定向覆盖度量在多种设置下表现具有竞争力或更优，确定了计算复杂性的可处理与难处理边界。

Conclusion: 质量度量的选择对所选解集特征有决定性影响，新提出的定向覆盖度量在多种场景下表现良好，为多目标决策提供了更好的工具。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [7] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 论文研究了如何将抽象论证框架问题编码为(Q)SAT问题，并设计了一种保持团宽度的有向分解引导(DDG)归约方法。


<details>
  <summary>Details</summary>
Motivation: 现有的SAT求解器对树宽度小的实例效率高，但团宽度作为更一般的图参数，其编码能力研究较少。作者希望通过研究抽象论证问题来理解团宽度的编码能力。

Method: 设计了从论证问题到(Q)SAT的新型归约方法，这些归约线性保持团宽度，称为有向分解引导(DDG)归约。

Result: 为所有论证语义(包括计数问题)建立了新结果，证明DDG归约的开销在合理假设下无法显著改进。

Conclusion: 该工作开启了理解团宽度编码能力的研究，为论证问题提供了高效的(Q)SAT编码方法，且证明了归约效率的紧致性。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [8] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文研究了反事实决策中的新指标：潜在结果排序概率(PoR)和获得最佳潜在结果概率(PoB)，建立了识别定理和边界估计方法，并通过实验验证了估计器的性能。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下的反事实决策中，决策者通常通过比较候选行动的期望潜在结果来进行选择。本文旨在引入新的决策规则来改进这一过程。

Method: 提出了PoR和PoB两个新指标，建立了识别定理并推导了边界，开发了相应的估计方法，并通过数值实验和真实数据集应用进行验证。

Result: 成功建立了PoR和PoB的识别定理和边界估计，数值实验表明估计器具有良好的有限样本性质，并在实际数据应用中展示了有效性。

Conclusion: PoR和PoB为反事实决策提供了新的有效工具，能够更好地指导个体层面的最优行动选择。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [9] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 本调查从适应性角度重新审视LLM推理，提出将推理努力根据输入难度和不确定性进行分配的能力作为核心挑战，并建立了包含训练式和训练无关方法的系统分类法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对推理任务采用统一策略，无法根据任务复杂度调整推理努力，导致简单问题过度推理而困难问题推理不足。

Method: 形式化演绎、归纳和溯因推理在LLM中的实现，将适应性推理定义为控制增强的策略优化问题，并建立包含训练式和训练无关方法的分类法。

Result: 提出了系统性的分类框架，将现有方法组织为训练式方法（强化学习、监督微调、学习控制器）和训练无关方法（提示条件化、反馈驱动停止、模块化组合）。

Conclusion: 该框架阐明了不同机制在实践中如何实现适应性推理，并识别了自评估、元推理和人类对齐推理控制等开放挑战。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [10] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一个混合知识图谱嵌入框架，通过自适应结合双曲、复数和欧几里得空间来解决现有方法在建模多样化关系类型时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理多样化关系类型时存在关键限制：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。

Method: 提出关系特定的空间加权策略，通过学习的注意力机制动态选择每个关系类型的最优几何空间，并使用多空间一致性损失确保跨空间预测的一致性。

Result: 在从1K论文到10M论文的知识图谱上评估，相比TransE、RotatE等基线方法取得一致改进。在10M论文数据集上达到0.612 MRR，比最佳基线相对提升4.8%，同时保持高效训练和85ms/三元组的推理速度。

Conclusion: HyperComplEx通过自适应维度分配实现与图谱大小的近线性扩展，为可扩展知识图谱嵌入研究提供了有效解决方案。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [11] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 本研究开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建事故前场景和推断车辆行为。该系统在39个复杂追尾事故案例中实现了100%的准确率，超越了人类研究人员92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故重建依赖人类专家，在处理不完整多模态数据时往往产生不一致结果。需要开发能够处理异构碰撞数据并提供精确重建的AI系统。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言事故重建；第二阶段结合这些重建与时序事件数据记录器进行深度事故推理。系统处理了277起追尾前车减速碰撞案例。

Result: 在39个复杂追尾事故案例评估中，框架在所有测试案例中实现了完美准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，超越了人类研究人员92%的准确率。

Conclusion: 本研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建碰撞动力学和表征事故前行为方面提供了前所未有的精确度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [12] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 本文提出了一种利用大型语言模型作为专家指导的新框架，通过整合多模态脑连接信息来改进神经退行性疾病进展的建模，解决了传统方法过度简化脑连接和可识别性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在建模神经退行性疾病进展时过度简化了脑连接的复杂性，通常假设单一模态的脑连接组作为疾病传播基质，导致对病理传播的预测不准确。同时，纯数据驱动的方法由于缺乏适当约束而面临可识别性问题。

Method: 利用大型语言模型作为专家指导，通过其整合多模态关系和多样化疾病驱动机制的能力，同时优化从个体观察构建长期疾病轨迹和具有更好可识别性的生物约束图结构。

Result: 在阿尔茨海默病队列的tau-PET成像数据上验证，新框架相比传统方法展现出更优的预测准确性和可解释性，并揭示了超越传统连接度量的额外疾病驱动因素。

Conclusion: 该框架通过整合LLMs的专家知识和多模态脑连接信息，显著改进了神经退行性疾病进展的建模能力，为理解疾病机制提供了更准确和可解释的工具。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [13] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 本文探讨了自主AI系统在遇到训练数据未覆盖的复杂场景时，如何构建、评估和证明候选行动方案，以满足人类期望和价值观。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际部署中必然会遇到训练数据未覆盖的复杂情境，需要超越训练策略来构建和评估行动方案，以实现与人类期望和价值观的对齐。

Method: 通过理论分析和实证案例研究，识别了智能体决策所需的知识类型，包括规范性、实用性和情境性理解。

Result: 提出了智能体在复杂现实环境中做出更符合对齐要求的决策所需的知识整合框架。

Conclusion: 智能体需要整合规范性、实用性和情境性知识，才能在复杂现实环境中选择和执行更符合人类期望的行动方案。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [14] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种新的不完全方法来打破抽象结构的对称性，通过更好地利用其表示来处理约束编程中常见的对称性问题。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，抽象结构（如嵌套集合）需要转换为求解器支持的表示形式。对称性破坏是提高求解效率的重要技术，但传统方法应用于抽象变量时会产生大量复杂约束，性能不佳。

Method: 开发了一种新的不完全对称性破坏方法，通过更好地利用抽象结构的表示来处理对称性，特别针对不可区分对象产生的对称性。

Result: 该方法比之前的方法（Akgün et al. 2025）更快。

Conclusion: 新方法通过改进抽象结构的表示利用，有效解决了对称性破坏中的性能问题，在不可区分对象对称性处理上表现优于现有方法。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [15] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 本文研究发现多智能体辩论中的角色分配策略对推理性能有显著影响，提出了"真理最后"策略可提升22%性能，并针对未知真理问题提出了基于一致性的MADC策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在增强推理能力方面显示出潜力，但角色分配策略这一关键方面尚未充分探索。

Method: 提出"真理最后"角色分配策略和基于路径一致性的MADC策略，通过模拟独立角色间的一致性来评估真理。

Result: 在9个LLM模型上的验证显示，MADC策略能持续展现先进性能，有效克服MAD的性能瓶颈。

Conclusion: MADC为LLM智能体扩展提供了关键改进路径，通过系统模拟和优化多智能体辩论的核心机制实现性能提升。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [16] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: DSS框架利用可微分模拟器Waymax作为状态预测器和评估器，通过梯度下降优化动作序列，显著提升自动驾驶的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统方法在需要学习所有组件（策略、状态预测器、评估器）时面临挑战。

Method: 提出DSS框架，利用可微分模拟器Waymax作为状态预测器和评估器，依赖其硬编码动力学实现高精度状态预测，并利用可微分特性在动作序列中进行有效搜索。

Result: 实验表明，DSS结合规划梯度和随机搜索，在跟踪和路径规划精度上显著优于序列预测、模仿学习、无模型RL及其他规划方法。

Conclusion: DSS框架通过可微分模拟器和梯度优化，为自动驾驶规划提供了有效解决方案，显著提升了性能。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [17] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出了一种新的广义规划方法，通过从训练问题中学习条件-动作规则来构建广义规划器，该方法在合成成本、规划覆盖率和解决方案质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在解决相关规划问题族的程序合成，现有方法在效率和效果方面存在不足，需要更简单有效的方法来生成可执行的广义计划。

Method: 对每个训练问题，按顺序计算每个目标原子的最优计划，执行目标回归，并将输出提升为一阶条件-动作规则集合，形成广义计划。

Result: 实验表明，该方法在经典和数值规划领域中，在合成成本、规划覆盖率和解决方案质量三个指标上显著优于现有最先进的广义规划器。

Conclusion: 提出的方法能够有效学习有效的广义规划和状态空间剪枝公理，为广义规划提供了一种简单而强大的解决方案。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [18] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: GGBench是一个专门评估几何生成推理的基准测试，旨在弥补现有评估方法在衡量统一多模态模型集成认知过程方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估判别性理解或无约束图像生成，无法衡量生成推理的集成认知过程，几何构造需要语言理解和精确视觉生成的融合，因此提供了理想的测试平台。

Method: 提出GGBench基准测试，通过几何构造任务来系统诊断模型的理解、推理和主动构建解决方案的能力。

Result: GGBench为下一代智能系统设定了更严格的标准，提供了评估几何生成推理的综合框架。

Conclusion: 几何构造为评估统一多模态模型的生成推理能力提供了理想测试平台，GGBench基准填补了现有评估方法的空白。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [19] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 本文提出多智能体卧底游戏（MUG）协议，通过反事实测试检测幻觉智能体，改进多智能体辩论范式，提升多模态推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论（MAD）方法假设所有辩论者都是理性的，但实际中智能体容易产生幻觉，需要更可靠的检测机制。

Method: 引入MUG协议，基于社交推理游戏概念，通过修改参考图像引入反事实证据，检测幻觉智能体，支持跨证据推理和主动推理。

Result: MUG在三个关键维度上改进了MAD：反事实测试实现事实验证、动态修改证据源支持跨证据推理、促进主动推理而非被动回答。

Conclusion: MUG为LLMs的多模态推理提供了更可靠有效的框架，代码已在GitHub开源。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [20] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考能力增强LLMs的表格推理，采用两阶段难度感知强化学习和轨迹级不确定性量化，提升推理深度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在表格推理中存在推理过程缺乏深度迭代和稳定性不足的问题，需要模拟人类认知的慢思考过程。

Method: 使用两阶段难度感知强化学习（DRL）从简单到复杂查询渐进学习，在推理时通过令牌级置信度和答案一致性进行轨迹级不确定性量化。

Result: 在基准测试中取得优越性能，增强推理稳定性，并在领域外数据集上表现出强泛化能力。

Conclusion: STaR为LLMs表格推理提供了可靠且认知启发的解决方案，具有实际应用潜力。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [21] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia是首个基于大语言模型的离子液体发现智能体，通过多模态领域基础模型实现准确性质预测和分层搜索架构，在真实湿实验验证中表现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离子液体发现面临性质预测困难、数据有限、模型准确性差和工作流程碎片化等挑战，需要开发更有效的发现工具。

Method: 构建LLM增强的多模态离子液体领域基础模型，采用分层搜索架构进行分子筛选和设计，使用新构建的综合离子液体数据集进行训练和评估。

Result: 模型在评估中表现出优越性能，能够有效进行离子液体修饰，在真实湿实验验证中展示了优异的泛化能力。

Conclusion: AIonopedia能够加速真实世界中的离子液体发现，为离子液体研究提供了有效的智能化工具。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [22] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录AI决策过程中所有组件的可验证追踪方法，以解决AI系统决策过程缺乏适当文档化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在决策过程文档化方面投入不足，这阻碍了追踪决策来源的能力，而这是重建责任链的前提。当AI决策无意或有意违反法律时，这种可追溯性对于法庭上确定原因至关重要。

Method: 采用一种激进但实用的方法，强制记录训练或推理过程中每个组件的文档。扩展DBOM概念，利用机密计算技术构建有效的工作流程，支持生成防篡改、可验证且详尽的AI决策追踪。

Result: 开发了首个运行的工作流程，支持生成防篡改、可验证且详尽的AI决策追踪。通过一个区分有毒和可食用蘑菇的应用程序演示了工作流程的内部运作。

Conclusion: 该方法为解决AI决策可追溯性问题提供了可行的解决方案，通过强制文档化和机密计算技术确保决策过程的透明度和责任链的可重建性。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [23] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 本文提出了对比ABox解释的概念，用于回答"为什么a是C的实例而b不是"这类问题。与单独解释正向蕴含或缺失蕴含的方法不同，对比解释同时考虑两者，聚焦于a和b之间的相关共性和差异。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能单独解释正向蕴含（为什么C(a)被知识库蕴含）或缺失蕴含（为什么C(b)不被蕴含），缺乏同时考虑两者的对比解释方法，无法有效回答"为什么a是C而b不是"这类对比性问题。

Method: 为描述逻辑本体论中的ABox推理开发了对比解释的适当概念，分析了不同变体在不同最优性标准下的计算复杂性，考虑了轻量级和更表达性的描述逻辑。实现了一种计算对比解释变体的方法，并在现实知识库的生成问题上进行了评估。

Result: 开发了对比ABox解释的形式化概念，分析了不同描述逻辑变体的计算复杂性，实现了计算对比解释的方法并在实际知识库上进行了评估。

Conclusion: 对比ABox解释能够有效回答对比性问题，通过同时考虑正向和缺失蕴含，聚焦于实例间的相关差异，为知识库推理提供了更全面的解释能力。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [24] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个推理时对齐框架，将大型视觉语言模型视为有限理性智能体，通过前瞻性函数动态权衡安全性、效用和成本，在降低计算成本的同时实现更好的安全性和效用。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐方法在安全性、效用和运营成本之间存在权衡困难，仅关注最终输出的过程盲目性会浪费大量计算资源在不安全的推理上，有害推理可能被良性理由伪装而绕过简单的安全评分。

Method: 将LVLM视为有限理性智能体，增量扩展思维图，使用前瞻性函数（类似净现值）对行动进行评分，动态权衡预期安全性、效用和成本与剩余预算，通过最弱环节原则强制执行路径安全性。

Result: 在3个闭源和2个开源模型上的6个数据集实验表明，EcoAlign以更低的计算成本匹配或超越了最先进的安全性和效用。

Conclusion: EcoAlign为稳健的LVLM对齐提供了一条原则性、经济高效的路径，解决了安全性与计算效率之间的平衡问题。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [25] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一个结合规则基础模型和强化学习的混合框架，用于社会感知导航，通过将基于行为实验的社会运动模型集成到奖励函数中，优化机械能和社会舒适度，在VR实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在人群环境中导航时避免引起不适的问题，结合规则方法的可解释性和数据驱动方法的灵活性，弥补两者各自的不足。

Method: 提出RLSLM混合强化学习框架，将基于经验行为实验的规则社会运动模型集成到强化学习的奖励函数中，生成方向敏感的社会舒适场，联合优化机械能和社会舒适度。

Result: 在基于沉浸式VR的人机交互实验中，RLSLM在用户体验方面优于最先进的规则基础模型，消融和敏感性分析显示其可解释性显著优于传统数据驱动方法。

Conclusion: 这项工作提出了一个可扩展的、以人为中心的方法论，有效整合认知科学和机器学习，实现现实世界的社会导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [26] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本调查系统回顾了在多智能体强化学习中，面对现实通信约束（如消息扰动、传输延迟和有限带宽）时的鲁棒高效通信策略进展，重点关注自动驾驶、分布式SLAM和联邦学习三个应用领域，并提出通信、学习和鲁棒性协同设计的统一方法。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信是瞬时、可靠且带宽无限的，但这些条件在现实部署中很少满足，因此需要研究在现实约束下的鲁棒高效通信策略。

Method: 系统调查和综述方法，分析在消息扰动、传输延迟和有限带宽等现实约束下的MARL通信策略，重点关注三个具体应用领域。

Result: 识别了现实MARL系统中的关键挑战，包括低延迟可靠性、带宽密集型数据共享和通信隐私权衡，并提出了相应的解决方案框架。

Conclusion: 需要采用通信、学习和鲁棒性协同设计的统一方法来弥合理论MARL模型与实际实现之间的差距，并指出了未来研究的关键开放挑战和方向。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [27] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet是一个多模态模型，整合了非结构化临床笔记、实验室测试和患者时间序列数据，使用LLM处理临床文本和文本化实验室测试，使用transformer编码器处理纵向序列就诊数据，在慢性疾病预测中达到超过94%的准确率。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含多种数据类型，但现有预测模型未能充分利用多模态数据之间的交互、冗余和时间模式，通常只关注单一数据类型或忽略这些复杂性。

Method: 使用大型语言模型处理临床文本和文本化实验室测试，使用transformer编码器处理纵向序列就诊数据，整合非结构化临床笔记、实验室测试和患者时间序列数据。

Result: 在MIMIC-III和FEMH数据集上评估，在多标签框架中预测前10种慢性疾病时达到超过94%的准确率。

Conclusion: 多模态EHR整合有潜力增强临床决策制定并改善患者预后。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [28] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个经验引导的推理系统，能够在推理时动态生成包含LLM调用、工具、采样参数和控制逻辑的完整计算策略，基于累积经验实现自适应问题解决。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在训练后无法灵活调整问题解决方法，要么只能修改文本输入，要么需要离线优化且部署后无法改变。需要一种能在推理时动态适应所有策略组件的系统。

Method: 使用基于LLM的元策略，包含两个组件：Guide基于当前问题和结构化记忆生成候选策略，Consolidator整合执行反馈改进未来策略生成。

Result: 在五个挑战性基准测试中，EGuR相比最强基线准确率提升高达14%，计算成本降低高达111倍，且随着经验积累性能持续提升。

Conclusion: EGuR通过动态生成完整计算策略实现了推理时的灵活适应，显著提升了AI系统的性能和效率。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [29] [BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2511.10714)
*Shuaitong Liu,Renjue Li,Lijia Yu,Lijun Zhang,Zhiming Liu,Gaojie Jin*

Main category: cs.CR

TL;DR: BadThink是一种针对思维链(CoT)提示的后门攻击，通过诱导模型产生过度思考行为来增加计算成本，同时保持最终输出的正确性以实现隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 思维链提示虽然提升了大型语言模型的推理能力，但也引入了计算效率这一新的攻击面。研究者希望揭示这种推理效率可能被隐蔽操纵的漏洞。

Method: 通过基于毒化的微调策略，采用基于LLM的迭代优化过程生成高度自然的毒化数据，将过度思考行为嵌入模型中。

Result: 在多个最先进模型和推理任务上的实验表明，BadThink能持续增加推理轨迹长度，在MATH-500数据集上实现了超过17倍的增长，同时保持隐蔽性和鲁棒性。

Conclusion: 这项工作揭示了一个关键且先前未被探索的漏洞，即推理效率可以被隐蔽地操纵，展示了一类针对CoT启用的系统的复杂攻击新类别。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of large language models (LLMs), but have also introduced their computational efficiency as a new attack surface. In this paper, we propose BadThink, the first backdoor attack designed to deliberately induce "overthinking" behavior in CoT-enabled LLMs while ensuring stealth. When activated by carefully crafted trigger prompts, BadThink manipulates the model to generate inflated reasoning traces - producing unnecessarily redundant thought processes while preserving the consistency of final outputs. This subtle attack vector creates a covert form of performance degradation that significantly increases computational costs and inference time while remaining difficult to detect through conventional output evaluation methods. We implement this attack through a sophisticated poisoning-based fine-tuning strategy, employing a novel LLM-based iterative optimization process to embed the behavior by generating highly naturalistic poisoned data. Our experiments on multiple state-of-the-art models and reasoning tasks show that BadThink consistently increases reasoning trace lengths - achieving an over 17x increase on the MATH-500 dataset - while remaining stealthy and robust. This work reveals a critical, previously unexplored vulnerability where reasoning efficiency can be covertly manipulated, demonstrating a new class of sophisticated attacks against CoT-enabled systems.

</details>


### [30] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: PISanitizer是一种针对长上下文LLM的提示注入防御方法，通过定位和清理潜在注入令牌来消除注入指令的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的提示注入防御方法主要针对短上下文设计，在长上下文场景下效果有限，因为注入指令只占长上下文的很小一部分，防御难度很大。

Method: 基于两个观察：1)提示注入攻击本质上是构建强制LLM遵循的指令；2)LLM利用注意力机制关注关键输入令牌。方法包括故意让LLM遵循上下文中的任意指令，然后清理驱动指令遵循行为的高注意力令牌。

Result: 评估显示PISanitizer能成功防止提示注入，保持实用性，优于现有防御方法，效率高，并对基于优化的强自适应攻击具有鲁棒性。

Conclusion: PISanitizer为攻击者制造了一个困境：注入指令越有效地强制LLM遵循，就越可能被PISanitizer清理掉。

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


### [31] [AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance](https://arxiv.org/abs/2511.10828)
*Weiheng Bai,Kefu Wu,Qiushi Wu,Kangjie Lu*

Main category: cs.CR

TL;DR: AFLGopher是一种可行性感知的定向模糊测试方法，通过改进距离计算机制，比现有定向模糊器更高效地到达目标代码和触发已知漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有定向模糊测试的距离计算机制缺乏可行性感知，无法有效指导模糊测试到达指定目标。

Method: 提出可行性感知的距离计算方法，包括基于有限轨迹预测所有分支可行性的分类方法，以及运行时逐步改进预测精度的机制。

Result: AFLGopher在到达目标方面比AFLGo、BEACON、WindRanger、SelectFuzz和增强版AFLGo分别快3.76倍、2.57倍、3.30倍、2.52倍和2.86倍；在触发已知漏洞方面分别快5.60倍、5.20倍、4.98倍、4.52倍和5.07倍。

Conclusion: 可行性感知的定向模糊测试能显著提高到达目标代码和触发漏洞的效率，AFLGopher在性能上大幅优于现有最先进的定向模糊器。

Abstract: Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \emph{feasibility-unaware}.
  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.

</details>


### [32] [Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation](https://arxiv.org/abs/2511.10863)
*Yiping Ma,Yue Guo,Harish Karthikeyan,Antigoni Polychroniadou*

Main category: cs.CR

TL;DR: Armadillo是一个具有抗破坏性的安全聚合系统，能够抵御恶意客户端的攻击，确保聚合结果只受客户端在预定义合法范围内误报私有输入的影响。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的抗破坏性方案要么客户端成本高，要么需要多轮通信。Armadillo旨在实现高效的抗破坏安全聚合，降低通信轮数和计算开销。

Method: 采用两层安全聚合协议和移除恶意客户端影响的协议，结合零知识证明技术，仅需简单算术计算。

Result: Armadillo在3轮通信内完成安全聚合，服务器和客户端计算负载轻，实现了高效的系统性能。

Conclusion: Armadillo为联邦学习提供了高效、抗破坏的安全聚合解决方案，在保证安全性的同时显著降低了通信和计算成本。

Abstract: This paper presents a secure aggregation system Armadillo that has disruptive resistance against adversarial clients, such that any coalition of malicious clients (within the tolerated threshold) can affect the aggregation result only by misreporting their private inputs in a pre-defined legitimate range. Armadillo is designed for federated learning setting, where a single powerful server interacts with many weak clients iteratively to train models on client's private data. While a few prior works consider disruption resistance under such setting, they either incur high per-client cost (Chowdhury et al. CCS '22) or require many rounds (Bell et al. USENIX Security '23). Although disruption resistance can be achieved generically with zero-knowledge proof techniques (which we also use in this paper), we realize an efficient system with two new designs: 1) a simple two-layer secure aggregation protocol that requires only simple arithmetic computation; 2) an agreement protocol that removes the effect of malicious clients from the aggregation with low round complexity. With these techniques, Armadillo completes each secure aggregation in 3 rounds while keeping the server and clients computationally lightweight.

</details>


### [33] [On the Information-Theoretic Fragility of Robust Watermarking under Diffusion Editing](https://arxiv.org/abs/2511.10933)
*Yunyi Ni,Ziyu Yang,Ze Niu,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: 本文研究了扩散模型图像编辑对鲁棒图像水印的威胁，证明了扩散变换会破坏水印信息，并提出了一种引导扩散攻击算法，能够在保持图像质量的同时几乎完全移除水印。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散模型的图像生成和编辑技术的出现，传统的鲁棒水印方案面临新的安全威胁，需要研究这些新技术如何影响水印的生存能力。

Method: 通过理论分析和实验验证，证明扩散变换会降低水印图像与嵌入载荷之间的互信息；提出引导扩散攻击算法，在生成过程中明确针对并擦除水印信号。

Result: 在最新的深度学习水印方案上评估，攻击后水印恢复率接近零，同时再生图像保持高视觉保真度。

Conclusion: 讨论了此类水印移除能力的伦理影响，并为未来在生成式AI时代设计更具弹性的水印策略提供了指导方针。

Abstract: Robust invisible watermarking embeds hidden information in images such that the watermark can survive various manipulations. However, the emergence of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we investigate the intersection of diffusion-based image editing and robust image watermarking. We analyze how diffusion-driven image edits can significantly degrade or even fully remove embedded watermarks from state-of-the-art robust watermarking systems. Both theoretical formulations and empirical experiments are provided. We prove that as a image undergoes iterative diffusion transformations, the mutual information between the watermarked image and the embedded payload approaches zero, causing watermark decoding to fail. We further propose a guided diffusion attack algorithm that explicitly targets and erases watermark signals during generation. We evaluate our approach on recent deep learning-based watermarking schemes and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Finally, we discuss ethical implications of such watermark removal capablities and provide design guidelines for future watermarking strategies to be more resilient in the era of generative AI.

</details>


### [34] [Gynopticon: Consensus-Based Cheating Detection System for Competitive Games](https://arxiv.org/abs/2511.10992)
*Jeuk Kang,Jungheum Park*

Main category: cs.CR

TL;DR: GYNOPTICON是一个基于用户共识的作弊检测框架，通过轻量级客户端检测和服务器端投票系统来识别游戏中的作弊行为，为竞争性在线游戏提供隐私保护的解决方案。


<details>
  <summary>Details</summary>
Motivation: 竞争性游戏类型（如MOBA、FPS、RTS）中的作弊检测研究不足，传统的内核级反作弊方案存在隐私和安全问题，需要开发更透明、隐私友好的替代方案。

Method: 结合轻量级客户端检测机制和服务器端投票系统：客户端检测可疑活动后向服务器投票，服务器聚合投票建立共识来区分作弊者和合法玩家。

Result: 在受控模拟和真实FPS环境中验证了可行性，模拟结果确认系统要求，真实实验证明能可靠检测作弊用户，公共数据集显示系统适用于长期游戏管理。

Conclusion: GYNOPTICON代表了一种用户驱动、基于共识的传统反作弊系统替代方案，为竞争性在线游戏提供了实用且保护隐私的解决方案。

Abstract: Cheating in online games poses significant threats to the gaming industry, yet most prior research has concentrated on Massively Multiplayer Online Role-Playing Games (MMORPGs). Competitive genres-such as Multiplayer Online Battle Arena (MOBA), First Person Shooter (FPS), Real Time Strategy (RTS), and Action games-remain underexplored due to the difficulty of detecting cheating users and the demand for complex data and techniques. To address this gap, many game companies rely on kernel-level anti-cheat solutions, which, while effective, raise serious concerns regarding user privacy and system security. In this paper, we propose GYNOPTICON, a novel cheating detection framework that leverages user consensus to identify abnormal behavior. GYNOPTICON integrates a lightweight client-side detection mechanism with a server-side voting system: when suspicious activity is identified, clients cast votes to the server, which aggregates them to establish consensus and distinguish cheaters from legitimate players. This architecture enables transparency, reduces reliance on intrusive monitoring, and mitigates privacy risks. We evaluate GYNOPTICON in both a controlled simulation and a real-world FPS environment. Simulation results verify its feasibility and requirements, while real-world experiments confirm its effectiveness in reliably detecting cheating users. Furthermore, we demonstrate the system's applicability and sustainability for long-term game management using public datasets. GYNOPTICON represents a user-driven, consensus-based alternative to conventional anti-cheat systems, offering a practical and privacy-preserving solution for competitive online games.

</details>


### [35] [Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis](https://arxiv.org/abs/2511.11020)
*Farhad Abtahi,Fernando Seoane,Iván Pau,Mario Vega-Barbas*

Main category: cs.CR

TL;DR: 医疗AI系统面临严重的数据中毒漏洞，攻击者仅需100-500个样本即可破坏系统，成功率超过60%，检测需要6-12个月甚至无法检测。


<details>
  <summary>Details</summary>
Motivation: 当前防御措施和法规无法充分应对医疗AI系统的数据中毒攻击，需要分析各种攻击场景并提出有效防御方案。

Method: 分析了四类攻击场景：架构攻击（CNN、LLM、RL）、基础设施攻击（联邦学习、医疗文档系统）、关键资源分配攻击（器官移植、危机分诊）和供应链攻击（商业基础模型）。

Result: 攻击者通过分布式医疗基础设施的多个入口点发起攻击，隐私法规无意中保护了攻击者，供应链弱点使单个受感染供应商可毒化50-200个机构的模型。

Conclusion: 建议采用多层防御，包括强制性对抗测试、基于集成的检测、隐私保护安全机制，以及向可解释系统转变，确保可验证的安全保证。

Abstract: Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.

</details>


### [36] [SALT-V: Lightweight Authentication for 5G V2X Broadcasting](https://arxiv.org/abs/2511.11028)
*Liu Cao,Weizheng Wang,Qipeng Xie,Dongyu Wei,Lyutianyang Zhang*

Main category: cs.CR

TL;DR: SALT-V是一个创新的混合认证框架，通过协议分层解决V2X通信中的认证困境，结合ECDSA和GMAC实现高效即时认证。


<details>
  <summary>Details</summary>
Motivation: 传统公钥方案（如ECDSA）验证延迟高（2ms），不适合碰撞避免；对称方案（如TESLA）效率高但密钥披露延迟大（20-100ms），都无法满足5G NR-V2X对即时认证和计算效率的严格要求。

Method: 采用分层协议设计：10%流量（BOOT帧）使用ECDSA签名建立发送者信任，90%消息（DATA帧）使用轻量级GMAC操作认证。核心创新是临时会话标签（EST）白名单机制和Bloom过滤器集成。

Result: SALT-V实现0.035ms平均计算时间（比纯ECDSA快57倍），1ms端到端延迟，41字节开销，可线性扩展到2000辆车，95%消息无需等待密钥披露即可即时验证。

Conclusion: SALT-V是首个满足实时V2X部署所有安全关键要求的实用解决方案，成功调和了认证延迟与计算效率之间的基本权衡。

Abstract: Vehicle-to-Everything (V2X) communication faces a critical authentication dilemma: traditional public-key schemes like ECDSA provide strong security but impose 2 ms verification delays unsuitable for collision avoidance, while symmetric approaches like TESLA achieve microsecond-level efficiency at the cost of 20-100 ms key disclosure latency. Neither meets 5G New Radio (NR)-V2X's stringent requirements for both immediate authentication and computational efficiency. This paper presents SALT-V, a novel hybrid authentication framework that reconciles this fundamental trade-off through intelligent protocol stratification. SALT-V employs ECDSA signatures for 10% of traffic (BOOT frames) to establish sender trust, then leverages this trust anchor to authenticate 90% of messages (DATA frames) using lightweight GMAC operations. The core innovation - an Ephemeral Session Tag (EST) whitelist mechanism - enables 95% of messages to achieve immediate verification without waiting for key disclosure, while Bloom filter integration provides O(1) revocation checking in 1 us. Comprehensive evaluation demonstrates that SALT-V achieves 0.035 ms average computation time (57x faster than pure ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles, making it the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment.

</details>


### [37] [Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach](https://arxiv.org/abs/2511.11249)
*Melih Coşğun,Mert Gençtürk,Sinem Sav*

Main category: cs.CR

TL;DR: 本文提出了联邦学习中的联邦归一化方法，通过安全交换归一化参数来模拟集中式归一化的效果，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据分布在多个参与方，传统本地归一化在非IID数据下效果差，而集中式归一化违背联邦学习的去中心化原则，需要一种既能保持数据本地化又能达到集中式归一化效果的解决方案。

Method: 提出联邦归一化概念，通过安全交换归一化参数来模拟集中式归一化；设计基于同态加密的k阶元素（中位数）计算方法；利用多方全同态加密实现隐私保护的归一化技术。

Result: 联邦归一化能够达到与集中式归一化相当的性能，同时不破坏数据本地化特性，并通过隐私保护技术有效降低隐私风险。

Conclusion: 联邦归一化是联邦学习中有效的预处理方法，能够在保护隐私的前提下实现与集中式归一化相当的性能，解决了传统方法在非IID数据下的局限性。

Abstract: Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).

</details>


### [38] [Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts](https://arxiv.org/abs/2511.11250)
*Biagio Boi,Christian Esposito*

Main category: cs.CR

TL;DR: 本文研究大型语言模型(LLMs)在检测非EVM区块链平台(Solana和Algorand)智能合约中OWASP相关漏洞的能力，通过构建合成数据集并评估不同配置下LLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 智能合约在去中心化环境中作为关键组件，如果代码设计不当会带来潜在风险。由于缺乏非EVM平台的标注数据集，需要研究LLMs在这些平台上的漏洞检测能力。

Method: 构建基于Rust(Solana)和PyTeal(Algorand)的合成标注数据集，采用OWASP漏洞分类法，评估LLMs在提示工程、微调和混合配置下的性能。

Result: 提示工程实现了一般鲁棒性，微调在语义较少的语言(如TEAL)上提高了精确率和召回率。分析了Solana和Algorand架构差异对漏洞表现和可检测性的影响。

Conclusion: LLM方法在智能合约静态漏洞检测中是可行的，前提是将领域特定数据和分类整合到训练流程中。

Abstract: Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.

</details>


### [39] [Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions](https://arxiv.org/abs/2511.11347)
*Shaowei Guan,Hin Chi Kwok,Ngai Fong Law,Gregor Stiglic,Vivian Hui*

Main category: cs.CR

TL;DR: 本文综述了医疗领域检索增强生成(RAG)应用的隐私保护现状，分析了敏感数据类型、隐私风险、保护机制及未来方向，提出了结构化框架来理解隐私漏洞并制定发展路线图。


<details>
  <summary>Details</summary>
Motivation: RAG在临床和生物医学工作流程中快速兴起，但受保护健康信息(PHI)暴露等隐私风险仍未得到一致缓解，需要系统分析隐私挑战和保护策略。

Method: 通过管道结构化框架分析23篇医疗RAG应用文献，涵盖数据存储、传输、检索和生成阶段，识别潜在故障模式、威胁模型和系统机制；同时评估17篇RAG系统隐私保护策略文献。

Result: 评估发现存在关键差距：临床验证不足、缺乏标准化评估框架、缺少自动化评估工具；提出了基于这些限制的可操作方向。

Conclusion: 为研究人员和从业者提供了理解医疗RAG隐私漏洞的结构化框架，并提供了实现临床有效性和强大隐私保护的路线图。

Abstract: Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.

</details>


### [40] [SEAL: Subspace-Anchored Watermarks for LLM Ownership](https://arxiv.org/abs/2511.11356)
*Yanbo Dai,Zongjie Li,Zhenlan Ji,Shuai Wang*

Main category: cs.CR

TL;DR: SEAL是一种基于子空间锚定的水印框架，通过将多比特签名嵌入到模型的潜在表示空间中，为大型语言模型提供有效的知识产权保护，支持白盒和黑盒验证场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型是宝贵知识产权资产，但现有IP保护方法存在局限：模型指纹技术无法识别具体模型实例，传统后门水印方法容易被微调或知识蒸馏等后处理操作移除。

Method: 利用模型编辑技术将选定锚点样本的隐藏表示与预定义正交比特向量对齐，从而在保持模型原始预测能力的同时嵌入水印，使水印功能无害且隐蔽。

Result: 在多个基准数据集和六个主流LLM上的实验表明，SEAL相比11种现有方法在有效性、保真度、效率和鲁棒性方面表现优越，即使在对手了解水印机制和嵌入签名的情况下仍保持强验证性能。

Conclusion: SEAL框架为LLM提供了强大且实用的IP保护解决方案，通过子空间锚定方法实现了既有效又隐蔽的水印嵌入。

Abstract: Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection approaches suffer from critical limitations. Model fingerprinting techniques can identify model architectures but fail to establish ownership of specific model instances. In contrast, traditional backdoor-based watermarking methods embed behavioral anomalies that can be easily removed through common post-processing operations such as fine-tuning or knowledge distillation.
  We propose SEAL, a subspace-anchored watermarking framework that embeds multi-bit signatures directly into the model's latent representational space, supporting both white-box and black-box verification scenarios. Our approach leverages model editing techniques to align the hidden representations of selected anchor samples with predefined orthogonal bit vectors. This alignment embeds the watermark while preserving the model's original factual predictions, rendering the watermark functionally harmless and stealthy. We conduct comprehensive experiments on multiple benchmark datasets and six prominent LLMs, comparing SEAL with 11 existing fingerprinting and watermarking methods to demonstrate its superior effectiveness, fidelity, efficiency, and robustness. Furthermore, we evaluate SEAL under potential knowledgeable attacks and show that it maintains strong verification performance even when adversaries possess knowledge of the watermarking mechanism and the embedded signatures.

</details>


### [41] [SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses](https://arxiv.org/abs/2511.11381)
*Gioliano de Oliveira Braga,Pedro Henrique dos Santos Rocha,Rafael Pimenta de Mattos Paixão,Giovani Hoff da Costa,Gustavo Cavalcanti Morais,Lourenço Alves Pereira Júnior*

Main category: cs.CR

TL;DR: 本文系统化分析了Wi-Fi CSI作为生物识别技术的安全性问题，揭示了现有研究在评估方法上的不一致性，并提出了统一的安全评估框架。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI作为生物识别技术虽然被广泛研究，但缺乏对其安全属性、对抗性恢复能力和方法一致性的系统理解，需要从安全角度进行系统化分析。

Method: 通过系统化知识方法，分析现有工作在感知基础设施、信号表示、特征管道、学习模型和评估方法上的差异，构建统一评估框架来实证揭示问题。

Result: 研究发现存在系统性不一致：依赖聚合精度指标、有限的FAR/FRR/EER报告、缺乏每用户风险分析、很少考虑威胁模型或对抗可行性。安全相关指标揭示了传统报告方法隐藏的风险集中问题。

Conclusion: 本文阐明了当前CSI生物识别的安全边界，为严格评估、可重复实验和未来研究方向提供了指导，为安全社区提供了对Wi-Fi CSI生物识别技术作为认证原语适用性的结构化重新评估。

Abstract: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.

</details>


### [42] [Automated Side-Channel Analysis of Cryptographic Protocol Implementations](https://arxiv.org/abs/2511.11385)
*Faezeh Nasrabadi,Robert Künnemann,Hamed Nemati*

Main category: cs.CR

TL;DR: 从WhatsApp二进制代码中提取首个形式化模型，结合侧信道泄漏合约分析，发现隐私攻击和已知漏洞


<details>
  <summary>Details</summary>
Motivation: 分析大型闭源应用WhatsApp的安全性和侧信道攻击抵抗力，填补规范与实现之间的功能差距

Method: 结合二进制分析(CryptoBap)和逆向工程(Ghidra)提取形式化模型，扩展CryptoBap框架集成硬件泄漏合约，使用DeepSec协议验证器进行安全分析

Result: 证明前向安全性，识别克隆攻击，发现WhatsApp实现与规范之间的功能差距，识别隐私攻击可泄露受害者联系人，确认电子护照BAC协议的非链接性攻击

Conclusion: 成功构建首个WhatsApp形式化模型，开发集成侧信道分析的框架，揭示规范方法不可见的关键漏洞

Abstract: We extract the first formal model of WhatsApp from its implementation by combining binary-level analysis (via CryptoBap) with reverse engineering (via Ghidra) to handle this large closed-source application. Using this model, we prove forward secrecy, identify a known clone-attack against post-compromise security and discover functional gaps between WhatsApp's implementation and its specification. We further introduce a methodology to analyze cryptographic protocol implementations for their resilience to side-channel attacks. This is achieved by extending the CryptoBap framework to integrate hardware leakage contracts into the protocol model, which we then pass to the state-of-the-art protocol prover, DeepSec. This enables a detailed security analysis against both functional bugs and microarchitectural side-channel attacks. Using this methodology, we identify a privacy attack in WhatsApp that allows a side-channel attacker to learn the victim's contacts and confirm a known unlinkability attack on the BAC protocol used in electronic passports.
  Key contributions include (1) the first formal model of WhatsApp, extracted from its binary, (2) a framework to integrate side-channel leakage contracts into protocol models for the first time, and (3) revealing critical vulnerabilities invisible to specification-based methods.

</details>


### [43] [Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging](https://arxiv.org/abs/2511.10712)
*Qinfeng Li,Miao Pan,Jintao Chen,Fu Teng,Zhiqiang Shen,Ge Su,Hao Peng,Xuhong Zhang*

Main category: cs.CR

TL;DR: MergeBarrier是一种即插即用的防御方法，通过破坏受保护模型与其同源模型之间的线性模式连接性，主动防止未经授权的模型合并窃取。


<details>
  <summary>Details</summary>
Motivation: 模型合并技术虽然能高效扩展大语言模型，但也带来了模型合并窃取的新威胁，现有防御机制无法同时满足主动防止未经授权合并、兼容开源设置和高安全性低性能损失这三个关键保护属性。

Method: 提出MergeBarrier防御方法，通过破坏受保护模型与其同源模型之间的线性模式连接性，消除有效模型合并所需的低损失路径。

Result: 大量实验表明，MergeBarrier能有效防止模型合并窃取，且准确率损失可忽略不计。

Conclusion: MergeBarrier为解决模型合并窃取问题提供了一种有效的防御方案，能够在保持高性能的同时实现主动防护。

Abstract: Model merging has emerged as an efficient technique for expanding large language models (LLMs) by integrating specialized expert models. However, it also introduces a new threat: model merging stealing, where free-riders exploit models through unauthorized model merging. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify three critical protection properties that existing methods fail to simultaneously satisfy: (1) proactively preventing unauthorized merging; (2) ensuring compatibility with general open-source settings; (3) achieving high security with negligible performance loss. To address the above issues, we propose MergeBarrier, a plug-and-play defense that proactively prevents unauthorized merging. The core design of MergeBarrier is to disrupt the Linear Mode Connectivity (LMC) between the protected model and its homologous counterparts, thereby eliminating the low-loss path required for effective model merging. Extensive experiments show that MergeBarrier effectively prevents model merging stealing with negligible accuracy loss.

</details>
