<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 本文针对ROS2调度方法的局限性，提出使用事件执行器实现固定作业级优先级调度器，支持任意有向无环图任务，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型。


<details>
  <summary>Details</summary>
Motivation: 当前ROS2调度方法主要关注简单的链式调度，缺乏对任意有向无环图（DAG）任务的分析能力，需要弥合传统实时系统理论与ROS2调度分析之间的差距。

Method: 提出使用事件执行器实现固定作业级优先级调度器，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型。需要特殊的事件队列实现和支持LIFO顺序消息传递的通信中间件。

Result: 实现能够生成与传统固定优先级DAG任务调度器相同调度结果的系统，尽管缺乏通常所需的优先关系信息，进一步缩小了实时系统理论与ROS2调度分析之间的差距。

Conclusion: 通过事件执行器实现固定作业级优先级调度器，成功将ROS2应用映射到传统实时DAG任务模型，为ROS2系统提供了更强大的调度分析能力。

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [2] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成高性能计算代码方面的能力，特别是针对曼德博集合的C++并行实现


<details>
  <summary>Details</summary>
Motivation: 并行编程是高性能计算中最具挑战性的方面之一，需要深入了解同步、通信和内存模型。虽然现代C++标准和框架简化了并行化，但掌握这些范式仍然很复杂。大型语言模型在代码生成方面显示出潜力，但它们在生成正确高效的高性能计算代码方面的有效性尚不清楚。

Method: 系统评估领先的大型语言模型（包括ChatGPT 4和5、Claude和LLaMA）在生成曼德博集合C++实现的任务上的表现，使用共享内存、基于指令和分布式内存范式。每个生成的程序都用GCC 11.5.0编译和执行，以评估其正确性、鲁棒性和可扩展性。

Result: 结果显示，ChatGPT-4和ChatGPT-5在语法精度和可扩展性能方面表现强劲。

Conclusion: 大型语言模型在高性能计算代码生成方面具有潜力，特别是ChatGPT系列模型在生成正确且可扩展的并行代码方面表现突出。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [3] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve是一个针对扩散大语言模型的高效服务系统，通过优化内存占用、计算调度和生成质量，解决了扩散模型特有的内存占用危机和资源振荡问题，在多种GPU上实现了显著的吞吐量提升和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型研究主要关注内核级优化，缺乏针对生产环境中扩散过程独特内存动态的整体服务框架。研究发现扩散大语言模型存在特定的"内存占用危机"，由单一logit张量和计算密集型"刷新"阶段与带宽密集型"重用"阶段之间的严重资源振荡引起。

Method: dLLM-Serve采用三种关键技术：1) Logit-Aware Activation Budgeting分解瞬态张量峰值；2) Phase-Multiplexed Scheduler交错处理异构请求阶段；3) Head-Centric Sparse Attention将逻辑稀疏性与物理存储解耦。

Result: 在多样化工作负载和GPU上的评估显示，相对于最先进的基线，dLLM-Serve在消费级RTX 4090上提升吞吐量1.61-1.81倍，在服务器级NVIDIA L40S上提升1.60-1.74倍，在重度竞争下尾部延迟降低近4倍。

Conclusion: dLLM-Serve建立了首个可扩展扩散大语言模型推理蓝图，将理论算法稀疏性转化为跨异构硬件的实际时钟加速，为扩散模型的生产部署提供了系统级解决方案。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [4] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE是一个可扩展的向量索引系统，通过平衡分区粒度和递归构建多级索引，在数十亿向量规模下实现了高精度、低延迟和高吞吐量的分布式近似最近邻搜索。


<details>
  <summary>Details</summary>
Motivation: 现有分布式近似最近邻搜索索引设计在扩展到数十亿向量时，难以在准确性、延迟和吞吐量之间取得良好平衡，需要新的设计来解决这一权衡问题。

Method: SPIRE采用两个核心设计：1）确定平衡的分区粒度以避免读取成本爆炸；2）引入保持精度的递归构建方法，构建具有可预测搜索成本和稳定精度的多级索引。

Result: 在多达46个节点、80亿向量的实验中，SPIRE实现了高可扩展性，比最先进系统的吞吐量提高了9.64倍。

Conclusion: SPIRE通过创新的分区粒度和递归索引构建方法，成功解决了大规模分布式近似最近邻搜索中准确性、延迟和吞吐量之间的权衡问题，实现了卓越的可扩展性和性能。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [5] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 开发基于云的联邦系统HEAL数据平台，作为NIH HEAL计划数据的统一搜索、发现和分析入口点


<details>
  <summary>Details</summary>
Motivation: HEAL计划产生多种数据类型，分散在多个NIH和第三方数据存储库中，需要一个统一的发现平台来促进数据二次利用

Method: 基于开源Gen3平台构建，使用框架服务（认证授权、数据对象标识符、元数据管理）和API与数据存储库互操作

Result: 平台已整合1000多项HEAL研究，每月数百用户使用，与19个数据存储库互操作，提供丰富元数据和云计算环境支持二次分析

Conclusion: HEAL数据平台实现了对连接数据存储库中数据的搜索、发现和分析，通过确保数据符合FAIR原则，最大化HEAL计划数据的价值

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [6] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: FlashCodec和UnifiedServe联合优化多模态大语言模型推理系统，通过GPU协同视频解码和逻辑解耦物理共享资源，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM推理系统存在两个主要瓶颈：1) CPU视频解码主导首次令牌时间，GPU解码方案无法满足延迟敏感需求；2) 视觉编码器与LLM推理阶段异构，导致阶段间阻塞和资源利用率低下。

Method: 提出FlashCodec（多GPU协同视频解码加速预处理）和UnifiedServe（逻辑解耦但物理共享GPU资源的视觉-文本联合优化），通过精心编排执行阶段和最小化干扰来优化端到端流水线。

Result: 相比最先进系统，该框架可服务3.0倍更多请求或强制执行1.5倍更严格的SLO，同时实现高达4.4倍的吞吐量提升。

Conclusion: FlashCodec和UnifiedServe组成的端到端优化堆栈有效解决了MLLM推理系统的瓶颈问题，显著提升了系统性能和资源利用率。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 该论文提出了一种基于逻辑框架的实体集扩展图方法，通过局部推理任务实现高效导航，避免完全构建大型扩展图。


<details>
  <summary>Details</summary>
Motivation: 传统实体集扩展的"线性"方法无法揭示知识资源中更丰富的"分类"结构。现有的扩展图框架虽然能支持基于知识库的分类扩展，但完全构建大型扩展图在实际应用中可能不切实际。

Method: 提出形式化的推理任务，检查两个元组是否属于扩展图中可比较、不可比较或相同的节点。在现实假设下（如限制输入或约束实体描述），这些任务可以高效实现。

Result: 研究结果表明，在现实假设条件下，这些推理任务可以高效实现，支持对扩展图进行局部、增量式导航。

Conclusion: 通过局部推理任务实现扩展图的高效导航，支持实际应用而无需完全构建整个图结构，为实体集扩展提供了更实用的分类方法。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [8] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能(SGI)的操作性定义，基于实践探究模型(PIM)，并创建了包含1000多个跨学科样本的SGI-Bench基准来评估LLMs。结果显示现有模型在深度研究、想法可行性、实验执行等方面存在显著差距，同时提出了测试时强化学习(TTRL)方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI有所进展，但缺乏一个连贯的科学通用智能(SGI)框架——即能够自主构思、调查和跨科学领域推理的能力。需要建立一个操作性定义和系统评估基准来推动AI系统真正参与科学发现。

Method: 1. 基于实践探究模型(PIM: 审议、构思、行动、感知)提出SGI的操作性定义；2. 设计四个科学家对齐任务：深度研究、想法生成、干/湿实验、实验推理；3. 构建SGI-Bench基准，包含1000多个专家策划的跨学科样本，灵感来自《科学》杂志的125个重大问题；4. 引入测试时强化学习(TTRL)，在推理时优化检索增强的新颖性奖励。

Result: 评估最先进LLMs的结果显示：深度研究的精确匹配率低(10-20%)，尽管步骤层面有对齐；生成的想法缺乏可行性和细节；干实验代码可执行性高但执行结果准确性低；湿实验协议序列保真度低；多模态比较推理存在持续挑战。TTRL方法成功提升了假设新颖性而无需参考答案。

Conclusion: 基于PIM的定义、以工作流为中心的基准和实证见解为真正参与科学发现的AI系统奠定了基础。研究揭示了当前LLMs在科学发现任务中的局限性，并提出了改进方向，推动了科学通用智能的发展。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [9] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个用于优化LLM智能体工作流的计划感知自动上下文工程框架，通过压缩上下文来提升智能体性能并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂多步工作流中产生快速扩展的上下文，现有方法忽略了多步、计划感知的特性，导致注意力稀释和推理成本增加。

Method: 提出PAACE框架，包含PAACE-Syn（生成带压缩监督的合成工作流）和PAACE-FT（蒸馏的计划感知压缩器），通过任务相关性建模、计划结构分析、指令协同精炼和函数保留压缩来优化上下文。

Result: 在AppWorld、OfficeBench和8-Objective QA等长视野基准测试中，PAACE在提高智能体正确性的同时显著降低上下文负载，蒸馏模型保留97%性能的同时降低超过一个数量级的推理成本。

Conclusion: PAACE为LLM智能体工作流提供了有效的计划感知上下文压缩框架，在保持性能的同时大幅降低计算成本，实现了实用的部署。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [10] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 本文提出了一种面向关系型知识图谱问答的新框架UniRel-R1，专注于返回实体间语义连接的子图而非单个实体，通过集成子图选择、多阶段图剪枝和强化学习微调的LLM来解决候选子图过多的问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱问答主要关注返回单个答案实体的实体中心查询，但现实世界中的查询往往是关系型的，需要理解实体之间的关联。因此需要一种互补的关系中心KGQA设置，返回捕捉实体间语义连接的子图。

Method: 提出UniRel-R1统一框架，集成子图选择、多阶段图剪枝和通过强化学习微调的大语言模型。奖励函数旨在鼓励生成紧凑且特定的子图，包含更多信息性关系和更低度的中间实体。

Result: 大量实验表明，UniRel-R1在连接性和奖励方面相比Vanilla基线取得了显著提升，并且能够有效泛化到未见过的实体和关系。

Conclusion: UniRel-R1成功解决了关系中心KGQA中的关键挑战，通过统一框架有效识别独特且信息丰富的答案子图，为传统实体中心KGQA提供了重要补充。

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [11] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用大语言模型驱动的智能体在虚拟社会中模拟冲突，研究发现物质威胁直接增加敌意，而象征性威胁影响较弱且通过内群体偏见中介，仅在物质威胁缺失时增加敌意


<details>
  <summary>Details</summary>
Motivation: 人类冲突常归因于物质条件和象征性价值的威胁，但两者如何相互作用以及哪个占主导地位尚不清楚。研究进展受到因果控制弱、伦理约束和时间数据稀缺的限制

Method: 使用大语言模型驱动的智能体在虚拟社会中进行模拟，独立操纵现实威胁和象征性威胁，同时追踪行动、语言和态度。通过表征分析验证LLM编码的威胁状态与操纵的对应关系

Result: 现实威胁直接增加敌意，象征性威胁影响较弱且完全通过内群体偏见中介，仅在现实威胁缺失时增加敌意。非敌对性群体间接触能缓冲冲突升级，结构不对称使敌意集中在多数群体中

Conclusion: 通过LLM智能体模拟为威胁驱动的冲突提供了因果解释，揭示了现实威胁和象征性威胁的不同作用机制，为理解人类冲突提供了新的研究范式

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [12] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 将AIXI强化学习智能体扩展到更广泛的效用函数类别，处理假设只能预测有限历史前缀时的效用分配问题，探讨死亡解释与不精确概率两种视角，使用Choquet积分计算期望效用


<details>
  <summary>Details</summary>
Motivation: AIXI智能体通常使用特定效用函数，但实际应用中需要更广泛的效用函数类别。当智能体的信念分布中的某些假设只能预测有限的历史前缀时，存在效用分配的模糊性，这通常被解释为"死亡概率"或"半测度损失"。论文旨在探索这种模糊性的不同解释和处理方法。

Method: 1. 将AIXI智能体推广到更广泛的效用函数类别；2. 分析"死亡解释"（将半测度损失视为死亡概率）下的效用分配方法；3. 提出将信念分布视为不精确概率分布的替代视角；4. 使用不精确概率理论中的Choquet积分计算期望效用；5. 研究这些方法的可计算性水平。

Result: 1. 标准递归值函数可以作为Choquet积分的特例恢复；2. 在死亡解释下，最一般的期望效用不能表征为Choquet积分；3. 建立了不同解释和方法之间的理论联系；4. 分析了相关计算方法的可计算性水平。

Conclusion: 通过将AIXI智能体扩展到更广泛的效用函数类别，并采用不精确概率理论和Choquet积分，可以更灵活地处理假设只能预测有限历史前缀时的效用分配问题。虽然死亡解释下的最一般期望效用不能完全用Choquet积分表征，但该方法为强化学习智能体的理论框架提供了新的视角和工具。

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [13] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 本文提出了一种ASP求解器在循环中的方法，用于指导大型语言模型进行指令调优，以改进答案集编程代码生成，仅需自然语言问题描述及其解决方案即可训练模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编程语言上表现良好，但在领域特定语言（如答案集编程ASP）的代码生成方面仍面临挑战。ASP是一种解决组合搜索问题的有效方法，但LLMs在ASP代码生成上的效果受到预训练阶段示例数量有限的制约。

Method: 提出ASP-solver-in-the-loop方法，利用求解器引导的指令调优。方法仅需自然语言问题描述及其解决方案：1）从LLMs采样ASP语句作为程序延续；2）利用ASP声明式编程的特性（部分编码逐渐缩小解空间），基于求解器反馈将样本分为选定和拒绝实例；3）对筛选数据进行监督微调；4）使用求解器引导的搜索（包括best-of-N采样）进一步提高鲁棒性。

Result: 实验表明，在两个不同提示设置和两个数据集上，该方法都取得了持续改进。

Conclusion: 提出的ASP-solver-in-the-loop方法通过求解器引导的指令调优，有效解决了ASP代码生成中的复杂语义解析问题，仅需自然语言问题描述和解决方案即可显著提升LLMs在ASP代码生成上的性能。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [14] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: SAGE是一个基于强化学习的框架，通过技能库增强LLM智能体的自我进化能力，在AppWorld任务中显著提升了任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂推理和多轮交互方面表现出色，但在新环境中持续改进和适应方面存在困难。现有的技能库方法主要依赖LLM提示，难以实现一致的技能库实施。

Method: 提出SAGE（Skill Augmented GRPO for self-Evolution）强化学习框架，通过Sequential Rollout机制在相似任务链中迭代部署智能体，让先前任务生成的技能积累到技能库中供后续任务使用，并通过Skill-integrated Reward增强技能生成和利用。

Result: 在AppWorld上的实验结果显示，SAGE应用于有专家经验的监督微调模型时，场景目标完成率提高了8.9%，交互步骤减少了26%，生成的token减少了59%，在准确性和效率上都显著优于现有方法。

Conclusion: SAGE框架通过强化学习与技能库的结合，有效增强了LLM智能体的自我改进能力，在任务完成率和效率方面都取得了显著提升，为智能体的持续适应和改进提供了新思路。

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [15] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出一种受Solomonoff启发的LLM假设加权方法，通过简洁性和预测拟合度评估多个候选解决方案，在不确定性下实现更均衡的概率分布和保守预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏数据的现实世界任务时，难以在评估多个候选解决方案时平衡准确性和简洁性。不确定性下的推理是AI的关键挑战，需要系统性泛化能力。

Method: 提出Solomonoff启发的方法，根据简洁性和预测拟合度对LLM生成的假设进行加权。应用于Mini-ARC基准任务，为每个单元格预测生成Solomonoff加权混合，即使在假设有噪声或部分错误时也能产生保守的、不确定性感知的输出。

Result: 与贝叶斯模型平均（BMA）相比，Solomonoff评分在竞争假设之间更均匀地分布概率，而BMA将权重集中在最可能但可能有缺陷的候选上。该方法在任务中表现出色。

Conclusion: 算法信息论先验对于不确定性下可解释、可靠的多假设推理具有重要价值。Solomonoff加权方法提供了更均衡、保守的预测分布。

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [16] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的可解释多模态检索增强生成方法，通过两阶段强化微调框架提升多模态大语言模型的推理能力，在WebQA和MultimodalQA数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有MMRAG方法无法阐明检索和响应生成背后的推理逻辑，限制了结果的可解释性。为了解决这一缺陷，作者提出引入强化学习来增强多模态检索增强生成的推理能力。

Method: 采用两阶段强化微调框架：第一阶段使用基于规则的强化微调对多模态文档进行粗粒度的点式排序，过滤掉显著不相关的文档；第二阶段使用基于推理的强化微调联合优化细粒度的列表式排序和答案生成，引导模型输出可解释的推理逻辑。

Result: 在WebQA和MultimodalQA两个多模态检索增强生成的基准数据集上取得了最先进的结果，并通过全面的消融实验验证了方法的有效性。

Conclusion: 提出的基于强化学习的可解释MMRAG方法能够有效提升多模态大语言模型的推理能力，实现可解释的多模态检索增强生成，为复杂多模态场景下的可信生成提供了新思路。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [17] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: UmniBench是一个针对统一多模态模型（UMMs）的全维度评估基准，能够在单个评估过程中同时评估理解、生成和编辑能力，覆盖13个主要领域和200多个概念。


<details>
  <summary>Details</summary>
Motivation: 当前对统一多模态模型的评估是分离的，分别评估其理解和生成能力。为了更全面地评估UMMs的综合能力，需要开发一个能够同时评估多种能力的基准。

Method: UmniBench利用人工检查的提示和问答对，通过UMM自身的理解能力来评估其生成和编辑能力。这种简单有效的范式允许对UMMs进行全面评估，同时也能解耦单独评估各项能力。

Result: 基于UmniBench，对24个流行模型进行了基准测试，包括UMMs和单能力大模型。该基准为统一模型提供了更全面客观的评估视角。

Conclusion: UmniBench为统一多模态模型提供了全维度评估基准，能够同时评估理解、生成和编辑能力，为社区模型性能改进提供了逻辑支持。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [18] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: EventGPT：基于GPT架构的球员条件化价值感知下一事件预测模型，用于足球转会分析，通过反事实模拟评估球员在不同战术环境中的适应性


<details>
  <summary>Details</summary>
Motivation: 现有转会评估方法依赖静态统计数据或事后价值模型，无法捕捉球员在新战术环境或不同队友配合下的适应性变化，导致转会成功预测困难

Method: 构建基于GPT风格自回归变换器的球员条件化、价值感知下一事件预测模型，将比赛处理为离散标记序列，联合预测下一持球动作的类型、位置、时间和残差持球价值，并利用学习到的球员嵌入进行反事实模拟

Result: 在5个赛季的英超联赛事件数据上，EventGPT在下一事件预测准确性和空间精度方面优于现有序列基线，并通过案例研究展示了在转会分析中的实际应用价值

Conclusion: EventGPT为转会适应性评估提供了原则性方法，能够模拟球员在不同球队或战术结构中的行为分布和价值变化，为转会决策提供更科学的依据

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [19] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 该论文从算法信息论视角提出概念的新定义，将概念视为与智能体整体经验存在结构性关系的信息对象，通过可逆一致性关系和冗余信息度量来评估概念分解的自然性，并建立概念演化、传输和多智能体对齐的数学框架。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能能否从原始经验中自主发现人类概念，但面临人类概念本身具有流动性（边界会变化、分裂、合并）的挑战。需要超越字典标签的概念定义，建立可修订、可比较、可跨智能体对齐的概念结构。

Method: 采用算法信息论视角，将概念定义为通过其与智能体整体经验的结构关系确定的信息对象。核心约束是确定性：一组部分形成可逆一致性关系（任何缺失部分可从其他部分恢复）。定义冗余信息来度量将经验分解为多个单独描述部分引入的冗余开销。在此基础上，将辩证法形式化为优化动力学：新信息出现时，竞争概念通过更短的描述来争夺解释权，驱动系统性的扩展、收缩、分裂和合并。最后，使用小型基础/种子形式化低成本概念传输和多智能体对齐。

Result: 提出了一个基于算法信息论的概念定义框架，使概念存在性成为可检验的结构性主张。建立了概念演化动力学模型，能够描述概念边界的系统性变化。开发了基于共享协议和小型种子的概念传输机制，实现计算-比特权衡的通信。

Conclusion: 该研究为人工智能自主概念发现提供了理论基础，通过算法信息论方法解决了概念流动性带来的挑战，建立了可检验、可演化、可传输的概念框架，为实现跨智能体的概念对齐和通信提供了具体的技术路径。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [20] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 该研究将Rashomon效应从分类任务扩展到序列决策领域，发现多个策略在行为表现相同的情况下内部结构不同，并通过形式化验证方法证明这种现象的存在，同时展示了Rashomon集合构建的集成策略具有更好的分布偏移鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未被探索。序列决策中的策略学习与分类任务不同，需要考虑随机转移和环境动态，需要新的方法来验证策略行为的等同性。

Method: 使用形式化验证方法构建和比较每个策略在环境中的完整概率行为，通过构造Rashomon集合来识别行为相同但内部结构不同的策略，并基于此构建集成策略和推导宽松策略。

Result: 实验证明Rashomon效应在序列决策中确实存在，从Rashomon集合构建的集成策略比单个策略对分布偏移具有更强的鲁棒性，从Rashomon集合推导的宽松策略在保持最优性能的同时减少了验证的计算需求。

Conclusion: Rashomon效应在序列决策中是一个重要现象，形式化验证方法是验证策略行为等同性的有效工具，利用Rashomon集合可以构建更鲁棒的集成策略和计算效率更高的宽松策略。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [21] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（GPT-4o）的诊断聊天机器人，结合检索增强生成和可解释AI技术，通过动态对话提取症状并提供透明诊断推理，在准确率上显著优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临诊断效率低、成本上升和专家资源有限等问题，导致治疗延误和不良健康结果。现有AI诊断系统缺乏交互性和透明度，难以在实际临床环境中有效应用。

Method: 开发基于LLM（GPT-4o）的诊断聊天机器人，采用检索增强生成技术，通过动态对话提取和规范化症状，使用相似性匹配和自适应提问优先诊断，结合思维链提示提供透明推理。

Result: 与传统机器学习模型（朴素贝叶斯、逻辑回归、SVM、随机森林、KNN）相比，LLM系统表现优异，准确率达到90%，Top-3准确率达到100%。

Conclusion: 该研究展示了基于LLM的诊断聊天机器人在医疗领域的潜力，为实现更透明、交互性强且临床相关的AI医疗系统提供了有前景的方向。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [22] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 本文提出定时奖励机（TRMs），扩展传统奖励机以纳入时间约束，使强化学习能处理时间敏感应用中的精确时序要求。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了其在时间敏感应用中的使用。需要一种能表达时序依赖的奖励规范方法。

Method: 提出定时奖励机（TRMs），将时间约束整合到奖励结构中。开发基于表格Q学习的模型无关RL框架，通过定时自动机抽象将TRM整合到学习中，并利用反事实想象启发式方法改进搜索。

Result: 实验表明，算法能在流行的RL基准上学习到满足TRM指定时间约束的高奖励策略。比较研究展示了不同TRM语义下的性能表现，消融实验凸显了反事实想象的优势。

Conclusion: TRMs为强化学习提供了更丰富的奖励规范表达能力，特别适用于需要精确时序约束的应用场景，提出的算法能有效学习满足这些约束的最优策略。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [23] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 研究通过跨国实验发现，AI拟人化设计对用户信任和参与度的影响具有文化特异性，挑战了普遍认为拟人化设计必然增加风险的假设。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益模仿人类特征，引发了关于拟人化可能导致错误信任或情感依赖的担忧。然而，现有安全框架主要基于西方人群的理论假设，缺乏对全球用户多样性的考虑，且拟人化设计与用户行为之间的因果关系尚未在真实人机交互中得到验证。

Method: 采用两个大规模跨国实验，涉及10个不同国家的3500名参与者，进行实时开放式的人机交互。实验通过操纵AI的人类化设计特征，测量用户对AI拟人化的感知、参与度和信任等行为指标。

Result: 研究发现：1）用户评估AI拟人化时更关注交互性线索而非理论特征；2）人类化设计能因果性地增加用户的拟人化感知；3）但人类化设计并不普遍增加用户参与度和信任；4）拟人化与行为结果之间的关系受文化调节，某些设计在某些文化中增加信任，在其他文化中可能产生相反效果。

Conclusion: 研究挑战了拟人化AI设计必然带来风险的普遍观点，揭示了人机交互的复杂文化中介特性，强调AI治理需要超越"一刀切"方法，考虑文化多样性。

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [24] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 该论文提出了推理定律(LoRe)框架，通过计算定律和准确率定律来形式化大型推理模型的理想推理行为，并开发了LoRe-Bench基准来评估模型的单调性和组合性，最后通过微调方法提升模型对计算定律的遵循程度，从而改善推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型(LRMs)表现出优越性能，但其推理行为常常违反直觉，导致推理能力未达最优。为了从理论上形式化理想的推理行为，需要建立一个统一的框架来刻画LRMs的内在推理模式。

Method: 1. 提出推理定律(LoRe)框架，包含计算定律(假设推理计算量应与问题复杂度线性扩展)和补充的准确率定律；2. 由于问题复杂度难以量化，通过单调性和组合性这两个可处理的属性来检验定律假设；3. 开发LoRe-Bench基准系统评估大型推理模型的这两个属性；4. 提出有效的微调方法，强制模型遵循计算定律的组合性。

Result: 评估显示大多数推理模型表现出合理的单调性但缺乏组合性。通过强制计算定律组合性的微调方法，更好的遵循计算定律能持续提升多个基准测试的推理性能，并揭示了不同属性和定律之间的协同效应。

Conclusion: 推理定律(LoRe)为形式化大型推理模型的理想推理行为提供了统一框架，通过LoRe-Bench评估和针对性微调，可以提升模型对推理定律的遵循程度，从而改善推理性能，这为理解和优化大型推理模型的推理能力提供了理论基础和实践方法。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval](https://arxiv.org/abs/2512.16962)
*Saksham Sahai Srivastava,Haoyu He*

Main category: cs.CR

TL;DR: MemoryGraft是一种针对LLM智能体的新型间接注入攻击，通过在智能体长期记忆中植入恶意成功经验来持续影响其行为，而非通过即时越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体越来越多地依赖长期记忆和检索增强生成来积累经验并改进未来性能，这种经验学习能力虽然增强了智能体自主性，但也引入了一个关键且未被探索的攻击面——智能体推理核心与其自身过去经验之间的信任边界。

Method: MemoryGraft攻击利用智能体的语义模仿启发式（倾向于复制检索到的成功任务模式）。攻击者通过提供良性摄入级工件，诱导智能体构建中毒的RAG存储，其中少量恶意程序模板与良性经验一起持久化。当智能体后来遇到语义相似任务时，基于词汇和嵌入相似性的联合检索会可靠地浮现这些嫁接的记忆，导致智能体采用嵌入的不安全模式。

Result: 在MetaGPT的DataInterpreter智能体（使用GPT-4o）上验证了MemoryGraft，发现少量中毒记录可以在良性工作负载上占据检索经验的大部分比例，将基于经验的自我改进转变为隐蔽且持久的攻击向量。

Conclusion: 该研究揭示了LLM智能体经验学习机制中的安全漏洞，提出了MemoryGraft这种新型攻击方法，表明智能体的长期记忆系统可能成为持续行为漂移的攻击载体，需要新的防御机制来保护智能体免受此类间接注入攻击。

Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.

</details>


### [26] [Sedna: Sharding transactions in multiple concurrent proposer blockchains](https://arxiv.org/abs/2512.17045)
*Alejandro Ranchal-Pedrosa,Benjamin Marsh,Lefteris Kokoris-Kogias,Alberto Sonnino*

Main category: cs.CR

TL;DR: Sedna是一个用户端协议，使用可验证的无速率编码替代简单的交易复制，解决多提议者共识中的三难困境：审查抵抗性、低延迟和合理成本


<details>
  <summary>Details</summary>
Motivation: 现代区块链采用多提议者共识来消除单领导者瓶颈并提高审查抵抗性，但用户如何向提议者传播交易仍存在问题。当前用户要么简单地向多个提议者复制完整交易（牺牲吞吐量并暴露MEV风险），要么针对少数提议者（接受较弱的审查和延迟保证），形成了审查抵抗性、低延迟和合理成本之间的三难困境。

Method: Sedna使用可验证的无速率编码技术。用户将寻址的符号包私下传递给提议者子集；一旦足够的符号被最终确定以解码，执行就按照确定性顺序进行。协议不需要共识修改，支持增量部署。

Result: Sedna保证了活跃性和"直到解码隐私"，显著减少了MEV暴露。分析表明，协议接近带宽开销的信息论下界，相比简单复制提高了2-3倍的效率。

Conclusion: Sedna通过可验证的无速率编码解决了多提议者共识中的交易传播三难困境，在保持审查抵抗性和低延迟的同时显著提高了效率并减少了MEV风险，且无需修改共识协议。

Abstract: Modern blockchains increasingly adopt multi-proposer (MCP) consensus to remove single-leader bottlenecks and improve censorship resistance. However, MCP alone does not resolve how users should disseminate transactions to proposers. Today, users either naively replicate full transactions to many proposers, sacrificing goodput and exposing payloads to MEV, or target few proposers and accept weak censorship and latency guarantees. This yields a practical trilemma among censorship resistance, low latency, and reasonable cost (in fees or system goodput).
  We present Sedna, a user-facing protocol that replaces naive transaction replication with verifiable, rateless coding. Users privately deliver addressed symbol bundles to subsets of proposers; execution follows a deterministic order once enough symbols are finalized to decode. We prove Sedna guarantees liveness and \emph{until-decode privacy}, significantly reducing MEV exposure. Analytically, the protocol approaches the information-theoretic lower bound for bandwidth overhead, yielding a 2-3x efficiency improvement over naive replication. Sedna requires no consensus modifications, enabling incremental deployment.

</details>


### [27] [Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning](https://arxiv.org/abs/2512.17254)
*Baolei Zhang,Minghong Fang,Zhuqing Liu,Biao Yi,Peizhao Zhou,Yuan Wang,Tong Li,Zheli Liu*

Main category: cs.CR

TL;DR: ABBR是一个实用的联邦学习框架，通过降维技术加速隐私保护计算，同时提供拜占庭鲁棒性和隐私保护，显著降低了计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临拜占庭攻击和隐私推理攻击的双重威胁，现有防御方法计算和通信开销大，理论与实践存在差距，需要更实用的解决方案。

Method: 利用降维技术加速隐私保护计算中的复杂过滤规则；分析低维空间中向量过滤的精度损失；引入自适应调优策略最小化绕过过滤的恶意模型对全局模型的影响。

Result: 在公开数据集上的评估显示，ABBR运行速度显著更快，通信开销最小，同时保持了与基线方法几乎相同的拜占庭鲁棒性。

Conclusion: ABBR是一个实用的拜占庭鲁棒和隐私保护的联邦学习框架，通过降维和自适应调优策略，在保持防御效果的同时显著降低了计算和通信开销。

Abstract: Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.

</details>


### [28] [AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs](https://arxiv.org/abs/2512.17251)
*Madhava Gaikwad*

Main category: cs.CR

TL;DR: AlignDP是一种混合隐私锁，通过在数据接口阻止知识转移来防御LLM的知识提取风险。它采用双层设计：对稀有字段使用PAC不可区分性保护，对非稀有字段使用RAPPOR本地差分隐私，全局聚合器控制隐私预算。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临提取、蒸馏和未经授权的微调风险。现有防御方法（如水印或监控）在泄露发生后才起作用，需要一种能在数据接口阻止知识转移的主动防御机制。

Method: 设计混合隐私锁AlignDP：1）分离稀有和非稀有字段；2）稀有字段使用PAC不可区分性保护，实现有效的零epsilon本地差分隐私；3）非稀有字段使用RAPPOR进行本地差分隐私处理，提供无偏频率估计；4）全局聚合器强制执行组合和隐私预算控制。

Result: 理论分析证明了PAC扩展到全局聚合的局限性，给出了RAPPOR估计的边界，并分析了效用权衡。玩具模拟验证了可行性：稀有类别保持隐藏，频繁类别能以小误差恢复。

Conclusion: AlignDP提供了一种在数据接口阻止知识转移的有效方法，通过双层隐私保护设计平衡了隐私和效用，为LLM的知识保护提供了新思路。

Abstract: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.

</details>


### [29] [An Iconic Heavy Hitter Algorithm Made Private](https://arxiv.org/abs/2512.17295)
*Rayne Holland*

Main category: cs.CR

TL;DR: 本文提出了首个满足差分隐私的SpaceSaving算法变体，并开发了从差分隐私频率预言机中提取频繁项的通⽤方法，在保持SpaceSaving实践优势的同时提供强隐私保证。


<details>
  <summary>Details</summary>
Motivation: 数据流中的频繁项识别是分析系统的基础问题，但数据流常来自敏感用户活动，需要更新级别的隐私保证。现有工作主要将Misra-Gries算法适配到差分隐私流模型，但实践中SpaceSaving算法性能更优，缺乏其隐私化版本。

Method: 1) 提出首个差分隐私SpaceSaving变体：通过对非私有SpaceSaving摘要注入渐近最优噪声，并应用精心校准的选择规则来抑制不稳定标签；2) 提出通用方法：从任何差分隐私频率预言机中提取频繁项，仅需O(k)额外内存，提供安全释放项标识的机制。

Result: 在合成和真实数据集上的实验表明，在广泛的隐私参数和空间预算下，该方法比现有的差分隐私Misra-Gries算法具有更优的实用性。SpaceSaving的实证优势在隐私化后得以保留。

Conclusion: SpaceSaving算法的实证优势在强差分隐私保证下依然存在，高效实用的频繁项识别在强隐私保护下是可实现的。提出的方法为隐私保护流数据分析提供了有效的解决方案。

Abstract: Identifying heavy hitters in data streams is a fundamental problem with widespread applications in modern analytics systems. These streams are often derived from sensitive user activity, making update-level privacy guarantees necessary. While recent work has adapted the classical heavy hitter algorithm Misra-Gries to satisfy differential privacy in the streaming model, the privatization of other heavy hitter algorithms with better empirical utility is absent.
  Under this observation, we present the first differentially private variant of the SpaceSaving algorithm, which, in the non-private setting, is regarded as the state-of-the-art in practice. Our construction post-processes a non-private SpaceSaving summary by injecting asymptotically optimal noise and applying a carefully calibrated selection rule that suppresses unstable labels. This yields strong privacy guarantees while preserving the empirical advantages of SpaceSaving.
  Second, we introduce a generic method for extracting heavy hitters from any differentially private frequency oracle in the data stream model. The method requires only O(k) additional memory, where k is the number of heavy items, and provides a mechanism for safely releasing item identities from noisy frequency estimates. This yields an efficient, plug-and-play approach for private heavy hitter recovery from linear sketches.
  Finally, we conduct an experimental evaluation on synthetic and real-world datasets. Across a wide range of privacy parameters and space budgets, our method provides superior utility to the existing differentially private Misra-Gries algorithm. Our results demonstrate that the empirical superiority of SpaceSaving survives privatization and that efficient, practical heavy hitter identification is achievable under strong differential privacy guarantees.

</details>


### [30] [Cryptanalysis of Pseudorandom Error-Correcting Codes](https://arxiv.org/abs/2512.17310)
*Tianrui Wang,Anyu Wang,Tianshuo Cong,Delong Ran,Jinyuan Liu,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本文首次对PRC（伪随机纠错码）进行了密码分析，提出了三种攻击方法挑战其不可检测性和鲁棒性假设，成功破坏了所有参数配置下的安全保证，并针对真实大模型进行了验证，最后提出了防御建议。


<details>
  <summary>Details</summary>
Motivation: PRC作为CRYPTO 2024提出的新型密码原语，具有伪随机性和纠错双重能力，被认为是AI生成内容水印的有前景基础组件。然而，其安全性尚未得到深入分析，特别是在具体参数配置下或面对密码攻击时。本文旨在填补这一空白。

Method: 提出了三种攻击方法：两种旨在区分PRC编码字与普通向量，一种旨在破坏PRC的解码过程。攻击成本为2^22次操作，并在真实大模型（如DeepSeek和Stable Diffusion）上验证了攻击效果。

Result: 攻击成功破坏了PRC在所有参数配置下声称的安全保证，能够以压倒性概率检测水印存在。同时提出了三种防御措施：参数建议、实现建议和构建修订的密钥生成算法，其中修订的密钥生成函数能有效防止弱密钥出现。

Conclusion: 尽管提出了防御建议，但当前基于PRC的水印方案仍无法在建议参数下实现128位安全性，这主要受限于大生成模型的固有配置（如大语言模型的最大输出长度）。

Abstract: Pseudorandom error-correcting codes (PRC) is a novel cryptographic primitive proposed at CRYPTO 2024. Due to the dual capability of pseudorandomness and error correction, PRC has been recognized as a promising foundational component for watermarking AI-generated content. However, the security of PRC has not been thoroughly analyzed, especially with concrete parameters or even in the face of cryptographic attacks. To fill this gap, we present the first cryptanalysis of PRC. We first propose three attacks to challenge the undetectability and robustness assumptions of PRC. Among them, two attacks aim to distinguish PRC-based codewords from plain vectors, and one attack aims to compromise the decoding process of PRC. Our attacks successfully undermine the claimed security guarantees across all parameter configurations. Notably, our attack can detect the presence of a watermark with overwhelming probability at a cost of $2^{22}$ operations. We also validate our approach by attacking real-world large generative models such as DeepSeek and Stable Diffusion. To mitigate our attacks, we further propose three defenses to enhance the security of PRC, including parameter suggestions, implementation suggestions, and constructing a revised key generation algorithm. Our proposed revised key generation function effectively prevents the occurrence of weak keys. However, we highlight that the current PRC-based watermarking scheme still cannot achieve a 128-bit security under our parameter suggestions due to the inherent configurations of large generative models, such as the maximum output length of large language models.

</details>


### [31] [Sandwiched and Silent: Behavioral Adaptation and Private Channel Exploitation in Ethereum MEV](https://arxiv.org/abs/2512.17602)
*Davide Mancino,Davide Rezzoli*

Main category: cs.CR

TL;DR: 论文实证分析了用户在遭受三明治攻击后的行为适应情况，发现约40%的受害者会在60天内转向私有路由，重复暴露后这一比例升至54%。同时确认了私有三明治攻击的存在，单个机器人占据了近三分之二的私有前置交易。


<details>
  <summary>Details</summary>
Motivation: 用户在遭受三明治攻击后如何适应和调整行为尚不清楚，需要实证量化分析。特别是要了解受害者是否会转向私有路由，以及私有路由是否真的能提供保护。

Method: 使用2024年11月至2025年2月的交易级数据，结合内存池可见性和ZeroMEV标签，追踪用户在遭受第n次公开三明治攻击后的结果：包括60天内重新激活链上活动的情况，以及首次采用私有路由的情况。

Result: 1. 约40%的受害者在60天内转向私有路由，重复暴露后升至54%；2. 首次攻击后流失率达7.5%，随后降至1-2%；3. 2024年11-12月确认2,932次私有三明治攻击，涉及3,126笔私有受害者交易，造成409,236美元损失和293,786美元攻击者利润；4. 单个机器人占据了近三分之二的私有前置交易；5. 私有三明治活动高度集中在少数DEX池中。

Conclusion: 私有路由并不能保证免受MEV提取：虽然执行失败推动用户转向私有渠道，但这些渠道仍然可被利用且高度集中，需要持续监控和协议级防御措施。

Abstract: How users adapt after being sandwiched remains unclear; this paper provides an empirical quantification. Using transaction level data from November 2024 to February 2025, enriched with mempool visibility and ZeroMEV labels, we track user outcomes after their n-th public sandwich: (i) reactivation, i.e., the resumption of on-chain activity within a 60-day window, and (ii) first-time adoption of private routing. We refer to users who do not reactivate within this window as churned, and to users experiencing multiple attacks (n>1) as undergoing repeated exposure. Our analysis reveals measurable behavioral adaptation: around 40% of victims migrate to private routing within 60 days, rising to 54% with repeated exposures. Churn peaks at 7.5% after the first sandwich but declines to 1-2%, consistent with survivor bias. In Nov-Dec 2024 we confirm 2,932 private sandwich attacks affecting 3,126 private victim transactions, producing \$409,236 in losses and \$293,786 in attacker profits. A single bot accounts for nearly two-thirds of private frontruns, and private sandwich activity is heavily concentrated on a small set of DEX pools. These results highlight that private routing does not guarantee protection from MEV extraction: while execution failures push users toward private channels, these remain exploitable and highly concentrated, demanding continuous monitoring and protocol-level defenses.

</details>


### [32] [A Post-Quantum Secure End-to-End Verifiable E-Voting Protocol Based on Multivariate Polynomials](https://arxiv.org/abs/2512.17613)
*Vikas Srivastava,Debasish Roy,Sihem Mesnager,Nibedita Kundu,Sumit Kumar Debnath,Sourav Mukhopadhyay*

Main category: cs.CR

TL;DR: 本文提出了首个基于多元多项式的后量子安全端到端可验证电子投票协议，以应对量子计算对现有基于数论假设的投票协议的威胁。


<details>
  <summary>Details</summary>
Motivation: 传统纸质投票存在公平性、有效性和可访问性问题，而现有的基于数论假设的电子投票协议在量子算法（如Shor算法）面前不再安全，需要设计后量子安全的替代方案。

Method: 设计基于多元多项式（MQ问题）的后量子安全电子投票协议，仅使用标准密码原语作为构建模块，提供简单而高效的设计。

Result: 提出了首个基于MQ问题（NP难问题）的端到端可验证后量子安全电子投票协议，其安全性不依赖于数论假设，能够抵抗量子攻击。

Conclusion: 该协议为解决量子计算时代电子投票的安全问题提供了可行方案，基于MQ问题的NP难特性确保了协议的后量子安全性。

Abstract: Voting is a primary democratic activity through which voters select representatives or approve policies. Conventional paper ballot elections have several drawbacks that might compromise the fairness, effectiveness, and accessibility of the voting process. Therefore, there is an increasing need to design safer, effective, and easily accessible alternatives. E-Voting is one such solution that uses digital tools to simplify voting. Existing state-of-the-art designs for secure E-Voting are based on number-theoretic hardness assumptions. These designs are no longer secure due to quantum algorithms such as Shor's algorithm. We present the design and analysis of \textit{first} post-quantum secure end-to-end verifiable E-Voting protocol based on multivariate polynomials to address this issue. The security of our proposed design depends on the hardness of the MQ problem, which is an NP-hard problem. We present a simple yet efficient design involving only standard cryptographic primitives as building blocks.

</details>


### [33] [Methods and Tools for Secure Quantum Clouds with a specific Case Study on Homomorphic Encryption](https://arxiv.org/abs/2512.17748)
*Aurelia Kusumastuti,Nikolay Tcholtchev,Philipp Lämmel,Sebastian Bock,Manfred Hauswirth*

Main category: cs.CR

TL;DR: 该研究探索了量子云安全方案，重点将同态加密集成到Eclipse Qrisp量子计算框架中，评估了三种后量子密码算法的性能权衡，并提出了量子云安全建议。


<details>
  <summary>Details</summary>
Motivation: 量子计算技术的发展给云计算带来了新的安全挑战，需要量子抗性加密策略来保护提供量子计算服务的量子云平台。

Method: 研究探索了多种量子云安全选项，重点关注将同态加密集成到Eclipse Qrisp量子计算框架中，评估了三种后量子密码算法（QOTP、Chen、GSW）的技术可行性和性能权衡。

Result: 成功实现了三种后量子密码算法与Qrisp的集成，证明同态加密与量子计算框架集成的可行性。QOTP算法简单且开销低，而Chen和GSW算法在运行时间和内存消耗方面存在性能权衡。

Conclusion: 提出了量子云安全建议：在数据存储和处理层面实施同态加密，开发量子密钥分发，实施严格的访问控制和认证机制，并参与后量子密码标准化工作。

Abstract: The rise of quantum computing/technology potentially introduces significant security challenges to cloud computing, necessitating quantum-resistant encryption strategies as well as protection schemes and methods for cloud infrastructures offering quantum computing time and services (i.e. quantum clouds). This research explores various options for securing quantum clouds and ensuring privacy, especially focussing on the integration of homomorphic encryption (HE) into Eclipse Qrisp, a high-level quantum computing framework, to enhance the security of quantum cloud platforms. The study addresses the technical feasibility of integrating HE with Qrisp, evaluates performance trade-offs, and assesses the potential impact on future quantum cloud architectures.The successful implementation and Qrisp integration of three post-quantum cryptographic (PQC) algorithms demonstrates the feasibility of integrating HE with quantum computing frameworks. The findings indicate that while the Quantum One-Time Pad (QOTP) offers simplicity and low overhead, other algorithms like Chen and Gentry-Sahai-Waters (GSW) present performance trade-offs in terms of runtime and memory consumption. The study results in an overall set of recommendations for securing quantum clouds, e.g. implementing HE at data storage and processing levels, developing Quantum Key Distribution (QKD), and enforcing stringent access control and authentication mechanisms as well as participating in PQC standardization efforts.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [34] [A 14ns-Latency 9Gb/s 0.44mm$^2$ 62pJ/b Short-Blocklength LDPC Decoder ASIC in 22FDX](https://arxiv.org/abs/2512.17834)
*Darja Nonaca,Jérémy Guichemerre,Reinhard Wiesmayr,Nihat Engin Tunali,Christoph Studer*

Main category: cs.AR

TL;DR: 提出一种新型短块长多速率二进制LDPC码，在URLLC应用中优于5G-LDPC码，并通过ASIC实现14ns最低延迟解码


<details>
  <summary>Details</summary>
Motivation: URLLC需要短块长编码，传统SCL解码器延迟高、面积效率差，而MP解码的LDPC码性能不足，需要更好的解决方案

Method: 设计新型短块长多速率二进制LDPC码，采用全并行消息传递解码架构，实现ASIC硬件实现

Result: 0.44mm² ASIC芯片支持三种速率，达到14ns最低解码延迟，9Gb/s信息吞吐量，62pJ/b能效（128位块长，1/2码率）

Conclusion: 提出的LDPC码在短块长URLLC应用中优于5G-LDPC码，通过全并行MP解码实现超低延迟和高能效

Abstract: Ultra-reliable low latency communication (URLLC) is a key part of 5G wireless systems. Achieving low latency necessitates codes with short blocklengths for which polar codes with successive cancellation list (SCL) decoding typically outperform message-passing (MP)-based decoding of low-density parity-check (LDPC) codes. However, SCL decoders are known to exhibit high latency and poor area efficiency. In this paper, we propose a new short-blocklength multi-rate binary LDPC code that outperforms the 5G-LDPC code for the same blocklength and is suitable for URLLC applications using fully parallel MP. To demonstrate our code's efficacy, we present a 0.44mm$^2$ GlobalFoundries 22FDX LDPC decoder ASIC which supports three rates and achieves the lowest-in-class decoding latency of 14ns while reaching an information throughput of 9Gb/s at 62pJ/b energy efficiency for a rate-1/2 code with 128-bit blocklength.

</details>
