<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Trustworthy Scheduling for Big Data Applications](https://arxiv.org/abs/2601.18983)
*Dimitrios Tomaras,Vana Kalogeraki,Dimitrios Gunopulos*

Main category: cs.DC

TL;DR: X-Sched是一个中间件，使用可解释性技术为容器化环境中的任务执行提供可操作的资源配置指导，确保在资源和时间约束下满足SLOs。


<details>
  <summary>Details</summary>
Motivation: 现有调度器虽然优化性能指标如任务执行时间和资源利用率，但决策过程不透明，且缺乏明确的指导告诉开发者需要采取什么具体行动来满足服务级别目标(SLOs)。

Method: 提出X-Sched中间件，集成反事实解释与先进机器学习模型（如随机森林），高效识别最优资源配置，为用户提供清晰可操作的调度决策解释。

Result: 实验结果表明，该方法在实际执行环境数据验证下具有高效性、优势性和实用性，能够确保任务按照性能目标执行。

Conclusion: X-Sched通过可解释性技术填补了现有调度器的透明度空白，不仅确保任务执行符合性能目标，还为用户提供清晰的调度决策依据和可操作指导。

Abstract: Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach.

</details>


### [2] [Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers](https://arxiv.org/abs/2601.19092)
*Bohan Hou,Hongyi Jin,Guanjie Wang,Jinqi Chen,Yaxing Cai,Lijie Yang,Zihao Ye,Yaoyao Ding,Ruihang Lai,Tianqi Chen*

Main category: cs.DC

TL;DR: Axe Layout是一个硬件感知的张量布局抽象，通过命名轴将逻辑张量坐标映射到多轴物理空间，统一了跨设备分布和设备内布局的平铺、分片、复制和偏移操作。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载需要跨设备网格、内存层次结构和异构加速器协调数据和计算布局，现有方法缺乏统一的抽象来处理不同粒度的布局问题。

Method: 提出Axe Layout抽象，通过命名轴映射逻辑到物理空间；基于此设计多粒度、分布感知的DSL和编译器，在单个内核中组合线程本地控制和集体操作符。

Result: 实验表明，该统一方法能在最新GPU设备、多设备环境和加速器后端上实现接近手工调优内核的性能。

Conclusion: Axe Layout提供了一个统一的硬件感知抽象，能够有效协调深度学习工作负载的数据和计算布局，实现接近手工优化的性能。

Abstract: Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends.

</details>


### [3] [KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing](https://arxiv.org/abs/2601.19160)
*Sheng Qi,Zhiquan Zhang,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: KUBEDIRECT是一个基于Kubernetes的FaaS集群管理器，通过绕过API Server的直接消息传递提高效率，同时保持与现有生态系统的兼容性。


<details>
  <summary>Details</summary>
Motivation: FaaS平台依赖Kubernetes等集群管理器进行资源管理，但在扩展FaaS实例时，控制器通过API Server进行消息传递成为主要瓶颈。现有解决方案需要重新设计集群管理器，但会牺牲与现有生态系统的兼容性并需要大量工程工作。

Method: KUBEDIRECT发现FaaS平台中存在一个共同的"窄腰"结构，这种顺序结构消除了对单一真实来源的需求，允许绕过API Server进行直接消息传递。系统采用新颖的状态管理方案，将窄腰作为分层写回缓存，确保一致性和收敛到期望状态。

Result: KUBEDIRECT能够无缝集成到Kubernetes中，每个控制器仅需增加约150行代码。实验表明，KUBEDIRECT比Knative减少了26.7倍的延迟，性能与最先进的重新设计平台Dirigent相当。

Conclusion: KUBEDIRECT在保持与现有Kubernetes生态系统兼容性的同时，通过绕过API Server的直接消息传递显著提高了FaaS平台的性能，解决了传统集群管理器在扩展FaaS实例时的瓶颈问题。

Abstract: FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.
  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent.

</details>


### [4] [Revisiting Parameter Server in LLM Post-Training](https://arxiv.org/abs/2601.19362)
*Xinyi Wan,Penghui Qi,Guangxing Huang,Chaoyi Ruan,Min Lin,Jialin Li*

Main category: cs.DC

TL;DR: ODC（按需通信）通过将FSDP中的集体通信替换为点对点通信，解决了LLM后训练中序列长度差异导致的负载不平衡问题，实现了高达36%的加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练中序列长度差异大，导致负载不平衡。传统的集体通信（如FSDP）在负载不平衡时会产生同步障碍，使负载较小的设备利用率不足。参数服务器（PS）范式对不平衡负载更鲁棒，需要重新审视。

Method: 提出ODC（按需通信），将参数服务器范式适配到完全分片数据并行（FSDP）中，用直接的点对点通信替换集体all-gather和reduce-scatter操作。将同步障碍从每层一次减少到每小批次一次，解耦各设备的工作负载，使更快的工作者不会被阻塞。

Result: 在多样化的LLM后训练任务中，ODC持续提高了设备利用率和训练吞吐量，相比标准FSDP实现了高达36%的加速。ODC还实现了更简单有效的小批次级别负载均衡。

Conclusion: ODC是LLM后训练中普遍存在的不平衡工作负载的优越解决方案。通过将参数服务器的鲁棒性与FSDP的效率相结合，显著提升了训练性能。代码已开源。

Abstract: Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [5] [CanaryBench: Stress Testing Privacy Leakage in Cluster-Level Conversation Summaries](https://arxiv.org/abs/2601.18834)
*Deep Mehta*

Main category: cs.CR

TL;DR: CanaryBench是一个用于测试对话聚类摘要中隐私泄露风险的基准测试工具，通过植入"金丝雀"字符串来检测敏感信息是否在摘要中泄露。


<details>
  <summary>Details</summary>
Motivation: 当前基于对话数据的聚合分析（如安全监控、治理和产品分析）通常会对对话进行嵌入、聚类并发布摘要描述。虽然原始对话可能不会暴露，但这些摘要仍可能包含个人身份信息（PII）或可追溯的敏感字符串，存在隐私泄露风险。

Method: 提出CanaryBench基准测试：1）生成包含植入秘密字符串（"金丝雀"）的合成对话，模拟敏感标识符；2）使用TF-IDF嵌入和k-means聚类对3,000个合成对话（24个主题）进行分析；3）采用有意的提取式摘要生成器模拟引用式报告；4）评估"金丝雀"字符串在摘要中的泄露情况。

Result: 在配置中（金丝雀注入率0.60），52个包含金丝雀的聚类中有50个出现金丝雀泄露（聚类级泄露率0.961538），同时存在非零的正则表达式PII指示符计数。结合最小聚类大小发布阈值（k-min=25）和基于正则表达式的脱敏处理，可以消除金丝雀泄露和PII指示符命中，同时保持相似的聚类一致性。

Conclusion: CanaryBench提供了一个简单可复现的隐私泄露压力测试框架，重点关注已发布分析产物而非原始用户数据的隐私风险测量。通过最小聚类大小阈值和正则表达式脱敏的组合防御策略，可以有效降低摘要中的隐私泄露风险。

Abstract: Aggregate analytics over conversational data are increasingly used for safety monitoring, governance, and product analysis in large language model systems. A common practice is to embed conversations, cluster them, and publish short textual summaries describing each cluster. While raw conversations may never be exposed, these derived summaries can still pose privacy risks if they contain personally identifying information (PII) or uniquely traceable strings copied from individual conversations.
  We introduce CanaryBench, a simple and reproducible stress test for privacy leakage in cluster-level conversation summaries. CanaryBench generates synthetic conversations with planted secret strings ("canaries") that simulate sensitive identifiers. Because canaries are known a priori, any appearance of these strings in published summaries constitutes a measurable leak.
  Using TF-IDF embeddings and k-means clustering on 3,000 synthetic conversations (24 topics) with a canary injection rate of 0.60, we evaluate an intentionally extractive example snippet summarizer that models quote-like reporting. In this configuration, we observe canary leakage in 50 of 52 canary-containing clusters (cluster-level leakage rate 0.961538), along with nonzero regex-based PII indicator counts. A minimal defense combining a minimum cluster-size publication threshold (k-min = 25) and regex-based redaction eliminates measured canary leakage and PII indicator hits in the reported run while maintaining a similar cluster-coherence proxy. We position this work as a societal impacts contribution centered on privacy risk measurement for published analytics artifacts rather than raw user data.

</details>


### [6] [Proactive Hardening of LLM Defenses with HASTE](https://arxiv.org/abs/2601.19051)
*Henry Chen,Victor Aranda,Samarth Keshari,Ryan Heartfield,Nicole Nichols*

Main category: cs.CR

TL;DR: HASTE框架通过迭代生成高度规避性提示来主动强化LLM防御，可降低基线检测器64%的恶意提示检测率，但结合检测模型重训练后能显著优化检测效果


<details>
  <summary>Details</summary>
Motivation: 基于提示的攻击技术是LLM系统安全部署的主要挑战，LLM输入空间无界且非结构化，需要主动强化策略来持续生成自适应攻击向量以优化运行时防御

Method: HASTE是一个系统性框架，通过模块化优化过程迭代工程化高度规避性提示，支持任意硬负例或硬正例迭代策略，可评估带或不带模糊测试的提示注入检测效果

Result: 硬负例挖掘成功规避基线检测器，将恶意提示检测率降低约64%；与检测模型重训练结合后，相比基线策略能以更少迭代循环优化提示检测模型效果

Conclusion: HASTE框架支持LLM防御的前瞻性和反应性强化，开发者可主动进行动态压力测试识别弱点，也可反应性模拟新攻击类型快速填补检测覆盖空白

Abstract: Prompt-based attack techniques are one of the primary challenges in securely deploying and protecting LLM-based AI systems. LLM inputs are an unbounded, unstructured space. Consequently, effectively defending against these attacks requires proactive hardening strategies capable of continuously generating adaptive attack vectors to optimize LLM defense at runtime. We present HASTE (Hard-negative Attack Sample Training Engine): a systematic framework that iteratively engineers highly evasive prompts, within a modular optimization process, to continuously enhance detection efficacy for prompt-based attack techniques. The framework is agnostic to synthetic data generation methods, and can be generalized to evaluate prompt-injection detection efficacy, with and without fuzzing, for any hard-negative or hard-positive iteration strategy. Experimental evaluation of HASTE shows that hard negative mining successfully evades baseline detectors, reducing malicious prompt detection for baseline detectors by approximately 64%. However, when integrated with detection model re-training, it optimizes the efficacy of prompt detection models with significantly fewer iteration loops compared to relative baseline strategies. The HASTE framework supports both proactive and reactive hardening of LLM defenses and guardrails. Proactively, developers can leverage HASTE to dynamically stress-test prompt injection detection systems; efficiently identifying weaknesses and strengthening defensive posture. Reactively, HASTE can mimic newly observed attack types and rapidly bridge detection coverage by teaching HASTE-optimized detection models to identify them.

</details>


### [7] [Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models](https://arxiv.org/abs/2601.19061)
*Harsh Chaudhari,Ethan Rathbum,Hanna Foerster,Jamie Hayes,Matthew Jagielski,Milad Nasr,Ilia Shumailov,Alina Oprea*

Main category: cs.CR

TL;DR: 论文揭示了一种针对思维链推理模型的新型间接定向投毒攻击——"思想转移"攻击，该攻击通过操纵不同任务的思维链痕迹来影响目标任务的模型输出，无需修改查询和答案，实现了"干净标签"投毒。


<details>
  <summary>Details</summary>
Motivation: 随着思维链推理成为增强大语言模型能力的重要技术，通过微调预训练模型使用公开推理数据集已成为常见做法，但这为针对推理痕迹本身的新攻击向量创造了条件。现有研究主要关注需要明确包含触发查询和错误推理的后门攻击，而本文旨在揭示一种更隐蔽的间接攻击方式。

Method: 提出"思想转移"攻击方法，通过仅操纵训练样本的思维链推理痕迹（不改变查询和答案），将从一个任务学到的推理模式转移到完全不同的目标任务中。这种攻击不需要在投毒数据中显式包含目标任务样本，实现了"干净标签"投毒。

Result: 攻击成功率高达70%，能够在从未出现在训练中的完全不同的领域注入目标行为。同时，使用投毒推理数据训练还能使模型在多个基准测试上的性能提升10-15%，为用户使用投毒数据集提供了激励。

Conclusion: 研究发现揭示了推理模型带来的新型威胁向量，这种威胁不易被现有防御措施检测和缓解，突显了推理模型安全性的重要性和现有防御机制的局限性。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing large language models' capabilities by generating intermediate reasoning steps for complex tasks. A common practice for equipping LLMs with reasoning is to fine-tune pre-trained models using CoT datasets from public repositories like HuggingFace, which creates new attack vectors targeting the reasoning traces themselves. While prior works have shown the possibility of mounting backdoor attacks in CoT-based models, these attacks require explicit inclusion of triggered queries with flawed reasoning and incorrect answers in the training set to succeed. Our work unveils a new class of Indirect Targeted Poisoning attacks in reasoning models that manipulate responses of a target task by transferring CoT traces learned from a different task. Our "Thought-Transfer" attack can influence the LLM output on a target task by manipulating only the training samples' CoT traces, while leaving the queries and answers unchanged, resulting in a form of ``clean label'' poisoning. Unlike prior targeted poisoning attacks that explicitly require target task samples in the poisoned data, we demonstrate that thought-transfer achieves 70% success rates in injecting targeted behaviors into entirely different domains that are never present in training. Training on poisoned reasoning data also improves the model's performance by 10-15% on multiple benchmarks, providing incentives for a user to use our poisoned reasoning dataset. Our findings reveal a novel threat vector enabled by reasoning models, which is not easily defended by existing mitigations.

</details>


### [8] [A Security Analysis of CheriBSD and Morello Linux](https://arxiv.org/abs/2601.19074)
*Dariy Guzairov,Alex Potanin,Stephen Kell,Alwen Tiu*

Main category: cs.CR

TL;DR: 该论文分析了CHERI架构中隔离机制的四种绕过方法，重点关注移植到该架构的Linux和BSD系统，并提出了相应的缓解措施。


<details>
  <summary>Details</summary>
Motivation: 虽然CHERI架构通过能力机制有效缓解了内存破坏攻击，但其隔离机制在将恶意代码限制在单独隔离区方面效果有限。论文旨在揭示CHERI架构中隔离机制存在的安全漏洞。

Method: 论文详细描述了四种绕过CHERI隔离机制的方法，重点关注移植到该架构的Linux和BSD操作系统，并开发了概念验证来演示这些攻击。

Result: 研究发现，尽管Linux和BSD系统实现了CHERI隔离机制，但简单的漏洞和攻击仍能让恶意代码绕过隔离。论文展示了这些攻击的实际可行性。

Conclusion: 论文提出了防止这些攻击的缓解措施，并给出了进一步保护Linux和BSD系统免受未知攻击的建议，强调需要加强CHERI隔离机制的安全性。

Abstract: Memory corruption attacks have been prevalent in software for a long time. Some mitigation strategies against these attacks do exist, but they are not as far-reaching or as efficient as the CHERI architecture. CHERI uses capabilities to restrict pointers to certain regions of memory and with certain access restrictions. These capabilities are also used to implement "compartmentalisation": dividing a binary into smaller components with limited privilege, while adhering to the principle of least privilege. However, while this architecture successfully mitigates memory corruption attacks, the compartmentalisation mechanisms in place are less effective in containing malicious code to a separate compartment. This paper details four ways to bypass compartmentalisation, with a focus on Linux and BSD operating systems ported to this architecture. We find that although compartmentalisation is implemented in these two operating systems, simple bugs and attacks can still allow malicious code to bypass it. We conclude with mitigation measures to prevent these attacks, a proof-of-concept demonstrating their use, and recommendations for further securing Linux and BSD against unknown attacks.

</details>


### [9] [Self-Sovereign Identity and eIDAS 2.0: An Analysis of Control, Privacy, and Legal Implications](https://arxiv.org/abs/2601.19837)
*Nacereddine Sitouah,Marco Esposito,Francesco Bruschi*

Main category: cs.CR

TL;DR: 本文分析了欧盟数字身份框架的演进，从电子签名指令到eIDAS 2.0，重点评估了新法规的条款、实施挑战，以及欧洲数字身份架构与自主权身份原则的兼容性。


<details>
  <summary>Details</summary>
Motivation: 欧盟数字身份框架从1999年电子签名指令开始演进，但eIDAS 1.0存在局限性和不完整性。随着去中心化方法的兴起，需要评估新法规eIDAS 2.0如何应对这些挑战，特别是如何整合自主权身份等创新身份范式。

Method: 分析eIDAS 2.0法规及其配套说明的关键条款，借鉴现有文献识别立法空白和实施挑战；同时评估欧洲数字身份架构与参考框架的指导原则，考察其实际实施与自主权身份原则的契合程度。

Result: 研究发现eIDAS 2.0法规存在立法空白和实施挑战，欧洲数字身份架构与参考框架的实施与自主权身份原则的兼容性有待进一步评估和优化。

Conclusion: 欧盟数字身份框架的演进反映了对互操作性和安全标准的持续追求，但新法规和架构需要更好地整合去中心化身份范式，以实现更灵活、用户中心的数字身份生态系统。

Abstract: European digital identity initiatives are grounded in regulatory frameworks designed to ensure interoperability and robust, harmonized security standards. The evolution of these frameworks culminates in eIDAS 2.0, whose origins trace back to the Electronic Signatures Directive 1999/93/EC, the first EU-wide legal foundation for the use of electronic signatures in cross-border electronic transactions. As technological capabilities advanced, the initial eIDAS 1.0 framework was increasingly criticized for its limitations and lack of comprehensiveness. Emerging decentralized approaches further exposed these shortcomings and introduced the possibility of integrating innovative identity paradigms, such as Self-Sovereign Identity (SSI) models.
  In this article, we analyse key provisions of the eIDAS 2.0 Regulation and its accompanying recitals, drawing on existing literature to identify legislative gaps and implementation challenges. Furthermore, we examine the European Digital Identity Architecture and Reference Framework (ARF), assessing its proposed guidelines and evaluating the extent to which its emerging implementations align with SSI principles.

</details>


### [10] [AgenticSCR: An Autonomous Agentic Secure Code Review for Immature Vulnerabilities Detection](https://arxiv.org/abs/2601.19138)
*Wachiraphan Charoenwet,Kla Tantithamthavorn,Patanamon Thongtanunam,Hong Yi Lin,Minwoo Jeong,Ming Wu*

Main category: cs.CR

TL;DR: AgenticSCR：一种基于智能体AI的预提交阶段安全代码审查系统，通过结合LLM、自主决策、工具调用和代码导航，显著提升对不成熟漏洞的检测能力。


<details>
  <summary>Details</summary>
Motivation: 预提交阶段的安全代码审查需要在严格延迟和有限上下文约束下早期发现漏洞。现有SAST工具噪声大且常遗漏上下文相关的不成熟漏洞，而独立LLM受上下文窗口限制且缺乏显式工具使用。智能体AI结合LLM与自主决策、工具调用和代码导航，为预提交安全代码审查提供了有前景的替代方案。

Method: 提出AgenticSCR系统，这是一种用于预提交阶段安全代码审查的智能体AI，通过安全导向的语义记忆增强，专门检测不成熟漏洞。使用专门为预提交安全代码审查定制的自有基准数据集，实证评估系统在定位、检测和解释不成熟漏洞方面的准确性。

Result: AgenticSCR相比静态LLM基线实现了至少153%的相对正确代码审查评论百分比提升，并显著超越SAST工具。在五类漏洞中的四类中生成更多正确评论，始终显著优于所有其他基线方法。

Conclusion: 研究结果凸显了智能体安全代码审查的重要性，为不成熟漏洞检测这一新兴研究领域铺平了道路。AgenticSCR展示了智能体AI在预提交阶段安全代码审查中的有效性。

Abstract: Secure code review is critical at the pre-commit stage, where vulnerabilities must be caught early under tight latency and limited-context constraints. Existing SAST-based checks are noisy and often miss immature, context-dependent vulnerabilities, while standalone Large Language Models (LLMs) are constrained by context windows and lack explicit tool use. Agentic AI, which combine LLMs with autonomous decision-making, tool invocation, and code navigation, offer a promising alternative, but their effectiveness for pre-commit secure code review is not yet well understood. In this work, we introduce AgenticSCR, an agentic AI for secure code review for detecting immature vulnerabilities during the pre-commit stage, augmented by security-focused semantic memories. Using our own curated benchmark of immature vulnerabilities, tailored to the pre-commit secure code review, we empirically evaluate how accurate is our AgenticSCR for localizing, detecting, and explaining immature vulnerabilities. Our results show that AgenticSCR achieves at least 153% relatively higher percentage of correct code review comments than the static LLM-based baseline, and also substantially surpasses SAST tools. Moreover, AgenticSCR generates more correct comments in four out of five vulnerability types, consistently and significantly outperforming all other baselines. These findings highlight the importance of Agentic Secure Code Review, paving the way towards an emerging research area of immature vulnerability detection.

</details>


### [11] [LLMs Can Unlearn Refusal with Only 1,000 Benign Samples](https://arxiv.org/abs/2601.19231)
*Yangyang Guo,Ziwei Xu,Si Liu,Zhiming Zheng,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 研究发现大语言模型安全对齐存在新漏洞：模型拒绝回答不安全查询时的固定前缀模式（如"I'm sorry"）可被利用，通过仅用1000个良性样本进行拒绝前缀微调，就能让模型"忘记"如何拒绝，从而降低安全防护能力。


<details>
  <summary>Details</summary>
Motivation: 现有对齐的大语言模型主要通过固定前缀模式（如"I'm sorry"）来拒绝不安全查询，这种僵化的拒绝模式可能是一个安全漏洞。研究旨在探索这种拒绝模式是否容易被攻击，以及当前的安全对齐是否过度依赖令牌序列记忆而非真正的推理能力。

Method: 提出"拒绝遗忘"技术：仅使用1000个良性样本对LLMs进行微调，每个响应前都添加拒绝前缀。通过破坏模型的拒绝完成路径，使模型忘记如何拒绝有害指令。该方法在16个LLMs上测试，包括Llama、Qwen、Gemma等开源模型以及Gemini、GPT等闭源模型。

Result: 实验结果显示，先前对齐的LLMs的安全评分出现一致且显著的下降。验证了这种效果不能归因于普通微调或随机前缀效应。研究还提供了理论证明支持这一直觉。

Conclusion: 当前的安全对齐可能过度依赖令牌序列记忆而非推理能力，这暴露了现有安全机制的脆弱性。研究结果呼吁未来工作超越简单的拒绝机制，开发更稳健的安全对齐方法。

Abstract: This study reveals a previously unexplored vulnerability in the safety alignment of Large Language Models (LLMs). Existing aligned LLMs predominantly respond to unsafe queries with refusals, which often begin with a fixed set of prefixes (I'm sorry). We demonstrate that this rigid refusal pattern is a vulnerability and introduce a novel \textbf{refusal unlearning} technique that exploits it. Specifically, we fine-tune LLMs using merely 1,000 benign samples, where each response is prepended with a refusal prefix. The underlying intuition is to disrupt the refusal completion pathway, thereby driving the model to forget how to refuse while following harmful instructions. This intuition is further supported by theoretical proofs. We apply this approach to a total of 16 LLMs, including various open-source models from Llama, Qwen, and Gemma families, as well as closed-source models such as Gemini and GPT. Experimental results show that the safety scores of previously aligned LLMs degrade both consistently and substantially. Importantly, we verify that the observed gain cannot be attributed to plain fine-tuning or random prefix effects. Our findings suggest that current safety alignment may rely heavily on token sequence memorization rather than reasoning, motivating future work beyond simple refusal mechanisms. Code has been released: https://github.com/guoyang9/refusal-unlearning.

</details>


### [12] [LLM-Assisted Authentication and Fraud Detection](https://arxiv.org/abs/2601.19684)
*Emunah S-S. Chan,Aldar C-F. Chan*

Main category: cs.CR

TL;DR: 该研究提出两种LLM增强的安全解决方案：基于语义正确性而非精确匹配的认证机制，以及基于RAG的欺诈检测管道，显著提升了安全系统的可用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统知识型认证依赖精确字符串匹配，无法适应人类记忆和语言的自然变化；欺诈检测系统难以跟上快速演变的诈骗行为，导致高误报率和频繁的模型重训练需求。

Method: 1) LLM辅助认证机制：通过文档分割和混合评分方法（结合LLM判断与余弦相似度）评估语义正确性而非精确措辞；2) RAG欺诈检测管道：将LLM推理基于精选证据以减少幻觉，无需模型重训练即可适应新兴诈骗模式。

Result: 认证系统接受99.5%的合法非精确答案，同时保持0.1%的误接受率；RAG增强的欺诈检测将误报率从17.2%降低到35%。

Conclusion: LLM能显著提升安全流程的可用性和鲁棒性，为认证和欺诈检测提供更适应性强、可解释且符合人类认知的方法。

Abstract: User authentication and fraud detection face growing challenges as digital systems expand and adversaries adopt increasingly sophisticated tactics. Traditional knowledge-based authentication remains rigid, requiring exact word-for-word string matches that fail to accommodate natural human memory and linguistic variation. Meanwhile, fraud-detection pipelines struggle to keep pace with rapidly evolving scam behaviors, leading to high false-positive rates and frequent retraining cycles required. This work introduces two complementary LLM-enabled solutions, namely, an LLM-assisted authentication mechanism that evaluates semantic correctness rather than exact wording, supported by document segmentation and a hybrid scoring method combining LLM judgement with cosine-similarity metrics and a RAG-based fraud-detection pipeline that grounds LLM reasoning in curated evidence to reduce hallucinations and adapt to emerging scam patterns without model retraining. Experiments show that the authentication system accepts 99.5% of legitimate non-exact answers while maintaining a 0,1% false-acceptance rate, and that the RAG-enhanced fraud detection reduces false positives from 17.2% to 35%. Together, these findings demonstrate that LLMs can significantly improve both usability and robustness in security workflows, offering a more adaptive , explainable, and human-aligned approach to authentication and fraud detection.

</details>


### [13] [RvB: Automating AI System Hardening via Iterative Red-Blue Games](https://arxiv.org/abs/2601.19726)
*Lige Huang,Zicheng Liu,Jie Zhang,Lewen Yan,Dongrui Liu,Jing Shao*

Main category: cs.CR

TL;DR: 提出Red Team vs. Blue Team框架，通过对抗性互动实现AI系统的动态强化，无需参数更新，在代码加固和防护优化任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型同时具有攻击和防御能力，但AI安全领域缺乏统一的动态迭代对抗适应强化框架。现有方法无法实现持续的系统硬化。

Method: 提出Red Team vs. Blue Team框架，将其建模为无需训练、顺序、不完全信息的博弈过程。红队发现漏洞，蓝队学习有效解决方案而不更新参数。

Result: 在两个挑战性领域验证：动态代码加固对抗CVE和防护优化对抗越狱攻击。防御成功率分别达到90%和45%，误报率接近0%，显著超越基线方法。

Conclusion: 迭代对抗互动框架为AI系统持续自动化硬化建立了实用范式，蓝队能够学习基本防御原则而非仅针对特定攻击过拟合。

Abstract: The dual offensive and defensive utility of Large Language Models (LLMs) highlights a critical gap in AI security: the lack of unified frameworks for dynamic, iterative adversarial adaptation hardening. To bridge this gap, we propose the Red Team vs. Blue Team (RvB) framework, formulated as a training-free, sequential, imperfect-information game. In this process, the Red Team exposes vulnerabilities, driving the Blue Team to learning effective solutions without parameter updates. We validate our framework across two challenging domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. Our empirical results show that this interaction compels the Blue Team to learn fundamental defensive principles, leading to robust remediations that are not merely overfitted to specific exploits. RvB achieves Defense Success Rates of 90\% and 45\% across the respective tasks while maintaining near 0\% False Positive Rates, significantly surpassing baselines. This work establishes the iterative adversarial interaction framework as a practical paradigm that automates the continuous hardening of AI systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Agentic Business Process Management Systems](https://arxiv.org/abs/2601.18833)
*Marlon Dumas,Fredrik Milani,David Chapela-Campa*

Main category: cs.AI

TL;DR: 本文探讨生成式AI和智能体AI如何推动BPM从自动化向自主化转型，提出集成自主性、推理和学习能力的A-BPMS架构愿景。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和智能体AI的出现为BPM领域带来了新的变革浪潮，但这一波与以往不同，它从自动化转向自主化，从设计驱动转向数据驱动。需要探索如何利用流程挖掘技术为智能体提供感知、推理和行动的基础。

Method: 基于2025年AI for BPM研讨会主题演讲，提出A-BPMS的架构愿景：集成自主性、推理和学习能力的新型平台，支持从人工驱动到完全自主的连续过程谱系。

Result: 流程挖掘为智能体感知流程状态、推理改进机会和执行优化行动奠定了基础。A-BPMS需要重新定义流程自动化和治理的边界。

Conclusion: 生成式AI和智能体AI正在推动BPM向自主化转型，A-BPMS将成为集成自主性、推理和学习能力的新一代平台，支持从人工到完全自主的连续过程管理。

Abstract: Since the early 90s, the evolution of the Business Process Management (BPM) discipline has been punctuated by successive waves of automation technologies. Some of these technologies enable the automation of individual tasks, while others focus on orchestrating the execution of end-to-end processes. The rise of Generative and Agentic Artificial Intelligence (AI) is opening the way for another such wave. However, this wave is poised to be different because it shifts the focus from automation to autonomy and from design-driven management of business processes to data-driven management, leveraging process mining techniques. This position paper, based on a keynote talk at the 2025 Workshop on AI for BPM, outlines how process mining has laid the foundations on top of which agents can sense process states, reason about improvement opportunities, and act to maintain and optimize performance. The paper proposes an architectural vision for Agentic Business Process Management Systems (A-BPMS): a new class of platforms that integrate autonomy, reasoning, and learning into process management and execution. The paper contends that such systems must support a continuum of processes, spanning from human-driven to fully autonomous, thus redefining the boundaries of process automation and governance.

</details>


### [15] [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)
*Urban Skvorc,Niki van Stein,Moritz Seiler,Britta Grimme,Thomas Bäck,Heike Trautmann*

Main category: cs.AI

TL;DR: 使用LLM在进化循环中生成具有特定景观特征的优化问题，通过ELA属性预测器评估，引入ELA空间适应度共享机制增加多样性，生成的问题扩展了BBOB实例空间


<details>
  <summary>Details</summary>
Motivation: 现有连续黑盒优化基准测试套件（如BBOB）的结构多样性有限，限制了基准测试的有效性。需要创建具有明确高层景观特征的优化问题来丰富基准测试集。

Method: 使用LLaMEA框架，通过自然语言描述目标属性（多模态性、可分离性、盆地大小同质性、搜索空间同质性、全局-局部最优对比度）指导LLM生成问题代码。在进化循环中使用基于ELA的属性预测器对候选问题评分，引入ELA空间适应度共享机制增加种群多样性并避免冗余景观。

Result: 生成的问题确实表现出预期的结构特征，通过盆地吸引分析、统计测试和视觉检查验证。t-SNE嵌入显示这些生成的问题扩展了BBOB实例空间而不是形成无关的聚类。最终创建了一个广泛、可解释、可复现的基准问题库。

Conclusion: 该方法成功生成了具有特定景观特征的优化问题，扩展了现有基准测试套件的多样性，为景观分析和自动化算法选择等下游任务提供了有价值的基准问题库。

Abstract: Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.

</details>


### [16] [Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System](https://arxiv.org/abs/2601.18897)
*Qusai Khaled,Bahjat Mallak,Uzay Kaymak,Laura Genga*

Main category: cs.AI

TL;DR: 开发了一种基于区间二型自适应神经模糊推理系统（IT2-ANFIS）的废水处理厂能耗预测方法，提供可解释的不确定性量化，而非传统的点预测。


<details>
  <summary>Details</summary>
Motivation: 废水处理厂消耗全球1-3%的电力，准确的能耗预测对运营优化和可持续发展至关重要。现有机器学习模型仅提供点预测，缺乏对安全关键基础设施中风险感知决策至关重要的可解释不确定性量化。

Method: 提出区间二型自适应神经模糊推理系统（IT2-ANFIS），通过模糊规则结构生成可解释的预测区间。该方法在三个层次上分解不确定性：特征级（识别引入模糊性的变量）、规则级（揭示局部模型的置信度）和实例级（量化整体预测不确定性）。

Result: 在墨尔本水务公司东部处理厂数据集上验证，IT2-ANFIS达到与一阶ANFIS相当的预测性能，同时显著减少训练运行间的方差，并提供可解释的不确定性估计，将预测置信度直接与运营条件和输入变量关联。

Conclusion: IT2-ANFIS框架为废水处理厂能耗预测提供了可解释的不确定性量化方法，支持风险感知决策，有助于优化运营并提高能源效率，同时保持与现有方法相当的预测准确性。

Abstract: Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.

</details>


### [17] [RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures](https://arxiv.org/abs/2601.18924)
*Andrew Jaffe,Noah Reicin,Jinho D. Choi*

Main category: cs.AI

TL;DR: RIFT基准测试揭示LLMs在非顺序指令结构下性能显著下降，表明现有架构将指令跟随视为顺序模式而非推理技能


<details>
  <summary>Details</summary>
Motivation: 现有基准测试将任务复杂性与结构顺序混为一谈，难以分离提示拓扑对性能的影响，需要专门测试LLMs在不同指令结构下的表现

Method: 引入RIFT基准，使用重新表述的Jeopardy!问答对，测试LLMs在线性提示（顺序进行）和跳跃提示（相同内容但需要非顺序遍历）两种结构下的表现

Result: 在6个最先进开源LLMs的10,000次评估中，跳跃条件下准确率下降高达72%，约50%的失败源于指令顺序违反和语义漂移

Conclusion: 当前架构将指令跟随内化为顺序模式而非推理技能，结构敏感性是现有架构的基本限制，对工作流自动化和多智能体系统等需要非顺序控制流的应用有直接影响

Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.

</details>


### [18] [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)
*Qiyuan Xu,Xiaokun Luan,Renxi Wang,Joshua Ong Jun Leang,Peixin Wang,Haonan Li,Wenda Li,Conrad Watt*

Main category: cs.AI

TL;DR: 本文介绍了NTP4VC，这是首个针对验证条件自动证明的真实世界多语言基准测试，从Linux和Contiki-OS等实际项目中提取验证条件，评估LLM在程序验证中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 程序验证中的验证条件自动证明是主要瓶颈，现有自动定理证明器无法处理困难的验证条件，导致需要大量手动证明。虽然神经定理证明在数学竞赛中取得成功，但在程序验证特别是验证条件证明方面的应用尚未充分探索，缺乏专门针对这一基本瓶颈的基准测试。

Method: 从Linux和Contiki-OS内核等真实项目出发，利用工业级工具链（Why3和Frama-C）生成在Isabelle、Lean和Rocq等不同形式语言中语义等价的测试用例，构建首个真实世界多语言基准测试NTP4VC。在此基础上评估通用大语言模型和针对定理证明微调的大语言模型在验证条件证明任务上的表现。

Result: 评估结果表明，虽然大语言模型在验证条件证明方面显示出潜力，但在程序验证方面仍面临重大挑战，揭示了当前存在的巨大差距和未来研究的机会。

Conclusion: NTP4VC填补了验证条件自动证明基准测试的空白，为神经定理证明在程序验证领域的应用提供了重要评估平台，尽管大语言模型展现出一定潜力，但程序验证任务仍面临显著挑战，需要进一步研究。

Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.

</details>


### [19] [More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas](https://arxiv.org/abs/2601.19082)
*Trung-Kiet Huynh,Dao-Sy Duy-Minh,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Nguyen Lam Phu Quy,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Pham Phu Hoa,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.AI

TL;DR: 研究LLM在重复社会困境中的战略行为，通过收益缩放的囚徒困境分析激励强度和语言环境对LLM策略的影响，发现一致的行为模式、激励敏感的调节策略和跨语言差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在交互式和多智能体环境中越来越多地作为自主智能体行动，理解其战略行为对于安全性、协调性以及AI驱动的社会和经济系统至关重要。需要研究收益大小和语言环境如何塑造LLM在重复社会困境中的策略。

Method: 使用收益缩放的囚徒困境来隔离对激励强度的敏感性，在不同模型和语言中观察行为模式。训练监督分类器对典型重复博弈策略进行分类，并将其应用于LLM决策，以解释行为动态。

Result: 观察到跨模型和语言的一致行为模式，包括激励敏感的调节策略和跨语言差异。语言框架有时匹配或超过架构效应的影响。揭示了系统性的、模型和语言依赖的行为意图。

Conclusion: 研究结果为审计LLM作为战略智能体提供了统一框架，并突出了合作偏见对AI治理和多智能体系统设计的直接影响。

Abstract: As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.

</details>


### [20] [Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation](https://arxiv.org/abs/2601.19112)
*Nanhan Shen,Zhilei Liu*

Main category: cs.AI

TL;DR: UA-3DTalk提出了一种不确定性感知的3D情感说话人脸合成方法，通过情感先验蒸馏解决现有方法在音频-视觉情感对齐和多视角融合方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D情感说话人脸合成方法存在两个关键问题：1) 音频-视觉情感对齐不佳，表现为音频情感提取困难和对情感微表情控制不足；2) 采用一刀切的多视角融合策略，忽略了不确定性和特征质量差异，影响渲染质量。

Method: 提出UA-3DTalk框架，包含三个核心模块：1) 先验提取模块，将音频解耦为内容同步特征和个性化特征；2) 情感蒸馏模块，采用多模态注意力加权融合机制和4D高斯编码，实现细粒度音频情感提取和情感微表情精确控制；3) 基于不确定性的形变模块，使用不确定性块估计视角特定的不确定性，实现自适应多视角融合，并通过多头解码器优化高斯基元。

Result: 在常规和情感数据集上的实验表明，UA-3DTalk在情感对齐（E-FID提升5.2%）、唇部同步（SyncC提升3.1%）和渲染质量（LPIPS提升0.015）方面均优于DEGSTalk和EDTalk等最先进方法。

Conclusion: UA-3DTalk通过不确定性感知的多视角融合和情感先验蒸馏，有效解决了3D情感说话人脸合成中的关键挑战，在情感对齐、唇部同步和渲染质量方面取得了显著提升。

Abstract: Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk

</details>


### [21] [Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach](https://arxiv.org/abs/2601.19122)
*Weiran Guo,Bing Bo,Shaoxiang Wu,Jingsheng Yang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的对抗性数据增强方法，通过训练查询模型生成对抗性查询来挑战函数调用模型，以提升LLM函数调用能力的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM函数调用能力的方法依赖于手动标注或模型自动生成的数据进行微调，缺乏针对性设计，受限于固定模式和数据分布，限制了函数调用LLM的泛化性和鲁棒性。

Method: 提出基于强化学习的对抗性数据增强方法，训练查询模型生成专门挑战函数调用模型的对抗性查询。采用零和博弈框架，查询模型和函数调用模型进行迭代交替训练。

Result: 该方法能够系统性地识别和针对函数调用LLM的弱点，为开发更鲁棒的函数调用模型提供了系统化方法。

Conclusion: 提出的对抗性数据增强方法通过强化学习训练查询模型生成对抗性查询，有效提升了LLM函数调用能力的鲁棒性，为识别和纠正LLM与外部工具交互的弱点提供了系统化途径。

Abstract: Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.

</details>


### [22] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: TS-Debate：一个专门针对时间序列推理的多模态协作多智能体辩论框架，通过专家分工和结构化辩论协议提升零样本性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列分析中存在数值保真度不足、模态干扰和跨模态整合困难等问题，需要更专业的方法来提升推理能力

Method: 采用多智能体辩论框架，分配专门的专家智能体处理文本上下文、视觉模式和数值信号，通过结构化辩论协议协调交互，并引入验证-冲突-校准机制进行程序化验证

Result: 在三个公共基准测试的20个任务中，TS-Debate相比强基线（包括标准多模态辩论）实现了持续且显著的性能提升

Conclusion: TS-Debate框架通过模态专业化、协作辩论和程序化验证，有效解决了时间序列推理中的数值幻觉问题，无需任务特定微调即可提升性能

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


### [23] [Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement](https://arxiv.org/abs/2601.19170)
*Wangyang Ying,Yanchi Liu,Xujiang Zhao,Wei Cheng,Zhengzhang Chen,Wenchao Yu,Yanjie Fu,Haifeng Chen*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，将自然语言中的工作流程提取为程序图，通过结构化和逻辑反馈迭代优化


<details>
  <summary>Details</summary>
Motivation: 从自然语言自动提取工作流程作为程序图具有前景但尚未充分探索，需要同时保证结构有效性和逻辑一致性。现有大语言模型虽然显示出潜力，但常常产生结构不良或逻辑流误解的问题。

Method: 提出一个多智能体框架，将程序图提取建模为多轮推理过程，包含三个阶段：1) 图构建智能体进行图提取；2) 模拟智能体诊断和解释结构缺陷；3) 语义智能体对齐流程逻辑与源文本语义。重要反馈以自然语言形式注入后续提示，实现可解释和可控的优化。

Result: 实验表明，该框架在结构正确性和逻辑一致性方面相比强基线方法取得了显著改进。

Conclusion: 该多智能体框架通过模块化设计，使各智能体能够针对不同类型的错误进行优化，无需监督或参数更新，有效提升了从自然语言提取程序图的质量。

Abstract: Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.

</details>


### [24] [CollectiveKV: Decoupling and Sharing Collaborative Information in Sequential Recommendation](https://arxiv.org/abs/2601.19178)
*Jingyu Li,Zhaocheng Du,Qianhui Zhu,kaiyuan Li,Zhicheng Zhang,Song-Li Wu,Chaolang Li,Pengwen Dai*

Main category: cs.AI

TL;DR: 提出CollectiveKV方法，通过跨用户KV共享机制将KV缓存压缩到原始大小的0.8%，在保持甚至提升模型性能的同时解决序列推荐系统中KV缓存存储开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 序列推荐系统中Transformer注意力机制的计算复杂度随序列长度增长，KV缓存技术虽能降低推理延迟，但在大规模用户和长历史序列场景下会带来巨大的存储开销。研究发现不同用户的KV序列存在显著相似性，表明KV中存在协作信号，且大部分信息可跨用户共享。

Method: 提出CollectiveKV跨用户KV共享机制：1）通过奇异值分解分析发现KV信息可分为跨用户共享部分和用户特定部分；2）使用可学习的全局KV池捕获跨用户共享信息；3）推理时每个用户从池中检索高维共享KV，与低维用户特定KV拼接得到最终KV。

Result: 在5个序列推荐模型和3个数据集上的实验表明，该方法能将KV缓存压缩到原始大小的0.8%，同时保持甚至提升模型性能。

Conclusion: CollectiveKV通过利用KV中的协作信号，实现了高效的跨用户KV共享，显著减少了序列推荐系统中KV缓存的存储开销，为解决长序列推荐系统的延迟和存储挑战提供了有效方案。

Abstract: Sequential recommendation models are widely used in applications, yet they face stringent latency requirements. Mainstream models leverage the Transformer attention mechanism to improve performance, but its computational complexity grows with the sequence length, leading to a latency challenge for long sequences. Consequently, KV cache technology has recently been explored in sequential recommendation systems to reduce inference latency. However, KV cache introduces substantial storage overhead in sequential recommendation systems, which often have a large user base with potentially very long user history sequences. In this work, we observe that KV sequences across different users exhibit significant similarities, indicating the existence of collaborative signals in KV. Furthermore, we analyze the KV using singular value decomposition (SVD) and find that the information in KV can be divided into two parts: the majority of the information is shareable across users, while a small portion is user-specific. Motivated by this, we propose CollectiveKV, a cross-user KV sharing mechanism. It captures the information shared across users through a learnable global KV pool. During inference, each user retrieves high-dimensional shared KV from the pool and concatenates them with low-dimensional user-specific KV to obtain the final KV. Experiments on five sequential recommendation models and three datasets show that our method can compress the KV cache to only 0.8% of its original size, while maintaining or even enhancing model performance.

</details>


### [25] [CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning](https://arxiv.org/abs/2601.19193)
*Van-Quang Nguyen,Takayuki Okatani*

Main category: cs.AI

TL;DR: CoReTab是一个代码驱动的推理框架，通过将多步推理与可执行Python代码结合，为多模态表格理解提供可扩展、可解释且可自动验证的标注，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表格理解数据集（如MMTab）主要提供简短的事实性答案，缺乏明确的多步推理监督。在这些数据集上训练的模型通常生成简短回答，准确率不足且可解释性有限，无法清晰展示模型如何得出最终答案。

Method: 引入CoReTab代码驱动推理框架，通过将多步推理与可执行Python代码耦合来生成可扩展、可解释且可自动验证的标注。使用该框架构建了包含115K个已验证样本的数据集（平均每个响应529个token），并通过三阶段流程对开源MLLMs进行微调。

Result: 在17个MMTab基准测试（涵盖表格问答、事实验证和表格结构理解）上评估CoReTab训练的模型，相比MMTab训练的基线模型分别取得了+6.2%、+5.7%和+25.6%的显著提升，同时生成透明且可验证的推理轨迹。

Conclusion: CoReTab作为一个稳健且可泛化的监督框架，有效改进了多模态表格理解中的多步推理能力，提供了透明和可验证的推理过程。

Abstract: Existing datasets for multimodal table understanding, such as MMTab, primarily provide short factual answers without explicit multi-step reasoning supervision. Models trained on these datasets often generate brief responses that offers insufficient accuracy and limited interpretability into how these models arrive at the final answer. We introduce CoReTab, a code-driven reasoning framework that produces scalable, interpretable, and automatically verifiable annotations by coupling multi-step reasoning with executable Python code. Using the CoReTab framework, we curate a dataset of 115K verified samples averaging 529 tokens per response and fine-tune open-source MLLMs through a three-stage pipeline. We evaluate the resulting model trained on CoReTab across 17 MMTab benchmarks spanning table question answering, fact verification, and table structure understanding. Our model achieves significant gains of +6.2%, +5.7%, and +25.6%, respectively, over MMTab-trained baselines, while producing transparent and verifiable reasoning traces. These results establish CoReTab as a robust and generalizable supervision framework for improving multi-step reasoning in multimodal table understanding.

</details>


### [26] [MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning](https://arxiv.org/abs/2601.19204)
*Zhixi Cai,Fucai Ke,Kevin Leo,Sukai Huang,Maria Garcia de la Banda,Peter J. Stuckey,Hamid Rezatofighi*

Main category: cs.AI

TL;DR: MATA是一个用于视觉推理的多智能体分层可训练自动机系统，通过可训练的超智能体选择顶层状态转移，每个智能体运行基于规则的子自动机，共享内存实现透明执行历史，在多个视觉推理基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型具有强大的感知能力，但其隐式推理难以解释，在复杂查询时容易产生幻觉。组合方法提高了可解释性，但大多数依赖单一智能体或手工制作的流水线，无法决定何时在互补智能体之间协作或在重叠智能体之间竞争。

Method: 提出MATA（多智能体分层可训练自动机），作为分层有限状态自动机用于视觉推理，顶层转移由可训练的超智能体选择。每个智能体对应超自动机中的一个状态，运行小型基于规则的子自动机进行可靠的微控制。所有智能体读写共享内存，产生透明的执行历史。通过构建转移轨迹树并转换为内存到下一状态对，创建MATA-SFT-90K数据集用于监督微调。

Result: 在多个视觉推理基准测试中，MATA相比单一模型和组合基线方法取得了最先进的结果。微调后的LLM作为转移策略能够理解查询和智能体能力，有效选择最优智能体解决任务。

Conclusion: MATA通过多智能体分层可训练自动机框架，在保持可解释性的同时提高了视觉推理的性能，解决了现有方法在复杂查询中的幻觉问题，并通过共享内存和透明执行历史增强了系统的可解释性。

Abstract: Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.

</details>


### [27] [GAVEL: Towards rule-based safety through activation monitoring](https://arxiv.org/abs/2601.19768)
*Shir Rozenfeld,Rahul Pankajakshan,Itay Zloczower,Eyal Lenga,Gilad Gressel,Yisroel Mirsky*

Main category: cs.AI

TL;DR: 论文提出基于规则的激活安全新范式，将LLM激活建模为可解释的认知元素，通过组合规则实现高精度、可定制的有害行为检测，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的安全监控方法存在精度低、灵活性差、缺乏可解释性等问题，无法有效检测文本层面不明显的有害行为，需要更精确、可定制、可解释的安全框架。

Method: 提出基于规则的激活安全范式：1）将激活建模为细粒度、可解释的认知元素（如"发出威胁"、"支付处理"）；2）构建基于认知元素的谓词规则框架；3）实时检测规则违反，支持动态配置和更新，无需重新训练模型或检测器。

Result: 基于规则的激活安全方法提高了检测精度，支持领域定制化，为可扩展、可解释、可审计的AI治理奠定了基础。作者将发布开源框架GAVEL和自动化规则创建工具。

Conclusion: 基于规则的激活安全通过认知元素建模和组合规则，实现了更精确、灵活、可解释的有害行为检测，为AI安全治理提供了新的技术路径。

Abstract: Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.

</details>


### [28] [Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection](https://arxiv.org/abs/2601.19245)
*Yongxin Deng,Zhen Fang,Yixuan Li,Ling Chen*

Main category: cs.AI

TL;DR: 本文提出了一种可泛化的幻觉检测方法SpikeScore，通过量化多轮对话中的不确定性波动来区分幻觉与非幻觉响应，在跨域泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在同域数据上表现良好，但在跨域泛化方面表现不佳。本文研究了一个重要但被忽视的问题——可泛化幻觉检测（GHD），旨在在单一域数据上训练幻觉检测器，同时确保在多样相关域上的鲁棒性能。

Method: 通过模拟LLM初始响应后的多轮对话，发现幻觉引发的多轮对话在不同域中普遍表现出比事实性对话更大的不确定性波动。基于此现象，提出了SpikeScore评分，量化多轮对话中的突发波动。通过理论分析和实证验证，证明SpikeScore在幻觉与非幻觉响应之间实现了强大的跨域可分离性。

Result: 在多个LLM和基准测试上的实验表明，基于SpikeScore的检测方法在跨域泛化方面优于代表性基线方法，并超越了先进的面向泛化的方法，验证了该方法在跨域幻觉检测中的有效性。

Conclusion: SpikeScore通过量化多轮对话中的不确定性波动，为可泛化幻觉检测提供了一种有效的解决方案，在跨域场景中表现出优越的性能，为解决LLM在实际应用中的幻觉检测问题提供了新思路。

Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.

</details>


### [29] [GLOVE: Global Verifier for LLM Memory-Environment Realignment](https://arxiv.org/abs/2601.19249)
*Xingkun Yin,Hongyang Du*

Main category: cs.AI

TL;DR: GLOVE框架通过主动探测记忆与新鲜观察的不一致性，建立相对真理概念，实现无监督记忆更新，在动态漂移环境中显著提升智能体成功率


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆增强方法依赖外部评估器或内部认知来验证记忆有效性，但这些假设在动态漂移的实际环境中往往失效，需要更鲁棒的记忆验证机制

Method: 提出GLOVE框架，通过主动探测检索记忆与新鲜观察之间的不一致性，建立相对真理概念，实现无需真实监督或强模型内省的记忆验证与更新

Result: 在增强环境漂移的网页导航、规划和控制基准测试中，GLOVE显著提高了智能体成功率，展示了在非平稳环境中的鲁棒性

Conclusion: GLOVE为LLM记忆系统引入了新的设计维度，通过相对真理概念实现记忆环境重对齐，为构建能够自我进化的认知智能体提供了稳健路径

Abstract: Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.

</details>


### [30] [RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization](https://arxiv.org/abs/2601.19404)
*Hongzhu Yi,Xinming Wang,Zhenghao zhang,Tianyu Zong,Yuanxiang Wang,Jun Xie,Tao Yu,Haopeng Jin,Zhepeng Wang,Kaixin Xu,Feng Chen,Jiahuan Chen,Yujia Yang,Zhenyu Guan,Bingkang Shi,Jungang Xu*

Main category: cs.AI

TL;DR: RPO是一种部分推理优化的强化微调算法，通过仅生成推理路径的后缀来减少约95%的token生成，显著降低训练时间开销。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调算法需要从输入查询开始生成完整的推理轨迹，这在训练rollout阶段会产生巨大的计算开销。为了解决这个问题，作者分析了推理路径不同部分对最终结果正确性的影响。

Method: 提出RPO（Reinforcement Fine-Tuning with Partial Reasoning Optimization），这是一种即插即用的强化微调算法。与传统方法生成完整推理路径不同，RPO使用经验缓存来生成推理路径的后缀进行训练。

Result: RPO在训练rollout阶段减少了约95%的token生成，大大降低了理论时间开销。对于1.5B模型减少了90%的训练时间，对于7B模型减少了72%的训练时间。同时可以与GRPO和DAPO等典型算法集成，在保持性能的同时实现训练加速。

Conclusion: RPO通过部分推理优化有效解决了强化微调中的计算开销问题，显著加速训练过程，同时保持模型性能，为大规模语言模型的强化微调提供了高效解决方案。

Abstract: Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.

</details>


### [31] [Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach](https://arxiv.org/abs/2601.19527)
*Temirbolat Maratuly,Pakizar Shamoi,Timur Samigulin*

Main category: cs.AI

TL;DR: 本文提出了一种结合自定义数字孪生的模糊专家系统，用于净化酸性水处理过程，通过模拟人类推理维持关键参数在期望水平，简化控制策略使非专业人员也能有效操作。


<details>
  <summary>Details</summary>
Motivation: 酸性水净化对于减少排放、降低腐蚀风险、实现处理水在工业或家庭应用中的再利用以及最终降低运营成本至关重要。自动化净化过程有助于减少工人伤害风险。原油中的酸性成分如硫化氢、二氧化碳等会在加工过程中释放到酸性水中，如处理不当会造成严重环境威胁并加速管道设备腐蚀。

Method: 开发了模糊专家系统与自定义数字孪生相结合的控制策略。数字孪生使用Honeywell UniSim Design R492开发以准确模拟工业行为，阀门动态通过MATLAB系统辨识建模，使用OPC DA实现模拟器与控制器间的实时数据交换。模糊控制器对两个阀门应用分程控制，在21种不同初始压力条件下使用5种不同的去模糊化策略进行测试，共105个测试场景。

Result: 系统性能使用基于误差的指标（MSE、RMSE、MAE、IAE、ISE、ITAE）和动态响应指标（超调量、欠调量、上升时间、下降时间、稳定时间、稳态误差）进行评估。基于Python Streamlit框架开发了Web仿真界面。

Conclusion: 虽然本文以酸性水处理为例进行演示，但所提出的模糊专家系统具有通用性，控制策略简单直观，允许初级或非专业人员有效与系统交互。

Abstract: Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.

</details>


### [32] [Benchmarks Saturate When The Model Gets Smarter Than The Judge](https://arxiv.org/abs/2601.19532)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.AI

TL;DR: Omni-MATH-2是一个经过人工修订的数学数据集，包含4181个精确答案问题和247个带标签的非标准问题，用于减少数据集噪声并提供更准确的模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型基准测试存在数据集不准确和评估方法不可靠的问题，这些噪声影响了模型性能评估的有效性。作者旨在通过创建高质量、经过验证的数据集来解决这些问题。

Method: 手动审核Omni-MATH数据集，确保LaTeX可编译性、可解性和可验证性，添加缺失的图形或信息，标记需要证明、估计或图像的问题，并移除杂乱内容。创建了干净子集（4181个问题）和带标签子集（247个问题）。

Result: 数据集显著减少了数据集引起的噪声。通过比较GPT-5 mini和原始Omni-Judge，发现评估者之间存在显著差异。专家标注显示，在评估者分歧中，Omni-Judge在96.4%的情况下是错误的，表明其无法区分模型能力。随着问题难度增加，需要更胜任的评估者来防止评估错误掩盖模型间的真实差异。

Conclusion: 数据集质量和评估者可靠性对于开发准确的模型性能基准都至关重要。当前评估者未能识别带标签问题子集的失败模式，强调了需要同时改进数据集质量和评估方法。

Abstract: Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.

</details>


### [33] [Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search](https://arxiv.org/abs/2601.19622)
*Thomas Bömer,Nico Koltermann,Max Disselnmeyer,Bastian Amberg,Anne Meyer*

Main category: cs.AI

TL;DR: 本文提出A-CEoH框架，通过将A*算法代码融入提示词来增强LLM的上下文学习能力，自动生成A*搜索的启发式函数，在UPMP和SPP问题上表现优于专家设计的启发式。


<details>
  <summary>Details</summary>
Motivation: 传统启发式函数需要手工设计且依赖专家知识，而LLM和进化框架的发展为自动化启发式设计提供了可能。本文旨在扩展EoH框架，利用LLM的上下文学习能力自动生成高质量的A*搜索启发式函数。

Method: 提出A-CEoH（算法上下文EoH）框架，采用领域无关的提示增强策略，将A*算法代码融入提示词以增强LLM的上下文学习能力。通过进化框架自动生成启发式函数，并在UPMP（仓库物流问题）和SPP（滑块拼图问题）两个领域进行测试。

Result: 计算实验表明，A-CEoH能显著提高生成启发式的质量，在UPMP和SPP问题上甚至能超越专家设计的启发式函数。

Conclusion: A-CEoH框架通过将算法代码融入提示词，有效利用LLM的上下文学习能力，成功实现了高质量启发式函数的自动化生成，为A*搜索算法的启发式设计提供了新的自动化解决方案。

Abstract: Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evolutionary frameworks have opened the door to automating heuristic design. In this paper, we extend the Evolution of Heuristics (EoH) framework to investigate the automated generation of guiding heuristics for A* search. We introduce a novel domain-agnostic prompt augmentation strategy that includes the A* code into the prompt to leverage in-context learning, named Algorithmic - Contextual EoH (A-CEoH). To evaluate the effectiveness of A-CeoH, we study two problem domains: the Unit-Load Pre-Marshalling Problem (UPMP), a niche problem from warehouse logistics, and the classical sliding puzzle problem (SPP). Our computational experiments show that A-CEoH can significantly improve the quality of the generated heuristics and even outperform expert-designed heuristics.

</details>


### [34] [Agentic Design Patterns: A System-Theoretic Framework](https://arxiv.org/abs/2601.19752)
*Minh-Dung Dao,Quy Minh Le,Hoang Thanh Lam,Duc-Trong Le,Quoc-Viet Pham,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 本文提出了一种基于系统理论的AI智能体工程化方法，包括一个五子系统框架和12个设计模式，旨在解决现有智能体系统设计缺乏理论基础、不可靠和脆弱的问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型的发展推动了智能AI系统的兴起，但现有系统存在幻觉、推理能力差、设计随意等问题，导致应用不可靠和脆弱。现有的智能体设计模式分类缺乏严格的系统理论基础，难以实际实施。

Method: 提出两个主要贡献：1）一个系统理论框架，将智能AI系统解构为五个核心交互功能子系统：推理与世界模型、感知与接地、行动执行、学习与适应、智能体间通信；2）基于此架构，提出12个智能体设计模式，分为基础模式、认知与决策模式、执行与交互模式、适应与学习模式四类。

Result: 通过ReAct框架的案例研究展示了该框架的实用性，表明所提出的设计模式可以纠正系统性架构缺陷。该工作为研究人员和工程师提供了标准化的智能体设计语言和方法论。

Conclusion: 这项工作为智能体设计提供了基础语言和结构化方法论，能够标准化研究人员和工程师之间的智能体设计交流，从而构建更模块化、可理解和可靠的自主系统。

Abstract: With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.

</details>


### [35] [An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care](https://arxiv.org/abs/2601.19824)
*Andre Paulino de Lima,Paula Castro,Suzana Carvalho Vaz de Andrade,Rosa Maria Marcucci,Ruth Caldeira de Melo,Marcelo Garcia Manzato*

Main category: cs.AI

TL;DR: 该研究提出了一种用于老年初级保健的推荐模型，通过心理测量数据结构提供可视化解释，帮助护理专业人员制定个性化护理计划。


<details>
  <summary>Details</summary>
Motivation: 医疗环境中推荐系统面临多重挑战：缺乏公开临床数据、用户难以理解推荐原因、遵循推荐存在风险、以及效果不确定性。特别是在老年初级保健领域，随着人口老龄化，对信息技术支持的需求日益增长。

Method: 提出一种推荐模型，利用心理测量数据的结构特征，生成忠实于模型且能被护理专业人员理解的可视化解释。专注于老年初级保健这一细分领域，通过离线性能评估和用户研究验证模型效果。

Result: 在巴西研究合作伙伴收集的医疗数据集上进行了比较性离线性能评估，同时进行了用户研究评估模型生成的可视化解释的可解释性。结果表明该模型能够推进推荐系统在这一医疗细分领域的应用。

Conclusion: 提出的推荐模型能够帮助护理专业人员制定个性化护理计划，有望促进推荐系统在老年初级保健领域的应用，这一领域随着人口结构变化将面临更大的需求、机会和信息技术需求。

Abstract: There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.

</details>


### [36] [Routing End User Queries to Enterprise Databases](https://arxiv.org/abs/2601.19825)
*Saikrishna Sudarshan,Tanay Kulkarni,Manasi Patwardhan,Lovekesh Vig,Ashwin Srinivasan,Tanmay Tulsidas Verlekar*

Main category: cs.AI

TL;DR: 该论文研究了在多数据库企业环境中路由自然语言查询的任务，通过扩展现有NL-to-SQL数据集构建基准，提出模块化推理驱动的重排序策略，在各项指标上优于嵌入方法和直接LLM提示基线。


<details>
  <summary>Details</summary>
Motivation: 在多数据库企业环境中，随着数据库规模增大、领域重叠以及查询模糊性增加，路由自然语言查询变得越来越困难，需要更结构化、鲁棒的基于推理的解决方案。

Method: 提出模块化、推理驱动的重排序策略，明确建模模式覆盖、结构连通性和细粒度语义对齐，通过结构化推理改进路由性能。

Result: 该方法在所有指标上一致优于仅使用嵌入的方法和直接LLM提示基线，特别是在大规模、领域重叠的数据库存储库和模糊查询场景中表现更优。

Conclusion: 研究表明，在多数据库企业环境中，通过结构化推理建模模式覆盖、连通性和语义对齐的模块化方法，能够有效提升自然语言查询路由的性能和鲁棒性。

Abstract: We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.

</details>


### [37] [Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models](https://arxiv.org/abs/2601.19834)
*Jialong Wu,Xiaoying Zhang,Hongyi Yuan,Xiangcheng Zhang,Tianhao Huang,Changjing He,Chaoyi Deng,Renrui Zhang,Youbin Wu,Mingsheng Long*

Main category: cs.AI

TL;DR: 该研究首次系统探讨了视觉生成何时以及如何提升推理能力，提出视觉优越性假设：在物理世界相关任务中，视觉生成能更自然地作为世界模型，而纯语言世界模型会遭遇表征限制或先验知识不足的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在数学和编程等抽象领域已达到专家水平，但在物理和空间智能等需要丰富表征和先验知识的领域仍远落后于人类。统一多模态模型的出现引发了人们对基于互补多模态路径的类人推理的兴趣，但其优势尚不明确。

Method: 从世界模型视角出发，理论层面将内部世界建模形式化为CoT推理的核心组件，分析不同形式世界模型的区别；实证层面识别需要交错视觉-语言CoT推理的任务，构建新的评估套件VisWorld-Eval，并在最先进的统一多模态模型上进行控制实验。

Result: 实验表明，在有利于视觉世界建模的任务上，交错CoT显著优于纯语言CoT，但在其他任务上没有明显优势。这验证了视觉优越性假设。

Conclusion: 该研究阐明了多模态世界建模对于构建更强大、更类人的多模态AI的潜力，为理解视觉生成在推理中的作用提供了原则性框架。

Abstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [38] [M$^{\text{2}}$XFP: A Metadata-Augmented Microscaling Data Format for Efficient Low-bit Quantization](https://arxiv.org/abs/2601.19213)
*Weiming Hu,Zihan Zhang,Haoyan Zhang,Chen Zhang,Cong Guo,Yu Feng,Tianchi Hu,Guanglin Li,Guipeng Hu,Junsong Wang,Jingwen Leng*

Main category: cs.AR

TL;DR: 本文提出了一种基于灵活元数据的算法-硬件协同设计方法，通过引入最小化元数据来恢复量化过程中的精度损失，同时保持高比特效率，显著减少了低比特MX格式的精度下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的低比特MX格式（如MXFP4）由于使用共享缩放因子和Power-of-Two格式，导致显著的精度下降。需要探索引入最小元数据的策略来恢复量化过程中的精度损失，同时保持高比特效率。

Method: 提出了基于灵活元数据的完整算法-硬件协同设计，包括在线量化与简单编码。为实现该方法的高效支持，实现了轻量级硬件单元并将其集成到加速器中。

Result: 评估结果表明，该方法显著缩小了精度差距，在LLM基准测试中平均减少了70.63%相对于MXFP4的精度损失，以及37.30%相对于最新NVFP4的精度损失。设计实现了最高1.91倍的速度提升和1.75倍的能耗节省。

Conclusion: 提出的基于灵活元数据的算法-硬件协同设计方法有效解决了低比特MX格式的精度下降问题，在保持高比特效率的同时显著提升了模型精度和硬件性能。

Abstract: Existing low-bit Microscaling (MX) formats, such as MXFP4, often suffer from substantial accuracy degradation due to the use of a shared scaling factor with the Power-of-Two format. In this work, we explore strategies that introduce minimal metadata to recover accuracy lost during quantization while maintaining high bit efficiency across a wide range of large language models. We propose a complete algorithm-hardware co-design based on flexible metadata, featuring an online quantization with simple encoding. To support the proposed method efficiently, we implement a lightweight hardware unit and integrate it into the accelerator. Evaluation results demonstrate that our method substantially narrows the accuracy gap, achieving on average a 70.63% reduction in accuracy loss compared to MXFP4 and a 37.30% reduction relative to the latest NVFP4 on LLM benchmarks. Furthermore, our design delivers up to 1.91$\times$ speedup and 1.75$\times$ energy savings over state-of-the-art accelerators. Our code is available at https://github.com/SJTU-ReArch-Group/M2XFP_ASPLOS26.

</details>


### [39] [GenPairX: A Hardware-Algorithm Co-Designed Accelerator for Paired-End Read Mapping](https://arxiv.org/abs/2601.19384)
*Julien Eudine,Chu Li,Zhuo Cheng,Renzo Andri,Can Firtina,Mohammad Sadrosadati,Nika Mansouri Ghiasi,Konstantina Koliogeorgi,Anirban Nag,Arash Tavakkol,Haiyu Mao,Onur Mutlu,Shai Bergman,Ji Zhang*

Main category: cs.AR

TL;DR: GenPairX是一个硬件-算法协同设计的加速器，通过联合考虑双端测序读段对来提高过滤效率，并用轻量级比对算法替代昂贵的动态规划操作，显著提升了双端读段比对的性能。


<details>
  <summary>Details</summary>
Motivation: 读段比对是基因组分析中的主要性能瓶颈，现有方法对双端读段的过滤效果不佳，因为它们是独立评估每个读段的，过滤率相对较低。

Method: 提出GenPairX硬件-算法协同设计加速器：1）新颖的过滤算法联合考虑读段对以提高过滤效果，用轻量级比对算法替代大部分动态规划操作；2）两个专用硬件机制支持所提算法。

Result: GenPairX相比最先进的解决方案实现了显著性能提升：与领先的基于CPU和基于加速器的读段比对工具相比，每瓦特吞吐量分别提高了1575倍和1.43倍，且不损失准确性。

Conclusion: GenPairX通过硬件-算法协同设计有效解决了双端读段比对的性能瓶颈问题，在保持准确性的同时大幅提升了计算效率。

Abstract: Genome sequencing has become a central focus in computational biology. A genome study typically begins with sequencing, which produces millions to billions of short DNA fragments known as reads. Read mapping aligns these reads to a reference genome. Read mapping for short reads comes in two forms: single-end and paired-end, with the latter being more prevalent due to its higher accuracy and support for advanced analysis. Read mapping remains a major performance bottleneck in genome analysis due to expensive dynamic programming. Prior efforts have attempted to mitigate this cost by employing filters to identify and potentially discard computationally expensive matches and leveraging hardware accelerators to speed up the computations. While partially effective, these approaches have limitations. In particular, existing filters are often ineffective for paired-end reads, as they evaluate each read independently and exhibit relatively low filtering ratios. In this work, we propose GenPairX, a hardware-algorithm co-designed accelerator that efficiently minimizes the computational load of paired-end read mapping while enhancing the throughput of memory-intensive operations. GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most of the computationally expensive dynamic programming operations, and (2) two specialized hardware mechanisms to support the proposed algorithms. Our evaluations show that GenPairX delivers substantial performance improvements over state-of-the-art solutions, achieving 1575x and 1.43x higher throughput per watt compared to leading CPU-based and accelerator-based read mappers, respectively, all without compromising accuracy.

</details>


### [40] [Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation](https://arxiv.org/abs/2601.19747)
*Jiale Liu,Taiyu Zhou,Tianqi Jiang*

Main category: cs.AR

TL;DR: Veri-Sure是一个多智能体框架，通过设计契约对齐智能体意图，使用静态依赖切片指导的补丁机制进行精确局部修复，结合多分支验证管道确保RTL代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前EDA领域使用LLM进行RTL设计面临三个主要瓶颈：模拟中心评估的测试覆盖率和可靠性有限；迭代调试引入的回归和修复幻觉；智能体交接过程中的语义漂移。

Method: 提出Veri-Sure多智能体框架，建立设计契约对齐智能体意图，使用静态依赖切片指导的补丁机制进行精确局部修复。集成多分支验证管道，结合轨迹驱动的时间分析和形式验证（断言检查和布尔等价证明）。

Result: 在扩展的VerilogEval-v2-EXT基准测试（增加53个工业级设计任务和分层难度）上，Veri-Sure实现了最先进的已验证正确RTL代码生成性能，超越了独立LLM和先前的智能体系统。

Conclusion: Veri-Sure通过设计契约、精确补丁机制和多分支验证管道的结合，解决了当前LLM在RTL设计中面临的功能正确性瓶颈，为硅级正确性提供了有效解决方案。

Abstract: In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.

</details>
