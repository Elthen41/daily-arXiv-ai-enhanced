<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [SwiftKV: An Edge-Oriented Attention Algorithm and Multi-Head Accelerator for Fast, Efficient LLM Decoding](https://arxiv.org/abs/2601.10953)
*Junming Zhang,Qinyan Zhang,Huajun Sun,Feiyang Gao,Sheng Hu,Rui Nie,Xiangshui Miao*

Main category: cs.AR

TL;DR: SwiftKV Attention：一种面向边缘加速器的单次流水线注意力推理算法，无需分数物化、分块softmax或二次遍历，在边缘设备上实现7.16倍加速；SwiftKV-MHA加速器支持多头并行解码，进一步降低13.48倍延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在边缘设备上的加速应用面临挑战，特别是在资源受限的边缘加速器上实现快速注意力推理和高效解码。现有方法难以在边缘设备上同时满足低延迟和高效率的要求。

Method: 提出SwiftKV Attention算法：采用每令牌流水线化的单次注意力推理，每个(k_t, v_t)在KV缓存中仅处理一次，无需分数物化、分块softmax或二次遍历。同时设计SwiftKV-MHA加速器，在同一处理器阵列上支持高精度注意力和低精度GEMV，实现多头并行解码。

Result: 在边缘加速器上，SwiftKV Attention算法相比原生注意力实现7.16倍加速，显著优于其他注意力算法。SwiftKV-MHA进一步将注意力延迟降低13.48倍；在相同设置下，相比最先进工作，生成速度提升17.4%，令牌效率提高1.98倍。

Conclusion: SwiftKV Attention算法和SwiftKV-MHA加速器有效解决了边缘设备上大语言模型注意力推理和解码的挑战，通过创新的单次流水线设计和硬件架构优化，在资源受限的边缘加速器上实现了显著的性能提升。

Abstract: Edge acceleration for large language models is crucial for their widespread application; however, achieving fast attention inference and efficient decoding on resource-constrained edge accelerators remains challenging. This paper presents SwiftKV Attention, a per-token pipelined, low-latency single-pass attention inference algorithm, where every (kt, vt) in the KV cache is processed exactly once in a uniform per-token pipeline without score materialization, blockwise softmax, or a second pass, thereby enabling fast execution on edge accelerators with a single hardware set and no resource-intensive parallelism. Furthermore, to address the limited support for multi-head LLM decoding in existing accelerators, we design the SwiftKV-MHA accelerator, which enables high precision attention and low precision GEMV on the same processor array, achieving fast and efficient multi-head parallel decoding. Experimental results show that, on the edge accelerator, the SwiftKV Attention algorithm achieves a 7.16* speedup over native attention and significantly outperforms other attention algorithms. SwiftKV-MHA further reduces attention latency by 13.48*; under the same settings, it improves generation speed by 17.4% and increases token efficiency by 1.98* compared with state-of-the-art works.

</details>


### [2] [RidgeWalker: Perfectly Pipelined Graph Random Walks on FPGAs](https://arxiv.org/abs/2601.11057)
*Hongshi Tan,Yao Chen,Xinyu Chen,Qizhen Zhang,Cheng Chen,Weng-Fai Wong,Bingsheng He*

Main category: cs.AR

TL;DR: RidgeWalker是一个基于FPGA的高性能图随机游走加速器，利用马尔可夫特性将任务分解为无状态细粒度任务，通过异步流水线和反馈驱动调度实现完美流水线和自适应负载均衡，相比现有FPGA和GPU方案分别获得7.0倍和8.1倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 图随机游走（GRW）在众多应用中广泛使用，但由于其强数据依赖性、不规则内存访问模式和执行不平衡等特性，GRW工作负载难以加速。现有FPGA加速器解决方案因流水线效率低下和静态调度而未能充分发挥硬件潜力。

Method: RidgeWalker基于GRW的马尔可夫特性，将任务分解为无状态细粒度任务，支持乱序执行而不影响正确性。采用基于排队论的异步流水线架构和反馈驱动调度器，实现完美流水线和自适应负载均衡。

Result: 在数据中心FPGA上原型实现，实验结果显示：相比最先进的FPGA解决方案平均加速7.0倍（峰值71.0倍），相比GPU解决方案平均加速8.1倍（峰值22.9倍）。

Conclusion: RidgeWalker通过创新的异步流水线架构和反馈驱动调度，有效解决了GRW加速中的关键挑战，显著提升了图随机游走的执行效率，为数据中心FPGA上的图计算加速提供了有效解决方案。

Abstract: Graph Random Walks (GRWs) offer efficient approximations of key graph properties and have been widely adopted in many applications. However, GRW workloads are notoriously difficult to accelerate due to their strong data dependencies, irregular memory access patterns, and imbalanced execution behavior. While recent work explores FPGA-based accelerators for GRWs, existing solutions fall far short of hardware potential due to inefficient pipelining and static scheduling. This paper presents RidgeWalker, a high-performance GRW accelerator designed for datacenter FPGAs. The key insight behind RidgeWalker is that the Markov property of GRWs allows decomposition into stateless, fine-grained tasks that can be executed out-of-order without compromising correctness. Building on this, RidgeWalker introduces an asynchronous pipeline architecture with a feedback-driven scheduler grounded in queuing theory, enabling perfect pipelining and adaptive load balancing. We prototype RidgeWalker on datacenter FPGAs and evaluated it across a range of GRW algorithms and real-world graph datasets. Experimental results demonstrate that RidgeWalker achieves an average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups of up to 71.0x and 22.9x, respectively. The source code is publicly available at https://github.com/Xtra-Computing/RidgeWalker.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [3] [Chatting with Confidants or Corporations? Privacy Management with AI Companions](https://arxiv.org/abs/2601.10754)
*Hsuen-Chi Chiu,Jeremy Foote*

Main category: cs.CR

TL;DR: 用户在与AI情感伴侣互动时，将人际亲密习惯与机构隐私意识相结合，通过分层策略管理隐私，但对平台数据控制感到无力，拟人化设计模糊隐私边界


<details>
  <summary>Details</summary>
Motivation: 研究AI情感伴侣如何模糊人际亲密与机构软件的边界，以及用户在这种复杂多维隐私环境中的隐私管理行为

Method: 基于沟通隐私管理理论和Masur的水平（用户-AI）与垂直（用户-平台）隐私框架，对15名Replika和Character.AI等伴侣AI平台用户进行深度访谈

Result: 用户将人际习惯与机构意识融合：AI的非评判性和随时可用性促进情感安全和自我表露，但用户仍警惕机构风险，通过分层策略和选择性分享管理隐私；许多人对平台级数据控制感到不确定或无力；拟人化设计进一步模糊隐私边界，有时导致无意过度分享和隐私动荡

Conclusion: 研究结果扩展了隐私理论，突出了人机伴侣关系中情感和机构隐私管理的独特相互作用

Abstract: AI chatbots designed as emotional companions blur the boundaries between interpersonal intimacy and institutional software, creating a complex, multi-dimensional privacy environment. Drawing on Communication Privacy Management theory and Masur's horizontal (user-AI) and vertical (user-platform) privacy framework, we conducted in-depth interviews with fifteen users of companion AI platforms such as Replika and Character.AI. Our findings reveal that users blend interpersonal habits with institutional awareness: while the non-judgmental, always-available nature of chatbots fosters emotional safety and encourages self-disclosure, users remain mindful of institutional risks and actively manage privacy through layered strategies and selective sharing. Despite this, many feel uncertain or powerless regarding platform-level data control. Anthropomorphic design further blurs privacy boundaries, sometimes leading to unintentional oversharing and privacy turbulence. These results extend privacy theory by highlighting the unique interplay of emotional and institutional privacy management in human-AI companionship.

</details>


### [4] [InterPUF: Distributed Authentication via Physically Unclonable Functions and Multi-party Computation for Reconfigurable Interposers](https://arxiv.org/abs/2601.11368)
*Ishraq Tashdid,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.CR

TL;DR: InterPUF：一种紧凑可扩展的认证框架，利用可重构中介层中的路由差分延迟PUF作为分布式信任根，通过多方计算保护认证过程，实现芯片间安全集成。


<details>
  <summary>Details</summary>
Motivation: 现代系统级封装平台采用可重构中介层实现芯片粒子的即插即用集成，但这种灵活性带来了严重的信任挑战，传统认证方案无法在去中心化、后制造可编程环境中扩展或适应。

Method: 提出InterPUF框架，将中介层转化为分布式信任根：1）在可重构互连中嵌入基于路由的差分延迟物理不可克隆函数；2）使用多方计算保护认证过程，确保原始PUF签名永不暴露；3）结合中介层驻留的PUF原语、密码哈希和协作验证。

Result: 硬件评估显示仅0.23%面积和0.072%功耗开销，认证延迟保持在数十纳秒内；pyPUF仿真确认在工艺、电压和温度变化下具有强唯一性、可靠性和建模抵抗性。

Conclusion: InterPUF通过结合中介层驻留PUF原语与密码哈希和协作验证，实现了不依赖中心化锚点的最小信任认证模型，为异构多供应商生态系统中的芯片粒子集成提供了安全解决方案。

Abstract: Modern system-in-package (SiP) platforms increasingly adopt reconfigurable interposers to enable plug-and-play chiplet integration across heterogeneous multi-vendor ecosystems. However, this flexibility introduces severe trust challenges, as traditional authentication schemes fail to scale or adapt in decentralized, post-fabrication programmable environments. This paper presents InterPUF, a compact and scalable authentication framework that transforms the interposer into a distributed root of trust. InterPUF embeds a route-based differential delay physically unclonable function (PUF) across the reconfigurable interconnect and secures authentication using multi-party computation (MPC), ensuring raw PUF signatures are never exposed. Our hardware evaluation shows only 0.23% area and 0.072% power overhead across diverse chiplets while preserving authentication latency within tens of nanoseconds. Simulation results using pyPUF confirm strong uniqueness, reliability, and modeling resistance under process, voltage, and temperature variations. By combining interposer-resident PUF primitives with cryptographic hashing and collaborative verification, InterPUF enforces a minimal-trust authentication model without relying on a centralized anchor.

</details>


### [5] [SecMLOps: A Comprehensive Framework for Integrating Security Throughout the MLOps Lifecycle](https://arxiv.org/abs/2601.10848)
*Xinrui Zhang,Pincan Zhao,Jason Jaskolka,Heng Li,Rongxing Lu*

Main category: cs.CR

TL;DR: 本文提出了SecMLOps框架，将安全措施整合到整个MLOps生命周期中，以应对机器学习部署中的安全挑战，特别是对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 机器学习在关键系统中广泛应用，但部署时面临严重的安全挑战，特别是对抗性攻击会损害系统的完整性和可靠性。现有MLOps框架缺乏足够的安全考虑，需要将安全措施整合到整个ML生命周期中。

Method: 基于MLOps原则，提出SecMLOps框架，将安全考虑嵌入从设计到部署和持续监控的整个MLOps生命周期。该框架特别关注保护MLOps各阶段免受复杂攻击，并通过高级行人检测系统(PDS)用例展示实际应用。

Result: 通过广泛的实证评估，展示了安全措施与系统性能之间的权衡关系，提供了在不影响操作效率的情况下优化安全的关键见解。SecMLOps框架能增强ML应用的韧性和可信度。

Conclusion: 平衡安全与性能的方法至关重要，SecMLOps框架为从业者提供了在ML部署中实现安全与性能最佳平衡的宝贵指导，适用于各个领域。

Abstract: Machine Learning (ML) has emerged as a pivotal technology in the operation of large and complex systems, driving advancements in fields such as autonomous vehicles, healthcare diagnostics, and financial fraud detection. Despite its benefits, the deployment of ML models brings significant security challenges, such as adversarial attacks, which can compromise the integrity and reliability of these systems. To address these challenges, this paper builds upon the concept of Secure Machine Learning Operations (SecMLOps), providing a comprehensive framework designed to integrate robust security measures throughout the entire ML operations (MLOps) lifecycle. SecMLOps builds on the principles of MLOps by embedding security considerations from the initial design phase through to deployment and continuous monitoring. This framework is particularly focused on safeguarding against sophisticated attacks that target various stages of the MLOps lifecycle, thereby enhancing the resilience and trustworthiness of ML applications. A detailed advanced pedestrian detection system (PDS) use case demonstrates the practical application of SecMLOps in securing critical MLOps. Through extensive empirical evaluations, we highlight the trade-offs between security measures and system performance, providing critical insights into optimizing security without unduly impacting operational efficiency. Our findings underscore the importance of a balanced approach, offering valuable guidance for practitioners on how to achieve an optimal balance between security and performance in ML deployments across various domains.

</details>


### [6] [Multi-Agent Taint Specification Extraction for Vulnerability Detection](https://arxiv.org/abs/2601.10865)
*Jonah Ghebremichael,Saastha Vasan,Saad Ullah,Greg Tystahl,David Adei,Christopher Kruegel,Giovanni Vigna,William Enck,Alexandros Kapravelos*

Main category: cs.CR

TL;DR: SemTaint是一个多智能体系统，结合大型语言模型的语义理解和传统静态程序分析，为JavaScript包生成污点规范，显著提升了CodeQL等SAST工具的漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: JavaScript静态污点分析面临两大挑战：1) JavaScript的动态特性使得数据流提取复杂化；2) npm庞大的库生态系统使得识别相关源/汇和建立跨依赖的污点传播变得困难。传统静态分析工具难以有效处理这些问题。

Method: SemTaint采用多智能体系统，结合LLM的语义理解和传统静态分析：1) 使用静态程序分析计算调用图，将无法静态解析的调用边委托给LLM处理；2) 利用LLM为给定CWE分类源和汇；3) 生成包含源、汇、调用边和库流摘要的污点规范，供SAST工具进行漏洞分析。

Result: 1) 集成到CodeQL后，成功检测出162个先前CodeQL无法检测的漏洞中的106个；2) 在4个流行的npm包中发现了4个新漏洞；3) 证明了LLM可以实际增强现有静态程序分析算法，结合符号推理和语义理解的优势。

Conclusion: LLM能够实际增强现有静态程序分析算法，结合符号推理和语义理解的优势，显著提升JavaScript应用的漏洞检测能力。SemTaint展示了这种混合方法在解决JavaScript动态特性和npm生态系统复杂性方面的有效性。

Abstract: Static Application Security Testing (SAST) tools using taint analysis are widely viewed as providing higher-quality vulnerability detection results compared to traditional pattern-based approaches. However, performing static taint analysis for JavaScript poses two major challenges. First, JavaScript's dynamic features complicate data flow extraction required for taint tracking. Second, npm's large library ecosystem makes it difficult to identify relevant sources/sinks and establish taint propagation across dependencies. In this paper, we present SemTaint, a multi-agent system that strategically combines the semantic understanding of Large Language Models (LLMs) with traditional static program analysis to extract taint specifications, including sources, sinks, call edges, and library flow summaries tailored to each package. Conceptually, SemTaint uses static program analysis to calculate a call graph and defers to an LLM to resolve call edges that cannot be resolved statically. Further, it uses the LLM to classify sources and sinks for a given CWE. The resulting taint specification is then provided to a SAST tool, which performs vulnerability analysis. We integrate SemTaint with CodeQL, a state-of-the-art SAST tool, and demonstrate its effectiveness by detecting 106 of 162 vulnerabilities previously undetectable by CodeQL. Furthermore, we find 4 novel vulnerabilities in 4 popular npm packages. In doing so, we demonstrate that LLMs can practically enhance existing static program analysis algorithms, combining the strengths of both symbolic reasoning and semantic understanding for improved vulnerability detection.

</details>


### [7] [Adaptive Privacy Budgeting](https://arxiv.org/abs/2601.10866)
*Yuting Liang,Ke Yi*

Main category: cs.CR

TL;DR: 本文提出了一个在广义差分隐私下的自适应隐私预算分配框架，通过根据先前查询输出动态调整用户隐私预算，实现隐私节约并提升后续查询效用。


<details>
  <summary>Details</summary>
Motivation: 在广义差分隐私设置中，用户数据的不同组成部分对函数计算的重要性不同，存在战略性地使用用户隐私预算的潜力。传统固定预算分配无法根据数据实例特点优化资源使用，需要自适应机制来在典型实例上实现更大的隐私节约。

Method: 提出了一个自适应隐私预算分配框架，允许根据先前查询的输出动态调整后续查询的隐私预算分配。该框架针对用户数据的不同组成部分，通过分析数据重要性差异，战略性地分配隐私资源。

Result: 开发了一个能够根据查询输出自适应调整隐私预算的框架，在多个应用场景中展示了其适用性。该框架能够在更典型的实例上实现更大的隐私节约，并将节约的预算用于提升后续查询的效用。

Conclusion: 自适应隐私预算分配框架为广义差分隐私下的多查询场景提供了有效的资源优化方案，通过动态调整预算分配策略，在保护用户隐私的同时提升了整体查询效用。

Abstract: We study the problem of adaptive privacy budgeting under generalized differential privacy. Consider the setting where each user $i\in [n]$ holds a tuple $x_i\in U:=U_1\times \dotsb \times U_T$, where $x_i(l)\in U_l$ represents the $l$-th component of their data. For every $l\in [T]$ (or a subset), an untrusted analyst wishes to compute some $f_l(x_1(l),\dots,x_n(l))$, while respecting the privacy of each user. For many functions $f_l$, data from the users are not all equally important, and there is potential to use the privacy budgets of the users strategically, leading to privacy savings that can be used to improve the utility of later queries. In particular, the budgeting should be adaptive to the outputs of previous queries, so that greater savings can be achieved on more typical instances. In this paper, we provide such an adaptive budgeting framework, with various applications demonstrating its applicability.

</details>


### [8] [Hidden-in-Plain-Text: A Benchmark for Social-Web Indirect Prompt Injection in RAG](https://arxiv.org/abs/2601.10923)
*Haoze Guo,Ziqi Wei*

Main category: cs.CR

TL;DR: OpenRAG-Soc是一个针对Web面向RAG系统的安全评估基准套件，专注于间接提示注入和检索污染攻击的测试


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统越来越依赖用户生成的Web内容进行回答，其攻击面也在扩大，特别是间接提示注入和检索污染攻击对通过摄取管道的内容构成严重威胁

Method: 开发了一个紧凑、可复现的基准测试套件，包含社交语料库、可互换的稀疏和密集检索器，以及可部署的缓解措施（HTML/Markdown净化、Unicode规范化、属性门控回答）

Result: 该套件标准化了从摄取到生成的端到端评估，报告攻击响应时间、稀疏和密集检索器的排名变化、效用和延迟，支持不同载体和防御措施之间的公平比较

Conclusion: OpenRAG-Soc面向需要快速、现实测试来跟踪风险并加强部署的实践者，为Web面向RAG系统提供安全评估工具

Abstract: Retrieval-augmented generation (RAG) systems put more and more emphasis on grounding their responses in user-generated content found on the Web, amplifying both their usefulness and their attack surface. Most notably, indirect prompt injection and retrieval poisoning attack the web-native carriers that survive ingestion pipelines and are very concerning. We provide OpenRAG-Soc, a compact, reproducible benchmark-and-harness for web-facing RAG evaluation under these threats, in a discrete data package. The suite combines a social corpus with interchangeable sparse and dense retrievers and deployable mitigations - HTML/Markdown sanitization, Unicode normalization, and attribution-gated answered. It standardizes end-to-end evaluation from ingestion to generation and reports attacks time of one of the responses at answer time, rank shifts in both sparse and dense retrievers, utility and latency, allowing for apples-to-apples comparisons across carriers and defenses. OpenRAG-Soc targets practitioners who need fast, and realistic tests to track risk and harden deployments.

</details>


### [9] [Shaping a Quantum-Resistant Future: Strategies for Post-Quantum PKI](https://arxiv.org/abs/2601.11104)
*Grazia D'Onghia,Diana Gratiela Berbecaru,Antonio Lioy*

Main category: cs.CR

TL;DR: 该论文探讨了量子计算时代下公钥基础设施(PKI)面临的量子威胁，提出了向抗量子PKI过渡的解决方案，重点关注X.509证书格式的适配以及证书撤销机制的抗量子化。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算时代的临近，保护经典密码协议变得至关重要。公钥密码学广泛用于签名和密钥交换，但正是这类密码学最受量子计算威胁。公钥证书作为签名数据结构，面临双重量子挑战：既要保护被认证的密钥，又要保护签名本身。

Method: 论文提出了选择鲁棒的后量子算法并研究其在公钥基础设施(PKI)环境中的适用性。贡献包括定义安全过渡到抗量子PKI的要求，重点关注X.509证书格式的适配，并探索证书撤销列表(CRL)和在线证书状态协议(OCSP)如何支持抗量子算法。

Result: 通过比较分析，论文阐明了向抗量子PKI过渡的复杂性，为后量子算法在PKI环境中的实际应用提供了指导框架。

Conclusion: 向抗量子PKI的过渡是复杂但必要的，需要系统性地考虑算法选择、证书格式适配和撤销机制更新等多个方面，以确保在量子计算时代下的密码安全。

Abstract: As the quantum computing era approaches, securing classical cryptographic protocols becomes imperative. Public key cryptography is widely used for signature and key exchange but it is the type of cryptography more threatened by quantum computing. Its application typically requires support via a public-key certificate, which is a signed data structure and must therefore face twice the quantum challenge: for the certified keys and for the signature itself. We present the latest developments in selecting robust Post-Quantum algorithms and investigate their applicability in the Public Key Infrastructure context. Our contribution entails defining requirements for a secure transition to a quantum-resistant Public Key Infrastructure, with a focus on adaptations for the X.509 certificate format. Additionally, we explore transitioning Certificate Revocation List and Online Certificate Status Protocol to support quantum-resistant algorithms. Through comparative analysis, we elucidate the complex transition to a quantum-resistant PKI.

</details>


### [10] [Proving Circuit Functional Equivalence in Zero Knowledge](https://arxiv.org/abs/2601.11173)
*Sirui Shen,Zunchen Huang,Chenglu Jin*

Main category: cs.CR

TL;DR: ZK-CEC是首个结合形式化验证和零知识证明的隐私保护硬件验证框架，能在不泄露设计机密的情况下验证IP正确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路生态系统依赖第三方IP集成，存在硬件木马和安全漏洞风险。现有隐私保护硬件验证方法都是基于仿真的，缺乏形式化保证，且无法在公式保密的情况下防止恶意证明者伪造公式。

Method: 提出ZK-CEC框架，结合形式化验证和零知识证明。首先提出证明秘密设计对公共约束不可满足性的蓝图，然后构建ZK-CEC使证明者能在零知识中证明秘密IP功能与公共规范完全一致，仅透露证明的长度和宽度。

Result: 实现了ZK-CEC并在多种电路上进行评估，包括算术单元和密码组件。实验结果表明ZK-CEC能在实际时间限制内成功验证实际设计，如AES S-Box。

Conclusion: ZK-CEC为硬件形式化验证建立了首个隐私保护框架，解决了IP供应商和系统集成商之间的信任僵局，同时保护设计机密性。

Abstract: The modern integrated circuit ecosystem is increasingly reliant on third-party intellectual property integration, which introduces security risks, including hardware Trojans and security vulnerabilities. Addressing the resulting trust deadlock between IP vendors and system integrators without exposing proprietary designs requires novel privacy-preserving verification techniques. However, existing privacy-preserving hardware verification methods are all simulation-based and fail to offer formal guarantees. In this paper, we propose ZK-CEC, the first privacy-preserving framework for hardware formal verification. By combining formal verification and zero-knowledge proof (ZKP), ZK-CEC establishes a foundation for formally verifying IP correctness and security without compromising the confidentiality of the designs.
  We observe that existing zero-knowledge protocols for formal verification are designed to prove statements of public formulas. However, in a privacy-preserving verification context where the formula is secret, these protocols cannot prevent a malicious prover from forging the formula, thereby compromising the soundness of the verification. To address these gaps, we first propose a blueprint for proving the unsatisfiability of a secret design against a public constraint, which is widely applicable to proving properties in software, hardware, and cyber-physical systems. Based on the proposed blueprint, we construct ZK-CEC, which enables a prover to convince the verifier that a secret IP's functionality aligns perfectly with the public specification in zero knowledge, revealing only the length and width of the proof. We implement ZK-CEC and evaluate its performance across various circuits, including arithmetic units and cryptographic components. Experimental results show that ZK-CEC successfully verifies practical designs, such as the AES S-Box, within practical time limits.

</details>


### [11] [SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11199)
*Aiman Al Masoud,Marco Arazzi,Antonino Nocera*

Main category: cs.CR

TL;DR: SD-RAG提出了一种新的选择性披露检索增强生成方法，通过在检索阶段而非生成阶段实施安全隐私约束，有效防止敏感信息泄露并抵抗提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法大多忽视敏感信息泄露风险，少数依赖生成模型自我约束的方法易受提示注入攻击，需要更可靠的安全隐私保护机制。

Method: SD-RAG将安全隐私约束执行与生成过程解耦，在检索阶段应用净化与披露控制；引入语义机制处理人类可读的动态安全隐私约束，采用优化的基于图的数据模型支持细粒度策略感知检索。

Result: 实验评估显示SD-RAG优于基线方法，隐私分数提升高达58%，同时对针对生成模型的提示注入攻击表现出强大抵抗力。

Conclusion: SD-RAG通过检索阶段的约束执行有效解决了RAG系统中的敏感信息泄露问题，为安全隐私保护提供了更可靠的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has attracted significant attention due to its ability to combine the generative capabilities of Large Language Models (LLMs) with knowledge obtained through efficient retrieval mechanisms over large-scale data collections. Currently, the majority of existing approaches overlook the risks associated with exposing sensitive or access-controlled information directly to the generation model. Only a few approaches propose techniques to instruct the generative model to refrain from disclosing sensitive information; however, recent studies have also demonstrated that LLMs remain vulnerable to prompt injection attacks that can override intended behavioral constraints. For these reasons, we propose a novel approach to Selective Disclosure in Retrieval-Augmented Generation, called SD-RAG, which decouples the enforcement of security and privacy constraints from the generation process itself. Rather than relying on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input. Moreover, we introduce a semantic mechanism to allow the ingestion of human-readable dynamic security and privacy constraints together with an optimized graph-based data model that supports fine-grained, policy-aware retrieval. Our experimental evaluation demonstrates the superiority of SD-RAG over baseline existing approaches, achieving up to a $58\%$ improvement in the privacy score, while also showing a strong resilience to prompt injection attacks targeting the generative model.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA是一个约束性时间分层架构，通过结构化流形投影和仲裁机制解决多时间尺度智能体架构中的协调稳定性问题，显著减少故障级联并提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 多时间尺度智能体架构虽然提升了性能，但破坏了统一智能体系统的协调稳定性，导致层间冲突、误差传播和可扩展性受限等问题。

Method: 提出约束性时间分层架构(CTHA)，包含三个关键约束：消息契约约束（通过类型化摘要、计划和策略包形式化层间信息流）、权威流形约束（根据时间范围界定各层决策空间）、仲裁器解决约束（保证多层决策的无冲突组合）。

Result: 实验表明CTHA在复杂任务执行中有效，故障级联减少47%，样本效率提升2.3倍，相比无约束分层基线具有更好的可扩展性。

Conclusion: CTHA作为时间分层架构的原则性扩展，有助于深入理解多智能体协调，并为稳健自主系统的演进提供了有前景的方向。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [13] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 该研究使用基于启发式的趋势模型分析复杂产品创新过程，通过简单趋势（增加、减少、恒定）作为最小信息强度量化器，避免依赖数值或粗糙集。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种分析复杂产品创新过程的方法，避免传统方法对数值数据和粗糙集的依赖，使用更简洁的趋势表示来捕捉系统动态。

Method: 方法基于一组启发式规则，每个启发式通过简单趋势（增加、减少、恒定）表达。构建趋势模型，解决方案定义为包含可能场景及其转换的集合，用转换图表示系统行为。

Result: 研究结果提出了一种趋势模型框架，能够通过转换图中的路径描绘系统所有可能的未来或过去行为，为复杂产品创新过程分析提供了新的形式化工具。

Conclusion: 该研究成功开发了一种基于启发式趋势的建模方法，为分析复杂产品创新过程提供了一种简洁、信息最小化的量化框架，避免了传统数值方法的复杂性。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [14] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: ARC-AGI-2竞赛显示AI在抽象推理任务上表现有限（24%准确率），2025年主要趋势是精炼循环方法的兴起，但前沿AI系统仍受限于知识覆盖而非真正的推理能力。


<details>
  <summary>Details</summary>
Motivation: 分析ARC-AGI-2竞赛结果，研究精炼循环方法在AGI进展中的作用，探讨当前AI推理能力的局限性，特别是知识依赖过拟合问题，并展望下一代ARC-AGI-3基准。

Method: 通过分析1455个团队、15154个参赛作品和90篇论文，调查了精炼循环方法（包括进化程序合成和商业AI系统应用层优化），以及零预训练深度学习方法。同时考察了四大AI实验室（Anthropic、Google DeepMind、OpenAI、xAI）的公开模型卡数据。

Result: ARC-AGI-2竞赛最高得分仅为24%，显示AI在复杂抽象推理任务上表现有限。精炼循环方法成为主流，零预训练小模型（700万参数）也能达到竞争性表现。前沿AI系统在ARC-AGI上的表现仍主要依赖知识覆盖而非真正的推理能力。

Conclusion: 当前AI推理能力仍受限于知识依赖过拟合，精炼循环方法虽能提升性能但未解决根本推理问题。ARC-AGI-3将引入需要探索、规划、记忆、目标获取和对齐能力的交互式推理挑战，推动更全面的智能评估。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [15] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 该研究通过NeurIPS 2025 DCVLR挑战赛探索多模态推理的数据策展，发现基于难度的样本选择是性能提升的主要驱动力，而数据集大小增加主要降低方差而非提升平均准确率。


<details>
  <summary>Details</summary>
Motivation: 研究多模态推理中的数据策展问题，通过固定模型和训练协议来隔离数据集选择的影响，探索在数据高效的多模态推理中什么因素真正起作用。

Method: 使用NeurIPS 2025 DCVLR挑战赛框架，主要基于Walton Multimodal Cold Start构建紧凑的策展数据集，通过竞赛后消融实验分析不同数据策展策略的效果。

Result: 该提交在挑战赛中排名第一；基于难度的样本选择在已对齐的基础数据集上是性能提升的主要驱动力；增加数据集大小主要减少运行间方差而非提升平均准确率；常用的多样性和合成增强启发式方法没有额外益处且常降低性能。

Conclusion: DCVLR代表了饱和状态评估，突出了对齐和难度在数据高效多模态推理中的核心作用，表明精心选择的高难度样本比简单增加数据量或应用复杂增强策略更有效。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [16] [MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting](https://arxiv.org/abs/2601.11089)
*Suhan Guo,Jiahong Deng,Furao Shen*

Main category: cs.AI

TL;DR: MiCA是一个轻量级的架构无关模块，通过因果发现推断移动关系并通过门控残差混合将其集成到时序预测模型中，在噪声和数据有限条件下提升流行病预测性能。


<details>
  <summary>Details</summary>
Motivation: 人类移动性在流行病空间传播中起关键作用，但移动数据通常噪声大、间接且难以与疾病记录可靠整合。同时，流行病病例时间序列通常较短且时间分辨率粗糙，这限制了依赖清洁丰富数据的参数密集型移动感知预测器的有效性。

Method: 提出Mobility-Informed Causal Adapter (MiCA)，通过因果发现推断移动关系，并通过门控残差混合将其集成到时序预测模型中。该设计允许轻量级预测器选择性地利用移动性衍生的空间结构，同时保持对噪声和数据有限条件的鲁棒性。

Result: 在四个真实世界流行病数据集（COVID-19发病率、COVID-19死亡率、流感和登革热）上的实验表明，MiCA持续改进轻量级时序主干模型，在预测时间范围内平均相对误差减少7.5%。MiCA在保持轻量级的同时，性能可与最先进的时空模型相媲美。

Conclusion: MiCA是一个有效的轻量级模块，能够在噪声和数据有限条件下通过因果推断的移动关系提升流行病预测性能，为公共卫生规划和干预提供更可靠的预测工具。

Abstract: Accurate forecasting of infectious disease dynamics is critical for public health planning and intervention. Human mobility plays a central role in shaping the spatial spread of epidemics, but mobility data are noisy, indirect, and difficult to integrate reliably with disease records. Meanwhile, epidemic case time series are typically short and reported at coarse temporal resolution. These conditions limit the effectiveness of parameter-heavy mobility-aware forecasters that rely on clean and abundant data. In this work, we propose the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module for epidemic forecasting. MiCA infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing. This design allows lightweight forecasters to selectively exploit mobility-derived spatial structure while remaining robust under noisy and data-limited conditions, without introducing heavy relational components such as graph neural networks or full attention. Extensive experiments on four real-world epidemic datasets, including COVID-19 incidence, COVID-19 mortality, influenza, and dengue, show that MiCA consistently improves lightweight temporal backbones, achieving an average relative error reduction of 7.5\% across forecasting horizons. Moreover, MiCA attains performance competitive with SOTA spatio-temporal models while remaining lightweight.

</details>


### [17] [ReCreate: Reasoning and Creating Domain Agents Driven by Experience](https://arxiv.org/abs/2601.11100)
*Zhezheng Hao,Hong Wang,Jian Luo,Jianqing Zhang,Yuyan Zhou,Qiang Lin,Can Wang,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: ReCreate是一个经验驱动的自动创建领域智能体框架，通过智能体交互历史学习成功失败原因，采用智能体即优化器范式，在多个领域超越人工设计和现有自动方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数实用智能体仍需要人工设计，因为任务差异大且构建劳动密集。现有自动创建方法将智能体生成视为黑盒过程，仅依赖最终性能指标，忽略了成功失败的关键证据且计算成本高。

Method: 提出ReCreate框架，采用智能体即优化器范式，包含三个关键组件：1) 经验存储和检索机制用于按需检查；2) 推理-创建协同管道将执行经验映射到脚手架编辑；3) 分层更新将实例级细节抽象为可重用领域模式。

Result: 在多个不同领域的实验中，ReCreate始终优于人工设计的智能体和现有自动智能体生成方法，即使从最小种子脚手架开始也能取得良好效果。

Conclusion: ReCreate通过系统利用智能体交互历史中的具体信号，成功解决了现有自动智能体创建方法的局限性，为在真实世界中自动创建和适应领域智能体提供了有效解决方案。

Abstract: Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.

</details>


### [18] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出SCALE框架，通过任务级工作流生成和自预测评估替代昂贵的执行验证，在保持性能的同时大幅降低token消耗


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统通常通过工作流协调智能体，但现有方法在任务级和查询级工作流生成之间的成本效益关系不明确，且执行验证的token成本极高且不可靠

Method: 提出SCALE框架：1）通过少量样本校准进行自预测评估替代昂贵的执行验证；2）利用任务级工作流生成覆盖多个查询；3）结合自演化和生成奖励建模思想

Result: SCALE在多个数据集上平均性能仅下降0.61%，同时将总体token使用量减少高达83%，在保持竞争力的同时显著降低成本

Conclusion: 查询级工作流生成并非总是必要，任务级工作流生成结合自预测评估可以显著降低多智能体系统的token成本，同时保持性能

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [19] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: TANDEM是一个统一框架，将视听仇恨检测从二元分类任务转化为结构化推理问题，通过跨模态上下文强化学习实现精确的时间定位和目标识别。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上长格式多模态内容中的有害叙述通过音频、视觉和文本线索的复杂交互构建，现有自动化系统虽然能高精度标记仇恨言论，但作为"黑盒"无法提供可解释的细粒度证据（如精确时间戳和目标身份），难以支持有效的人机协同审核。

Method: 提出TANDEM框架，采用新颖的串联强化学习策略，让视觉-语言和音频-语言模型通过自约束的跨模态上下文相互优化，在不需要密集帧级监督的情况下稳定处理长时间序列的推理。

Result: 在三个基准数据集上的实验显示，TANDEM显著优于零样本和上下文增强基线，在HateMM数据集上目标识别F1达到0.73（比现有最佳方法提升30%），同时保持精确的时间定位。二元检测稳健，但在多类别设置中区分冒犯性和仇恨内容仍具挑战性。

Conclusion: 即使在复杂的多模态环境中，结构化、可解释的对齐也是可实现的，这为下一代透明且可操作的在线安全审核工具提供了蓝图。

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [20] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice是一个可解释的框架，用于评估约束决策中AI与人类的对齐程度，通过机制建模超越传统准确性指标，揭示决策因素权重、约束敏感性和权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估主要关注结果一致性（如准确率、F1分数），缺乏对决策机制的深入理解，无法诊断AI与人类在约束决策中的根本差异。

Method: XChoice构建基于机制的决策模型，拟合人类数据和LLM生成的决策，恢复可解释参数（决策因素重要性、约束敏感性、隐含权衡），通过比较参数向量评估对齐度。

Result: 在美国时间分配研究中发现模型间和活动间的异质对齐，黑人和已婚群体存在显著不对齐；通过不变性分析验证鲁棒性，RAG干预实现针对性缓解。

Conclusion: XChoice提供基于机制的指标，能够诊断不对齐问题并支持超越表面结果匹配的改进，为AI与人类对齐评估提供新方法。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [21] [AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems](https://arxiv.org/abs/2601.11354)
*Weiyi Wang,Xinchi Chen,Jingjing Gong,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: AstroReason-Bench是一个用于评估智能体在空间规划问题中规划能力的基准测试，该基准整合了多种调度机制，发现当前智能体在现实约束下的表现远不如专用求解器。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注符号化或弱接地环境，而在物理约束的现实世界领域中的性能尚未得到充分探索。需要建立一个能够评估智能体在具有异质目标、严格物理约束和长时程决策的高风险空间规划问题中规划能力的基准。

Method: 引入了AstroReason-Bench基准，该基准整合了多种调度机制（包括地面站通信和敏捷地球观测），并提供了统一的智能体导向交互协议。在多种最先进的开放和闭源智能体LLM系统上进行了评估。

Result: 评估发现，当前智能体在现实约束下的表现远不如专用求解器，突显了通用规划器在现实约束下的关键局限性。

Conclusion: AstroReason-Bench为未来智能体研究提供了一个具有挑战性和诊断性的测试平台，有助于推动智能体在物理约束现实世界领域中的发展。

Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

</details>


### [22] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 提出"探测与求解"两阶段框架，通过贝叶斯优化自动调优约束求解器超参数，在CPMpy库中实现，相比默认配置显著提升求解性能。


<details>
  <summary>Details</summary>
Motivation: 约束求解器的性能对超参数选择高度敏感，手动寻找最佳配置需要专家知识且耗时。需要自动化方法来优化超参数配置。

Method: 提出两阶段框架：1) 探测阶段使用可配置的超参数优化方法（贝叶斯优化和汉明距离搜索）探索不同超参数集；2) 求解阶段使用找到的最佳配置在剩余时间内解决问题。在CPMpy库中实现该算法。

Result: 贝叶斯优化在ACE求解器上25.4%的实例中优于默认配置，57.9%持平；在Choco求解器上38.6%的实例中表现更优。贝叶斯优化始终优于汉明距离搜索，证实了基于模型的探索优于简单局部搜索。

Conclusion: 探测与求解算法为约束求解器调优提供了实用、资源感知的方法，在不同问题类型上都能获得稳健的改进，模型化探索策略效果显著。

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [23] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了基于LLM的预测性流程监控框架，在数据稀缺场景下（仅100条轨迹）评估其通用性、语义利用和推理机制，结果表明LLM在总时间和活动发生预测方面优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 预测性流程监控旨在预测进行中流程的结果，虽然已有基于机器学习和深度学习的架构，但作者希望扩展其先前基于LLM的框架，全面评估其在多个关键绩效指标上的通用性、语义利用和推理机制。

Method: 扩展了先前基于LLM的预测性流程监控框架，通过提示方式进行总时间预测，并在三个不同的事件日志上进行实证评估，涵盖总时间和活动发生预测两个关键绩效指标，特别关注数据稀缺场景（仅100条轨迹）。

Result: 在数据稀缺设置下（仅100条轨迹），LLM在总时间和活动发生预测方面超越了基准方法。实验还表明LLM利用了其先验知识和训练轨迹间的内部相关性，并且进行了高阶推理而不仅仅是复制现有预测方法。

Conclusion: LLM在预测性流程监控中表现出色，特别是在数据稀缺场景下，不仅利用了先验知识和数据相关性，还展示了高阶推理能力，为流程预测提供了新的有效方法。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [24] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型与扩展贪心算法的混合框架（LEG），用于优化埃塞俄比亚农村卫生站升级的优先级决策，在保证人口覆盖率理论保证的同时，整合专家定性指导。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部正在升级卫生站以改善基本医疗服务可及性，但资源有限需要优先考虑哪些设施进行升级。传统优化方法需要明确的量化目标，而利益相关者的标准通常用自然语言表达且难以形式化，需要一种能整合专家知识与优化技术的解决方案。

Method: 开发了LEG（大语言模型与扩展贪心）框架，将人口覆盖率优化的可证明近似算法与LLM驱动的迭代优化相结合。该框架通过人机对齐确保解决方案反映专家的定性指导，同时保持覆盖率保证。

Result: 在埃塞俄比亚三个地区的真实数据上进行实验，证明了该框架的有效性，展示了其在为公平、数据驱动的卫生系统规划提供信息方面的潜力。

Conclusion: LEG框架成功地将专家知识与优化技术相结合，为资源有限的卫生设施升级决策提供了系统化的解决方案，既能保证理论覆盖率，又能整合利益相关者的定性偏好，有助于实现更公平的卫生系统规划。

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


### [25] [BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics](https://arxiv.org/abs/2601.11492)
*Kaiwen Wang,Kaili Zheng,Rongrong Deng,Qingmin Fan,Milin Zhang,Zongrui Li,Xuesi Zhou,Bo Han,Liren Chen,Chenyi Guo,Ji Wu*

Main category: cs.AI

TL;DR: BoxMind：一个用于拳击战术分析的闭环AI专家系统，通过定义原子击打事件、构建层次化技术战术指标，结合图预测模型和可学习嵌入来预测比赛结果并生成战术建议，在2024巴黎奥运会上为中国国家队取得历史性成绩提供支持。


<details>
  <summary>Details</summary>
Motivation: 格斗类运动如拳击在AI驱动分析方面发展不足，主要因为动作动态复杂且缺乏结构化战术表示。需要将非结构化视频数据转化为战略智能，弥合计算机视觉与竞技体育决策支持之间的差距。

Method: 1. 定义具有精确时间边界和空间技术属性的原子击打事件；2. 将比赛视频解析为18个层次化技术战术指标；3. 提出基于图的预测模型，融合显式技术战术特征与可学习的时间变化潜在嵌入；4. 将比赛结果建模为技术战术指标的可微分函数，将获胜概率梯度转化为可执行的战术调整。

Result: 1. 结果预测模型在BoxerGraph测试集上达到69.8%准确率，在奥运比赛上达到87.5%准确率；2. 系统生成的战略建议达到与人类专家相当的水平；3. 在2024巴黎奥运会闭环部署中，直接帮助中国国家队获得3金2银的历史性成绩。

Conclusion: BoxMind建立了一个可复制的范式，将非结构化视频数据转化为战略智能，弥合了计算机视觉与竞技体育决策支持之间的差距，为格斗类运动的战术分析提供了有效的AI驱动解决方案。

Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [26] [AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning](https://arxiv.org/abs/2601.10998)
*Shinsuk Kang,Youngjae Kim*

Main category: cs.DC

TL;DR: AFLL系统通过机器学习实时调整MMO游戏服务器的消息权重，实现预测性限流，在保证关键消息的同时降低服务器负载


<details>
  <summary>Details</summary>
Motivation: 传统MMO服务器负载管理方法存在缺陷：要么对所有消息类型统一限流（损害游戏体验），要么使用固定启发式规则（无法适应动态工作负载）。需要一种能实时学习消息因果关系的自适应系统来维持亚100毫秒响应时间

Method: AFLL（自适应反馈循环学习）系统使用反向传播算法持续学习服务器发出消息与后续客户端请求之间的因果关系，动态调整消息类型权重，实现预测性限流。系统通过后台计算和缓存优化实现零学习开销

Result: 在1000名并发玩家的实验中，AFLL将平均CPU时间降低48.3%（13.2ms到6.8ms），峰值CPU时间降低51.7%（54.0ms到26.1ms），线程争用降低64.4%（19.6%到7.0%）。所有指标的可重复性变异系数小于2%，并识别出消息阻塞到负载减少的三阶段因果链

Conclusion: AFLL证明了循环因果学习能够为延迟关键系统提供实用的实时自适应能力，通过智能消息优先级管理在保证关键消息交付的同时有效防止服务器过载

Abstract: Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.

</details>


### [27] [Konflux: Optimized Function Fusion for Serverless Applications](https://arxiv.org/abs/2601.11156)
*Niklas Kowallik,Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: 提出一个系统来分析FaaS应用中所有可能的函数融合配置，通过本地仿真替代生产环境测试，显著降低优化成本和时间。


<details>
  <summary>Details</summary>
Motivation: FaaS已成为无服务器计算的核心范式，但优化FaaS部署仍然具有挑战性。函数融合可以将多个函数组合成单个部署单元，从而降低复杂服务器应用的延迟和成本。然而，即使在小规模应用中，可能的融合配置数量也极其庞大，在生产环境中进行暴力基准测试既昂贵又耗时。

Method: 开发了一个能够分析复杂应用中所有可能融合配置的系统。通过仿真FaaS平台，该系统支持本地实验，无需重新配置实时平台，显著降低了相关成本和时间。评估了多个示例FaaS应用和资源限制下的所有融合配置。

Result: 研究结果表明，在分析成本和延迟权衡时，只有有限的融合配置代表最优解，这些最优解受到特定定价模型的强烈影响。

Conclusion: 通过本地仿真系统可以高效分析FaaS函数融合配置，识别最优解集，且最优配置受定价模型影响显著，为FaaS部署优化提供了实用工具。

Abstract: Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.
  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.

</details>


### [28] [Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering](https://arxiv.org/abs/2601.11487)
*Paulo Sérgio Almeida*

Main category: cs.DC

TL;DR: 本文提出了一种新的因果排序消息传递算法，通过结合发送方缓冲和接收方缓冲的混合方法，实现了恒定元数据大小和计算最优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的因果排序消息传递方法存在元数据开销过大或性能限制的问题。接收方缓冲方法需要大量元数据，而发送方缓冲方法（如Cykas算法）存在吞吐量扩展性和活性问题。需要一种既高效又通用的解决方案。

Method: 提出SPS+FIFO策略，证明SPS+FIFO蕴含因果排序。设计了一种新颖的混合算法：使用发送方缓冲来强制执行SPS（发送方发送许可），使用接收方缓冲来强制执行FIFO。通过精心选择数据结构，实现计算最优和摊销恒定处理开销。

Result: 新算法克服了纯发送方缓冲的限制，实现了每消息恒定元数据大小。算法计算最优，具有摊销恒定处理开销。据作者所知，这是首个具有这些特性的拓扑无关因果传递算法。

Conclusion: 提出的混合SPS+FIFO算法在保持通用性的同时，解决了现有因果排序算法的性能瓶颈，实现了高效、可扩展的因果消息传递。

Abstract: Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.

</details>
