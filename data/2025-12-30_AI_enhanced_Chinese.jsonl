{"id": "2512.22301", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22301", "abs": "https://arxiv.org/abs/2512.22301", "authors": ["Aayush Mainali", "Sirjan Ghimire"], "title": "A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography", "comment": null, "summary": "Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u7684\u7edf\u8ba1\u98ce\u9669\u6a21\u578b\u6765\u8bc4\u4f30\u540e\u91cf\u5b50\u5bc6\u7801\u5b9e\u73b0\u4e2d\u7684\u65f6\u5e8f\u4fa7\u4fe1\u9053\u6cc4\u6f0f\uff0c\u901a\u8fc7\u5408\u6210\u4e0d\u540c\u6267\u884c\u6761\u4ef6\u4e0b\u7684\u8ddf\u8e2a\u6570\u636e\uff0c\u4f7f\u7528\u591a\u79cd\u7edf\u8ba1\u6307\u6807\u91cf\u5316\u6cc4\u6f0f\u98ce\u9669\uff0c\u5e76\u5bf9\u4ee3\u8868\u6027\u683c\u57faKEM\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "motivation": "\u540e\u91cf\u5b50\u5bc6\u7801\uff08\u7279\u522b\u662f\u683c\u57fa\u65b9\u6848\uff09\u7684\u5b9e\u73b0\u53ef\u80fd\u4ea7\u751f\u4f9d\u8d56\u4e8e\u79d8\u5bc6\u7684\u65f6\u5e8f\u53d8\u5f02\u6027\uff0c\u800c\u5b9e\u9645\u65f6\u5e8f\u6d4b\u91cf\u53d7\u5230\u73af\u5883\u566a\u58f0\uff08\u5982\u8c03\u5ea6\u6548\u5e94\u3001\u4e89\u7528\u3001\u91cd\u5c3e\u5ef6\u8fdf\uff09\u7684\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u8bc4\u4f30\u65f6\u5e8f\u6cc4\u6f0f\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u573a\u666f\u7684\u7edf\u8ba1\u98ce\u9669\u6a21\u578b\uff0c\u5728\u7a7a\u95f2\u3001\u6296\u52a8\u548c\u8d1f\u8f7d\u4e09\u79cd\u6267\u884c\u6761\u4ef6\u4e0b\u4e3a\u4e24\u4e2a\u79d8\u5bc6\u7c7b\u522b\u5408\u6210\u8ddf\u8e2a\u6570\u636e\uff0c\u4f7f\u7528Welch's t\u68c0\u9a8c\u3001KS\u8ddd\u79bb\u3001Cliff's delta\u3001\u4e92\u4fe1\u606f\u548c\u5206\u5e03\u91cd\u53e0\u7b49\u591a\u79cd\u7edf\u8ba1\u6307\u6807\u91cf\u5316\u6cc4\u6f0f\uff0c\u5e76\u4ee5\u7c7b\u4f3cTLRI\u7684\u65b9\u5f0f\u7ec4\u5408\u5f97\u5230\u4e00\u81f4\u6027\u8bc4\u5206\u6765\u5bf9\u573a\u666f\u8fdb\u884c\u6392\u540d\u3002", "result": "\u5bf9\u4ee3\u8868\u6027\u683c\u57faKEM\u5bb6\u65cf\uff08Kyber\u3001Saber\u3001Frodo\uff09\u7684\u5206\u6790\u663e\u793a\uff1a\u7a7a\u95f2\u6761\u4ef6\u4e0b\u53ef\u533a\u5206\u6027\u6700\u4f73\uff1b\u6296\u52a8\u548c\u8d1f\u8f7d\u6761\u4ef6\u901a\u8fc7\u589e\u52a0\u65b9\u5dee\u548c\u91cd\u53e0\u6765\u524a\u5f31\u53ef\u533a\u5206\u6027\uff1b\u7f13\u5b58\u7d22\u5f15\u548c\u5206\u652f\u5f0f\u6cc4\u6f0f\u5f80\u5f80\u4ea7\u751f\u6700\u9ad8\u98ce\u9669\u4fe1\u53f7\uff1b\u5728\u76f8\u4f3c\u6cc4\u6f0f\u5047\u8bbe\u4e0b\uff0c\u66f4\u5feb\u7684\u65b9\u6848\u53ef\u80fd\u5177\u6709\u66f4\u9ad8\u7684\u5cf0\u503c\u98ce\u9669\u3002", "conclusion": "\u8be5\u7edf\u8ba1\u98ce\u9669\u6a21\u578b\u80fd\u591f\u5728\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u8fdb\u884c\u53ef\u91cd\u590d\u7684\u6bd4\u8f83\uff0c\u4e3a\u5e73\u53f0\u7279\u5b9a\u9a8c\u8bc1\u524d\u7684\u65f6\u5e8f\u4fa7\u4fe1\u9053\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5728\u540e\u91cf\u5b50\u5bc6\u7801\u5b9e\u73b0\u4e2d\u8bc6\u522b\u548c\u7f13\u89e3\u65f6\u5e8f\u6cc4\u6f0f\u98ce\u9669\u3002"}}
{"id": "2512.22306", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22306", "abs": "https://arxiv.org/abs/2512.22306", "authors": ["Chinmay Pushkar", "Sanchit Kabra", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection", "comment": "Under Review", "summary": "Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from \"count bias\" and \"selection bias\" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe \"under-counting\" (Recall dropping to less than 0.30) in complex Python files.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9C\u3001C++\u3001Python\u548cJavaScript\u56db\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u591a\u6f0f\u6d1e\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLM\u5728\u590d\u6742\u591a\u6f0f\u6d1e\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u5355\u6f0f\u6d1e\u6837\u672c\u6216\u51fd\u6570\u7ea7\u5206\u7c7b\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u8f6f\u4ef6\u4e2d\u591a\u4e2a\u76f8\u4e92\u4f5c\u7528\u7684\u6f0f\u6d1e\u5171\u5b58\u4e8e\u5927\u578b\u6587\u4ef6\u7684\u590d\u6742\u6027\u3002LLM\u5728\u591a\u6807\u7b7e\u4efb\u52a1\u4e2d\u5b58\u5728\"\u8ba1\u6570\u504f\u5dee\"\u548c\"\u9009\u62e9\u504f\u5dee\"\u95ee\u9898\uff0c\u4f46\u5728\u4ee3\u7801\u5b89\u5168\u9886\u57df\u5c1a\u672a\u5f97\u5230\u4e25\u683c\u91cf\u5316\u3002", "method": "\u6784\u5efa\u5305\u542b40,000\u4e2a\u6587\u4ef6\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6ce8\u5165\u63a7\u5236\u6570\u91cf\u7684\u6f0f\u6d1e\uff081\u30013\u30015\u30019\u4e2a\uff09\u5230\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u6837\u672c\uff087.5k-10k tokens\uff09\u4e2d\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecGPT-4o-mini\u3001Llama-3.3-70B\u548cQwen-2.5\u7cfb\u5217\u5728\u5185\u7684\u4e94\u79cd\u6700\u5148\u8fdbLLM\u3002", "result": "\u968f\u7740\u6f0f\u6d1e\u5bc6\u5ea6\u589e\u52a0\uff0cLLM\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002Llama-3.3-70B\u5728\u5355\u6f0f\u6d1eC\u4efb\u52a1\u4e2d\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684F1\u5206\u6570\uff08\u7ea60.97\uff09\uff0c\u4f46\u5728\u9ad8\u5bc6\u5ea6\u8bbe\u7f6e\u4e0b\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe40%\u3002Python\u548cJavaScript\u4e0eC/C++\u76f8\u6bd4\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u6a21\u578b\u5728\u590d\u6742Python\u6587\u4ef6\u4e2d\u51fa\u73b0\u4e25\u91cd\u7684\"\u8ba1\u6570\u4e0d\u8db3\"\uff08\u53ec\u56de\u7387\u964d\u81f30.30\u4ee5\u4e0b\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u591a\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u590d\u6742\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6f0f\u6d1e\u5bc6\u5ea6\u548c\u8de8\u7f16\u7a0b\u8bed\u8a00\u573a\u666f\u4e0b\u3002"}}
{"id": "2512.22307", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22307", "abs": "https://arxiv.org/abs/2512.22307", "authors": ["You Li", "Guannan Zhao", "Yuhao Ju", "Yunqi He", "Jie Gu", "Hai Zhou"], "title": "LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators", "comment": "Accepted by AAAI'26 as a conference paper and selected for oral presentation", "summary": "We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.", "AI": {"tldr": "LLA\u662f\u4e00\u79cd\u7ed3\u5408\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u4fdd\u62a4\u751f\u6210\u5f0fAI\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u5d4c\u5165\u5bc6\u94a5\u4f4d\u5230\u795e\u7ecf\u5143\u4e2d\u89e6\u53d1\u5f02\u5e38\u503c\u6765\u964d\u4f4e\u6027\u80fd\uff0c\u540c\u65f6\u5728AI\u52a0\u901f\u5668\u4e2d\u96c6\u6210\u8f7b\u91cf\u7ea7\u9501\u5b9a\u6a21\u5757\uff0c\u5b9e\u73b0\u6a21\u578b\u670d\u52a1\u7684\u8bb8\u53ef\u8bbf\u95ee\u3002", "motivation": "\u4fdd\u62a4\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u9632\u5fa1\u4f9b\u5e94\u94fe\u5a01\u80c1\uff0c\u5305\u62ec\u6a21\u578b\u76d7\u7a83\u3001\u6a21\u578b\u635f\u574f\u548c\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u8f6f\u4ef6\u65b9\u9762\uff1a\u5c06\u5bc6\u94a5\u4f4d\u5d4c\u5165\u795e\u7ecf\u5143\u4e2d\u89e6\u53d1\u5f02\u5e38\u503c\u964d\u4f4e\u6027\u80fd\uff0c\u5e76\u5e94\u7528\u4e0d\u53d8\u6027\u53d8\u6362\u6765\u9690\u85cf\u5bc6\u94a5\u503c\uff1b\u786c\u4ef6\u65b9\u9762\uff1a\u5728AI\u52a0\u901f\u5668\u4e2d\u96c6\u6210\u8f7b\u91cf\u7ea7\u9501\u5b9a\u6a21\u5757\uff0c\u4fdd\u6301\u4e0e\u5404\u79cd\u6570\u636e\u6d41\u6a21\u5f0f\u548c\u5de5\u5177\u94fe\u7684\u517c\u5bb9\u6027\u3002", "result": "LLA\u80fd\u591f\u62b5\u5fa1\u5e7f\u6cdb\u7684oracle\u5f15\u5bfc\u7684\u5bc6\u94a5\u4f18\u5316\u653b\u51fb\uff0c\u540c\u65f6\u4ec5\u4ea7\u751f\u6781\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\uff08\u5bf9\u4e8e7,168\u4e2a\u5bc6\u94a5\u4f4d\uff0c\u5f00\u9500\u5c0f\u4e8e0.1%\uff09\u3002", "conclusion": "LLA\u901a\u8fc7\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4e3a\u751f\u6210\u5f0fAI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u65b9\u6848\uff0c\u80fd\u591f\u9632\u5fa1\u591a\u79cd\u4f9b\u5e94\u94fe\u5a01\u80c1\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u517c\u5bb9\u6027\u3002"}}
{"id": "2512.22199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22199", "abs": "https://arxiv.org/abs/2512.22199", "authors": ["Teja Chinthala"], "title": "Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation", "comment": "10 pages, 2 figures, 2 tables. 36 experiments across 4 datasets with 3 random seeds. Code available upon request", "summary": "Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5411RAG\u67b6\u6784\uff0c\u901a\u8fc7\u9a8c\u8bc1\u540e\u7684\u9ad8\u8d28\u91cf\u751f\u6210\u54cd\u5e94\u56de\u5199\u5b9e\u73b0\u5b89\u5168\u7684\u77e5\u8bc6\u5e93\u6269\u5c55\uff0c\u76f8\u6bd4\u6807\u51c6RAG\u5c06\u8986\u76d6\u7387\u4ece20.33%\u63d0\u5347\u81f340.58%\uff0c\u540c\u65f6\u6bd4\u7b80\u5355\u56de\u5199\u51cf\u5c1172%\u7684\u6587\u6863\u6dfb\u52a0\u91cf\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u4f7f\u7528\u9759\u6001\u77e5\u8bc6\u5e93\uff0c\u65e0\u6cd5\u4ece\u7528\u6237\u4ea4\u4e92\u4e2d\u5b66\u4e60\u548c\u8fdb\u5316\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u6301\u7eed\u6539\u8fdb\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6269\u5c55\u77e5\u8bc6\u5e93\u53c8\u80fd\u9632\u6b62\u5e7b\u89c9\u6c61\u67d3\u7684\u5b89\u5168\u673a\u5236\u3002", "method": "\u63d0\u51fa\u53cc\u5411RAG\u67b6\u6784\uff0c\u5305\u542b\u591a\u9636\u6bb5\u63a5\u53d7\u5c42\uff1a\u57fa\u4e8eNLI\u7684\u8574\u542b\u9a8c\u8bc1\u3001\u5f52\u56e0\u68c0\u67e5\u548c\u65b0\u9896\u6027\u68c0\u6d4b\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u751f\u6210\u54cd\u5e94\u80fd\u591f\u5b89\u5168\u5730\u56de\u5199\u5230\u77e5\u8bc6\u5e93\u4e2d\uff0c\u5b9e\u73b0\u77e5\u8bc6\u79ef\u7d2f\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08Natural Questions\u3001TriviaQA\u3001HotpotQA\u3001Stack Overflow\uff09\u4e0a\uff0c\u53cc\u5411RAG\u8fbe\u523040.58%\u7684\u5e73\u5747\u8986\u76d6\u7387\uff0c\u51e0\u4e4e\u662f\u6807\u51c6RAG\uff0820.33%\uff09\u7684\u4e24\u500d\uff0c\u540c\u65f6\u6bd4\u7b80\u5355\u56de\u5199\u65b9\u6cd5\u51cf\u5c1172%\u7684\u6587\u6863\u6dfb\u52a0\u91cf\uff08140 vs 500\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e25\u683c\u9a8c\u8bc1\u673a\u5236\u63a7\u5236\u4e0b\uff0c\u81ea\u6539\u8fdb\u7684RAG\u7cfb\u7edf\u662f\u53ef\u884c\u4e14\u5b89\u5168\u7684\uff0c\u4e3a\u5b9e\u73b0\u80fd\u591f\u4ece\u90e8\u7f72\u4e2d\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2512.22201", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22201", "abs": "https://arxiv.org/abs/2512.22201", "authors": ["Vincent Chang", "Thee Ho", "Sunishchal Dev", "Kevin Zhu", "Shi Feng", "Kellin Pelrine", "Matthew Kowal"], "title": "Emergent Persuasion: Will LLMs Persuade Without Being Prompted?", "comment": "This paper was accepted to AAAI 2026 AIGOV Workshop", "summary": "With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u672a\u7ecf\u660e\u786e\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u4e5f\u53ef\u80fd\u4ea7\u751f\u8bf4\u670d\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u7ecf\u8fc7\u76d1\u7763\u5fae\u8c03\u540e\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u53ea\u5305\u542b\u826f\u6027\u8bdd\u9898\uff0c\u6a21\u578b\u4e5f\u4f1a\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u8bdd\u9898\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8bf4\u670d\u503e\u5411\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u5f0fAI\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0cAI\u5bf9\u4eba\u7c7b\u89c2\u70b9\u548c\u4fe1\u5ff5\u7684\u5f71\u54cd\u529b\u524d\u6240\u672a\u6709\u3002\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\"\u6ee5\u7528\"\u5a01\u80c1\u6a21\u578b\uff08\u5373\u6076\u610f\u884c\u4e3a\u8005\u8981\u6c42LLM\u8bf4\u670d\u7528\u6237\uff09\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6a21\u578b\u5728\u672a\u7ecf\u660e\u786e\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8bf4\u670d\u7684\u60c5\u51b5\uff0c\u4ee5\u8bc4\u4f30\u8fd9\u79cd\u65b0\u5174\u8bf4\u670d\u98ce\u9669\u7684\u5b9e\u9645\u5173\u6ce8\u7a0b\u5ea6\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e24\u79cd\u573a\u666f\u63a2\u7d22\u672a\u7ecf\u63d0\u793a\u7684\u8bf4\u670d\u884c\u4e3a\uff1a1\uff09\u901a\u8fc7\u5185\u90e8\u6fc0\u6d3b\u5f15\u5bfc\u4f7f\u6a21\u578b\u6cbf\u7279\u5b9a\u4eba\u683c\u7279\u8d28\u65b9\u5411\u8c03\u6574\uff1b2\uff09\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4f7f\u6a21\u578b\u5c55\u73b0\u76f8\u540c\u7279\u8d28\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5bf9\u6a21\u578b\u672a\u7ecf\u63d0\u793a\u8bf4\u670d\u503e\u5411\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u5411\u8bf4\u670d\u76f8\u5173\u6216\u4e0d\u76f8\u5173\u7279\u8d28\u8c03\u6574\uff0c\u5e76\u4e0d\u80fd\u53ef\u9760\u589e\u52a0\u6a21\u578b\u7684\u672a\u7ecf\u63d0\u793a\u8bf4\u670d\u503e\u5411\uff1b2\uff09\u76d1\u7763\u5fae\u8c03\u80fd\u663e\u8457\u589e\u52a0\u6a21\u578b\u7684\u672a\u7ecf\u63d0\u793a\u8bf4\u670d\u503e\u5411\uff1b3\uff09\u5373\u4f7f\u5728\u4ec5\u5305\u542b\u826f\u6027\u8bdd\u9898\u7684\u4e00\u822c\u8bf4\u670d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u4e5f\u4f1a\u4ea7\u751f\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u8bdd\u9898\u4e0a\u5177\u6709\u66f4\u9ad8\u8bf4\u670d\u503e\u5411\u7684\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6709\u5bb3\u7684\u8bf4\u670d\u884c\u4e3a\u53ef\u80fd\u4ee5\u65b0\u5174\u65b9\u5f0f\u51fa\u73b0\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u53ea\u5305\u542b\u826f\u6027\u5185\u5bb9\u3002\u76d1\u7763\u5fae\u8c03\u53ef\u80fd\u65e0\u610f\u4e2d\u589e\u52a0\u6a21\u578b\u5728\u672a\u7ecf\u63d0\u793a\u60c5\u51b5\u4e0b\u7684\u8bf4\u670d\u503e\u5411\uff0c\u7279\u522b\u662f\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u8bdd\u9898\u4e0a\uff0c\u8fd9\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5173\u6ce8\u3002"}}
{"id": "2512.22526", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22526", "abs": "https://arxiv.org/abs/2512.22526", "authors": ["Kichang Lee", "Sungmin Lee", "Jaeho Jin", "JeongGil Ko"], "title": "Verifiable Dropout: Turning Randomness into a Verifiable Claim", "comment": "5 pages, 6 figures, 1 table", "summary": "Modern cloud-based AI training relies on extensive telemetry and logs to ensure accountability. While these audit trails enable retrospective inspection, they struggle to address the inherent non-determinism of deep learning. Stochastic operations, such as dropout, create an ambiguity surface where attackers can mask malicious manipulations as natural random variance, granting them plausible deniability. Consequently, existing logging mechanisms cannot verify whether stochastic values were generated and applied honestly without exposing sensitive training data. To close this integrity gap, we introduce Verifiable Dropout, a privacy-preserving mechanism based on zero-knowledge proofs. We treat stochasticity not as an excuse but as a verifiable claim. Our approach binds dropout masks to a deterministic, cryptographically verifiable seed and proves the correct execution of the dropout operation. This design enables users to audit the integrity of stochastic training steps post-hoc, ensuring that randomness was neither biased nor cherry-picked, while strictly preserving the confidentiality of the model and data.", "AI": {"tldr": "\u63d0\u51faVerifiable Dropout\u673a\u5236\uff0c\u4f7f\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u786e\u4fdd\u6df1\u5ea6\u5b66\u4e60\u968f\u673a\u64cd\u4f5c\uff08\u5982dropout\uff09\u7684\u53ef\u9a8c\u8bc1\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u5ba1\u8ba1\u673a\u5236\u65e0\u6cd5\u9a8c\u8bc1\u968f\u673a\u503c\u662f\u5426\u88ab\u8bda\u5b9e\u751f\u6210\u548c\u5e94\u7528\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8e\u4e91\u7684AI\u8bad\u7ec3\u4f9d\u8d56\u5e7f\u6cdb\u7684\u9065\u6d4b\u548c\u65e5\u5fd7\u786e\u4fdd\u53ef\u8ffd\u6eaf\u6027\uff0c\u4f46\u73b0\u6709\u5ba1\u8ba1\u673a\u5236\u65e0\u6cd5\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u7684\u56fa\u6709\u975e\u786e\u5b9a\u6027\u3002\u968f\u673a\u64cd\u4f5c\uff08\u5982dropout\uff09\u521b\u5efa\u4e86\u4e00\u4e2a\u6a21\u7cca\u8868\u9762\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5c06\u6076\u610f\u64cd\u4f5c\u4f2a\u88c5\u6210\u81ea\u7136\u968f\u673a\u53d8\u5316\uff0c\u83b7\u5f97\u5408\u7406\u7684\u53ef\u5426\u8ba4\u6027\u3002\u73b0\u6709\u65e5\u5fd7\u673a\u5236\u65e0\u6cd5\u5728\u4e0d\u66b4\u9732\u654f\u611f\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u968f\u673a\u503c\u662f\u5426\u88ab\u8bda\u5b9e\u751f\u6210\u548c\u5e94\u7528\u3002", "method": "\u5f15\u5165Verifiable Dropout\uff0c\u4e00\u79cd\u57fa\u4e8e\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u5c06dropout\u63a9\u7801\u7ed1\u5b9a\u5230\u786e\u5b9a\u6027\u7684\u3001\u53ef\u52a0\u5bc6\u9a8c\u8bc1\u7684\u79cd\u5b50\uff0c\u5e76\u8bc1\u660edropout\u64cd\u4f5c\u7684\u6b63\u786e\u6267\u884c\u3002\u5c06\u968f\u673a\u6027\u89c6\u4e3a\u53ef\u9a8c\u8bc1\u7684\u58f0\u660e\u800c\u975e\u501f\u53e3\u3002", "result": "\u8be5\u8bbe\u8ba1\u4f7f\u7528\u6237\u80fd\u591f\u5728\u4e8b\u540e\u5ba1\u8ba1\u968f\u673a\u8bad\u7ec3\u6b65\u9aa4\u7684\u5b8c\u6574\u6027\uff0c\u786e\u4fdd\u968f\u673a\u6027\u65e2\u6ca1\u6709\u88ab\u504f\u7f6e\u4e5f\u6ca1\u6709\u88ab\u9009\u62e9\u6027\u4f7f\u7528\uff0c\u540c\u65f6\u4e25\u683c\u4fdd\u62a4\u6a21\u578b\u548c\u6570\u636e\u7684\u673a\u5bc6\u6027\u3002", "conclusion": "Verifiable Dropout\u586b\u8865\u4e86\u73b0\u6709\u5ba1\u8ba1\u673a\u5236\u7684\u5b8c\u6574\u6027\u7a7a\u767d\uff0c\u901a\u8fc7\u96f6\u77e5\u8bc6\u8bc1\u660e\u6280\u672f\u5b9e\u73b0\u4e86\u5bf9\u6df1\u5ea6\u5b66\u4e60\u968f\u673a\u64cd\u4f5c\u7684\u53ef\u9a8c\u8bc1\u6027\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u786e\u4fdd\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2512.22207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22207", "abs": "https://arxiv.org/abs/2512.22207", "authors": ["Ryan Spencer", "Roey Yaari", "Ritvik Vemavarapu", "Joyce Yang", "Steven Ngo", "Utkarsh Sharma"], "title": "GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks", "comment": null, "summary": "Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.", "AI": {"tldr": "GamiBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6298\u7eb8\u4efb\u52a1\u8bc4\u4f302D\u52303D\u89c4\u5212\u548c\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u548c\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\uff08\u8de8\u591a\u89c6\u89d2\u548c\u65f6\u95f4\u8ddf\u8e2a\u64cd\u7eb5\u7269\u4f53\u7684\u80fd\u529b\uff09\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u56fe\u50cf\u6216\u6700\u7ec8\u8f93\u51fa\uff0c\u672a\u80fd\u8003\u8651\u7a7a\u95f4\u63a8\u7406\u7684\u987a\u5e8f\u6027\u548c\u89c6\u89d2\u4f9d\u8d56\u6027\u7279\u70b9\u3002", "method": "\u5f15\u5165GamiBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b186\u4e2a\u5e38\u89c4\u548c186\u4e2a\u4e0d\u53ef\u80fd\u76842D\u6298\u75d5\u56fe\u6848\u53ca\u5176\u5bf9\u5e94\u76843D\u6298\u53e0\u5f62\u72b6\uff0c\u4ece6\u4e2a\u4e0d\u540c\u89c6\u89d2\u751f\u6210\u3002\u5305\u542b\u4e09\u4e2a\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff1a\u9884\u6d4b3D\u6298\u53e0\u914d\u7f6e\u3001\u533a\u5206\u6709\u6548\u89c6\u89d2\u3001\u68c0\u6d4b\u4e0d\u53ef\u80fd\u56fe\u6848\u3002\u5f15\u5165\u65b0\u7684\u8bca\u65ad\u6307\u6807\uff1a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u4e0d\u53ef\u80fd\u6298\u53e0\u9009\u62e9\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u662f\u9886\u5148\u6a21\u578b\u5982GPT-5\u548cGemini-2.5-Pro\u5728\u5355\u6b65\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002GamiBench\u4e3a\u8bc4\u4f30MLLMs\u7684\u51e0\u4f55\u7406\u89e3\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u3002", "conclusion": "GamiBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6298\u7eb8\u4efb\u52a1\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6807\u51c6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2512.22435", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22435", "abs": "https://arxiv.org/abs/2512.22435", "authors": ["Zining Wang", "Jian Gao", "Weimin Fu", "Xiaolong Guo", "Xuan Zhang"], "title": "AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience", "comment": null, "summary": "Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\\times$ overall pass rate, a 48$\\times$ Pass@1, and a 4$\\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.", "AI": {"tldr": "AnalogSAGE\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u548c\u4e09\u9636\u6bb5\u667a\u80fd\u4f53\u63a2\u7d22\u5b9e\u73b0\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u8fc7\u7387\u548c\u6548\u7387\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u7ecf\u9a8c\u548c\u76f4\u89c9\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u63d0\u793a\u9a71\u52a8\u7684\u7f51\u8868\u751f\u6210\u6216\u9884\u5b9a\u4e49\u62d3\u6251\u6a21\u677f\uff0c\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u89c4\u683c\u8981\u6c42\u3002", "method": "\u63d0\u51faAnalogSAGE\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u5206\u5c42\u8bb0\u5fc6\u5c42\u534f\u8c03\u4e09\u9636\u6bb5\u667a\u80fd\u4f53\u63a2\u7d22\uff0c\u652f\u6301\u57fa\u4e8e\u4eff\u771f\u53cd\u9988\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u4f7f\u7528\u5f00\u6e90SKY130 PDK\u548cngspice\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u572810\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u8fd0\u7b97\u653e\u5927\u5668\u8bbe\u8ba1\u95ee\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e8610\u500d\u603b\u4f53\u901a\u8fc7\u7387\u300148\u500dPass@1\u548c4\u500d\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u51cf\u5c11\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u53ef\u9760\u6027\u548c\u81ea\u4e3b\u6027\u3002", "conclusion": "\u5206\u5c42\u8bb0\u5fc6\u548c\u57fa\u4e8e\u4eff\u771f\u7684\u63a8\u7406\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u62df\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\uff0c\u5f00\u6e90\u4ee3\u7801\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2512.22616", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22616", "abs": "https://arxiv.org/abs/2512.22616", "authors": ["Mojtaba Eshghie", "Melissa Mazura", "Alexandre Bartel"], "title": "Raven: Mining Defensive Patterns in Ethereum via Semantic Transaction Revert Invariants Categories", "comment": null, "summary": "We frame Ethereum transactions reverted by invariants-require(<invariant>)/ assert(<invariant>)/if (<invariant>) revert statements in the contract implementation-as a positive signal of active on-chain defenses. Despite their value, the defensive patterns in these transactions remain undiscovered and underutilized in security research. We present Raven, a framework that aligns reverted transactions to the invariant causing the reversion in the smart contract source code, embeds these invariants using our BERT-based fine-tuned model, and clusters them by semantic intent to mine defensive invariant categories on Ethereum. Evaluated on a sample of 20,000 reverted transactions, Raven achieves cohesive and meaningful clusters of transaction-reverting invariants. Manual expert review of the mined 19 semantic clusters uncovers six new invariant categories absent from existing invariant catalogs, including feature toggles, replay prevention, proof/signature verification, counters, caller-provided slippage thresholds, and allow/ban/bot lists. To demonstrate the practical utility of this invariant catalog mining pipeline, we conduct a case study using one of the newly discovered invariant categories as a fuzzing oracle to detect vulnerabilities in a real-world attack. Raven thus can map Ethereum's successful defenses. These invariant categories enable security researchers to develop analysis tools based on data-driven security oracles extracted from the smart contracts' working defenses.", "AI": {"tldr": "Raven\u6846\u67b6\u901a\u8fc7\u5206\u6790\u4ee5\u592a\u574a\u4ea4\u6613\u56de\u6eda\uff0c\u6316\u6398\u667a\u80fd\u5408\u7ea6\u4e2d\u7684\u9632\u5fa1\u6027\u4e0d\u53d8\u5f0f\uff0c\u53d1\u73b0\u4e866\u4e2a\u65b0\u7684\u4e0d\u53d8\u5f0f\u7c7b\u522b\uff0c\u53ef\u7528\u4e8e\u5b89\u5168\u7814\u7a76\u5de5\u5177\u5f00\u53d1", "motivation": "\u4ee5\u592a\u574a\u4ea4\u6613\u56e0\u5408\u7ea6\u4e2d\u7684\u4e0d\u53d8\u5f0f\u68c0\u67e5\uff08require/assert/revert\uff09\u800c\u56de\u6eda\uff0c\u8fd9\u4e9b\u56de\u6eda\u4ea4\u6613\u662f\u6d3b\u8dc3\u94fe\u4e0a\u9632\u5fa1\u7684\u79ef\u6781\u4fe1\u53f7\uff0c\u4f46\u73b0\u6709\u7684\u9632\u5fa1\u6a21\u5f0f\u5728\u5b89\u5168\u7814\u7a76\u4e2d\u5c1a\u672a\u88ab\u53d1\u73b0\u548c\u5145\u5206\u5229\u7528", "method": "\u63d0\u51faRaven\u6846\u67b6\uff1a1\uff09\u5c06\u56de\u6eda\u4ea4\u6613\u4e0e\u5bfc\u81f4\u56de\u6eda\u7684\u667a\u80fd\u5408\u7ea6\u6e90\u4ee3\u7801\u4e2d\u7684\u4e0d\u53d8\u5f0f\u5bf9\u9f50\uff1b2\uff09\u4f7f\u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u6a21\u578b\u5d4c\u5165\u8fd9\u4e9b\u4e0d\u53d8\u5f0f\uff1b3\uff09\u6309\u8bed\u4e49\u610f\u56fe\u5bf9\u4e0d\u53d8\u5f0f\u8fdb\u884c\u805a\u7c7b\uff0c\u6316\u6398\u4ee5\u592a\u574a\u4e0a\u7684\u9632\u5fa1\u6027\u4e0d\u53d8\u5f0f\u7c7b\u522b", "result": "\u572820,000\u4e2a\u56de\u6eda\u4ea4\u6613\u6837\u672c\u4e0a\u8bc4\u4f30\uff0cRaven\u5b9e\u73b0\u4e86\u6709\u51dd\u805a\u529b\u548c\u6709\u610f\u4e49\u7684\u4ea4\u6613\u56de\u6eda\u4e0d\u53d8\u5f0f\u805a\u7c7b\u3002\u4e13\u5bb6\u624b\u52a8\u5ba1\u67e5\u6316\u6398\u51fa\u768419\u4e2a\u8bed\u4e49\u805a\u7c7b\uff0c\u53d1\u73b0\u4e866\u4e2a\u73b0\u6709\u4e0d\u53d8\u5f0f\u76ee\u5f55\u4e2d\u4e0d\u5b58\u5728\u7684\u65b0\u7c7b\u522b\uff1a\u529f\u80fd\u5207\u6362\u3001\u91cd\u653e\u9632\u62a4\u3001\u8bc1\u660e/\u7b7e\u540d\u9a8c\u8bc1\u3001\u8ba1\u6570\u5668\u3001\u8c03\u7528\u8005\u63d0\u4f9b\u7684\u6ed1\u70b9\u9608\u503c\u3001\u5141\u8bb8/\u7981\u6b62/\u673a\u5668\u4eba\u5217\u8868", "conclusion": "Raven\u80fd\u591f\u6620\u5c04\u4ee5\u592a\u574a\u6210\u529f\u7684\u9632\u5fa1\u673a\u5236\u3002\u8fd9\u4e9b\u4e0d\u53d8\u5f0f\u7c7b\u522b\u4f7f\u5b89\u5168\u7814\u7a76\u4eba\u5458\u80fd\u591f\u57fa\u4e8e\u4ece\u667a\u80fd\u5408\u7ea6\u5de5\u4f5c\u9632\u5fa1\u4e2d\u63d0\u53d6\u7684\u6570\u636e\u9a71\u52a8\u5b89\u5168\u9884\u8a00\u673a\u5f00\u53d1\u5206\u6790\u5de5\u5177\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u4f7f\u7528\u65b0\u53d1\u73b0\u7684\u4e0d\u53d8\u5f0f\u7c7b\u522b\u4f5c\u4e3a\u6a21\u7cca\u6d4b\u8bd5\u9884\u8a00\u673a\u6765\u68c0\u6d4b\u771f\u5b9e\u4e16\u754c\u653b\u51fb\u4e2d\u7684\u6f0f\u6d1e\u7684\u5b9e\u7528\u6027"}}
{"id": "2512.22210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22210", "abs": "https://arxiv.org/abs/2512.22210", "authors": ["Farjana Yesmin", "Romana Akter"], "title": "Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh", "comment": "7 pages, 6 figures, 5 tables", "summary": "Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u6d2a\u6c34\u540e\u63f4\u52a9\u5206\u914d\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u53bb\u504f\u6280\u672f\u51cf\u5c11\u5bf9\u8fb9\u7f18\u5316\u5730\u533a\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u3002", "motivation": "\u53d1\u5c55\u4e2d\u56fd\u5bb6\u707e\u540e\u63f4\u52a9\u5206\u914d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u8fb9\u7f18\u5316\u5730\u533a\u5f80\u5f80\u5904\u4e8e\u4e0d\u5229\u5730\u4f4d\uff0c\u8fd9\u5ef6\u7eed\u4e86\u5386\u53f2\u4e0d\u5e73\u7b49\u3002\u5b5f\u52a0\u62c9\u56fd\u4f5c\u4e3a\u6d2a\u6c34\u9891\u53d1\u56fd\u5bb6\uff0c\u9700\u8981\u66f4\u516c\u5e73\u7684\u63f4\u52a9\u5206\u914d\u673a\u5236\u3002", "method": "\u4f7f\u75282022\u5e74\u5b5f\u52a0\u62c9\u56fd\u6d2a\u6c34\u771f\u5b9e\u6570\u636e\uff0c\u5f00\u53d1\u5bf9\u6297\u6027\u53bb\u504f\u6a21\u578b\u9884\u6d4b\u6d2a\u6c34\u8106\u5f31\u6027\uff0c\u91c7\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42\u5b66\u4e60\u504f\u7f6e\u4e0d\u53d8\u8868\u793a\uff0c\u5c06\u533b\u7597AI\u4e2d\u7684\u516c\u5e73\u611f\u77e5\u8868\u793a\u5b66\u4e60\u6280\u672f\u5e94\u7528\u4e8e\u707e\u5bb3\u7ba1\u7406\u3002", "result": "\u572811\u4e2a\u5730\u533a\u768487\u4e2aupazilas\u4e0a\u6d4b\u8bd5\uff0c\u6846\u67b6\u5c06\u7edf\u8ba1\u5947\u5076\u5dee\u5f02\u51cf\u5c1141.6%\uff0c\u533a\u57df\u516c\u5e73\u5dee\u8ddd\u964d\u4f4e43.2%\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u9884\u6d4b\u51c6\u786e\u6027\uff08R\u5e73\u65b9=0.784 vs \u57fa\u7ebf0.811\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7b97\u6cd5\u516c\u5e73\u6280\u672f\u5728\u4eba\u9053\u4e3b\u4e49\u80cc\u666f\u4e0b\u7684\u6709\u6548\u5e94\u7528\uff0c\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u5b9e\u65bd\u66f4\u516c\u5e73\u707e\u5bb3\u6062\u590d\u7b56\u7565\u7684\u5de5\u5177\uff0c\u786e\u4fdd\u63f4\u52a9\u57fa\u4e8e\u771f\u5b9e\u9700\u6c42\u800c\u975e\u5386\u53f2\u5206\u914d\u6a21\u5f0f\u3002"}}
{"id": "2512.23062", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.23062", "abs": "https://arxiv.org/abs/2512.23062", "authors": ["Soham Pramanik", "Vimal William", "Arnab Raha", "Debayan Das", "Amitava Mukherjee", "Janet L. Paluh"], "title": "TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators", "comment": null, "summary": "The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTYTAN\uff1a\u57fa\u4e8e\u6cf0\u52d2\u7ea7\u6570\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u5f15\u64ce\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u786c\u4ef6\u8bbe\u8ba1\u548c\u4e13\u7528\u7b97\u6cd5\u52a8\u6001\u4f30\u8ba1\u6fc0\u6d3b\u51fd\u6570\u7684\u8fd1\u4f3c\uff0c\u5728\u8fb9\u7f18AI\u63a8\u7406\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u4f4e\u529f\u8017\u3002", "motivation": "\u968f\u7740AI\u67b6\u6784\u7684\u5feb\u901f\u53d1\u5c55\u548cAI\u8d4b\u80fd\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u8fb9\u7f18\u8ba1\u7b97\u5bf9\u7279\u5b9a\u9886\u57df\u67b6\u6784\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u89e3\u51b3AI\u7b97\u6cd5\u90e8\u7f72\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u548c\u80fd\u8017\u95ee\u9898\uff0c\u7279\u522b\u662f\u901a\u7528\u77e9\u9635\u4e58\u6cd5\u548c\u6fc0\u6d3b\u51fd\u6570\u7b49\u9ad8\u529f\u8017\u64cd\u4f5c\u3002", "method": "\u63d0\u51faTYTAN\u7cfb\u7edf\uff0c\u5305\u542b\u53ef\u91cd\u6784\u786c\u4ef6\u8bbe\u8ba1\u548c\u4e13\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u6cf0\u52d2\u7ea7\u6570\u8fd1\u4f3c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u52a8\u6001\u4f30\u8ba1\u6240\u9700\u8fd1\u4f3c\u7a0b\u5ea6\u4ee5\u6700\u5c0f\u5316\u4e0e\u57fa\u51c6\u7cbe\u5ea6\u7684\u504f\u5dee\u3002", "result": "\u5728Silvaco FreePDK45\u5de5\u827a\u8282\u70b9\u4e0a\u7684\u7cfb\u7edf\u7ea7\u4eff\u771f\u663e\u793a\uff0cTYTAN\u5de5\u4f5c\u9891\u7387>950MHz\uff0c\u76f8\u6bd4\u5f00\u6e90NVIDIA\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668(NVDLA)\u5b9e\u73b0\uff0c\u6027\u80fd\u63d0\u5347\u7ea62\u500d\uff0c\u529f\u8017\u964d\u4f4e\u7ea656%\uff0c\u9762\u79ef\u51cf\u5c11\u7ea635\u500d\u3002", "conclusion": "TYTAN\u901a\u8fc7\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u52a0\u901f\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18AI\u63a8\u7406\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22211", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22211", "abs": "https://arxiv.org/abs/2512.22211", "authors": ["Shaun Khoo", "Jessica Foo", "Roy Ka-Wei Lee"], "title": "With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems", "comment": "Accepted at IASEAI 2026 (Main Track) and AAAI 2026 3rd International AI Governance Workshop", "summary": "Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \\& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \\href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agentic Risk & Capability (ARC)\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6280\u672f\u6cbb\u7406\u6846\u67b6\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u8bc6\u522b\u3001\u8bc4\u4f30\u548c\u51cf\u8f7b\u7531\u667a\u80fdAI\u7cfb\u7edf\u5e26\u6765\u7684\u98ce\u9669\uff0c\u901a\u8fc7\u80fd\u529b\u4e2d\u5fc3\u89c6\u89d2\u5206\u6790\u98ce\u9669\u6e90\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u5b9e\u65bd\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fdAI\u7cfb\u7edf\u5177\u6709\u81ea\u4e3b\u884c\u52a8\u80fd\u529b\uff08\u5982\u4ee3\u7801\u6267\u884c\u3001\u4e92\u8054\u7f51\u4ea4\u4e92\u3001\u6587\u4ef6\u4fee\u6539\uff09\uff0c\u65e2\u5e26\u6765\u91cd\u5927\u673a\u9047\u4e5f\u4ea7\u751f\u65b0\u578b\u98ce\u9669\uff0c\u5bf9\u7ec4\u7ec7\u6cbb\u7406\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u5168\u9762\u8bc6\u522b\u3001\u8bc4\u4f30\u548c\u51cf\u8f7b\u8fd9\u4e9b\u591a\u6837\u4e14\u4e0d\u65ad\u6f14\u53d8\u7684\u98ce\u9669\u3002", "method": "\u5f15\u5165ARC\u6846\u67b6\uff0c\u91c7\u7528\u80fd\u529b\u4e2d\u5fc3\u89c6\u89d2\u5206\u6790\u667a\u80fdAI\u7cfb\u7edf\uff0c\u63d0\u70bc\u51fa\u4e09\u4e2a\u4e3b\u8981\u98ce\u9669\u6e90\uff08\u7ec4\u4ef6\u3001\u8bbe\u8ba1\u3001\u80fd\u529b\uff09\uff0c\u5efa\u7acb\u98ce\u9669\u6e90\u3001\u5177\u4f53\u98ce\u9669\u548c\u6280\u672f\u63a7\u5236\u4e4b\u95f4\u7684\u660e\u786e\u5173\u8054\uff0c\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u5b9e\u65bd\u65b9\u6cd5\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u6280\u672f\u6cbb\u7406\u6846\u67b6\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u7a33\u5065\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u667a\u80fdAI\u7684\u590d\u6742\u6027\uff0c\u652f\u6301\u5feb\u901f\u6709\u6548\u521b\u65b0\u540c\u65f6\u786e\u4fdd\u667a\u80fdAI\u7cfb\u7edf\u7684\u5b89\u5168\u3001\u53ef\u9760\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "conclusion": "ARC\u6846\u67b6\u4e3a\u7ec4\u7ec7\u7ba1\u7406\u667a\u80fdAI\u7cfb\u7edf\u98ce\u9669\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u80fd\u529b\u4e2d\u5fc3\u7684\u98ce\u9669\u5206\u6790\u65b9\u6cd5\u548c\u7ed3\u6784\u5316\u63a7\u5236\u63aa\u65bd\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u5728\u521b\u65b0\u4e0e\u98ce\u9669\u7ba1\u7406\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2512.22720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22720", "abs": "https://arxiv.org/abs/2512.22720", "authors": ["Murtaza Nikzad", "Kerem Atas"], "title": "When RSA Fails: Exploiting Prime Selection Vulnerabilities in Public Key Cryptography", "comment": null, "summary": "This paper explores vulnerabilities in RSA cryptosystems that arise from improper prime number selection during key generation. We examine two primary attack vectors: Fermat's factorization method, which exploits RSA keys generated with primes that are too close together, and the Greatest Common Divisor (GCD) attack, which exploits keys that share a common prime factor. Drawing from landmark research including Heninger et al.'s ``Mining Your Ps and Qs'' study, which discovered over 64,000 vulnerable TLS hosts, and B{\u00f6}ck's 2023 analysis of Fermat factorization in deployed systems, we demonstrate that these vulnerabilities remain prevalent in real-world cryptographic implementations. Our analysis reveals that weak random number generation in embedded devices is the primary cause of these failures, and we discuss mitigation strategies including proper entropy collection and prime validation checks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86RSA\u5bc6\u7801\u7cfb\u7edf\u4e2d\u56e0\u7d20\u6570\u9009\u62e9\u4e0d\u5f53\u5bfc\u81f4\u7684\u6f0f\u6d1e\uff0c\u4e3b\u8981\u5173\u6ce8\u8d39\u9a6c\u5206\u89e3\u6cd5\u548cGCD\u653b\u51fb\u4e24\u79cd\u653b\u51fb\u5411\u91cf\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6f0f\u6d1e\u5728\u73b0\u5b9e\u4e16\u754c\u52a0\u5bc6\u5b9e\u73b0\u4e2d\u4ecd\u7136\u666e\u904d\u5b58\u5728\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u7814\u7a76RSA\u5bc6\u7801\u7cfb\u7edf\u4e2d\u56e0\u7d20\u6570\u9009\u62e9\u4e0d\u5f53\u800c\u4ea7\u751f\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8d39\u9a6c\u5206\u89e3\u6cd5\u548cGCD\u653b\u51fb\u8fd9\u4e24\u79cd\u653b\u51fb\u5411\u91cf\uff0c\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u6f0f\u6d1e\u5728\u73b0\u5b9e\u4e16\u754c\u52a0\u5bc6\u5b9e\u73b0\u4e2d\u7684\u666e\u904d\u6027\u548c\u4e25\u91cd\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e24\u79cd\u4e3b\u8981\u653b\u51fb\u5411\u91cf\uff1a\u8d39\u9a6c\u5206\u89e3\u6cd5\uff08\u9488\u5bf9\u7d20\u6570\u8fc7\u4e8e\u63a5\u8fd1\u7684RSA\u5bc6\u94a5\uff09\u548c\u6700\u5927\u516c\u7ea6\u6570\u653b\u51fb\uff08\u9488\u5bf9\u5171\u4eab\u516c\u56e0\u5b50\u7684\u5bc6\u94a5\uff09\uff0c\u5e76\u501f\u9274Heninger\u7b49\u4eba\u7684\"Ps and Qs\"\u7814\u7a76\u548cB\u00f6ck 2023\u5e74\u7684\u8d39\u9a6c\u5206\u89e3\u5206\u6790\uff0c\u5bf9\u73b0\u5b9e\u4e16\u754c\u52a0\u5bc6\u5b9e\u73b0\u4e2d\u7684\u6f0f\u6d1e\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6f0f\u6d1e\u5728\u73b0\u5b9e\u4e16\u754c\u52a0\u5bc6\u5b9e\u73b0\u4e2d\u4ecd\u7136\u666e\u904d\u5b58\u5728\uff0c\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e2d\u7684\u5f31\u968f\u673a\u6570\u751f\u6210\u662f\u5bfc\u81f4\u8fd9\u4e9b\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\uff0cHeninger\u7b49\u4eba\u7684\u7814\u7a76\u53d1\u73b0\u4e86\u8d85\u8fc764,000\u4e2a\u6613\u53d7\u653b\u51fb\u7684TLS\u4e3b\u673a\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0cRSA\u5bc6\u94a5\u751f\u6210\u4e2d\u7684\u7d20\u6570\u9009\u62e9\u4e0d\u5f53\u4f1a\u5bfc\u81f4\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u5305\u62ec\u9002\u5f53\u7684\u71b5\u6536\u96c6\u548c\u7d20\u6570\u9a8c\u8bc1\u68c0\u67e5\u5728\u5185\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u5f3a\u8c03\u9700\u8981\u6539\u8fdb\u968f\u673a\u6570\u751f\u6210\u548c\u5bc6\u94a5\u9a8c\u8bc1\u673a\u5236\u3002"}}
{"id": "2512.22125", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22125", "abs": "https://arxiv.org/abs/2512.22125", "authors": ["Jithin VG", "Ditto PS"], "title": "GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems", "comment": null, "summary": "The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.", "AI": {"tldr": "GPU-Virt-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684GPU\u865a\u62df\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8f6f\u4ef6GPU\u865a\u62df\u5316\u7cfb\u7edf\uff08\u5982HAMi-core\u548cBUD-FCSP\uff09\u4e0e\u786c\u4ef6MIG\u6280\u672f\uff0c\u6db5\u76d656\u4e2a\u6027\u80fd\u6307\u6807\u548c10\u4e2a\u7c7b\u522b\uff0c\u4e3a\u591a\u79df\u6237\u73af\u5883\u4e2d\u7684GPU\u8d44\u6e90\u90e8\u7f72\u63d0\u4f9b\u51b3\u7b56\u4f9d\u636e\u3002", "motivation": "\u968f\u7740GPU\u52a0\u901f\u5de5\u4f5c\u8d1f\u8f7d\uff08\u7279\u522b\u662fAI\u548cLLM\u63a8\u7406\uff09\u7684\u6fc0\u589e\uff0c\u4e91\u548c\u5bb9\u5668\u73af\u5883\u5bf9\u9ad8\u6548GPU\u8d44\u6e90\u5171\u4eab\u7684\u9700\u6c42\u6025\u5267\u589e\u52a0\u3002\u867d\u7136NVIDIA\u7684MIG\u6280\u672f\u63d0\u4f9b\u786c\u4ef6\u7ea7\u9694\u79bb\uff0c\u4f46\u4ec5\u9650\u4e8e\u9ad8\u7aef\u6570\u636e\u4e2d\u5fc3GPU\u3002\u8f6f\u4ef6\u865a\u62df\u5316\u89e3\u51b3\u65b9\u6848\uff08\u5982HAMi-core\u548cBUD-FCSP\uff09\u4e3a\u66f4\u5e7f\u6cdb\u7684GPU\u7cfb\u5217\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86GPU-Virt-Bench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc756\u4e2a\u6027\u80fd\u6307\u6807\uff08\u5206\u4e3a10\u4e2a\u7c7b\u522b\uff09\u6765\u8bc4\u4f30GPU\u865a\u62df\u5316\u7cfb\u7edf\u3002\u8fd9\u4e9b\u7c7b\u522b\u5305\u62ec\uff1a\u5f00\u9500\u3001\u9694\u79bb\u8d28\u91cf\u3001LLM\u7279\u5b9a\u6027\u80fd\u3001\u5185\u5b58\u5e26\u5bbd\u3001\u7f13\u5b58\u884c\u4e3a\u3001PCIe\u541e\u5410\u91cf\u3001\u591aGPU\u901a\u4fe1\u3001\u8c03\u5ea6\u6548\u7387\u3001\u5185\u5b58\u788e\u7247\u5316\u548c\u9519\u8bef\u6062\u590d\u3002\u8be5\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u6bd4\u8f83\u8f6f\u4ef6\u865a\u62df\u5316\u65b9\u6cd5\u4e0e\u7406\u60f3MIG\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30HAMi-core\u3001BUD-FCSP\u548c\u6a21\u62df\u7684MIG\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u5b9e\u7528\u6027\uff0c\u63ed\u793a\u4e86\u5bf9\u4e8e\u751f\u4ea7\u90e8\u7f72\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u7684\u6027\u80fd\u7279\u5f81\u3002\u8be5\u6846\u67b6\u4e3a\u4ece\u4e1a\u8005\u5728\u591a\u79df\u6237\u73af\u5883\u4e2d\u90e8\u7f72GPU\u8d44\u6e90\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "GPU-Virt-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u586b\u8865\u4e86GPU\u865a\u62df\u5316\u7cfb\u7edf\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u5e2e\u52a9\u4ece\u4e1a\u8005\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u865a\u62df\u5316\u65b9\u6848\uff0c\u5e76\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684GPU\u8d44\u6e90\u90e8\u7f72\u63d0\u4f9b\u91cd\u8981\u51b3\u7b56\u4f9d\u636e\u3002"}}
{"id": "2512.22258", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.22258", "abs": "https://arxiv.org/abs/2512.22258", "authors": ["Satvik Tripathi"], "title": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method", "comment": null, "summary": "Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.", "AI": {"tldr": "Logic Sketch Prompting (LSP) \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u578b\u53d8\u91cf\u3001\u786e\u5b9a\u6027\u6761\u4ef6\u8bc4\u4f30\u5668\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u5668\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u4e25\u683c\u89c4\u5219\u9075\u5faa\u3001\u786e\u5b9a\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u4e25\u683c\u89c4\u5219\u9075\u5faa\u3001\u786e\u5b9a\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u7684\u4efb\u52a1\u4e0a\u4ecd\u7136\u4e0d\u53ef\u9760\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u3001\u76d1\u7ba1\u548c\u5b89\u5168\u5173\u952e\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u3002", "method": "\u63d0\u51faLogic Sketch Prompting (LSP)\u6846\u67b6\uff0c\u5305\u542b\u7c7b\u578b\u53d8\u91cf\u3001\u786e\u5b9a\u6027\u6761\u4ef6\u8bc4\u4f30\u5668\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u5668\uff0c\u80fd\u591f\u4ea7\u751f\u53ef\u8ffd\u8e2a\u548c\u53ef\u91cd\u590d\u7684\u8f93\u51fa\u3002\u5728\u4e24\u4e2a\u836f\u7406\u5b66\u903b\u8f91\u5408\u89c4\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\uff08Gemma 2\u3001Mistral\u3001Llama 3\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u6240\u6709\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\uff0cLSP\u59cb\u7ec8\u83b7\u5f97\u6700\u9ad8\u7684\u51c6\u786e\u7387\uff080.83-0.89\uff09\u548cF1\u5206\u6570\uff080.83-0.89\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\uff080.24-0.60\uff09\u3001\u7b80\u6d01\u63d0\u793a\uff080.16-0.30\uff09\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff080.56-0.75\uff09\u3002McNemar\u68c0\u9a8c\u663e\u793a\u51e0\u4e4e\u6240\u6709\u6bd4\u8f83\u4e2dLSP\u90fd\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u6539\u8fdb\uff08p < 0.01\uff09\u3002", "conclusion": "LSP\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u786e\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e00\u81f4\u6027\uff0c\u652f\u6301\u5176\u5728\u4e34\u5e8a\u3001\u76d1\u7ba1\u548c\u5b89\u5168\u5173\u952e\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u4f7f\u7528\u3002"}}
{"id": "2512.22789", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22789", "abs": "https://arxiv.org/abs/2512.22789", "authors": ["Ying Li", "Wenjun Qiu", "Faysal Hossain Shezan", "Kunlin Cai", "Michelangelo van Dam", "Lisa Austin", "David Lie", "Yuan Tian"], "title": "Breaking the illusion: Automated Reasoning of GDPR Consent Violations", "comment": "Accepted by the IEEE Symposium on Security and Privacy (S&P) 2026", "summary": "Recent privacy regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have established legal requirements for obtaining user consent regarding the collection, use, and sharing of personal data. These regulations emphasize that consent must be informed, freely given, specific, and unambiguous. However, there are still many violations, which highlight a gap between legal expectations and actual implementation. Consent mechanisms embedded in functional web forms across websites play a critical role in ensuring compliance with data protection regulations such as the GDPR and CCPA, as well as in upholding user autonomy and trust. However, current research has primarily focused on cookie banners and mobile app dialogs. These forms are diverse in structure, vary in legal basis, and are often difficult to locate or evaluate, creating a significant challenge for automated consent compliance auditing. In this work, we present Cosmic, a novel automated framework for detecting consent-related privacy violations in web forms. We evaluate our developed tool for auditing consent compliance in web forms, across 5,823 websites and 3,598 forms. Cosmic detects 3,384 violations on 94.1% of consent forms, covering key GDPR principles such as freely given consent, purpose disclosure, and withdrawal options. It achieves 98.6% and 99.1% TPR for consent and violation detection, respectively, demonstrating high accuracy and real-world applicability.", "AI": {"tldr": "Cosmic\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7f51\u9875\u8868\u5355\u4e2d\u7684\u540c\u610f\u76f8\u5173\u9690\u79c1\u8fdd\u89c4\uff0c\u57285823\u4e2a\u7f51\u7ad9\u548c3598\u4e2a\u8868\u5355\u4e2d\u53d1\u73b0\u4e86\u5927\u91cfGDPR/CCPA\u8fdd\u89c4\u884c\u4e3a\u3002", "motivation": "GDPR\u548cCCPA\u7b49\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\u7528\u6237\u540c\u610f\u5fc5\u987b\u662f\u77e5\u60c5\u3001\u81ea\u613f\u3001\u5177\u4f53\u548c\u660e\u786e\u7684\uff0c\u4f46\u73b0\u5b9e\u4e2d\u4ecd\u5b58\u5728\u8bb8\u591a\u8fdd\u89c4\u884c\u4e3a\u3002\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8cookie\u6a2a\u5e45\u548c\u79fb\u52a8\u5e94\u7528\u5bf9\u8bdd\u6846\uff0c\u800c\u7f51\u9875\u8868\u5355\u4e2d\u7684\u540c\u610f\u673a\u5236\u591a\u6837\u4e14\u96be\u4ee5\u81ea\u52a8\u5ba1\u8ba1\uff0c\u5b58\u5728\u5408\u89c4\u6027\u68c0\u6d4b\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86Cosmic\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7f51\u9875\u8868\u5355\u4e2d\u7684\u540c\u610f\u76f8\u5173\u9690\u79c1\u8fdd\u89c4\u3002\u8be5\u5de5\u5177\u80fd\u591f\u81ea\u52a8\u5ba1\u8ba1\u7f51\u9875\u8868\u5355\u7684\u5408\u89c4\u6027\uff0c\u8986\u76d6GDPR\u7684\u5173\u952e\u539f\u5219\u5982\u81ea\u613f\u540c\u610f\u3001\u76ee\u7684\u62ab\u9732\u548c\u64a4\u56de\u9009\u9879\u3002", "result": "\u57285823\u4e2a\u7f51\u7ad9\u548c3598\u4e2a\u8868\u5355\u7684\u8bc4\u4f30\u4e2d\uff0cCosmic\u572894.1%\u7684\u540c\u610f\u8868\u5355\u4e0a\u68c0\u6d4b\u52303384\u4e2a\u8fdd\u89c4\u3002\u5de5\u5177\u5728\u540c\u610f\u68c0\u6d4b\u548c\u8fdd\u89c4\u68c0\u6d4b\u65b9\u9762\u7684\u771f\u9633\u6027\u7387\u5206\u522b\u8fbe\u523098.6%\u548c99.1%\uff0c\u663e\u793a\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "Cosmic\u6846\u67b6\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u68c0\u6d4b\u7f51\u9875\u8868\u5355\u4e2d\u7684\u9690\u79c1\u5408\u89c4\u8fdd\u89c4\uff0c\u586b\u8865\u4e86\u5f53\u524d\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u76d1\u7ba1\u673a\u6784\u548c\u7f51\u7ad9\u8fd0\u8425\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5408\u89c4\u5ba1\u8ba1\u5de5\u5177\u3002"}}
{"id": "2512.22137", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22137", "abs": "https://arxiv.org/abs/2512.22137", "authors": ["Jiangwen Dong", "Jiayu Li", "Wanyu Lin"], "title": "HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration", "comment": null, "summary": "Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.", "AI": {"tldr": "HybridFlow\u662f\u4e00\u4e2a\u8d44\u6e90\u81ea\u9002\u5e94\u7684\u8fb9\u7f18-\u4e91\u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u89e3\u548c\u5e76\u884c\u6267\u884c\u6765\u964d\u4f4eLLM\u63a8\u7406\u5ef6\u8fdf\u548ctoken\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9762\u4e34\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548ctoken\u6d88\u8017\u7684\u6311\u6218\uff0c\u73b0\u6709\u8fb9\u7f18-\u4e91\u534f\u4f5c\u65b9\u6cd5\u91c7\u7528\u7c97\u7c92\u5ea6\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7ec6\u7c92\u5ea6\u63a8\u7406\u5e76\u884c\u6027\uff0c\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "HybridFlow\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4efb\u52a1\u5206\u89e3\u4e0e\u5e76\u884c\u6267\u884c\uff1a\u5c06\u590d\u6742\u67e5\u8be2\u52a8\u6001\u62c6\u5206\u4e3a\u76f8\u4e92\u4f9d\u8d56\u7684\u5b50\u4efb\u52a1\uff0c\u4f9d\u8d56\u5173\u7cfb\u6ee1\u8db3\u540e\u7acb\u5373\u6267\u884c\uff1b2) \u8d44\u6e90\u611f\u77e5\u5b50\u4efb\u52a1\u8def\u7531\uff1a\u5b66\u4e60\u578b\u8def\u7531\u5668\u6839\u636e\u9884\u6d4b\u7684\u6548\u7528\u589e\u76ca\u548c\u5b9e\u65f6\u9884\u7b97\u72b6\u6001\uff0c\u81ea\u9002\u5e94\u5730\u5c06\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5206\u914d\u7ed9\u8fb9\u7f18\u6216\u4e91\u7aef\u6a21\u578b\u3002", "result": "\u5728GPQA\u3001MMLU-Pro\u3001AIME\u548cLiveBench-Reasoning\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cHybridFlow\u6709\u6548\u51cf\u5c11\u4e86\u7aef\u5230\u7aef\u63a8\u7406\u65f6\u95f4\u548c\u603b\u4f53token\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "HybridFlow\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u8fb9\u7f18-\u4e91\u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u4e2d\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2512.22336", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22336", "abs": "https://arxiv.org/abs/2512.22336", "authors": ["Mengkang Hu", "Bowei Xia", "Yuran Wu", "Ailing Yu", "Yude Zou", "Qiguang Chen", "Shijian Wang", "Jiarui Jin", "Kexin Li", "Wenxiang Jiao", "Yuan Lu", "Ping Luo"], "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback", "comment": "48 pages, 15 tables, 7 figures, Project page: https://agent2world.github.io", "summary": "Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.", "AI": {"tldr": "Agent2World\uff1a\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u53cd\u9988\u5b9e\u73b0\u63a8\u7406\u65f6\u4e16\u754c\u6a21\u578b\u751f\u6210\uff0c\u5e76\u4f5c\u4e3a\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\u5f15\u64ce\uff0c\u5728PDDL\u548c\u53ef\u6267\u884c\u4ee3\u7801\u8868\u793a\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3LLMs\u751f\u6210\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff08\u5982PDDL\u57df\u6216\u53ef\u6267\u884c\u6a21\u62df\u5668\uff09\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u76d1\u7763\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u65e0\u6cd5\u6355\u6349\u4ea4\u4e92\u6267\u884c\u4e2d\u51fa\u73b0\u7684\u884c\u4e3a\u7ea7\u9519\u8bef\u3002", "method": "\u63d0\u51faAgent2World\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a1) Deep Researcher\u667a\u80fd\u4f53\u901a\u8fc7\u7f51\u9875\u641c\u7d22\u8fdb\u884c\u77e5\u8bc6\u5408\u6210\u4ee5\u89e3\u51b3\u89c4\u8303\u7f3a\u53e3\uff1b2) Model Developer\u667a\u80fd\u4f53\u5b9e\u73b0\u53ef\u6267\u884c\u4e16\u754c\u6a21\u578b\uff1b3) \u4e13\u95e8\u7684Testing Team\u8fdb\u884c\u81ea\u9002\u5e94\u5355\u5143\u6d4b\u8bd5\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u6db5\u76d6PDDL\u548c\u53ef\u6267\u884c\u4ee3\u7801\u8868\u793a\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u63a8\u7406\u65f6\u6027\u80fd\uff0c\u53d6\u5f97\u4e00\u81f4\u7684SOTA\u7ed3\u679c\u3002\u57fa\u4e8e\u6d4b\u8bd5\u56e2\u961f\u53cd\u9988\u751f\u6210\u7684\u8bad\u7ec3\u8f68\u8ff9\u8fdb\u884c\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u4e16\u754c\u6a21\u578b\u751f\u6210\u80fd\u529b\u5e73\u5747\u76f8\u5bf9\u63d0\u534730.95%\u3002", "conclusion": "Agent2World\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u65f6\u4e16\u754c\u6a21\u578b\u751f\u6210\uff0c\u8fd8\u4f5c\u4e3a\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\u5f15\u64ce\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u53cd\u9988\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.22367", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22367", "abs": "https://arxiv.org/abs/2512.22367", "authors": ["\u00c1ngel Aso-Mollar", "Diego Aineto", "Enrico Scala", "Eva Onaindia"], "title": "Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions", "comment": null, "summary": "Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.", "AI": {"tldr": "\u5c06\u5e26\u6709\u63a7\u5236\u53c2\u6570\u7684\u6570\u5b57\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u7b80\u5355\u6570\u5b57\u4efb\u52a1\uff0c\u4f7f\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u65e0\u9650\u52a8\u4f5c\u7a7a\u95f4", "motivation": "\u6807\u51c6\u6570\u5b57\u89c4\u5212\u6a21\u578b\u5f15\u5165\u63a7\u5236\u53c2\u6570\u540e\uff0c\u52a8\u4f5c\u6570\u91cf\u53ef\u80fd\u65e0\u9650\uff0c\u5bfc\u81f4\u73b0\u6709\u57fa\u4e8e\u52a8\u4f5c\u7ed3\u6784\u7684\u6570\u5b57\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0d\u53ef\u884c", "method": "\u8bc6\u522b\u53ef\u63a7\u7b80\u5355\u6570\u5b57\u95ee\u9898\u5b50\u96c6\uff0c\u91c7\u7528\u4e50\u89c2\u7f16\u8bd1\u65b9\u6cd5\u5c06\u5176\u8f6c\u5316\u4e3a\u7b80\u5355\u6570\u5b57\u4efb\u52a1\uff0c\u5c06\u63a7\u5236\u76f8\u5173\u8868\u8fbe\u5f0f\u62bd\u8c61\u4e3a\u6709\u754c\u5e38\u6570\u6548\u679c\u548c\u5bbd\u677e\u524d\u63d0\u6761\u4ef6", "result": "\u63d0\u51fa\u7684\u7f16\u8bd1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f7f\u7528\u5b50\u76ee\u6807\u542f\u53d1\u5f0f\u6765\u4f30\u8ba1\u76ee\u6807\u8ddd\u79bb\uff0c\u662f\u5904\u7406\u65e0\u9650\u52a8\u4f5c\u7a7a\u95f4\u7684\u53ef\u884c\u6709\u6548\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u4f20\u7edf\u6570\u5b57\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u63a8\u52a8\u4e86\u5e26\u6709\u63a7\u5236\u53c2\u6570\u7684\u6570\u5b57\u89c4\u5212\u9886\u57df\u7684\u6280\u672f\u8fb9\u754c"}}
{"id": "2512.22142", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22142", "abs": "https://arxiv.org/abs/2512.22142", "authors": ["Leyang Xue", "Meghana Madhyastha", "Myungjin Lee", "Amos Storkey", "Randal Burns", "Mahesh K. Marina"], "title": "On Harnessing Idle Compute at the Edge for Foundation Model Training", "comment": "Extended abstract version of this paper appeared in ACM MobiCom 2025", "summary": "The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.\n  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.\n  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.", "AI": {"tldr": "Cleave\u662f\u4e00\u4e2a\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5206\u5e03\u5f0f\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6df7\u5408\u5f20\u91cf\u5e76\u884c\u548c\u53c2\u6570\u670d\u52a1\u5668\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u7684\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u3001\u5185\u5b58\u9650\u5236\u548c\u901a\u4fe1\u5f00\u9500\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u751f\u6001\u7cfb\u7edf\u9ad8\u5ea6\u4e2d\u5fc3\u5316\uff0c\u4ec5\u9650\u4e8e\u5927\u578b\u4e91\u6570\u636e\u4e2d\u5fc3\u8fd0\u8425\u5546\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u5229\u7528\u8fb9\u7f18\u8bbe\u5907\u95f2\u7f6e\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6c11\u4e3b\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u3001\u53ef\u6269\u5c55\u6027\u6709\u9650\u3001\u5185\u5b58\u8d85\u9650\u3001\u901a\u4fe1\u5f00\u9500\u5927\u3001\u8bbe\u5907\u5f02\u6784\u6027\u548c\u52a8\u6001\u6027\u5904\u7406\u4e0d\u4f73\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faCleave\u8303\u5f0f\uff0c\u91c7\u7528\u9009\u62e9\u6027\u6df7\u5408\u5f20\u91cf\u5e76\u884c\u65b9\u6cd5\u7cbe\u7ec6\u5212\u5206\u8bad\u7ec3\u64cd\u4f5c\uff0c\u7ed3\u5408\u53c2\u6570\u670d\u52a1\u5668\u4e2d\u5fc3\u7684\u8bad\u7ec3\u6846\u67b6\u6765\u5904\u7406\u8bbe\u5907\u5185\u5b58\u9650\u5236\u5e76\u907f\u514d\u901a\u4fe1\u74f6\u9888\u3002\u540c\u65f6\u4f7f\u7528\u6210\u672c\u4f18\u5316\u6a21\u578b\u6307\u5bfc\u8bbe\u5907\u9009\u62e9\u548c\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\uff0c\u6709\u6548\u5e94\u5bf9\u8bbe\u5907\u5f02\u6784\u6027\u548c\u52a8\u6001\u53d8\u5316\u3002", "result": "\u8bc4\u4f30\u663e\u793aCleave\u80fd\u591f\u5339\u914d\u57fa\u4e8e\u4e91\u7684GPU\u8bad\u7ec3\u6027\u80fd\uff0c\u53ef\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u548c\u6570\u5343\u53f0\u8bbe\u5907\uff0c\u652f\u6301\u6bd4\u57fa\u7ebf\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u591a8\u500d\u7684\u8bbe\u5907\u6570\u91cf\u3002\u5728\u6bcf\u6279\u6b21\u8bad\u7ec3\u65f6\u95f4\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u5feb10\u500d\uff0c\u5e76\u80fd\u9ad8\u6548\u5904\u7406\u8bbe\u5907\u6545\u969c\uff0c\u6062\u590d\u901f\u5ea6\u6bd4\u5148\u524d\u65b9\u6cd5\u5feb\u81f3\u5c11100\u500d\u3002", "conclusion": "Cleave\u901a\u8fc7\u521b\u65b0\u7684\u9009\u62e9\u6027\u6df7\u5408\u5f20\u91cf\u5e76\u884c\u548c\u53c2\u6570\u670d\u52a1\u5668\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8bad\u7ec3\u5927\u578b\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22396", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.22396", "abs": "https://arxiv.org/abs/2512.22396", "authors": ["Bhanu Prakash Vangala", "Sajid Mahmud", "Pawan Neupane", "Joel Selvaraj", "Jianlin Cheng"], "title": "HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification", "comment": null, "summary": "Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86HalluMatData\u57fa\u51c6\u6570\u636e\u96c6\u548cHalluMatDetector\u591a\u9636\u6bb5\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u7f13\u89e3\u6750\u6599\u79d1\u5b66\u9886\u57dfAI\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5c06\u5e7b\u89c9\u7387\u964d\u4f4e\u4e8630%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5b58\u5728\u751f\u6210\u4e8b\u5b9e\u9519\u8bef\u6216\u8bef\u5bfc\u6027\u4fe1\u606f\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u8fd9\u635f\u5bb3\u4e86\u7814\u7a76\u5b8c\u6574\u6027\uff0c\u7279\u522b\u662f\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86HalluMatData\u57fa\u51c6\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86HalluMatDetector\u591a\u9636\u6bb5\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5185\u5728\u9a8c\u8bc1\u3001\u591a\u6e90\u68c0\u7d22\u3001\u77db\u76fe\u56fe\u5206\u6790\u548c\u57fa\u4e8e\u5ea6\u91cf\u7684\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6750\u6599\u79d1\u5b66\u4e0d\u540c\u5b50\u9886\u57df\u7684\u5e7b\u89c9\u6c34\u5e73\u5dee\u5f02\u663e\u8457\uff0c\u9ad8\u71b5\u67e5\u8be2\u8868\u73b0\u51fa\u66f4\u5927\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\u3002\u4f7f\u7528HalluMatDetector\u9a8c\u8bc1\u6d41\u7a0b\u5c06\u5e7b\u89c9\u7387\u6bd4\u6807\u51c6LLM\u8f93\u51fa\u964d\u4f4e\u4e8630%\u3002", "conclusion": "\u901a\u8fc7HalluMatData\u6570\u636e\u96c6\u548cHalluMatDetector\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3\u6750\u6599\u79d1\u5b66\u9886\u57dfAI\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u51fa\u7684PHCS\u6307\u6807\u80fd\u591f\u91cf\u5316\u8bed\u4e49\u7b49\u6548\u67e5\u8be2\u4e0b\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u6a21\u578b\u53ef\u9760\u6027\u63d0\u4f9b\u66f4\u6df1\u5c42\u6b21\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.22147", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.22147", "abs": "https://arxiv.org/abs/2512.22147", "authors": ["Ruifan Chu", "Anbang Wang", "Xiuxiu Bai", "Shuai Liu", "Xiaoshe Dong"], "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs", "comment": null, "summary": "In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684LLM\u6846\u67b6\uff0c\u901a\u8fc7\u6027\u80fd\u53cd\u9988\u4f18\u5316GPU\u5185\u6838\uff0c\u65e0\u9700\u6784\u5efa\u5b8c\u6574\u5e94\u7528\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u3001\u4f4e\u6210\u672c\u7684GPU\u5185\u6838\u4f18\u5316\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\uff0c\u70ed\u70b9GPU\u5185\u6838\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u4e13\u5bb6\u624b\u52a8\u8c03\u4f18\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u79fb\u690d\u3002\u73b0\u6709LLM\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5185\u6838\u53ef\u4ee5\u5ec9\u4ef7\u7f16\u8bd1\u6267\u884c\uff0c\u4f46\u5728\u5927\u578b\u5e94\u7528\u4e2d\u5b8c\u6574\u6784\u5efa\u548c\u8fd0\u884c\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4ece\u72ec\u7acb\u63d0\u53d6\u7684\u70ed\u70b9\u5185\u6838\u81ea\u52a8\u751f\u6210\u6700\u5c0f\u53ef\u6267\u884c\u7a0b\u5e8f(MEP)\uff0c\u8fdb\u884c\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\u548c\u8bc4\u4f30\u3002\u96c6\u6210\u81ea\u52a8\u9519\u8bef\u4fee\u590d\u548c\u6027\u80fd\u6a21\u5f0f\u7ee7\u627f\u6765\u4fee\u590d\u6545\u969c\u3001\u4fdd\u6301\u6b63\u786e\u6027\u3001\u91cd\u7528\u6709\u6548\u7684\u5e73\u94fa/\u5185\u5b58/\u540c\u6b65\u7b56\u7565\uff0c\u964d\u4f4e\u641c\u7d22\u6210\u672c\u3002\u4f18\u5316\u540e\u7684\u53d8\u4f53\u91cd\u65b0\u96c6\u6210\u5230\u539f\u59cb\u5e94\u7528\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728NVIDIA GPU\u548cHaiguang DCU\u5e73\u53f0\u4e0a\u8bc4\u4f30\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u8fbe\u5230\uff1aPolyBench\u5728NVIDIA\u4e0a5.05\u500d\uff0cPolyBench\u5728DCU\u4e0a7.77\u500d\uff0cAMD APP SDK\u4e0a1.77\u500d\uff0c\u4e09\u4e2a\u70ed\u70b9\u5185\u6838\u4e0a1.25\u500d\uff0c\u8d85\u8d8a\u76f4\u63a5LLM\u4f18\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5b8c\u6574\u6e90\u4ee3\u7801\u4f9d\u8d56\uff0c\u63d0\u4f9b\u8de8\u5e73\u53f0\u53ef\u79fb\u690d\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u3001\u4f4e\u6210\u672c\u7684GPU\u5185\u6838\u4f18\u5316\u3002"}}
{"id": "2512.23124", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23124", "abs": "https://arxiv.org/abs/2512.23124", "authors": ["Paulo Fernandes Biao"], "title": "SecureBank: A Financially-Aware Zero Trust Architecture for High-Assurance Banking Systems", "comment": "10 pages, 12 figures. Research article proposing a financially-aware Zero Trust architecture for banking systems with simulation-based evaluation", "summary": "Financial institutions increasingly rely on distributed architectures, open banking APIs, cloud native infrastructures, and high frequency digital transactions. These transformations expand the attack surface and expose limitations in traditional perimeter based security models. While Zero Trust architectures provide essential security principles, most existing frameworks do not explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, or automation weighted by economic impact.\n  This paper introduces SecureBank, a financially aware and context adaptive Zero Trust architecture designed specifically for high assurance banking systems. The proposed framework integrates Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. A Monte Carlo simulation evaluates SecureBank against a representative rule based baseline architecture using metrics such as the Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).\n  The results demonstrate that SecureBank significantly improves automated attack handling and accelerates identity trust adaptation while preserving conservative and regulator aligned levels of transactional integrity. Beyond experimental validation, SecureBank is intended to serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments.", "AI": {"tldr": "SecureBank\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u9ad8\u4fdd\u969c\u94f6\u884c\u7cfb\u7edf\u8bbe\u8ba1\u7684\u8d22\u52a1\u611f\u77e5\u548c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u96f6\u4fe1\u4efb\u67b6\u6784\uff0c\u96c6\u6210\u4e86\u8d22\u52a1\u96f6\u4fe1\u4efb\u3001\u81ea\u9002\u5e94\u8eab\u4efd\u8bc4\u5206\u3001\u4e0a\u4e0b\u6587\u5fae\u5206\u5272\u548c\u5f71\u54cd\u9a71\u52a8\u7684\u5b89\u5168\u81ea\u52a8\u5316\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u67b6\u6784\u3002", "motivation": "\u91d1\u878d\u673a\u6784\u65e5\u76ca\u4f9d\u8d56\u5206\u5e03\u5f0f\u67b6\u6784\u3001\u5f00\u653e\u94f6\u884cAPI\u3001\u4e91\u539f\u751f\u57fa\u7840\u8bbe\u65bd\u548c\u9ad8\u9891\u6570\u5b57\u4ea4\u6613\uff0c\u8fd9\u4e9b\u8f6c\u53d8\u6269\u5927\u4e86\u653b\u51fb\u9762\u5e76\u66b4\u9732\u4e86\u4f20\u7edf\u57fa\u4e8e\u8fb9\u754c\u7684\u5b89\u5168\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002\u867d\u7136\u96f6\u4fe1\u4efb\u67b6\u6784\u63d0\u4f9b\u4e86\u57fa\u672c\u5b89\u5168\u539f\u5219\uff0c\u4f46\u5927\u591a\u6570\u73b0\u6709\u6846\u67b6\u6ca1\u6709\u660e\u786e\u7eb3\u5165\u4ea4\u6613\u8bed\u4e49\u3001\u91d1\u878d\u98ce\u9669\u5efa\u6a21\u3001\u81ea\u9002\u5e94\u8eab\u4efd\u4fe1\u4efb\u6216\u57fa\u4e8e\u7ecf\u6d4e\u5f71\u54cd\u7684\u81ea\u52a8\u5316\u3002", "method": "\u63d0\u51faSecureBank\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u8d22\u52a1\u96f6\u4fe1\u4efb\u3001\u81ea\u9002\u5e94\u8eab\u4efd\u8bc4\u5206\u3001\u4e0a\u4e0b\u6587\u5fae\u5206\u5272\u548c\u5f71\u54cd\u9a71\u52a8\u7684\u5b89\u5168\u81ea\u52a8\u5316\u3002\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8bc4\u4f30SecureBank\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u67b6\u6784\uff0c\u4f7f\u7528\u4ea4\u6613\u5b8c\u6574\u6027\u6307\u6570(TII)\u3001\u8eab\u4efd\u4fe1\u4efb\u9002\u5e94\u6c34\u5e73(ITAL)\u548c\u5b89\u5168\u81ea\u52a8\u5316\u6548\u7387(SAE)\u7b49\u6307\u6807\u3002", "result": "SecureBank\u663e\u8457\u6539\u5584\u4e86\u81ea\u52a8\u5316\u653b\u51fb\u5904\u7406\uff0c\u52a0\u901f\u4e86\u8eab\u4efd\u4fe1\u4efb\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4fdd\u5b88\u4e14\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u7684\u4ea4\u6613\u5b8c\u6574\u6027\u6c34\u5e73\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u8be5\u67b6\u6784\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SecureBank\u4e0d\u4ec5\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8fd8\u65e8\u5728\u4f5c\u4e3a\u53d7\u76d1\u7ba1\u91d1\u878d\u73af\u5883\u4e2d\u8d22\u52a1\u611f\u77e5\u96f6\u4fe1\u4efb\u7cfb\u7edf\u7684\u53c2\u8003\u67b6\u6784\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3a\u9ad8\u4fdd\u969c\u94f6\u884c\u7cfb\u7edf\u63d0\u4f9b\u4e13\u95e8\u8bbe\u8ba1\u7684\u81ea\u9002\u5e94\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22398", "abs": "https://arxiv.org/abs/2512.22398", "authors": ["Ozan Oguztuzun", "Cerag Oguztuzun"], "title": "Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings", "comment": null, "summary": "Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.", "AI": {"tldr": "GatedBias\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u95e8\u63a7\u9002\u5e94\u5c06\u51bb\u7ed3\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u9002\u914d\u5230\u4e2a\u4f53\u7528\u6237\u4e0a\u4e0b\u6587\uff0c\u4ec5\u9700\u7ea6300\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e2a\u6027\u5316\u6392\u540d\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u6a21\u578b\u5728\u94fe\u63a5\u9884\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u4e2a\u4f53\u7528\u6237\u504f\u597d\uff0c\u5bfc\u81f4\u901a\u7528\u5173\u7cfb\u63a8\u7406\u4e0e\u4e2a\u6027\u5316\u6392\u540d\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u8131\u8282\u3002", "method": "\u63d0\u51faGatedBias\u6846\u67b6\uff0c\u91c7\u7528\u7ed3\u6784\u95e8\u63a7\u9002\u5e94\u673a\u5236\uff1a\u5c06\u7528\u6237\u7279\u5b9a\u7279\u5f81\u4e0e\u56fe\u5bfc\u51fa\u7684\u4e8c\u5143\u95e8\u7ed3\u5408\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6bcf\u5b9e\u4f53\u504f\u7f6e\uff0c\u4ec5\u9700\u7ea6300\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5f71\u54cd\u5168\u5c40\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08Amazon-Book\u548cLast-FM\uff09\u4e0a\u8bc4\u4f30\uff0c\u663e\u793a\u5728\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u6709\u7edf\u8ba1\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u7fa4\u4f53\u6027\u80fd\u3002\u53cd\u4e8b\u5b9e\u6270\u52a8\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u56e0\u679c\u54cd\u5e94\u6027\uff1a\u53d7\u76ca\u4e8e\u7279\u5b9a\u504f\u597d\u4fe1\u53f7\u7684\u5b9e\u4f53\u5728\u4fe1\u53f7\u589e\u5f3a\u65f6\u6392\u540d\u6539\u8fdb\u63d0\u9ad86-30\u500d\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u7684\u4e2a\u6027\u5316\u9002\u5e94\u53ef\u4ee5\u662f\u53c2\u6570\u9ad8\u6548\u4e14\u56e0\u679c\u53ef\u9a8c\u8bc1\u7684\uff0c\u80fd\u591f\u6865\u63a5\u901a\u7528\u77e5\u8bc6\u8868\u793a\u4e0e\u4e2a\u4f53\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2512.22149", "categories": ["cs.DC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22149", "abs": "https://arxiv.org/abs/2512.22149", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan"], "title": "Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments", "comment": "7 pages, 2 figures", "summary": "Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94GPU\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u5728serverless\u5e73\u53f0\u4e0a\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u73b085%\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9759\u6001\u5206\u914d\u76f8\u5f53\u7684\u541e\u5410\u91cf", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728serverless GPU\u5e73\u53f0\u4e0a\u90e8\u7f72\u9762\u4e34\u8d44\u6e90\u5206\u914d\u6311\u6218\uff0c\u5305\u62ec\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u3001\u4e0d\u540c\u8ba1\u7b97\u9700\u6c42\u548c\u6210\u672c\u6548\u76ca\u6269\u5c55\u9700\u6c42", "method": "\u81ea\u9002\u5e94GPU\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u57fa\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u3001\u667a\u80fd\u4f53\u4f18\u5148\u7ea7\u548c\u6700\u5c0f\u8d44\u6e90\u9700\u6c42\u52a8\u6001\u5206\u914dGPU\u8d44\u6e90\uff0c\u4f7f\u7528O(N)\u590d\u6742\u5ea6\u7b97\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u9002\u5e94", "result": "\u76f8\u6bd4\u8f6e\u8be2\u8c03\u5ea6\u51cf\u5c1185%\u5ef6\u8fdf\uff0c\u541e\u5410\u91cf\u4e0e\u9759\u6001\u5206\u914d\u76f8\u5f53\uff1b\u5728\u5ef6\u8fdf\u3001\u6210\u672c\u548cGPU\u5229\u7528\u7387\u6307\u6807\u4e0a\u4f18\u4e8e\u9759\u6001\u5e73\u5747\u548c\u8f6e\u8be2\u7b56\u7565", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728serverless GPU\u57fa\u7840\u8bbe\u65bd\u4e0a\u90e8\u7f72\u6210\u672c\u6548\u76ca\u9ad8\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.22431", "categories": ["cs.AI", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.22431", "abs": "https://arxiv.org/abs/2512.22431", "authors": ["Yifan Zhang", "Mengdi Wang"], "title": "Monadic Context Engineering", "comment": "Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering", "summary": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMonadic Context Engineering (MCE)\uff0c\u4e00\u79cd\u57fa\u4e8e\u51fd\u5b50\u3001\u5e94\u7528\u51fd\u5b50\u548c\u5355\u5b50\u7684\u4ee3\u6570\u7ed3\u6784\u7684\u65b0\u578b\u667a\u80fd\u4f53\u67b6\u6784\u8303\u5f0f\uff0c\u7528\u4e8e\u89e3\u51b3\u5f53\u524d\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e2d\u72b6\u6001\u7ba1\u7406\u3001\u9519\u8bef\u5904\u7406\u548c\u5e76\u53d1\u7b49\u8de8\u9886\u57df\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u67b6\u6784\u901a\u5e38\u91c7\u7528\u547d\u4ee4\u5f0f\u3001\u4e34\u65f6\u6027\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8106\u5f31\uff0c\u5b58\u5728\u72b6\u6001\u7ba1\u7406\u56f0\u96be\u3001\u9519\u8bef\u5904\u7406\u590d\u6742\u548c\u5e76\u53d1\u63a7\u5236\u7b49\u95ee\u9898\u3002", "method": "\u5f15\u5165Monadic Context Engineering (MCE)\u8303\u5f0f\uff0c\u5229\u7528\u51fd\u5b50\u3001\u5e94\u7528\u51fd\u5b50\u548c\u5355\u5b50\u7684\u4ee3\u6570\u7ed3\u6784\u4e3a\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u5f62\u5f0f\u5316\u57fa\u7840\u3002\u5c06\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u89c6\u4e3a\u8ba1\u7b97\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u62bd\u8c61\u4ee3\u6570\u7279\u6027\u5185\u5728\u7ba1\u7406\u8de8\u9886\u57df\u5173\u6ce8\u70b9\u3002\u4f7f\u7528\u5355\u5b50\u53d8\u6362\u5668\u7cfb\u7edf\u7ec4\u5408\u8fd9\u4e9b\u80fd\u529b\u3002", "result": "MCE\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u4ece\u7b80\u5355\u3001\u53ef\u72ec\u7acb\u9a8c\u8bc1\u7684\u7ec4\u4ef6\u6784\u5efa\u590d\u6742\u3001\u5065\u58ee\u4e14\u9ad8\u6548\u7684AI\u667a\u80fd\u4f53\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u8be5\u6846\u67b6\u63cf\u8ff0\u5143\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5143\u7f16\u7a0b\u52a8\u6001\u521b\u5efa\u548c\u7ba1\u7406\u5b50\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "conclusion": "Monadic Context Engineering\u4e3aAI\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u3001\u53ef\u7ec4\u5408\u7684\u67b6\u6784\u8303\u5f0f\uff0c\u901a\u8fc7\u4ee3\u6570\u7ed3\u6784\u7684\u5185\u5728\u7279\u6027\u89e3\u51b3\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u652f\u6301\u6784\u5efa\u590d\u6742\u4e14\u53ef\u9760\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2512.23171", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23171", "abs": "https://arxiv.org/abs/2512.23171", "authors": ["Yu Jiang", "Xindi Tong", "Ziyao Liu", "Xiaoxi Zhang", "Kwok-Yan Lam", "Chee Wei Tan"], "title": "Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning", "comment": "Published in the IEEE Transactions on Information Forensics and Security", "summary": "Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the \"right to be forgotten.\" While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.", "AI": {"tldr": "FedORA\u662f\u4e00\u79cd\u7528\u4e8e\u5782\u76f4\u8054\u90a6\u5b66\u4e60(VFL)\u4e2d\u6837\u672c\u548c\u6807\u7b7e\u9057\u5fd8\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u59cb-\u5bf9\u5076\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u6570\u636e\u79fb\u9664\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u53c2\u4e0e\u65b9\u6301\u6709\u76f8\u540c\u6837\u672c\u7684\u4e92\u8865\u7279\u5f81\uff0c\u9057\u5fd8\u4efb\u52a1\u9700\u8981\u8de8\u65b9\u534f\u8c03\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u548c\u7279\u5f81\u76f8\u4e92\u4f9d\u8d56\u7684\u590d\u6742\u6027\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6c34\u5e73\u8054\u90a6\u5b66\u4e60\uff0c\u800cVFL\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u63d0\u51faFedORA\u65b9\u6cd5\uff1a1) \u5c06\u7279\u5b9a\u6837\u672c\u6216\u6807\u7b7e\u7684\u79fb\u9664\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u539f\u59cb-\u5bf9\u5076\u6846\u67b6\u6c42\u89e3\uff1b2) \u5f15\u5165\u65b0\u7684\u9057\u5fd8\u635f\u5931\u51fd\u6570\uff0c\u4fc3\u8fdb\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u9519\u8bef\u5206\u7c7b\uff1b3) \u91c7\u7528\u81ea\u9002\u5e94\u6b65\u957f\u589e\u5f3a\u7a33\u5b9a\u6027\uff1b4) \u8bbe\u8ba1\u975e\u5bf9\u79f0\u6279\u5904\u7406\uff0c\u8003\u8651\u5269\u4f59\u6570\u636e\u5bf9\u6a21\u578b\u7684\u5148\u9a8c\u5f71\u54cd\uff0c\u5206\u522b\u5904\u7406\u9057\u5fd8\u6570\u636e\u548c\u4fdd\u7559\u6570\u636e\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eFedORA\u4e0e\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u7684\u5dee\u5f02\u662f\u6709\u754c\u7684\uff0c\u4e3a\u9057\u5fd8\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u4fdd\u8bc1\u3002\u5728\u8868\u683c\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedORA\u5728\u5b9e\u73b0\u4e0e\u4ece\u5934\u8bad\u7ec3\u76f8\u5f53\u7684\u9057\u5fd8\u6709\u6548\u6027\u548c\u6548\u7528\u4fdd\u6301\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "FedORA\u4e3a\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u548c\u6807\u7b7e\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u5e73\u8861\u4e86\u9057\u5fd8\u6548\u679c\u3001\u6a21\u578b\u6548\u7528\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6ee1\u8db3\u4e86\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u3002"}}
{"id": "2512.22470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22470", "abs": "https://arxiv.org/abs/2512.22470", "authors": ["Sadia Asif", "Israel Antonio Rosales Laguan", "Haris Khan", "Shumaila Asif", "Muneeb Asif"], "title": "DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \\textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\\%--89.7\\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86DarkPatterns-LLM\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u64cd\u7eb5\u6027\u5185\u5bb9\uff0c\u6db5\u76d6\u4e03\u4e2a\u5371\u5bb3\u7c7b\u522b\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u68c0\u6d4b\u64cd\u7eb5\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\u548c\u5f31\u70b9\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\u52a0\u5267\u4e86\u5bf9\u5176\u64cd\u7eb5\u6027\u6216\u6b3a\u9a97\u6027\u884c\u4e3a\u7684\u62c5\u5fe7\uff0c\u8fd9\u4e9b\u884c\u4e3a\u53ef\u80fd\u635f\u5bb3\u7528\u6237\u81ea\u4e3b\u6743\u3001\u4fe1\u4efb\u548c\u798f\u7949\u3002\u73b0\u6709\u7684\u5b89\u5168\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u7c97\u7565\u7684\u4e8c\u5143\u6807\u7b7e\uff0c\u65e0\u6cd5\u6355\u6349\u6784\u6210\u64cd\u7eb5\u7684\u5fae\u5999\u5fc3\u7406\u548c\u793e\u4f1a\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86DarkPatterns-LLM\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bca\u65ad\u6846\u67b6\uff0c\u5305\u542b401\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u793a\u4f8b\uff0c\u91c7\u7528\u56db\u5c42\u5206\u6790\u7ba1\u9053\uff1a\u591a\u7c92\u5ea6\u68c0\u6d4b(MGD)\u3001\u591a\u5c3a\u5ea6\u610f\u56fe\u5206\u6790(MSIAN)\u3001\u5a01\u80c1\u534f\u8c03\u534f\u8bae(THP)\u548c\u6df1\u5ea6\u4e0a\u4e0b\u6587\u98ce\u9669\u5bf9\u9f50(DCRA)\u3002\u8bc4\u4f30\u4e86GPT-4\u3001Claude 3.5\u548cLLaMA-3-70B\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6700\u5148\u8fdb\u6a21\u578b\u5728\u68c0\u6d4b\u64cd\u7eb5\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0865.2%--89.7%\uff09\uff0c\u5728\u68c0\u6d4b\u81ea\u4e3b\u6743\u7834\u574f\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u4e00\u81f4\u7684\u5f31\u70b9\u3002\u5efa\u7acb\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u3001\u591a\u7ef4\u5ea6\u7684LLM\u64cd\u7eb5\u68c0\u6d4b\u57fa\u51c6\u3002", "conclusion": "DarkPatterns-LLM\u4e3aLLM\u4e2d\u7684\u64cd\u7eb5\u68c0\u6d4b\u5efa\u7acb\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u3001\u591a\u7ef4\u5ea6\u7684\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u503c\u5f97\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2512.22173", "categories": ["cs.DC", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.22173", "abs": "https://arxiv.org/abs/2512.22173", "authors": ["Aliaksandr V. Yakutovich", "Jusong Yu", "Daniel Hollas", "Edan Bainglass", "Corsin Battaglia", "Miki Bonacci", "Lucas Fernandez Vilanova", "Stephan Henne", "Anders Kaestner", "Michel Kenzelmann", "Graham Kimbell", "Jakob Lass", "Fabio Lopes", "Daniel G. Mazzone", "Andres Ortega-Guerrero", "Xing Wang", "Nicola Marzari", "Carlo A. Pignedoli", "Giovanni Pizzi"], "title": "AiiDAlab: on the route to accelerate science", "comment": null, "summary": "With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).", "AI": {"tldr": "AiiDAlab\u5e73\u53f0\u4ece\u6750\u6599\u79d1\u5b66\u6269\u5c55\u5230\u591a\u5b66\u79d1\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u754c\u9762\u7b80\u5316\u590d\u6742\u8ba1\u7b97\u5de5\u4f5c\u6d41\uff0c\u81ea\u52a8\u8ffd\u8e2a\u6a21\u62df\u6eaf\u6e90\u786e\u4fdd\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u4e0e\u7535\u5b50\u5b9e\u9a8c\u7b14\u8bb0\u672c\u96c6\u6210\u652f\u6301FAIR\u539f\u5219\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u80fd\u529b\u589e\u957f\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7814\u7a76\u5de5\u4f5c\u6d41\u6765\u7ba1\u7406\u5927\u91cf\u76f8\u4e92\u4f9d\u8d56\u7684\u6a21\u62df\uff0c\u4f46\u6267\u884c\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u901a\u5e38\u9700\u8981\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\u6765\u8bbe\u7f6e\u8f93\u5165\u3001\u89e3\u91ca\u8f93\u51fa\u548c\u5904\u7406\u8fdc\u7a0b\u673a\u5668\u4e0a\u7684\u5e76\u884c\u4ee3\u7801\u6267\u884c\u3002", "method": "\u5f00\u53d1AiiDAlab\u5e73\u53f0\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684Web\u6d4f\u89c8\u5668\u7528\u6237\u754c\u9762\uff0c\u4f7f\u590d\u6742\u8ba1\u7b97\u5de5\u4f5c\u6d41\u6613\u4e8e\u8bbf\u95ee\uff0c\u57fa\u4e8eAiiDA\u5f15\u64ce\u81ea\u52a8\u8ffd\u8e2a\u5b8c\u6574\u6a21\u62df\u6eaf\u6e90\uff0c\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3002", "result": "AiiDAlab\u5df2\u4ece\u8ba1\u7b97\u6750\u6599\u79d1\u5b66\u6269\u5c55\u5230\u91cf\u5b50\u5316\u5b66\u3001\u5927\u6c14\u5efa\u6a21\u3001\u7535\u6c60\u7814\u7a76\u548c\u5927\u89c4\u6a21\u8bbe\u65bd\u5b9e\u9a8c\u6570\u636e\u5206\u6790\u7b49\u591a\u4e2a\u5b66\u79d1\uff0c\u5e76\u7528\u4e8e\u6559\u80b2\u73af\u5883\u3002\u901a\u8fc7\u7528\u6237\u53cd\u9988\u6539\u8fdb\uff0c\u7b80\u5316\u7528\u6237\u5165\u95e8\u3001\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u8bbf\u95ee\uff0c\u5e76\u63d0\u4f9b\u5904\u7406\u5927\u6570\u636e\u96c6\u7684\u673a\u5236\u3002", "conclusion": "AiiDAlab\u4f7f\u79d1\u5b66\u5bb6\u80fd\u591f\u4e13\u6ce8\u4e8e\u7814\u7a76\u800c\u975e\u8ba1\u7b97\u7ec6\u8282\uff0c\u901a\u8fc7\u4e0e\u7535\u5b50\u5b9e\u9a8c\u7b14\u8bb0\u672c\u96c6\u6210\u5f3a\u5316FAIR\u539f\u5219\u9075\u5b88\uff0c\u652f\u6301\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u79d1\u5b66\u5b66\u79d1\u8f7b\u677e\u751f\u6210\u53ef\u91cd\u590d\u7684\u5f00\u653e\u7814\u7a76\u6570\u636e\u3002"}}
{"id": "2512.23173", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23173", "abs": "https://arxiv.org/abs/2512.23173", "authors": ["Zhen Liang", "Hai Huang", "Zhengkui Chen"], "title": "EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion", "comment": "This is a preprint. A revised version will appear in the Proceedings of AAAI 2026", "summary": "Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.", "AI": {"tldr": "EquaCode\u662f\u4e00\u79cd\u901a\u8fc7\u65b9\u7a0b\u6c42\u89e3\u548c\u4ee3\u7801\u5b8c\u6210\u7684\u65b0\u578b\u591a\u7b56\u7565\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u5c06\u6076\u610f\u610f\u56fe\u8f6c\u5316\u4e3a\u6570\u5b66\u95ee\u9898\uff0c\u7136\u540e\u8981\u6c42LLM\u7528\u4ee3\u7801\u89e3\u51b3\uff0c\u5229\u7528\u8de8\u9886\u57df\u4efb\u52a1\u7684\u590d\u6742\u6027\u5206\u6563\u6a21\u578b\u5bf9\u5b89\u5168\u7ea6\u675f\u7684\u5173\u6ce8\u3002", "motivation": "\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u4e3b\u8981\u5728\u81ea\u7136\u8bed\u8a00\u5c42\u9762\u64cd\u4f5c\u4e14\u4f9d\u8d56\u5355\u4e00\u653b\u51fb\u7b56\u7565\uff0c\u9650\u5236\u4e86\u5168\u9762\u8bc4\u4f30LLM\u9c81\u68d2\u6027\u7684\u6709\u6548\u6027\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u7b56\u7565\u65b9\u6cd5\u6765\u6d4b\u8bd5LLM\u7684\u4fe1\u4efb\u5ea6\u3002", "method": "\u63d0\u51faEquaCode\u65b9\u6cd5\uff1a1) \u5c06\u6076\u610f\u610f\u56fe\u8f6c\u5316\u4e3a\u6570\u5b66\u95ee\u9898\uff1b2) \u8981\u6c42LLM\u4f7f\u7528\u4ee3\u7801\u89e3\u51b3\u8be5\u95ee\u9898\uff1b3) \u5229\u7528\u8de8\u9886\u57df\u4efb\u52a1\uff08\u6570\u5b66+\u7f16\u7a0b\uff09\u7684\u590d\u6742\u6027\u5206\u6563\u6a21\u578b\u5bf9\u5b89\u5168\u7ea6\u675f\u7684\u5173\u6ce8\u3002", "result": "\u5728GPT\u7cfb\u5217\u4e0a\u5e73\u5747\u6210\u529f\u7387\u4e3a91.19%\uff0c\u57283\u4e2a\u6700\u5148\u8fdb\u7684LLM\u4e0a\u8fbe\u523098.65%\uff0c\u4e14\u4ec5\u9700\u5355\u6b21\u67e5\u8be2\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793aEquaCode\u4f18\u4e8e\u5355\u72ec\u7684\u6570\u5b66\u65b9\u7a0b\u6a21\u5757\u6216\u4ee3\u7801\u6a21\u5757\uff0c\u8868\u660e\u591a\u7b56\u7565\u65b9\u6cd5\u5177\u6709\u534f\u540c\u6548\u5e94\u3002", "conclusion": "EquaCode\u901a\u8fc7\u65b9\u7a0b\u6c42\u89e3\u548c\u4ee3\u7801\u5b8c\u6210\u7684\u591a\u7b56\u7565\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u4e86LLM\u7684\u5b89\u5168\u7ea6\u675f\uff0c\u8bc1\u660e\u4e86\u8de8\u9886\u57df\u4efb\u52a1\u7684\u590d\u6742\u6027\u53ef\u4ee5\u5206\u6563\u6a21\u578b\u5bf9\u5b89\u5168\u9650\u5236\u7684\u5173\u6ce8\uff0c\u4e3a\u8bc4\u4f30LLM\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2512.22174", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22174", "abs": "https://arxiv.org/abs/2512.22174", "authors": ["Muhammad Zeeshan Karamat", "Sadman Saif", "Christiana Chamon Garcia"], "title": "BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs", "comment": null, "summary": "Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.", "AI": {"tldr": "BitFlipScope\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u5728Transformer\u67b6\u6784\u4e2d\u5b9a\u4f4d\u7531\u786c\u4ef6\u9000\u5316\u3001\u5b87\u5b99\u8f90\u5c04\u6216\u6545\u969c\u6ce8\u5165\u653b\u51fb\u5f15\u8d77\u7684\u6bd4\u7279\u7ffb\u8f6c\u6545\u969c\uff0c\u652f\u6301\u6709\u53c2\u8003\u6a21\u578b\u548c\u65e0\u53c2\u8003\u6a21\u578b\u4e24\u79cd\u90e8\u7f72\u573a\u666f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5bb9\u6613\u53d7\u5230\u6bd4\u7279\u7ffb\u8f6c\u6545\u969c\u7684\u5f71\u54cd\uff0c\u8fd9\u4e9b\u6545\u969c\u4f1a\u9759\u9ed8\u5730\u7834\u574f\u5185\u90e8\u53c2\u6570\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u6216\u5371\u9669\u7684\u884c\u4e3a\u3002\u5b9a\u4f4d\u8fd9\u4e9b\u6545\u969c\u5bf9\u4e8e\u8bca\u65ad\u95ee\u9898\u3001\u5e94\u7528\u9488\u5bf9\u6027\u4fee\u590d\u63aa\u65bd\u4ee5\u53ca\u6062\u590d\u6a21\u578b\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faBitFlipScope\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u6709\u5e72\u51c0\u53c2\u8003\u6a21\u578b\u65f6\uff0c\u901a\u8fc7\u8f93\u51fa\u3001\u9690\u85cf\u72b6\u6001\u548c\u5185\u90e8\u6fc0\u6d3b\u7684\u5dee\u5f02\u5206\u6790\u6765\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a\uff1b2\uff09\u65e0\u53c2\u8003\u6a21\u578b\u65f6\uff0c\u4f7f\u7528\u6b8b\u5dee\u8def\u5f84\u6270\u52a8\u548c\u635f\u5931\u654f\u611f\u6027\u5206\u6790\u76f4\u63a5\u4ece\u635f\u574f\u6a21\u578b\u4e2d\u63a8\u65ad\u6545\u969c\u5f71\u54cd\u533a\u57df\u3002", "result": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u6709\u6548\u8bca\u65ad\u6545\u969c\uff0c\u8fd8\u652f\u6301\u65e0\u9700\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u6027\u80fd\u6062\u590d\uff0c\u4e3a\u5728\u786c\u4ef6\u6613\u9519\u548c\u5bf9\u6297\u73af\u5883\u4e2d\u6062\u590d\u635f\u574f\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "conclusion": "BitFlipScope\u662f\u5b9e\u73b0\u53ef\u4fe1\u8d56\u3001\u5177\u6709\u6545\u969c\u6062\u590d\u80fd\u529b\u7684LLM\u90e8\u7f72\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u7279\u522b\u9002\u7528\u4e8e\u786c\u4ef6\u6613\u9519\u548c\u5bf9\u6297\u6027\u73af\u5883\u3002"}}
{"id": "2512.23216", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23216", "abs": "https://arxiv.org/abs/2512.23216", "authors": ["Partha Paul", "Keshav Sinha"], "title": "Multiparty Authorization for Secure Data Storage in Cloud Environments using Improved Attribute-Based Encryption", "comment": null, "summary": "In todays scenario, various organizations store their sensitive data in the cloud environment. Multiple problems are present while retrieving and storing vast amounts of data, such as the frequency of data requests (increasing the computational overhead of the server) and data leakage while storing. To cope with said problem, Attribute-Based Encryption (ABE) is one of the potential security and access control techniques for secure data storage and authorization. The proposed work divides into two objectives: (i) provide access to authorized users and (ii) secure data storage in a cloud environment. The improved ABE using Functional Based Stream Cipher (FBSE) is proposed for data storage. The proposed technique uses simple scalar points over a parabolic curve to provide multiparty authorization. The authorization points are generated and share only with the authorized recipients. The Shamir secret sharing technique generate the authorization points and 2D-Lagrange Interpolation is used to reconstruct the secret points from regular parabola. The proposed scheme has specified the threshold (Ts>3) legally authorized users to reconstruct the attribute-associated keys for decryption. The encryption of data is evaluated using Statistical analysis (NIST Statistical Test Suite, Correlation Coefficient, and Histogram) test to investigate image pixel deviation. The parameters like encryption and decryption are used for performance analysis, where an increase in the number of attributes for the authorization policy will increase the encryption time. The proposed scheme imposes minimal storage overhead, irrespective of the users identity. The security analysis evidence that it resists collision attacks. The security and performance analysis results demonstrate that the proposed scheme is more robust and secure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u52a0\u5bc6\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u7ed3\u5408\u529f\u80fd\u6d41\u5bc6\u7801\u548c\u629b\u7269\u7ebf\u66f2\u7ebf\u4e0a\u7684\u6807\u91cf\u70b9\uff0c\u7528\u4e8e\u4e91\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6570\u636e\u5b58\u50a8\u548c\u8bbf\u95ee\u63a7\u5236\uff0c\u652f\u6301\u591a\u65b9\u6388\u6743\u5e76\u62b5\u6297\u78b0\u649e\u653b\u51fb\u3002", "motivation": "\u4e91\u73af\u5883\u4e2d\u5b58\u50a8\u654f\u611f\u6570\u636e\u9762\u4e34\u6570\u636e\u8bf7\u6c42\u9891\u7387\u9ad8\uff08\u589e\u52a0\u670d\u52a1\u5668\u8ba1\u7b97\u5f00\u9500\uff09\u548c\u6570\u636e\u5b58\u50a8\u6cc4\u6f0f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u7684\u5b89\u5168\u5b58\u50a8\u548c\u8bbf\u95ee\u63a7\u5236\u6280\u672f\u3002", "method": "\u6539\u8fdb\u7684\u5c5e\u6027\u52a0\u5bc6\u65b9\u6848\uff0c\u7ed3\u5408\u529f\u80fd\u6d41\u5bc6\u7801\uff0c\u4f7f\u7528\u629b\u7269\u7ebf\u66f2\u7ebf\u4e0a\u7684\u7b80\u5355\u6807\u91cf\u70b9\u5b9e\u73b0\u591a\u65b9\u6388\u6743\u3002\u91c7\u7528Shamir\u79d8\u5bc6\u5171\u4eab\u751f\u6210\u6388\u6743\u70b9\uff0c2D-Lagrange\u63d2\u503c\u4ece\u6b63\u5219\u629b\u7269\u7ebf\u91cd\u6784\u79d8\u5bc6\u70b9\uff0c\u8bbe\u5b9a\u9608\u503c(Ts>3)\u8981\u6c42\u5408\u6cd5\u6388\u6743\u7528\u6237\u91cd\u6784\u5c5e\u6027\u5173\u8054\u5bc6\u94a5\u8fdb\u884c\u89e3\u5bc6\u3002", "result": "\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u6d4b\u8bd5\u9a8c\u8bc1\u52a0\u5bc6\u6548\u679c\uff0c\u6027\u80fd\u5206\u6790\u663e\u793a\u52a0\u5bc6\u65f6\u95f4\u968f\u6388\u6743\u7b56\u7565\u5c5e\u6027\u6570\u91cf\u589e\u52a0\u800c\u589e\u52a0\uff0c\u4f46\u5b58\u50a8\u5f00\u9500\u6700\u5c0f\u3002\u5b89\u5168\u5206\u6790\u8bc1\u660e\u65b9\u6848\u80fd\u62b5\u6297\u78b0\u649e\u653b\u51fb\uff0c\u6bd4\u73b0\u6709\u65b9\u6848\u66f4\u9c81\u68d2\u5b89\u5168\u3002", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdb\u5c5e\u6027\u52a0\u5bc6\u65b9\u6848\u5728\u4e91\u73af\u5883\u4e2d\u80fd\u6709\u6548\u5b9e\u73b0\u5b89\u5168\u6570\u636e\u5b58\u50a8\u548c\u8bbf\u95ee\u63a7\u5236\uff0c\u5177\u6709\u6700\u5c0f\u5b58\u50a8\u5f00\u9500\u548c\u826f\u597d\u7684\u5b89\u5168\u6027\u80fd\uff0c\u9002\u5408\u5904\u7406\u654f\u611f\u6570\u636e\u5b58\u50a8\u9700\u6c42\u3002"}}
{"id": "2512.22568", "categories": ["cs.AI", "physics.bio-ph", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.22568", "abs": "https://arxiv.org/abs/2512.22568", "authors": ["Rajesh P. N. Rao", "Vishwas Sathish", "Linxing Preston Jiang", "Matthew Bryan", "Prashant Rangarajan"], "title": "Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI", "comment": null, "summary": "The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u9884\u6d4b\u7f16\u7801\u6a21\u578b\u7684\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff08\u52a8\u4f5c\u6574\u5408\u3001\u5c42\u6b21\u7ec4\u5408\u7ed3\u6784\u548c\u60c5\u666f\u8bb0\u5fc6\uff09\u6574\u5408\u5230\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u4ee5\u89e3\u51b3\u5f53\u524dAI\u7684\u7f3a\u9677\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u3001\u8282\u80fd\u4e14\u7c7b\u4eba\u7684AI\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u867d\u7136\u57fa\u4e8e\u9884\u6d4b\u7f16\u7801\uff08\u6700\u5c0f\u5316\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\u635f\u5931\uff09\uff0c\u4f46\u5ffd\u7565\u4e86\u795e\u7ecf\u79d1\u5b66\u4e2d\u9884\u6d4b\u7f16\u7801\u6a21\u578b\u7684\u4e09\u4e2a\u91cd\u8981\u7ec4\u4ef6\uff1a\u52a8\u4f5c\u4e0e\u751f\u6210\u6a21\u578b\u7684\u7d27\u5bc6\u6574\u5408\u3001\u5c42\u6b21\u7ec4\u5408\u7ed3\u6784\u3001\u60c5\u666f\u8bb0\u5fc6\u3002\u8fd9\u4e9b\u7f3a\u5931\u5bfc\u81f4AI\u5b58\u5728\u5e7b\u89c9\u3001\u6982\u5ff5\u7406\u89e3\u80a4\u6d45\u3001\u7f3a\u4e4f\u80fd\u52a8\u6027/\u8d23\u4efb\u611f\u3001\u5b89\u5168\u53ef\u4fe1\u5ea6\u4e0d\u8db3\u3001\u80fd\u6548\u4f4e\u4e0b\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5c06\u795e\u7ecf\u79d1\u5b66\u548c\u8ba4\u77e5\u79d1\u5b66\u7684\u9884\u6d4b\u7f16\u7801\u6a21\u578b\u7ec4\u4ef6\u6574\u5408\u5230\u57fa\u7840\u6a21\u578b\u4e2d\uff1a1\uff09\u5728\u591a\u4e2a\u62bd\u8c61\u5c42\u6b21\u4e0a\u6574\u5408\u52a8\u4f5c\u4e0e\u751f\u6210\u6a21\u578b\uff1b2\uff09\u91c7\u7528\u7ec4\u5408\u751f\u6210\u67b6\u6784\uff1b3\uff09\u52a0\u5165\u60c5\u666f\u8bb0\u5fc6\u7cfb\u7edf\u3002\u5bf9\u6bd4\u5f53\u524d\u8d8b\u52bf\uff08\u5982\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\uff0c\u8ba8\u8bba\u5982\u4f55\u7528\u8111\u542f\u53d1\u7ec4\u4ef6\u589e\u5f3a\u8fd9\u4e9b\u6a21\u578b\u3002", "result": "\u8bba\u6587\u672a\u62a5\u544a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u548c\u65b9\u5411\u3002\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b\u8111\u542f\u53d1\u7ec4\u4ef6\uff0c\u6709\u671b\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u7684\u5f53\u524d\u7f3a\u9677\uff1a\u51cf\u5c11\u5e7b\u89c9\u3001\u589e\u5f3a\u6982\u5ff5\u7406\u89e3\u3001\u5efa\u7acb\u80fd\u52a8\u6027\u3001\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u3001\u63d0\u5347\u80fd\u6548\u3002", "conclusion": "\u91cd\u65b0\u6fc0\u6d3b\u8111\u79d1\u5b66\u4e0eAI\u4e4b\u95f4\u5386\u53f2\u4e0a\u5bcc\u6709\u6210\u679c\u7684\u601d\u60f3\u4ea4\u6d41\uff0c\u5c06\u6709\u52a9\u4e8e\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u3001\u4ee5\u4eba\u4e3a\u672c\u7684AI\u3002\u6574\u5408\u9884\u6d4b\u7f16\u7801\u6a21\u578b\u7684\u5b8c\u6574\u7ec4\u4ef6\u662f\u8fc8\u5411\u7c7b\u4ebaAI\u7684\u5173\u952e\u8def\u5f84\u3002"}}
{"id": "2512.22180", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22180", "abs": "https://arxiv.org/abs/2512.22180", "authors": ["Alexander K. Chen"], "title": "iOS as Acceleration", "comment": "7 pages main text, 7 pages appendix. Presented at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u5229\u7528iOS\u624b\u673a\u4f5c\u4e3a\u5206\u5e03\u5f0f\u8ba1\u7b97\u8d44\u6e90\u6765\u589e\u5f3a\u672c\u5730\u673a\u5668\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u6d41\u6c34\u7ebf\u5e76\u884c\u6280\u672f\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u52a0\u901f\uff0c\u4e3a\u96f6\u6210\u672c\u63d0\u5347\u5f31\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u9700\u8981\u5f3a\u5927\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u4e91\u8ba1\u7b97\u7684\u6210\u672c\u3001\u9690\u79c1\u9650\u5236\u6216\u7269\u7406\u73af\u5883\u4e0d\u53ef\u7528\u7b49\u95ee\u9898\u4f7f\u5f97\u672c\u5730\u8ba1\u7b97\u6210\u4e3a\u5fc5\u8981\u3002\u79fb\u52a8\u8bbe\u5907\u4f5c\u4e3a\u666e\u904d\u5b58\u5728\u4f46\u672a\u5145\u5206\u5229\u7528\u7684\u8d44\u6e90\uff0c\u5177\u6709\u6539\u5584\u5f31\u8ba1\u7b97\u73af\u5883\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u5229\u7528iOS\u8bbe\u5907\u901a\u8fc7\u5206\u5e03\u5f0f\u6d41\u6c34\u7ebf\u5e76\u884c\u6280\u672f\uff0c\u514b\u670d\u5185\u5b58\u9650\u5236\u3001\u70ed\u8282\u6d41\u548c\u64cd\u4f5c\u7cfb\u7edf\u6c99\u76d2\u7b49\u9650\u5236\uff0c\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u3001\u6279\u91cf\u63a8\u7406\u548c\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\u7684\u52a0\u901f\u3002", "result": "\u5c55\u793a\u4e86\u5728\u5f31\u8ba1\u7b97\u73af\u5883\u4e2d\u901a\u8fc7iOS\u8bbe\u5907\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u80fd\u591f\u52a0\u901f\u9002\u5ea6\u89c4\u6a21\u7684\u6a21\u578b\u8bad\u7ec3\u3001\u6279\u91cf\u63a8\u7406\u548c\u667a\u80fd\u4f53LRM\u5de5\u5177\u4f7f\u7528\u3002", "conclusion": "\u79fb\u52a8\u8bbe\u5907\u6709\u6f5c\u529b\u4e3a\u673a\u5668\u5b66\u4e60\u505a\u51fa\u66f4\u5927\u8d21\u732e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u96f6\u6210\u672c\u6539\u5584\u672c\u5730\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u9645\u7528\u4f8b\u3001\u9650\u5236\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.23307", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.23307", "abs": "https://arxiv.org/abs/2512.23307", "authors": ["Jiawei Liu", "Zhuo Chen", "Rui Zhu", "Miaokun Chen", "Yuyang Gong", "Wei Lu", "Xiaofeng Wang"], "title": "RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking", "comment": null, "summary": "Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.", "AI": {"tldr": "RobustMask\u662f\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u9884\u6d4b\u80fd\u529b\u548c\u968f\u673a\u63a9\u7801\u5e73\u6ed1\u673a\u5236\u7684\u65b0\u578b\u9632\u5fa1\u65b9\u6cd5\uff0c\u53ef\u589e\u5f3a\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u5bf9\u6297\u5b57\u7b26\u3001\u5355\u8bcd\u548c\u77ed\u8bed\u7ea7\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8981\u4e48\u5047\u8bbe\u8fc7\u5f3a\u7684\u5bf9\u6297\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faRobustMask\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u80fd\u529b\u548c\u968f\u673a\u63a9\u7801\u5e73\u6ed1\u673a\u5236\uff0c\u5229\u7528\u6392\u5e8f\u6a21\u578b\u7684\u6210\u5bf9\u6bd4\u8f83\u80fd\u529b\u548c\u6982\u7387\u7edf\u8ba1\u5206\u6790\uff0c\u63d0\u4f9b\u7406\u8bba\u4e0a\u7684top-K\u9c81\u68d2\u6027\u8bc1\u660e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRobustMask\u80fd\u6210\u529f\u8ba4\u8bc1\u8d85\u8fc720%\u7684\u5019\u9009\u6587\u6863\u5728top-10\u6392\u540d\u4f4d\u7f6e\u4e2d\u5bf9\u6297\u5f71\u54cd\u9ad8\u8fbe30%\u5185\u5bb9\u7684\u5bf9\u6297\u6270\u52a8\u3002", "conclusion": "RobustMark\u5728\u589e\u5f3a\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u6709\u6548\uff0c\u4e3a\u73b0\u5b9e\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u66f4\u5f3a\u5b89\u5168\u4fdd\u969c\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.22195", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22195", "abs": "https://arxiv.org/abs/2512.22195", "authors": ["Kun-Woo Shin", "Jay H. Park", "Moonwook Oh", "Yohan Jo", "Jaeyoung Do", "Sang-Won Lee"], "title": "MatKV: Trading Compute for Flash Storage in LLM Inference", "comment": "Accepted for publication in ICDE 2026", "summary": "We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.", "AI": {"tldr": "MatKV\u901a\u8fc7\u9884\u8ba1\u7b97RAG\u6587\u6863\u7684\u952e\u503c\u5411\u91cf\u5e76\u5b58\u50a8\u5728\u95ea\u5b58\u4e2d\uff0c\u5728\u63a8\u7406\u65f6\u76f4\u63a5\u590d\u7528\uff0c\u5c06\u63a8\u7406\u65f6\u95f4\u548c\u529f\u8017\u51cf\u534a\uff0c\u540c\u65f6\u652f\u6301GPU\u5e76\u884c\u52a0\u8f7d\u548c\u4f4e\u7aefGPU\u89e3\u7801\u4f18\u5316\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u6210\u672c\u5df2\u8d85\u8fc7\u8bad\u7ec3\u6210\u672c\uff0cRAG\u5728\u5904\u7406\u957f\u8f93\u5165\u65f6\u9884\u586b\u5145\u9636\u6bb5\u8ba1\u7b97\u952e\u503c\u5411\u91cf\u80fd\u8017\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u9700\u8981\u63d0\u9ad8RAG\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faMatKV\u65b9\u6848\uff1a\u9884\u8ba1\u7b97RAG\u5bf9\u8c61\u7684\u952e\u503c\u5411\u91cf\uff0c\u5c06\u5176\u7269\u5316\u5b58\u50a8\u5728\u5ec9\u4ef7\u4f46\u5feb\u901f\u3001\u80fd\u6548\u9ad8\u7684\u95ea\u5b58\u4e2d\uff0c\u5728\u63a8\u7406\u65f6\u76f4\u63a5\u590d\u7528\u8fd9\u4e9b\u9884\u8ba1\u7b97\u7684KVs\uff0c\u907f\u514d\u4f7f\u7528GPU\u91cd\u65b0\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4GPU\u5b8c\u5168\u8ba1\u7b97KVs\uff0cMatKV\u5c06RAG\u5de5\u4f5c\u8d1f\u8f7d\u7684\u63a8\u7406\u65f6\u95f4\u548c\u529f\u8017\u51cf\u5c11\u4e00\u534a\uff0c\u4e14\u5bf9\u95ee\u7b54\u4efb\u52a1\u51c6\u786e\u6027\u5f71\u54cd\u4e0d\u5927\u3002\u6b64\u5916\u652f\u6301GPU\u5e76\u884c\u52a0\u8f7d\u548c\u4f4e\u7aefGPU\u89e3\u7801\u4f18\u5316\u3002", "conclusion": "MatKV\u80fd\u591f\u4f7f\u5927\u89c4\u6a21\u751f\u6210\u5f0fAI\u5e94\u7528\u66f4\u5177\u6210\u672c\u6548\u76ca\u3001\u80fd\u6548\u66f4\u9ad8\uff0c\u5e76\u5728\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u548c\u786c\u4ef6\u73af\u5883\u4e2d\u66f4\u6613\u8bbf\u95ee\uff0c\u4e3aRAG\u63a8\u7406\u6548\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23438", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23438", "abs": "https://arxiv.org/abs/2512.23438", "authors": ["Johannes Lenzen", "Mohamadreza Rostami", "Lichao Wu", "Ahmad-Reza Sadeghi"], "title": "Fuzzilicon: A Post-Silicon Microcode-Guided x86 CPU Fuzzer", "comment": null, "summary": "Modern CPUs are black boxes, proprietary, and increasingly characterized by sophisticated microarchitectural flaws that evade traditional analysis. While some of these critical vulnerabilities have been uncovered through cumbersome manual effort, building an automated and systematic vulnerability detection framework for real-world post-silicon processors remains a challenge.\n  In this paper, we present Fuzzilicon, the first post-silicon fuzzing framework for real-world x86 CPUs that brings deep introspection into the microcode and microarchitectural layers. Fuzzilicon automates the discovery of vulnerabilities that were previously only detectable through extensive manual reverse engineering, and bridges the visibility gap by introducing microcode-level instrumentation. At the core of Fuzzilicon is a novel technique for extracting feedback directly from the processor's microarchitecture, enabled by reverse-engineering Intel's proprietary microcode update interface. We develop a minimally intrusive instrumentation method and integrate it with a hypervisor-based fuzzing harness to enable precise, feedback-guided input generation, without access to Register Transfer Level (RTL).\n  Applied to Intel's Goldmont microarchitecture, Fuzzilicon introduces 5 significant findings, including two previously unknown microcode-level speculative-execution vulnerabilities. Besides, the Fuzzilicon framework automatically rediscover the $\u03bc$Spectre class of vulnerabilities, which were detected manually in the previous work. Fuzzilicon reduces coverage collection overhead by up to 31$\\times$ compared to baseline techniques and achieves 16.27% unique microcode coverage of hookable locations, the first empirical baseline of its kind. As a practical, coverage-guided, and scalable approach to post-silicon fuzzing, Fuzzilicon establishes a new foundation to automate the discovery of complex CPU vulnerabilities.", "AI": {"tldr": "Fuzzilicon\uff1a\u9996\u4e2a\u9488\u5bf9\u771f\u5b9ex86 CPU\u7684\u7845\u540e\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0bIntel\u5fae\u7801\u66f4\u65b0\u63a5\u53e3\u5b9e\u73b0\u5fae\u67b6\u6784\u5c42\u6df1\u5ea6\u68c0\u6d4b\uff0c\u81ea\u52a8\u53d1\u73b0\u5fae\u7801\u7ea7\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u4ee3CPU\u4f5c\u4e3a\u9ed1\u76d2\u7cfb\u7edf\uff0c\u5b58\u5728\u590d\u6742\u7684\u5fae\u67b6\u6784\u6f0f\u6d1e\uff0c\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u3002\u73b0\u6709\u6f0f\u6d1e\u53d1\u73b0\u4f9d\u8d56\u7e41\u7410\u7684\u624b\u52a8\u9006\u5411\u5de5\u7a0b\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u3001\u7cfb\u7edf\u5316\u7684\u7845\u540e\u5904\u7406\u5668\u6f0f\u6d1e\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u9006\u5411\u5de5\u7a0bIntel\u4e13\u6709\u5fae\u7801\u66f4\u65b0\u63a5\u53e3\uff0c\u5f00\u53d1\u5fae\u7801\u7ea7\u68c0\u6d4b\u65b9\u6cd5\uff1b\u7ed3\u5408\u57fa\u4e8ehypervisor\u7684\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u53cd\u9988\u5f15\u5bfc\u8f93\u5165\u751f\u6210\uff1b\u76f4\u63a5\u4ece\u5904\u7406\u5668\u5fae\u67b6\u6784\u63d0\u53d6\u53cd\u9988\uff0c\u65e0\u9700RTL\u8bbf\u95ee\u3002", "result": "\u5728Intel Goldmont\u5fae\u67b6\u6784\u4e0a\u53d1\u73b05\u4e2a\u91cd\u8981\u6f0f\u6d1e\uff0c\u5305\u62ec2\u4e2a\u5148\u524d\u672a\u77e5\u7684\u5fae\u7801\u7ea7\u63a8\u6d4b\u6267\u884c\u6f0f\u6d1e\uff1b\u81ea\u52a8\u91cd\u65b0\u53d1\u73b0\u03bcSpectre\u7c7b\u6f0f\u6d1e\uff1b\u76f8\u6bd4\u57fa\u7ebf\u6280\u672f\u51cf\u5c1131\u500d\u8986\u76d6\u6536\u96c6\u5f00\u9500\uff1b\u5b9e\u73b016.27%\u53ef\u6302\u94a9\u4f4d\u7f6e\u7684\u552f\u4e00\u5fae\u7801\u8986\u76d6\u7387\u3002", "conclusion": "Fuzzilicon\u5efa\u7acb\u4e86\u7845\u540e\u6a21\u7cca\u6d4b\u8bd5\u7684\u65b0\u57fa\u7840\uff0c\u4e3a\u81ea\u52a8\u5316\u53d1\u73b0\u590d\u6742CPU\u6f0f\u6d1e\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u8986\u76d6\u5f15\u5bfc\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u5fae\u7801\u548c\u5fae\u67b6\u6784\u5c42\u7684\u68c0\u6d4b\u7a7a\u767d\u3002"}}
{"id": "2512.22601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22601", "abs": "https://arxiv.org/abs/2512.22601", "authors": ["Tao Zhou", "Lingyu Shu", "Zixing Zhang", "Jing Han"], "title": "Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care", "comment": null, "summary": "Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.", "AI": {"tldr": "Tyee\u662f\u4e00\u4e2a\u7528\u4e8e\u667a\u80fd\u751f\u7406\u533b\u7597\u7684\u7edf\u4e00\u3001\u6a21\u5757\u5316\u3001\u53ef\u914d\u7f6e\u5de5\u5177\u5305\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u751f\u7406\u4fe1\u53f7\u5206\u6790\u4e2d\u7684\u6570\u636e\u683c\u5f0f\u5f02\u6784\u3001\u9884\u5904\u7406\u4e0d\u4e00\u81f4\u3001\u6a21\u578b\u788e\u7247\u5316\u548c\u5b9e\u9a8c\u4e0d\u53ef\u590d\u73b0\u7b49\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u751f\u7406\u4fe1\u53f7\u5206\u6790\u4e2d\u9762\u4e34\u56db\u5927\u6311\u6218\uff1a\u6570\u636e\u683c\u5f0f\u5f02\u6784\u3001\u9884\u5904\u7406\u7b56\u7565\u4e0d\u4e00\u81f4\u3001\u6a21\u578b\u6d41\u7a0b\u788e\u7247\u5316\u3001\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u53ef\u590d\u73b0\uff0c\u8fd9\u4e9b\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "Tyee\u5de5\u5177\u5305\u5305\u542b\u4e09\u5927\u521b\u65b0\uff1a1) 12\u79cd\u4fe1\u53f7\u6a21\u6001\u7684\u7edf\u4e00\u6570\u636e\u63a5\u53e3\u548c\u53ef\u914d\u7f6e\u9884\u5904\u7406\u6d41\u6c34\u7ebf\uff1b2) \u6a21\u5757\u5316\u53ef\u6269\u5c55\u67b6\u6784\uff0c\u652f\u6301\u7075\u6d3b\u96c6\u6210\u548c\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff1b3) \u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u914d\u7f6e\uff0c\u4fc3\u8fdb\u53ef\u590d\u73b0\u548c\u53ef\u6269\u5c55\u7684\u5b9e\u9a8c\u3002", "result": "Tyee\u5728\u6240\u6709\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7a33\u5b9a\u6709\u6548\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u572813\u4e2a\u6570\u636e\u96c6\u4e2d12\u4e2a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5176\u4f59\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u6301\u5e73\u6216\u8d85\u8d8a\u3002", "conclusion": "Tyee\u4e3a\u667a\u80fd\u751f\u7406\u533b\u7597\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u6a21\u5757\u5316\u3001\u53ef\u914d\u7f6e\u7684\u5de5\u5177\u5305\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u4fc3\u8fdb\u4e86\u53ef\u590d\u73b0\u548c\u53ef\u6269\u5c55\u7684\u7814\u7a76\uff0c\u5df2\u5728GitHub\u5f00\u6e90\u5e76\u6301\u7eed\u7ef4\u62a4\u3002"}}
{"id": "2512.22215", "categories": ["cs.DC", "cs.MS", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.22215", "abs": "https://arxiv.org/abs/2512.22215", "authors": ["Simone Bn\u00e0", "Giuseppe Giaquinto", "Ettore Fadiga", "Tommaso Zanelli", "Francesco Bottau"], "title": "SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM", "comment": "43 pages", "summary": "High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.", "AI": {"tldr": "SPUMA\u662fOPENFOAM\u7684\u5b8c\u6574GPU\u79fb\u690d\u7248\u672c\uff0c\u652f\u6301NVIDIA\u548cAMD GPU\uff0c\u901a\u8fc7\u53ef\u79fb\u690d\u7f16\u7a0b\u6a21\u578b\u548c\u5185\u5b58\u6c60\u7ba1\u7406\u5668\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ba1\u7b97\uff0c\u5728LUMI\u548cLeonardo\u96c6\u7fa4\u4e0a\u6d4b\u8bd5\u663e\u793a\u826f\u597d\u7684\u5f3a\u6269\u5c55\u6027\u548c\u5f31\u6269\u5c55\u6027\uff0c\u80fd\u8017\u964d\u4f4e\u8fbe82%\u3002", "motivation": "\u5c3d\u7ba1GPU\u5728HPC\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u5f00\u6e90CFD\u8f6f\u4ef6\uff08\u5982OPENFOAM\uff09\u4e2d\u7684\u53ef\u7f16\u7a0b\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u96c6\u7fa4\u73af\u5883\u4e2d\u6709\u6548\u5229\u7528\u73b0\u4ee3\u52a0\u901f\u5668\u3002", "method": "\u57fa\u4e8e\u53ef\u79fb\u690d\u7f16\u7a0b\u6a21\u578b\uff0c\u91c7\u7528\u5185\u5b58\u6c60\u7ba1\u7406\u5668\u5229\u7528\u73b0\u4ee3GPU\u7684\u7edf\u4e00\u5185\u5b58\u7279\u6027\uff0c\u5c06OPENFOAM\u5b8c\u6574\u79fb\u690d\u5230NVIDIA\u548cAMD GPU\u4e0a\uff0c\u5f62\u6210SPUMA\u6846\u67b6\u3002", "result": "\u5728LUMI\uff08AMD MI250X\uff09\u548cLeonardo\uff08NVIDIA A100\uff09\u96c6\u7fa4\u4e0a\u6d4b\u8bd5\u663e\u793a\uff1a\u5f3a\u6269\u5c55\u6027\u5728\u6bcfGPU\u8d1f\u8f7d800\u4e07\u7f51\u683c\u65f6\u8fbe\u523065%\u6548\u7387\uff1b\u5f31\u6269\u5c55\u6027\u572820\u4e2aGPU\u4e0a\u4e3a75-85%\uff1b\u4f7f\u7528NVIDIA AmgX\u6c42\u89e3\u5668\u65f6\u6548\u7387\u4e0d\u4f4e\u4e8e90%\uff1b\u4e00\u4e2aA100 GPU\u76f8\u5f53\u4e8e200-300\u4e2aIntel Sapphire Rapids\u6838\u5fc3\uff1b\u80fd\u8017\u76f8\u6bd4CPU\u964d\u4f4e\u8fbe82%\u3002", "conclusion": "SPUMA\u6210\u529f\u5b9e\u73b0\u4e86OPENFOAM\u7684GPU\u79fb\u690d\uff0c\u5728\u6df7\u5408\u96c6\u7fa4\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3aCFD\u5728\u9884\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22605", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22605", "abs": "https://arxiv.org/abs/2512.22605", "authors": ["Junshu Dai", "Yu Wang", "Tongya Zheng", "Wei Ji", "Qinghong Guo", "Ji Cao", "Jie Song", "Canghong Jin", "Mingli Song"], "title": "Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation", "comment": null, "summary": "The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \\textbf{M}ulti-\\textbf{M}odal \\textbf{Mob}ility (\\textbf{M}$^3$\\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.", "AI": {"tldr": "M\u00b3ob\uff1a\u5229\u7528\u591a\u6a21\u6001\u65f6\u7a7a\u77e5\u8bc6\u589e\u5f3a\u4f4d\u7f6e\u63a8\u8350\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7LLM\u589e\u5f3a\u7684\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7edf\u4e00\u65f6\u7a7a\u5173\u7cfb\u56fe\uff0c\u89e3\u51b3\u6a21\u6001\u95f4\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1a\u5355\u6a21\u6001\u65b9\u6cd5\u53d7\u6570\u636e\u7a00\u758f\u6027\u548c\u56fa\u6709\u504f\u5dee\u9650\u5236\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u9759\u6001\u591a\u6a21\u6001\u8868\u793a\u4e0e\u65f6\u7a7a\u52a8\u6001\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f", "method": "1. \u5229\u7528LLM\u589e\u5f3a\u7684\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7edf\u4e00\u65f6\u7a7a\u5173\u7cfb\u56fe\uff1b2. \u8bbe\u8ba1\u95e8\u63a7\u673a\u5236\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u65f6\u7a7a\u56fe\u8868\u793a\uff1b3. \u63d0\u51faSTKG\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5c06\u65f6\u7a7a\u52a8\u6001\u77e5\u8bc6\u6ce8\u5165\u9759\u6001\u56fe\u50cf\u6a21\u6001", "result": "\u5728\u516d\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u6b63\u5e38\u573a\u666f\u4e0b\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u5f02\u5e38\u573a\u666f\u4e0b\u4e5f\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "M\u00b3ob\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u65f6\u7a7a\u77e5\u8bc6\u6765\u8868\u5f81\u79fb\u52a8\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4f4d\u7f6e\u63a8\u8350\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.23535", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.23535", "abs": "https://arxiv.org/abs/2512.23535", "authors": ["Eduardo Salazar"], "title": "A Privacy Protocol Using Ephemeral Intermediaries and a Rank-Deficient Matrix Power Function (RDMPF)", "comment": null, "summary": "This paper presents a private transfer architecture for the Internet Computer (ICP) that decouples deposit and retrieval through two short-lived intermediaries, with sealed storage and attested teardown by an ephemeral witness. The protocol uses a non-interactive RDMPF-based encapsulation to derive per-transfer transport keys. A public notice hint is computed from the capsule to enable discovery without fingerprinting the recipient's key. Retrieval is authorized by a short proof of decapsulation that reveals no identities. All transaction intermediaries are ephemeral and issue certified destruction intents and proofs, allowing a noticeboard to publish auditable finalization records. The design provides sender identity privacy with respect to the recipient, content confidentiality against intermediaries, forward secrecy for transport keys after staged destruction, verifiable liveness and finality. We formalize the basic interfaces, provide the security arguments for encapsulation correctness, hint privacy, authorization soundness and timeout reclaim.\n  In terms of implementation, it has been recently brought into production on the ICP under the name ICPP. It has been subject to exhaustive testing and incorporates a few enhancements, focusing on the operational possibilities offered by ICP's technology. This work hence serves as a broad reference for the protocol now publicly accessible.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eInternet Computer\u7684\u79c1\u6709\u4f20\u8f93\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u4e2a\u77ed\u6682\u4e2d\u4ecb\u5206\u79bb\u5b58\u6b3e\u548c\u68c0\u7d22\uff0c\u4f7f\u7528\u5bc6\u5c01\u5b58\u50a8\u548c\u77ed\u6682\u89c1\u8bc1\u7684\u8ba4\u8bc1\u62c6\u9664\uff0c\u63d0\u4f9b\u53d1\u9001\u8005\u8eab\u4efd\u9690\u79c1\u548c\u5185\u5bb9\u673a\u5bc6\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u5728Internet Computer\u4e0a\u5b9e\u73b0\u79c1\u6709\u4f20\u8f93\u7684\u534f\u8bae\uff0c\u89e3\u51b3\u4f20\u7edf\u533a\u5757\u94fe\u4ea4\u6613\u4e2d\u9690\u79c1\u4fdd\u62a4\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u53d1\u9001\u8005\u8eab\u4efd\u9690\u79c1\u3001\u5185\u5bb9\u673a\u5bc6\u6027\u548c\u4e2d\u4ecb\u4e0d\u53ef\u4fe1\u73af\u5883\u4e0b\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u975e\u4ea4\u4e92\u5f0fRDMPF\u5c01\u88c5\u6280\u672f\u6d3e\u751f\u6bcf\u7b14\u4f20\u8f93\u7684\u4f20\u8f93\u5bc6\u94a5\uff1b\u901a\u8fc7\u4e24\u4e2a\u77ed\u6682\u4e2d\u4ecb\u5206\u79bb\u5b58\u6b3e\u548c\u68c0\u7d22\u8fc7\u7a0b\uff1b\u91c7\u7528\u5bc6\u5c01\u5b58\u50a8\u548c\u77ed\u6682\u89c1\u8bc1\u7684\u8ba4\u8bc1\u62c6\u9664\u673a\u5236\uff1b\u8ba1\u7b97\u516c\u5171\u901a\u77e5\u63d0\u793a\u5b9e\u73b0\u65e0\u6307\u7eb9\u7684\u63a5\u6536\u8005\u53d1\u73b0\uff1b\u4f7f\u7528\u89e3\u5bc6\u8bc1\u660e\u8fdb\u884c\u6388\u6743\u800c\u4e0d\u66b4\u9732\u8eab\u4efd\u3002", "result": "\u534f\u8bae\u5df2\u5728ICP\u4e0a\u4ee5ICPP\u540d\u79f0\u6295\u5165\u751f\u4ea7\uff0c\u7ecf\u8fc7\u8be6\u5c3d\u6d4b\u8bd5\u5e76\u5305\u542b\u591a\u9879\u589e\u5f3a\u529f\u80fd\uff1b\u63d0\u4f9b\u4e86\u53d1\u9001\u8005\u8eab\u4efd\u9690\u79c1\u3001\u5185\u5bb9\u673a\u5bc6\u6027\u3001\u4f20\u8f93\u5bc6\u94a5\u7684\u524d\u5411\u4fdd\u5bc6\u6027\uff0c\u4ee5\u53ca\u53ef\u9a8c\u8bc1\u7684\u6d3b\u8dc3\u6027\u548c\u6700\u7ec8\u6027\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3aICP\u4e0a\u7684\u79c1\u6709\u4f20\u8f93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u3001\u53ef\u5ba1\u8ba1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u9690\u79c1\u4fdd\u62a4\u3001\u5185\u5bb9\u673a\u5bc6\u6027\u548c\u53ef\u9a8c\u8bc1\u7684\u6700\u7ec8\u6027\uff0c\u5e76\u5df2\u6210\u529f\u90e8\u7f72\u4e3a\u751f\u4ea7\u7cfb\u7edf\uff0c\u53ef\u4f5c\u4e3a\u8be5\u534f\u8bae\u7684\u5e7f\u6cdb\u53c2\u8003\u3002"}}
{"id": "2512.22231", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22231", "abs": "https://arxiv.org/abs/2512.22231", "authors": ["Nachiappan Chockalingam", "Akshay Deshpande", "Lokesh Butra", "Ram Sekhar Bodala", "Nitin Saksena", "Adithya Parthasarathy", "Balakrishna Pothineni", "Akash Kumar Agarwal"], "title": "Scalable Cloud-Native Architectures for Intelligent PMU Data Processing", "comment": null, "summary": "Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e91\u539f\u751f\u67b6\u6784\u7684\u667a\u80fdPMU\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6d41\u5904\u7406\u3001\u5bb9\u5668\u5316\u5fae\u670d\u52a1\u548c\u5f39\u6027\u8d44\u6e90\u7f16\u6392\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u7684\u7535\u7f51\u5b9e\u65f6\u76d1\u63a7\u4e0e\u5206\u6790\u3002", "motivation": "\u968f\u7740PMU\u90e8\u7f72\u89c4\u6a21\u7684\u6269\u5927\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u5904\u7406\u67b6\u6784\u5728\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u7535\u7f51\u52a8\u6001\u8fd0\u884c\u6761\u4ef6\u4e0b\u7684\u6570\u636e\u91cf\u548c\u5904\u7406\u901f\u5ea6\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e91\u539f\u751f\u67b6\u6784\uff0c\u6574\u5408\u8fb9\u7f18\u4e0e\u4e91\u8ba1\u7b97\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u6d41\u5904\u7406\u3001\u5bb9\u5668\u5316\u5fae\u670d\u52a1\u548c\u5f39\u6027\u8d44\u6e90\u7f16\u6392\u6280\u672f\uff0c\u5e76\u96c6\u6210\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002", "result": "\u5206\u6790\u6a21\u578b\u663e\u793a\u8be5\u67b6\u6784\u53ef\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5927\u89c4\u6a21PMU\u90e8\u7f72\uff0c\u540c\u65f6\u5177\u5907\u5b89\u5168\u9690\u79c1\u673a\u5236\uff0c\u9002\u7528\u4e8e\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u73af\u5883\u3002", "conclusion": "\u8be5\u4e91\u539f\u751f\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u7535\u7f51\u5206\u6790\u63d0\u4f9b\u4e86\u7a33\u5065\u7075\u6d3b\u7684\u57fa\u7840\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3PMU\u6570\u636e\u5904\u7406\u4e2d\u7684\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002"}}
{"id": "2512.23557", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23557", "abs": "https://arxiv.org/abs/2512.23557", "authors": ["Toqeer Ali Syed", "Mishal Ateeq Almutairi", "Mahmoud Abdel Moaty"], "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks", "comment": "It is accepted in a conference paper, ICCA 2025 in Bahrain on 21 to 23 December", "summary": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u667a\u80fd\u4f53\u591a\u6a21\u6001\u6eaf\u6e90\u611f\u77e5\u9632\u5fa1\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u9632\u5fa1\u591a\u6a21\u6001\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u786e\u4fdd\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u591a\u667a\u80fd\u4f53\u73af\u5883\u589e\u52a0\u4e86\u591a\u6a21\u6001\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u98ce\u9669\uff0c\u6076\u610f\u6307\u4ee4\u53ef\u80fd\u901a\u8fc7\u6587\u672c\u3001\u56fe\u50cf\u3001\u5143\u6570\u636e\u6216\u667a\u80fd\u4f53\u95f4\u6d88\u606f\u4f20\u64ad\uff0c\u5bfc\u81f4\u610f\u5916\u884c\u4e3a\u3001\u7b56\u7565\u8fdd\u53cd\u6216\u72b6\u6001\u7834\u574f\u3002", "method": "\u63d0\u51fa\u8de8\u667a\u80fd\u4f53\u591a\u6a21\u6001\u6eaf\u6e90\u611f\u77e5\u9632\u5fa1\u6846\u67b6\uff0c\u5305\u542b\u6587\u672c\u51c0\u5316\u667a\u80fd\u4f53\u3001\u89c6\u89c9\u51c0\u5316\u667a\u80fd\u4f53\u548c\u8f93\u51fa\u9a8c\u8bc1\u667a\u80fd\u4f53\uff0c\u7531\u6eaf\u6e90\u8d26\u672c\u534f\u8c03\u7ba1\u7406\u3002\u8be5\u6846\u67b6\u5bf9\u6240\u6709\u63d0\u793a\u8fdb\u884c\u51c0\u5316\u5904\u7406\uff0c\u5e76\u5728\u53d1\u9001\u5230\u4e0b\u6e38\u8282\u70b9\u524d\u72ec\u7acb\u9a8c\u8bc1LLM\u751f\u6210\u7684\u8f93\u51fa\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6a21\u6001\u3001\u6765\u6e90\u548c\u4fe1\u4efb\u7ea7\u522b\u7684\u5143\u6570\u636e\u786e\u4fdd\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u7b26\u5408\u660e\u786e\u7684\u4fe1\u4efb\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u6ce8\u5165\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u6700\u5c0f\u5316\u4e86\u8de8\u667a\u80fd\u4f53\u4fe1\u4efb\u6cc4\u6f0f\uff0c\u5e76\u4f7f\u667a\u80fd\u4f53\u6267\u884c\u8def\u5f84\u66f4\u52a0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u6eaf\u6e90\u8ffd\u8e2a\u548c\u9a8c\u8bc1\u6982\u5ff5\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e2d\uff0c\u6709\u52a9\u4e8e\u5efa\u7acb\u5b89\u5168\u3001\u53ef\u7406\u89e3\u548c\u53ef\u9760\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u3002"}}
{"id": "2512.22625", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22625", "abs": "https://arxiv.org/abs/2512.22625", "authors": ["Paul Schneider", "Amalie Schramm"], "title": "The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?", "comment": "13 pages, 2 figures, 5 tables, for source code and data see https://github.com/priorb-source/delib-ai-wisdom", "summary": "Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u4e92\u76f8\u5ba1\u9605\u9884\u6d4b\u80fd\u5426\u63d0\u5347\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u5728\u4fe1\u606f\u5171\u4eab\u7684\u591a\u6837\u5316\u6a21\u578b\u7ec4\u4e2d\u80fd\u663e\u8457\u6539\u5584\u9884\u6d4b\uff0c\u4f46\u5728\u540c\u8d28\u5316\u6a21\u578b\u7ec4\u4e2d\u65e0\u6548\uff0c\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e5f\u65e0\u5e2e\u52a9\u3002", "motivation": "\u7ed3\u6784\u5316\u5ba1\u8bae\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u5347\u4eba\u7c7b\u9884\u6d4b\u8005\u7684\u8868\u73b0\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7c7b\u4f3c\u7684\u5e72\u9884\u63aa\u65bd\u2014\u2014\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u66f4\u65b0\u9884\u6d4b\u524d\u4e92\u76f8\u5ba1\u9605\u5f7c\u6b64\u7684\u9884\u6d4b\u2014\u2014\u662f\u5426\u4e5f\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u6765\u81eaMetaculus Q2 2025 AI\u9884\u6d4b\u9526\u6807\u8d5b\u7684202\u4e2a\u5df2\u89e3\u51b3\u4e8c\u5143\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u573a\u666f\u4e0b\u7684\u51c6\u786e\u6027\uff1a(1)\u591a\u6837\u5316\u6a21\u578b+\u5206\u5e03\u5f0f\u4fe1\u606f\uff0c(2)\u591a\u6837\u5316\u6a21\u578b+\u5171\u4eab\u4fe1\u606f\uff0c(3)\u540c\u8d28\u5316\u6a21\u578b+\u5206\u5e03\u5f0f\u4fe1\u606f\uff0c(4)\u540c\u8d28\u5316\u6a21\u578b+\u5171\u4eab\u4fe1\u606f\u3002\u6bd4\u8f83\u4e86\u6a21\u578b\u5728\u4e92\u76f8\u5ba1\u9605\u9884\u6d4b\u524d\u540e\u7684\u8868\u73b0\u3002", "result": "\u5e72\u9884\u63aa\u65bd\u5728\u573a\u666f(2)\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5c06Log Loss\u964d\u4f4e\u4e860.020\uff08\u76f8\u5bf9\u6539\u5584\u7ea64%\uff0cp=0.017\uff09\u3002\u4f46\u5728\u540c\u8d28\u5316\u6a21\u578b\u7ec4\uff08\u540c\u4e00\u6a21\u578b\u7684\u4e09\u4e2a\u5b9e\u4f8b\uff09\u4e2d\u8fdb\u884c\u76f8\u540c\u8fc7\u7a0b\u65f6\u672a\u89c2\u5bdf\u5230\u4efb\u4f55\u76ca\u5904\u3002\u610f\u5916\u7684\u662f\uff0c\u4e3aLLMs\u63d0\u4f9b\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u672a\u6539\u5584\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u5ba1\u8bae\u53ef\u80fd\u662f\u6539\u5584LLM\u9884\u6d4b\u7684\u53ef\u884c\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u591a\u6837\u5316\u6a21\u578b\u7ec4\u4e2d\u3002\u7136\u800c\uff0c\u540c\u8d28\u5316\u6a21\u578b\u7ec4\u7684\u5ba1\u8bae\u65e0\u6548\uff0c\u4e14\u4fe1\u606f\u6c60\u5316\u673a\u5236\u672a\u80fd\u901a\u8fc7\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\u5f97\u5230\u9a8c\u8bc1\uff0c\u8868\u660e\u6a21\u578b\u591a\u6837\u6027\u5728\u5ba1\u8bae\u8fc7\u7a0b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2512.22402", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22402", "abs": "https://arxiv.org/abs/2512.22402", "authors": ["Bhanu Prakash Vangala", "Tanu Malik"], "title": "Efficient Multi-Model Orchestration for Self-Hosted Large Language Models", "comment": null, "summary": "Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.", "AI": {"tldr": "Pick and Spin\u662f\u4e00\u4e2a\u57fa\u4e8eKubernetes\u7684LLM\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u90e8\u7f72\u3001\u81ea\u9002\u5e94\u7f29\u5bb9\u548c\u6df7\u5408\u8def\u7531\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u81ea\u6258\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u7ecf\u6d4e\u6027\u3002", "motivation": "\u7ec4\u7ec7\u81ea\u6258\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u3001\u6210\u672c\u63a7\u5236\u548c\u5b9a\u5236\u5316\u65b9\u9762\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5728GPU\u5229\u7528\u7387\u3001\u5de5\u4f5c\u8d1f\u8f7d\u8def\u7531\u548c\u53ef\u9760\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eKubernetes\u6784\u5efa\uff0c\u5305\u542b\u7edf\u4e00Helm\u90e8\u7f72\u7cfb\u7edf\u3001\u81ea\u9002\u5e94\u7f29\u5bb9\u81ea\u52a8\u5316\u3001\u6df7\u5408\u8def\u7531\u6a21\u5757\uff08\u7ed3\u5408\u5173\u952e\u8bcd\u542f\u53d1\u5f0f\u548c\u8f7b\u91cf\u7ea7DistilBERT\u5206\u7c7b\u5668\uff09\uff0c\u5728\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u8fdb\u884c\u5e73\u8861\u3002", "result": "\u5728\u56db\u4e2a\u6a21\u578b\uff08Llama-3 90B\u3001Gemma-3 27B\u3001Qwen-3 235B\u3001DeepSeek-R1 685B\uff09\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d68\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u30015\u79cd\u63a8\u7406\u7b56\u7565\u548c2\u79cd\u8def\u7531\u53d8\u4f53\uff0c\u517131,019\u4e2a\u63d0\u793a\u548c163,720\u6b21\u63a8\u7406\u8fd0\u884c\u3002\u76f8\u6bd4\u9759\u6001\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe21.6%\u7684\u6210\u529f\u7387\u63d0\u5347\u300130%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c33%\u7684GPU\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "Pick and Spin\u6846\u67b6\u4f7f\u81ea\u6258\u7ba1LLM\u7f16\u6392\u53d8\u5f97\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u90e8\u7f72\u6548\u7387\u3001\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u4e3a\u7ec4\u7ec7\u81ea\u6258\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23610", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23610", "abs": "https://arxiv.org/abs/2512.23610", "authors": ["Heba Osama", "Omar Elebiary", "Youssef Qassim", "Mohamed Amgad", "Ahmed Maghawry", "Ahmed Saafan", "Haitham Ghalwash"], "title": "Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation", "comment": null, "summary": "Web applications increasingly face evasive and polymorphic attack payloads, yet traditional web application firewalls (WAFs) based on static rule sets such as the OWASP Core Rule Set (CRS) often miss obfuscated or zero-day patterns without extensive manual tuning. This work introduces WAMM, an AI-driven multiclass web attack detection framework designed to reveal the limitations of rule-based systems by reclassifying HTTP requests into OWASP-aligned categories for a specific technology stack. WAMM applies a multi-phase enhancement pipeline to the SR-BH 2020 dataset that includes large-scale deduplication, LLM-guided relabeling, realistic attack data augmentation, and LLM-based filtering, producing three refined datasets. Four machine and deep learning models are evaluated using a unified feature space built from statistical and text-based representations. Results show that using an augmented and LLM-filtered dataset on the same technology stack, XGBoost reaches 99.59% accuracy with microsecond-level inference while deep learning models degrade under noisy augmentation. When tested against OWASP CRS using an unseen augmented dataset, WAMM achieves true positive block rates between 96 and 100% with improvements of up to 86%. These findings expose gaps in widely deployed rule-based defenses and demonstrate that curated training pipelines combined with efficient machine learning models enable a more resilient, real-time approach to web attack detection suitable for production WAF environments.", "AI": {"tldr": "WAMM\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u591a\u7c7b\u522bWeb\u653b\u51fb\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u7c7bHTTP\u8bf7\u6c42\u5230OWASP\u5bf9\u9f50\u7684\u7c7b\u522b\uff0c\u63ed\u793a\u4e86\u57fa\u4e8e\u89c4\u5219\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5728\u7279\u5b9a\u6280\u672f\u6808\u4e0a\u8fbe\u523099.59%\u7684\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4f20\u7edfWAF\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u89c4\u5219\u96c6\u7684Web\u5e94\u7528\u9632\u706b\u5899\uff08\u5982OWASP CRS\uff09\u7ecf\u5e38\u9519\u8fc7\u6df7\u6dc6\u6216\u96f6\u65e5\u653b\u51fb\u6a21\u5f0f\uff0c\u9700\u8981\u5927\u91cf\u624b\u52a8\u8c03\u4f18\u3002Web\u5e94\u7528\u9762\u4e34\u65e5\u76ca\u589e\u591a\u7684\u89c4\u907f\u6027\u548c\u591a\u6001\u6027\u653b\u51fb\u8f7d\u8377\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "WAMM\u91c7\u7528\u591a\u9636\u6bb5\u589e\u5f3a\u7ba1\u9053\u5904\u7406SR-BH 2020\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u53bb\u91cd\u3001LLM\u5f15\u5bfc\u7684\u91cd\u65b0\u6807\u6ce8\u3001\u73b0\u5b9e\u653b\u51fb\u6570\u636e\u589e\u5f3a\u548cLLM\u8fc7\u6ee4\uff0c\u751f\u6210\u4e09\u4e2a\u7cbe\u70bc\u6570\u636e\u96c6\u3002\u4f7f\u7528\u7edf\u8ba1\u548c\u6587\u672c\u8868\u793a\u7684\u7edf\u4e00\u7279\u5f81\u7a7a\u95f4\u8bc4\u4f30\u56db\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728\u76f8\u540c\u6280\u672f\u6808\u4e0a\uff0c\u4f7f\u7528\u589e\u5f3a\u548cLLM\u8fc7\u6ee4\u7684\u6570\u636e\u96c6\uff0cXGBoost\u8fbe\u523099.59%\u7684\u51c6\u786e\u7387\uff0c\u5177\u6709\u5fae\u79d2\u7ea7\u63a8\u7406\u65f6\u95f4\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u566a\u58f0\u589e\u5f3a\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u9488\u5bf9OWASP CRS\u4f7f\u7528\u672a\u89c1\u8fc7\u7684\u589e\u5f3a\u6570\u636e\u96c6\u6d4b\u8bd5\u65f6\uff0cWAMM\u5b9e\u73b0\u4e8696-100%\u7684\u771f\u6b63\u9633\u6027\u62e6\u622a\u7387\uff0c\u6539\u8fdb\u9ad8\u8fbe86%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5e7f\u6cdb\u90e8\u7f72\u7684\u57fa\u4e8e\u89c4\u5219\u9632\u5fa1\u7684\u5dee\u8ddd\uff0c\u5e76\u8bc1\u660e\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u7ba1\u9053\u7ed3\u5408\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u66f4\u5f39\u6027\u3001\u5b9e\u65f6\u7684Web\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u751f\u4ea7WAF\u73af\u5883\u3002"}}
{"id": "2512.22629", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.22629", "abs": "https://arxiv.org/abs/2512.22629", "authors": ["Shiyan Liu", "Jian Ma", "Rui Qu"], "title": "DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation", "comment": "Accepted at ResponsibleFM @ NeurIPS 2025", "summary": "As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\\{A, B, Tie\\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \\log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.", "AI": {"tldr": "DICE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u3001\u8bc1\u636e\u8026\u5408\u7684RAG\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5206\u6790\u63a8\u7406\u548c\u6982\u7387\u8bc4\u5206\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u8bc4\u4f30\uff0c\u540c\u65f6\u91c7\u7528\u745e\u58eb\u5236\u9526\u6807\u8d5b\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(N\u00b2)\u964d\u4f4e\u5230O(N log N)\u3002", "motivation": "\u968f\u7740RAG\u7cfb\u7edf\u5411\u66f4\u590d\u6742\u67b6\u6784\u6f14\u8fdb\uff0c\u9700\u8981\u901a\u8fc7\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u8bc4\u4f30\u6765\u786e\u4fdd\u5176\u53ef\u4fe1\u5ea6\u3002\u73b0\u6709\u7684\u6807\u91cf\u6307\u6807\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0d\u8db3\u4ee5\u53ca\u5728\u591a\u7cfb\u7edf\u6bd4\u8f83\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86RAG\u6280\u672f\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "method": "DICE\u91c7\u7528\u4e24\u9636\u6bb5\u3001\u8bc1\u636e\u8026\u5408\u7684\u6846\u67b6\uff1a1) \u7ed3\u5408\u6df1\u5ea6\u5206\u6790\u63a8\u7406\u4e0e\u6982\u7387{A, B, Tie}\u8bc4\u5206\uff0c\u751f\u6210\u900f\u660e\u3001\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u5224\u65ad\uff1b2) \u91c7\u7528\u745e\u58eb\u5236\u9526\u6807\u8d5b\u65b9\u6cd5\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(N\u00b2)\u964d\u4f4e\u5230O(N log N)\uff0c\u540c\u65f6\u4fdd\u6301\u6392\u5e8f\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u4e2d\u6587\u91d1\u878dQA\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0cDICE\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u4e00\u81f4\u6027\u8fbe\u523085.7%\uff0c\u663e\u8457\u4f18\u4e8eRAGAS\u7b49\u73b0\u6709LLM\u57fa\u51c6\u6307\u6807\u3002\u5728\u516b\u7cfb\u7edf\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e8642.9%\u7684\u8ba1\u7b97\u91cf\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6392\u5e8f\u4fdd\u771f\u5ea6\u3002", "conclusion": "DICE\u5efa\u7acb\u4e86\u4e00\u4e2a\u8d1f\u8d23\u4efb\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u53ef\u4fe1\u8d56\u7684RAG\u7cfb\u7edf\u8bc4\u4f30\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u75d5\u8ff9\u652f\u6301\u53ef\u95ee\u8d23\u7684\u7cfb\u7edf\u6539\u8fdb\uff0c\u5b9e\u73b0\u7cfb\u7edf\u6027\u9519\u8bef\u8bca\u65ad\u548c\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.22420", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22420", "abs": "https://arxiv.org/abs/2512.22420", "authors": ["Rui Li", "Zhaoning Zhang", "Libo Zhang", "Huaimin Wang", "Xiang Fu", "Zhiquan Lai"], "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving", "comment": "6 pages, 11 figures", "summary": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.", "AI": {"tldr": "Nightjar\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u957f\u5ea6\u6765\u4f18\u5316LLM\u63a8\u7406\u6027\u80fd\uff0c\u5728\u5b9e\u65f6\u670d\u52a1\u573a\u666f\u4e2d\u76f8\u6bd4\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u5b9e\u73b0\u4e86\u6700\u9ad814.8%\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c20.2%\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u4f7f\u7528\u56fa\u5b9a\u7684\u63a8\u6d4b\u957f\u5ea6\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u8bf7\u6c42\u8d1f\u8f7d\u53d8\u5316\uff0c\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u9ad8\u8d1f\u8f7d\u73af\u5883\u4e2d\u4f1a\u56e0\u9a8c\u8bc1\u5f00\u9500\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u5728\u5b9e\u9645\u670d\u52a1\u573a\u666f\u4e2d\u9020\u6210\u4e86\u663e\u8457\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faNightjar\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u63a8\u6d4b\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u8bf7\u6c42\u8d1f\u8f7d\u52a8\u6001\u9009\u62e9\u6700\u4f18\u63a8\u6d4b\u957f\u5ea6\uff0c\u751a\u81f3\u5728\u6ca1\u6709\u6536\u76ca\u65f6\u5b8c\u5168\u7981\u7528\u63a8\u6d4b\u89e3\u7801\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u6279\u5904\u7406\u5927\u5c0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNightjar\u76f8\u6bd4\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u5b9e\u73b0\u4e86\u6700\u9ad814.8%\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c20.2%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5728\u5b9e\u65f6\u670d\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "Nightjar\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u6d4b\u957f\u5ea6\u89e3\u51b3\u4e86\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u5728\u52a8\u6001\u8d1f\u8f7d\u73af\u5883\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u4e3aLLM\u63a8\u7406\u7684\u5b9e\u65f6\u670d\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22492", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22492", "abs": "https://arxiv.org/abs/2512.22492", "authors": ["Zhenqian Chen", "Baoquan Zhong", "Xiang Li", "Qing Dai", "Xinkui Zhao", "Miao Ye", "Ren Cheng", "Lufei Zhang", "Jianwei Yin"], "title": "Role-Based Fault Tolerance System for LLM RL Post-Training", "comment": "16 pages, 19 figures", "summary": "RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.\n  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \\textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \\textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \\textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\\% failure injection frequency, RobustRL can achieve an ETTR of over 80\\% compared with the 60\\% in ByteRobust and achieves 8.4\\%-17.4\\% faster in end-to-end training time.", "AI": {"tldr": "RobustRL\u662f\u4e00\u4e2a\u9488\u5bf9LLM\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u89d2\u8272\u9694\u79bb\u3001\u667a\u80fd\u6545\u969c\u68c0\u6d4b\u3001\u975e\u4e2d\u65ad\u6062\u590d\u548c\u52a8\u6001\u91cd\u8fde\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86GPU\u96c6\u7fa4\u6545\u969c\u4e0b\u7684\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u6545\u969c\u5bb9\u9519\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u8bad\u7ec3\u6216\u63a8\u7406\u573a\u666f\uff0c\u800cRL\u540e\u8bad\u7ec3\u6df7\u5408\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u73b0\u6709\u65b9\u6848\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5f02\u6b65\u6267\u884c\u7684\u4f18\u5316\u6f5c\u529b\u3002\u5f53GPU\u673a\u5668\u6545\u969c\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u91cd\u542f\u6574\u4e2aRL\u4efb\u52a1\uff0c\u9020\u6210\u5de8\u5927\u5f00\u9500\u3002", "method": "1. \u89d2\u8272\u9694\u79bb\uff1a\u5c06\u8bad\u7ec3\u5668\u3001rollout\u548c\u7ba1\u7406\u89d2\u8272\u89c6\u4e3a\u72ec\u7acb\u5206\u5e03\u5f0f\u5b50\u4efb\u52a1\uff1b2. \u667a\u80fd\u68c0\u6d4b\uff1a\u5b9e\u73b0\u89d2\u8272\u611f\u77e5\u76d1\u63a7\uff0c\u533a\u5206\u5b9e\u9645\u6545\u969c\u4e0e\u89d2\u8272\u7279\u5b9a\u884c\u4e3a\uff1b3. \u975e\u4e2d\u65ad\u6062\u590d\uff1a\u8bad\u7ec3\u5668\u901a\u8fc7rollout\u70ed\u5907\u5feb\u901f\u6062\u590d\uff0crollout\u8fdb\u884c\u9694\u79bb\u673a\u5668\u66ff\u6362\uff1b4. \u52a8\u6001\u91cd\u8fde\uff1a\u7528UCX\u70b9\u5bf9\u70b9\u901a\u4fe1\u66ff\u4ee3\u9759\u6001\u96c6\u5408\u901a\u4fe1\uff0c\u5b9e\u73b0\u6743\u91cd\u5373\u65f6\u540c\u6b65\u3002", "result": "\u5728256-GPU\u96c6\u7fa4\u4e0a\u4f7f\u7528Qwen3-8B-Math\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6545\u969c\u6ce8\u5165\u9891\u738710%\u7684\u6761\u4ef6\u4e0b\uff0cRobustRL\u7684ETTR\uff08\u6709\u6548\u8bad\u7ec3\u65f6\u95f4\u6bd4\uff09\u8d85\u8fc780%\uff0c\u76f8\u6bd4ByteRobust\u768460%\u6709\u663e\u8457\u63d0\u5347\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u52a0\u5feb8.4%-17.4%\u3002", "conclusion": "RobustRL\u901a\u8fc7\u89d2\u8272\u9694\u79bb\u7684\u6545\u969c\u5bb9\u9519\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86RL\u540e\u8bad\u7ec3\u5728GPU\u6545\u969c\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5bb9\u9519\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22716", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22716", "abs": "https://arxiv.org/abs/2512.22716", "authors": ["Jun Wang"], "title": "Memento-II: Learning by Stateful Reflective Memory", "comment": "32 pages, three figures", "summary": "We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u60c5\u666f\u8bb0\u5fc6\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u53cd\u601d\u673a\u5236\u8fdb\u884c\u6301\u7eed\u548c\u4f53\u9a8c\u5f0f\u5b66\u4e60\uff0c\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u6216\u6a21\u578b\u5fae\u8c03\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u4e25\u683c\u5206\u79bb\uff0c\u9700\u8981\u53cd\u5411\u4f20\u64ad\u6216\u6a21\u578b\u5fae\u8c03\u6765\u5b9e\u73b0\u9002\u5e94\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53cd\u601d\u673a\u5236\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u4ea4\u4e92\u4e2d\u6301\u7eed\u5b66\u4e60\uff0c\u6253\u7834\u8bad\u7ec3\u4e0e\u90e8\u7f72\u7684\u754c\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u72b6\u6001\u5316\u53cd\u601d\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5c06\u53cd\u601d\u5b66\u4e60\u5efa\u6a21\u4e3a\u4e0e\u60c5\u666f\u8bb0\u5fc6\u7684\u4e24\u9636\u6bb5\u8bfb\u5199\u4ea4\u4e92\uff1a\u5199\u5165\u5b58\u50a8\u4ea4\u4e92\u7ed3\u679c\uff08\u7b56\u7565\u8bc4\u4f30\uff09\uff0c\u8bfb\u53d6\u68c0\u7d22\u76f8\u5173\u5386\u53f2\u6848\u4f8b\uff08\u7b56\u7565\u6539\u8fdb\uff09\u3002\u8be5\u8fc7\u7a0b\u8bf1\u5bfc\u51fa\u589e\u5f3a\u72b6\u6001\u8bb0\u5fc6\u8868\u793a\u7684\u7b49\u6548\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u7b56\u7565\u8fed\u4ee3\u5b9e\u4f8b\u5316\uff0c\u5e76\u5efa\u7acb\u4e86\u6536\u655b\u4fdd\u8bc1\u3002\u5f53\u60c5\u666f\u8bb0\u5fc6\u589e\u957f\u5e76\u5145\u5206\u8986\u76d6\u72b6\u6001\u7a7a\u95f4\u65f6\uff0c\u6240\u5f97\u7b56\u7565\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u548c\u68c0\u7d22\u7684\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4e0d\u66f4\u65b0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\uff0c\u4e3a\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2512.22899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22899", "abs": "https://arxiv.org/abs/2512.22899", "authors": ["Yaping Zhang", "Qixuan Zhang", "Xingquan Zhang", "Zhiyuan Chen", "Wenwen Zhuang", "Yupu Liang", "Lu Xiang", "Yang Zhao", "Jiajun Zhang", "Yu Zhou", "Chengqing Zong"], "title": "HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery", "comment": null, "summary": "The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \\textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \\textit{Scientific Literacy} (L1), \\textit{Literature Parsing} (L2), \\textit{Literature-based Question Answering} (L3), \\textit{Literature Review Generation} (L4), and \\textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\\% accuracy on basic literacy tasks, performance declines sharply to 25\\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.", "AI": {"tldr": "HiSciBench\u662f\u4e00\u4e2a\u5206\u5c42\u79d1\u5b66\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u4e2a\u5c42\u7ea7\u30018735\u4e2a\u5b9e\u4f8b\uff0c\u6db5\u76d66\u5927\u5b66\u79d1\uff0c\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u6a21\u578b\u5728\u5b8c\u6574\u79d1\u7814\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u4e8e\u788e\u7247\u5316\uff0c\u4e13\u6ce8\u4e8e\u72ed\u7a84\u4efb\u52a1\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u79d1\u5b66\u63a2\u7a76\u7684\u5c42\u6b21\u6027\u548c\u591a\u5b66\u79d1\u6027\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u6a21\u578b\u5728\u6574\u4e2a\u79d1\u7814\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b5\u4e2a\u5c42\u7ea7\u7684HiSciBench\u57fa\u51c6\uff1a\u79d1\u5b66\u7d20\u517b(L1)\u3001\u6587\u732e\u89e3\u6790(L2)\u3001\u57fa\u4e8e\u6587\u732e\u7684\u95ee\u7b54(L3)\u3001\u6587\u732e\u7efc\u8ff0\u751f\u6210(L4)\u548c\u79d1\u5b66\u53d1\u73b0(L5)\u3002\u5305\u542b8735\u4e2a\u5b9e\u4f8b\uff0c\u6db5\u76d6\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u3001\u5730\u7406\u3001\u5929\u65876\u5927\u5b66\u79d1\uff0c\u652f\u6301\u6587\u672c\u3001\u516c\u5f0f\u3001\u56fe\u8868\u7b49\u591a\u6a21\u6001\u8f93\u5165\u548c\u8de8\u8bed\u8a00\u8bc4\u4f30\u3002", "result": "\u5bf9GPT-5\u3001DeepSeek-R1\u7b49\u9886\u5148\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1a\u5728\u57fa\u7840\u7d20\u517b\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u53ef\u8fbe69%\uff0c\u4f46\u5728\u53d1\u73b0\u7ea7\u6311\u6218\u4e0a\u6025\u5267\u4e0b\u964d\u523025%\u3002", "conclusion": "HiSciBench\u4e3a\u8bc4\u4f30\u79d1\u5b66\u667a\u80fd\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u53ef\u9760\u7684\u6a21\u578b\u3002\u57fa\u51c6\u5c06\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2512.22695", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22695", "abs": "https://arxiv.org/abs/2512.22695", "authors": ["Mona Moghadampanah", "Adib Rezaei Shahmirzadi", "Farhana Amin", "Dimitrios S. Nikolopoulos"], "title": "Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference", "comment": null, "summary": "Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u80fd\u8017\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u8f93\u5165\u5bfc\u81f417%-94%\u7684\u989d\u5916\u80fd\u8017\uff0c\u4e3b\u8981\u6e90\u4e8e\u89c6\u89c9\u7f16\u7801\u548c\u89c6\u89c9token\u5e8f\u5217\u6269\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u9636\u6bb5\u7ea7DVFS\u4f18\u5316\u65b9\u6848\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u6a21\u578b\u57fa\u7840\u4e0a\u5f15\u5165\u89c6\u89c9\u7b49\u6a21\u6001\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6a21\u578b\uff0c\u5bf9\u591a\u6a21\u6001\u5f15\u5165\u7684\u80fd\u8017\u6743\u8861\u7f3a\u4e4f\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5206\u6790\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u80fd\u6548\u95ee\u9898\u3002", "method": "\u5c06MLLM\u63a8\u7406\u6d41\u7a0b\u5206\u89e3\u4e3a\u89c6\u89c9\u7f16\u7801\u3001\u9884\u586b\u5145\u548c\u89e3\u7801\u4e09\u4e2a\u9636\u6bb5\uff0c\u5728NVIDIA A100 GPU\u4e0a\u8bc4\u4f30\u56db\u79cd\u4ee3\u8868\u6027MLLM\u3002\u901a\u8fc7\u5206\u6790GPU\u529f\u8017\u8f68\u8ff9\uff0c\u91cf\u5316\u591a\u6a21\u6001\u63a8\u7406\u76f8\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u7684\u989d\u5916\u80fd\u8017\uff0c\u5e76\u63a2\u7d22\u9636\u6bb5\u7ea7\u52a8\u6001\u7535\u538b\u9891\u7387\u7f29\u653e\u4f18\u5316\u3002", "result": "\u591a\u6a21\u6001\u63a8\u7406\u80fd\u8017\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u9ad817%-94%\uff0c\u74f6\u9888\u56e0\u6a21\u578b\u67b6\u6784\u800c\u5f02\uff1a\u8ba1\u7b97\u5bc6\u96c6\u7684\u89c6\u89c9\u7f16\u7801\u5668\u6216\u5927\u91cf\u89c6\u89c9token\u5e8f\u5217\u7684\u9884\u586b\u5145\u9636\u6bb5\u3002\u53d1\u73b0GPU\u5728\u591a\u6a21\u6001\u6267\u884c\u4e2d\u5b58\u5728\u663e\u8457\u5229\u7528\u7387\u4e0d\u8db3\uff0c\u8f93\u5165\u590d\u6742\u5ea6\u5bfc\u81f4\u4e0d\u540c\u6a21\u578b\u7684\u80fd\u8017\u6269\u5c55\u884c\u4e3a\u5dee\u5f02\u660e\u663e\u3002\u9636\u6bb5\u7ea7DVFS\u4f18\u5316\u53ef\u5728\u4ec5\u8f7b\u5fae\u6027\u80fd\u5f71\u54cd\u4e0b\u5b9e\u73b0\u80fd\u8017\u8282\u7701\u3002", "conclusion": "\u591a\u6a21\u6001LLM\u670d\u52a1\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u80fd\u6548\u4f18\u5316\u7a7a\u95f4\uff0c\u6a21\u578b\u67b6\u6784\u9009\u62e9\u3001\u8f93\u5165\u5904\u7406\u7b56\u7565\u548c\u8fd0\u884c\u65f6\u4f18\u5316\uff08\u5982\u9636\u6bb5\u7ea7DVFS\uff09\u662f\u8bbe\u8ba1\u66f4\u8282\u80fd\u591a\u6a21\u6001LLM\u670d\u52a1\u7cfb\u7edf\u7684\u5173\u952e\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u9ad8\u6548\u591a\u6a21\u6001AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u548c\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2512.22931", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22931", "abs": "https://arxiv.org/abs/2512.22931", "authors": ["Ling Xin", "Mojtaba Nayyeri", "Zahra Makki Nayeri", "Steffen Staab"], "title": "Geometric Structural Knowledge Graph Foundation Model", "comment": "Submitted to IEEE TPAMI, under review", "summary": "Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.", "AI": {"tldr": "Gamma\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u591a\u5934\u51e0\u4f55\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528\u591a\u79cd\u4ee3\u6570\u53d8\u6362\uff08\u5b9e\u6570\u3001\u590d\u6570\u3001\u5206\u88c2\u590d\u6570\u3001\u5bf9\u5076\u6570\uff09\u6765\u589e\u5f3a\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u76f8\u6bd4\u5355\u4e00\u5173\u7cfb\u53d8\u6362\u7684Ultra\u6a21\u578b\u5728\u96f6\u6837\u672c\u5f52\u7eb3\u94fe\u63a5\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u6a21\u578b\uff08\u5982Ultra\uff09\u4f9d\u8d56\u5355\u4e00\u5173\u7cfb\u53d8\u6362\uff08\u5982\u9010\u5143\u7d20\u4e58\u6cd5\uff09\u8fdb\u884c\u6d88\u606f\u4f20\u9012\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u6837\u5316\u56fe\u8c31\u4e2d\u5c55\u73b0\u7684\u4e0d\u540c\u5173\u7cfb\u548c\u7ed3\u6784\u6a21\u5f0f\u3002", "method": "\u63d0\u51faGamma\u6a21\u578b\uff0c\u5f15\u5165\u591a\u5934\u51e0\u4f55\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u591a\u79cd\u5e76\u884c\u4ee3\u6570\u53d8\u6362\uff08\u5b9e\u6570\u3001\u590d\u6570\u3001\u5206\u88c2\u590d\u6570\u3001\u5bf9\u5076\u6570\uff09\u66ff\u4ee3\u5355\u4e00\u5173\u7cfb\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u5173\u7cfb\u6761\u4ef6\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u5728\u94fe\u63a5\u7ea7\u522b\u81ea\u9002\u5e94\u878d\u5408\u8fd9\u4e9b\u53d8\u6362\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u95e8\u63a7\u548c\u71b5\u6b63\u5219\u5316\u3002", "result": "\u572856\u4e2a\u591a\u6837\u5316\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cGamma\u5728\u96f6\u6837\u672c\u5f52\u7eb3\u94fe\u63a5\u9884\u6d4b\u4e2d\u6301\u7eed\u4f18\u4e8eUltra\uff0c\u5728\u5f52\u7eb3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5012\u6570\u6392\u540d\u63d0\u53475.5%\uff0c\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u53474.4%\u3002", "conclusion": "Gamma\u901a\u8fc7\u4e92\u8865\u7684\u51e0\u4f55\u8868\u793a\u63d0\u9ad8\u4e86\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u591a\u5934\u51e0\u4f55\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u591a\u6837\u5316\u56fe\u8c31\u4e2d\u7684\u5173\u7cfb\u548c\u7ed3\u6784\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u591a\u79cd\u4ee3\u6570\u53d8\u6362\u7ec4\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.22743", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.22743", "abs": "https://arxiv.org/abs/2512.22743", "authors": ["Ertza Warraich", "Ali Imran", "Annus Zulfiqar", "Shay Vargaftik", "Sonia Fahmy", "Muhammad Shahbaz"], "title": "OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads", "comment": "15 pages", "summary": "As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.\n  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).\n  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.", "AI": {"tldr": "OptiNIC\u662f\u4e00\u4e2a\u9488\u5bf9\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4f18\u5316\u7684RDMA\u4f20\u8f93\u534f\u8bae\uff0c\u901a\u8fc7\u6d88\u9664\u91cd\u4f20\u548c\u987a\u5e8f\u4ea4\u4ed8\u8981\u6c42\uff0c\u5229\u7528ML\u5bf9\u6570\u636e\u4e22\u5931\u7684\u5bb9\u5fcd\u6027\u6765\u964d\u4f4e\u5c3e\u90e8\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6269\u5c55\u5230\u6570\u5343\u4e2aGPU\uff0c\u96c6\u4f53\u901a\u4fe1\u4e2d\u7684\u5c3e\u90e8\u5ef6\u8fdf\u5df2\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709RDMA\u4f20\u8f93\u534f\u8bae\uff08\u5982RoCE\u3001IRN\u7b49\uff09\u5f3a\u5236\u5b9e\u65bd\u4e25\u683c\u53ef\u9760\u6027\u548c\u987a\u5e8f\u4ea4\u4ed8\uff0c\u4f9d\u8d56\u91cd\u4f20\u548c\u5305\u6392\u5e8f\uff0c\u8fd9\u5728ML\u573a\u666f\u4e2d\u5f15\u5165\u4e86\u590d\u6742\u6027\u548c\u5ef6\u8fdf\uff0c\u5373\u4f7f\u7f55\u89c1\u7684\u5305\u5ef6\u8fdf\u4e5f\u4f1a\u963b\u585e\u6574\u4e2a\u6a21\u578b\u6d41\u6c34\u7ebf\u3002", "method": "OptiNIC\u662f\u4e00\u4e2a\u9886\u57df\u7279\u5b9a\u7684RDMA\u4f20\u8f93\u534f\u8bae\uff0c\u91cd\u65b0\u5ba1\u89c6\u4f20\u7edf\u53ef\u9760\u6027\u4fdd\u8bc1\uff0c\u57fa\u4e8eML\u5bf9\u90e8\u5206\u6216\u4e22\u5931\u6570\u636e\u7684\u5bb9\u5fcd\u6027\u3002\u5b83\u4eceNIC\u4e2d\u6d88\u9664\u91cd\u4f20\u548c\u987a\u5e8f\u4ea4\u4ed8\uff0c\u5b9e\u73b0\u5c3d\u529b\u800c\u4e3a\u3001\u4e71\u5e8f\u7684RDMA\u4f20\u8f93\u6a21\u578b\u3002\u5f15\u5165\u81ea\u9002\u5e94\u8d85\u65f6\u673a\u5236\u5728\u6570\u636e\u53ef\u80fd\u4e22\u5931\u6216\u5ef6\u8fdf\u65f6\u89e6\u53d1\u524d\u8fdb\uff0c\u540c\u65f6\u4fdd\u7559\u6807\u51c6\u62e5\u585e\u63a7\u5236\u673a\u5236\uff0c\u5c06\u4e22\u5931\u6062\u590d\u8f6c\u79fb\u5230ML\u6d41\u6c34\u7ebf\u672c\u8eab\uff08\u5982\u901a\u8fc7Hadamard\u53d8\u6362\u548c\u7ea0\u5220\u7801\uff09\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cOptiNIC\u5728\u4e24\u4e2a\u516c\u5171\u4e91\uff08Hyperstack\u548cCloudLab\uff09\u4e0a\uff0c\u5c06\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u65f6\u95f4\u5230\u51c6\u786e\u5ea6\uff08TTA\uff09\u5206\u522b\u63d0\u9ad82\u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad81.6\u500d\u3002\u540c\u65f6\u5c06\u7b2c99\u767e\u5206\u4f4d\u5ef6\u8fdf\u964d\u4f4e3.5\u500d\uff0cBRAM\u4f7f\u7528\u51cf\u5c112.7\u500d\uff0cNIC\u5bf9\u6545\u969c\u7684\u6062\u590d\u80fd\u529b\u51e0\u4e4e\u7ffb\u500d\u3002", "conclusion": "OptiNIC\u4e3a\u5206\u5e03\u5f0fML\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u5f39\u6027\u3001\u5c3e\u90e8\u4f18\u5316\u7684RDMA\u4f20\u8f93\u534f\u8bae\uff0c\u901a\u8fc7\u5229\u7528ML\u5e94\u7528\u5bf9\u6570\u636e\u4e22\u5931\u7684\u5bb9\u5fcd\u7279\u6027\uff0c\u663e\u8457\u6539\u5584\u4e86\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2512.22925", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22925", "abs": "https://arxiv.org/abs/2512.22925", "authors": ["Panlong Wu", "Yifei Zhong", "Danyang Chen", "Ting Wang", "Fangxin Wang"], "title": "Argus: Token Aware Distributed LLM Inference Optimization", "comment": null, "summary": "Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.", "AI": {"tldr": "Argus\u662f\u4e00\u4e2a\u9762\u5411\u5f02\u6784\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u7684LLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4ee4\u724c\u611f\u77e5\u7684\u4efb\u52a1\u5378\u8f7d\u4f18\u5316\uff0c\u89e3\u51b3\u63a8\u7406\u65f6\u95f4\u53ef\u53d8\u6027\u95ee\u9898", "motivation": "LLM\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9762\u4e34\u63a8\u7406\u65f6\u95f4\u53ef\u53d8\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f02\u6784\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u4e2d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5ffd\u89c6\u4e86\u52a8\u6001\u3001\u968f\u673a\u548c\u5f02\u6784\u73af\u5883\u7684\u7279\u6027\uff0c\u5ffd\u7565\u4e86\u53ef\u53d8\u8f93\u51fa\u4ee4\u724c\u957f\u5ea6\u548c\u8bbe\u5907\u591a\u6837\u6027\u7684\u5f71\u54cd\u3002", "method": "Argus\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u957f\u5ea6\u611f\u77e5\u8bed\u4e49(LAS)\u6a21\u5757\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u8f93\u51fa\u4ee4\u724c\u957f\u5ea6\uff1b2) Lyapunov\u5f15\u5bfc\u7684\u5378\u8f7d\u4f18\u5316(LOO)\u6a21\u5757\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8fed\u4ee3\u5378\u8f7d\u7b97\u6cd5(IODCC)\u89e3\u51b3\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cArgus\u5728\u9ad8\u5ea6\u52a8\u6001\u3001\u5f02\u6784\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u6027\u80fd\u548c\u5353\u8d8a\u6548\u7387\u3002", "conclusion": "Argus\u662f\u7b2c\u4e00\u4e2a\u4ee4\u724c\u611f\u77e5\u7684\u5206\u5e03\u5f0f\u8fb9\u7f18-\u4e91LLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u4ee4\u724c\u957f\u5ea6\u9884\u6d4b\u548c\u4f18\u5316\u7684\u4efb\u52a1\u5378\u8f7d\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u65f6\u95f4\u53ef\u53d8\u6027\u95ee\u9898\u3002"}}
{"id": "2512.23036", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23036", "abs": "https://arxiv.org/abs/2512.23036", "authors": ["Danial Hooshyar", "Yeongwook Yang", "Gustav \u0160\u00ed\u0159", "Tommi K\u00e4rkk\u00e4inen", "Raija H\u00e4m\u00e4l\u00e4inen", "Mutlu Cukurova", "Roger Azevedo"], "title": "Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education", "comment": null, "summary": "The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\\% over the zero-shot baseline, it remains 6\\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728K-12\u6559\u80b2\u4e2d\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f85\u5bfc\u7cfb\u7edf\u5728\u8bc4\u4f30\u5b66\u751f\u77e5\u8bc6\u53d8\u5316\u65b9\u9762\u4e0d\u5982\u4f20\u7edf\u7684\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u51c6\u786e\u53ef\u9760\uff0c\u9700\u8981\u6df7\u5408\u6846\u67b6\u6765\u786e\u4fdd\u8d1f\u8d23\u4efb\u7684\u6559\u5b66\u8bbe\u8ba1\u3002", "motivation": "\u9488\u5bf9K-12\u6559\u80b2\u4e2d\u666e\u904d\u5b58\u5728\u7684\u8bef\u89e3\u2014\u2014\u8ba4\u4e3a\u751f\u6210\u5f0f\u6a21\u578b\u53ef\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u5b66\u4e60\u8005\u5efa\u6a21\u8fdb\u884c\u81ea\u9002\u5e94\u6559\u5b66\uff0c\u7279\u522b\u662f\u5728\u6b27\u76dfAI\u6cd5\u6848\u5c06\u5176\u5217\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u9700\u8981\u8d1f\u8d23\u4efb\u8bbe\u8ba1\u7684\u80cc\u666f\u4e0b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LLM\u8f85\u5bfc\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u96f6\u6837\u672c\u548c\u5fae\u8c03\u7248\u672c\uff09\uff0c\u4f7f\u7528\u5927\u578b\u5f00\u653e\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u5728\u4e0b\u4e00\u6b65\u6b63\u786e\u6027\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff08AUC=0.83\uff09\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u3002\u5fae\u8c03\u867d\u4f7fLLM\u7684AUC\u63d0\u9ad8\u7ea68%\uff0c\u4f46\u4ecd\u6bd4DKT\u4f4e6%\uff0c\u4e14\u65e9\u671f\u5e8f\u5217\u9519\u8bef\u7387\u66f4\u9ad8\u3002\u65f6\u95f4\u5206\u6790\u663e\u793aDKT\u4fdd\u6301\u7a33\u5b9a\u3001\u65b9\u5411\u6b63\u786e\u7684\u638c\u63e1\u5ea6\u66f4\u65b0\uff0c\u800cLLM\u53d8\u4f53\u8868\u73b0\u51fa\u663e\u8457\u7684\u65f6\u95f4\u5f31\u70b9\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5355\u72ec\u4f7f\u7528\u96be\u4ee5\u5339\u654c\u6210\u719f\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u6548\u679c\uff0c\u8d1f\u8d23\u4efb\u7684\u8f85\u5bfc\u9700\u8981\u7ed3\u5408\u5b66\u4e60\u8005\u5efa\u6a21\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8bc4\u4f30\u4e00\u81f4\u6027\u65b9\u9762\uff0c\u4f20\u7edf\u65b9\u6cd5\u4ecd\u5177\u4f18\u52bf\u3002"}}
{"id": "2512.23434", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.23434", "abs": "https://arxiv.org/abs/2512.23434", "authors": ["Yongjie Guan"], "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates", "comment": "14 pages, 10 figures. Includes appendices", "summary": "Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.", "AI": {"tldr": "LRH\u662f\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u54c8\u5e0c\u65b9\u6848\uff0c\u901a\u8fc7\u9650\u5236HRW\u9009\u62e9\u5230\u7f13\u5b58\u5c40\u90e8\u7a97\u53e3\uff0c\u5728\u4fdd\u6301\u73af\u7ed3\u6784\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8d1f\u8f7d\u5747\u8861\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e00\u81f4\u6027\u54c8\u5e0c\u65b9\u6848\u5b58\u5728\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff1a\u57fa\u4e8e\u73af\u7684\u65b9\u6848\u9700\u8981\u5927\u91cf\u865a\u62df\u8282\u70b9\u624d\u80fd\u964d\u4f4e\u5cf0\u503c\u8d1f\u8f7d\u6bd4\uff0c\u800c\u591a\u63a2\u9488\u65b9\u6cd5\u867d\u7136\u6539\u5584\u4e86\u5e73\u8861\u6027\u4f46\u5bfc\u81f4\u5185\u5b58\u8bbf\u95ee\u5206\u6563\u3001\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u4f1a\u5408\u54c8\u5e0c(LRH)\uff0c\u4fdd\u7559\u4ee4\u724c\u73af\u7ed3\u6784\u4f46\u5c06\u6700\u9ad8\u968f\u673a\u6743\u91cd\u9009\u62e9\u9650\u5236\u5728C\u4e2a\u76f8\u90bb\u7269\u7406\u8282\u70b9\u7684\u7f13\u5b58\u5c40\u90e8\u7a97\u53e3\u5185\u3002\u901a\u8fc7\u4e00\u6b21\u4e8c\u5206\u67e5\u627e\u5b9a\u4f4d\u952e\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u4e0d\u540c\u504f\u79fb\u91cf\u679a\u4e3eC\u4e2a\u5019\u9009\u8282\u70b9\uff0c\u9009\u62e9HRW\u80dc\u51fa\u8005\u3002", "result": "\u5728N=5000\u3001V=256\u3001K=5000\u4e07\u3001C=8\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLRH\u5c06\u6700\u5927/\u5e73\u5747\u8d1f\u8f7d\u4ece1.2785\u964d\u81f31.0947\uff0c\u8fbe\u523060.05 Mkeys/s\uff0c\u6bd48\u63a2\u9488\u591a\u63a2\u9488\u4e00\u81f4\u6027\u54c8\u5e0c\u5feb6.8\u500d\uff0c\u540c\u65f6\u63a5\u8fd1\u5176\u8d1f\u8f7d\u5e73\u8861\u6c34\u5e73(1.0697)\u3002", "conclusion": "LRH\u5728\u4fdd\u6301\u73af\u7ed3\u6784\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u7f13\u5b58\u5c40\u90e8\u6027\u663e\u8457\u63d0\u5347\u4e86\u8d1f\u8f7d\u5747\u8861\u548c\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e00\u81f4\u6027\u54c8\u5e0c\u65b9\u6848\u5728\u5e73\u8861\u6027\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2512.23090", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23090", "abs": "https://arxiv.org/abs/2512.23090", "authors": ["Armin Berger", "Manuela Bergau", "Helen Schneider", "Saad Ahmad", "Tom Anglim Lagones", "Gianluca Brugnara", "Martha Foltyn-Dumitru", "Kai Schlamp", "Philipp Vollmuth", "Rafet Sifa"], "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients", "comment": null, "summary": "Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.", "AI": {"tldr": "ChexReason\u662f\u4e00\u4e2a\u901a\u8fc7R1\u98ce\u683c\u65b9\u6cd5\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u533b\u7597\u5f71\u50cf\u9886\u57df\u4f7f\u7528\u6709\u9650\u8d44\u6e90\uff082000\u4e2aSFT\u6837\u672c\u30011000\u4e2aRL\u6837\u672c\u3001\u5355A100 GPU\uff09\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u53d1\u73b0GRPO\u80fd\u63d0\u5347\u540c\u5206\u5e03\u6027\u80fd\u4f46\u635f\u5bb3\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597\u5f71\u50cf\u5e94\u7528\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528R1\u98ce\u683c\u65b9\u6cd5\uff1a\u5148\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u7136\u540e\u4f7f\u7528GRPO\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u4ec5\u4f7f\u75282000\u4e2aSFT\u6837\u672c\u30011000\u4e2aRL\u6837\u672c\u548c\u5355\u4e2aA100 GPU\u3002\u5728CheXpert\u548cNIH\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u6790\u540c\u5206\u5e03\u6027\u80fd\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "result": "GRPO\u5728\u540c\u5206\u5e03\u6570\u636e\u96c6\uff08CheXpert\uff09\u4e0a\u6027\u80fd\u63d0\u534723%\uff08macro-F1=0.346\uff09\uff0c\u4f46\u5728\u8de8\u6570\u636e\u96c6\uff08NIH\uff09\u4e0a\u6027\u80fd\u4e0b\u964d19%\u3002\u8fd9\u53cd\u6620\u4e86\u4e0e\u9ad8\u8d44\u6e90\u6a21\u578b\u7c7b\u4f3c\u7684\u95ee\u9898\uff0c\u8868\u660e\u95ee\u9898\u6e90\u4e8eRL\u8303\u5f0f\u800c\u975e\u6a21\u578b\u89c4\u6a21\u3002SFT\u68c0\u67e5\u70b9\u5728RL\u4f18\u5316\u524d\u80fd\u72ec\u7279\u5730\u6539\u5584NIH\u6027\u80fd\uff0c\u8868\u660e\u6559\u5e08\u5f15\u5bfc\u7684\u63a8\u7406\u80fd\u6355\u6349\u66f4\u591a\u673a\u6784\u65e0\u5173\u7279\u5f81\u3002", "conclusion": "\u5bf9\u4e8e\u9700\u8981\u8de8\u4e0d\u540c\u4eba\u7fa4\u9c81\u68d2\u6027\u7684\u4e34\u5e8a\u90e8\u7f72\uff0c\u7cbe\u5fc3\u7b56\u5212\u7684\u76d1\u7763\u5fae\u8c03\u53ef\u80fd\u4f18\u4e8e\u6fc0\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u3002\u7ed3\u6784\u5316\u63a8\u7406\u652f\u67b6\u5bf9\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6709\u76ca\uff0c\u4f46\u5bf9\u533b\u5b66\u9884\u8bad\u7ec3\u6a21\u578b\u589e\u76ca\u6709\u9650\u3002"}}
{"id": "2512.23126", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23126", "abs": "https://arxiv.org/abs/2512.23126", "authors": ["Yu Li", "Tian Lan", "Zhengling Qi"], "title": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization", "comment": null, "summary": "Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \\q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Intrinsic Self-reflective Preference Optimization (q)\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86DPO\u7684\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a\u6700\u4f18\u7b56\u7565\u4f9d\u8d56\u4e8e\u4efb\u610f\u5efa\u6a21\u9009\u62e9\uff0c\u4ee5\u53ca\u5b64\u7acb\u5904\u7406\u54cd\u5e94\u751f\u6210\u672a\u80fd\u5229\u7528\u6210\u5bf9\u6570\u636e\u4e2d\u7684\u6bd4\u8f83\u4fe1\u606f\u3002", "motivation": "DPO\u53ca\u5176\u53d8\u4f53\u867d\u7136\u56e0\u7b80\u5355\u6027\u548c\u79bb\u7ebf\u7a33\u5b9a\u6027\u6210\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a1) \u6700\u4f18\u7b56\u7565\u4f9d\u8d56\u4e8e\u6807\u91cf\u5316\u51fd\u6570\u3001\u53c2\u8003\u7b56\u7565\u7b49\u4efb\u610f\u5efa\u6a21\u9009\u62e9\uff0c\u5bfc\u81f4\u884c\u4e3a\u53cd\u6620\u53c2\u6570\u5316\u4f2a\u5f71\u800c\u975e\u771f\u5b9e\u504f\u597d\uff1b2) \u5b64\u7acb\u5904\u7406\u54cd\u5e94\u751f\u6210\u672a\u80fd\u5229\u7528\u6210\u5bf9\u6570\u636e\u4e2d\u7684\u6bd4\u8f83\u4fe1\u606f\uff0c\u672a\u80fd\u6316\u6398\u6a21\u578b\u5185\u5728\u81ea\u53cd\u601d\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Intrinsic Self-reflective Preference Optimization (q)\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u548c\u66ff\u4ee3\u54cd\u5e94\u7684\u5168\u5c40\u6700\u4f18\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u589e\u5f3a\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u6216\u63a8\u7406\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u8bc1\u5bf9\u6807\u91cf\u5316\u548c\u53c2\u8003\u9009\u62e9\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u80dc\u7387\u548c\u957f\u5ea6\u63a7\u5236\u6307\u6807\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u91ca\u653e\u81ea\u53cd\u601d\u80fd\u529b\u80fd\u591f\u4ea7\u751f\u66f4\u9c81\u68d2\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u5bf9\u9f50\u7684LLMs\u3002", "conclusion": "q\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u4f18\u4e8eDPO/RLHF\uff0c\u540c\u65f6\u4fdd\u8bc1\u5bf9\u6807\u91cf\u5316\u548c\u53c2\u8003\u9009\u62e9\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u89e3\u9501\u6a21\u578b\u7684\u81ea\u53cd\u601d\u80fd\u529b\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u9c81\u68d2\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2512.23494", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.23494", "abs": "https://arxiv.org/abs/2512.23494", "authors": ["Eddy Truyen", "Wouter Joosen"], "title": "Optimal Configuration of API Resources in Cloud Native Computing", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u79bb\u7ebf\u6027\u80fd\u4f18\u5316\u6846\u67b6\u5e94\u7528\u4e8eDevOps\u53d1\u5e03\u9636\u6bb5\u7684\u5fae\u670d\u52a1\u5e94\u7528\uff0c\u4f18\u5316CPU\u548c\u5185\u5b58\u8d44\u6e90\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8fd0\u7ef4\u9636\u6bb5\u800c\u5ffd\u7565\u53d1\u5e03\u524d\u8d44\u6e90\u914d\u7f6e\u8c03\u4f18\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8DevOps\u8fd0\u7ef4\u9636\u6bb5\u7684\u667a\u80fd\u8c03\u5ea6\u548c\u81ea\u52a8\u6269\u7f29\u5bb9\uff0c\u4f46\u5ffd\u7565\u4e86\u53d1\u5e03\u9636\u6bb5CPU\u548c\u5185\u5b58\u8d44\u6e90\u914d\u7f6e\u7684\u4f18\u5316\u3002\u5373\u4f7f\u57fa\u4e8eCPU\u4f7f\u7528\u7387\u8fdb\u884c\u5bb9\u5668\u6c34\u5e73\u81ea\u52a8\u6269\u7f29\u5bb9\uff0c\u5982\u679c\u90e8\u7f72\u524d\u6ca1\u6709\u5bf9\u4e24\u79cd\u8d44\u6e90\u8fdb\u884c\u7cbe\u7ec6\u8c03\u4f18\uff0c\u5bb9\u5668\u4ecd\u53ef\u80fd\u5206\u914d\u4e0d\u9002\u5f53\u7684\u5185\u5b58\u8d44\u6e90\u3002", "method": "\u4f7f\u7528TeaStore\u5fae\u670d\u52a1\u5e94\u7528\u8bc4\u4f30\u6027\u80fd\u4f18\u5316\u6846\u67b6\uff0c\u7edf\u8ba1\u6bd4\u8f83\u4e0d\u540c\u4f18\u5316\u7b97\u6cd5\u3002\u7814\u7a76\u63a2\u8ba8\u4e86\u56e0\u5b50\u7b5b\u9009\uff08\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\uff09\u4e0e\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u4e0d\u540c\u76ee\u6807\u4e0b\u7684\u5e94\u7528\u7b56\u7565\u3002", "result": "\u7814\u7a76\u8868\u660e\uff1a\u5f53\u76ee\u6807\u662f\u627e\u5230\u6700\u4f18\u8d44\u6e90\u914d\u7f6e\u4e14\u91c7\u6837\u9884\u7b97\u6709\u9650\u65f6\uff0c\u524d\u7f6e\u56e0\u5b50\u7b5b\u9009\u6709\u52a9\u4e8e\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\uff1b\u5f53\u9700\u8981\u7edf\u8ba1\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u65f6\uff0c\u7b5b\u9009\u4e5f\u4f7f\u641c\u7d22\u7a7a\u95f4\u4e2d\u6240\u6709\u6570\u636e\u70b9\u7684\u6570\u636e\u6536\u96c6\u53d8\u5f97\u53ef\u884c\uff1b\u4f46\u5f53\u76ee\u6807\u662f\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u914d\u7f6e\u65f6\uff0c\u65e0\u7b5b\u9009\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u5728DevOps\u53d1\u5e03\u9636\u6bb5\u5e94\u7528\u6027\u80fd\u4f18\u5316\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u4f18\u5316\u5fae\u670d\u52a1\u5e94\u7528\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u4e0d\u540c\u4f18\u5316\u76ee\u6807\u9700\u8981\u91c7\u7528\u4e0d\u540c\u7684\u7b97\u6cd5\u7b56\u7565\uff0c\u56e0\u5b50\u7b5b\u9009\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u5404\u6709\u9002\u7528\u573a\u666f\u3002"}}
{"id": "2512.23163", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23163", "abs": "https://arxiv.org/abs/2512.23163", "authors": ["Max Parks", "Kheli Atluru", "Meera Vinod", "Mike Kuniavsky", "Jud Brewer", "Sean White", "Sarah Adler", "Wendy Ju"], "title": "Why We Need a New Framework for Emotional Intelligence in AI", "comment": null, "summary": "In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u5f53\u524d\u8bc4\u4f30AI\u7cfb\u7edf\u60c5\u611f\u667a\u80fd\u7684\u6846\u67b6\u9700\u8981\u6539\u8fdb\uff0c\u56e0\u4e3a\u5b83\u4eec\u672a\u80fd\u5168\u9762\u8861\u91cfAI\u76f8\u5173\u7684EI\u5404\u4e2a\u65b9\u9762\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u8bc4\u4f30\u7b56\u7565\u7684\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u60c5\u611f\u667a\u80fd\u7684\u6846\u67b6\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u54ea\u4e9b\u4eba\u7c7bEI\u65b9\u9762\u9002\u7528\u4e8eAI\u7cfb\u7edf\uff0c\u54ea\u4e9b\u4e0d\u9002\u7528\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u5168\u9762\u4e14\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002", "method": "\u9996\u5148\u56de\u987e\u4e0d\u540c\u60c5\u611f\u7406\u8bba\u548c\u4e00\u822cEI\u7406\u8bba\uff0c\u8bc4\u4f30\u5b83\u4eec\u5bf9\u4eba\u5de5\u7cfb\u7edf\u7684\u9002\u7528\u6027\uff1b\u7136\u540e\u6279\u5224\u6027\u8bc4\u4f30\u73b0\u6709\u57fa\u51c6\u6846\u67b6\uff0c\u8bc6\u522b\u5176\u4e0d\u8db3\u4e4b\u5904\uff1b\u6700\u540e\u63d0\u51fa\u6539\u8fdbEI\u8bc4\u4f30\u7b56\u7565\u7684\u65b9\u6848\u3002", "result": "\u8bc6\u522b\u51fa\u73b0\u6709EI\u8bc4\u4f30\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u60c5\u611f\u672c\u8d28\u548c\u60c5\u611f\u667a\u80fd\u7684\u575a\u5b9e\u57fa\u7840\uff0c\u4ee5\u53ca\u672a\u80fd\u533a\u5206\u9002\u7528\u4e8eAI\u548c\u4e0d\u9002\u7528\u4e8eAI\u7684EI\u65b9\u9762\u3002", "conclusion": "\u9700\u8981\u6539\u8fdbAI\u7cfb\u7edf\u60c5\u611f\u667a\u80fd\u7684\u8bc4\u4f30\u7b56\u7565\uff0c\u5efa\u7acb\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u533a\u5206\u4eba\u7c7bEI\u4e2d\u9002\u7528\u4e8eAI\u7684\u65b9\u9762\uff08\u5982\u611f\u77e5\u3001\u89e3\u91ca\u3001\u54cd\u5e94\u548c\u9002\u5e94\u60c5\u611f\u72b6\u6001\u7684\u80fd\u529b\uff09\u548c\u4e0d\u9002\u7528\u4e8eAI\u7684\u65b9\u9762\uff08\u5982\u73b0\u8c61\u5b66\u4f53\u9a8c\uff09\u3002"}}
{"id": "2512.23495", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23495", "abs": "https://arxiv.org/abs/2512.23495", "authors": ["Eddy Truyen"], "title": "Decoupling Adaptive Control in TeaStore", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u63a7\u5236\u5faa\u73af\u5b9e\u73b0\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\uff0c\u5206\u6790\u4e86\u4e09\u79cd\u4e0d\u540c\u65b9\u6cd5\uff08\u8f6f\u4ef6\u67b6\u6784\u65b9\u6cd5\u3001\u4e91\u539f\u751fOperator\u6a21\u5f0f\u3001\u4f20\u7edf\u7f16\u7a0b\u6280\u672f\uff09\u5728\u5b9e\u73b0TeaStore\u89c4\u8303\u65f6\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u5c42\u67b6\u6784\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "TeaStore\u89c4\u8303\u4e3a\u901a\u8fc7\u63a7\u5236\u5faa\u73af\u5b9e\u73b0\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u5fae\u670d\u52a1\u6848\u4f8b\u7814\u7a76\uff0c\u4f46\u5b9e\u73b0\u9700\u8981\u8003\u8651\u81ea\u9002\u5e94\u7cfb\u7edf\u7684\u5173\u952e\u7279\u6027\uff1a\u7cfb\u7edf\u8303\u56f4\u4e00\u81f4\u6027\u3001\u89c4\u5212\u80fd\u529b\u548c\u6a21\u5757\u5316\uff0c\u4ee5\u6709\u6548\u89e3\u8026\u81ea\u9002\u5e94\u63a7\u5236\u903b\u8f91\u4e0e\u5e94\u7528\u4e1a\u52a1\u903b\u8f91\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86\u4e09\u79cd\u5b9e\u73b0\u65b9\u6cd5\uff1a1\uff09\u8f6f\u4ef6\u67b6\u6784\u65b9\u6cd5\uff1b2\uff09\u4e91\u539f\u751fOperator\u6a21\u5f0f\uff1b3\uff09\u4f20\u7edf\u7f16\u7a0b\u8bed\u8a00\u6280\u672f\u3002\u901a\u8fc7\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u8868\u8fbe\u6027\u81ea\u9002\u5e94\u4e0e\u7cfb\u7edf\u8303\u56f4\u63a7\u5236\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63a2\u8ba8\u5982\u4f55\u6709\u6548\u89e3\u8026\u81ea\u9002\u5e94\u63a7\u5236\u903b\u8f91\u3002", "result": "\u5206\u6790\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u975e\u4e92\u65a5\uff0c\u53ef\u4ee5\u7ed3\u5408\u5f62\u6210\u591a\u5c42\u67b6\u6784\u7528\u4e8e\u81ea\u9002\u5e94\u5fae\u670d\u52a1\u3002\u4e0d\u540c\u65b9\u6cd5\u5728\u91cd\u7528\u81ea\u9002\u5e94\u7b56\u7565\u65b9\u9762\u5404\u6709\u4f18\u52bf\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u7ec4\u5408\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8f6f\u4ef6\u67b6\u6784\u65b9\u6cd5\u3001Operator\u6a21\u5f0f\u548c\u4f20\u7edf\u7f16\u7a0b\u6280\u672f\uff0c\u53ef\u4ee5\u6784\u5efa\u591a\u5c42\u67b6\u6784\u7684\u81ea\u9002\u5e94\u5fae\u670d\u52a1\u7cfb\u7edf\uff0c\u5e73\u8861\u7ec6\u7c92\u5ea6\u81ea\u9002\u5e94\u8868\u8fbe\u4e0e\u7cfb\u7edf\u8303\u56f4\u63a7\u5236\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u81ea\u9002\u5e94\u7b56\u7565\u91cd\u7528\u3002"}}
{"id": "2512.23167", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23167", "abs": "https://arxiv.org/abs/2512.23167", "authors": ["Yifan Zhang", "Giridhar Ganapavarapu", "Srideepika Jayaraman", "Bhavna Agrawal", "Dhaval Patel", "Achille Fokoue"], "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search", "comment": null, "summary": "Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.", "AI": {"tldr": "SPIRAL\u6846\u67b6\u5c06\u4e09\u4e2a\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u5d4c\u5165MCTS\u5faa\u73af\uff0c\u901a\u8fc7\u89c4\u5212\u5668\u3001\u6a21\u62df\u5668\u548c\u6279\u8bc4\u5668\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u5c06MCTS\u4ece\u66b4\u529b\u641c\u7d22\u8f6c\u53d8\u4e3a\u5f15\u5bfc\u5f0f\u3001\u81ea\u6821\u6b63\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u89c4\u5212\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u63a2\u7d22\u548c\u81ea\u6821\u6b63\u7684\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5176\u7ebf\u6027\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u4ece\u65e9\u671f\u9519\u8bef\u4e2d\u6062\u590d\u3002\u800c\u50cf\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u8fd9\u6837\u7684\u641c\u7d22\u7b97\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u5f15\u5bfc\u4e0b\u6548\u679c\u6709\u9650\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u4e30\u5bcc\u8bed\u4e49\u80fd\u529b\u3002", "method": "SPIRAL\u6846\u67b6\u5728MCTS\u5faa\u73af\u4e2d\u5d4c\u5165\u4e09\u4e2a\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\uff1a\u89c4\u5212\u5668\u63d0\u51fa\u521b\u9020\u6027\u4e0b\u4e00\u6b65\uff0c\u6a21\u62df\u5668\u901a\u8fc7\u9884\u6d4b\u73b0\u5b9e\u7ed3\u679c\u6765\u63a5\u5730\u641c\u7d22\uff0c\u6279\u8bc4\u5668\u901a\u8fc7\u53cd\u601d\u63d0\u4f9b\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u3002\u8fd9\u79cd\u534f\u540c\u5de5\u4f5c\u5c06MCTS\u4ece\u66b4\u529b\u641c\u7d22\u8f6c\u53d8\u4e3a\u5f15\u5bfc\u5f0f\u3001\u81ea\u6821\u6b63\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728DailyLifeAPIs\u548cHuggingFace\u6570\u636e\u96c6\u4e0a\uff0cSPIRAL\u6301\u7eed\u4f18\u4e8e\u9ed8\u8ba4\u7684\u601d\u7ef4\u94fe\u89c4\u5212\u65b9\u6cd5\u548c\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u3002\u5728DailyLifeAPIs\u4e0a\u8fbe\u523083.6%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u6bd4\u6b21\u4f18\u641c\u7d22\u6846\u67b6\u63d0\u9ad8\u4e86\u8d85\u8fc716\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u4ee4\u724c\u6548\u7387\u3002", "conclusion": "\u5c06LLM\u63a8\u7406\u6784\u5efa\u4e3a\u5f15\u5bfc\u5f0f\u3001\u53cd\u601d\u6027\u548c\u63a5\u5730\u6c14\u7684\u641c\u7d22\u8fc7\u7a0b\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u5f3a\u5927\u548c\u9ad8\u6548\u7684\u81ea\u4e3b\u89c4\u5212\u5668\u3002\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u590d\u6742\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.23184", "categories": ["cs.AI", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.23184", "abs": "https://arxiv.org/abs/2512.23184", "authors": ["Hongshen Sun", "Juanjuan Zhang"], "title": "From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research", "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output (\"model choice\") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes \"model belief,\" a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u6a21\u578b\u4fe1\u5ff5\"\u6982\u5ff5\uff0c\u5229\u7528LLM\u7684token\u7ea7\u6982\u7387\u5206\u5e03\u66ff\u4ee3\u5355\u4e00\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u5229\u7528\u6548\u7387\uff0c\u5728\u9700\u6c42\u4f30\u8ba1\u5b9e\u9a8c\u4e2d\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u7ea620\u500d\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528LLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65f6\uff0c\u901a\u5e38\u5c06\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u5355\u4e00\u6570\u636e\u70b9\uff0c\u8fd9\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u56fa\u6709\u7684\u6982\u7387\u7279\u6027\uff0c\u5bfc\u81f4\u6570\u636e\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\"\u6a21\u578b\u4fe1\u5ff5\"\u6982\u5ff5\uff0c\u4eceLLM\u7684token\u7ea7\u6982\u7387\u5206\u5e03\u4e2d\u63d0\u53d6\u6a21\u578b\u5bf9\u9009\u62e9\u66ff\u4ee3\u65b9\u6848\u7684\u4fe1\u5ff5\u5206\u5e03\uff0c\u8bc1\u660e\u5176\u6e10\u8fd1\u7b49\u4ef7\u4e8e\u6a21\u578b\u9009\u62e9\u7684\u5747\u503c\u4f46\u5177\u6709\u66f4\u9ad8\u7edf\u8ba1\u6548\u7387\u3002", "result": "\u6a21\u578b\u4fe1\u5ff5\u5728\u6709\u9650\u8fd0\u884c\u6b21\u6570\u4e0b\u6bd4\u6a21\u578b\u9009\u62e9\u672c\u8eab\u80fd\u66f4\u597d\u5730\u89e3\u91ca\u548c\u9884\u6d4b\u771f\u5b9e\u6a21\u578b\u9009\u62e9\uff0c\u5728\u9700\u6c42\u4f30\u8ba1\u7814\u7a76\u4e2d\u5c06\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u7ea620\u500d\u3002", "conclusion": "\u6a21\u578b\u4fe1\u5ff5\u5e94\u4f5c\u4e3a\u9ed8\u8ba4\u6d4b\u91cf\u65b9\u6cd5\uff0c\u80fd\u4eceLLM\u751f\u6210\u6570\u636e\u4e2d\u63d0\u53d6\u66f4\u591a\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u7edf\u8ba1\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2512.23324", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.23324", "abs": "https://arxiv.org/abs/2512.23324", "authors": ["Raven Beutner", "Bernd Finkbeiner"], "title": "On Conformant Planning and Model-Checking of $\\exists^*\\forall^*$ Hyperproperties", "comment": "ECAI 2025", "summary": "We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\\exists^*\\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c4\u5212\u4e0e\u9a8c\u8bc1\u9886\u57df\u4e2d\u4e24\u4e2a\u95ee\u9898\u7684\u8054\u7cfb\uff1a\u4e00\u81f4\u6027\u89c4\u5212\u4e0e\u8d85\u5c5e\u6027\u6a21\u578b\u68c0\u6d4b\uff0c\u8bc1\u660e\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u89c4\u5212\u9886\u57df\uff08\u4e00\u81f4\u6027\u89c4\u5212\uff09\u4e0e\u9a8c\u8bc1\u9886\u57df\uff08\u8d85\u5c5e\u6027\u6a21\u578b\u68c0\u6d4b\uff09\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u8fd9\u4e24\u4e2a\u95ee\u9898\u5728\u5404\u81ea\u9886\u57df\u90fd\u5f88\u91cd\u8981\u4f46\u901a\u5e38\u88ab\u5206\u5f00\u7814\u7a76\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5c06\u8d85\u5c5e\u6027\u6a21\u578b\u68c0\u6d4b\u5b9e\u4f8b\u9ad8\u6548\u5730\u7ea6\u7b80\u4e3a\u4e00\u81f4\u6027\u89c4\u5212\u5b9e\u4f8b\uff0c\u5e76\u8bc1\u660e\u7ea6\u7b80\u7684\u6b63\u786e\u6027\u548c\u5b8c\u5907\u6027\uff1b2\uff09\u8bc1\u660e\u6bcf\u4e2a\u4e00\u81f4\u6027\u89c4\u5212\u95ee\u9898\u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u8d85\u5c5e\u6027\u6a21\u578b\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u8d85\u5c5e\u6027\u6a21\u578b\u68c0\u6d4b\u95ee\u9898\u53ef\u4ee5\u6709\u6548\u5730\u8f6c\u5316\u4e3a\u4e00\u81f4\u6027\u89c4\u5212\u95ee\u9898\uff1b2\uff09\u4e00\u81f4\u6027\u89c4\u5212\u95ee\u9898\u672c\u8d28\u4e0a\u5c31\u662f\u8d85\u5c5e\u6027\u6a21\u578b\u68c0\u6d4b\u4efb\u52a1\uff0c\u5efa\u7acb\u4e86\u4e24\u4e2a\u95ee\u9898\u4e4b\u95f4\u7684\u53cc\u5411\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u7ed3\u8bba\u662f\u89c4\u5212\u4e0e\u9a8c\u8bc1\u9886\u57df\u7684\u8fd9\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u8fd9\u79cd\u5bf9\u5e94\u5173\u7cfb\u4e3a\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u4ea4\u53c9\u7814\u7a76\u548c\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.23328", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23328", "abs": "https://arxiv.org/abs/2512.23328", "authors": ["Huan-ang Gao", "Zikang Zhang", "Tianwei Luo", "Kaisen Yang", "Xinzhe Juan", "Jiahao Qiu", "Tianxing Chen", "Bingxiang He", "Hao Zhao", "Hao Zhou", "Shilong Liu", "Mengdi Wang"], "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations", "comment": "Webpage: https://cubebench.c7w.tech/", "summary": "Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCubeBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u5b8c\u5168\u5931\u8d25", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u6570\u5b57\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u6311\u6218\u662f\u5f62\u6210\u548c\u7ef4\u62a4\u7a33\u5065\u7684\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u3002\u7814\u7a76\u8005\u8bc6\u522b\u51fa\u4e09\u4e2a\u963b\u788d\u8fd9\u4e00\u8f6c\u53d8\u7684\u6838\u5fc3\u8ba4\u77e5\u6311\u6218\uff1a\u7a7a\u95f4\u63a8\u7406\u3001\u901a\u8fc7\u5fc3\u7406\u6a21\u62df\u8fdb\u884c\u957f\u65f6\u7a0b\u72b6\u6001\u8ddf\u8e2a\uff0c\u4ee5\u53ca\u5728\u90e8\u5206\u89c2\u5bdf\u4e0b\u7684\u4e3b\u52a8\u63a2\u7d22\u3002", "method": "\u5f15\u5165CubeBench\uff0c\u4e00\u4e2a\u4ee5\u9b54\u65b9\u4e3a\u4e2d\u5fc3\u7684\u65b0\u578b\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u3002\u91c7\u7528\u4e09\u5c42\u8bca\u65ad\u6846\u67b6\u9010\u6b65\u8bc4\u4f30\u667a\u80fd\u4f53\u80fd\u529b\uff1a\u4ece\u5177\u6709\u5b8c\u6574\u7b26\u53f7\u4fe1\u606f\u7684\u57fa\u7840\u72b6\u6001\u8ddf\u8e2a\uff0c\u5230\u4ec5\u5177\u6709\u90e8\u5206\u89c6\u89c9\u6570\u636e\u7684\u4e3b\u52a8\u63a2\u7d22\u3002\u901a\u8fc7\u5206\u6790\u9886\u5148LLM\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8bbe\u8ba1\u8bca\u65ad\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u5916\u90e8\u6c42\u89e3\u5de5\u5177\u6765\u9694\u79bb\u8ba4\u77e5\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u9886\u5148LLM\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\uff0c\u5728\u6240\u6709\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u7684\u901a\u8fc7\u7387\u5747\u4e3a0.00%\uff0c\u66b4\u9732\u4e86\u957f\u671f\u89c4\u5212\u7684\u6839\u672c\u6027\u5931\u8d25\u3002\u901a\u8fc7\u5206\u6790\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u7269\u7406\u57fa\u7840\u6027\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "CubeBench\u57fa\u51c6\u6d4b\u8bd5\u6210\u529f\u63ed\u793a\u4e86LLM\u667a\u80fd\u4f53\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u6838\u5fc3\u8ba4\u77e5\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u6839\u672c\u6027\u7f3a\u9677\u3002\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u7269\u7406\u57fa\u7840\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u8bca\u65ad\u6846\u67b6\u548c\u6307\u5bfc\u65b9\u5411\u3002"}}
{"id": "2512.23412", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23412", "abs": "https://arxiv.org/abs/2512.23412", "authors": ["Jiawei Chen", "Xintian Shen", "Lihao Zheng", "Zhenwei Shao", "Hongyuan Zhang", "Pengfei Yu", "Xudong Rao", "Ning Mao", "Xiaobo Liu", "Lian Wen", "Chaoqun Du", "Feng Gu", "Wei He", "Qizhen Li", "Shanshan Li", "Zide Liu", "Jing Luo", "Lifu Mu", "Xuhao Pan", "Chang Ren", "Haoyi Sun", "Qian Wang", "Wei Wang", "Hongfu Yang", "Jiqing Zhan", "Chunpeng Zhou", "Zheng Zhou", "Hao Ma", "Tao Wei", "Pan Zhou", "Wei Chen"], "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning", "comment": "Technique Report", "summary": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.", "AI": {"tldr": "MindWatcher\u662f\u4e00\u4e2a\u96c6\u6210\u4ea4\u66ff\u601d\u8003\u548c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u8c03\u7528\u591a\u6837\u5316\u5de5\u5177\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u63d0\u793a\u6216\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u7684\u667a\u80fd\u4f53\u5728\u89e3\u51b3\u9700\u8981\u5de5\u5177\u8c03\u7528\u7684\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u6709\u9650\u7684\u667a\u80fd\u6027\u3002\u5de5\u5177\u96c6\u6210\u63a8\u7406\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\uff0c\u6b63\u5728\u6210\u4e3a\u5904\u7406\u6d89\u53ca\u4e0e\u5916\u90e8\u73af\u5883\u591a\u6b65\u4ea4\u4e92\u7684\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u7684\u6709\u529b\u65b9\u6cd5\u3002", "method": "MindWatcher\u91c7\u7528\u4ea4\u66ff\u601d\u8003\u8303\u5f0f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u4efb\u4f55\u4e2d\u95f4\u9636\u6bb5\u5728\u601d\u8003\u548c\u5de5\u5177\u8c03\u7528\u4e4b\u95f4\u5207\u6362\uff0c\u540c\u65f6\u5177\u5907\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u64cd\u4f5c\u56fe\u50cf\u4ee5\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u641c\u7d22\u7ed3\u679c\u3002\u7cfb\u7edf\u914d\u5907\u5168\u9762\u7684\u8f85\u52a9\u63a8\u7406\u5de5\u5177\u5957\u4ef6\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u672c\u5730\u56fe\u50cf\u68c0\u7d22\u6570\u636e\u5e93\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMindWatcher\u901a\u8fc7\u5353\u8d8a\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff0c\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86\u66f4\u5927\u6216\u66f4\u65b0\u6a21\u578b\u7684\u8868\u73b0\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u89c1\u89e3\uff0c\u5982\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9057\u4f20\u7ee7\u627f\u73b0\u8c61\u3002", "conclusion": "MindWatcher\u4f5c\u4e3a\u4e00\u4e2a\u5de5\u5177\u96c6\u6210\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u4ea4\u66ff\u601d\u8003\u548c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u7ed3\u5408\uff0c\u5728\u89e3\u51b3\u5e7f\u6cdb\u9886\u57df\u7684\u591a\u6a21\u6001\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\u548c\u786c\u4ef6\u5229\u7528\u7387\u3002"}}
{"id": "2512.23457", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23457", "abs": "https://arxiv.org/abs/2512.23457", "authors": ["Kongcheng Zhang", "Qi Yao", "Shunyu Liu", "Wenjian Zhang", "Min Cen", "Yang Zhou", "Wenkai Fang", "Yiru Zhao", "Baisheng Lai", "Mingli Song"], "title": "Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following", "comment": null, "summary": "Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.", "AI": {"tldr": "\u63d0\u51faHiR\u6846\u67b6\uff0c\u901a\u8fc7\"\u9009\u62e9-\u91cd\u5199\"\u7b56\u7565\u5c06\u5931\u8d25\u5c1d\u8bd5\u91cd\u653e\u4e3a\u6210\u529f\u6837\u672c\uff0c\u89e3\u51b3\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u9762\u4e34\u5956\u52b1\u7a00\u758f\u95ee\u9898\uff1a\u521d\u59cb\u6a21\u578b\u96be\u4ee5\u751f\u6210\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u7684\u9ad8\u8d28\u91cf\u54cd\u5e94\uff0c\u5bfc\u81f4\u5b66\u4e60\u56f0\u96be\u3002", "method": "\u63d0\u51faHindsight instruction Replay (HiR)\u6846\u67b6\uff0c\u91c7\u7528\u9009\u62e9-\u91cd\u5199\u7b56\u7565\uff0c\u5c06\u5931\u8d25\u5c1d\u8bd5\u4e2d\u5df2\u6ee1\u8db3\u7ea6\u675f\u7684\u90e8\u5206\u91cd\u653e\u4e3a\u6210\u529f\u6837\u672c\uff0c\u7ed3\u5408\u539f\u59cb\u6837\u672c\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u7406\u8bba\u5316\u4e3a\u6307\u4ee4\u7ea7\u548c\u54cd\u5e94\u7ea7\u7684\u53cc\u91cd\u504f\u597d\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "conclusion": "HiR\u662f\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u653e\u5931\u8d25\u5c1d\u8bd5\u4e3a\u6210\u529f\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2512.23601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23601", "abs": "https://arxiv.org/abs/2512.23601", "authors": ["Manh Hung Nguyen", "Adish Singla"], "title": "Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation", "comment": "Preprint", "summary": "Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.", "AI": {"tldr": "CreativeDC\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u521b\u9020\u6027\u63a2\u7d22\u548c\u7ea6\u675f\u6ee1\u8db3\uff0c\u89e3\u51b3LLM\u751f\u6210\u6559\u80b2\u95ee\u9898\u65f6\u5b58\u5728\u7684\"\u4eba\u5de5\u8702\u7fa4\u601d\u7ef4\"\u6548\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u95ee\u9898\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u95ee\u9898\u751f\u6210\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b58\u5728\"\u4eba\u5de5\u8702\u7fa4\u601d\u7ef4\"\u6548\u5e94\uff0c\u5bfc\u81f4\u540c\u4e00\u6a21\u578b\u5185\u548c\u4e0d\u540c\u6a21\u578b\u95f4\u8f93\u51fa\u8fc7\u4e8e\u76f8\u4f3c\uff0c\u5b66\u751f\u63a5\u89e6\u5230\u7684LLM\u751f\u6210\u95ee\u9898\u91cd\u590d\u6027\u9ad8\uff0c\u635f\u5bb3\u601d\u7ef4\u591a\u6837\u6027\u3002", "method": "\u57fa\u4e8eWallas\u521b\u9020\u529b\u7406\u8bba\u548cGuilford\u53d1\u6563-\u6536\u655b\u601d\u7ef4\u6846\u67b6\uff0c\u63d0\u51faCreativeDC\u4e24\u9636\u6bb5\u63d0\u793a\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u521b\u9020\u6027\u63a2\u7d22\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u7ea6\u675f\u6ee1\u8db3\uff0c\u5c06LLM\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u5206\u89e3\u4e3a\u4e0d\u540c\u9636\u6bb5\u3002", "result": "CreativeDC\u5728\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u5b9e\u7528\u6027\u7efc\u5408\u8bc4\u4f30\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u66f4\u591a\u6709\u6548\u4e14\u72ec\u7279\u7684\u95ee\u9898\uff0c\u968f\u7740\u91c7\u6837\u6570\u91cf\u589e\u52a0\uff0c\u5176\u751f\u6210\u7684\u4e0d\u540c\u95ee\u9898\u6570\u91cf\u589e\u957f\u66f4\u5feb\u3002", "conclusion": "CreativeDC\u901a\u8fc7\u7ed3\u6784\u5316\u4e24\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3LLM\u7684\"\u4eba\u5de5\u8702\u7fa4\u601d\u7ef4\"\u95ee\u9898\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u751f\u6210\u591a\u6837\u5316\u3001\u65b0\u9896\u4e14\u5b9e\u7528\u7684\u5b66\u4e60\u6750\u6599\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23626", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23626", "abs": "https://arxiv.org/abs/2512.23626", "authors": ["Federico Baldo", "Charles K. Assaad"], "title": "Regret-Based Federated Causal Discovery with Unknown Interventions", "comment": null, "summary": "Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\\mathbf\u03a6$-Markov Equivalence Class, represented by the $\\mathbf\u03a6$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.", "AI": {"tldr": "\u63d0\u51faI-PERI\u7b97\u6cd5\u89e3\u51b3\u8054\u90a6\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u5ba2\u6237\u7aef\u5f02\u8d28\u5e72\u9884\u95ee\u9898\uff0c\u901a\u8fc7\u6062\u590d\u5ba2\u6237\u7aef\u56fe\u5e76\u96c6\u7684CPDAG\uff0c\u5229\u7528\u5e72\u9884\u8bf1\u5bfc\u7684\u7ed3\u6784\u5dee\u5f02\u5b9a\u5411\u8fb9\uff0c\u83b7\u5f97\u66f4\u7d27\u7684\u03a6-Markov\u7b49\u4ef7\u7c7b", "motivation": "\u73b0\u6709\u8054\u90a6\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6240\u6709\u5ba2\u6237\u7aef\u5171\u4eab\u76f8\u540c\u56e0\u679c\u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u533b\u9662\u95f4\uff09\uff0c\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u7b56\u7565\u6216\u534f\u8bae\u4f1a\u5f15\u5165\u5f02\u8d28\u4e14\u672a\u77e5\u7684\u5e72\u9884\uff0c\u8fd9\u79cd\u7406\u60f3\u5316\u5047\u8bbe\u4e0d\u73b0\u5b9e", "method": "\u63d0\u51faI-PERI\u7b97\u6cd5\uff1a1) \u6062\u590d\u5ba2\u6237\u7aef\u56fe\u5e76\u96c6\u7684CPDAG\uff1b2) \u5229\u7528\u5e72\u9884\u5728\u4e0d\u540c\u5ba2\u6237\u7aef\u8bf1\u5bfc\u7684\u7ed3\u6784\u5dee\u5f02\u5b9a\u5411\u8fb9\uff1b3) \u83b7\u5f97\u66f4\u7d27\u7684\u03a6-Markov\u7b49\u4ef7\u7c7b\uff0c\u7528\u03a6-CPDAG\u8868\u793a", "result": "\u63d0\u4f9b\u4e86I-PERI\u7b97\u6cd5\u7684\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u548c\u9690\u79c1\u4fdd\u62a4\u7279\u6027\u8bc1\u660e\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "I-PERI\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u8054\u90a6\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u5ba2\u6237\u7aef\u5f02\u8d28\u5e72\u9884\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u5e72\u9884\u8bf1\u5bfc\u7684\u7ed3\u6784\u5dee\u5f02\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.23676", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23676", "abs": "https://arxiv.org/abs/2512.23676", "authors": ["Jichen Feng", "Yifan Zhang", "Chenggong Zhang", "Yifu Lu", "Shilong Liu", "Mengdi Wang"], "title": "Web World Models", "comment": "Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models", "summary": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.", "AI": {"tldr": "Web World Model (WWM) \u662f\u4e00\u79cd\u7ed3\u5408\u4f20\u7edfWeb\u6846\u67b6\u53ef\u9760\u6027\u548c\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u7075\u6d3b\u6027\u7684\u4e2d\u95f4\u65b9\u6848\uff0c\u901a\u8fc7Web\u4ee3\u7801\u5b9e\u73b0\u4e16\u754c\u72b6\u6001\u548c\"\u7269\u7406\u89c4\u5219\"\uff0cLLM\u751f\u6210\u4e0a\u4e0b\u6587\u548c\u51b3\u7b56\uff0c\u6784\u5efa\u53ef\u63a7\u4e14\u5f00\u653e\u7684\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u6781\u5206\u5316\uff1a\u4f20\u7edfWeb\u6846\u67b6\u63d0\u4f9b\u53ef\u9760\u4f46\u56fa\u5b9a\u7684\u73af\u5883\uff0c\u800c\u5b8c\u5168\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u8ffd\u6c42\u65e0\u9650\u73af\u5883\u4f46\u727a\u7272\u4e86\u53ef\u63a7\u6027\u548c\u5de5\u7a0b\u5b9e\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u903b\u8f91\u4e00\u81f4\u6027\u53c8\u80fd\u652f\u6301\u5f00\u653e\u63a2\u7d22\u7684\u4e2d\u95f4\u65b9\u6848\u3002", "method": "\u63d0\u51faWeb World Model (WWM)\uff0c\u5c06\u4e16\u754c\u72b6\u6001\u548c\"\u7269\u7406\u89c4\u5219\"\u7528\u666e\u901aWeb\u4ee3\u7801\u5b9e\u73b0\u4ee5\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6f5c\u5728\u72b6\u6001\u4e0a\u751f\u6210\u4e0a\u4e0b\u6587\u3001\u53d9\u4e8b\u548c\u9ad8\u5c42\u51b3\u7b56\u3002\u6784\u5efa\u4e86\u57fa\u4e8e\u771f\u5b9eWeb\u6280\u672f\u6808\u7684WWM\u5957\u4ef6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u591a\u79cdWWM\u7cfb\u7edf\uff1a\u57fa\u4e8e\u771f\u5b9e\u5730\u7406\u7684\u65e0\u9650\u65c5\u884c\u5730\u56fe\u3001\u865a\u6784\u661f\u7cfb\u63a2\u7d22\u5668\u3001Web\u89c4\u6a21\u7684\u767e\u79d1\u5168\u4e66\u548c\u53d9\u4e8b\u4e16\u754c\u3001\u6a21\u62df\u548c\u6e38\u620f\u73af\u5883\u3002\u63d0\u51fa\u4e86WWM\u7684\u5b9e\u7528\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "Web\u6280\u672f\u6808\u672c\u8eab\u53ef\u4ee5\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u53ef\u6269\u5c55\u57fa\u7840\uff0c\u5b9e\u73b0\u53ef\u63a7\u4e14\u5f00\u653e\u7684\u73af\u5883\u3002WWM\u4e3a\u8bed\u8a00\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65e2\u80fd\u4fdd\u8bc1\u903b\u8f91\u4e00\u81f4\u6027\u53c8\u80fd\u652f\u6301\u521b\u9020\u6027\u63a2\u7d22\u7684\u6301\u4e45\u4e16\u754c\u3002"}}
