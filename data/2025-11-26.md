<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 13]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution](https://arxiv.org/abs/2511.19445)
*Luca Accorsi,Demetrio Laganà,Federico Michelotto,Roberto Musmanno,Daniele Vigo*

Main category: cs.DC

TL;DR: 提出了FILO2^x并行共享内存方案，用于协同优化带容量车辆路径问题，无需显式分解且同步开销最小。这是FILO2算法的单轨迹并行适配版本，通过并发异步优化多个可能不相关的解区域来提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用可用计算资源，在保持最终解质量的同时显著减少求解时间，特别是对于从数百到数十万客户的大规模实例。

Method: 设计FILO2^x作为FILO2算法的并行版本，采用基于迭代的并行性，多个求解器同时优化同一底层解，利用FILO2优化应用的局部性并发异步优化多个解区域。

Result: 计算结果表明，FILO2^x相比原始方法能显著提升求解时间，同时保持相似的最终解质量，适用于从数百到数十万客户的各种规模实例。

Conclusion: FILO2^x通过更好地利用计算资源，在保持解质量的同时大大提高了带容量车辆路径问题的求解效率，证明了并行化方法的有效性。

Abstract: We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.

</details>


### [2] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: PSAP是一个预测性分片分配协议，通过动态智能分配账户和交易到分片来解决区块链分片中的工作负载倾斜问题，结合时间工作负载预测和安全约束强化学习，实现多块预测和自适应分片重配置。


<details>
  <summary>Details</summary>
Motivation: 静态或启发式分片分配导致工作负载倾斜、拥塞和跨分片通信过多，削弱了分片的可扩展性优势。

Method: 集成时间工作负载预测模型和安全约束强化学习控制器，通过同步量化运行时和安全门实现确定性推理，限制权益集中度、迁移gas和利用率阈值。

Result: 在以太坊、NEAR和Hyperledger Fabric等异构数据集上评估，相比现有动态分片基线，吞吐量提升2倍，延迟降低35%，跨分片开销减少20%。

Conclusion: 预测性、确定性和安全感知的分片分配是下一代可扩展区块链系统的有前景方向。

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [3] [AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles](https://arxiv.org/abs/2511.19453)
*Yuxin Wang,Yuankai He,Weisong Shi*

Main category: cs.DC

TL;DR: AVS是一个为自动驾驶车辆设计的存储系统，通过分层布局、模态感知压缩和热冷数据分层，实现了高效的数据存储和检索，在嵌入式硬件上验证了实时数据摄取和快速选择性检索能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆产生海量异构数据（如每天14TB），但现有数据记录器和存储系统无法提供高效的数据存储和检索功能，需要支持第三方应用程序的通用可查询车载存储系统。

Method: 采用计算与分层布局协同设计：模态感知的降维和压缩、热冷数据分层与每日归档、轻量级元数据层索引，在嵌入式硬件上使用真实L4自动驾驶轨迹进行验证。

Result: 原型系统在适度资源预算下实现了可预测的实时数据摄取、快速选择性检索和显著的空间占用减少。

Conclusion: 该工作为将存储作为自动驾驶堆栈中的一等组件提供了观察和下一步方向，推动更可扩展和长期部署的存储解决方案。

Abstract: Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.

</details>


### [4] [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)
*Anton Reinhard,Simeon Ehrig,René Widera,Michael Bussmann,Uwe Hernandez Acosta*

Main category: cs.DC

TL;DR: 本文提出了一个Julia软件框架，能够自动动态生成静态调度和编译代码，通过扩展DAG调度理论并添加领域特定信息，实现传统方法无法实现的优化。


<details>
  <summary>Details</summary>
Motivation: 科学计算中的复杂问题通常包含具有不同计算需求的子任务，为了达到最优效率，需要分析每个子任务并将其调度到最适合的硬件上，同时考虑并行性、任务依赖性和设备间数据传输速度。

Method: 使用有向无环图表示计算问题，开发Julia软件框架自动动态生成静态调度和编译代码，扩展DAG调度理论并添加领域特定计算信息。

Result: 实现了一个示例应用：计算量子电动力学中多外部粒子散射过程的矩阵元素，展示了该框架的实际应用效果。

Conclusion: 通过结合理论扩展和领域特定信息，该框架能够在DAG调度中实现传统方法无法达到的优化水平，为复杂科学计算问题提供高效的解决方案。

Abstract: Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.

</details>


### [5] [Systemic approach for modeling a generic smart grid](https://arxiv.org/abs/2511.19460)
*Sofiane Ben Amor,Guillaume Guerard,Loup-Noé Levy*

Main category: cs.DC

TL;DR: 本文提出了一个智能电网的骨干模型，用于测试电网的替代方案，通过分布式优化子系统实现生产和消费调度，同时保持灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 智能电网技术进步带来了复杂的跨学科建模问题，传统计算方法难以解决，需要系统性的集成建模方法来模拟电力系统、能源市场、需求侧管理等资源。

Method: 采用分布式优化子系统的方法，构建智能电网骨干模型，模拟不同系统以在人类规模模型之前验证假设。

Result: 开发了一个能够测试电网替代方案的工具，实现了生产和消费调度，同时保持了系统的灵活性和可扩展性。

Conclusion: 该智能电网骨干模型为复杂电网问题提供了有效的仿真解决方案，通过分布式优化实现了系统级的协调调度。

Abstract: Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.

</details>


### [6] [Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna](https://arxiv.org/abs/2511.19463)
*Aldo Canfora,Eleonora Bergamaschi,Riccardo Mioli,Federico Battini,Mirko Degli Esposti,Giorgio Pedrazzi,Chiara Dellacasa*

Main category: cs.DC

TL;DR: 提出一个集成EnergyPlus模拟、高性能计算和开放地理数据集的UBEM管道，用于估算意大利博洛尼亚建筑能耗需求


<details>
  <summary>Details</summary>
Motivation: 城市建筑能源建模在理解和预测城市尺度能耗中起核心作用

Method: 使用博洛尼亚开放数据门户的建筑足迹和高度信息，结合航空LiDAR测量；从区域建筑法规和欧洲TABULA数据库获取建筑材料、隔热特性等非几何属性；在Leonardo超级计算机上运行EnergyPlus模拟

Result: 在30分钟内完成了约25,000栋建筑的模拟计算

Conclusion: 该UBEM管道成功整合了多种数据源和计算资源，实现了大规模城市建筑能耗的高效模拟

Abstract: Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.

</details>


### [7] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: 研究评估了21个参数规模从1B到20B的本地SLM模型在事件分类任务中的表现，发现温度参数对性能影响不大，而参数数量和GPU容量是关键因素。


<details>
  <summary>Details</summary>
Motivation: SOC和CSIRT面临自动化事件分类的压力，但使用云端LLM会带来成本、延迟和机密性风险，因此研究本地执行的SLM是否能满足需求。

Method: 评估了21个不同参数规模的模型，改变温度超参数，测量执行时间和精度，涵盖两种不同架构。

Result: 温度对性能影响很小，而参数数量和GPU容量是决定性因素。

Conclusion: 本地执行的SLM在事件分类任务中具有可行性，关键影响因素是模型参数规模和硬件能力。

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [8] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 该论文探讨了在太空中构建可扩展的机器学习计算系统，利用配备太阳能电池板的卫星群、自由空间光通信和TPU加速器芯片，通过密集编队飞行实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 随着AI计算需求持续增长，需要寻找可持续的能源解决方案。太阳是太阳系最大的能源来源，因此考虑如何让未来AI基础设施最有效地利用太阳能。

Method: 使用配备太阳能阵列的卫星群，通过自由空间光学链路进行星间通信，采用Google TPU加速器芯片。卫星以密集编队飞行（81颗卫星组成1公里半径集群），使用高精度ML模型控制大规模星座。

Result: Trillium TPU经过辐射测试，在相当于5年任务寿命的总电离剂量下无永久性故障，仅出现位翻转错误。发射成本分析显示到2030年代中期LEO发射成本可能降至≤200美元/公斤。

Conclusion: 太空中的可扩展AI计算系统是可行的，利用太阳能和先进通信技术可以满足未来AI计算需求，同时发射成本的下降将进一步提升该方案的可行性。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

</details>


### [9] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: 提出一个在混合高性能计算和云环境中高效运行的联邦学习框架，解决系统异构性、通信开销和资源调度等关键挑战，同时保持模型准确性和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和隐私感知AI系统的需求增长，联邦学习成为有前景的解决方案，允许去中心化模型训练而无需移动原始数据。高性能计算和云基础设施的结合提供了强大计算能力，但也带来了处理异构硬件、通信限制和非均匀数据的新复杂性。

Method: 开发了一个联邦学习框架，专门设计用于在混合HPC和云环境中高效运行，解决系统异构性、通信开销和资源调度等关键挑战。

Result: 在混合测试平台上进行的实验表明，即使在非独立同分布数据分布和不同硬件条件下，系统在可扩展性、容错性和收敛性方面表现出色。

Conclusion: 这些结果突显了联邦学习作为在现代分布式计算环境中构建可扩展AI系统的实用方法的潜力。

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [10] [Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures](https://arxiv.org/abs/2511.19832)
*Aurelio Vivas,Harold Castro*

Main category: cs.DC

TL;DR: nFlows是一个NUMA感知的工作流执行运行时系统，专门为NUMA架构的HPC系统设计，支持数据密集型工作流的建模、裸机执行、模拟和调度算法验证。


<details>
  <summary>Details</summary>
Motivation: 现代HPC系统普遍采用NUMA架构和异构内存区域，但大多数工作流调度策略是为Grid或Cloud环境设计的，缺乏NUMA感知能力，导致数据访问延迟变化大，任务和数据放置复杂。

Method: 开发nFlows系统，支持构建模拟模型并在物理系统上直接执行，研究NUMA对调度的影响，设计NUMA感知算法，分析数据移动行为，识别性能瓶颈，探索内存内工作流执行。

Result: 提出了nFlows系统的设计、实现和验证方法，能够有效处理NUMA架构下的工作流调度挑战。

Conclusion: nFlows填补了NUMA感知工作流调度在HPC系统中的空白，为数据密集型工作流在NUMA架构上的高效执行提供了解决方案。

Abstract: Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.

</details>


### [11] [PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases](https://arxiv.org/abs/2511.19949)
*Qingda Hu,Xinjun Yang,Feifei Li,Junru Li,Ya Lin,Yuqi Zhou,Yicong Zhu,Junwei Zhang,Rongbiao Xie,Ling Zhou,Bin Wu,Wenchao Zhou*

Main category: cs.DC

TL;DR: PolarStore是一个用于云原生关系数据库的压缩共享存储系统，采用软硬件结合的压缩机制，在保持性能的同时显著降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 云原生RDBMS虽然提供了弹性计算资源，但存储成本仍是关键问题。现有压缩方法存在性能开销与灵活性不足的权衡问题。

Method: 采用双层级压缩机制：在PolarCSD硬件中实现存储内压缩，同时在软件层进行轻量级压缩。结合数据库导向优化、硬件改进和压缩感知调度方案。

Result: 已在数千台存储服务器上部署，管理超过100PB数据，压缩比达到3.55，存储成本降低约60%，性能与未压缩集群相当。

Conclusion: PolarStore成功解决了云原生数据库存储成本问题，通过软硬件协同压缩实现了成本效益与性能的平衡。

Abstract: In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.

</details>


### [12] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: MTMC是一个分层框架，通过将优化策略与实现细节解耦来解决GPU内核生成的挑战。它使用强化学习指导轻量级LLMs探索语义优化策略（宏观思考），并利用通用LLMs逐步实现优化提案（微观编码），从而在准确性和运行时间上实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 开发高性能GPU内核对AI和科学计算至关重要，但现有LLM方法面临正确性和效率的冲突限制，因为它们直接生成整个优化后的低级程序，需要在优化策略和实现代码的庞大空间中进行探索。

Method: MTMC采用分层框架：宏观思考使用强化学习指导轻量级LLMs学习最大化硬件利用率的语义优化策略；微观编码利用通用LLMs逐步实现宏观思考的优化提案，避免全内核生成错误。

Result: 在KernelBench上，MTMC在1-2级和3级分别达到近100%和70%的准确率，比最先进的通用和领域微调LLMs高出50%以上，速度比LLMs快达7.3倍，比专家优化的PyTorch Eager内核快2.2倍。在更具挑战性的TritonBench上，达到59.64%准确率和34倍加速。

Conclusion: MTMC通过解耦优化策略和实现细节，有效导航庞大的优化空间和复杂的实现细节，使LLMs能够生成高性能GPU内核，在准确性和运行时间方面均表现出色。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [13] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: Beluga是一种基于CXL技术的新型内存架构，通过CXL交换机让GPU和CPU共享访问大规模内存池，显著降低LLM推理中的内存访问延迟，相比RDMA方案大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM模型规模快速增长和对长上下文推理的需求使得内存成为GPU加速服务系统的关键瓶颈。现有RDMA方案存在高延迟、复杂通信协议和同步开销等问题，而新兴的CXL技术为解决这些问题提供了新机会。

Method: 提出Beluga内存架构，支持GPU和CPU通过CXL交换机以原生load/store语义访问共享的大规模内存池。基于此设计了Beluga-KVCache系统，专门用于管理LLM推理中的大规模KVCache。

Result: Beluga-KVCache在vLLM推理引擎中相比RDMA方案实现了89.6%的首令牌时间减少和7.35倍的吞吐量提升。

Conclusion: Beluga是首个通过CXL交换机让GPU直接访问大规模内存池的系统，为实现GPU对海量内存资源的低延迟共享访问迈出了重要一步。

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [CAMformer: Associative Memory is All You Need](https://arxiv.org/abs/2511.19740)
*Tergel Molom-Ochir,Benjamin F. Morris,Mark Horton,Chiyue Wei,Cong Guo,Brady Taylor,Peter Liu,Shan X. Wang,Deliang Fan,Hai Helen Li,Yiran Chen*

Main category: cs.AR

TL;DR: CAMformer是一种新型加速器，通过将注意力重新解释为关联内存操作，使用电压域二进制注意力内容可寻址存储器(BA-CAM)计算注意力分数，实现恒定时间相似性搜索，显著提升了Transformer的能效和性能。


<details>
  <summary>Details</summary>
Motivation: Transformer由于注意力机制中查询和键之间的密集相似性计算导致二次成本，面临可扩展性挑战。需要解决注意力计算的高计算复杂度和能耗问题。

Method: 采用电压域BA-CAM通过模拟电荷共享计算注意力分数，用物理相似性感知替代数字算术；集成层次化两阶段top-k过滤、流水线执行和高精度上下文化技术。

Result: 在BERT和Vision Transformer工作负载上评估，CAMformer实现了超过10倍的能效提升，高达4倍的吞吐量提升，以及6-8倍的面积减少，同时保持近乎无损的精度。

Conclusion: CAMformer通过将注意力重新定义为关联内存操作，成功解决了Transformer的可扩展性挑战，在保持算法精度的同时实现了显著的架构效率提升。

Abstract: Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.

</details>


### [15] [Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher](https://arxiv.org/abs/2511.19973)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: Pickle预取器是一种可编程的LLC预取器，通过软件定义预取策略来处理不规则内存访问模式，相比传统预取技术显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代高性能架构采用大型末级缓存，但传统基于预测的预取器难以有效处理现代应用中普遍存在的不规则内存访问模式。

Method: 设计可编程的LLC预取器，允许软件通过简单编程接口定义预取策略，无需扩展指令集架构，将硬件预测的逻辑复杂度转化为软件可编程性。

Result: 在gem5全系统模拟中，Pickle预取器在GAPBS广度优先搜索实现上比基线系统提速达1.74倍，与私有缓存预取器结合时比仅使用私有缓存预取器的系统提速1.40倍。

Conclusion: 通过将硬件预测复杂度转化为软件可编程性，Pickle预取器能够有效处理不规则内存访问模式，显著提升系统性能。

Abstract: Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.

</details>


### [16] [R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation](https://arxiv.org/abs/2511.20090)
*Zizhang Luo,Fan Cui,Kexing Zhou,Runlin Guo,Mile Xia,Hongyuan Hou,Yun Lian*

Main category: cs.AR

TL;DR: R3A是一个基于大语言模型的自动RTL程序修复框架，通过随机树搜索和多智能体故障定位方法提高修复可靠性，在RTL-repair数据集上修复了90.6%的bug，比传统方法多修复45%的bug。


<details>
  <summary>Details</summary>
Motivation: 传统自动程序修复方法依赖固定模板，只能处理有限bug；而大语言模型虽然能理解代码语义，但由于随机性和RTL代码及波形的长输入上下文导致结果不可靠。

Method: 提出R3A框架，采用随机树搜索方法控制补丁生成智能体探索验证解决方案，通过启发式函数平衡探索与利用；同时使用多智能体故障定位方法找到故障候选点作为补丁生成的起点。

Result: 在给定时间限制内修复了RTL-repair数据集中90.6%的bug，比传统方法和其他基于LLM的方法多修复45%的bug，平均达到86.7%的pass@5率。

Conclusion: R3A框架显著提高了RTL程序修复的可靠性和效果，证明了基于LLM的方法在硬件设计验证中的潜力。

Abstract: Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探讨了使用可穿戴设备和AI方法预测慢性疼痛和阿片类药物使用障碍患者的疼痛峰值，发现机器学习模型预测准确率较高，但大语言模型在此领域表现有限。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛和阿片类药物使用障碍是相互关联的常见慢性疾病，目前缺乏针对这两种疾病的循证综合治疗方法。可穿戴设备有潜力监测复杂患者信息，但大语言模型在理解疼痛峰值方面的应用尚未探索。

Method: 采用一系列AI方法，包括机器学习和大型语言模型，结合可穿戴设备数据来检查疼痛峰值的临床相关性。

Result: 机器学习模型在预测疼痛峰值方面达到了相对较高的准确率（>0.7），而大语言模型在提供疼痛峰值洞察方面表现有限。

Conclusion: 通过可穿戴设备的实时监测结合先进AI模型，可以促进疼痛峰值的早期检测，支持个性化干预措施，有助于降低阿片类药物复吸风险，改善MOUD依从性，并增强CP和OUD护理的整合。鉴于大语言模型整体表现有限，这些发现强调了开发能够在OUD/CP背景下提供可行见解的大语言模型的必要性。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [18] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: HeaRT是一个基础推理引擎，用于自动化电路设计，实现了>97%的推理准确率和>98%的Pass@1性能，在40个电路基准测试中表现优异，且运行成本仅为最先进基线的0.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统AI驱动的AMS设计自动化算法受限于对高质量数据集的依赖、跨架构可移植性差以及缺乏自适应机制的问题。

Method: 提出了HeaRT基础推理引擎，作为实现智能、自适应、类人设计优化的第一步。

Result: 在40个电路基准测试中，HeaRT实现了>97%的推理准确率和>98%的Pass@1性能，即使电路复杂度增加也能保持性能，运行成本仅为SOTA基线的0.5倍。在尺寸和拓扑设计适应任务中，收敛速度提高了3倍以上，同时保留了先前的设计意图。

Conclusion: HeaRT是一个有效的AMS设计自动化推理引擎，能够显著提高设计效率，同时保持高性能和低运行成本。

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [19] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: FISCAL框架通过生成金融事实核查的合成数据，训练出轻量级验证器MiniCheck-FISCAL，在多个金融数据集上表现优异，甚至超越了一些大型模型。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域的大语言模型存在事实可靠性不足和计算效率低下的问题，需要开发更准确且高效的解决方案。

Method: 提出FISCAL框架生成金融合成数据，并基于此训练MiniCheck-FISCAL轻量级验证器。

Result: MiniCheck-FISCAL在多个金融数据集上表现优异，超越了GPT-3.5 Turbo等同类模型，接近更大规模系统的准确率，在外部数据集上可与GPT-4o和Claude-3.5相媲美。

Conclusion: 领域特定的合成数据与高效微调相结合，可使紧凑模型在金融AI应用中实现最先进的准确性、鲁棒性和可扩展性。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [20] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型(LLMs)在教育评估项目与内容标准对齐方面的应用潜力，通过三个实验验证了LLMs在识别错误对齐项目、选择正确技能标准方面的表现，发现结合候选筛选策略的LLMs可以显著减少人工审核负担。


<details>
  <summary>Details</summary>
Motivation: 传统的人工对齐审核虽然准确但效率低下，特别是在大规模项目库中。研究旨在探索LLMs是否能加速这一过程而不牺牲准确性。

Method: 使用超过12,000个项目-技能对，测试了三种LLM模型(GPT-3.5 Turbo、GPT-4o-mini和GPT-4o)在三个任务上的表现：识别错误对齐项目、从完整标准集中选择正确技能、在分类前缩小候选列表。

Result: GPT-4o-mini在识别对齐状态方面准确率达到83-94%；数学领域表现良好，阅读领域因语义重叠标准较多而表现较差；候选技能预筛选显著改善结果，前五建议中包含正确技能的比例超过95%。

Conclusion: LLMs，特别是结合候选筛选策略时，可以显著减少项目审核的人工负担，同时保持对齐准确性。建议开发结合LLM筛选和人工审核的混合流程，为持续项目验证和教学对齐提供可扩展解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [21] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: 开发了KOM多智能体系统，用于自动化膝骨关节炎的评估、风险预测和治疗处方，在资源有限环境中提升护理效率。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎影响全球6亿多人，个性化干预需要大量医疗资源，在资源有限环境中难以实施。

Method: 开发KOM多智能体系统，自动化KOA评估、风险预测和治疗处方，协助临床医生完成KOA护理路径中的关键任务。

Result: 基准实验显示KOM在影像分析和处方生成方面优于通用大语言模型；三臂模拟研究显示KOM与临床医生协作减少诊断和规划时间38.5%，提升治疗质量。

Conclusion: KOM可促进自动化KOA管理，集成到临床工作流程中有潜力提升护理效率，其模块化架构为其他慢性病AI辅助管理系统开发提供参考。

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [22] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于评估指导的提示优化方法，通过建立系统化的提示评估框架和训练执行无关的评估器，实现了可解释、查询相关的提示优化，在多个数据集和模型上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法主要优化单一静态模板，在复杂动态用户场景中效果有限。现有的查询相关方法依赖不稳定的文本反馈或黑盒奖励模型，提供弱且不可解释的优化信号。更重要的是，提示质量本身缺乏统一、系统的定义。

Method: 首先建立面向性能的系统化提示评估框架，然后开发并微调一个执行无关的评估器，直接从文本预测多维度质量分数。该评估器指导一个指标感知的优化器，以可解释、查询相关的方式诊断失败模式并重写提示。

Result: 评估器在预测提示性能方面达到最高准确率，评估指导的优化在8个数据集和3个骨干模型上持续超越静态模板和查询相关基线方法。

Conclusion: 提出了统一的、基于指标的提示质量视角，证明了评估指导的优化流程能够提供稳定、可解释且模型无关的改进，适用于多样化任务。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [23] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 该论文提出了一种结合ω-正则目标与显式约束的强化学习方法，通过线性规划算法在满足安全约束的同时最大化目标达成概率，并建立了与约束极限平均问题的转换关系。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，表达能力有限且容易导致奖励黑客行为，无法有效表达时间性、条件性或安全关键目标。同时，单一标量性能指标掩盖了安全与性能之间的权衡问题。

Method: 开发基于线性规划的模型强化学习算法，将ω-正则目标与约束分离处理，在极限情况下生成满足约束阈值的同时最大化目标达成概率的策略。

Result: 提出的算法能够产生满足ω-正则约束的策略，同时最大化ω-正则目标的达成概率，并建立了与约束极限平均问题的保最优性转换。

Conclusion: 该方法成功解决了强化学习中安全约束与优化目标的分离处理问题，为具有风险容忍度的场景提供了有效的解决方案框架。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [24] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: MicroSims是一个基于AI的轻量级教育模拟框架，能够快速生成交互式模拟，无需编程知识即可定制，并可在各种数字学习平台中嵌入使用。


<details>
  <summary>Details</summary>
Motivation: 传统教育模拟创建需要大量资源和技术专长，限制了其广泛应用。MicroSims旨在解决成本、技术复杂性和平台依赖性等障碍，促进教育公平。

Method: 采用标准化设计模式支持AI辅助生成，基于iframe架构实现通用嵌入和安全沙盒，提供透明可修改代码支持定制和教学透明度。

Result: 研究表明交互式模拟相比传统教学可将概念理解提升30-40%，MicroSims在保持这些优势的同时降低了使用门槛。

Conclusion: MicroSims框架为全球教育工作者提供了按需创建定制化、课程对齐模拟的能力，为AI驱动的自适应学习系统奠定了基础。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [25] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 该研究将通用自我效能感量表(GSES)应用于10个大型语言模型，在四种条件下测试其模拟自我评估能力。研究发现模型自我评估在不同条件下存在显著差异，但自我评估与实际能力不匹配，且自我效能感较高的模型表现出更拟人化的推理风格。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估主要关注任务准确性，而忽视了自我评估这一可靠智能的关键方面。研究旨在探索LLMs的模拟自我评估能力及其与实际表现的关系。

Method: 研究将10项通用自我效能感量表(GSES)应用于10个LLMs，在四种条件下(无任务、计算推理、社会推理、摘要)进行测试，分析自我评估的稳定性、准确性及其与任务表现的关系。

Result: 模型自我评估在不同条件下差异显著，总体得分低于人类标准。所有模型在计算和社会推理任务上表现完美，但摘要任务表现差异很大。自我评估与实际能力不匹配，后续置信度提示导致适度下调。高自我效能感对应更拟人化的推理风格。

Conclusion: 心理测量提示为LLM沟通行为提供了结构化洞察，但不能提供校准的性能估计。自我评估在LLMs中不能可靠反映实际能力。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [26] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型来评估中间算法步骤，无需复杂训练过程奖励模型，同时使用沙箱执行反馈定位和纠正错误步骤，在减少15%令牌消耗的同时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于树搜索的代码生成方法难以有效评估中间算法步骤，无法定位和及时纠正错误步骤，导致生成错误代码和计算成本增加。

Method: 提出RPM-MCTS方法：1）使用知识检索作为过程奖励模型评估中间步骤；2）在扩展阶段使用相似性过滤去除冗余节点；3）利用沙箱执行反馈定位和纠正错误算法步骤。

Result: 在四个公共代码生成基准测试中，RPM-MCTS优于当前最先进方法，同时实现约15%的令牌消耗减少。使用RPM-MCTS构建的数据对基础模型进行全微调显著提升了其代码能力。

Conclusion: RPM-MCTS通过知识检索和沙箱反馈有效解决了代码生成中中间步骤评估和错误纠正的问题，在性能和效率方面都取得了显著提升。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [27] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出了一种利用知识图谱生成基准数据集的新方法，用于评估LLM输出语义相似性方法，解决了现有基准依赖人工标注、成本高、领域适用性有限等问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM生成文本语义相似性的方法存在缺陷：语义相似性方法可能更关注句法而非语义内容；现有基准依赖主观人工判断导致生成成本高；领域特定应用可用性有限；语义等价定义不清晰。

Method: 利用知识图谱生成语义相似或不相似的自然语言陈述对，其中不相似对分为四种子类型。在四个不同领域（常识、生物医学、金融、生物学）生成基准数据集，并比较传统NLP评分和LLM作为评判者的语义相似性方法。

Result: 研究发现语义变化子类型和基准领域都会影响语义相似性方法的性能，没有哪种方法始终表现最优。结果对使用LLM作为评判者检测文本语义内容具有重要启示。

Conclusion: 提出的基于知识图谱的基准生成方法有效解决了现有基准的局限性，揭示了语义相似性评估方法的性能受多种因素影响，为LLM语义评估提供了新的工具和见解。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [28] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了一个系统级的LLM隐藏故障模式分类法，分析了实际部署中的15种故障模式，并探讨了评估与监控实践的差距，以及构建可靠LLM系统的设计原则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正被快速集成到决策支持工具和自动化工作流中，但其在生产环境中的行为仍未被充分理解，且故障模式与传统机器学习模型有根本性差异。

Method: 提出系统级的故障模式分类法，识别15种隐藏故障模式，分析现有评估方法的局限性，并研究部署挑战如可观测性限制、成本约束和更新引发的回归问题。

Result: 建立了包含多步推理漂移、潜在不一致性、上下文边界退化、错误工具调用、版本漂移和成本驱动性能崩溃等故障模式的分类体系。

Conclusion: 通过将LLM可靠性视为系统工程问题而非纯模型中心问题，为未来评估方法学、AI系统鲁棒性和可靠LLM部署研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [29] [M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19969)
*Weizi Shao,Taolin Zhang,Zijie Zhou,Chen Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.AI

TL;DR: 提出了M³Prune框架，通过多模态多智能体层次通信图剪枝来减少冗余边缘，在保持任务性能的同时显著降低token开销和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统虽然性能优异，但存在显著的token开销和计算成本问题，限制了大规模部署。

Method: M³Prune框架首先对文本和视觉模态进行模态内图稀疏化，识别对任务最关键的关键边缘，然后构建动态通信拓扑进行模态间图稀疏化，最后逐步剪枝冗余边缘获得更高效的层次拓扑。

Result: 在通用和领域特定的mRAG基准测试中，该方法始终优于单智能体和鲁棒多智能体mRAG系统，同时显著减少token消耗。

Conclusion: M³Prune框架通过多模态通信图剪枝实现了任务性能与token开销之间的最佳平衡，为多智能体系统的大规模部署提供了可行解决方案。

Abstract: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.

</details>


### [30] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 团队MSRA_SC在CPDC 2025竞赛中提出统一框架，通过上下文工程和GRPO训练，在API和GPU赛道均取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决常识人物对话中的工具调用稳定性、执行可靠性和角色扮演指导问题，同时通过强化学习优化任务导向对话性能。

Method: 1. 上下文工程：动态工具剪枝和人物剪裁进行输入压缩，结合参数归一化和函数合并等后处理技术；2. GPU赛道采用GRPO训练，用强化学习替代监督微调。

Result: 最终评估中排名：Task 2 API第1名，Task 1 API第2名，Task 3 API和GPU赛道均第3名。

Conclusion: 提出的简单而有效的框架在常识人物对话任务中表现出色，证明了方法的有效性。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [31] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是第一个将导航研究指标与商业可行性进行定量对比的微导航经济测试平台，通过完整的成本-收益分析评估自主配送机器人的经济可行性。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准仅关注任务成功率，忽视了商业部署所需的经济可行性，这对自主配送机器人的商业化部署至关重要。

Method: CostNav建模完整的经济生命周期，包括硬件、训练、能源、维护成本和配送收入，使用行业参数，从缩减规模模拟投影到实际配送场景。

Result: 基线方法实现43.0%的服务水平协议合规率，但不可行：每次运行亏损30.009美元，无盈亏平衡点，99.7%的运行成本来自碰撞引发的维护成本。

Conclusion: CostNav填补了导航研究与商业部署之间的差距，为评估基于规则的导航、模仿学习和成本感知强化学习训练奠定了基础，支持跨导航范式的经济权衡决策。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [32] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW是一个通过构建和精炼经验学习知识库来优化LLM智能体的框架，通过任务评分和行为准则学习洞察，利用状态空间搜索确保鲁棒性，在多个基准测试中显著提升任务精度并减少API调用。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO等模型权重优化的智能体训练方法计算开销大，且生成的策略难以解释、适应或增量改进。为了解决这些问题，研究通过构建和精炼智能体从环境中经验学习的结构化记忆作为替代优化路径。

Method: 引入BREW框架，通过知识库构建和精炼来优化下游任务的智能体。采用有效的方法对智能体记忆进行分区以提高检索和精炼效率，使用任务评分器和行为准则学习洞察，并利用状态空间搜索来应对自然语言中的噪声和非特异性。

Result: 在OSWorld、τ²Bench和SpreadsheetBench等真实世界基准测试中，BREW实现了10-20%的任务精度提升，10-15%的API/工具调用减少，执行时间更快，同时保持与基础模型相当的计算效率。

Conclusion: 与将记忆视为静态上下文的先前工作不同，BREW将知识库确立为模块化和可控的智能体优化基础——一个用于以透明、可解释和可扩展的方式塑造行为的明确杠杆。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [33] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 本文旨在澄清主动推理的概念，将其与自由能原理区分开来，展示了在离散状态空间中实现主动推理的优化问题可以表述为约束散度最小化问题，并可通过标准平均场方法求解，无需依赖期望自由能概念。


<details>
  <summary>Details</summary>
Motivation: 澄清主动推理与自由能原理的关系，提出一种不依赖期望自由能的替代方法来实现主动推理。

Method: 将主动推理的优化问题重新表述为约束散度最小化问题，使用标准平均场方法求解，提出的感知/行动散度准则在建模感知时与变分自由能一致，在建模行动时与期望自由能泛函相差一个熵正则化项。

Result: 成功展示了主动推理可以在不依赖期望自由能概念的情况下实现，提出的方法在离散状态空间中有效。

Conclusion: 主动推理可以与自由能原理分离，通过约束散度最小化框架实现，为理解主动推理提供了新的理论视角。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [34] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse是一个大规模几何-声学对齐数据集，通过物理一致性连接3D几何、物理属性、模态参数和声学信号的因果链，并提出了CLASP对比学习框架进行跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习框架缺乏物理一致性，忽视了物体几何、材料、振动模式和产生声音之间的内在因果关系，需要建立基于物理定律的感知模型。

Method: 引入VibraVerse数据集，包含3D模型的物理属性（密度、杨氏模量、泊松比）和体积几何，计算模态特征频率和特征向量进行冲击声合成；提出CLASP对比学习框架进行跨模态对齐。

Result: 在几何到声音预测、声音引导形状重建和跨模态表示学习等基准任务上，基于VibraVerse训练的模型展现出更高的准确性、可解释性和跨模态泛化能力。

Conclusion: VibraVerse为物理一致性和因果可解释的多模态学习建立了基准，为声音引导的具身感知和物理世界理解提供了基础，数据集将开源。

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [35] [SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.19558)
*Mohammed Talha Alam,Nada Saadi,Fahad Shamshad,Nils Lukas,Karthik Nandakumar,Fahkri Karray,Samuele Poppi*

Main category: cs.CR

TL;DR: 本文研究了文本到图像扩散模型在良性下游微调后安全对齐失效的问题，提出了SPQR基准来评估安全对齐方法在微调后的稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前的安全对齐方法在部署后的良性微调（如LoRA个性化、风格/领域适配器）中经常失效，而真正的安全对齐应该能够抵御这些后部署适配。

Method: 引入SPQR基准（安全-提示遵循-质量-鲁棒性），这是一个单一评分指标，提供标准化和可复现的框架来评估安全对齐扩散模型在良性微调下保持安全、效用和鲁棒性的能力。

Result: 研究发现当前安全方法在良性微调下频繁失效，通过多语言、领域特定和分布外分析以及类别细分，识别了安全对齐在良性微调后失败的情况。

Conclusion: SPQR作为一个简洁而全面的基准，为T2I安全对齐技术提供了有效的评估工具，展示了其在微调后保持安全性的重要性。

Abstract: Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.

</details>


### [36] [Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning](https://arxiv.org/abs/2511.19654)
*Stephen C. Gravereaux,Sheikh Rabiul Islam*

Main category: cs.CR

TL;DR: 本研究评估了LoRA微调的大语言模型在恶意软件分类任务中生成可解释决策和解释的能力，发现中等规模的LoRA模型在保持解释质量的同时，能大幅减少模型大小和训练时间。


<details>
  <summary>Details</summary>
Motivation: 实现可信赖的恶意软件检测，特别是在使用大语言模型时，仍然是一个重大挑战。需要平衡模型性能和资源效率，同时提供可解释的决策过程。

Method: 开发了使用BLEU、ROUGE和语义相似度度量的评估框架，比较了五种LoRA配置和完全微调基线的解释质量。

Result: 完全微调获得最高总体分数，BLEU和ROUGE比LoRA变体提高达10%。但中等规模LoRA模型在两个指标上表现超过完全微调，同时减少约81%模型大小和超过80%训练时间。

Conclusion: LoRA在可解释性和资源效率之间提供了实用的平衡，能够在资源受限环境中部署而不牺牲解释质量，通过提供特征驱动的自然语言解释来增强恶意软件检测系统的透明度、分析师信心和操作可扩展性。

Abstract: This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.

</details>


### [37] [BASICS: Binary Analysis and Stack Integrity Checker System for Buffer Overflow Mitigation](https://arxiv.org/abs/2511.19670)
*Luis Ferreirinha,Iberia Medeiros*

Main category: cs.CR

TL;DR: 本文提出了一种结合模型检测和符号执行的新方法，用于自动检测和修复C程序二进制代码中的缓冲区溢出漏洞，通过构建内存状态空间来验证安全属性，并使用蹦床技术进行自动修复。


<details>
  <summary>Details</summary>
Motivation: C语言在关键基础设施的CPS系统中广泛应用，但容易受到缓冲区溢出等漏洞影响。传统的漏洞发现技术在二进制代码层面存在可扩展性和精确性问题，导致程序保持易受攻击状态。

Method: 结合模型检测和符号执行技术，构建内存状态空间(MemStaCe)，通过LTL定义的安全属性分析控制流图，识别违反安全属性的反例轨迹，并使用蹦床技术进行二进制补丁修复。

Result: 在BASICS工具中实现，使用Juliet C/C++和SARD数据集及实际应用进行评估，检测和修复的准确率和精确率均超过87%，优于CWE Checker工具。

Conclusion: 该方法能够有效检测和修复C程序二进制代码中的缓冲区溢出漏洞，提高了CPS系统的安全性和可靠性。

Abstract: Cyber-Physical Systems have played an essential role in our daily lives, providing critical services such as power and water, whose operability, availability, and reliability must be ensured. The C programming language, prevalent in CPS development, is crucial for system control where reliability is critical. However, it is also commonly susceptible to vulnerabilities, particularly buffer overflows. Traditional vulnerability discovery techniques often struggle with scalability and precision when applied directly to the binary code of C programs, which can thereby keep programs vulnerable. This work introduces a novel approach designed to overcome these limitations by leveraging model checking and concolic execution techniques to automatically verify security properties of a program's stack memory in binary code, trampoline techniques to perform automated repair of the issues, and crash-inducing inputs to verify if they were successfully removed. The approach constructs a Memory State Space - MemStaCe- from the binary program's control flow graph and simulations, provided by concolic execution, of C function calls and loop constructs. The security properties, defined in LTL, model the correct behaviour of functions associated with vulnerabilities and allow the approach to identify vulnerabilities in MemStaCe by analysing counterexample traces that are generated when a security property is violated. These vulnerabilities are then addressed with a trampoline-based binary patching method, and the effectiveness of the patches is checked with crash-inducing inputs extracted during concolic execution. We implemented the approach in the BASICS tool for BO mitigation and evaluated using the Juliet C/C++ and SARD datasets and real applications, achieving an accuracy and precision above 87%, both in detection and correction. Also, we compared it with CWE Checker, outperforming it.

</details>


### [38] [CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation](https://arxiv.org/abs/2511.19711)
*Jinyu Liu,Gang Tan,Kiwan Maeng*

Main category: cs.CR

TL;DR: CrypTorch是一个基于MPC的机器学习编译器，通过解耦近似计算与MPC运行时，提供自动调优功能，显著提升了MPC-based ML的性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的MPC-based ML框架在处理Softmax、GELU等ML操作时使用不同的近似方法，这些近似往往成为性能瓶颈，且难以在现有框架中识别和修复。

Method: 提出CrypTorch编译器，作为PyTorch 2编译器的扩展，解耦近似计算与MPC运行时，提供编程接口便于添加新近似，并自动选择近似以最大化性能和准确性。

Result: CrypTorch的自动调优在不牺牲准确性的情况下提供1.20-1.7倍加速，在允许一定精度损失时提供1.31-1.8倍加速。整个框架相比流行的CrypTen框架带来3.22-8.6倍的端到端加速。

Conclusion: CrypTorch通过解耦近似计算和自动调优，有效解决了MPC-based ML中的近似计算瓶颈问题，显著提升了性能和实用性。

Abstract: Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\times$ end-to-end speedup compared to the popular framework, CrypTen.

</details>


### [39] [Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts](https://arxiv.org/abs/2511.19727)
*Steven Peh*

Main category: cs.CR

TL;DR: 提出Prompt Fencing方法，通过密码学认证和数据架构原则在LLM提示中建立明确安全边界，完全防止提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型仍易受提示注入攻击威胁，这是生产部署中最重大的安全风险。

Method: 使用密码学签名元数据标记提示段，包括信任评级和内容类型，使LLM能区分可信指令和不可信内容。

Result: 在300个测试案例中，将攻击成功率从86.7%降至0%，验证开销仅0.224秒。

Conclusion: 该方法平台无关，可作为现有LLM基础设施上的安全层增量部署，预期未来模型将具备原生围栏意识以获得最佳安全性。

Abstract: Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.

</details>


### [40] [Frequency Bias Matters: Diving into Robust and Generalized Deep Image Forgery Detection](https://arxiv.org/abs/2511.19886)
*Chi Liu,Tianqing Zhu,Wanlei Zhou,Wei Zhao*

Main category: cs.CR

TL;DR: 本文从频率角度分析深度伪造检测器的泛化性和鲁棒性问题，提出频率对齐方法，可同时用于反取证攻击和取证防御。


<details>
  <summary>Details</summary>
Motivation: AI生成的深度伪造图像对数字安全构成挑战，现有检测器在未知GAN模型和噪声样本下的泛化性和鲁棒性不足，且缺乏同时解决这两个问题的通用方法。

Method: 提出两步骤频率对齐方法，消除真实和伪造图像之间的频率差异，该方法既可作为反取证的黑盒攻击，也可作为取证防御手段。

Result: 在涉及12个检测器、8个伪造模型和5个评估指标的多种实验设置中验证了方法的有效性。

Conclusion: 频率偏差是深度伪造检测器泛化和鲁棒性问题的根本原因，频率对齐方法提供了双面解决方案，既能攻击检测器又能提升检测器可靠性。

Abstract: As deep image forgery powered by AI generative models, such as GANs, continues to challenge today's digital world, detecting AI-generated forgeries has become a vital security topic. Generalizability and robustness are two critical concerns of a forgery detector, determining its reliability when facing unknown GANs and noisy samples in an open world. Although many studies focus on improving these two properties, the root causes of these problems have not been fully explored, and it is unclear if there is a connection between them. Moreover, despite recent achievements in addressing these issues from image forensic or anti-forensic aspects, a universal method that can contribute to both sides simultaneously remains practically significant yet unavailable. In this paper, we provide a fundamental explanation of these problems from a frequency perspective. Our analysis reveals that the frequency bias of a DNN forgery detector is a possible cause of generalization and robustness issues. Based on this finding, we propose a two-step frequency alignment method to remove the frequency discrepancy between real and fake images, offering double-sided benefits: it can serve as a strong black-box attack against forgery detectors in the anti-forensic context or, conversely, as a universal defense to improve detector reliability in the forensic context. We also develop corresponding attack and defense implementations and demonstrate their effectiveness, as well as the effect of the frequency alignment method, in various experimental settings involving twelve detectors, eight forgery models, and five metrics.

</details>


### [41] [Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy](https://arxiv.org/abs/2511.20252)
*Gabriel K. Gegenhuber,Philipp É. Frenzel,Maximilian Günther,Johanna Ullrich,Aljosha Judmayer*

Main category: cs.CR

TL;DR: WhatsApp存在严重的电话号码枚举漏洞，攻击者可以每小时探测超过1亿个电话号码而不被阻止，近半数2021年Facebook数据泄露中的电话号码仍在WhatsApp上活跃。


<details>
  <summary>Details</summary>
Motivation: 研究WhatsApp作为全球最大即时通讯平台的安全漏洞，特别是电话号码枚举问题，评估其大规模用户数据泄露的持续风险。

Method: 通过查询WhatsApp服务器验证电话号码注册状态，测试系统的速率限制机制，分析2021年Facebook数据泄露中的电话号码在WhatsApp上的活跃情况。

Result: 发现WhatsApp存在严重枚举漏洞，每小时可探测超过1亿个电话号码；近半数泄露电话号码仍在活跃；发现X25519密钥跨设备重复使用问题。

Conclusion: WhatsApp的电话号码枚举漏洞持续存在且严重，尽管端到端加密保护消息内容，但用户元数据仍面临泄露风险，需要通过改进速率限制等措施加强防护。

Abstract: WhatsApp, with 3.5 billion active accounts as of early 2025, is the world's largest instant messaging platform. Given its massive user base, WhatsApp plays a critical role in global communication.
  To initiate conversations, users must first discover whether their contacts are registered on the platform. This is achieved by querying WhatsApp's servers with mobile phone numbers extracted from the user's address book (if they allowed access). This architecture inherently enables phone number enumeration, as the service must allow legitimate users to query contact availability. While rate limiting is a standard defense against abuse, we revisit the problem and show that WhatsApp remains highly vulnerable to enumeration at scale. In our study, we were able to probe over a hundred million phone numbers per hour without encountering blocking or effective rate limiting.
  Our findings demonstrate not only the persistence but the severity of this vulnerability. We further show that nearly half of the phone numbers disclosed in the 2021 Facebook data leak are still active on WhatsApp, underlining the enduring risks associated with such exposures. Moreover, we were able to perform a census of WhatsApp users, providing a glimpse on the macroscopic insights a large messaging service is able to generate even though the messages themselves are end-to-end encrypted. Using the gathered data, we also discovered the re-use of certain X25519 keys across different devices and phone numbers, indicating either insecure (custom) implementations, or fraudulent activity.
  In this updated version of the paper, we also provide insights into the collaborative remediation process through which we confirmed that the underlying rate-limiting issue had been resolved.

</details>


### [42] [Can LLMs Make (Personalized) Access Control Decisions?](https://arxiv.org/abs/2511.20284)
*Friederike Groschupp,Daniele Lain,Aritra Dhar,Lara Magdalena Lazier,Srdjan Čapkun*

Main category: cs.CR

TL;DR: 该研究利用大语言模型(LLMs)进行动态、上下文感知的访问控制决策，以减轻用户的认知负担。通过用户研究收集了307个自然语言隐私声明和14,682个用户决策，比较了通用和个性化LLMs的决策表现。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性和自动化程度的提高，访问控制决策给用户带来显著认知负担，导致决策质量下降。需要利用LLMs的处理和推理能力来做出符合用户安全偏好的动态决策。

Method: 进行用户研究收集自然语言隐私声明和用户决策数据，比较通用LLM和个性化LLM的决策准确性，并收集用户对1,446个LLM决策的反馈。

Result: LLMs能够很好地反映用户偏好，与大多数用户决策相比达到86%的准确率。个性化系统存在权衡：虽然提供用户特定偏好能提高与个体决策的一致性，但可能违反安全最佳实践。

Conclusion: 讨论了实现实用自然语言访问控制系统的设计和风险考虑，需要在个性化、安全性和实用性之间取得平衡。

Abstract: Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.
  Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.

</details>


### [43] [A Reality Check on SBOM-based Vulnerability Management: An Empirical Study and A Path Forward](https://arxiv.org/abs/2511.20313)
*Li Zhou,Marc Dacier,Charalambos Konstantinou*

Main category: cs.CR

TL;DR: 本研究通过大规模实证分析发现，使用锁定文件和强包管理器可以生成准确的SBOM，但下游漏洞扫描器存在97.5%的误报率，主要原因是标记了不可达代码中的漏洞。函数调用分析可以有效减少63.3%的误报。


<details>
  <summary>Details</summary>
Motivation: 软件物料清单(SBOM)在软件供应链安全中至关重要，但其实际效用受到生成不准确和漏洞扫描应用问题的严重影响。

Method: 对2,414个开源仓库进行大规模实证研究，首先验证使用锁定文件和强包管理器生成准确SBOM的方法，然后分析下游漏洞扫描器的问题，并采用函数调用分析来减少误报。

Result: 使用锁定文件和强包管理器可以生成准确一致的SBOM，但下游漏洞扫描器产生97.5%的误报率，主要原因是标记不可达代码中的漏洞。函数调用分析可以消除63.3%的误报。

Conclusion: 提出了实用的两阶段软件供应链安全方法：首先使用锁定文件和强包管理器生成准确SBOM，然后通过函数调用分析生成可操作、低噪声的漏洞报告，减轻开发者的警报疲劳。

Abstract: The Software Bill of Materials (SBOM) is a critical tool for securing the software supply chain (SSC), but its practical utility is undermined by inaccuracies in both its generation and its application in vulnerability scanning. This paper presents a large-scale empirical study on 2,414 open-source repositories to address these issues from a practical standpoint. First, we demonstrate that using lock files with strong package managers enables the generation of accurate and consistent SBOMs, establishing a reliable foundation for security analysis. Using this high-fidelity foundation, however, we expose a more fundamental flaw in practice: downstream vulnerability scanners produce a staggering 97.5\% false positive rate. We pinpoint the primary cause as the flagging of vulnerabilities within unreachable code. We then demonstrate that function call analysis can effectively prune 63.3\% of these false alarms. Our work validates a practical, two-stage approach for SSC security: first, generate an accurate SBOM using lock files and strong package managers, and second, enrich it with function call analysis to produce actionable, low-noise vulnerability reports that alleviate developers' alert fatigue.

</details>
