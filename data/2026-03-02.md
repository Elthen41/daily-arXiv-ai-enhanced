<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [QoSFlow: Ensuring Service Quality of Distributed Workflows Using Interpretable Sensitivity Models](https://arxiv.org/abs/2602.23598)
*Md Hasanur Rashid,Jesun Firoz,Nathan R. Tallent,Luanzheng Guo,Meng Tang,Dong Dai*

Main category: cs.DC

TL;DR: QoSFlow是一种性能建模方法，通过将工作流执行配置空间划分为具有相似行为的区域，为分布式科学工作流提供QoS保证，相比最佳标准启发式方法性能提升27.38%


<details>
  <summary>Details</summary>
Motivation: 随着分布式科学工作流的重要性日益增加，需要确保服务质量约束（如最小化时间或限制在资源子集上执行），但工作流行为的不可预测性使得提供QoS保证变得困难

Method: 引入QoSFlow性能建模方法，将工作流的执行配置空间划分为具有相似行为的区域，每个区域根据给定的统计敏感性将具有可比执行时间的配置分组，通过分析推理而非穷举测试实现高效的QoS驱动调度

Result: 在三个不同工作流上的评估显示，QoSFlow的执行建议比最佳标准启发式方法性能高出27.38%，经验验证确认QoSFlow的推荐配置在不同QoS约束下始终与实测执行结果匹配

Conclusion: QoSFlow通过配置空间分区方法有效解决了分布式科学工作流的QoS保证问题，实现了基于分析推理的高效调度，显著提升了性能表现

Abstract: With the increasing importance of distributed scientific workflows, there is a critical need to ensure Quality of Service (QoS) constraints, such as minimizing time or limiting execution to resource subsets. However, the unpredictable nature of workflow behavior, even with similar configurations, makes it difficult to provide QoS guarantees. For effective reasoning about QoS scheduling, we introduce QoSFlow, a performance modeling method that partitions a workflow's execution configuration space into regions with similar behavior. Each region groups configurations with comparable execution times according to a given statistical sensitivity, enabling efficient QoS-driven scheduling through analytical reasoning rather than exhaustive testing. Evaluation on three diverse workflows shows that QoSFlow's execution recommendations outperform the best-performing standard heuristic by 27.38%. Empirical validation confirms that QoSFlow's recommended configurations consistently match measured execution outcomes across different QoS constraints.

</details>


### [2] [Green or Fast? Learning to Balance Cold Starts and Idle Carbon in Serverless Computing](https://arxiv.org/abs/2602.23935)
*Bowen Sun,Christos D. Antonopoulos,Evgenia Smirni,Bin Ren,Nikolaos Bellas,Spyros Lalis*

Main category: cs.DC

TL;DR: LACE-RL是一个基于深度强化学习的服务器无服务计算框架，通过动态调整函数实例的保活时长，在减少冷启动延迟和降低碳排放之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 无服务计算简化了云部署，但带来了服务延迟和碳排放管理的新挑战。减少冷启动延迟需要保留热函数实例，而最小化碳排放则倾向于回收空闲资源。这种平衡在时变的电网碳强度和不同工作负载模式下变得复杂，静态保活策略效率低下。

Method: LACE-RL将服务器无服务Pod保留问题建模为序列决策问题，使用深度强化学习动态调整保活时长，联合建模冷启动概率、函数特定的延迟成本和实时碳强度。

Result: 使用华为公共云追踪数据，LACE-RL相比华为静态策略减少了51.69%的冷启动和77.08%的空闲保活碳排放，在延迟-碳排放权衡方面优于最先进的启发式和单目标基线方法，接近Oracle性能。

Conclusion: LACE-RL框架通过动态调整保活策略，有效解决了无服务计算中延迟和碳排放的权衡问题，实现了比静态策略和现有方法更好的性能。

Abstract: Serverless computing simplifies cloud deployment but introduces new challenges in managing service latency and carbon emissions. Reducing cold-start latency requires retaining warm function instances, while minimizing carbon emissions favors reclaiming idle resources. This balance is further complicated by time-varying grid carbon intensity and varying workload patterns, under which static keep-alive policies are inefficient. We present LACE-RL, a latency-aware and carbon-efficient management framework that formulates serverless pod retention as a sequential decision problem. LACE-RL uses deep reinforcement learning to dynamically tune keep-alive durations, jointly modeling cold-start probability, function-specific latency costs, and real-time carbon intensity. Using the Huawei Public Cloud Trace, we show that LACE-RL reduces cold starts by 51.69% and idle keep-alive carbon emissions by 77.08% compared to Huawei's static policy, while achieving better latency-carbon trade-offs than state-of-the-art heuristic and single-objective baselines, approaching Oracle performance.

</details>


### [3] [Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving](https://arxiv.org/abs/2602.24044)
*Ferran Agullo,Joan Oliveras,Chen Wang,Alberto Gutierrez-Torre,Olivier Tardieu,Alaa Youssef,Jordi Torres,Josep Ll. Berral*

Main category: cs.DC

TL;DR: 提出一个数据驱动管道，通过优化适配器放置策略，在分布式LLM服务系统中最大化GPU效率，减少所需GPU数量，同时避免请求饥饿和内存错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型适配器虽然实现了低成本模型专业化，但在分布式服务系统中引入复杂的缓存和调度挑战。现有研究主要关注延迟最小化，而通过吞吐量最大化实现资源效率的研究仍然不足。

Method: 提出包含三个组件的管道：1) 针对LLM-适配器服务定制的数字孪生(DT)；2) 基于DT生成数据训练的蒸馏机器学习模型；3) 利用ML性能估计的贪心放置算法来最大化GPU效率。

Result: 数字孪生实现了高保真度仿真，吞吐量估计误差低于5%，执行速度比完整LLM基准测试快90倍。实验结果表明，该管道显著提高了GPU效率，减少了维持目标工作负载所需的GPU数量。

Conclusion: 该管道不仅提高了GPU效率，还可适应其他目标（如延迟最小化），展示了其在大规模LLM服务基础设施中的多功能性和未来潜力。

Abstract: Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance](https://arxiv.org/abs/2602.23367)
*Shubh Laddha,Lucas Changbencharoen,Win Kuptivej,Surya Shringla,Archana Vaidheeswaran,Yash Bhaskar*

Main category: cs.AI

TL;DR: 本文介绍了首个大规模MCP数据集，包含针对308个MCP服务器中2800个工具的多样化高质量用户查询，解决了现有数据集缺乏真实用户查询模式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MCP服务器数据集和基准测试缺乏真实、类似人类的用户查询，无法反映不同用户如何表达请求，导致泛化能力差和基准测试可靠性被夸大。

Method: 基于MCP Zero数据集开发，为2800个工具生成多样化高质量用户查询，每个工具配对多个独特的用户角色，涵盖从精确任务请求到模糊探索性命令的不同用户意图层次。

Result: 创建了首个大规模MCP数据集，包含针对308个MCP服务器中2800个工具的多样化用户查询，反映了真实世界交互模式的复杂性。

Conclusion: 该数据集填补了MCP服务器工具使用和生态系统评估的关键空白，能够更好地评估工具使用效果和泛化能力。

Abstract: Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.

</details>


### [5] [Causal Identification from Counterfactual Data: Completeness and Bounding Results](https://arxiv.org/abs/2602.23541)
*Arvind Raghavan,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文提出了CTFIDU+算法，用于从任意Layer 3反事实分布中识别反事实查询，证明了该算法的完备性，并建立了非参数设置下精确因果推理的理论极限。


<details>
  <summary>Details</summary>
Motivation: 先前关于反事实识别完备性的研究仅限于观测或干预分布（因果层次结构的Layer 1和2），因为一般认为无法获得Layer 3的反事实分布数据。然而，最近的研究表明某些反事实分布可以通过实验方法直接估计（反事实可实现性），这引出了一个问题：在获得这些Layer 3数据后，哪些额外的反事实量变得可识别？

Method: 开发了CTFIDU+算法，用于从任意Layer 3分布中识别反事实查询，并证明了该算法在此任务上的完备性。基于此，建立了从物理可实现分布中识别反事实的理论极限。

Result: CTFIDU+算法被证明是完备的，能够识别从任意Layer 3分布中可识别的所有反事实查询。研究确定了非参数设置下精确因果推理的基本极限，并针对不可识别的关键反事实类型推导了新的解析边界。

Conclusion: 本文建立了从物理可实现的反事实分布中识别反事实的理论极限，填补了因果推理完备性研究的空白。对于不可识别的反事实量，反事实数据在实践中确实有助于收紧边界，为因果推理提供了新的理论框架和实用工具。

Abstract: Previous work establishing completeness results for $\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\textit{counterfactual realizabilty}$. This leaves open the question of what $\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.

</details>


### [6] [Planning under Distribution Shifts with Causal POMDPs](https://arxiv.org/abs/2602.23545)
*Matteo Ceriscioli,Karthika Mohan*

Main category: cs.AI

TL;DR: 该论文提出了一个理论框架，用于在部分可观测环境下处理分布偏移问题，通过因果POMDP模型将环境变化表示为干预，保持值函数的PWLC特性以确保规划的可处理性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的规划经常面临分布偏移的挑战，环境模型在状态分布或环境动态变化时可能失效，导致先前学习的策略失败。需要一种能够处理环境变化的理论框架。

Method: 使用基于因果知识的部分可观测马尔可夫决策过程（POMDP）构建理论框架，将环境变化表示为对该因果POMDP的干预。该框架能够评估假设变化下的计划，并主动识别环境中的哪些组件发生了变化。同时维护和更新关于潜在状态和底层领域的信念。

Result: 证明了在该增强信念空间中，值函数保持分段线性凸（PWLC）特性。PWLC在分布偏移下的保持具有优势，能够维持基于α向量的POMDP方法的可处理性。

Conclusion: 该框架为部分可观测环境下的分布偏移规划提供了理论基础，通过因果表示和PWLC特性的保持，确保了规划方法在实际环境变化中的有效性和可处理性。

Abstract: In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $α$-vector-based POMDP methods.

</details>


### [7] [Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem](https://arxiv.org/abs/2602.23579)
*Guillem Rodríguez-Corominas,Maria J. Blesa,Christian Blum*

Main category: cs.AI

TL;DR: 提出RL-CMSA方法解决对称单仓库min-max mTSP问题，结合强化学习引导的构造、精确优化和自适应机制，在平衡工作负载方面优于现有混合遗传算法。


<details>
  <summary>Details</summary>
Motivation: 解决多旅行商问题中的min-max变体，目标是平衡各销售员的工作负载，最小化最长路线。现有方法在问题规模和销售员数量增加时表现受限，需要更有效的混合方法。

Method: 提出RL-CMSA方法：1) 使用基于学习到的成对q值的概率聚类构造多样化解；2) 合并路线到紧凑池；3) 求解受限集合覆盖MILP；4) 通过路线间移除、转移和交换操作精化解；5) 用高质量解中的城市对共现更新q值；6) 通过老化和剪枝自适应调整池。

Result: 在随机和TSPLIB实例上的计算结果表明，RL-CMSA能持续找到(接近)最优解，在可比时间限制下优于最先进的混合遗传算法，特别是当实例规模和销售员数量增加时。

Conclusion: RL-CMSA结合精确优化和强化学习引导的构造，平衡了探索和利用，为对称单仓库min-max mTSP提供了有效的解决方案，在求解质量和可扩展性方面表现优异。

Abstract: The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.

</details>


### [8] [SleepLM: Natural-Language Intelligence for Human Sleep](https://arxiv.org/abs/2602.23605)
*Zongzhe Xu,Zitao Shuai,Eideen Mozaffari,Ravi S. Aysola,Rajesh Kumar,Yuzhe Yang*

Main category: cs.AI

TL;DR: SleepLM是一个睡眠-语言基础模型，通过自然语言与多模态多导睡眠图对齐，实现睡眠生理的语言表征，支持零样本和少样本学习、跨模态检索等任务。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的睡眠分析系统局限于封闭标签空间（如预定义阶段或事件），无法描述、查询或泛化到新的睡眠现象，需要将自然语言与多模态睡眠数据对齐。

Method: 1) 引入多级睡眠描述生成流程，创建首个大规模睡眠-文本数据集（10万+小时，1万+个体）；2) 提出统一预训练目标，结合对比对齐、描述生成和信号重建，捕捉生理保真度和跨模态交互。

Result: SleepLM在真实世界睡眠理解任务中优于最先进方法，在零样本/少样本学习、跨模态检索和睡眠描述方面表现优异，并展现出语言引导事件定位、目标洞察生成和零样本泛化到未见任务的能力。

Conclusion: SleepLM通过自然语言与多模态睡眠数据的对齐，实现了更灵活、可解释的睡眠分析，为睡眠研究开辟了新方向，所有代码和数据将开源。

Abstract: We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.

</details>


### [9] [MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs](https://arxiv.org/abs/2602.23632)
*Lun Zhan,Feng Xiong,Huanyong Liu,Feng Zhang,Yuhui Yin*

Main category: cs.AI

TL;DR: MMKG-RDS是一个基于多模态知识图谱的推理数据合成框架，通过细粒度知识提取、可定制路径采样和多维数据质量评分，提升领域模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长尾知识覆盖、有效性验证和可解释性方面存在局限，基于知识图谱的方法在功能性、粒度、可定制性和评估方面仍有不足，需要更灵活的推理数据合成框架。

Method: 提出MMKG-RDS框架，利用多模态知识图谱支持细粒度知识提取、可定制路径采样和多维数据质量评分，构建MMKG-RDS-Bench数据集覆盖5个领域、17种任务类型和14,950个样本。

Result: 在少量合成样本上微调Qwen3模型（0.6B/8B/32B）可将推理准确率提升9.2%，框架生成的数据能挑战现有模型在表格和公式任务上的表现，适用于复杂基准构建。

Conclusion: MMKG-RDS是一个有效的多模态知识图谱驱动的推理数据合成框架，能显著提升模型推理能力，生成具有挑战性的数据，为复杂基准构建提供支持。

Abstract: Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS

</details>


### [10] [AI Must Embrace Specialization via Superhuman Adaptable Intelligence](https://arxiv.org/abs/2602.23643)
*Judah Goldfeder,Philippe Wyder,Yann LeCun,Ravid Shwartz Ziv*

Main category: cs.AI

TL;DR: 该论文批判了当前对通用人工智能（AGI）的定义，认为人类本身并非真正"通用"，提出了"超人适应性智能（SAI）"作为更实用的AI发展方向，强调专业化而非通用性。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域对AGI的定义存在混乱和不一致，各方对AGI的理解各不相同。作者认为现有的AGI概念存在问题，需要重新思考AI的发展方向，特别是"人类能做的一切"这种定义是否合理和有用。

Method: 通过分析现有AGI定义的问题，论证人类并非真正"通用"，提出SAI（超人适应性智能）作为替代概念。SAI定义为能够学习在人类能做的任何重要事情上超越人类，并能填补人类能力空白的智能。

Result: 提出了SAI概念作为AGI的替代框架，认为AI应该追求专业化而非通用性，在专业化中实现超人性能。SAI能够帮助澄清因AGI定义过载而模糊的AI讨论。

Conclusion: AGI是一个有缺陷的概念，AI发展应该专注于专业化而非通用性。SAI提供了更清晰、更实用的框架来指导AI的未来发展，强调在特定领域实现超人性能并填补人类能力空白。

Abstract: Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.

</details>


### [11] [PseudoAct: Leveraging Pseudocode Synthesis for Flexible Planning and Action Control in Large Language Model Agents](https://arxiv.org/abs/2602.23668)
*Yihan,Wen,Xin Chen*

Main category: cs.AI

TL;DR: PseudoAct：通过伪代码合成实现LLM智能体灵活规划和行动控制的新框架，显著提升复杂长时任务性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体（如ReAct）依赖反应式决策范式，在复杂长时任务中面临冗余工具使用、推理不稳定、token消耗高等问题，需要更结构化的规划方法

Method: 利用LLM将任务解决策略表达为代码的能力，合成结构化伪代码计划，将任务分解为子任务并显式编码控制流（顺序、条件、循环、并行等），然后按全局计划执行行动

Result: 在基准数据集上显著优于现有反应式智能体方法，在FEVER上实现20.93%的绝对成功率提升，在HotpotQA上达到新的最先进水平

Conclusion: PseudoAct通过伪代码合成实现显式、时间一致的决策逻辑，减少冗余行动、防止无限循环、避免无信息替代探索，为LLM智能体提供一致高效的长期决策能力

Abstract: Large language model (LLM) agents typically rely on reactive decision-making paradigms such as ReAct, selecting actions conditioned on growing execution histories. While effective for short tasks, these approaches often lead to redundant tool usage, unstable reasoning, and high token consumption in complex long-horizon tasks involving branching, iteration, or multi-tool coordination. To address these limitations, this paper introduces PseudoAct, a novel framework for flexible planning and action control in LLM agents through pseudocode synthesis. Leveraging the ability of LLMs to express task-solving strategies as code, PseudoAct synthesizes a structured pseudocode plan that decomposes a task into subtasks and explicitly encodes control flow, including sequencing, conditionals, loops, parallel composition, and combinations of these logic primitives. Actions are then executed by following this global plan, making the decision logic explicit and temporally coherent. This design reduces redundant actions, prevents infinite loops, and avoids uninformative alternative exploration, enabling consistent and efficient long-horizon decision-making. Experiments on benchmark datasets show that our method significantly outperforms existing reactive agent approaches, achieving a 20.93% absolute gain in success rate on FEVER and setting a new state-of-the-art on HotpotQA.

</details>


### [12] [From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.23701)
*Yawen Wang,Wenjie Wu,Junjie Wang,Qing Wang*

Main category: cs.AI

TL;DR: CHIEF是一个新颖的框架，通过将多智能体系统的混沌轨迹转换为结构化层次因果图，并采用层次化回溯和反事实归因，显著提升了故障归因的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统在复杂领域表现出色，但存在固有的脆弱性和不透明的故障机制。现有的故障归因方法通常将执行日志视为扁平序列，这种线性视角无法解耦MAS中复杂的因果联系，导致可观测性弱和责任边界模糊。

Method: CHIEF框架包含三个核心模块：1) 将混沌轨迹转换为结构化层次因果图；2) 采用层次化oracle引导的回溯，通过合成虚拟oracle高效剪枝搜索空间；3) 实施渐进因果筛选策略进行反事实归因，严格区分真正的根本原因和传播的症状。

Result: 在Who&When基准测试中，CHIEF在智能体级和步骤级准确性方面均优于八个强大的基线方法。消融研究进一步证实了每个提出模块的关键作用。

Conclusion: CHIEF通过结构化层次因果表示和系统化归因方法，显著提升了多智能体系统的故障诊断能力，为解决MAS的脆弱性和不透明性问题提供了有效解决方案。

Abstract: LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.

</details>


### [13] [Unlocking Cognitive Capabilities and Analyzing the Perception-Logic Trade-off](https://arxiv.org/abs/2602.23730)
*Longyin Zhang,Shuo Sun,Yingxu He,Won Cheng Yi Lewis,Muhammad Huzaifah Bin Md Shahrin,Hardik Bhupendra Sailor,Heng Meng Jeremy Wong,Tarun Kumar Vangani,Yi Ma,Qiongqiong Wang,Minh Duc Pham,Ridong Jiang,Jingtao Li,Jingyi Liao,Zhuohan Liu,Yanfeng Lu,Manas Gupta,Ai Ti Aw*

Main category: cs.AI

TL;DR: MERaLiON2-Omni (Alpha) 是一个针对东南亚地区的10B参数多语言全感知模型，通过分离和整合"系统1"（感知）与"系统2"（推理）能力，解决了多模态大语言模型中感知与推理结合的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型追求全感知能力，但将稳健的感官基础与复杂推理相结合仍然具有挑战性，特别是在代表性不足的地区（如东南亚）。需要解决区域特定的视听线索（如新加坡式英语语码转换、当地文化地标）与多语言LLM的整合问题。

Method: 采用渐进式训练流程：1) 通过正交模态适应建立稳健的感知骨干，将区域特定的视听线索与多语言LLM对齐；2) 提出成本效益高的生成-判断-精炼流程，利用超级LLM过滤幻觉并通过共识机制解决冲突，合成高质量银数据，将文本链式思维推理转移到多模态场景。

Result: 在新引入的SEA-Omni基准套件上的评估揭示了效率-稳定性悖论：推理作为抽象任务的非线性放大器（显著提升数学和指令跟随性能），但在低级感官处理中引入不稳定性，具体表现为长上下文音频中的时间漂移和视觉过度解释问题。

Conclusion: 该研究详细介绍了架构、数据高效训练方法，以及对稳健感知与结构化推理之间权衡的诊断分析，为多模态大语言模型在区域特定应用中的发展提供了重要见解。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) pursue omni-perception capabilities, yet integrating robust sensory grounding with complex reasoning remains a challenge, particularly for underrepresented regions. In this report, we introduce the research preview of MERaLiON2-Omni (Alpha), a 10B-parameter multilingual omni-perception tailored for Southeast Asia (SEA). We present a progressive training pipeline that explicitly decouples and then integrates "System 1" (Perception) and "System 2" (Reasoning) capabilities. First, we establish a robust Perception Backbone by aligning region-specific audio-visual cues (e.g., Singlish code-switching, local cultural landmarks) with a multilingual LLM through orthogonal modality adaptation. Second, to inject cognitive capabilities without large-scale supervision, we propose a cost-effective Generate-Judge-Refine pipeline. By utilizing a Super-LLM to filter hallucinations and resolve conflicts via a consensus mechanism, we synthesize high-quality silver data that transfers textual Chain-of-Thought reasoning to multimodal scenarios.
  Comprehensive evaluation on our newly introduced SEA-Omni Benchmark Suite reveals an Efficiency-Stability Paradox: while reasoning acts as a non-linear amplifier for abstract tasks (boosting mathematical and instruction-following performance significantly), it introduces instability in low-level sensory processing. Specifically, we identify Temporal Drift in long-context audio, where extended reasoning desynchronizes the model from acoustic timestamps, and Visual Over-interpretation, where logic overrides pixel-level reality. This report details the architecture, the data-efficient training recipe, and a diagnostic analysis of the trade-offs between robust perception and structured reasoning.

</details>


### [14] [Reasoning-Driven Multimodal LLM for Domain Generalization](https://arxiv.org/abs/2602.23777)
*Zhipeng Xu,Zilong Wang,Xinyang Jiang,Dongsheng Li,De Cheng,Nannan Wang*

Main category: cs.AI

TL;DR: 本文提出RD-MLDG框架，利用多模态大语言模型的推理能力解决领域泛化问题，通过推理链构建实现更鲁棒的域外预测。


<details>
  <summary>Details</summary>
Motivation: 大多数领域泛化方法关注视觉特征不变性，本文探索利用多模态大语言模型的推理能力，通过构建推理链来获得更鲁棒的域外预测性能。

Method: 提出RD-MLDG框架，包含两个组件：1) MTCT（多任务交叉训练），引入直接分类路径指导推理监督；2) SARR（自对齐推理正则化），通过迭代自标注保持推理链的语义丰富性并缓解推理模式不匹配问题。

Result: 在标准DomainBed数据集（PACS、VLCS、OfficeHome、TerraInc）上的实验表明，RD-MLDG达到了最先进的性能。

Conclusion: 推理能力是提升领域泛化性能的有前景的补充信号，RD-MLDG框架通过有效利用推理链实现了更鲁棒的域外泛化。

Abstract: This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.

</details>


### [15] [EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2602.23802)
*Yiyang Fang,Wenke Huang,Pei Fu,Yihao Yang,Kehua Su,Zhenbo Luo,Jian Luan,Mang Ye*

Main category: cs.AI

TL;DR: EMO-R3框架通过结构化情感思维和反思性情感奖励，增强多模态大语言模型的情感推理能力，显著提升解释性和情感智能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉推理方面取得显著进展，但在捕捉人类情感的复杂性和主观性方面仍存在困难。现有监督微调方法泛化能力有限且解释性差，而强化学习方法如GRPO未能与情感认知的内在特性对齐。

Method: 提出EMO-R3框架，包含两个核心组件：1) 结构化情感思维，引导模型以结构化、可解释的方式进行逐步情感推理；2) 反思性情感奖励，使模型能够基于视觉-文本一致性和情感连贯性重新评估其推理过程。

Result: 大量实验表明，EMO-R3显著提升了多模态大语言模型的解释性和情感智能，在多个视觉情感理解基准测试中取得了优越性能。

Conclusion: EMO-R3框架有效解决了多模态大语言模型在情感推理方面的局限性，通过结构化思维和反思性奖励机制，实现了更好的情感理解和解释能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.

</details>


### [16] [Pessimistic Auxiliary Policy for Offline Reinforcement Learning](https://arxiv.org/abs/2602.23974)
*Fan Zhang,Baoru Huang,Xin Zhang*

Main category: cs.AI

TL;DR: 提出一种悲观辅助策略，通过最大化Q函数的下置信界来采样可靠动作，缓解离线强化学习中的误差累积和过高估计问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习从预收集数据集中学习，避免实时交互的风险和低效。但在学习过程中不可避免地访问分布外动作会引入近似误差，导致误差累积和严重的过高估计问题。

Method: 构建一个悲观辅助策略，通过最大化Q函数的下置信界来采样可靠动作。该策略在学习策略附近表现出相对较高的价值和较低的不确定性，避免学习过程采样具有潜在高误差的高价值动作。

Result: 在离线强化学习基准测试上的广泛实验表明，利用悲观辅助策略能有效提高其他离线RL方法的性能。

Conclusion: 悲观辅助策略通过采样可靠动作减少近似误差，从而缓解误差累积问题，为离线强化学习提供了一种有效的改进方法。

Abstract: Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.

</details>


### [17] [Portfolio Reinforcement Learning with Scenario-Context Rollout](https://arxiv.org/abs/2602.24037)
*Vanya Priscillia Bendatu,Yao Lu*

Main category: cs.AI

TL;DR: 提出了一种宏观条件场景上下文展开方法，用于在市场压力事件下生成合理的次日多变量收益场景，并通过反事实状态构建解决强化学习中的奖励-转移不匹配问题，显著提升了投资组合再平衡策略的表现。


<details>
  <summary>Details</summary>
Motivation: 市场机制转变会导致分布偏移，从而降低投资组合再平衡策略的性能。历史数据无法告诉我们如果发生不同情况会怎样，这给生成合理的压力事件场景带来了挑战。

Method: 提出宏观条件场景上下文展开方法，生成压力事件下的次日多变量收益场景。为了解决强化学习中奖励-转移不匹配问题，通过展开隐含的连续性构建反事实下一状态，并增强评论者智能体的引导目标。

Result: 在31个不同的美国股票和ETF投资组合的样本外评估中，该方法相比经典和基于强化学习的投资组合再平衡基线，夏普比率提升高达76%，最大回撤降低高达53%。

Conclusion: 通过解决强化学习中的奖励-转移不匹配问题，提出的方法能够稳定学习过程并提供可行的偏差-方差权衡，显著改善投资组合再平衡策略在压力市场条件下的表现。

Abstract: Market regime shifts induce distribution shifts that can degrade the performance of portfolio rebalancing policies. We propose macro-conditioned scenario-context rollout (SCR) that generates plausible next-day multivariate return scenarios under stress events. However, doing so faces new challenges, as history will never tell what would have happened differently. As a result, incorporating scenario-based rewards from rollouts introduces a reward--transition mismatch in temporal-difference learning, destabilizing RL critic training.
  We analyze this inconsistency and show it leads to a mixed evaluation target. Guided by this analysis, we construct a counterfactual next state using the rollout-implied continuations and augment the critic agent's bootstrap target. Doing so stabilizes the learning and provides a viable bias-variance tradeoff.
  In out-of-sample evaluations across 31 distinct universes of U.S. equity and ETF portfolios, our method improves Sharpe ratio by up to 76% and reduces maximum drawdown by up to 53% compared with classic and RL-based portfolio rebalancing baselines.

</details>


### [18] [CIRCLE: A Framework for Evaluating AI from a Real-World Lens](https://arxiv.org/abs/2602.24055)
*Reva Schwartz,Carina Westling,Morgan Briggs,Marzieh Fadaee,Isar Nejadgholi,Matthew Holmes,Fariza Rashid,Maya Carlyle,Afaf Taïk,Kyra Wilson,Peter Douglas,Theodora Skeadas,Gabriella Waters,Rumman Chowdhury,Thiago Lacerda*

Main category: cs.AI

TL;DR: CIRCLE是一个六阶段生命周期框架，旨在弥合模型中心性能指标与AI实际部署效果之间的现实差距，通过将利益相关者关注转化为可测量信号，提供系统化的证据支持决策。


<details>
  <summary>Details</summary>
Motivation: 现有MLOps框架关注系统稳定性，基准测试衡量抽象能力，但AI技术在实际部署中面临真实用户变异性和约束，决策者缺乏关于AI在现实条件下行为的系统证据。

Method: CIRCLE将TEVV（测试、评估、验证和确认）中的验证阶段操作化，通过六阶段生命周期框架，将堆栈外的利益相关者关注正式转化为可测量信号。整合现场测试、红队测试和纵向研究等方法，形成协调的管道。

Result: CIRCLE产生系统化知识：跨站点可比较但对本地上下文敏感的证据。与参与式设计（通常局限于本地）或算法审计（通常是回顾性）不同，CIRCLE提供了将上下文敏感定性见解与可扩展定量指标连接的结构化前瞻性协议。

Conclusion: CIRCLE框架能够实现基于实际下游效应而非理论能力的治理，为AI部署提供更全面的评估方法，弥合模型性能与实际应用效果之间的差距。

Abstract: This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.

</details>


### [19] [Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction](https://arxiv.org/abs/2602.24080)
*Xiang Li,Jiabao Gao,Sipei Lin,Xuan Zhou,Chi Zhang,Bo Cheng,Jiale Han,Benyou Wang*

Main category: cs.AI

TL;DR: 该研究首次对语音到语音系统进行了图灵测试，发现现有系统均未通过测试，主要瓶颈在于副语言特征、情感表达和对话个性，而非语义理解。


<details>
  <summary>Details</summary>
Motivation: 现代语音到语音系统能否像人类一样对话是一个关键但未解答的问题，需要建立首个针对S2S系统的人类相似性评估框架。

Method: 收集了2,968个人类判断，评估9个最先进的S2S系统与28名人类参与者的对话；开发了包含18个维度的细粒度人类相似性分类法，并对收集的对话进行众包标注。

Result: 所有评估的S2S系统均未通过图灵测试；瓶颈主要在于副语言特征、情感表达和对话个性；现成的AI模型作为图灵测试评判者表现不可靠。

Conclusion: 提出了一个利用细粒度人类相似性评分的可解释模型，能够准确透明地区分人类与机器对话，为自动人类相似性评估提供了强大工具，推动了对话AI系统向人类相似性的改进。

Abstract: The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.

</details>


### [20] [Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance](https://arxiv.org/abs/2602.24110)
*Yanwei Ren,Haotian Zhang,Likang Xiao,Xikai Zhang,Jiaxing Huang,Jiayan Qiu,Baosheng Yu,Quan Chen,Liu Liu*

Main category: cs.AI

TL;DR: SCOPE框架通过过程奖励模型定位推理轨迹中的首个错误步骤，进行细粒度修正，有效利用部分正确的推理路径，提升探索多样性13.5%，在数学推理任务上达到46.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习监督存在严重缺陷：对部分正确但包含几个错误步骤的推理轨迹给予与完全错误轨迹相同的惩罚，导致模型丢弃有价值的推理路径，探索空间过早缩小，多样性下降。

Method: 提出SCOPE框架，利用过程奖励模型精确定位次优推理轨迹中的第一个错误步骤，应用细粒度的、步骤级别的离策略修正，对部分正确的推理轨迹进行精确修正。

Result: SCOPE显著提升了推理轨迹的多样性（提升13.5%），在数学推理任务上达到46.6%的平均准确率，在分布外推理任务上达到53.4%的准确率，建立了新的最先进结果。

Conclusion: 通过精确定位和修正推理轨迹中的首个错误步骤，SCOPE能够有效利用部分正确的推理路径，维持广阔的探索空间，显著提升大型推理模型的复杂推理能力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.

</details>


### [21] [LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics](https://arxiv.org/abs/2602.24173)
*Antoine Peyronnet,Fabian Gloeckle,Amaury Hayat*

Main category: cs.AI

TL;DR: 提出一个基于arXiv最新数学研究论文的可更新基准测试，用于评估大语言模型在数学研究层面的能力，相比传统静态基准更能反映真实研究水平。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准测试主要依赖静态的手工整理题目（如竞赛题或教科书习题），这些无法准确评估LLM在真实数学研究中的能力。需要建立能反映最新数学研究进展的动态评估体系。

Method: 开发自动化流水线：从arXiv提取引理，通过明确所有假设和定义将其重写为自包含的陈述，创建可定期更新的基准测试。旧实例可用于训练而不影响未来评估。

Result: 当前最先进的LLM在定理证明任务上准确率约为10-15%（pass@1），表明LLM要达到人类在数学研究中的证明能力仍有很大差距。

Conclusion: 该可更新基准测试为评估LLM在数学研究能力提供了更真实的评估框架，揭示了当前模型与人类研究水平之间的显著差距，为未来改进指明了方向。

Abstract: We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context.

</details>


### [22] [Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume](https://arxiv.org/abs/2602.24195)
*Gregory Kang Ruey Lau,Hieu Dao,Nicole Kan Hui Lin,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: UMPIRE是一个无需训练的多模态大语言模型不确定性量化框架，通过计算采样响应的语义体积来评估不确定性，支持多种输入输出模态且无需外部工具


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型可能产生看似合理但错误的输出，影响可靠部署。现有不确定性度量方法存在局限性：仅适用于特定模态、依赖外部工具或计算成本高

Method: UMPIRE框架基于模型内部模态特征，计算采样响应的不连贯调整语义体积，同时捕捉样本的全局语义多样性和基于内部模型置信度的局部不连贯性

Result: 在图像、音频和视频-文本基准测试（包括对抗性和分布外设置）中，UMPIRE在错误检测和不确定性校准方面持续优于基线方法，并能泛化到非文本输出任务（如图像和音频生成）

Conclusion: UMPIRE提供了一个高效、无需训练的不确定性量化框架，适用于多种模态，不依赖外部工具，能有效评估多模态大语言模型输出的可靠性

Abstract: Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [23] [FPPS: An FPGA-Based Point Cloud Processing System](https://arxiv.org/abs/2602.23787)
*Xiaofeng Zhou,Linfeng Du,Hanwei Fan,Wei Zhang*

Main category: cs.AR

TL;DR: FPPS：基于FPGA加速的点云处理系统，针对自动驾驶中的ICP算法优化，在KITTI数据集上实现最高35倍加速和8.58倍能效提升


<details>
  <summary>Details</summary>
Motivation: 点云处理是自动驾驶系统的计算瓶颈，特别是在实时应用中，而能效仍然是关键的系统约束。需要为资源受限的嵌入式自动驾驶平台提供低延迟、高能效的解决方案。

Method: 提出了FPPS系统，这是一个基于FPGA加速的点云处理系统，专门针对迭代最近点（ICP）算法进行优化设计。ICP是3D定位和感知流水线中的经典核心算法。

Result: 在广泛使用的KITTI基准数据集上评估，该系统相比最先进的CPU基线实现了最高35倍（运行时加权平均15.95倍）的加速，同时保持等效的配准精度。设计将平均能效提高了8.58倍。

Conclusion: FPPS在性能和能耗之间提供了令人信服的平衡，使其成为资源受限的嵌入式自动驾驶平台的可行解决方案，其中延迟和功耗都是关键的设计优先级。

Abstract: Point cloud processing is a computational bottleneck in autonomous driving systems, especially for real-time applications, while energy efficiency remains a critical system constraint. This work presents FPPS, an FPGA-accelerated point cloud processing system designed to optimize the iterative closest point (ICP) algorithm, a classic cornerstone of 3D localization and perception pipelines. Evaluated on the widely used KITTI benchmark dataset, the proposed system achieves up to 35$\times$ (and a runtime-weighted average of 15.95x) speedup over a state-of-the-art CPU baseline while maintaining equivalent registration accuracy. Notably, the design improves average power efficiency by 8.58x, offering a compelling balance between performance and energy consumption. These results position FPPS as a viable solution for resource-constrained embedded autonomous platforms where both latency and power are key design priorities.

</details>


### [24] [GenDRAM:Hardware-Software Co-Design of General Platform in DRAM](https://arxiv.org/abs/2602.23828)
*Tsung-Han Lu,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: GenDRAM是一个基于单片3D DRAM的大规模并行内存处理加速器，专门针对动态规划算法（如全对最短路径和基因组序列比对）的数据移动瓶颈问题，通过将完整数据密集型工作流集成到单个芯片上，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 动态规划算法（如APSP和基因组序列比对）在传统架构上受限于数据移动瓶颈，而现有的内存处理加速器通常只解决工作流的一部分，导致新的系统级瓶颈（主机-加速器通信和片外数据流）。

Method: 利用单片3D DRAM的巨大容量和内部带宽，将完整数据密集型流水线（如从种子到比对的完整基因组工作流）集成到单个异构芯片上。核心架构包括专门用于内存密集型任务的搜索处理单元和通用的无乘法器计算处理单元，采用3D感知数据映射策略利用M3D DRAM的分层延迟进行性能优化。

Result: 通过全面仿真验证，GenDRAM实现了变革性的性能飞跃，在全对最短路径上比最先进的GPU系统快68倍以上，在端到端基因组流水线上快22倍以上。

Conclusion: GenDRAM通过利用单片3D DRAM的独特特性，成功解决了动态规划算法的数据移动瓶颈问题，为数据密集型计算提供了高效的内存处理加速解决方案。

Abstract: Dynamic programming (DP) algorithms, such as All-Pairs Shortest Path (APSP) and genomic sequence alignment, are fundamental to many scientific domains but are severely bottlenecked by data movement on conventional architectures. While Processing-in-Memory (PIM) offers a promising solution, existing accelerators often address only a fraction of the work-flow, creating new system-level bottlenecks in host-accelerator communication and off-chip data streaming. In this work, we propose GenDRAM, a massively parallel PIM accelerator that overcomes these limitations. GenDRAM leverages the immense capacity and internal bandwidth of monolithic 3D DRAM(M3D DRAM) to integrate entire data-intensive pipelines, such as the full genomics workflow from seeding to alignment, onto a single heterogeneous chip. At its core is a novel architecture featuring specialized Search PUs for memory-intensive tasks and universal, multiplier-less Compute PUs for diverse DP calculations. This is enabled by a 3D-aware data mapping strategy that exploits the tiered latency of M3D DRAM for performance optimization. Through comprehensive simulation, we demonstrate that GenDRAM achieves a transformative performance leap, outperforming state-of-the-art GPU systems by over 68x on APSP and over 22x on the end-to-end genomics pipeline.

</details>


### [25] [LeGend: A Data-Driven Framework for Lemma Generation in Hardware Model Checking](https://arxiv.org/abs/2602.24010)
*Mingkai Miao,Guangyu Hu,Wei Zhang,Hongce Zhang*

Main category: cs.AR

TL;DR: LeGend提出了一种基于全局表示学习的方法来改进IC3/PDR中的归纳泛化步骤，通过预训练领域适应的自监督模型生成门锁嵌入，从而以可忽略的开销预测高质量引理，显著加速形式验证。


<details>
  <summary>Details</summary>
Motivation: IC3/PDR是形式验证中广泛使用的核心引擎，其性能关键取决于归纳泛化步骤。现有基于机器学习的指导方法大多采用逐子句图分析范式，需要重复构建和分析图，导致沉重的开销和可扩展性瓶颈。

Method: LeGend采用一次性全局表示学习范式，预训练一个领域适应的自监督模型来生成捕获全局电路特性的门锁嵌入。这些预计算的嵌入允许一个轻量级模型以可忽略的开销预测高质量引理，有效解耦昂贵的训练和快速推理。

Result: 实验表明，LeGend在多样化的基准测试集上加速了两个最先进的IC3/PDR引擎，展示了在形式验证中扩展应用的前景。

Conclusion: LeGend通过全局表示学习取代逐子句图分析范式，解决了现有机器学习指导方法的可扩展性瓶颈，为形式验证的规模化提供了有前景的路径。

Abstract: Property checking of RTL designs is a central task in formal verification. Among available engines, IC3/PDR is a widely used backbone whose performance critically depends on inductive generalization, the step that generalizes a concrete counterexample-to-induction (CTI) cube into a lemma. Prior work has explored machine learning to guide this step and achieved encouraging results, yet most methods adopt a per-clause graph analysis paradigm: for each clause they repeatedly build and analyze graphs, incurring heavy overhead and creating a scalability bottleneck. We introduce LeGend, which replaces this paradigm with one-time global representation learning. LeGend pre-trains a domain-adapted self-supervised model to produce latch embeddings that capture global circuit properties. These precomputed embeddings allow a lightweight model to predict high-quality lemmas with negligible overhead, effectively decoupling expensive learning from fast inference. Experiments show LeGend accelerates two state-of-the-art IC3/PDR engines across a diverse set of benchmarks, presenting a promising path to scale up formal verification.

</details>


### [26] [Shifting in-DRAM](https://arxiv.org/abs/2602.24269)
*William C. Tegge,Alex K. Jones*

Main category: cs.AR

TL;DR: 提出了一种新的DRAM子阵列设计，通过重新利用和扩展"迁移单元"功能，在开放位线架构中实现DRAM内比特移位操作，无需数据转置或额外复杂电路。


<details>
  <summary>Details</summary>
Motivation: 处理内存(PIM)架构通过在DRAM内直接计算来解决内存墙问题。比特移位是PIM应用中的基本操作，如移位加法乘法、进位传播加法器以及AES和Reed-Solomon纠错码等密码学算法中的伽罗华域算术。现有DRAM内移位方法需要添加专用移位电路或采用垂直数据布局，存在开销大、需要数据转置等问题。

Method: 基于先前在非对称子阵列中用于行迁移的"迁移单元"设计，在每个子阵列的顶部和底部添加一行迁移单元，重新利用并扩展其功能，实现任意给定行内的双向比特移位。该设计保持与标准DRAM操作的兼容性，对水平存储的数据进行操作，无需数据转置，并利用现有单元结构，无需额外复杂逻辑和电路。

Result: 使用NVMain进行时序和能量分析，通过LTSPICE进行电路级验证，在Cadence Virtuoso中实现VLSI布局。设计能够有效实现DRAM内比特移位，同时保持与标准DRAM操作的兼容性。

Conclusion: 提出了一种新颖的DRAM子阵列设计，能够在开放位线架构中实现高效的DRAM内比特移位操作。该设计通过重新利用迁移单元，避免了数据转置开销，无需额外复杂电路，为PIM应用提供了有效的比特移位支持。

Abstract: Processing-in-Memory (PIM) architectures enable computation directly within DRAM and help combat the memory wall problem. Bit-shifting is a fundamental operation that enables PIM applications such as shift-and-add multiplication, adders using carry propagation, and Galois field arithmetic used in cryptography algorithms like AES and Reed-Solomon error correction codes. Existing approaches to in-DRAM shifting require adding dedicated shifter circuits beneath the sense amplifiers to enable horizontal data movement across adjacent bitlines or vertical data layouts which store operand bits along a bitline to implement shifts as row-copy operations. In this paper, we propose a novel DRAM subarray design that enables in-DRAM bit-shifting for open-bitline architectures. In this new design, we built upon prior work that introduced a new type of cell used for row migration in asymmetric subarrays, called a "migration cell". We repurpose and extend the functionality by adding a row of migration cells at the top and bottom of each subarray which enables bidirectional bit-shifting within any given row. This new design maintains compatibility with standard DRAM operations. Unlike previous approaches to shifting, our design operates on horizontally-stored data, eliminating the need and overhead of data transposition, and our design leverages the existing cell structures, eliminating the need for additional complex logic and circuitry. We present an evaluation of our design that includes timing and energy analysis using NVMain, circuit-level validation of the in-DRAM shift operation using LTSPICE, and a VLSI layout implementation in Cadence Virtuoso.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [27] [Learning to Generate Secure Code via Token-Level Rewards](https://arxiv.org/abs/2602.23407)
*Jiazheng Quan,Xiaodong Li,Bin Wang,Guo An,Like Liu,Degen Huang,Lin Liu,Chengbin Hou*

Main category: cs.CR

TL;DR: Vul2Safe框架通过LLM自反思构建高质量安全修复数据集PrimeVul+，并引入SRCode训练框架使用token级奖励进行强化学习，显著降低代码生成中的安全漏洞


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但仍容易产生安全漏洞。现有方法存在两个关键限制：高质量安全数据稀缺和强化学习奖励信号过于粗粒度

Method: 1. 提出Vul2Safe框架，利用LLM自反思从真实漏洞构建高置信度修复对，并生成多样化隐式提示构建PrimeVul+数据集
2. 引入SRCode训练框架，首次在代码安全强化学习中使用token级奖励，使模型在训练中持续关注和强化细粒度安全模式

Result: 广泛实验表明，PrimeVul+和SRCode显著减少了生成代码中的安全漏洞，同时在多个基准测试中提高了整体代码质量

Conclusion: 与传统实例级奖励方案相比，该方法能够更精确地优化局部安全实现，为安全代码生成提供了有效解决方案

Abstract: Large language models (LLMs) have demonstrated strong capabilities in code generation, yet they remain prone to producing security vulnerabilities. Existing approaches commonly suffer from two key limitations: the scarcity of high-quality security data and coarse-grained reinforcement learning reward signals. To address these challenges, we propose Vul2Safe, a new secure code generation framework that leverages LLM self-reflection to construct high-confidence repair pairs from real-world vulnerabilities, and further generates diverse implicit prompts to build the PrimeVul+ dataset. Meanwhile, we introduce SRCode, a novel training framework that pioneers the use of token-level rewards in reinforcement learning for code security, which enables the model to continuously attend to and reinforce critical fine-grained security patterns during training. Compared with traditional instance-level reward schemes, our approach allows for more precise optimization of local security implementations. Extensive experiments show that PrimeVul+ and SRCode substantially reduce security vulnerabilities in generated code while improving overall code quality across multiple benchmarks.

</details>


### [28] [SAILOR: A Scalable and Energy-Efficient Ultra-Lightweight RISC-V for IoT Security](https://arxiv.org/abs/2602.24166)
*Christian Ewert,Tim Hardow,Melf Fritsch,Leon Dietrich,Henrik Strunck,Rainer Buchty,Mladen Berekovic,Saleh Mulhem*

Main category: cs.CR

TL;DR: SAILOR是一个面向物联网加密应用的超轻量级RISC-V核心系列，通过模块化设计和可扩展数据路径（1-32位串行执行）实现了面积、能效和加密功能的平衡优化。


<details>
  <summary>Details</summary>
Motivation: 当前大多数RISC-V物联网核心要么优先考虑面积占用，要么优先考虑能效，而添加加密支持会进一步影响紧凑性。真正同时优化能效和安全性的集成架构仍未被充分探索，导致受限物联网环境面临性能和安全性的权衡问题。

Method: 提出SAILOR核心系列，采用模块化设计，支持1、2、4、8、16和32位串行执行数据路径，优先考虑最小化面积。这种模块化设计和可适应数据路径最小化了集成RISC-V加密扩展的开销。

Result: SAILOR在性能和能效方面超越现有解决方案达13倍，面积减少高达59%。验证了轻量级加密功能可以在不过度增加开销的情况下实现，且能效或面积优化的设计无需牺牲性能。

Conclusion: SAILOR展示了在物联网加密应用中实现面积、能效和安全性平衡的可行性，为受限物联网环境提供了无需在性能和安全之间做出妥协的解决方案。

Abstract: Recently, RISC-V has contributed to the development of IoT devices, requiring architectures that balance energy efficiency, compact area, and integrated security. However, most recent RISC-V cores for IoT prioritize either area footprint or energy efficiency, while adding cryptographic support further compromises compactness. As a result, truly integrated architectures that simultaneously optimize efficiency and security remain largely unexplored, leaving constrained IoT environments vulnerable to performance and security trade-offs. In this paper, we introduce SAILOR, an energy-efficient and scalable ultra-lightweight RISC-V core family for cryptographic applications in IoT. Our design is modular and spans 1-, 2-, 4-, 8-, 16-, and 32-bit serialized execution data-paths, prioritizing minimal area. This modular design and adaptable data-path minimizes the overhead of integrating RISC-V cryptography extensions, achieving low hardware cost while significantly improving energy efficiency. We validate our design approach through a comprehensive analysis of area, energy, and efficiency trade-offs. The results surpass state-of-the-art solutions in both performance and energy efficiency by up to 13x and reduce area by up to 59 %, demonstrating that lightweight cryptographic features can be added without prohibitive overhead, and that energy- or area-efficient designs need not compromise performance.

</details>


### [29] [Lap2: Revisiting Laplace DP-SGD for High Dimensions via Majorization Theory](https://arxiv.org/abs/2602.23516)
*Meisam Mohammady,Qin Yang,Nicholas Stout,Ayesha Samreen,Han Wang,Christopher J Quinn,Yuan Hong*

Main category: cs.CR

TL;DR: Lap2：一种新的差分隐私随机梯度下降方法，使用L2裁剪实现Laplace机制，解决了传统Laplace DP-SGD因L1裁剪在高维模型中噪声过大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Laplace DP-SGD依赖L1范数裁剪，在高维模型中L1范数可能比L2范数大sqrt(n)倍，导致噪声规模随模型维度急剧增加，严重影响模型实用性。

Method: 通过计算坐标方向矩边界，应用优化理论构建数据无关的紧致上界，利用矩计数函数的Schur凸性，设计满足L2裁剪约束的优化集，实现多变量隐私计数。

Result: Lap2显著提升了Laplace DP-SGD性能，在强隐私约束下达到或超越Gaussian DP-SGD。例如，在RoBERTa-base上微调SST-2，ε=0.54时准确率达到87.88%，优于Gaussian的87.16%和标准Laplace的48.97%。

Conclusion: Lap2成功解决了Laplace机制在高维差分隐私深度学习中的实用性障碍，为DP-SGD提供了新的有效选择，在保持强隐私保证的同时显著提升了模型性能。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a cornerstone technique for ensuring privacy in deep learning, widely used in both training from scratch and fine-tuning large-scale language models. While DP-SGD predominantly relies on the Gaussian mechanism, the Laplace mechanism remains underutilized due to its reliance on L1 norm clipping. This constraint severely limits its practicality in high-dimensional models because the L1 norm of an n-dimensional gradient can be up to sqrt(n) times larger than its L2 norm. As a result, the required noise scale grows significantly with model size, leading to poor utility or untrainable models.
  In this work, we introduce Lap2, a new solution that enables L2 clipping for Laplace DP-SGD while preserving strong privacy guarantees. We overcome the dimensionality-driven clipping barrier by computing coordinate-wise moment bounds and applying majorization theory to construct a tight, data-independent upper bound over the full model. By exploiting the Schur-convexity of the moment accountant function, we aggregate these bounds using a carefully designed majorization set that respects the L2 clipping constraint. This yields a multivariate privacy accountant that scales gracefully with model dimension and enables the use of thousands of moments. Empirical evaluations demonstrate that our approach significantly improves the performance of Laplace DP-SGD, achieving results comparable to or better than Gaussian DP-SGD under strong privacy constraints. For instance, fine-tuning RoBERTa-base (125M parameters) on SST-2 achieves 87.88% accuracy at epsilon=0.54, outperforming Gaussian (87.16%) and standard Laplace (48.97%) under the same budget.

</details>


### [30] [CLOAQ: Combined Logic and Angle Obfuscation for Quantum Circuits](https://arxiv.org/abs/2602.23569)
*Vincent Langford,Shihan Zhao,Hongyu Zhang,Ben Dong,Qian Wang,Anees Rehman,Yuntao Liu*

Main category: cs.CR

TL;DR: CLOAQ是一种量子电路混淆方法，通过隐藏选定门的逻辑和相位角度来保护量子电路设计，防止不信任的量子编译器窃取知识产权。


<details>
  <summary>Details</summary>
Motivation: 不信任的量子编译器可能窃取量子电路设计，危及敏感的知识产权。现有量子电路混淆方法存在局限性，需要更有效的保护方案。

Method: 提出CLOAQ量子电路混淆方法，同时隐藏选定门的逻辑和相位角度。采用从所有量子比特的希尔伯特空间中均匀采样输入状态的方法进行评估，比之前使用全|0>输入的方法更准确。

Result: CLOAQ受益于逻辑和相位保护的协同作用。与仅使用单一视角的先前QCO方法相比，组合方法对攻击更具弹性，当解锁密钥错误时会造成更大的功能破坏。

Conclusion: CLOAQ通过结合逻辑和相位保护，提供了一种更有效的量子电路混淆方法，能够更好地保护量子电路设计免受不信任量子编译器的威胁。

Abstract: In the realm of quantum computing, quantum circuits serve as essential depictions of quantum algorithms, which are then compiled into executable operations for quantum computations. Quantum compilers are responsible for converting these algorithmic quantum circuits into versions compatible with specific quantum hardware, thus connecting quantum software with hardware. Nevertheless, untrusted quantum compilers present notable threats. They have the potential to result in the theft of quantum circuit designs and jeopardize sensitive intellectual property (IP). In this work, we propose CLOAQ, a quantum circuit obfuscation (QCO) approach that hides the logic and the phase angles of selected gates within the obfuscated quantum circuit. To evaluate the effectiveness of CLOAQ, we sample the input state uniformly from the Hilbert space of all qubits, which is more accurate than prior work that use all-|0> inputs. Our results show that CLOAQ benefits from the synergy between logic and phase protections. Compared with prior QCO approaches using only one perspective, the combined method is more resilient to attacks and causes greater functional disruption when the unlocking key is incorrect.

</details>


### [31] [Central Bank Digital Currencies: Where is the Privacy, Technology, and Anonymity?](https://arxiv.org/abs/2602.23659)
*Jeff Nijsse,Andrea Pinto*

Main category: cs.CR

TL;DR: 该研究探讨了央行数字货币中的隐私定义及其对设计的影响，分析了可用的隐私增强技术，并通过20个CBDC案例验证了研究发现。


<details>
  <summary>Details</summary>
Motivation: 随着金融系统数字化和数字货币采用增加，央行数字货币成为技术创新焦点。隐私合规已成为CBDC成功设计的关键因素，不仅涉及技术要求，还影响法律要求、用户信任和安全考虑。然而，缺乏对隐私的共同理解和技术特性限制了隐私增强技术在CBDC中的应用进展。

Method: 研究提出一个全面的隐私定义，并将其映射到密码学领域以进行功能实现。通过对20个当前CBDC的案例研究来验证研究成果。

Result: 研究表明，全面的隐私可以在提案阶段进行设计，但隐私功能通常无法在最终发布的CBDC版本中实现。

Conclusion: CBDC设计需要跨学科方法来整合隐私增强技术，但当前实践表明隐私功能在从提案到实际发布的转化过程中存在实施差距。

Abstract: In an age of financial system digitisation and the increasing adoption of digital currencies, Central Bank Digital Currencies (CBDCs) have emerged as a focal point for technological innovation. Privacy compliance has become a key factor in the successful design of CBDCs, extending beyond technical requirements to influence legal requirements, user trust, and security considerations. Implementing Privacy-Enhancing Technologies (PETs) in CBDCs requires an interdisciplinary approach, however, the lack of a common understanding of privacy and the essential technological characteristics restricts progress. This work investigates: (1) How privacy can be defined within the framework of CBDCs and what implications does this definition have for CBDCs design? and (2) Which PETs can be employed to enhance privacy in CBDC design? We propose a comprehensive definition for privacy that is mapped to the cryptographic landscape for feature implementation. The research is validated against case studies from 20 current CBDCs. The study shows that comprehensive privacy can be designed in the proposal stage, but that privacy does not reach the launched version of the CBDC.

</details>


### [32] [Privacy-Preserving Local Energy Trading Considering Network Fees](https://arxiv.org/abs/2602.23698)
*Eman Alqahtani,Mustafa A. Mustafa*

Main category: cs.CR

TL;DR: 本文提出了一种保护隐私的本地能源市场协议，该协议结合了电网费用考虑，使用安全多方计算和Schnorr身份验证协议，在保护参与者数据隐私的同时确保电网物理约束。


<details>
  <summary>Details</summary>
Motivation: 本地能源市场涉及处理敏感的参与者数据，存在隐私风险，同时电力交换需要考虑电网物理约束和相关成本。现有工作通常单独处理这些问题，要么关注电网方面，要么提供隐私保护，缺乏综合考虑。

Method: 基于双拍卖机制，采用安全多方计算保护参与者数据隐私，使用Schnorr身份验证协议进行多方验证确保认证参与而不泄露隐私，并对协议进行优化以减少通信和轮次复杂度。

Result: 协议满足安全要求，实验表明在典型本地能源市场规模下具有可行性：一个拥有5,000名参与者的市场可以在4.17分钟内完成清算。

Conclusion: 提出的隐私保护协议成功解决了本地能源市场中隐私保护和电网约束的双重挑战，为大规模分布式能源交易提供了可行的技术方案。

Abstract: Driven by the widespread deployment of distributed energy resources, local energy markets (LEMs) have emerged as a promising approach for enabling direct trades among prosumers and consumers to balance intermittent generation and demand locally. However, LEMs involve processing sensitive participant data, which, if not protected, poses privacy risks. At the same time, since electricity is exchanged over the physical power network, market mechanisms should consider physical constraints and network-related costs. Existing work typically addresses these issues separately, either by incorporating grid-related aspects or by providing privacy protection. To address this gap, we propose a privacy-preserving protocol for LEMs, with consideration of network fees that can incite participants to respect physical limits. The protocol is based on a double-auction mechanism adapted from prior work to enable more efficient application of our privacy-preserving approach. To protect participants' data, we use secure multiparty computation. In addition, Schnorr's identification protocol is employed with multiparty verification to ensure authenticated participation without compromising privacy. We further optimise the protocol to reduce communication and round complexity. We prove that the protocol meets its security requirements and show through experimentation its feasibility at a typical LEM scale: a market with 5,000 participants can be cleared in 4.17 minutes.

</details>


### [33] [PLA for Drone RID Frames via Motion Estimation and Consistency Verification](https://arxiv.org/abs/2602.23760)
*Jie Li,Jing Li,Lu Lv,Zhanyu Ju,Fengkui Gong*

Main category: cs.CR

TL;DR: 提出基于一致性验证的物理层认证算法，通过融合无线感知参数和运动状态信息来增强无人机远程识别系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 无人机远程识别(RID)系统的广播特性和缺乏密码保护使其容易受到欺骗和重放攻击，需要更可靠的身份验证机制。

Method: 开发RID感知的解码模块提取感知参数，结合偏航增强的恒定加速度扩展卡尔曼滤波器和LSTM运动估计器，通过误差感知融合策略进行一致性验证。

Result: 仿真结果表明，该算法在真实无线损伤和复杂无人机机动条件下，显著提高了认证可靠性和鲁棒性，优于现有的RF特征和运动模型方案。

Conclusion: 提出的物理层认证算法通过多源信息融合和一致性验证，有效增强了无人机RID系统的安全性，对抗欺骗和重放攻击。

Abstract: Drone Remote Identification (RID) plays a critical role in low-altitude airspace supervision, yet its broadcast nature and lack of cryptographic protection make it vulnerable to spoofing and replay attacks. In this paper, we propose a consistency verification-based physical-layer authentication (PLA) algorithm for drone RID frames. A RID-aware sensing and decoding module is first developed to extract communication-derived sensing parameters, including angle-of-arrival, Doppler shift, average channel gain, and the number of transmit antennas, together with the identity and motion-related information decoded from previously authenticated RID frames. Rather than fusing all heterogeneous information into a single representation, different types of information are selectively utilized according to their physical relevance and reliability. Specifically, real-time wireless sensing parameter constraints and previously authenticated motion states are incorporated in a yaw-augmented constant-acceleration extended Kalman filter (CA-EKF) to estimate the three-dimensional position and motion states of the drone. To further enhance authentication reliability under highly maneuverable and non-stationary flight scenarios, a data-driven long short-term memory-based motion estimator is employed, and its predictions are adaptively combined with the CA-EKF via an error-aware fusion strategy. Finally, RID frames are authenticated by verifying consistency in the number of transmit antennas, motion estimates, and no-fly-zone constraints. Simulation results demonstrate that the proposed algorithm significantly improves authentication reliability and robustness under realistic wireless impairments and complex drone maneuvers, outperforming existing RF feature-based and motion model-based PLA schemes.

</details>


### [34] [Enhancing Continual Learning for Software Vulnerability Prediction: Addressing Catastrophic Forgetting via Hybrid-Confidence-Aware Selective Replay for Temporal LLM Fine-Tuning](https://arxiv.org/abs/2602.23834)
*Xuhui Dou,Hayretdin Bahsi,Alejandro Guerra-Manzanares*

Main category: cs.CR

TL;DR: 该研究提出Hybrid-CASR方法，通过置信度感知的重放和类别平衡策略，在持续学习框架下提升LLM对源代码漏洞的时序检测性能，相比基线方法在准确率和计算效率上取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的源代码漏洞检测研究大多采用随机划分的训练测试集，忽略了时间因素，高估了实际性能。在实际部署中，检测器需要在不断演化的代码库上工作，面临时序分布偏移的挑战。

Method: 使用基于CVE的2018-2024数据集，按双月窗口组织。采用phi-2模型配合LoRA进行持续微调，评估了8种持续学习策略。提出了Hybrid-CASR方法，这是一种置信度感知的重放方法，优先选择不确定样本，同时在重放缓冲区中保持漏洞函数和修复函数的平衡比例。

Result: 在双月前向评估中，Hybrid-CASR达到Macro-F1为0.667，相比仅窗口训练基线（0.651）提升0.016，具有统计显著性（p=0.026）。同时后向保留能力更强（IBR@1为0.741），每个窗口训练时间减少约17%。累积训练仅带来微小F1提升（0.661）但计算成本增加15.9倍。

Conclusion: 带有类别平衡的选择性重放策略为基于LLM的时序漏洞检测提供了实用的准确率-效率权衡方案，能够有效应对持续的时间漂移问题。

Abstract: Recent work applies Large Language Models (LLMs) to source-code vulnerability detection, but most evaluations still rely on random train-test splits that ignore time and overestimate real-world performance. In practice, detectors are deployed on evolving code bases and must recognise future vulnerabilities under temporal distribution shift. This paper investigates continual fine-tuning of a decoder-style language model (microsoft/phi-2 with LoRA) on a CVE-linked dataset spanning 2018-2024, organised into bi-monthly windows. We evaluate eight continual learning strategies, including window-only and cumulative training, replay-based baselines and regularisation-based variants. We propose Hybrid Class-Aware Selective Replay (Hybrid-CASR), a confidence-aware replay method for binary vulnerability classification that prioritises uncertain samples while maintaining a balanced ratio of VULNERABLE and FIXED functions in the replay buffer. On bi-monthly forward evaluation Hybrid-CASR achieves a Macro-F1 of 0.667, improving on the window-only baseline (0.651) by 0.016 with statistically significant gains ($p = 0.026$) and stronger backward retention (IBR@1 of 0.741). Hybrid-CASR also reduces training time per window by about 17 percent compared to the baseline, whereas cumulative training delivers only a minor F1 increase (0.661) at a 15.9-fold computational cost. Overall, the results show that selective replay with class balancing offers a practical accuracy-efficiency trade-off for LLM-based temporal vulnerability detection under continuous temporal drift.

</details>


### [35] [Jailbreak Foundry: From Papers to Runnable Attacks for Reproducible Benchmarking](https://arxiv.org/abs/2602.24009)
*Zhicheng Fang,Jingjie Zheng,Chenxu Fu,Wei Xu*

Main category: cs.CR

TL;DR: JAILBREAK FOUNDRY (JBF) 是一个解决大语言模型越狱技术评估标准化问题的系统，通过多智能体工作流将越狱论文转化为可执行模块，实现统一框架下的即时评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的越狱技术发展速度快于基准测试，导致鲁棒性评估过时且难以跨论文比较，主要原因是数据集、测试框架和评判协议存在漂移问题。

Method: JBF系统包含三个核心组件：JBF-LIB（共享合约和可复用工具）、JBF-FORGE（多智能体论文到模块转换）、JBF-EVAL（标准化评估）。通过多智能体工作流将越狱论文转化为可执行模块。

Result: 在30个复现的攻击中，JBF实现了高保真度，平均攻击成功率偏差仅为+0.26个百分点。通过共享基础设施，JBF将攻击特定实现代码减少了近一半，平均代码复用率达到82.5%。

Conclusion: JBF通过自动化攻击集成和标准化评估，为创建能够跟上快速变化安全格局的活基准测试提供了可扩展解决方案。

Abstract: Jailbreak techniques for large language models (LLMs) evolve faster than benchmarks, making robustness estimates stale and difficult to compare across papers due to drift in datasets, harnesses, and judging protocols. We introduce JAILBREAK FOUNDRY (JBF), a system that addresses this gap via a multi-agent workflow to translate jailbreak papers into executable modules for immediate evaluation within a unified harness. JBF features three core components: (i) JBF-LIB for shared contracts and reusable utilities; (ii) JBF-FORGE for the multi-agent paper-to-module translation; and (iii) JBF-EVAL for standardizing evaluations. Across 30 reproduced attacks, JBF achieves high fidelity with a mean (reproduced-reported) attack success rate (ASR) deviation of +0.26 percentage points. By leveraging shared infrastructure, JBF reduces attack-specific implementation code by nearly half relative to original repositories and achieves an 82.5% mean reused-code ratio. This system enables a standardized AdvBench evaluation of all 30 attacks across 10 victim models using a consistent GPT-4o judge. By automating both attack integration and standardized evaluation, JBF offers a scalable solution for creating living benchmarks that keep pace with the rapidly shifting security landscape.

</details>
