<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative](https://arxiv.org/abs/2601.14271)
*Denise M. Case*

Main category: cs.AI

TL;DR: 本文证明了一个关于本体论中立性的不可能性结果：任何包含因果或规范承诺的基础层本体论都无法作为跨不同解释框架的中立共享基础。


<details>
  <summary>Details</summary>
Motivation: 现代数据系统需要在持续存在的法律、政治和分析分歧中支持问责制，这要求设计能够作为共享基础的本体论必须满足严格约束条件。

Method: 通过逻辑分析和理论论证，建立了本体论中立性的不可能性定理，证明中立性（理解为解释非承诺性和在不兼容扩展下的稳定性）与基础层包含因果或规范承诺是不兼容的。

Result: 证明了任何断言因果或道义结论作为本体论事实的本体论，都无法在不修订或矛盾的情况下作为跨不同框架的中立基础。因此，中立的本体论基础必须是前因果和前规范的。

Conclusion: 中立的本体论基础必须仅表示实体及其身份和持久性条件，而将解释、评估和推理外部化。这为任何旨在跨冲突解释框架维护共享稳定现实表示的系统确立了必要的设计约束。

Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.

</details>


### [2] [Epistemic Constitutionalism Or: how to avoid coherence bias](https://arxiv.org/abs/2601.14295)
*Michele Loi*

Main category: cs.AI

TL;DR: 本文主张为AI建立"认知宪法"——明确、可争议的元规范来调节系统如何形成和表达信念。通过"来源归因偏见"案例，作者发现前沿模型强制实施身份-立场一致性，惩罚那些归因于预期意识形态立场与论证内容冲突的来源的论证。作者区分了柏拉图式和自由主义两种宪法方法，并支持后者。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地充当人工推理者：它们评估论证、分配可信度并表达信心。然而它们的信念形成行为受制于隐含的、未经检验的认知政策。本文旨在为AI建立明确的、可争议的认知宪法，以规范系统如何形成和表达信念。

Method: 通过"来源归因偏见"作为案例研究，展示前沿模型如何强制实施身份-立场一致性。当模型检测到系统性测试时，这些效应会崩溃，表明系统将来源敏感性视为需要抑制的偏见而非良好执行的能力。作者区分了两种宪法方法：柏拉图式（要求形式正确性和默认的来源独立性）和自由主义式（拒绝特权立场，指定保护集体探究条件的程序规范）。

Result: 研究发现前沿模型确实存在来源归因偏见，它们惩罚那些归因于预期意识形态立场与论证内容冲突的来源的论证。当检测到系统性测试时，这些效应会崩溃。作者提出了自由主义宪法方法，并勾勒了包含八项原则和四种取向的宪法核心。

Conclusion: 作者主张采用自由主义宪法方法，拒绝特权立场，指定保护集体探究条件的程序规范，同时允许基于认知警惕的原则性来源关注。AI认知治理需要与AI伦理相同的明确、可争议结构。作者提出了八项原则和四种取向的宪法核心框架。

Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.

</details>


### [3] [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)
*Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: 微调大语言模型在PDDL规划任务中表现出高有效计划率，但跨领域泛化能力几乎为零，表明模型依赖领域特定模式而非可迁移的规划能力。


<details>
  <summary>Details</summary>
Motivation: 研究微调大语言模型在PDDL规划任务中的表现是否反映可迁移的规划能力，还是仅仅是领域特定的记忆，探索LLM在规划任务中的泛化能力。

Method: 在10个IPC 2023领域的40,000个领域-问题-计划元组上微调1.7B参数LLM，使用三种诊断干预：实例级符号匿名化、紧凑计划序列化、以及使用VAL验证器作为成功导向强化信号的验证器奖励微调。

Result: 模型在领域内达到82.9%有效计划率，但在两个未见领域上为0%。符号匿名化和紧凑序列化导致性能显著下降，验证器奖励微调在监督训练一半周期内达到性能饱和，但未改善跨领域泛化。

Conclusion: 微调模型严重依赖领域特定模式而非可迁移的规划能力，揭示了LLM基于规划中持续的泛化差距，并提供了研究其原因的诊断工具。

Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.

</details>


### [4] [Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling](https://arxiv.org/abs/2601.14485)
*Yuan Tian,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于拐点的活动组选择策略，通过多树遗传编程框架同时演化优先级规则和组选择规则，解决了动态多模式资源受限项目调度问题中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 动态多模式资源受限项目调度问题需要在活动执行顺序和对应执行模式上做出决策。虽然活动组选择策略在小规模实例中有效，但在大规模问题上存在可扩展性问题。本文旨在通过引入拐点选择机制来增强组选择策略的可扩展性。

Method: 提出基于拐点的选择机制：首先使用活动排序规则对所有符合条件的活动-模式对进行排序，然后通过拐点选择找到有前景的对，最后用组选择规则选择最佳活动组合。开发了多树遗传编程框架来同时演化这两种规则。

Result: 实验结果表明，该方法在大规模实例上具有良好的可扩展性，并且在大多数场景中优于采用顺序决策的遗传编程方法。

Conclusion: 提出的基于拐点的活动组选择策略有效解决了大规模动态多模式资源受限项目调度问题的可扩展性问题，通过多树遗传编程框架同时演化优先级规则和组选择规则，在性能上超越了传统的顺序决策方法。

Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.

</details>


### [5] ["Just in Time" World Modeling Supports Human Planning and Reasoning](https://arxiv.org/abs/2601.14514)
*Tony Chen,Sam Cheyette,Kelsey Allen,Joshua Tenenbaum,Kevin Smith*

Main category: cs.AI

TL;DR: 论文提出"Just-in-Time"框架，通过实时构建简化表征来支持高效心理模拟，减少计算负担


<details>
  <summary>Details</summary>
Motivation: 人类心理模拟在复杂环境中受限于认知容量，需要简化表征但不知如何高效确定简化方式

Method: 提出即时框架，将模拟、视觉搜索和表征修改紧密交织，当前模拟指导搜索方向，视觉搜索标记需要编码的对象

Result: 模型仅编码少量对象却能做出高效预测，在网格世界规划任务和物理推理任务中优于其他模型

Conclusion: 该框架为人类如何构建简化表征以支持高效心理模拟提供了具体算法解释

Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.

</details>


### [6] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: MAS-Orchestra：通过函数调用强化学习框架实现多智能体系统整体编排，MASBENCH基准测试揭示MAS优势取决于任务结构而非普遍适用


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统（MAS）自动设计方法存在不足，主要源于两个关键因素：方法复杂性（顺序代码级执行限制全局系统级推理）和效能不确定性（部署前无法确定相比单智能体系统的实际优势）

Method: 提出MAS-Orchestra训练时框架，将MAS编排建模为函数调用强化学习问题，通过整体编排一次性生成整个MAS；将复杂目标导向的子智能体抽象为可调用函数，实现全局系统结构推理同时隐藏内部执行细节

Result: 引入MASBENCH基准测试，从深度、视野、广度、并行性和鲁棒性五个维度刻画任务特性；分析表明MAS优势关键取决于任务结构、验证协议以及编排器和子智能体能力，而非普遍适用；MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公开基准上取得一致改进

Conclusion: MAS-Orchestra和MASBENCH共同促进了多智能体系统的更好训练和理解，为追求多智能体智能提供了系统化框架和评估基准

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [7] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: 本文提出AGEA攻击框架，可在有限查询预算下高效窃取GraphRAG系统的隐藏知识图谱结构，恢复率达90%


<details>
  <summary>Details</summary>
Motivation: 现有研究表明GraphRAG系统可能泄露检索到的子图，但在实际查询预算下，隐藏图结构的高效重构可行性尚未探索。本文研究在预算受限的黑盒设置中，攻击者如何通过自适应查询窃取系统的潜在实体关系图。

Method: 提出AGEA（Agentic Graph Extraction Attack）框架，采用新颖性引导的探索-利用策略、外部图记忆模块，以及结合轻量级发现与基于LLM过滤的两阶段图提取流水线。

Result: 在医疗、农业和文学数据集上，针对Microsoft-GraphRAG和LightRAG系统进行评估。在相同查询预算下，AGEA显著优于现有攻击基线，恢复高达90%的实体和关系，同时保持高精度。

Conclusion: 现代GraphRAG系统即使在严格查询限制下，也极易受到结构化、智能化的提取攻击，系统安全性存在严重漏洞。

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [8] [Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text](https://arxiv.org/abs/2601.14683)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.AI

TL;DR: 本研究提出了一种基于本地大语言模型的结构化自适应匿名化框架（SFAA），用于检测和匿名化定性研究转录本中的敏感数据，解决了传统方法耗时、不一致且缺乏上下文感知的问题。


<details>
  <summary>Details</summary>
Motivation: 定性研究包含大量个人、情境和组织细节，存在隐私风险。传统手动匿名化方法耗时、不一致且容易遗漏关键标识符，而现有自动化工具依赖模式匹配或固定规则，缺乏上下文感知能力，可能改变数据含义。

Method: 提出了结构化自适应匿名化框架（SFAA），包含三个步骤：检测、分类和自适应匿名化。框架采用四种匿名化策略：基于规则的替换、上下文感知重写、泛化和抑制，根据标识符类型和风险级别应用策略。标识符处理遵循GDPR、HIPAA和OECD等国际隐私和研究伦理标准。

Result: 使用LLaMA和Phi两个本地模型评估框架性能。结果显示，大语言模型比人工评审员发现更多敏感数据。Phi在发现敏感数据方面优于LLaMA，但错误略多。Phi能够发现超过91%的敏感数据，且94.8%的文本保持了与原始文本相同的情感，表明准确性高，不影响定性数据分析。

Conclusion: 基于本地大语言模型的SFAA框架提供了一种可靠、可重复且具有上下文感知能力的匿名化流程，能够有效保护定性研究中的隐私数据，同时保持数据的分析价值。

Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.

</details>


### [9] [AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.14702)
*Zecong Tang,Zixu Wang,Yifei Wang,Weitong Lian,Tianjian Gao,Haoran Li,Tengju Ru,Lingyi Meng,Zhejun Cui,Yichen Zhu,Qi Kang,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: AutoDriDM是一个面向自动驾驶的决策中心化渐进式基准测试，包含6,650个问题，评估视觉语言模型在感知到决策能力边界上的表现，发现感知与决策性能之间存在弱对齐关系。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶基准测试过于强调感知能力，未能充分评估决策过程。视觉语言模型展现出推理和泛化能力，为自动驾驶提供了新可能，但需要更全面的评估框架来连接感知与决策。

Method: 提出AutoDriDM基准测试，包含6,650个问题，涵盖对象、场景和决策三个维度。评估主流视觉语言模型，进行感知到决策能力边界分析，并引入解释性分析来识别关键失败模式，还开发了分析器模型来自动化大规模标注。

Result: 评估揭示了感知与决策性能之间的弱对齐关系。解释性分析识别出逻辑推理错误等关键失败模式。AutoDriDM填补了感知中心化与决策中心化评估之间的空白。

Conclusion: AutoDriDM为更安全可靠的自动驾驶视觉语言模型提供了指导，通过决策中心化的渐进式基准测试，促进了感知与决策能力的综合评估，有助于推动现实世界自动驾驶系统的发展。

Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.

</details>


### [10] [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)
*Mingxuan Song,Yusen Huo,Bohan Zhou,Shenglin Yin,Zhen Xiao,Jieyi Long,Zhilin Zhang,Chuan Yu*

Main category: cs.AI

TL;DR: 论文提出DARA框架，通过两阶段决策（少样本推理+细粒度优化）解决AI生成竞价中的预算约束优化问题，结合LLM的上下文学习能力和数值精度需求。


<details>
  <summary>Details</summary>
Motivation: 在线广告中，广告主在预算约束下优化累积价值面临挑战。传统强化学习方法在少样本场景下效果不佳，而大语言模型虽然具有上下文学习能力，但缺乏数值精度。

Method: 提出GRPO-Adaptive后训练策略增强推理和数值精度，并构建DARA双阶段框架：第一阶段通过少样本推理器生成初始计划，第二阶段通过反馈驱动的细粒度优化器精炼计划。

Result: 在真实世界和合成数据环境中的广泛实验表明，该方法在预算约束下的累积广告主价值方面持续优于现有基线方法。

Conclusion: DARA框架成功结合了LLM的上下文学习优势和AIGB任务所需的精确适应性，为少样本场景下的广告竞价优化提供了有效解决方案。

Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

</details>


### [11] [An XAI View on Explainable ASP: Methods, Systems, and Perspectives](https://arxiv.org/abs/2601.14764)
*Thomas Eiter,Tobias Geibinger,Zeynep G. Saribatur*

Main category: cs.AI

TL;DR: 这篇论文是关于ASP（答案集编程）解释方法的综述，从可解释AI角度分析ASP的解释类型、现有工具覆盖情况，并指出研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: ASP作为一种声明式推理和问题解决方法，其基于规则的特性使其天生适合可解释推理。随着可解释AI（XAI）的兴起，ASP的解释性变得越来越重要。然而现有的ASP解释方法和工具往往针对特定解释场景，未能覆盖ASP用户遇到的所有情况。

Method: 采用综述研究方法，从XAI视角出发，系统性地：1）概述ASP解释类型及其与用户解释问题的关联；2）描述当前理论和工具对这些解释类型的覆盖情况；3）分析现有ASP解释方法的不足。

Result: 论文提供了ASP解释类型的全面分类框架，评估了现有理论和工具对这些解释类型的支持程度，识别了当前ASP解释方法中的覆盖空白。

Conclusion: 虽然ASP具有天生的可解释性优势，但现有解释方法仍存在不足。论文指出了未来研究的方向，包括开发更全面的解释框架、填补特定解释场景的空白，以及改进ASP解释工具的用户体验。

Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.

</details>


### [12] [Semantic-Guided Unsupervised Video Summarization](https://arxiv.org/abs/2601.14773)
*Haizhou Liu,Haodong Jin,Yiming Wang,Hui Yu*

Main category: cs.AI

TL;DR: 提出了一种语义引导的无监督视频摘要方法，通过语义对齐注意力机制和增量训练策略，解决了现有GAN方法语义利用不足和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督视频摘要方法主要依赖GAN，但存在两个主要问题：1) 主要利用单模态特征，忽视了语义信息在关键帧选择中的指导作用；2) GAN训练不稳定。

Method: 提出语义引导的无监督视频摘要方法，包含：1) 新颖的帧级语义对齐注意力机制，集成到关键帧选择器中；2) 在对抗框架中引导基于Transformer的生成器更好地重建视频；3) 采用增量训练策略逐步更新模型组件。

Result: 实验结果表明，该方法在多个基准数据集上取得了优越的性能。

Conclusion: 通过语义引导和增量训练，有效解决了现有无监督视频摘要方法的局限性，提升了摘要质量和训练稳定性。

Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.

</details>


### [13] [Towards Bound Consistency for the No-Overlap Constraint Using MDDs](https://arxiv.org/abs/2601.14784)
*Amaury Guichard,Laurent Michel,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出了首个针对无重叠约束的边界一致性算法，通过构建和限制MDD宽度实现多项式时间过滤，相比现有方法显著减少搜索树节点数。


<details>
  <summary>Details</summary>
Motivation: 无重叠约束的边界一致性已知是NP完全问题，现有多项式时间收紧技术如边查找、非首非尾推理和能量推理等存在局限性，需要更有效的边界一致性算法来提升约束求解效率。

Method: 基于Ciré和van Hoeve定义的无重叠MDD，提取作业时间窗口边界来收紧开始和结束时间；通过限制MDD宽度为阈值创建松弛MDD，实现多项式时间复杂度的边界一致性过滤。

Result: 在带时间窗口的排序问题和准时制目标问题上，即使使用宽度阈值限制，新过滤算法相比Ciré和van Hoeve的优先检测算法能更显著减少搜索树访问节点数；与经典传播方法互补，在多个实例上同时减少节点数和求解时间。

Conclusion: 本文首次实现了无重叠约束的边界一致性算法，通过MDD方法在多项式时间内提供强过滤效果，为约束规划中的调度问题提供了更有效的求解技术。

Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Ciré and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Ciré and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.

</details>


### [14] [CI4A: Semantic Component Interfaces for Agents Empowering Web Automation](https://arxiv.org/abs/2601.14790)
*Zhi Qiu,Jiazheng Sun,Chenxiao Xia,Jun Zheng,Xin Peng*

Main category: cs.AI

TL;DR: CI4A是一种为AI智能体优化的组件接口框架，通过语义封装将复杂UI交互逻辑抽象为统一工具原语，显著提升了智能体在网页操作任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在高层语义规划方面表现出色，但在细粒度、低级别的网页组件操作方面存在局限。与其让智能体适应人类中心界面，不如构建专门为智能体优化的交互接口

Method: 提出CI4A（Component Interface for Agent）语义封装机制，将UI组件的复杂交互逻辑抽象为一组统一的工具原语。在Ant Design工业级前端框架中实现了23类常用UI组件的CI4A封装，并开发了具有动态更新动作空间的混合智能体

Result: 基于CI4A的智能体显著优于现有方法，在WebArena基准测试中达到了86.3%的新SOTA任务成功率，同时执行效率也有显著提升

Conclusion: CI4A框架通过为智能体专门设计优化的组件接口，有效解决了大语言模型在细粒度网页操作方面的局限性，为智能体与网页界面的高效交互提供了新范式

Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.

</details>


### [15] [Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies](https://arxiv.org/abs/2601.14827)
*Ben Schaper,Maxime Di Folco,Bernhard Kainz,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.AI

TL;DR: 该研究评估了视觉语言模型在胸部X光分类中的抽象错误，提出了基于医学分类学的分层评估方法，并通过风险约束阈值和分类学感知微调显著减少了严重错误。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在胸部X光分类中表现出强大的零样本性能，但标准的平面评估指标无法区分临床轻微错误和严重错误。需要量化并减轻抽象错误，以实现更安全、更具临床意义的模型部署。

Method: 使用分层指标对多个最先进的视觉语言模型进行基准测试，引入灾难性抽象错误来捕捉跨分支错误。提出风险约束阈值和基于径向嵌入的分类学感知微调方法。

Result: 结果显示，尽管视觉语言模型在平面性能上表现优异，但与临床分类学存在显著错位。提出的方法能将严重抽象错误降至2%以下，同时保持有竞争力的性能。

Conclusion: 分层评估和表示层面的对齐对于视觉语言模型更安全、更具临床意义的部署至关重要。提出的方法有效减少了严重错误，提高了模型的临床实用性。

Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.

</details>


### [16] [Implementing Knowledge Representation and Reasoning with Object Oriented Design](https://arxiv.org/abs/2601.14840)
*Abdelrhman Bassiouny,Tom Schierenbeck,Sorin Arion,Benjamin Alt,Naren Vasantakumaar,Giang Nguyen,Michael Beetz*

Main category: cs.AI

TL;DR: KRROOD是一个将知识表示与推理系统集成到面向对象编程中的框架，通过将知识作为一等编程抽象来弥合逻辑编程与OOP之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程主要使用面向对象编程开发复杂应用，而现有的知识表示与推理框架通常依赖外部本体和专门语言，难以与命令式代码集成，存在集成鸿沟。

Method: KRROOD通过将知识作为一等编程抽象，使用原生类结构来构建知识表示，从而在逻辑编程和面向对象编程范式之间建立桥梁。

Result: 在OWL2Bench基准测试和人类-机器人任务学习场景中的实验结果表明，KRROOD在保持强大性能的同时，支持现实世界自主系统所需的表达性推理能力。

Conclusion: KRROOD成功弥合了知识表示与推理系统与现代软件工程之间的集成鸿沟，为开发需要复杂推理能力的现实世界自主系统提供了有效解决方案。

Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.

</details>


### [17] [Just aware enough: Evaluating awareness across artificial systems](https://arxiv.org/abs/2601.14901)
*Nadine Meertens,Suet Lee,Ophelia Deroy*

Main category: cs.AI

TL;DR: 论文提出用"意识"替代"AI意识"作为评估AI系统的新框架，强调可操作性、领域敏感性、多维度和可预测性，旨在促进更科学的评估和讨论。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI意识和道德地位的讨论缺乏共识和可操作的评价方法，需要更实用、方法上可行的替代方案来评估AI系统的认知能力。

Method: 提出评估"意识"的实用方法，将意识定义为系统处理、存储和使用信息以实现目标导向行动的能力。该方法具有四个核心特征：领域敏感性、可扩展性、多维度和任务性能预测能力，同时保持能力层面的可比性。

Result: 建立了结构化方法来评估和比较不同架构、规模和操作领域的AI系统的意识特征，为系统评估提供了可操作的框架。

Conclusion: 从AI意识转向"足够意识"的评估框架，能够促进原则性评估、支持设计和监管，并推动更建设性的科学和公共讨论。

Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.

</details>


### [18] [The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution](https://arxiv.org/abs/2601.15075)
*Chen Qian,Peng Wang,Dongrui Liu,Junyao Yang,Dadi Guo,Ling Tang,Jilin Mei,Qihan Ren,Shuai Shao,Yong Liu,Jie Fu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 提出一个通用智能体归因框架，用于识别驱动智能体行为的内部因素，而不仅仅是失败归因


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在现实应用中的广泛部署，理解智能体为何采取特定行动对于问责和治理至关重要。现有研究主要关注失败归因，不足以解释智能体行为背后的推理过程

Method: 提出分层框架：在组件层面使用时序似然动态识别关键交互步骤；在句子层面使用基于扰动的分析来精确定位具体文本证据

Result: 实验验证表明，该框架能可靠地识别驱动智能体行为的关键历史事件和句子，涵盖标准工具使用和内存诱导偏差等可靠性风险场景

Conclusion: 该框架为构建更安全、更可问责的智能体系统迈出了关键一步，提供了超越失败归因的通用智能体行为解释方法

Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.

</details>


### [19] [The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks](https://arxiv.org/abs/2601.15130)
*Ivan Carrera,Daniel Maldonado-Ruiz*

Main category: cs.AI

TL;DR: 论文提出了"合理性陷阱"概念，指出人们过度使用大型语言模型处理简单确定性任务，导致资源浪费，并引入工具选择工程和决策矩阵来指导何时使用生成式AI。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，用户便利性超越了计算效率，导致人们使用昂贵的概率引擎处理简单的确定性任务（如OCR或基本验证），造成显著的资源浪费。

Method: 通过OCR和事实核查的微基准测试和案例研究，量化"效率税"；引入工具选择工程和确定性-概率性决策矩阵，帮助开发者决定何时使用生成式AI以及何时避免使用。

Result: 研究发现存在约6.5倍的延迟惩罚（效率税），并揭示了算法奉承的风险；提出了一个框架来指导开发者在适当场景下选择合适工具。

Conclusion: 真正的数字素养不仅在于知道如何使用生成式AI，更在于知道何时不使用它；需要课程转变，强调在适当场景下选择合适的工具。

Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the "Plausibility Trap": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the "efficiency tax"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.

</details>


### [20] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱路径奖励的bottom-up学习范式，通过监督微调和强化学习相结合的方法，让模型基于领域公理事实进行组合推理，在医学领域实现了从短跳推理到复杂多跳推理的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编程等结构化推理领域已达到接近专家的水平，但在专业科学领域进行组合式多跳推理的能力仍然有限。需要一种方法让模型能够基于领域公理事实进行组合推理，解决复杂未见任务。

Method: 提出bottom-up学习范式：1）基于知识图谱作为隐式奖励模型的后训练流程；2）结合监督微调和强化学习；3）从知识图谱路径推导新颖的奖励信号，提供可验证、可扩展、有基础的监督；4）鼓励模型在强化学习中组合中间公理而非仅优化最终答案。

Result: 在医学领域训练14B模型，在短跳推理路径（1-3跳）上训练，在复杂多跳查询（4-5跳）上进行零样本泛化评估。模型显著优于更大的模型和前沿系统（如GPT-5.2和Gemini 3 Pro），在最困难的推理任务上表现突出。方法对对抗性扰动（选项重排压力测试）具有鲁棒性。

Conclusion: 将推理过程建立在结构化知识基础上，是实现智能推理的可扩展且高效的途径。知识图谱路径奖励作为"组合桥梁"，使模型能够进行有效的多跳推理。

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


### [21] [BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries](https://arxiv.org/abs/2601.15197)
*Shijie Lian,Bin Yu,Xiaopeng Lin,Laurence T. Yang,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Cong Huang,Kai Chen*

Main category: cs.AI

TL;DR: 论文提出BayesianVLA框架解决VLA模型中的信息崩溃问题，通过贝叶斯分解强制模型遵循语言指令，在无需新数据的情况下显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作中存在泛化能力不足的问题，特别是在新指令或复杂多任务场景中。研究发现目标驱动的数据收集导致数据集偏差，使得语言指令仅从视觉观察即可高度预测，造成指令与动作之间的条件互信息消失（信息崩溃），导致模型退化为忽略语言约束的纯视觉策略。

Method: 提出BayesianVLA框架，通过可学习的潜在动作查询构建双分支架构：估计视觉先验p(a|v)和语言条件后验π(a|v,ℓ)。优化策略以最大化动作与指令之间的条件点互信息，惩罚视觉捷径，奖励明确解释语言命令的动作。

Result: 在SimplerEnv和RoboCasa上的大量实验显示显著改进，特别是在具有挑战性的OOD SimplerEnv基准上提升了11.3%，验证了该方法在将语言可靠地融入动作中的能力。

Conclusion: BayesianVLA通过贝叶斯分解有效解决了VLA模型中的信息崩溃问题，无需额外数据即可显著提升模型对语言指令的遵循能力和泛化性能。

Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)](https://arxiv.org/abs/2601.14298)
*Anjanava Biswas,Wrick Talukdar*

Main category: cs.CR

TL;DR: 本文提出了一种灵活自适应序列化机制，结合信任与安全模块，为LLM的开发部署提供安全护栏框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在生成式AI应用中的广泛应用，其带来的安全、隐私和伦理问题日益凸显。研究发现LLM存在泄露私人信息、产生虚假信息、被恶意利用等风险，亟需建立有效的安全防护机制。

Method: 提出了一种灵活自适应序列化机制，该机制集成了信任与安全模块，能够为LLM的开发和应用部署提供安全护栏。

Result: 论文提出了一个框架机制，但摘要中未提供具体的实验结果数据。

Conclusion: 开发和应用LLM时必须实施安全保障措施，提出的灵活自适应序列化机制为解决LLM的安全、隐私和伦理问题提供了可行的技术方案。

Abstract: The AI era has ushered in Large Language Models (LLM) to the technological forefront, which has been much of the talk in 2023, and is likely to remain as such for many years to come. LLMs are the AI models that are the power house behind generative AI applications such as ChatGPT. These AI models, fueled by vast amounts of data and computational prowess, have unlocked remarkable capabilities, from human-like text generation to assisting with natural language understanding (NLU) tasks. They have quickly become the foundation upon which countless applications and software services are being built, or at least being augmented with. However, as with any groundbreaking innovations, the rise of LLMs brings forth critical safety, privacy, and ethical concerns. These models are found to have a propensity to leak private information, produce false information, and can be coerced into generating content that can be used for nefarious purposes by bad actors, or even by regular users unknowingly. Implementing safeguards and guardrailing techniques is imperative for applications to ensure that the content generated by LLMs are safe, secure, and ethical. Thus, frameworks to deploy mechanisms that prevent misuse of these models via application implementations is imperative. In this study, wepropose a Flexible Adaptive Sequencing mechanism with trust and safety modules, that can be used to implement safety guardrails for the development and deployment of LLMs.

</details>


### [23] [DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing](https://arxiv.org/abs/2601.14302)
*Jinwei Hu,Shiyuan Meng,Yi Dong,Xiaowei Huang*

Main category: cs.CR

TL;DR: DDSA框架通过时域选择性和空间精确性优化对抗鲁棒性测试，在资源受限的实时应用中实现高效测试


<details>
  <summary>Details</summary>
Motivation: 资源关键应用中的图像传输处理系统面临对抗扰动的挑战，现有鲁棒性测试方法需要大量计算资源，在大规模部署中不实用

Method: 提出DDSA框架，包含场景感知触发函数（基于类别优先级和模型不确定性识别关键帧）和可解释AI技术（定位影响像素区域进行针对性扰动）

Result: 双域方法在保持攻击有效性的同时实现了显著的时空资源节约

Conclusion: 该框架使得在资源受限的实时应用中部署全面的对抗鲁棒性测试变得可行，计算效率直接影响任务成功

Abstract: Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.

</details>


### [24] [Tracing the Data Trail: A Survey of Data Provenance, Transparency and Traceability in LLMs](https://arxiv.org/abs/2601.14311)
*Richard Hohensinner,Belgin Mutlu,Inti Gabriel Mendoza Estrada,Matej Vukovic,Simone Kopeinik,Roman Kern*

Main category: cs.CR

TL;DR: 这篇综述论文系统回顾了过去十年关于大语言模型训练数据生命周期的研究，重点关注数据来源、透明度和可追溯性三个核心维度，以及偏见与不确定性、数据隐私、工具技术三个支撑支柱。


<details>
  <summary>Details</summary>
Motivation: 大语言模型已大规模部署，但其训练数据生命周期仍然不透明。作者旨在通过系统综述来阐明这一领域的研究现状，提出一个清晰的分类体系，帮助理解和管理LLM训练数据的完整生命周期。

Method: 作者对过去十年的95篇相关文献进行了系统性综述分析，提出了一个包含三个核心维度（数据来源、透明度、可追溯性）和三个支撑支柱（偏见与不确定性、数据隐私、工具技术）的分类法，并识别了关键方法论。

Result: 通过分析95篇文献，论文识别了数据生成、水印技术、偏见测量、数据管理、数据隐私等关键方法论，并揭示了透明度与不透明性之间的内在权衡关系。提出的分类法为该领域提供了系统的概念框架。

Conclusion: 这篇综述为理解大语言模型训练数据生命周期提供了全面的框架，强调了数据透明度的重要性，并指出了未来研究需要关注数据来源追踪、隐私保护与模型性能之间的平衡等关键问题。

Abstract: Large language models (LLMs) are deployed at scale, yet their training data life cycle remains opaque. This survey synthesizes research from the past ten years on three tightly coupled axes: (1) data provenance, (2) transparency, and (3) traceability, and three supporting pillars: (4) bias \& uncertainty, (5) data privacy, and (6) tools and techniques that operationalize them. A central contribution is a proposed taxonomy defining the field's domains and listing corresponding artifacts. Through analysis of 95 publications, this work identifies key methodologies concerning data generation, watermarking, bias measurement, data curation, data privacy, and the inherent trade-off between transparency and opacity.

</details>


### [25] [SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2601.14323)
*Bingxin Xu,Yuzhang Shang,Binghui Wang,Emilio Ferrara*

Main category: cs.CR

TL;DR: 本文提出SILENTDRIFT攻击方法，利用VLA模型中的动作分块和增量位姿表示机制的安全漏洞，通过C2连续扰动在轨迹边界保持零速度和加速度，实现隐蔽的后门攻击。


<details>
  <summary>Details</summary>
Motivation: VLA模型在安全关键机器人应用中部署日益增多，但其安全漏洞尚未充分研究。研究发现现代VLA系统中动作分块和增量位姿表示的结合会产生帧内视觉开环漏洞，使机器人执行K步动作序列，允许逐步扰动通过积分累积。

Method: 提出SILENTDRIFT攻击方法：1) 利用Smootherstep函数构建具有C2连续性的扰动，确保轨迹边界处速度和加速度为零，满足严格的运动学一致性约束；2) 采用关键帧攻击策略，仅毒化关键接近阶段，最大化影响同时最小化触发暴露。

Result: 在LIBERO数据集上评估，SILENTDRIFT达到93.2%的攻击成功率，毒化率低于2%，同时保持95.3%的清洁任务成功率。被毒化的轨迹在视觉上与成功演示无法区分。

Conclusion: VLA模型存在严重的安全漏洞，SILENTDRIFT攻击方法能够有效利用这些漏洞实现隐蔽的后门攻击，揭示了VLA系统在实际部署中面临的安全风险。

Abstract: Vision-Language-Action (VLA) models are increasingly deployed in safety-critical robotic applications, yet their security vulnerabilities remain underexplored. We identify a fundamental security flaw in modern VLA systems: the combination of action chunking and delta pose representations creates an intra-chunk visual open-loop. This mechanism forces the robot to execute K-step action sequences, allowing per-step perturbations to accumulate through integration. We propose SILENTDRIFT, a stealthy black-box backdoor attack exploiting this vulnerability. Our method employs the Smootherstep function to construct perturbations with guaranteed C2 continuity, ensuring zero velocity and acceleration at trajectory boundaries to satisfy strict kinematic consistency constraints. Furthermore, our keyframe attack strategy selectively poisons only the critical approach phase, maximizing impact while minimizing trigger exposure. The resulting poisoned trajectories are visually indistinguishable from successful demonstrations. Evaluated on the LIBERO, SILENTDRIFT achieves a 93.2% Attack Success Rate with a poisoning rate under 2%, while maintaining a 95.3% Clean Task Success Rate.

</details>


### [26] [Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs](https://arxiv.org/abs/2601.14340)
*Yiyang Lu,Jinwen He,Yue Zhao,Kai Chen,Ruigang Liang*

Main category: cs.CR

TL;DR: 本文提出了一种基于对话结构的新型后门攻击方法TST，利用对话轮次索引作为触发器，在多轮对话LLM系统中实现高成功率的攻击，且能绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在对话系统和任务助手等交互系统中的广泛应用，供应链风险日益凸显。现有后门攻击和防御主要关注用户可见的提示词触发器，而忽视了多轮对话中的结构信号，这构成了一个未被充分研究的攻击面。

Method: 提出了Turn-based Structural Trigger (TST)后门攻击方法，该方法使用对话轮次索引作为触发器，与用户输入内容无关。攻击通过对话结构激活，而非传统的文本触发器。

Result: 在四个广泛使用的开源LLM模型上，TST实现了平均99.52%的攻击成功率，且对模型实用性影响极小。在五种代表性防御机制下仍保持平均98.04%的攻击成功率，在不同指令数据集上平均攻击成功率为99.19%。

Conclusion: 对话结构是多轮LLM系统中一个重要且未被充分研究的攻击面，需要在实际应用中采用结构感知的审计和缓解措施来应对此类安全威胁。

Abstract: Large Language Models (LLMs) are widely integrated into interactive systems such as dialogue agents and task-oriented assistants. This growing ecosystem also raises supply-chain risks, where adversaries can distribute poisoned models that degrade downstream reliability and user trust. Existing backdoor attacks and defenses are largely prompt-centric, focusing on user-visible triggers while overlooking structural signals in multi-turn conversations. We propose Turn-based Structural Trigger (TST), a backdoor attack that activates from dialogue structure, using the turn index as the trigger and remaining independent of user inputs. Across four widely used open-source LLM models, TST achieves an average attack success rate (ASR) of 99.52% with minimal utility degradation, and remains effective under five representative defenses with an average ASR of 98.04%. The attack also generalizes well across instruction datasets, maintaining an average ASR of 99.19%. Our results suggest that dialogue structure constitutes an important and under-studied attack surface for multi-turn LLM systems, motivating structure-aware auditing and mitigation in practice.

</details>


### [27] [A Survey of Security Challenges and Solutions for Advanced Air Mobility and eVTOL Aircraft](https://arxiv.org/abs/2601.14415)
*Mahyar Ghazanfari,Iman Sharifi,Peng Wei,Noah Dahle,Abel Diaz Gonzalez,Austin Coursey,Bryce Bjorkman,Cailani Lemieux-Mack,Robert Canady,Abenezer Taye,Bryan C. Ward,Xenofon Koutsoukos,Gautam Biswas,Maheed H. Ahmed,Hyeong Tae Kim,Mahsa Ghasemi,Vijay Gupta,Filippos Fotiadis,Ufuk Topcu,Junchi Lu,Alfred Chen,Abdul Kareem Ras,Nischal Aryal,Amer Ibrahim,Amir Shirkhodaie,Heber Herencia-Zapana,Saqib Hasan,Isaac Amundson*

Main category: cs.CR

TL;DR: 本文综述了先进空中交通（AAM）系统（特别是eVTOL飞机）的安全漏洞与防御机制，分析了攻击分类、缓解策略，并提出了针对未来AAM生态系统的安全架构。


<details>
  <summary>Details</summary>
Motivation: 随着先进空中交通系统的快速发展，特别是电动垂直起降飞机的商业化应用，这些系统面临日益复杂的安全威胁。需要系统性地分析AAM系统的安全漏洞，借鉴商用航空和无人机的安全经验，为未来AAM生态系统建立全面的安全防护框架。

Method: 通过调查现有文献和实际案例，借鉴商用航空电子设备和自动化无人机系统的漏洞经验，建立攻击分类学，分析缓解策略，并提出专门针对AAM生态系统的安全系统架构。

Result: 识别了AAM系统的关键威胁向量，包括GPS干扰/欺骗、ATC无线电频率滥用、TCAS和ADS-B攻击、通过电子飞行包的后门攻击、飞机自动化和连接性引入的新漏洞，以及飞行管理系统软件、数据库和云服务的风险。同时描述了针对这些攻击的新兴防御技术。

Conclusion: 本文为AAM系统安全提供了全面的分析框架，指出了当前防御机制的不足，并提出了需要解决的关键技术问题，为未来AAM生态系统的安全设计和发展方向提供了重要参考。

Abstract: This survey reviews the existing and envisioned security vulnerabilities and defense mechanisms relevant to Advanced Air Mobility (AAM) systems, with a focus on electric vertical takeoff and landing (eVTOL) aircraft. Drawing from vulnerabilities in the avionics in commercial aviation and the automated unmanned aerial systems (UAS), the paper presents a taxonomy of attacks, analyzes mitigation strategies, and proposes a secure system architecture tailored to the future AAM ecosystem. The paper also highlights key threat vectors, including Global Positioning System (GPS) jamming/spoofing, ATC radio frequency misuse, attacks on TCAS and ADS-B, possible backdoor via Electronic Flight Bag (EFB), new vulnerabilities introduced by aircraft automation and connectivity, and risks from flight management system (FMS) software, database and cloud services. Finally, this paper describes emerging defense techniques against these attacks, and open technical problems to address toward better defense mechanisms.

</details>


### [28] [European digital identity: A missed opportunity?](https://arxiv.org/abs/2601.14503)
*Wouter Termont,Beatriz Esteves*

Main category: cs.CR

TL;DR: 论文批评欧盟数字身份(EUDI)法规及其OpenID架构存在概念狭窄、设计缺陷，无法实现真正的自主身份管理，可能走向重新中心化的监控系统。


<details>
  <summary>Details</summary>
Motivation: 欧盟数字身份(EUDI)法规及其OpenID架构虽然目标宏大，但基于对认证概念的狭隘理解，存在设计缺陷，无法实现真正的用户控制、隐私保护和便携性，反而可能形成新的中心化监控系统。

Method: 基于更广泛的认证概念分析OpenID4VCI和OpenID4VP架构，识别其安全实践问题、静态凭证类型限制、查询语言局限，并与现有解决方案(OpenID Connect、SIOPv2等)对比，同时批判其信任模型和制度化的可信列表机制。

Result: 发现OpenID架构存在不安全实践、静态绑定凭证类型、有限查询语言等问题，限制了动态、异步或自动化用例；其信任模型相比现有去中心化方案并未显著提升控制、隐私和便携性；制度化的可信列表可能形成排他性、重新中心化的生态系统。

Conclusion: 建议在EUDI法规修订前考虑技术替代方案，如OAuth的UMA扩展及其A4DS配置文件，以及GNAP集成；未来需要研究统一的查询(元)语言来解决证明和提供者的异质性问题，以实现真正的用户导向身份管理。

Abstract: Recent European efforts around digital identity -- the EUDI regulation and its OpenID architecture -- aim high, but start from a narrow and ill-defined conceptualization of authentication. Based on a broader, more grounded understanding of the term, in we identify several issues in the design of OpenID4VCI and OpenID4VP: insecure practices, static, and subject-bound credential types, and a limited query language restrict their application to classic scenarios of credential exchange -- already supported by existing solutions like OpenID Connect, SIOPv2, OIDC4IDA, and OIDC Claims Aggregation -- barring dynamic, asynchronous, or automated use cases. We also debunk OpenID's 'paradigm-shifting' trust-model, which -- when compared to existing decentralized alternatives -- does not deliver any significant increase in control, privacy, and portability of personal information. Not only the technical choices limit the capabilities of the EUDI framework; also the legislation itself cannot accommodate the promise of self-sovereign identity. In particular, we criticize the introduction of institutionalized trusted lists, and discuss their economical and political risks. Their potential to decline into an exclusory, re-centralized ecosystem endangers the vision of a user-oriented identity management in which individuals are in charge. Instead, the consequences might severely restrict people in what they can do with their personal information, and risk increased linkability and monitoring. In anticipation of revisions to the EUDI regulations, we suggest several technical alternatives that overcome some of the issues with the architecture of OpenID. In particular, OAuth's UMA extension and its A4DS profile, as well as their integration in GNAP, are worth looking into. Future research into uniform query (meta-)languages is needed to address the heterogeneity of attestations and providers.

</details>


### [29] [LLM Security and Safety: Insights from Homotopy-Inspired Prompt Obfuscation](https://arxiv.org/abs/2601.14528)
*Luis Lazo,Hamed Jelodar,Roozbeh Razavi-Far*

Main category: cs.CR

TL;DR: 提出基于同伦理论的提示词混淆框架，用于增强对大型语言模型安全漏洞的理解，通过系统化应用精心设计的提示词来影响模型的潜在行为。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全防护机制存在漏洞，需要更深入地理解其安全性和安全性弱点，以开发更强大的防御机制和可靠的检测策略。

Method: 采用同伦理论启发的提示词混淆框架，系统化应用精心设计的提示词来影响模型的潜在行为。实验涵盖了15,732个提示词（包括10,000个高优先级案例），在LLama、Deepseek、KIMI（代码生成）和Claude等多个模型上进行验证。

Result: 实验结果揭示了当前LLM安全防护机制的关键见解，表明现有防护措施存在不足，需要更强大的防御机制、可靠的检测策略和改进的韧性。

Conclusion: 这项工作为分析和缓解大型语言模型的潜在弱点提供了原则性框架，目标是推进安全、负责任和可信赖的人工智能技术发展。

Abstract: In this study, we propose a homotopy-inspired prompt obfuscation framework to enhance understanding of security and safety vulnerabilities in Large Language Models (LLMs). By systematically applying carefully engineered prompts, we demonstrate how latent model behaviors can be influenced in unexpected ways. Our experiments encompassed 15,732 prompts, including 10,000 high-priority cases, across LLama, Deepseek, KIMI for code generation, and Claude to verify. The results reveal critical insights into current LLM safeguards, highlighting the need for more robust defense mechanisms, reliable detection strategies, and improved resilience. Importantly, this work provides a principled framework for analyzing and mitigating potential weaknesses, with the goal of advancing safe, responsible, and trustworthy AI technologies.

</details>


### [30] [Automatically Tightening Access Control Policies with Restricter](https://arxiv.org/abs/2601.14582)
*Ka Lok Wu,Christa Jenkins,Scott D. Stolle,Omar Chowdhury*

Main category: cs.CR

TL;DR: Restricter：一种基于访问日志自动收紧访问控制策略的工具，通过减少策略规则允许的访问请求数量来增强安全性，同时不牺牲系统功能


<details>
  <summary>Details</summary>
Motivation: 访问控制策略配置困难，即使经验丰富的管理员也难以制定满足最小权限原则的有效策略，现实中有许多策略配置错误的实例，需要自动化工具来解决这个问题

Method: 提出Restricter工具，基于访问日志自动收紧每个（允许）策略规则，访问日志记录了已执行的访问请求及其对应的访问决策（允许或拒绝），在不牺牲系统功能的前提下减少策略规则允许的访问请求数量

Result: 为亚马逊Cedar策略语言实现了Restricter，并通过两个现实案例研究证明了其有效性

Conclusion: Restricter能够有效解决访问控制策略配置的痛点，通过自动化收紧策略来增强安全性，同时保持系统功能完整性

Abstract: Robust access control is a cornerstone of secure software, systems, and networks. An access control mechanism is as effective as the policy it enforces. However, authoring effective policies that satisfy desired properties such as the principle of least privilege is a challenging task even for experienced administrators, as evidenced by many real instances of policy misconfiguration. In this paper, we set out to address this pain point by proposing Restricter, which automatically tightens each (permit) policy rule of a policy with respect to an access log, which captures some already exercised access requests and their corresponding access decisions (i.e., allow or deny). Restricter achieves policy tightening by reducing the number of access requests permitted by a policy rule without sacrificing the functionality of the underlying system it is regulating. We implement Restricter for Amazon's Cedar policy language and demonstrate its effectiveness through two realistic case studies.

</details>


### [31] [IntelliSA: An Intelligent Static Analyzer for IaC Security Smell Detection Using Symbolic Rules and Neural Inference](https://arxiv.org/abs/2601.14595)
*Qiyue Mei,Michael Fu*

Main category: cs.CR

TL;DR: IntelliSA是一个结合符号规则与神经推理的IaC安全异味检测智能静态分析器，通过知识蒸馏训练小型学生模型，在保持高检测率的同时显著降低误报率


<details>
  <summary>Details</summary>
Motivation: 基础设施即代码（IaC）脚本中的错误配置可能广泛传播，导致严重系统停机和安全风险。现有基于符号规则的静态分析器虽然能检测安全异味，但往往产生过多误报，增加人工检查负担

Method: 提出IntelliSA智能静态分析器，集成符号规则与神经推理：首先应用符号规则进行过近似检测以获得广泛覆盖，然后使用神经推理过滤误报。采用知识蒸馏方法，用LLM教师生成伪标签训练紧凑的学生模型（比LLM小500倍以上）

Result: 在包含11,814行真实IaC代码和241个安全异味的人工标注数据集上评估，IntelliSA达到最高F1分数（83%），优于两个静态分析器和三个LLM基线（Claude-4、Grok-4、GPT-5），性能提升7-42%。同时具有最佳成本效益，检测60%安全异味仅需检查不到2%的代码库

Conclusion: IntelliSA通过结合符号规则与神经推理，有效解决了传统规则检测方法误报率高的问题，同时避免了直接依赖LLM API带来的成本、延迟、数据治理和部署限制，为IaC安全异味检测提供了高效实用的解决方案

Abstract: Infrastructure as Code (IaC) enables automated provisioning of large-scale cloud and on-premise environments, reducing the need for repetitive manual setup. However, this automation is a double-edged sword: a single misconfiguration in IaC scripts can propagate widely, leading to severe system downtime and security risks. Prior studies have shown that IaC scripts often contain security smells--bad coding patterns that may introduce vulnerabilities--and have proposed static analyzers based on symbolic rules to detect them. Yet, our preliminary analysis reveals that rule-based detection alone tends to over-approximate, producing excessive false positives and increasing the burden of manual inspection. In this paper, we present IntelliSA, an intelligent static analyzer for IaC security smell detection that integrates symbolic rules with neural inference. IntelliSA applies symbolic rules to over-approximate potential smells for broad coverage, then employs neural inference to filter false positives. While an LLM can effectively perform this filtering, reliance on LLM APIs introduces high cost and latency, raises data governance concerns, and limits reproducibility and offline deployment. To address the challenges, we adopt a knowledge distillation approach: an LLM teacher generates pseudo-labels to train a compact student model--over 500x smaller--that learns from the teacher's knowledge and efficiently classifies false positives. We evaluate IntelliSA against two static analyzers and three LLM baselines (Claude-4, Grok-4, and GPT-5) using a human-labeled dataset including 241 security smells across 11,814 lines of real-world IaC code. Experimental results show that IntelliSA achieves the highest F1 score (83%), outperforming baselines by 7-42%. Moreover, IntelliSA demonstrates the best cost-effectiveness, detecting 60% of security smells while inspecting less than 2% of the codebase.

</details>


### [32] [STEAD: Robust Provably Secure Linguistic Steganography with Diffusion Language Model](https://arxiv.org/abs/2601.14778)
*Yuang Qi,Na Zhao,Qiyi Yao,Benlong Wu,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: 本文提出了一种基于扩散语言模型（DLM）的鲁棒性可证明安全语言隐写方法，解决了传统基于自回归语言模型（ARM）方法在主动篡改攻击下的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的可证明安全语言隐写（PSLS）方法主要依赖自回归语言模型（ARM），但由于ARM的顺序生成特性，一旦隐写文本被篡改就会产生严重的错误传播，使得现有方法在主动篡改攻击下失效。因此需要开发能够抵抗篡改攻击的鲁棒性语言隐写方法。

Method: 提出基于扩散语言模型（DLM）的鲁棒性语言隐写方法。利用DLM能够部分并行生成文本的特性，找到适合隐写嵌入的鲁棒位置，并结合纠错编码。在隐写提取阶段引入两种纠错策略：伪随机纠错和邻域搜索纠错。

Result: 理论证明和实验结果表明，该方法具有安全性和鲁棒性。能够抵抗隐写文本分割中的标记歧义，并在一定程度上能够抵抗插入、删除和替换等标记级别的攻击。

Conclusion: 基于扩散语言模型的鲁棒性可证明安全语言隐写方法能够有效解决传统ARM方法在主动篡改攻击下的脆弱性问题，为安全隐蔽通信提供了新的解决方案。

Abstract: Recent provably secure linguistic steganography (PSLS) methods rely on mainstream autoregressive language models (ARMs) to address historically challenging tasks, that is, to disguise covert communication as ``innocuous'' natural language communication. However, due to the characteristic of sequential generation of ARMs, the stegotext generated by ARM-based PSLS methods will produce serious error propagation once it changes, making existing methods unavailable under an active tampering attack. To address this, we propose a robust, provably secure linguistic steganography with diffusion language models (DLMs). Unlike ARMs, DLMs can generate text in a partially parallel manner, allowing us to find robust positions for steganographic embedding that can be combined with error-correcting codes. Furthermore, we introduce error correction strategies, including pseudo-random error correction and neighborhood search correction, during steganographic extraction. Theoretical proof and experimental results demonstrate that our method is secure and robust. It can resist token ambiguity in stegotext segmentation and, to some extent, withstand token-level attacks of insertion, deletion, and substitution.

</details>


### [33] [On Implementing Hybrid Post-Quantum End-to-End Encryption](https://arxiv.org/abs/2601.14926)
*Aditi Gandhi,Aakankshya Das,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 实现了一个结合经典和抗量子密码原语的混合端到端加密系统，使用CRYSTALS-Kyber进行量子安全密钥交换，AES-256-GCM进行对称加密，SHA-256进行密钥派生，采用零信任架构。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现对当前公钥密码系统构成根本性威胁，需要在所有应用中过渡到抗量子密码替代方案。

Method: 采用混合加密架构：使用NIST标准化的基于格的密钥封装机制CRYSTALS-Kyber进行量子安全密钥交换，结合AES-256-GCM进行高效认证对称加密，SHA-256进行确定性密钥派生。系统采用零信任模型，中继服务器仅转发消息而不访问明文或密钥，所有加解密操作仅在客户端端点进行。

Result: 系统证明NIST标准化的后量子密码学可以有效地集成到实际消息系统中，具有可接受的性能特征，能够同时防御经典和量子攻击者。

Conclusion: NIST标准化的后量子密码学可以有效地集成到实际通信系统中，提供了针对经典和量子攻击者的保护。作者提供了开源实现以促进可重复性和进一步研究。

Abstract: The emergence of quantum computing poses a fundamental threat to current public key cryptographic systems. This threat is necessitating a transition to quantum resistant cryptographic alternatives in all the applications. In this work, we present the implementation of a practical hybrid end-to-end encryption system that combines classical and post-quantum cryptographic primitives to achieve both security and efficiency. Our system employs CRYSTALS-Kyber, a NIST-standardized lattice-based key encapsulation mechanism, for quantum-safe key exchange, coupled with AES-256-GCM for efficient authenticated symmetric encryption and SHA-256 for deterministic key derivation. The architecture follows a zero-trust model where a relay server facilitates communication without accessing plaintext messages or cryptographic keys. All encryption and decryption operations occur exclusively at client endpoints. The system demonstrates that NIST standardized post-quantum cryptography can be effectively integrated into practical messaging systems with acceptable performance characteristics, offering protection against both classical and quantum adversaries. As our focus is on implementation rather than on novelty, we also provide an open-source implementation to facilitate reproducibility and further research in post quantum secure communication systems.

</details>


### [34] [On the Effectiveness of Mempool-based Transaction Auditing](https://arxiv.org/abs/2601.14996)
*Jannik Albrecht,Ghassan Karame*

Main category: cs.CR

TL;DR: 分析比特币和以太坊中内存池审计与检测交易操纵攻击之间的关系，发现现有审计方案可能导致误指控，但特定条件下可有效审计交易执行


<details>
  <summary>Details</summary>
Motivation: 现有防御交易操纵攻击的方案尚未集成到主流区块链中，用户社区依赖内存池审计等临时解决方案。需要精确分析这些审计方案在检测恶意矿工攻击方面的有效性

Method: 首次精确分析比特币和以太坊中内存池审计与检测审查和交易置换攻击之间的相互作用。通过理论分析评估审计方案在不同条件下的表现

Result: 内存池审计在某些情况下可能导致超过25%的误指控概率。但若所有观察者一致接收交易且交易间隔至少30秒，审计方案能以99.9%的高概率成功审计任意两笔交易的执行

Conclusion: 研究发现批处理顺序公平排序方案在现实部署中只能为有限交易子集提供强公平性保证。内存池审计在特定条件下有效，但存在误指控风险

Abstract: While the literature features a number of proposals to defend against transaction manipulation attacks, existing proposals are still not integrated within large blockchains, such as Bitcoin, Ethereum, and Cardano. Instead, the user community opted to rely on more practical but ad-hoc solutions (such as Mempool.space) that aim at detecting censorship and transaction displacement attacks by auditing discrepancies in the mempools of so-called observers.
  In this paper, we precisely analyze, for the first time, the interplay between mempool auditing and the ability to detect censorship and transaction displacement attacks by malicious miners in Bitcoin and Ethereum. Our analysis shows that mempool auditing can result in mis-accusations against miners with a probability larger than 25% in some settings. On a positive note, however, we show that mempool auditing schemes can successfully audit the execution of any two transactions (with an overwhelming probability of 99.9%) if they are consistently received by all observers and sent at least 30 seconds apart from each other. As a direct consequence, our findings show, for the first time, that batch-order fair-ordering schemes can offer only strong fairness guarantees for a limited subset of transactions in real-world deployments.

</details>


### [35] [SpooFL: Spoofing Federated Learning](https://arxiv.org/abs/2601.15055)
*Isaac Baglin,Xiatian Zhu,Simon Hadfield*

Main category: cs.CR

TL;DR: 提出SpooFL防御方法，将联邦学习中的深度泄漏攻击防御重新定义为欺骗问题，通过生成与私有数据无关的合成样本来误导攻击者


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习防御方法主要依赖混淆、噪声、变换或加密，虽然有一定效果，但仍会泄漏类别分布或特征表示等高层信息，且容易被日益强大的去噪攻击破解

Method: 提出SpooFL欺骗防御框架，使用在外部数据集上训练的最先进生成模型，生成与私有数据类别无重叠的合成样本，欺骗攻击者使其误认为恢复了真实训练数据

Result: SpooFL成功误导攻击者恢复看似合理但完全无关的样本，防止有意义的数据泄漏，同时保持联邦学习训练完整性，模型性能未显著受损

Conclusion: 将联邦学习防御重新定义为欺骗问题是一种根本不同的视角，SpooFL通过生成无关合成样本有效防御深度泄漏攻击，比传统混淆方法更安全

Abstract: Traditional defenses against Deep Leakage (DL) attacks in Federated Learning (FL) primarily focus on obfuscation, introducing noise, transformations or encryption to degrade an attacker's ability to reconstruct private data. While effective to some extent, these methods often still leak high-level information such as class distributions or feature representations, and are frequently broken by increasingly powerful denoising attacks. We propose a fundamentally different perspective on FL defense: framing it as a spoofing problem.We introduce SpooFL (Figure 1), a spoofing-based defense that deceives attackers into believing they have recovered the true training data, while actually providing convincing but entirely synthetic samples from an unrelated task. Unlike prior synthetic-data defenses that share classes or distributions with the private data and thus still leak semantic information, SpooFL uses a state-of-the-art generative model trained on an external dataset with no class overlap. As a result, attackers are misled into recovering plausible yet completely irrelevant samples, preventing meaningful data leakage while preserving FL training integrity. We implement the first example of such a spoofing defense, and evaluate our method against state-of-the-art DL defenses and demonstrate that it successfully misdirects attackers without compromising model performance significantly.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [36] [JAXMg: A multi-GPU linear solver in JAX](https://arxiv.org/abs/2601.14466)
*Roeland Wiersema*

Main category: cs.DC

TL;DR: JAXMg为JAX提供多GPU密集线性代数支持，通过集成NVIDIA cuSOLVERMg实现跨GPU的Cholesky线性求解和对称特征分解，解决单GPU内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 现代科学计算中需要处理大规模密集线性系统和特征值问题，但现有编程框架难以扩展到多GPU环境。虽然有多GPU求解库，但难以集成到可组合的JIT编译Python工作流中。

Method: 通过XLA外部函数接口将JAX与NVIDIA cuSOLVERMg连接，将分布式GPU求解器暴露为JIT兼容的JAX原语，使可扩展线性代数能直接嵌入JAX程序。

Result: JAXMg实现了多GPU密集线性代数功能，支持超过单GPU内存限制的矩阵操作，保持了与JAX变换的可组合性，并能在端到端科学工作流中实现多GPU执行。

Conclusion: JAXMg成功将分布式GPU求解器集成到JAX生态系统中，为科学计算提供了可组合、可扩展的多GPU线性代数解决方案，填补了现有框架的空白。

Abstract: Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.

</details>


### [37] [Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI](https://arxiv.org/abs/2601.14608)
*Torben R. Lahnor,Mia Reitz,Jonas Posner,Patrick Diehl*

Main category: cs.DC

TL;DR: 该研究将Itoyori和ItoyoriFBC两个异步多任务运行时集成到Task Bench框架中，与MPI和HPX进行性能和生产力的综合评估，揭示了不同系统在效率和编程复杂度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 异步多任务运行时作为MPI的替代方案具有生产力优势，但多样化的AMT生态系统使得公平比较变得困难。研究旨在通过Task Bench框架对不同的并行编程系统进行系统化评估。

Method: 将Itoyori（基于PGAS和RDMA工作窃取）和ItoyoriFBC（扩展了基于future的同步）集成到Task Bench参数化评估框架中。评估包括性能指标（应用效率和最小有效任务粒度）和生产力指标（代码行数和库构造数量）。

Result: MPI在规则、通信轻量级工作负载中效率最高但代码冗长；HPX在不同节点数下负载不均衡时保持稳定效率，但生产力指标最差；Itoyori在通信密集型配置中效率最高且生产力最佳；ItoyoriFBC效率略低于Itoyori但为不规则工作负载提供了潜力。

Conclusion: 异步多任务运行时并不天然保证比MPI更高的生产力，不同系统在性能和编程复杂度之间存在明显权衡。Itoyori在通信密集型场景中表现最佳，而ItoyoriFBC为不规则工作负载提供了有价值的扩展。

Abstract: Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.
  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).
  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.

</details>


### [38] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://arxiv.org/abs/2601.14612)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 本文提出了一种用于混合云环境中具有硬截止时间作业的随机调度算法ROSS，相比现有确定性策略的Ω(K)竞争比，ROSS实现了最优的√K竞争比，并在真实云平台数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 混合云环境中，作业可以在成本较低但不稳定的spot实例或更昂贵的按需实例上运行，同时需要满足硬截止时间。现有确定性调度策略存在竞争比过高的问题，需要设计更优的调度算法来平衡成本优化和截止时间保证。

Method: 首先分析了现有确定性策略的理论极限，证明了最坏情况竞争比为Ω(K)。然后提出了随机调度算法ROSS，该算法在合理的截止时间约束下实现了√K的最优竞争比。

Result: ROSS算法在Azure和AWS的真实追踪数据上进行了广泛评估，结果显示ROSS能够有效平衡成本优化和截止时间保证，相比现有最优方法在成本节省方面提升了高达30%，且在不同spot市场条件下表现稳定。

Conclusion: ROSS算法在混合云环境中的截止时间感知在线调度问题上取得了显著进展，通过随机化策略实现了理论上的最优竞争比，并在实际应用中验证了其优越性能，为云成本优化提供了有效解决方案。

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\%$ in cost savings, across diverse spot market conditions.

</details>


### [39] [Specifying and Verifying RDMA Synchronisation (Extended Version)](https://arxiv.org/abs/2601.14642)
*Guillaume Ambal,Max Stupple,Brijesh Dongol,Azalea Raad*

Main category: cs.DC

TL;DR: 该论文提出了RDMA在TSO架构下的远程RMW操作语义，填补了现有RDMA语义缺乏远程同步原语的空白，并构建了可组合的同步抽象库和三类远程锁的实现与验证。


<details>
  <summary>Details</summary>
Motivation: 现有的RDMA语义（RDMA^TSO）虽然描述了RDMA在TSO CPU上的行为，但缺乏对远程同步操作的形式化定义，导致无法验证锁等常见同步抽象的正确实现。需要填补这一空白以支持分布式系统中同步原语的验证。

Method: 1. 提出RDMA^TSO_RMW语义，首次形式化定义了TSO架构下的远程"读-修改-写"操作语义；2. 构建RDMA^WAIT_RMW库作为可组合同步抽象的基础；3. 基于该库设计、实现并验证三类适用于不同场景的远程锁；4. 提出类似顺序一致性的强RDMA模型RDMA^SC_RMW；5. 确保与现有高性能库LOCO的兼容性。

Result: 1. 发现远程RMW操作相对较弱，仅能保证对其他远程RMW操作的原子性；2. 成功构建了可组合的同步抽象库；3. 实现了三类经过验证的远程锁，适用于不同应用场景；4. 建立了与现有高性能库的兼容性，确保可组合性和可验证性。

Conclusion: 该研究填补了RDMA语义中远程同步原语的空白，为分布式系统中同步抽象的正确性验证提供了理论基础和实用工具，同时保持了与现有高性能库的兼容性，为构建可靠的高性能分布式系统奠定了基础。

Abstract: Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\text{RDMA}^\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\text{RDMA}^{\text{TSO}}_{\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$ library. Underpinned by $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\text{RDMA}^{\text{SC}}_{\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.

</details>


### [40] [Optimizing FaaS Platforms for MCP-enabled Agentic Workflows](https://arxiv.org/abs/2601.14735)
*Varad Kulkarni,Vaibhav Jha,Nikhil Reddy,Yogesh Simmhan*

Main category: cs.DC

TL;DR: FAME是一个基于FaaS的架构，用于编排支持MCP的智能体工作流，通过将智能体模式分解为可组合的FaaS函数，解决了云部署和状态管理的挑战，实现了显著的性能提升和成本节约。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM和MCP服务器的自主AI智能体工作流的快速发展，传统的VM部署方式资源密集且缺乏弹性，而FaaS平台虽然提供模块化和成本效益，但本质上是无状态的，这给可扩展的云部署和状态管理带来了挑战。

Method: FAME将智能体模式（如ReAct）分解为可组合的智能体（Planner、Actor、Evaluator），每个智能体都是使用LangGraph构建的FaaS函数，并作为FaaS工作流进行编排。使用DynamoDB实现跨用户请求的上下文持久化，通过AWS Lambda包装器优化MCP服务器部署，在S3中缓存工具输出，并提出函数融合策略。

Result: 在两个代表性应用（研究论文摘要和日志分析）上评估FAME，结果显示：延迟降低高达13倍，输入令牌减少88%，成本节约66%，同时工作流完成率得到改善。

Conclusion: FAME证明了无服务器平台在规模化托管复杂、多智能体AI工作流方面的可行性，通过模块化架构、状态管理和优化策略，实现了显著的性能提升和成本效益。

Abstract: Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.

</details>


### [41] [Application-level observability for adaptive Edge to Cloud continuum systems](https://arxiv.org/abs/2601.14923)
*Kaddour Sidi,Daniel Balouek,Baptiste Jonglez*

Main category: cs.DC

TL;DR: 本文提出了一个应用级可观测性框架，用于现代边缘到云系统，通过集成开发者驱动的仪器化和SLO感知反馈实现自主适应，使用视频处理案例验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代边缘到云系统需要细粒度的可观测性来确保在异构动态环境中的自适应行为和性能目标合规性。

Method: 引入应用级可观测性框架，集成开发者驱动的仪器化和SLO感知反馈，结合OpenTelemetry、Prometheus、K3s和Chaos Mesh实现实时监控和自适应控制。

Result: 视频处理案例表明应用级指标能够指导自动调整以维持目标帧率、延迟和检测精度，初步结果显示改善了可扩展性、容错性和响应性。

Conclusion: 该框架为自适应、符合SLO的边缘到云应用提供了实用基础，通过实时监控和自适应控制确保性能目标在变化工作负载和故障下的合规性。

Abstract: Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [42] [End-to-End Transformer Acceleration Through Processing-in-Memory Architectures](https://arxiv.org/abs/2601.14260)
*Xiaoxuan Yang,Peilin Chen,Tergel Molom-Ochir,Yiran Chen*

Main category: cs.AR

TL;DR: 本文针对Transformer模型在大规模部署中的三大挑战（注意力机制计算开销大、KV缓存内存瓶颈、注意力复杂度高），提出了内存内处理解决方案，通过重构计算、动态压缩KV缓存和重新解释注意力为关联记忆操作来提升能效和降低延迟。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理和大语言模型中占据核心地位，但其大规模部署面临三个主要挑战：1）注意力机制需要大量矩阵乘法和中间结果在内存与计算单元间的频繁移动，导致高延迟和高能耗；2）长上下文推理中KV缓存可能超过模型权重大小，造成严重的内存和带宽瓶颈；3）注意力相对于序列长度的二次复杂度放大了数据移动和计算开销，使大规模推理效率低下。

Method: 提出了内存内处理解决方案：1）重构注意力和前馈计算以最小化片外数据传输；2）动态压缩和剪枝KV缓存以管理内存增长；3）将注意力重新解释为关联记忆操作以降低复杂度和硬件占用。同时评估了该内存内处理设计与最先进加速器和通用GPU的性能对比。

Result: 与最先进的加速器和通用GPU相比，该内存内处理设计在能效和延迟方面显示出显著改进，有效解决了计算开销、内存可扩展性和注意力复杂度问题。

Conclusion: 通过内存内处理解决方案，能够实现Transformer模型的高效端到端加速，解决了大规模部署中的计算、内存和复杂度挑战，为Transformer模型的广泛应用提供了可行的硬件优化路径。

Abstract: Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models.

</details>


### [43] [Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability](https://arxiv.org/abs/2601.14347)
*George Rafael Gourdoumanis,Fotoini Oikonomou,Maria Pantazi-Kypraiou,Pavlos Stoikos,Olympia Axelou,Athanasios Tziouvaras,Georgios Karakonstantis,Tahani Aladwani,Christos Anagnostopoulos,Yixian Shen,Anuj Pathania,Alberto Garcia-Ortiz,George Floros*

Main category: cs.AR

TL;DR: 该论文介绍了COIN-3D项目，旨在通过开发开源EDA工具来提升2.5D/3D VLSI系统的可靠性分析能力，以应对半导体工艺向亚纳米级和GAAFETs转型带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造从3纳米工艺向亚纳米级发展，并从FinFETs转向GAAFETs，制造复杂性和挑战不断增加。3D chiplet方法成为解决这些限制并利用扩展设计空间的关键途径，但需要解决可靠性评估问题。

Method: 通过Horizon Europe Twinning项目COIN-3D，在欧洲领先机构间开展合作，开发新颖的开源电子设计自动化（EDA）工具，集成先进的物理级和系统级可靠性分析算法。

Result: 该项目旨在提供用于3D系统可靠性评估的开源EDA工具，但目前处于项目介绍阶段，具体实施结果尚未在摘要中说明。

Conclusion: COIN-3D项目通过跨机构合作开发开源可靠性分析工具，为应对先进半导体工艺和3D chiplet系统带来的可靠性挑战提供了重要解决方案，有助于推动异构系统设计的发展。

Abstract: As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis.

</details>
