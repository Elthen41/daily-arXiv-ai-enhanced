<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE是一个新颖的LLM压缩框架，首次将形式化方法（信号时序逻辑）引入压缩过程，在保证语言属性满足形式化约束的前提下，实现最高3.3倍计算成本降低和68.8%模型大小压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现出色，但计算资源需求大，难以部署在资源受限的边缘设备上。现有的压缩技术（如量化和剪枝）通常会损害关键的语言属性，且缺乏保证模型行为的形式化验证。

Method: 提出TOGGLE框架，利用信号时序逻辑（STL）形式化指定和强制执行语言属性。采用STL鲁棒性引导的贝叶斯优化，系统探索层级的量化和剪枝配置，生成满足形式化语言约束的压缩模型，无需重新训练或微调。

Result: 在四种LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B、Mistral 7B）上评估，实现了最高3.3倍计算成本（FLOPs）降低和最高68.8%模型大小压缩，同时满足所有语言属性约束。

Conclusion: TOGGLE首次将形式化方法集成到LLM压缩中，实现了高效、可验证的LLM边缘部署，为资源受限环境下的模型压缩提供了形式化保证的新途径。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [2] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协作实现，而非单一AGI系统，因此需要建立分布式的AGI安全框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AI系统的保护，假设最终会出现单一的通用人工智能。但另一种可能性是通用能力首先通过多个子AGI智能体的协调协作实现，这种"拼凑式AGI"假说需要被认真考虑并制定相应的安全保障措施。

Method: 提出分布式AGI安全框架，超越对单个智能体的评估和对齐。该框架核心是设计和实施虚拟智能体沙盒经济（不可渗透或半渗透），其中智能体间的交易由强大的市场机制管理，并配合适当的可审计性、声誉管理和监督，以减轻集体风险。

Result: 论文提出了一个系统性的框架来应对拼凑式AGI可能带来的安全挑战，强调需要通过经济机制、审计和监管来管理多个智能体协作产生的集体风险。

Conclusion: 随着具有工具使用能力和协调能力的先进AI智能体的快速部署，考虑拼凑式AGI假说并建立相应的分布式安全框架已成为紧迫的安全需求，这需要超越传统的单一智能体对齐方法。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [3] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 论文提出了"社会责任堆栈"框架，将社会价值观嵌入AI系统作为显式约束，通过六层架构实现AI系统的责任治理。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在影响人类行为、机构决策和社会结果方面日益重要，但现有的负责任AI和治理努力缺乏可执行的工程机制，无法在整个系统生命周期中有效运作。

Method: 引入社会责任堆栈框架，这是一个六层架构框架，将社会价值观作为显式约束、保障措施、行为接口、审计机制和治理流程嵌入AI系统。将责任建模为社会技术系统的闭环监督控制问题，整合设计时保障与运行时监控和机构监督。

Result: 开发了统一的基于约束的表述，引入了安全包络和反馈解释，展示了如何持续监控和执行公平性、自主性、认知负担和解释质量。通过临床决策支持、协作自动驾驶车辆和公共部门系统的案例研究，展示了SRS如何将规范性目标转化为可操作的工程和运营控制。

Conclusion: 该框架连接了伦理学、控制理论和AI治理，为可问责、自适应和可审计的社会技术AI系统提供了实用基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


### [4] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 提出Generative Adversarial Reasoner框架，通过对抗强化学习联合训练推理器和判别器，提升LLM数学推理能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理中仍存在计算错误、逻辑脆弱和表面合理但无效步骤等问题，需要改进推理过程的质量

Method: 采用生成对抗推理器框架，将推理链划分为逻辑完整的切片，通过判别器评估每个切片的合理性，结合推理器生成正确步骤和判别器检测错误的互补信号进行对抗强化学习

Result: 在多个数学基准测试中取得显著提升，特别是在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3，DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7

Conclusion: 提出的对抗强化学习框架通过密集、校准良好的步骤级奖励信号，改善了信用分配，提高了样本效率，增强了LLM的整体推理质量，且模块化判别器支持灵活奖励塑造

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>
