<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 13]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 33]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Towards Contextual Sensitive Data Detection](https://arxiv.org/abs/2512.04120)
*Liang Telkamp,Madelon Hulsebos*

Main category: cs.CR

TL;DR: 本文提出基于上下文的敏感数据检测方法，通过类型上下文化和领域上下文化两种机制，利用大语言模型显著提升敏感数据检测的准确性和召回率。


<details>
  <summary>Details</summary>
Motivation: 开放数据门户的兴起需要更好的敏感数据保护方法。现有敏感数据检测方法主要关注个人隐私数据，但敏感数据的定义需要根据上下文进行细化和扩展，因为数据的敏感性取决于其具体使用场景。

Method: 提出两种上下文敏感数据检测机制：1) 类型上下文化 - 先检测数据值的语义类型，然后考虑其在数据集或文档中的整体上下文；2) 领域上下文化 - 基于从相关文档中检索的敏感数据规则（如数据主题和地理来源），在更广泛的上下文中确定数据集的敏感性。两种机制都利用大语言模型辅助实现。

Result: 实验表明：1) 类型上下文化显著减少了基于类型的敏感数据检测的误报，召回率达到94%，而商业工具仅为63%；2) 领域上下文化在非标准数据领域（如人道主义数据集）中有效进行基于上下文的敏感数据检测。专家评估还显示，基于上下文的LLM解释在手动数据审计过程中提供了有用指导，提高了审计一致性。

Conclusion: 基于上下文的敏感数据检测方法能够更准确地识别敏感数据，特别是在复杂和非标准的数据领域中。该方法通过开源实现和标注数据集，为数据发布前的敏感数据保护提供了实用工具。

Abstract: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that con- sider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.

</details>


### [2] [Tipping the Dominos: Topology-Aware Multi-Hop Attacks on LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2512.04129)
*Ruichao Liang,Le Yin,Jing Chen,Cong Wu,Xiaoyu Zhang,Huangpeng Gu,Zijian Zhang,Yang Liu*

Main category: cs.CR

TL;DR: TOMA是一种针对LLM多智能体系统的拓扑感知多跳攻击方案，通过优化污染传播和控制对抗载荷的多跳扩散，揭示新的攻击向量，无需特权访问或直接操控智能体。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的安全评估局限于有限攻击场景，导致安全问题不清晰且可能被低估，需要更全面的安全分析。

Method: 提出TOMA攻击方案，利用MAS拓扑结构优化污染传播，控制环境起源的对抗载荷的多跳扩散，无需特权访问或直接操控智能体。

Result: 在三种先进MAS架构和五种代表性拓扑上实现40%-78%的攻击成功率，揭示了现有研究可能忽视的内在漏洞。

Conclusion: 基于拓扑信任的概念性防御框架能有效阻止94.8%的自适应和复合攻击，为MAS安全提供了新思路。

Abstract: LLM-based multi-agent systems (MASs) have reshaped the digital landscape with their emergent coordination and problem-solving capabilities. However, current security evaluations of MASs are still confined to limited attack scenarios, leaving their security issues unclear and likely underestimated. To fill this gap, we propose TOMA, a topology-aware multi-hop attack scheme targeting MASs. By optimizing the propagation of contamination within the MAS topology and controlling the multi-hop diffusion of adversarial payloads originating from the environment, TOMA unveils new and effective attack vectors without requiring privileged access or direct agent manipulation. Experiments demonstrate attack success rates ranging from 40% to 78% across three state-of-the-art MAS architectures: \textsc{Magentic-One}, \textsc{LangManus}, and \textsc{OWL}, and five representative topologies, revealing intrinsic MAS vulnerabilities that may be overlooked by existing research. Inspired by these findings, we propose a conceptual defense framework based on topology trust, and prototype experiments show its effectiveness in blocking 94.8% of adaptive and composite attacks.

</details>


### [3] [Primitive Vector Cipher(PVC): A Hybrid Encryption Scheme based on the Vector Computational Diffie-Hellman (V-CDH) Problem](https://arxiv.org/abs/2512.04237)
*Gülçin ÇİVİ BİLİR*

Main category: cs.CR

TL;DR: PVC是一种新型混合加密方案，结合矩阵密码学和改进的Diffie-Hellman密钥交换，基于V-CDH问题硬度，采用两层设计提供强安全性和并行处理能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种结合矩阵密码学和Diffie-Hellman密钥交换的新型加密方案，解决传统加密中确定性重复问题，增强对线性攻击和已知明文攻击的抵抗力。

Method: 采用两层设计：第一层使用HKDF通过DH认证的共享原始向量掩蔽明文；第二层使用每块偏移随机化密文块。结合STS协议提升安全性。

Result: PVC消除了确定性重复，提供对线性和已知明文攻击的强抵抗力，支持大规模并行处理和优秀线性扩展，在V-CDH下证明具有IND-CPA安全性。

Conclusion: PVC是一种安全高效的混合加密方案，结合矩阵密码学和Diffie-Hellman密钥交换，具有强安全性和并行处理能力，通过STS协议可达到IND-CCA安全级别。

Abstract: This work introduces the Primitive Vector Cipher (PVC), a novel hybrid encryption scheme integrating matrix-based cryptography with advanced Diffie-Hellman key exchange. PVC's security is grounded on the established hardness of the Vector Computational Diffie- Hellman (V-CDH) problem. The two-layered design uses HKDF to mask plaintext via a DH-authenticated shared primitive vector and randomize cipher blocks with a per-block offset. This approach eliminates deterministic repetitions and provides strong resistance against linear and known-plaintext attacks. PVC's block-wise structure allows for massive parallelism and excellent linear scaling. Security is formally analyzed, demonstrating INDCPA security under V-CDH. STS protocol integration elevates security toward IND-CCA guarantees.

</details>


### [4] [Hey GPT-OSS, Looks Like You Got It - Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics](https://arxiv.org/abs/2512.04254)
*Gaëtan Michelet,Janine Schneider,Aruna Withanage,Frank Breitinger*

Main category: cs.CR

TL;DR: 本文首次探讨了推理语言模型在数字取证中的应用潜力，通过四个测试用例评估推理组件对结果可解释性的支持效果，发现中等推理水平有助于解释和验证模型输出，但支持有限且更高推理水平不会提升响应质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数字取证中应用广泛，但有限的结果可解释性降低了其操作和法律可用性。虽然新型推理语言模型能够处理逻辑任务，但用户通常只能看到最终答案而非底层推理过程。gpt-oss等可本地部署的推理模型提供了访问其底层推理过程的机会，这为提升数字取证结果的可解释性提供了可能。

Method: 研究采用gpt-oss推理语言模型，设计了四个测试用例来评估推理组件在支持结果可解释性方面的可用性。评估结合了新的定量指标和定性分析，考察不同推理水平下模型的表现。

Result: 研究发现推理组件在中等推理水平下有助于解释和验证数字取证中语言模型的输出，但这种支持通常是有限的。更重要的是，更高的推理水平并不会增强响应质量，表明推理深度与结果质量之间不存在正相关关系。

Conclusion: 推理语言模型在数字取证中具有提升结果可解释性的潜力，特别是在中等推理水平下。然而，当前的支持仍然有限，且增加推理深度不会自动改善响应质量。这为未来优化推理模型在数字取证中的应用提供了方向。

Abstract: The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.

</details>


### [5] [WildCode: An Empirical Analysis of Code Generated by ChatGPT](https://arxiv.org/abs/2512.04259)
*Kobra Khanmohammadi,Pooria Roy,Raphael Khoury,Abdelwahab Hamou-Lhadj,Wilfried Patrick Konan*

Main category: cs.CR

TL;DR: 对ChatGPT生成的真实代码进行大规模实证分析，发现LLM生成的代码在安全性方面存在不足，且用户很少关注代码安全特性


<details>
  <summary>Details</summary>
Motivation: LLM模型越来越多地用于生成代码，但其代码质量和安全性存在不确定性。现有研究大多使用专门为研究生成的代码，这引发了实验真实性的问题。本研究旨在分析ChatGPT生成的真实代码，评估其正确性和安全性，并探究用户请求代码的意图。

Method: 对ChatGPT生成的真实代码进行大规模实证分析，评估代码的正确性和安全性，并深入分析用户请求代码时的意图和关注点。

Result: 研究证实了先前使用合成查询的研究结果：LLM生成的代码在安全性方面通常不足。同时发现用户对请求LLM生成的代码的安全特性表现出很少的好奇心，从他们缺乏相关查询可以看出这一点。

Conclusion: LLM生成的代码存在安全隐患，且用户安全意识不足。这强调了在AI代码生成工具中加强安全考虑和用户教育的重要性。

Abstract: LLM models are increasingly used to generate code, but the quality and security of this code are often uncertain. Several recent studies have raised alarm bells, indicating that such AI-generated code may be particularly vulnerable to cyberattacks. However, most of these studies rely on code that is generated specifically for the study, which raises questions about the realism of such experiments. In this study, we perform a large-scale empirical analysis of real-life code generated by ChatGPT. We evaluate code generated by ChatGPT both with respect to correctness and security and delve into the intentions of users who request code from the model. Our research confirms previous studies that used synthetic queries and yielded evidence that LLM-generated code is often inadequate with respect to security. We also find that users exhibit little curiosity about the security features of the code they ask LLMs to generate, as evidenced by their lack of queries on this topic.

</details>


### [6] [Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks](https://arxiv.org/abs/2512.04260)
*Gaoning Pan,Yiming Tao,Qinying Wang,Chunming Wu,Mingde Hu,Yizhi Ren,Shouling Ji*

Main category: cs.CR

TL;DR: 该论文提出了一种新的虚拟机攻击方法——跨域攻击(CDA)，利用现代虚拟化环境中弱内存隔离的特性，通过受攻击者控制的访客内存来绕过ASLR等安全机制，实现了对虚拟机管理程序的系统性利用。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟机管理程序面临严重的内存安全漏洞威胁，特别是指针损坏漏洞。传统利用框架依赖识别主机中的特定结构并确定其运行时地址，这在虚拟机环境中效果不佳，因为相关结构稀少且受到ASLR的进一步混淆。

Method: 基于现代虚拟化环境中弱内存隔离的观察（访客内存完全由攻击者控制但主机可访问），提出了跨域攻击(CDA)的系统性分类和分类学。开发了一个自动化系统，用于识别跨域小工具、匹配损坏的指针、合成触发输入并组装完整的利用链。

Result: 在QEMU和VirtualBox的15个真实漏洞上进行评估，结果显示CDA方法具有广泛的适用性和有效性，能够成功利用这些漏洞。

Conclusion: 跨域攻击(CDA)为虚拟机管理程序漏洞利用提供了一种有效的新方法，揭示了现代虚拟化环境中弱内存隔离带来的安全风险，并提供了自动化的利用链构建系统。

Abstract: Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.

</details>


### [7] [One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises](https://arxiv.org/abs/2512.04338)
*Biagio Montaruli,Luca Compagna,Serena Elisa Ponta,Davide Balzarotti*

Main category: cs.CR

TL;DR: 提出了一种针对恶意Python包的鲁棒检测器，能够对抗源代码混淆攻击，并适应不同用户（如PyPI维护者和企业安全团队）对误报率的不同要求。


<details>
  <summary>Details</summary>
Motivation: 当前供应链攻击通过恶意Python包日益严重，现有检测方法存在两个关键问题：1) 对对抗性源代码变换的鲁棒性不足；2) 无法适应不同参与者（从要求低误报率的仓库维护者到容忍较高误报率的企业安全团队）的多样化需求。

Method: 1) 提出使用细粒度代码混淆生成对抗性包的方法；2) 结合对抗训练提升检测器鲁棒性；3) 设计可适应不同误报率要求的检测器配置；4) 在PyPI上收集122,398个包进行80天评估；5) 通过两个案例研究展示生产适应性。

Result: 1) 对抗训练使检测器鲁棒性提升2.5倍，能多发现10%的混淆包，但对非混淆包性能略有下降；2) PyPI维护者配置（0.1%误报率）：37天内分析91,949个包，每日检测2.48个恶意包，仅2.18个误报；3) 企业配置（10%误报率）：分析1,596个公司采用包，每日仅1.24个误报；4) 总共发现346个恶意包并报告给社区。

Conclusion: 该检测器能够无缝集成到PyPI等公共仓库和企业生态系统中，通过对抗训练显著提升对混淆攻击的鲁棒性，并能根据不同用户需求调整误报率，实际应用中只需几分钟时间审查误报结果。

Abstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).
  We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.
  We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.
  Overall, we uncovered 346 malicious packages, now reported to the community.

</details>


### [8] [ReFuzz: Reusing Tests for Processor Fuzzing with Contextual Bandits](https://arxiv.org/abs/2512.04436)
*Chen Chen,Zaiyan Xu,Mohamadreza Rostami,David Liu,Dileep Kalathil,Ahmad-Reza Sadeghi,Jeyavijayan,Rajendran*

Main category: cs.CR

TL;DR: ReFuzz是一个基于上下文老虎机的自适应硬件模糊测试框架，通过重用先前处理器中的高效测试来检测新处理器中的相似漏洞变体，实现了511.23倍的平均覆盖率加速和9.33%的总覆盖率提升。


<details>
  <summary>Details</summary>
Motivation: 处理器设计通常基于迭代修改和重用现有设计，这导致相似漏洞在不同处理器中传播。现有硬件模糊测试方法缺乏利用已知漏洞知识来优化测试的能力，无法有效检测相似或新的漏洞变体。

Method: 提出ReFuzz自适应模糊测试框架，采用上下文老虎机算法，智能重用先前处理器中触发漏洞的高效测试，针对目标处理器进行变异和测试，以检测相似和新变体漏洞。

Result: 发现了3个新的安全漏洞和2个新的功能bug，其中一个漏洞通过重用已知漏洞测试检测到，一个bug存在于共享设计模块的三个处理器中。相比现有模糊测试器，实现了平均511.23倍的覆盖率加速和最高9.33%的总覆盖率提升。

Conclusion: ReFuzz通过重用先前处理器中的高效测试，能够有效检测相似和新变体漏洞，显著提升硬件模糊测试的效率和覆盖率，为处理器安全验证提供了新方法。

Abstract: Processor designs rely on iterative modifications and reuse well-established designs. However, this reuse of prior designs also leads to similar vulnerabilities across multiple processors. As processors grow increasingly complex with iterative modifications, efficiently detecting vulnerabilities from modern processors is critical. Inspired by software fuzzing, hardware fuzzing has recently demonstrated its effectiveness in detecting processor vulnerabilities. Yet, to our best knowledge, existing processor fuzzers fuzz each design individually, lacking the capability to understand known vulnerabilities in prior processors to fine-tune fuzzing to identify similar or new variants of vulnerabilities.
  To address this gap, we present ReFuzz, an adaptive fuzzing framework that leverages contextual bandit to reuse highly effective tests from prior processors to fuzz a processor-under-test (PUT) within a given ISA. By intelligently mutating tests that trigger vulnerabilities in prior processors, ReFuzz effectively detects similar and new variants of vulnerabilities in PUTs. ReFuzz uncovered three new security vulnerabilities and two new functional bugs. ReFuzz detected one vulnerability by reusing a test that triggers a known vulnerability in a prior processor. One functional bug exists across three processors that share design modules. The second bug has two variants. Additionally, ReFuzz reuses highly effective tests to enhance efficiency in coverage, achieving an average 511.23x coverage speedup and up to 9.33% more total coverage, compared to existing fuzzers.

</details>


### [9] [A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution](https://arxiv.org/abs/2512.04580)
*Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin*

Main category: cs.CR

TL;DR: CryptoTensors是一种基于Safetensors格式的扩展文件结构，通过张量级加密和嵌入式访问控制策略，为大型语言模型提供安全的机密分发解决方案，同时保持格式兼容性和低开销。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在医疗、法律、金融等敏感领域应用的增加，使用敏感数据进行私有定制或微调变得普遍。这些私有适配的LLMs被视为个人隐私资产或企业知识产权，但现有模型格式和部署框架缺乏对机密性、访问控制或可信硬件安全集成的内置支持。现有安全部署方法要么依赖计算昂贵的加密技术，要么需要严格控制的私有基础设施，难以广泛部署。

Method: 提出CryptoTensors作为Safetensors格式的扩展文件结构，包含张量级加密和嵌入式访问控制策略，同时保留延迟加载和部分反序列化等关键特性。支持透明解密和自动化密钥管理，实现灵活的许可和安全模型执行。开发了概念验证库，并在序列化和运行时场景中进行性能基准测试。

Result: CryptoTensors被证明是一个轻量级、高效且开发者友好的解决方案，能够保护LLM权重在现实世界和广泛部署中的安全。与现有推理框架（如Hugging Face Transformers和vLLM）保持兼容性，同时最小化性能开销。

Conclusion: CryptoTensors为LLM的安全分发提供了一种实用的解决方案，填补了现有模型格式在机密性、访问控制和安全集成方面的空白，支持广泛部署而不依赖昂贵的基础设施或复杂的加密技术。

Abstract: To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.
  In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.

</details>


### [10] [PBFuzz: Agentic Directed Fuzzing for PoV Generation](https://arxiv.org/abs/2512.04611)
*Haochen Zeng,Andrew Bao,Jiajun Cheng,Chengyu Song*

Main category: cs.CR

TL;DR: PBFuzz是一个基于智能体（agentic）的定向模糊测试框架，通过模仿人类专家分析代码、提取语义约束、形成假设并利用调试反馈的迭代过程，高效生成漏洞证明（PoV）输入。


<details>
  <summary>Details</summary>
Motivation: 生成PoV输入需要同时满足可达性约束和触发约束，现有方法（如定向灰盒模糊测试和LLM辅助模糊测试）难以高效解决这些约束。人类专家通过迭代分析代码、提取语义约束、形成假设并利用调试反馈的过程更为有效，因此需要自动化这一过程。

Method: PBFuzz框架采用智能体方法，自动化人类专家的分析过程：1）自主代码推理提取语义约束；2）定制程序分析工具进行针对性推断；3）持久化内存避免假设漂移；4）基于属性的测试在保持输入结构的同时高效解决约束。

Result: 在Magma基准测试中，PBFuzz触发了57个漏洞，超过所有基线方法，并独特触发了17个现有模糊测试器未发现的漏洞。在每目标30分钟预算内完成，而传统方法需要24小时。中位暴露时间为339秒（AFL++ with CmpLog为8680秒），效率提升25.6倍，每个漏洞API成本为1.83美元。

Conclusion: PBFuzz通过模仿人类专家的智能体方法，在PoV输入生成方面显著优于现有技术，实现了更高的效率和效果，为软件安全测试提供了新的有效工具。

Abstract: Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.

</details>


### [11] [Cryptanalysis of Gleeok-128](https://arxiv.org/abs/2512.04675)
*Siwei Chen,Peipei Xie,Shengyuan Xu,Xiutao Feng,Zejun Xiang,Xiangyong Zeng*

Main category: cs.CR

TL;DR: 本文首次对Gleeok-128密码算法进行了全面的第三方密码分析，提出了基于MILP的差分-线性区分器构建框架和针对多分支设计的积分攻击密钥恢复框架，发现了原设计中Branch 3的线性安全性评估缺陷。


<details>
  <summary>Details</summary>
Motivation: Gleeok是一种低延迟密钥伪随机函数家族，采用三个并行SPN置换结构。由于其多分支结构，评估安全边界和进行有效的密钥恢复攻击面临挑战。本文旨在对Gleeok-128进行首次全面的第三方密码分析。

Method: 提出了两阶段MILP框架构建分支级和全密码差分-线性区分器；开发了针对多分支设计的积分攻击密钥恢复框架；通过收紧代数度边界推导积分区分器；识别了原设计中Branch 3的线性安全性评估缺陷；提出了优化的线性层参数。

Result: 差分-线性分析得到7、7、8、4轮区分器（分别对应三个分支和完整PRF），平方相关性约2^-88.12、2^-88.12、2^-38.73、2^-49.04；积分分析得到9、9、7轮分支区分器和7轮完整PRF区分器；基于积分特性实现了7轮和8轮密钥恢复攻击；发现Branch 3在所有12轮中可被区分，数据复杂度约2^48。

Conclusion: 本文推进了对Gleeok-128的理解，为分析多分支对称设计提供了通用方法。研究结果表明原设计存在安全缺陷，并提出了改进的线性层参数以增强线性抗性而不牺牲扩散性。

Abstract: Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.

</details>


### [12] [SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://arxiv.org/abs/2512.04841)
*Wei Zhao,Zhe Li,Jun Sun*

Main category: cs.CR

TL;DR: 该论文提出了一个统一的因果分析框架，用于系统研究大语言模型的安全漏洞（如越狱攻击），支持从token级到表示级的因果干预，并通过实验发现安全机制高度集中在早期到中间层的少量神经元中。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然表现出色，但仍容易受到对抗性操纵（如越狱攻击），其中精心设计的提示可以绕过安全机制。理解这些漏洞背后的因果因素对于构建可靠的防御至关重要。

Method: 引入一个统一的因果分析框架，系统支持LLM中所有级别的因果调查，包括token级、神经元级、层级干预和表示级分析。该框架支持一致的实验和比较，涵盖多种基于因果的攻击和防御方法。

Result: 研究发现：(1) 对因果关键组件的针对性干预可以可靠地修改安全行为；(2) 安全相关机制高度局部化（集中在早期到中间层，只有1-2%的神经元表现出因果影响）；(3) 从框架中提取的因果特征在多种威胁类型中达到超过95%的检测准确率。

Conclusion: 通过桥接理论因果分析和实际模型安全，该框架为基于因果的攻击、可解释性以及鲁棒的攻击检测和缓解研究建立了可重复的基础。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.
  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\% detection accuracy across multiple threat types.
  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.

</details>


### [13] [Opacity problems in multi-energy timed automata](https://arxiv.org/abs/2512.04950)
*Étienne André,Lydia Bakiri*

Main category: cs.CR

TL;DR: 该论文研究了具有时间和能量观测的系统的信息泄露验证问题，引入了带守卫的多能量时间自动机，并在多个子类上建立了可判定性结果。


<details>
  <summary>Details</summary>
Motivation: 信息物理系统可能面临信息泄露风险，当涉及时间和能量等连续变量时，这些泄露难以检测。需要研究在同时观测时间和能量信息的情况下，系统不透明性问题的验证方法。

Method: 引入带守卫的多能量时间自动机作为时间自动机的扩展，支持多个能量变量和相关的守卫条件。虽然一般形式是不可判定的，但在多个子类上建立了正面结果。

Result: 尽管一般形式的带守卫多能量时间自动机是不可判定的，但在多个子类上获得了可判定性结果：1）攻击者观测最终能量和/或执行时间的情况；2）攻击者每时间单位访问能量变量值的情况。

Conclusion: 通过扩展时间自动机模型并考虑能量变量，为信息物理系统的信息泄露验证提供了理论框架，在多个实际相关的观测场景下获得了可判定性结果，为系统安全分析提供了理论基础。

Abstract: Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 该论文提出了一种面向边缘集群的可持续性感知LLM推理框架，通过碳感知和延迟感知的路由策略，在NVIDIA Jetson Orin NX和Ada 2000设备上平衡推理延迟与碳足迹。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理需要大量计算资源，导致显著的碳排放和运营成本。云端推理存在延迟和带宽限制，而边缘集群虽然能实现本地化执行，但在性能、能效和设备约束之间存在权衡。需要一种可持续的LLM推理方法来解决这些问题。

Method: 在NVIDIA Jetson Orin NX（8GB）和Nvidia Ada 2000（16GB）设备组成的边缘集群上，通过实证基准测试不同提示和批处理配置的能耗和执行时间，开发碳感知和延迟感知的路由策略。将贪婪基线策略与碳感知和延迟感知策略进行比较。

Result: 实验评估显示，批处理大小为4个提示时能在吞吐量和能效之间取得最佳权衡，更大的批处理大小可能导致GPU内存饱和。碳感知和延迟感知策略相比基线策略能更好地平衡推理延迟和碳足迹。

Conclusion: 该研究证明了在边缘集群上实现可持续LLM推理的可行性，通过智能路由策略可以在保持性能的同时减少环境影响，为绿色AI计算提供了实用解决方案。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [15] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 评估WebAssembly在浏览器、边缘和云环境中执行无服务器工作流的性能表现，分析不同编译模式和部署环境对性能的影响


<details>
  <summary>Details</summary>
Motivation: WebAssembly作为一种跨平台二进制指令格式，在无服务器工作流执行中具有潜力，但其性能受启动开销、运行时编译模式（AOT/JIT）和资源可变性等因素影响，需要系统评估在不同部署环境中的表现

Method: 使用wasm32-wasi模块，在浏览器中通过web worker执行，在边缘和云环境中通过HTTP shim流式传输帧到Wasm运行时；测量冷启动/热启动延迟、每步延迟、工作流完成时间、吞吐量和CPU/内存利用率

Result: AOT编译和实例预热显著减少启动延迟；对于小负载工作流，浏览器因完全内存数据交换而具有竞争力；随着负载增大，工作流进入计算和内存密集型阶段，边缘和云节点的AOT执行性能明显超越浏览器

Conclusion: WebAssembly无服务器工作流的性能表现高度依赖于部署环境和负载特性，AOT编译和预热策略对性能优化至关重要，不同环境适用于不同负载规模的工作流执行

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [16] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文对2020-2024年间微服务化雾计算和边缘计算中的资源管理策略进行了全面综述，重点关注能源效率，系统分析了136项研究，将其分为服务放置、资源供应、任务调度与卸载、资源分配和实例选择五个子领域，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的指数级增长对高效响应服务提出了更高要求，雾计算和边缘计算作为分布式范式虽然能降低延迟、带宽限制和能耗，但由于资源约束、计算异构性、动态工作负载和多样化QoS需求，在资源管理方面面临重大挑战。

Method: 对2020-2024年间的136项研究进行了系统性综述和分类，将其分为五个关键子领域：服务放置、资源供应、任务调度与卸载、资源分配和实例选择。分类基于优化技术、目标目标以及每种方法的优缺点。

Result: 通过系统分析发现，现有研究在微服务化雾计算和边缘计算的资源管理方面取得了显著进展，特别是在能源效率方面。但同时也识别出文献中未解决的挑战和差距，特别是基本资源管理组件之间缺乏协同性。

Conclusion: 本文为研究人员和从业者提供了微服务化雾计算和边缘计算中资源管理的统一且能源感知的视角，指出了利用AI驱动优化、量子计算和无服务器计算等有前景的研究方向，为更集成、高效和可持续的未来解决方案铺平了道路。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [17] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS是一个文件基础的有序消息传递系统，通过结合远程过程调用和远程内存访问两种通信原语，实现跨集群的低延迟消息传递，支持数千消费者，达到Tbps级流量。


<details>
  <summary>Details</summary>
Motivation: 实时系统需要低延迟消息传递，数据从生产者传输到可能分布在跨大洲集群的数千消费者。系统需要保证消息顺序和至少一次交付，同时避免消费者过载，让消费者按自己的节奏消费消息。

Method: 设计Fast ACS（广告复制服务），一个基于文件的有序消息传递系统，结合两种通信原语：集群间的远程过程调用和集群内的远程内存访问。系统已部署到数十个生产集群。

Result: 系统扩展到每个集群容纳数千消费者，峰值时达到Tbps级的集群内消费者流量。根据消息量和消费者规模，能在几秒甚至亚秒级（p99）内将消息传递到全球消费者，资源成本低。

Conclusion: Fast ACS成功实现了大规模、低延迟、有序的消息传递系统，满足实时系统对跨集群消息传递的需求，已在生产环境中验证其可扩展性和性能。

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [18] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS是一个基于架构参数建模的确定性GPU GEMM内核生成框架，无需运行时自动调优即可达到接近最优性能


<details>
  <summary>Details</summary>
Motivation: 传统GPU GEMM内核通常依赖耗时的运行时自动调优来获得最佳性能，这限制了生产环境中的实际应用。需要一种能够基于架构参数预测最优配置的确定性模型

Method: 开发了tritonBLAS模型，利用缓存层次结构、代码和数据相对位置等架构参数，显式建模架构拓扑、矩阵形状和算法分块行为之间的关系。基于该模型在Triton中实现了轻量级GEMM框架

Result: 在现代GPU上评估多种GEMM问题规模，tritonBLAS达到自动调优解决方案95%以上的性能，同时将自动调优时间降为零

Conclusion: tritonBLAS是生产HPC和ML工作负载中经验调优的实际替代方案，提供高性能且无需调优开销

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [19] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: VLCs（虚拟库上下文）是一种进程子单元，能够封装库和资源分配，在不修改库代码的情况下控制库的资源使用，解决并行库组合时的资源争用问题。


<details>
  <summary>Details</summary>
Motivation: 随着并行机规模和复杂度的增长，程序员越来越依赖软件库的组合来封装和利用并行性。然而许多库在设计时没有考虑组合使用，假设独占所有资源，导致并发使用时产生资源争用和性能下降。现有解决方案需要修改库或操作系统，通常不可行。

Method: 提出虚拟库上下文（VLCs）作为进程子单元，封装库集合和相关资源分配。VLCs能够在不修改库代码的情况下控制库的资源使用，允许用户在库之间划分资源以防止争用，或加载同一库的多个副本以支持并行执行原本线程不安全的代码。

Result: 开发了C++和Python原型，实验显示VLCs在使用OpenMP、OpenBLAS和LibTorch等库的应用中，最高可实现2.85倍的加速。

Conclusion: VLCs提供了一种有效的方法来解决并行库组合时的资源争用问题，无需修改库代码或操作系统，显著提升了并行应用的性能。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [20] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 论文提出了gpuFLOPBench基准测试，用于评估大语言模型在不运行代码的情况下预测CUDA内核浮点运算次数的能力，揭示了当前模型在涉及编译器隐式操作时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代GPU软件开发需要开发者在运行内核前就能预测性能瓶颈，但当前的大语言模型很少测试这种前瞻性推理能力。现有代码助手无法内化硬件特定的微码效应，需要专门的基准测试来评估和改进模型对性能的推理能力。

Method: 创建gpuFLOPBench基准测试，包含从HeCBench中提取的577个CUDA内核，每个内核都标注了真实性能分析和8个执行属性。这些属性区分了可简单分析的代码和那些浮点运算依赖于隐藏编译器或运行时行为的复杂内核。

Result: 评估显示最新的大语言模型在简单内核上能实现完美分类，但当涉及除法、内在数学函数或公共子表达式等产生隐式浮点运算的情况时，仍会出现数量级误差。这表明现有模型无法内化硬件特定的微码效应。

Conclusion: gpuFLOPBench为开发能够像经验丰富的GPU开发者一样严谨推理性能的LLM工具提供了专门的测试平台，揭示了当前代码助手在硬件特定性能推理方面的核心局限性。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [21] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 提出了一种针对稀疏LU分解的结构感知不规则分块方法，通过引入对角线块特征来表征局部非零元分布，并根据局部密度调整块大小，在GPU上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏LU分解中，符号分解后的非零元倾向于分布在矩阵的对角线和右下区域。传统的规则二维分块在这种非均匀分布结构上可能导致块间工作负载不平衡，且现有矩阵特征无法有效指导分块策略。

Method: 提出了一种结构感知的不规则分块方法：1）引入新颖的对角线块特征来有效表征稀疏矩阵的局部非零元分布；2）基于此特征提出不规则分块方法，根据局部非零元分布调整块大小；3）在密集区域使用细粒度块，在稀疏区域使用粗粒度块，在依赖树的同一层级和跨层级间充分平衡各块的非零元数量。

Result: 实验表明，在单个NVIDIA A100 GPU上，提出的不规则分块方法相比PanguLU和最新的SuperLU_DIST分别实现了平均1.50倍和3.32倍的加速比。在4个NVIDIA A100 GPU上，相比PanguLU和SuperLU_DIST分别实现了1.40倍和3.84倍的加速比。

Conclusion: 提出的结构感知不规则分块方法能够有效解决稀疏LU分解中的工作负载不平衡问题，通过自适应调整块大小显著提升了GPU上的计算性能，为稀疏矩阵分解提供了有效的分块策略。

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [22] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: KAI系统通过异步回传流协议优化CXL计算内存的性能，减少端到端运行时间达50.4%，显著降低空闲时间


<details>
  <summary>Details</summary>
Motivation: 现有CXL计算内存的操作卸载机制无法充分利用不同CXL协议模型的权衡优势，难以应对多样化数据处理需求

Method: 提出异步回传流协议，在底层CXL协议上分层处理数据和控制传输，设计KAI系统实现异步数据移动和轻量级流水线

Result: KAI减少端到端运行时间达50.4%，CCM和主机空闲时间分别平均降低22.11倍和3.85倍

Conclusion: 异步回传流协议和KAI系统能有效优化CXL计算内存性能，显著提升系统效率和资源利用率

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [23] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: THz通信与联邦学习结合时，宽带损伤（如波束偏移、分子吸收）会导致收敛误差，标准聚合方法在频谱空洞处失效，需要SNR加权聚合来恢复收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究太赫兹通信与联邦学习结合时，现实宽带损伤对优化动态的理论影响尚未被充分表征，需要填补这一理论空白。

Method: 开发多载波随机框架，将本地梯度更新与频率选择性THz效应（波束偏移、分子吸收、抖动）耦合，分析收敛误差，提出SNR加权聚合策略。

Result: 发现关键多样性陷阱：标准无偏聚合下收敛误差由子载波SNR的调和均值驱动；识别基本带宽限制；证明SNR加权聚合能抑制频谱空洞处的方差奇异性。

Conclusion: THz-FL系统中物理层参数对性能有显著影响，标准平均聚合在波束偏移严重时会失效，需要采用SNR加权聚合策略来确保可靠收敛。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [24] [FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration](https://arxiv.org/abs/2512.04527)
*Xingyu Liu,Jiawei Liang,Linfeng Du,Yipu Zhang,Chaofang Ma,Hanwei Fan,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: FLEX是一个用于混合单元高度合法化任务的FPGA-CPU加速器，通过优化任务分配策略、采用多粒度流水线技术，相比现有CPU-GPU和多线程CPU合法化器实现了最高18.3倍和5.4倍的加速，同时提高了合法化质量。


<details>
  <summary>Details</summary>
Motivation: 混合单元高度合法化任务在物理设计流程中计算密集，现有CPU-GPU和多线程CPU解决方案在性能和可扩展性方面仍有提升空间，需要利用FPGA和CPU的互补优势来加速这一关键步骤。

Method: 1. 优化任务分配策略，在FPGA和CPU之间进行高效任务划分以发挥各自优势；2. 采用多粒度流水线技术加速最耗时的步骤——寻找最优放置位置(FOP)；3. 针对FOP中计算密集的单元移位过程进行优化设计，使其与多粒度流水线框架无缝对齐。

Result: FLEX相比最先进的CPU-GPU和多线程CPU合法化器实现了最高18.3倍和5.4倍的加速，具有更好的可扩展性，同时将合法化质量分别提高了4%和1%。

Conclusion: FLEX通过创新的FPGA-CPU协同加速架构，有效解决了混合单元高度合法化的性能瓶颈问题，在保持高质量的同时实现了显著的加速效果，为物理设计流程提供了高效的解决方案。

Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.

</details>


### [25] [Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming](https://arxiv.org/abs/2512.04910)
*Fang Li*

Main category: cs.AR

TL;DR: 本文提出了一种使用答案集编程（ASP）进行自动条板电路布局设计的新方法，将布局问题同时表述为合成和多目标优化任务，在生成可行布局的同时最小化板面积和元件条交叉。


<details>
  <summary>Details</summary>
Motivation: 条板电路布局设计传统上依赖人工经验，过程耗时且容易出错。需要一种自动化方法来提高设计效率和质量，同时满足复杂的几何和电气约束。

Method: 采用答案集编程（ASP）的声明式方法，将布局问题形式化为合成和多目标优化任务。使用两阶段求解方法：首先确保可行性，然后优化布局质量。通过ASP自然简洁地表达复杂的几何和电气约束。

Result: 实验结果表明，该方法能够为各种复杂度的电路生成紧凑、可制造的布局。ASP方法在解决复杂设计自动化问题方面展现出强大能力。

Conclusion: 这项工作代表了自动条板电路布局设计的重大进展，为电子原型制作和教育提供了实用工具，同时展示了声明式编程在解决复杂设计自动化问题方面的强大能力。

Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Solving N-Queen Problem using Las Vegas Algorithm with State Pruning](https://arxiv.org/abs/2512.04139)
*Susmita Sharma,Aayush Shrestha,Sitasma Thapa,Prashant Timalsina,Prakash Poudyal*

Main category: cs.AI

TL;DR: 本文提出了一种基于Las Vegas算法的混合方法，通过迭代剪枝动态消除无效皇后放置，显著减少搜索空间，相比传统回溯法在处理大规模N皇后问题时具有更快的求解速度。


<details>
  <summary>Details</summary>
Motivation: N皇后问题是约束满足算法的经典问题。虽然回溯法等完整方法能保证找到解，但其指数时间复杂度使其在处理大规模实例时不切实际。而Las Vegas算法等随机方法虽然能提供更快的近似解，但由于皇后在棋盘上的随机放置，存在显著的性能波动。

Method: 研究提出了一种基于标准Las Vegas框架的混合算法，通过迭代剪枝在随机分配阶段动态消除无效的皇后放置，从而有效减少搜索空间。

Result: 分析结果显示，传统回溯法随着N的增加扩展性差。相比之下，所提出的技术能更快速一致地生成有效解，成为在需要及时获得单个解而非完整解时的优越替代方案。虽然大规模N会导致一些性能波动，但该算法在计算成本和解决方案保真度之间实现了高效权衡。

Conclusion: 该混合算法特别适合资源受限的计算环境，在需要快速获得单个解而非完整解的场景下表现出色，在计算成本和解决方案质量之间实现了有效平衡。

Abstract: The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.

</details>


### [27] [RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories](https://arxiv.org/abs/2512.04144)
*Roy Rinberg,Usha Bhalla,Igor Shilov,Flavio P. Calmon,Rohit Gandikota*

Main category: cs.AI

TL;DR: RippleBench-Maker是一个自动生成问答数据集的工具，用于测量模型编辑任务中的涟漪效应（副作用传播），通过基于维基百科的RAG管道生成与目标概念不同语义距离的多选题，并创建了RippleBench-Bio基准来评估8种最先进的遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型的针对性干预（如遗忘、去偏、模型编辑）是优化模型行为和更新知识的重要方法，但这些干预通常会产生涟漪效应——对相关但非目标领域产生意外影响。目前缺乏系统测量这种涟漪效应的工具和基准。

Method: 开发RippleBench-Maker工具，基于维基百科的RAG管道（WikiRAG）自动生成与目标概念在不同语义距离上的多选题。使用该框架从WMDP数据集构建RippleBench-Bio基准，评估8种最先进的遗忘方法。

Result: 所有评估的遗忘方法都显示出在距离遗忘知识越来越远的主题上存在显著的准确率下降，每种方法都有不同的传播特征。研究发布了代码库和RippleBench-Bio基准以支持持续研究。

Conclusion: 涟漪效应是模型编辑任务中普遍存在的现象，RippleBench-Maker提供了系统测量这种效应的方法，有助于更好地理解和控制模型干预的副作用传播。

Abstract: Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.

</details>


### [28] [Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care](https://arxiv.org/abs/2512.04207)
*Xizhi Wu,Nelly Estefanie Garduno-Rapp,Justin F Rousseau,Mounika Thakkallapally,Hang Zhang,Yuelyu Ji,Shyam Visweswaran,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: 基于大语言模型的多智能体临床决策支持系统，采用编排器-专家架构，通过七个领域专家智能体进行可解释的继发性头痛诊断，相比单LLM基线在准确率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 继发性头痛需要专科治疗且延误可能导致严重后果，临床指南虽有"红旗征"警示特征，但在基层医疗中仍难以准确识别需要紧急评估的患者。临床医生面临时间有限、信息不全、症状多样等挑战，容易导致漏诊和不恰当处理。

Method: 开发基于大语言模型的多智能体临床决策支持系统，采用编排器-专家架构。系统将诊断分解为七个领域专家智能体，每个智能体生成结构化、有证据支持的推理，中央编排器负责任务分解和智能体路由协调。使用90个专家验证的继发性头痛病例进行评估，比较多智能体系统与单LLM基线的性能，测试两种提示策略：基于问题的提示和基于临床实践指南的提示。

Result: 测试了五个开源LLM（Qwen-30B、GPT-OSS-20B、Qwen-14B、Qwen-8B和Llama-3.1-8B），发现编排多智能体系统结合基于临床实践指南的提示策略始终获得最高的F1分数，在较小模型中提升更明显。多智能体推理超越了单纯提示工程的准确性。

Conclusion: 结构化多智能体推理不仅提高了继发性头痛诊断的准确性，还提供了透明、与临床实践对齐的可解释决策支持方法，为临床决策支持系统提供了新的架构思路。

Abstract: Unlike most primary headaches, secondary headaches need specialized care and can have devastating consequences if not treated promptly. Clinical guidelines highlight several 'red flag' features, such as thunderclap onset, meningismus, papilledema, focal neurologic deficits, signs of temporal arteritis, systemic illness, and the 'worst headache of their life' presentation. Despite these guidelines, determining which patients require urgent evaluation remains challenging in primary care settings. Clinicians often work with limited time, incomplete information, and diverse symptom presentations, which can lead to under-recognition and inappropriate care. We present a large language model (LLM)-based multi-agent clinical decision support system built on an orchestrator-specialist architecture, designed to perform explicit and interpretable secondary headache diagnosis from free-text clinical vignettes. The multi-agent system decomposes diagnosis into seven domain-specialized agents, each producing a structured and evidence-grounded rationale, while a central orchestrator performs task decomposition and coordinates agent routing. We evaluated the multi-agent system using 90 expert-validated secondary headache cases and compared its performance with a single-LLM baseline across two prompting strategies: question-based prompting (QPrompt) and clinical practice guideline-based prompting (GPrompt). We tested five open-source LLMs (Qwen-30B, GPT-OSS-20B, Qwen-14B, Qwen-8B, and Llama-3.1-8B), and found that the orchestrated multi-agent system with GPrompt consistently achieved the highest F1 scores, with larger gains in smaller models. These findings demonstrate that structured multi-agent reasoning improves accuracy beyond prompt engineering alone and offers a transparent, clinically aligned approach for explainable decision support in secondary headache diagnosis.

</details>


### [29] [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](https://arxiv.org/abs/2512.04210)
*Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daumé*

Main category: cs.AI

TL;DR: 该研究提出了一种迭代部署后对齐框架，使用KTO和DPO方法优化医疗对话助手的安全性，在CARES-18K基准测试中实现了有害查询检测42%的性能提升，同时揭示了架构相关的校准偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域的应用日益增多，但确保其安全性和可信度仍然是部署的主要障碍。医疗对话助手需要在避免不安全合规的同时，不过度拒绝良性查询。

Method: 提出了迭代部署后对齐框架，应用Kahneman-Tversky优化(KTO)和直接偏好优化(DPO)方法，针对特定领域的安全信号进行模型精炼。使用CARES-18K基准测试评估了四个LLM模型(Llama-3B/8B, Meditron-8B, Mistral-7B)的对抗鲁棒性。

Result: 结果显示有害查询检测的安全相关指标提升了高达42%，同时揭示了与错误拒绝之间的权衡关系，暴露了架构依赖的校准偏差。消融研究确定了何时自我评估可靠，何时需要外部或微调的评估器来最大化性能增益。

Conclusion: 研究结果强调了在医疗对话助手设计中采用平衡患者安全、用户信任和临床效用的最佳实践的重要性。

Abstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.

</details>


### [30] [Educational Cone Model in Embedding Vector Spaces](https://arxiv.org/abs/2512.04227)
*Yo Ehara*

Main category: cs.AI

TL;DR: 提出教育锥体模型，基于文本难度在嵌入空间中呈现锥形分布的假设，通过优化问题评估不同嵌入方法对教育文本难度分析的适用性。


<details>
  <summary>Details</summary>
Motivation: 教育智能系统需要基于难度标注的数据集，但众多嵌入方法难以选择最适合教育文本难度分析的方法，需要一种评估框架。

Method: 提出教育锥体模型，假设简单文本多样性低（聚焦基础概念），困难文本多样性高，在嵌入空间中形成锥形分布。通过设计损失函数将嵌入评估转化为优化问题，推导出高效闭式解。

Result: 在真实数据集上的实证测试验证了模型的有效性和速度，能够识别与难度标注教育文本最匹配的嵌入空间。

Conclusion: 教育锥体模型为评估嵌入方法在教育文本难度分析中的适用性提供了有效的几何框架，解决了嵌入方法选择难题。

Abstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.

</details>


### [31] [Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://arxiv.org/abs/2512.04228)
*Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell*

Main category: cs.AI

TL;DR: 本文提出了一种双推理训练框架，将肯定生成与结构化反事实否定相结合，以解决LLMs在科学推理中的逻辑弱点。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要基于肯定推理（类似肯定前件式），虽然在生成流畅性上有效，但容易受到逻辑谬误、对抗性操纵和因果推理失败的影响。特别是在科学领域中处理否定、反例或错误前提时表现出系统性弱点。

Method: 引入双推理训练框架，整合肯定生成与结构化反事实否定。基于形式逻辑、认知科学和对抗训练，将"否定前件"形式化为计算模拟，作为证伪和鲁棒性机制。通过将生成合成与显式的否定感知目标相结合。

Result: 该框架使模型不仅能肯定有效推理，还能拒绝无效推理，产生更具弹性、可解释性且与人类推理更一致的系统。

Conclusion: 通过整合肯定推理和否定推理的双重训练方法，可以显著提升LLMs在科学领域的逻辑推理能力、鲁棒性和对齐性，为科学、医疗和决策支持提供更可靠的AI工具。

Abstract: Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.

</details>


### [32] [The Geometry of Benchmarks: A New Path Toward AGI](https://arxiv.org/abs/2512.04276)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 论文提出几何框架分析AI基准测试，定义自主AI等级，构建基准模空间，引入GVU算子量化自我改进能力，将AGI进展理解为基准模空间上的流


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试实践存在局限性：孤立评估模型，缺乏对通用性和自主自我改进能力的指导。需要系统化框架来理解AI能力发展和AGI进展的本质

Method: 1. 定义自主AI等级（AAI Scale），基于任务族性能建立自主性层次结构
2. 构建基准测试模空间，识别在智能体排序和能力推断层面不可区分的基准等价类
3. 引入生成器-验证器-更新器（GVU）算子，统一强化学习、自我博弈、辩论和验证器微调等方法
4. 定义自我改进系数κ作为能力泛函沿GVU诱导流的李导数

Result: 1. 几何框架提供确定性结果：密集基准族足以认证整个任务空间区域的性能
2. 生成和验证的联合噪声方差不等式为κ>0提供充分条件
3. 将AGI进展重新概念化为基准模空间上的流，由GVU动力学驱动而非单个排行榜分数

Conclusion: AI向AGI的进展应理解为基准测试模空间上的流，由GVU动力学驱动。该框架为系统评估AI自主性和自我改进能力提供了理论基础，超越了传统孤立基准测试的局限性

Abstract: Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $κ$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $κ> 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.

</details>


### [33] [Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases](https://arxiv.org/abs/2512.04287)
*Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida*

Main category: cs.AI

TL;DR: 本文综述了人工智能在传染病预警扫描中的应用，探讨AI如何增强信号检测、数据监控、情景分析和决策支持，同时分析AI采用的风险并提出实施策略。


<details>
  <summary>Details</summary>
Motivation: 随着传染病威胁的不断演变，传统预警扫描方法面临挑战。人工智能技术为增强公共卫生预警能力提供了新的可能性，但需要系统评估其应用潜力、局限性和风险。

Method: 采用文献综述方法，系统分析人工智能在传染病预警扫描中的集成应用，包括信号检测、数据监控、情景分析和决策支持等关键领域，同时评估相关风险并提出治理策略。

Result: 研究发现AI能够显著增强传染病预警扫描的效率和准确性，特别是在早期信号检测和大规模数据分析方面。但同时也存在数据质量、算法偏见、隐私保护等风险挑战。

Conclusion: 人工智能在传染病预警扫描中具有重要应用价值，但需要建立有效的实施框架和治理机制，平衡技术创新与风险管理，以增强公共卫生应急准备能力。

Abstract: This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.

</details>


### [34] [Towards better dense rewards in Reinforcement Learning Applications](https://arxiv.org/abs/2512.04302)
*Shuyuan Zhang*

Main category: cs.AI

TL;DR: 该论文探讨强化学习中稠密奖励函数的设计问题，旨在解决稀疏奖励信号导致的低效学习，通过多种方法提升奖励函数的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中，当奖励信号稀疏、延迟或与任务目标不匹配时，智能体学习效率低下。稠密奖励函数能提供更频繁的反馈，但设计不当会导致意外行为、奖励黑客或探索效率低下，特别是在复杂高维环境中。

Method: 论文探索了多种方法，包括逆强化学习、基于人类偏好的奖励建模、自监督学习内在奖励等，旨在解决稠密奖励构造中的未解问题。

Result: 虽然这些方法提供了有前景的方向，但通常在通用性、可扩展性和与人类意图对齐之间存在权衡，需要在不同RL应用中进一步探索。

Conclusion: 需要继续研究以增强稠密奖励构造的有效性和可靠性，解决当前方法在通用性、可扩展性和对齐性方面的局限性。

Abstract: Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.

</details>


### [35] [Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning](https://arxiv.org/abs/2512.04359)
*Hongye Cao,Zhixin Bai,Ziyue Peng,Boyan Wang,Tianpei Yang,Jing Huo,Yuyao Zhang,Yang Gao*

Main category: cs.AI

TL;DR: 提出一个结合语义熵和词元熵的强化学习框架，通过课程学习和非均匀词元处理缓解熵崩溃问题，提升大语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 虽然基于可验证奖励的强化学习（RLVR）能提升大语言模型的推理能力，但这种以准确性为导向的学习范式容易导致熵崩溃，减少策略探索并限制推理能力的发展

Method: 从数据角度引入语义熵引导的课程学习，按语义熵从低到高组织训练数据；从算法设计角度采用非均匀词元处理，对影响策略探索的低熵词元施加KL正则化，并在这些词元的高协方差部分施加更强约束

Result: 在6个基准测试和3种不同参数规模的基模型上，该方法优于其他基于熵的方法，有效缓解了熵崩溃并提升了推理能力

Conclusion: 通过联合优化数据组织和算法设计，提出的方法能有效缓解熵崩溃问题，增强大语言模型的推理能力，为提升强化学习在语言模型推理任务中的应用提供了有效解决方案

Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.

</details>


### [36] [GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](https://arxiv.org/abs/2512.04416)
*Zhou Liu,Zhaoyang Han,Guochen Yan,Hao Liang,Bohan Zeng,Xing Chen,Yuanfeng Song,Wentao Zhang*

Main category: cs.AI

TL;DR: GovBench基准测试揭示当前LLM在数据治理任务上的不足，DataGovAgent框架通过规划-执行-评估架构显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有自动化数据科学基准测试主要关注代码片段或高层分析，未能捕捉数据治理特有的确保数据正确性和质量的挑战，需要专门基准来评估LLM在数据治理任务上的能力

Method: 提出GovBench基准，包含150个基于真实场景的任务，采用"反向目标"方法合成真实噪声；提出DataGovAgent框架，采用规划器-执行器-评估器架构，集成约束规划、检索增强生成和沙盒反馈驱动调试

Result: 当前模型在复杂多步工作流上表现不佳，缺乏鲁棒的错误纠正机制；DataGovAgent将复杂任务的平均任务分数从39.7提升到54.9，调试迭代减少超过77.9%

Conclusion: 专门的数据治理基准和框架对于提升LLM在数据治理任务上的性能至关重要，DataGovAgent通过结构化规划和反馈机制显著改善了自动化数据治理的效果

Abstract: Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel "reversed-objective" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.

</details>


### [37] [Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions](https://arxiv.org/abs/2512.04419)
*Weiwei Wang,Weijie Zou,Jiyong Min*

Main category: cs.AI

TL;DR: 本文针对LLM在批量代码解释任务中的重复生成问题，识别了三种重复模式，通过马尔可夫模型分析根源，并提出了三种有效解决方案：Beam Search解码、presence_penalty参数调整和DPO微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生产部署中持续生成重复内容而无法正常终止的问题，导致严重的性能下降和系统停滞，这对实际应用构成了关键挑战。

Method: 基于马尔可夫模型进行理论分析，识别三种重复模式：业务规则生成重复、方法调用关系分析重复和PlantUML图语法生成重复。通过实验评估三种解决方案：Beam Search解码（early_stopping=True）、presence_penalty超参数调整和DPO微调。

Result: 实验证明：Beam Search解码能有效解决所有三种重复模式；presence_penalty参数专门解决第一种重复模式；DPO微调为所有三种重复模式提供模型级解决方案。early_stopping被确定为Beam Search有效性的关键参数。

Conclusion: 本研究结合生产经验和实验验证，提供了对重复机制的系统理论分析、多种解决方案的全面评估、任务适用性映射，以及在实际部署环境中验证的生产就绪解决方案。

Abstract: The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.
  We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.
  Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.
  The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.

</details>


### [38] [TaskEval: Synthesised Evaluation for Foundation-Model Tasks](https://arxiv.org/abs/2512.04442)
*Dilani Widanapathiranage,Scott Barnett,Stefanus Kurniawan,Wannita Takerngsaksiri*

Main category: cs.AI

TL;DR: 提出了一种为特定基础模型任务合成评估程序的方法，通过任务无关元模型、人机交互协议和评估合成器，解决缺乏标准评估指标时的评估难题。


<details>
  <summary>Details</summary>
Motivation: 基础模型在应用中产生的幻觉问题是关键挑战，现有评估方法要么关注新评估方法，要么关注特定任务的基准数据集，但都无法帮助软件团队在没有现成评估指标或数据集的情况下评估任务特定的基础模型应用。

Method: 提出了一种合成基础模型任务特定评估程序的方法，包含三个核心创新：1）任务无关元模型，捕捉任何基础模型任务的属性；2）高效利用人类反馈的交互协议；3）评估合成器，选择或生成适当的评估集。该方法在工具中实现，并在图表数据提取和文档问答两个不同任务上验证。

Result: 在两个不同基础模型任务上的初步评估显示，所选评估的质量分别达到93%和90%的准确率，表明该方法能有效评估基础模型任务的输出。

Conclusion: 该研究解决了工程团队面临的一个日益严重的问题：如何评估和审查基础模型任务的输出，为缺乏标准评估指标的任务提供了实用的评估解决方案。

Abstract: Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\% and 90\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.

</details>


### [39] [MARL Warehouse Robots](https://arxiv.org/abs/2512.04463)
*Price Allman,Lian Thang,Dre Simmons,Salmon Riaz*

Main category: cs.AI

TL;DR: QMIX在仓库机器人协作任务中显著优于独立学习算法，但需要大量超参数调优，特别是在稀疏奖励发现方面需要延长探索时间。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习算法在合作式仓库机器人任务中的性能表现，比较不同算法的优劣，为实际部署提供指导。

Method: 在Robotic Warehouse环境和自定义Unity 3D仿真中，比较QMIX和IPPO两种MARL算法，重点关注价值分解与独立学习方法的差异。

Result: QMIX的平均回报达到3.25，显著优于IPPO的0.38；成功在Unity ML-Agents中部署，经过100万训练步数后实现稳定的包裹配送。

Conclusion: MARL在小规模机器人部署（2-4台）中表现出潜力，但面临显著的扩展挑战；QMIX需要大量超参数调优，特别是稀疏奖励发现需要延长探索时间。

Abstract: We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/

</details>


### [40] [Mathematical Framing for Different Agent Strategies](https://arxiv.org/abs/2512.04469)
*Philip Stephens,Emmanuel Salawu*

Main category: cs.AI

TL;DR: 该论文提出了一个统一的数学和概率框架，用于理解和比较不同的AI智能体策略，将高层设计概念与严格的数学形式化联系起来。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体设计缺乏统一的数学框架来理解和比较不同策略（如ReAct、多智能体系统、控制流等），导致难以精确分析和评估各种架构的权衡。

Method: 将智能体过程框架化为概率链，分析不同策略如何操纵这些概率以达到期望结果，并引入"自由度"概念来区分不同方法的可优化杠杆。

Result: 建立了一个统一的数学概率框架，为讨论各种智能体架构的权衡提供了共同语言，并通过自由度概念指导针对特定任务选择适当策略。

Conclusion: 该框架提高了AI智能体设计和评估的清晰度和精确性，为在复杂智能体系统中最大化成功行动概率提供了理论洞察。

Abstract: We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the "Degrees of Freedom" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.

</details>


### [41] [Persona-based Multi-Agent Collaboration for Brainstorming](https://arxiv.org/abs/2512.04488)
*Nate Straub,Saara Khan,Kat Jay,Brian Cabral,Oskar Linde*

Main category: cs.AI

TL;DR: 本文提出基于角色的多智能体头脑风暴框架，通过角色领域筛选提升创意生成质量，实验表明角色选择影响创意领域，协作模式改变创意多样性，多智能体角色驱动方法能产生深度和跨领域覆盖的创意。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明广义多智能体协作通常比单个智能体提供更好的推理能力，但如何通过角色化智能体选择来提升头脑风暴效果尚未充分探索。本文旨在研究基于角色的多智能体头脑风暴在不同主题和领域创意生成中的重要性。

Method: 提出并开发了一个基于角色的智能体选择框架，通过角色领域筛选来改进头脑风暴结果。使用多种实验设置，评估不同角色配对（如医生vs虚拟现实工程师）和智能体间动态（分离、一起、先分离后一起）下的头脑风暴输出。

Result: 结果显示：(1) 角色选择塑造创意领域，(2) 协作模式改变创意生成的多样性，(3) 多智能体角色驱动的头脑风暴能产生深度创意和跨领域覆盖。

Conclusion: 基于角色的多智能体头脑风暴框架能有效提升创意生成的质量和多样性，角色领域筛选和协作模式设计对头脑风暴结果有重要影响，该方法为多智能体协作在创意生成任务中的应用提供了新思路。

Abstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.

</details>


### [42] [A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework](https://arxiv.org/abs/2512.04500)
*Edervaldo Melo*

Main category: cs.AI

TL;DR: Nemosine框架是一个模块化认知架构，通过功能性认知模块支持辅助推理、结构化思维和系统分析，结合元认知、分布式认知和模块化认知系统原理，为辅助问题解决和决策支持提供操作结构。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在提供一个清晰的认知架构基础，支持辅助推理和结构化思维，为未来的计算实现提供概念基础，并促进符号-模块化推理架构的研究。

Method: 采用模块化认知架构设计，通过功能性认知模块（"personas"）组织规划、评估、交叉检查和叙事合成等任务，结合元认知、分布式认知和模块化认知系统原理，并通过形式化规范、内部一致性标准和可复现结构组件进行文档化。

Result: 提出了Nemosine框架，这是一个支持辅助推理和决策支持的模块化认知架构，提供了清晰的架构文档和可复现的结构组件，为未来计算实现奠定了基础。

Conclusion: Nemosine框架为辅助问题解决和决策支持提供了一个有效的模块化认知架构，通过功能性认知模块和系统化设计，为符号-模块化推理架构的研究做出了贡献，并为未来的计算实现提供了明确的概念基础。

Abstract: This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules ("personas") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.

</details>


### [43] [ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications](https://arxiv.org/abs/2512.04785)
*Eranga Bandara,Amin Hass,Ross Gore,Sachin Shetty,Ravi Mukkamala,Safdar H. Bouk,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: ASTRIDE是一个针对AI智能体系统的自动化威胁建模平台，通过扩展STRIDE框架并引入AI特定威胁类别，结合视觉语言模型和推理大模型，实现从架构图到威胁分析的端到端自动化。


<details>
  <summary>Details</summary>
Motivation: AI智能体系统在现代软件架构中日益重要，但引入了传统威胁建模框架无法有效捕捉的新型安全挑战，如提示注入攻击、上下文污染、模型操纵和不透明的智能体间通信等。

Method: 扩展经典STRIDE框架，新增A类别（AI智能体特定攻击）；结合微调的视觉语言模型联盟和OpenAI GPT推理大模型，直接从视觉架构图（如数据流图）进行端到端分析；通过LLM智能体协调VLM联盟和推理LLM之间的交互。

Result: 评估表明ASTRIDE为下一代智能系统提供了准确、可扩展和可解释的威胁建模。据作者所知，这是第一个既扩展STRIDE框架包含AI特定威胁，又集成微调VLM与推理LLM实现AI智能体应用中图驱动威胁建模完全自动化的框架。

Conclusion: ASTRIDE平台成功解决了AI智能体系统特有的安全挑战，通过创新的自动化威胁建模方法，为智能系统的安全分析提供了有效的解决方案。

Abstract: AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.

</details>


### [44] [BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models](https://arxiv.org/abs/2512.04513)
*Yu-Wei Zhan,Xin Wang,Pengzhe Mao,Tongtong Feng,Ren Wang,Wenwu Zhu*

Main category: cs.AI

TL;DR: BiTAgent提出了一种任务感知的动态联合框架，通过双向耦合多模态大语言模型（MLLMs）和世界模型（WMs），实现语义推理与动态预测的协调，提升具身智能的多任务学习和跨环境泛化能力。


<details>
  <summary>Details</summary>
Motivation: 构建通用具身智能体需要统一系统来理解多模态目标、建模环境动态并执行可靠动作。MLLMs提供语义先验和跨模态泛化，WMs提供可操作的潜在动态，但两者结合面临两个关键挑战：1）建立MLLMs语义意图与WMs潜在空间动态状态表示之间的紧密耦合；2）实现支持多任务学习和跨环境泛化的任务感知适应性。

Method: 提出BiTAgent框架，建立两个互补路径：前向路径将MLLM表示注入WM潜在空间进行语义引导的想象；后向路径通过密集文本条件奖励让WM生成反馈来优化MLLM语义空间。通过三个协同组件实现：任务感知动态联合学习、任务感知行为学习和MLLM-WM联合优化。

Result: 在多任务和跨环境设置下的广泛实验表明，BiTAgent在稳定性和泛化能力上优于最先进的基线方法，标志着向开放端具身学习迈出了一步。

Conclusion: BiTAgent通过双向耦合MLLMs和WMs，成功协调了语义推理和动态预测，为解决具身智能中的语义-动态对齐和任务适应性挑战提供了有效框架，推动了开放端具身学习的发展。

Abstract: Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.

</details>


### [45] [SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation](https://arxiv.org/abs/2512.04529)
*Xin Liang,Xiang Zhang,Yiwei Xu,Siqi Sun,Chenyu You*

Main category: cs.AI

TL;DR: SlideGen：一个基于多智能体协作的自动化学术幻灯片生成框架，通过视觉语言智能体协同工作，从科学论文生成具有专业质量的PPTX幻灯片。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将学术幻灯片生成简化为文本摘要任务，忽略了视觉组件和幻灯片创建的设计密集型特性。需要一种能够同时处理长上下文理解和视觉规划的多模态推理方法。

Method: SlideGen采用模块化、智能体驱动、视觉在环的框架，通过协调多个视觉语言智能体协同工作，执行大纲制定、内容映射、布局安排、笔记合成和迭代优化等任务，生成可编辑的PPTX幻灯片。

Result: 在多个基准测试和强基线对比中，SlideGen在视觉质量、内容忠实度和可读性方面优于现有方法，成为自动幻灯片生成领域的新SOTA，能够生成专家级质量的幻灯片。

Conclusion: SlideGen为设计感知的多模态幻灯片生成奠定了基础，展示了智能体协作如何在复杂的多模态推理任务中弥合理解和呈现之间的鸿沟。

Abstract: Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.

</details>


### [46] [The Ethics of Generative AI](https://arxiv.org/abs/2512.04598)
*Michael Klenk*

Main category: cs.AI

TL;DR: 本章探讨生成式AI的伦理问题，分析其如何让人将技术体验为类人实体，并讨论其如何加剧或缓解AI伦理中的经典问题，以及由生成能力引发的特定伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了独特的伦理挑战，特别是其能够让人将技术体验为类人实体，这为哲学伦理分析提供了新的焦点。需要系统探讨生成式AI如何影响责任、隐私、偏见等传统AI伦理问题，并分析其生成能力引发的特定伦理困境。

Method: 首先提供技术入门，解释生成式AI如何让人体验技术为类人实体；然后分析生成式AI如何影响传统AI伦理问题（责任、隐私、偏见、异化与剥削）；最后专门探讨由生成式AI的模仿生成能力引发的特定伦理问题。

Result: 生成式AI既可能加剧也可能缓解传统AI伦理问题，同时引发了关于作者身份、人机关系、影响与操纵等新的伦理挑战。其让人将技术体验为类人实体的特性成为伦理分析的关键焦点。

Conclusion: 生成式AI的伦理分析需要同时关注其对传统AI伦理问题的影响，以及由其独特生成能力引发的特定伦理挑战。将技术体验为类人实体的特性为哲学伦理提供了富有成果的研究方向。

Abstract: This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.

</details>


### [47] [Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning](https://arxiv.org/abs/2512.04618)
*Mohamed Baha Ben Ticha,Xingchen Ran,Guillaume Saldanha,Gaël Le Godais,Philémon Roussel,Marc Aubert,Amina Fontanell,Thomas Costecalde,Lucas Struber,Serpil Karakas,Shaomin Zhang,Philippe Kahane,Guillaume Charvet,Stéphan Chabardès,Blaise Yvert*

Main category: cs.AI

TL;DR: 该研究提出了一种基于编码器-解码器深度神经架构的离线语音解码流水线，整合Vision Transformers和对比学习，用于从ECoG信号直接回归重建语音，并在两种不同类型的数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 语音脑机接口为严重瘫痪患者提供了沟通解决方案。虽然已有研究从皮层电信号重建可理解语音，但当前挑战在于通过直接回归皮层信号到声学语音来实现流式语音重建。特别是对于表面ECoG记录，需要优化神经解码器以获得与皮层内记录相当的结果。

Method: 提出离线语音解码流水线，采用编码器-解码器深度神经架构，整合Vision Transformers和对比学习技术，增强从ECoG信号到语音的直接回归能力。在两个数据集上评估：一个来自癫痫患者的临床硬膜下电极数据，另一个来自运动BCI试验参与者使用的完全植入式无线WIMAGINE硬膜外系统。

Result: 据作者所知，这是首次尝试从完全植入式无线硬膜外记录系统解码语音，为长期使用提供了前景。研究展示了从两种不同类型ECoG记录系统重建语音的可行性。

Conclusion: 该研究提出的整合Vision Transformers和对比学习的编码器-解码器架构，为从ECoG信号直接回归语音提供了有效方法，特别是在完全植入式无线硬膜外系统上的成功应用，为长期语音BCI系统开发开辟了新途径。

Abstract: Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.

</details>


### [48] [Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning](https://arxiv.org/abs/2512.04632)
*Thibaut Boissin,Thomas Massena,Franck Mamalet,Mathieu Serrurier*

Main category: cs.AI

TL;DR: 提出一种加速牛顿-舒尔茨正交化收敛的预处理方法，显著降低计算成本，实现2.8倍加速，并在实际训练中带来5-10%的端到端性能提升。


<details>
  <summary>Details</summary>
Motivation: 基于正交性的优化器（如Muon）在大规模训练中表现优异，但依赖昂贵的梯度正交化步骤。即使使用牛顿-舒尔茨等迭代近似方法，仍需要数十次矩阵乘法，计算成本高昂。

Method: 引入预处理程序加速牛顿-舒尔茨收敛，降低计算成本。该方法无需超参数调优，可作为简单替换使用，且预处理开销可忽略不计。

Result: 牛顿-舒尔茨近似实现高达2.8倍加速，在常规5次迭代中可减少1次而不降低近似质量。实际训练场景中端到端运行时间改善5-10%，在语言和视觉任务中保持或提升模型性能。

Conclusion: 提出的预处理方法能显著加速正交化优化器的收敛，降低计算成本，在实际训练中带来明显性能提升，且无需额外调优，易于部署。

Abstract: Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.

</details>


### [49] [Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2512.04691)
*Jae Hee Lee,Anne Lauscher,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文提出从机制可解释性角度研究大型语言模型多智能体系统（MALMs）伦理行为的研究议程，包括评估框架、内部机制解析和参数高效对齐三个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多智能体系统在增强能力和实现复杂任务方面展现出潜力，但也带来了显著的伦理挑战，需要确保这些系统的伦理行为。

Method: 从机制可解释性角度提出研究议程，包括：1）开发评估个体、交互和系统层面伦理行为的综合框架；2）通过机制可解释性阐明涌现行为的内部机制；3）实施有针对性的参数高效对齐技术。

Result: 本文是立场论文，提出了研究议程而非具体实验结果，旨在为MALMs伦理行为研究提供系统性框架。

Conclusion: 需要从机制可解释性角度系统研究MALMs的伦理行为，通过评估、理解和干预三个层面确保多智能体系统的伦理对齐，同时保持其性能。

Abstract: Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.

</details>


### [50] [Playing the Player: A Heuristic Framework for Adaptive Poker AI](https://arxiv.org/abs/2512.04714)
*Andrew Paterson,Carl Sanders*

Main category: cs.AI

TL;DR: 该论文挑战了扑克AI追求不可剥削的完美解的传统观念，提出了Patrick AI，其核心理念是通过最大程度地剥削人类对手的心理缺陷和非理性行为来获胜，而非追求不可剥削性。


<details>
  <summary>Details</summary>
Motivation: 长期以来扑克AI领域被求解器和追求机器完美、不可剥削的玩法所主导。该论文挑战这一正统观念，认为真正的胜利之路不在于不可剥削性，而在于最大程度地剥削人类对手的缺陷。

Method: Patrick AI采用专门设计的架构，旨在理解和攻击人类对手的缺陷、心理和非理性特征。采用新颖的预测锚定学习方法，专注于识别和利用人类玩家的弱点。

Result: 在64,267手牌的试验中，Patrick AI表现出盈利性能，证明了其剥削性策略的有效性。

Conclusion: "求解神话"分散了对真正更有趣挑战的注意力：创建能够掌握人类不完美艺术的AI。Patrick的成功表明，针对人类对手的剥削性策略比追求不可剥削的完美解更有实际价值。

Abstract: For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.

</details>


### [51] [SIMA 2: A Generalist Embodied Agent for Virtual Worlds](https://arxiv.org/abs/2512.04797)
*SIMA team,Adrian Bolton,Alexander Lerchner,Alexandra Cordell,Alexandre Moufarek,Andrew Bolt,Andrew Lampinen,Anna Mitenkova,Arne Olav Hallingstad,Bojan Vujatovic,Bonnie Li,Cong Lu,Daan Wierstra,Daniel P. Sawyer,Daniel Slater,David Reichert,Davide Vercelli,Demis Hassabis,Drew A. Hudson,Duncan Williams,Ed Hirst,Fabio Pardo,Felix Hill,Frederic Besse,Hannah Openshaw,Harris Chan,Hubert Soyer,Jane X. Wang,Jeff Clune,John Agapiou,John Reid,Joseph Marino,Junkyung Kim,Karol Gregor,Kaustubh Sridhar,Kay McKinney,Laura Kampis,Lei M. Zhang,Loic Matthey,Luyu Wang,Maria Abi Raad,Maria Loks-Thompson,Martin Engelcke,Matija Kecman,Matthew Jackson,Maxime Gazeau,Ollie Purkiss,Oscar Knagg,Peter Stys,Piermaria Mendolicchio,Raia Hadsell,Rosemary Ke,Ryan Faulkner,Sarah Chakera,Satinder Singh Baveja,Shane Legg,Sheleem Kashem,Tayfun Terzi,Thomas Keck,Tim Harley,Tim Scholtes,Tyson Roberts,Volodymyr Mnih,Yulan Liu,Zhengdong Wang,Zoubin Ghahramani*

Main category: cs.AI

TL;DR: SIMA 2是基于Gemini基础模型构建的通用具身智能体，能够在多种3D虚拟世界中理解和行动，相比前代支持更复杂的语言和图像指令，接近人类表现并具备自我改进能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在多样化3D虚拟环境中主动、目标导向交互的通用具身智能体，超越之前仅限于简单语言指令的局限，实现更自然的用户交互和复杂任务处理。

Method: 基于Gemini基础模型构建，支持语言和图像输入的复杂指令理解，作为交互伙伴进行高层次目标推理和对话，利用Gemini生成任务和提供奖励实现自主技能学习。

Result: 在多样化游戏组合中显著缩小了与人类表现的差距，展示了对未见环境的强大泛化能力，同时保持了基础模型的核心推理能力，并实现了从零开始在新环境中自主学习新技能。

Conclusion: SIMA 2验证了创建适用于虚拟和未来物理世界的多功能、持续学习智能体的可行路径，代表了向主动、目标导向的具身交互迈出的重要一步。

Abstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

</details>


### [52] [Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing](https://arxiv.org/abs/2512.04829)
*Rasul Tutunov,Alexandre Maraval,Antoine Grosnit,Xihan Li,Jun Wang,Haitham Bou-Ammar*

Main category: cs.AI

TL;DR: 该论文提出了一种基于模型的高效搜索方法，通过将球堆积问题的SDP构造建模为顺序决策过程，在维度4-16中获得了新的最优上界。


<details>
  <summary>Details</summary>
Motivation: 希尔伯特第十八问题（球堆积问题）在密码学、晶体学和医学成像等领域有重要应用，但除了少数特殊维度外，既不知道最优堆积方式，也没有紧的上界。现有的三点法需要求解大型高精度半定规划（SDP），每个候选SDP可能需要数天评估，传统数据密集型AI方法不可行。

Method: 将SDP构造建模为顺序决策过程（SDP博弈），策略从一组允许的组件中组装SDP公式。采用样本高效的基于模型框架，结合贝叶斯优化和蒙特卡洛树搜索。

Result: 在维度4-16中获得了新的最优上界，展示了基于模型的搜索能够在长期存在的几何问题上取得计算进展。

Conclusion: 样本高效的基于模型搜索能够在数学刚性、评估受限的问题上取得实质性进展，为超越大规模LLM驱动探索的AI辅助发现指出了补充方向。

Abstract: Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.

</details>


### [53] [Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case](https://arxiv.org/abs/2512.04834)
*Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera*

Main category: cs.AI

TL;DR: 本文研究了开源多语言大语言模型在理解意大利语电子健康记录并实时提取信息的能力，特别是在共病提取任务上的表现，发现部分模型在零样本、本地部署环境下表现不佳，且不同疾病间的泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 临床记录中的信息提取是数字医疗的关键任务，传统NLP技术因临床语言的复杂性、变异性和高度语义化而表现不足。大语言模型因其强大的理解和生成人类语言能力，成为该领域的有力工具，但需要评估其在特定语言（意大利语）和医疗场景下的实际表现。

Method: 使用开源多语言大语言模型，在意大利语电子健康记录上进行实验，重点研究共病提取任务。采用零样本、本地部署的设置，并与原生模式匹配方法和人工标注结果进行比较。

Result: 实验结果显示，部分大语言模型在零样本、本地部署环境下表现不佳，性能存在显著差异。模型在不同疾病间的泛化能力有限，与原生模式匹配方法和人工标注相比存在差距。

Conclusion: 虽然大语言模型在临床文本理解方面具有潜力，但在特定语言（意大利语）的电子健康记录信息提取任务中，特别是在零样本和本地部署环境下，其表现仍有局限性，需要进一步优化以提高泛化能力和实际应用效果。

Abstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.

</details>


### [54] [From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research](https://arxiv.org/abs/2512.04854)
*Lukas Weidener,Marko Brkić,Chiara Bacci,Mihailo Jovanović,Emre Ulgac,Alex Dobrin,Johannes Weniger,Martin Vlas,Ritvik Singh,Aakaash Meduri*

Main category: cs.AI

TL;DR: 该综述发现当前AI在生物医学研究中的评估框架主要关注孤立组件能力，缺乏对真实研究协作所需整合工作流程的评估，提出了包含对话质量、工作流程编排、会话连续性和研究者体验四个维度的过程导向评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在生物医学研究中部署日益增多，但现有评估框架可能不足以评估其作为研究合作者的有效性。需要审查现有基准测试实践，识别评估差距，并提出更全面的评估方法。

Method: 采用快速综述方法，检索了三个主要数据库和两个预印本服务器（2018年1月1日至2025年10月31日），识别出14个评估AI在文献理解、实验设计和假设生成方面能力的基准测试。

Result: 发现所有现有基准测试仅评估孤立组件能力（数据分析质量、假设有效性、实验方案设计），而真实研究协作需要跨多个会话的整合工作流程，包括上下文记忆、自适应对话和约束传播。系统可能在组件基准上表现出色，但作为实际研究协作者却失败。

Conclusion: 提出了一个过程导向评估框架，包含四个当前基准测试缺乏的关键维度：对话质量、工作流程编排、会话连续性和研究者体验。这些维度对于评估AI作为研究协作者而非孤立任务执行者至关重要。

Abstract: Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.

</details>


### [55] [STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions](https://arxiv.org/abs/2512.04871)
*Junjie Fan,Hongye Zhao,Linduo Wei,Jiayu Rao,Guijia Li,Jiaxin Yuan,Wenqi Xu,Yong Qi*

Main category: cs.AI

TL;DR: STELLA框架通过语义-时间对齐增强LLM在时间序列预测中的能力，通过动态语义抽象机制分解时间序列并生成分层语义锚点，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在时间序列预测中的适配方法未能有效增强原始序列信息，LLM的推理能力未被充分利用。现有提示策略依赖静态相关性而非对动态行为的生成性解释，缺乏关键的全局和实例特定上下文。

Method: 提出STELLA框架，采用动态语义抽象机制将输入序列解耦为趋势、季节性和残差分量，将这些分量的内在行为特征转化为分层语义锚点：用于全局上下文的语料级语义先验(CSP)和用于实例级模式的细粒度行为提示(FBP)，将这些锚点作为前缀提示来指导LLM建模内在动态。

Result: 在八个基准数据集上的实验表明，STELLA在长期和短期预测中都优于最先进的方法，在零样本和少样本设置中表现出卓越的泛化能力。消融研究进一步验证了动态生成的语义锚点的有效性。

Conclusion: STELLA通过系统地挖掘和注入结构化的补充和互补信息，有效解决了LLM在时间序列预测中信息增强不足的问题，显著提升了预测性能并展示了强大的泛化能力。

Abstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.

</details>


### [56] [Algorithmic Thinking Theory](https://arxiv.org/abs/2512.04923)
*MohammadHossein Bateni,Vincent Cohen-Addad,Yuzhou Gu,Silvio Lattanzi,Simon Meierhans,Christopher Mohri*

Main category: cs.AI

TL;DR: 论文提出了一个理论框架来分析基于LLM的迭代推理算法，将推理计划视为使用概率神谕的算法，为设计更强大的推理方法提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现出色，但研究发现通过迭代改进先前生成的解决方案可以进一步提升其能力。目前缺乏一个统一的理论框架来分析这类迭代推理算法，无法为设计更强大的推理方法提供理论基础。

Method: 引入一个理论框架，将推理计划形式化为使用概率神谕的算法。该框架基于实验证据而非架构细节，能够形式化迭代改进和答案聚合等流行技术的基本原理。

Result: 提出了一个通用的理论框架，能够分析基于LLM的迭代推理算法，为理解当前流行的推理技术提供了理论基础，并为设计新一代更强大的推理方法奠定了基础。

Conclusion: 该理论框架为分析基于概率神谕的推理算法提供了通用视角，能够扩展到当前和未来的各种推理神谕，为设计更有效的迭代推理方法提供了理论指导。

Abstract: Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.

</details>


### [57] [Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases](https://arxiv.org/abs/2512.04938)
*Raquel Norel,Michele Merler,Pavitra Modi*

Main category: cs.AI

TL;DR: 通过智能手机语音分析结合关系图变换器架构，实现对罕见神经系统疾病患者认知症状的连续监测，在苯丙酮尿症中验证了语音指标与血液生化指标的相关性


<details>
  <summary>Details</summary>
Motivation: 传统认知测试无法有效检测罕见神经系统疾病患者的"脑雾"症状，需要开发连续、客观的监测方法来捕捉这些难以察觉的认知变化

Method: 采用智能手机语音分析结合关系图变换器架构，整合语音、实验室数据和临床评估等多源异构医疗数据，实现对认知功能的连续监测

Result: 在苯丙酮尿症的概念验证中，语音衍生的"言语表达熟练度"指标与血液苯丙氨酸水平显著相关（p = -0.50, p < 0.005），而与传统认知测试相关性较弱（|r| < 0.35）

Conclusion: 关系图变换器架构能够克服异构医疗数据的信息瓶颈，有望在症状恶化前数周提供预测性警报，将间歇性神经学评估转变为连续个性化监测

Abstract: Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived "Proficiency in Verbal Discourse" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.

</details>


### [58] [Detecting Perspective Shifts in Multi-agent Systems](https://arxiv.org/abs/2512.05013)
*Eric Bridgeford,Hayden Helm*

Main category: cs.AI

TL;DR: 本文提出了Temporal Data Kernel Perspective Space (TDKPS)框架，用于在时间维度上联合嵌入多智能体系统中的智能体，并提出了几种新颖的假设检验方法来检测黑盒多智能体系统中智能体和群体层面的行为变化。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体工具的广泛应用，动态多智能体系统自然涌现。现有研究主要关注基于单一时间点查询响应的低维智能体表示，缺乏对智能体行为随时间变化的系统性监测框架，这在智能体部署规模不断扩大时尤为关键。

Method: 提出TDKPS框架，在时间维度上联合嵌入智能体；开发了多种假设检验方法，用于检测智能体层面和群体层面的行为变化；通过模拟实验验证方法的敏感性，并在自然实验中测试其检测真实外生事件相关变化的能力。

Result: TDKPS能够敏感、特异且显著地检测与真实外生事件相关的行为变化。模拟实验表明该方法对关键超参数具有敏感性，自然实验验证了其在实际应用中的有效性。

Conclusion: TDKPS是首个用于监测黑盒多智能体系统行为动态的原则性框架，为生成式智能体大规模部署提供了关键的监测能力。

Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.

</details>
