<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CR](#cs.CR) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: LiveMedBench是一个持续更新的、无数据污染的、基于评分标准的医学基准测试，通过每周从在线医疗社区收集真实临床案例，解决现有医学基准测试的数据污染和时间错位问题，并开发了自动化的基于评分标准的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准测试存在两个关键限制：1) 数据污染问题，测试集无意中泄露到训练语料中，导致性能估计虚高；2) 时间错位问题，无法捕捉医学知识的快速演变。此外，当前开放式临床推理的评估指标要么依赖浅层词汇重叠，要么依赖主观的LLM-as-a-Judge评分，都不足以验证临床正确性。

Method: 1) 引入LiveMedBench基准测试，每周从在线医疗社区收集真实临床案例，确保与模型训练数据的严格时间分离；2) 提出多智能体临床筛选框架，过滤原始数据噪声并根据循证医学原则验证临床完整性；3) 开发自动化的基于评分标准的评估框架，将医生回答分解为细粒度的、病例特定的标准。

Result: LiveMedBench目前包含2,756个真实世界病例，涵盖38个医学专业和多种语言，配有16,702个独特的评估标准。对38个LLM的广泛评估显示，即使表现最好的模型也只达到39.2%的准确率，84%的模型在截止日期后的病例上表现出性能下降，证实了普遍存在的数据污染风险。错误分析进一步识别出上下文应用（而非事实知识）是主要瓶颈，35-48%的失败源于无法将医学知识适应于患者特定的约束。

Conclusion: LiveMedBench为临床LLM评估提供了一个动态、无污染、基于评分标准的基准测试，揭示了当前LLM在临床推理中的局限性，特别是将医学知识适应于具体患者情境的能力不足，强调了开发更稳健的临床AI系统需要关注上下文应用能力而非仅仅事实知识。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [2] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: Found-RL是一个专门为自动驾驶设计的强化学习平台，通过异步批量推理框架将视觉语言模型的知识高效集成到RL训练中，解决了VLM推理延迟问题，使轻量级RL模型能达到接近VLM的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶中存在样本效率低和语义可解释性不足的问题，而基础模型（特别是视觉语言模型）虽然能提供丰富的上下文感知知识，但其高推理延迟阻碍了在高频RL训练循环中的部署。

Method: 1. 异步批量推理框架：将繁重的VLM推理与仿真循环解耦，解决延迟瓶颈；2. 多样化监督机制：包括价值边际正则化和优势加权动作指导，将VLM专家动作建议蒸馏到RL策略中；3. 高吞吐量CLIP用于密集奖励塑造；4. 条件对比动作对齐：通过基于离散化速度/命令的条件提示解决CLIP的动态盲点问题。

Result: Found-RL提供了一个端到端的微调VLM集成管道，轻量级RL模型能够达到接近十亿参数VLM的性能，同时保持实时推理（约500 FPS）。

Conclusion: Found-RL成功地将基础模型的高质量知识与强化学习的高效训练相结合，为自动驾驶领域提供了一种既具有语义理解能力又保持实时性能的解决方案。

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [3] [MERIT Feedback Elicits Better Bargaining in LLM Negotiators](https://arxiv.org/abs/2602.10467)
*Jihwan Oh,Murad Aghazada,Yooju Shin,Se-Young Yun,Taehyeon Kim*

Main category: cs.AI

TL;DR: 提出了AgoraBench基准测试和基于效用反馈的框架，通过人类偏好对齐的指标和数据集增强LLM的谈判能力


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在谈判任务中表现不佳，缺乏战略深度和适应复杂人类因素的能力，而当前基准测试未能充分捕捉这些局限性

Method: 提出了基于效用反馈的框架，包括：1）AgoraBench基准测试，涵盖9个具有挑战性的场景；2）基于效用理论的人类对齐经济指标；3）基于人类偏好的数据集和学习流程

Result: 基线LLM策略常偏离人类偏好，而提出的机制显著提升了谈判性能，产生了更深层的战略行为和更强的对手意识

Conclusion: 通过效用反馈框架、综合基准测试和人类偏好对齐方法，能够有效增强LLM的谈判能力，使其更接近人类谈判行为

Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

</details>


### [4] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: LLMs能够生成用于广义规划的定性数值规划抽象，结合自动调试方法可以修正抽象错误


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够作为QNP抽象生成器来解决广义规划问题，以及如何通过自动调试来修正抽象

Method: 提出提示协议：输入GP领域和训练任务给LLMs，让它们生成抽象特征并将初始状态、动作集和目标抽象为QNP问题；设计自动调试方法来检测抽象错误并指导LLMs修正

Result: 实验表明，在自动调试的适当指导下，一些LLMs能够生成有用的QNP抽象

Conclusion: LLMs可以作为QNP抽象生成器，结合自动调试方法能够有效生成和修正抽象，为广义规划问题提供解决方案

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [5] [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583)
*Bo Xue,Yunchong Song,Fanghao Shao,Xuekai Zhu,Lin Chen,Luoyi Fu,Xinbing Wang,Zhouhan Lin*

Main category: cs.AI

TL;DR: FoSS提出基于GFlowNets的跨度生成框架，通过动态跨度词汇和DAG状态空间提升文本生成的多样性和质量


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型基于固定词汇的token级生成形成树状状态空间，限制了灵活性和表达能力。现有动态词汇方法虽然引入检索文本跨度，但忽略了同一句子可由不同长度跨度组合的可能性，缺乏对DAG状态空间的显式建模，导致组合路径探索受限和路径选择偏差。

Method: 提出Flow of SpanS (FoSS)框架：1) 通过灵活分割检索文本构建动态跨度词汇；2) 确保形成DAG结构的状态空间；3) 利用GFlowNets探索多样组合路径；4) 结合专用奖励模型生成高质量文本。

Result: FoSS在文本生成任务上比Transformer提升MAUVE分数达12.5%，在知识密集型任务上获得3.5%的增益，持续优于最先进方法。扩展实验表明FoSS受益于更大模型、更多数据和更丰富的检索语料库。

Conclusion: FoSS通过将GFlowNets应用于跨度生成，构建DAG状态空间，有效解决了传统方法在组合路径探索和泛化方面的限制，为文本生成提供了更灵活、多样且高质量的解决方案。

Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.

</details>


### [6] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: NSAM是一个神经符号动作屏蔽框架，能自动学习与高维状态约束一致的符号模型，在DRL训练中减少不可行动作探索并提高样本效率


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在训练和执行过程中可能探索不可行的动作。现有方法需要手动指定符号接地函数和动作屏蔽技术，限制了方法的自动化程度和适应性。

Method: 提出神经符号动作屏蔽（NSAM）框架，在最小监督下自动学习与领域约束一致的符号模型，基于学习的符号状态模型生成动作屏蔽，排除不可行动作，实现符号推理与深度策略优化的端到端集成。

Result: 在多个约束领域评估NSAM，实验结果表明NSAM显著提高了DRL智能体的样本效率，同时大幅减少了约束违反。

Conclusion: NSAM通过自动学习符号模型和动作屏蔽，实现了符号推理与深度强化学习的有效集成，提高了训练效率和安全性，为约束环境下的智能体学习提供了新方法。

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [7] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 研究发现大型推理模型在心理理论任务上表现不佳，推理能力无法完全迁移到社会认知领域，存在慢思考崩溃、选项匹配捷径等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在数学和编程等领域的逐步推理能力有所提升，但尚不清楚这种优势是否能迁移到社会认知技能（如心理理论）上。本研究旨在系统评估推理模型在心理理论任务上的表现。

Method: 对9个先进的大型语言模型进行系统研究，比较推理模型与非推理模型在三个代表性心理理论基准测试上的表现。通过细粒度分析揭示性能差异，并设计了两种干预方法：慢到快自适应推理和思考到匹配捷径预防。

Result: 推理模型在心理理论任务上并不总是优于非推理模型，有时表现更差。分析发现：1）慢思考崩溃：回答越长准确率越低，更大的推理预算反而损害性能；2）适度自适应推理有益：限制推理长度可缓解失败，不同成功模式显示需要动态适应；3）选项匹配捷径：移除多项选择选项后推理模型表现显著改善，表明其依赖选项匹配而非真正推理。

Conclusion: 大型推理模型在形式推理（如数学、编程）方面的进步无法完全迁移到心理理论这种典型的社会推理任务上。实现稳健的心理理论需要开发超越现有推理方法的独特能力。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [8] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: HARPO方法通过调节优势函数平衡异构任务和样本的学习，训练出Omnisapiens-7B 2.0社交行为基础模型，在多项任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立地建模人类行为维度（情感、认知或社会属性），任务特定建模增加了训练成本并限制了跨行为设置的泛化能力。虽然最近的推理RL方法支持在多个行为任务上训练统一模型，但未明确解决跨异构行为数据的学习问题。

Method: 提出异构感知相对策略优化（HARPO），这是一种RL方法，通过调节优势函数来确保在策略优化过程中没有任何单个任务或样本产生不成比例的影响，从而平衡跨异构任务和样本的学习。

Result: 使用HARPO开发了Omnisapiens-7B 2.0社交行为处理基础模型。相对于现有行为基础模型，在多任务和保留设置上分别实现了高达+16.85%和+9.37%的性能提升，同时产生更明确和鲁棒的推理轨迹。HARPO在行为任务上也比最近的RL方法表现更一致。

Conclusion: HARPO方法有效解决了跨异构行为数据学习的问题，训练出的Omnisapiens-7B 2.0模型在社交行为处理任务上表现出色，为开发社会智能AI提供了有效工具。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [9] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: V-STAR框架通过价值引导采样和树状优势强化学习解决生成式推荐中RL训练的概率-奖励不匹配问题，提升探索效率和多样性


<details>
  <summary>Details</summary>
Motivation: 生成式推荐的自回归模型在RL微调中存在概率-奖励不匹配问题：传统基于似然的解码（如束搜索）对局部概率高的前缀存在短视偏见，导致探索不足（高奖励但低概率分支被过早剪枝）和优势压缩（共享高概率前缀的轨迹奖励高度相关，方差低，RL信号弱）

Method: 提出V-STAR框架，包含两个协同组件：1）价值引导高效解码（VED），识别决策节点并选择性加深高潜力前缀，提升探索效率；2）Sibling-GRPO，利用诱导的树拓扑计算兄弟相对优势，将学习信号集中在决策分支决策上

Result: 在离线和在线数据集上的广泛实验表明，V-STAR在严格延迟约束下优于最先进的基线方法，提供更高的准确性和候选集多样性

Conclusion: V-STAR通过价值引导采样和树状优势强化学习有效解决了生成式推荐中RL训练的概率-奖励不匹配问题，实现了更好的探索效率和性能表现

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [10] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: 研究调查了高等教育中生成式AI的感知差异，特别关注ITEE领域，通过混合方法识别了共享和学科特定的主题，提出了负责任整合的框架。


<details>
  <summary>Details</summary>
Motivation: 高等教育中生成式AI工具已被广泛采用，但利益相关者的看法因文化、学科和制度背景而异。欧盟AI法案要求大学确保认知系统的监管合规性，因此需要了解不同学科对GenAI的感知，以制定负责任的整合策略。

Method: 采用混合方法，对奥卢大学ITEE学院的61名教职员工和37名学生进行了调查，分析他们对生成式AI的感知和态度。

Result: 研究揭示了共享和学科特定的主题：ITEE领域对GenAI在编程支持方面有强烈兴趣，但也关注响应质量、隐私和学术诚信问题。研究识别了一套高层次要求，并提出了负责任GenAI整合的概念框架。

Conclusion: 学科特定的要求强调了在高等教育中整合GenAI时利益相关者参与的重要性。高层次要求和框架为大学利用GenAI同时解决利益相关者关切和确保监管合规提供了实用指导。

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [11] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: SynergyKGC通过跨模态协同专家和密度依赖的身份锚定策略，解决了知识图谱补全中结构分辨率不匹配问题，显著提升了命中率。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法面临"结构分辨率不匹配"问题，无法协调不同图密度下的表示需求，导致密集簇中的结构噪声干扰和稀疏区域的表示崩溃。

Method: 提出SynergyKGC框架：1) 将传统邻居聚合提升为基于关系感知交叉注意力和语义意图驱动门控的主动跨模态协同专家；2) 结合密度依赖的身份锚定策略；3) 采用双塔一致性架构确保表示稳定性。

Result: 在两个公共基准测试上的系统评估验证了方法的优越性，显著提升了KGC命中率，为非均匀结构化数据中的弹性信息整合提供了经验证据。

Conclusion: SynergyKGC通过自适应框架有效协调拓扑异质性，确保训练和推理阶段的表示稳定性，为知识图谱补全中的弹性信息整合提供了通用原则。

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [12] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RLCER：一种无需人工标注的自监督强化学习方法，通过自我提出和演进的评分标准来奖励思维链推理过程


<details>
  <summary>Details</summary>
Motivation: 传统思维链奖励方法面临两大挑战：1）训练奖励模型需要大量人工标注；2）静态奖励模型难以适应思维链分布的动态变化且容易受到奖励攻击。需要一种无需人工标注且能自主演进的思维链奖励方法。

Method: 提出RLCER（强化学习与思维链监督通过自我演进评分标准），在结果中心的RLVR基础上，通过自我提出和演进的评分标准来奖励思维链。该方法能够提供可靠的思维链监督信号，即使在没有结果奖励的情况下也能工作。

Result: RLCER在性能上超越了结果中心的RLVR方法。此外，当将这些自我提出的评分标准作为提示中的提示使用时，还能进一步提升推理时的性能表现。

Conclusion: 自我提出和演进的评分标准能够为思维链提供有效的监督信号，实现无需人工标注的自监督强化学习，并在推理性能上取得显著提升。

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [13] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: LLMs在文化内容生成中存在系统性文化偏见，无法有效适应不同文化背景，特别是在烹饪食谱这一文化敏感领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于生成和塑造文化内容，但研究表明它们存在系统性文化偏见，可能导致刻板印象、同质化和特定文化表达形式的消失。理解LLMs是否能超越主流文化而真正适应多元文化是一个关键挑战。

Method: 通过烹饪食谱这一文化、传统和创造力紧密交织的领域研究LLMs的文化适应能力。使用GlobalFusion数据集，该数据集根据文化距离测量将不同国家的人类食谱配对。使用相同的国家配对，用多个LLMs生成文化适应食谱，从而直接比较人类和LLM在跨文化内容创作中的行为。

Result: LLMs无法产生具有文化代表性的适应内容。与人类不同，它们生成的食谱差异与文化距离不相关。研究发现：文化信息在模型内部表示中保存较弱；模型通过误解创造性和传统等概念来夸大新颖性；无法将适应内容与其相关国家联系起来，也无法将其建立在文化显著元素（如食材）上。

Conclusion: 当前LLMs在文化导向的生成方面存在根本性局限性，这对它们在文化敏感应用中的使用具有重要影响。需要更好的方法来确保LLMs能够真正理解和适应多元文化表达。

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis](https://arxiv.org/abs/2602.10246)
*Mayur Akewar,Sandeep Madireddy,Dongsheng Luo,Janki Bhimani*

Main category: cs.DC

TL;DR: KORAL是一个结合大型语言模型和知识图谱的SSD诊断框架，能从碎片化数据生成专家级分析，减少人工干预，提供可解释的推理。


<details>
  <summary>Details</summary>
Motivation: SSD性能与可靠性诊断困难，数据碎片化且时间不连续，现有方法需要大量数据集和专家输入，但只能提供有限洞察。性能退化不仅来自工作负载变化和架构演进，还受温度、湿度、振动等环境因素影响。

Method: 提出KORAL框架，将大型语言模型与结构化知识图谱结合。从碎片化遥测数据生成数据知识图谱，并集成已组织文献、报告和跟踪数据的文献知识图谱。将非结构化源转化为可查询图，遥测数据转化为结构化知识，两者共同指导LLM提供基于证据、可解释的分析。

Result: 使用真实生产跟踪数据评估显示，KORAL能提供专家级诊断和建议，支持基于证据的解释，提高推理透明度，指导操作员决策，减少人工工作量，并提供可操作的见解以改善服务质量。

Conclusion: KORAL是首个结合LLM和KG实现全谱SSD推理（描述性、预测性、规范性、假设分析）的端到端系统。发布SSD特定知识图谱以推进基于知识的存储系统分析的可重复研究。

Abstract: Solid State Drives (SSDs) are critical to datacenters, consumer platforms, and mission-critical systems. Yet diagnosing their performance and reliability is difficult because data are fragmented and time-disjoint, and existing methods demand large datasets and expert input while offering only limited insights. Degradation arises not only from shifting workloads and evolving architectures but also from environmental factors such as temperature, humidity, and vibration. We present KORAL, a knowledge driven reasoning framework that integrates Large Language Models (LLMs) with a structured Knowledge Graph (KG) to generate insights into SSD operations. Unlike traditional approaches that require extensive expert input and large datasets, KORAL generates a Data KG from fragmented telemetry and integrates a Literature KG that already organizes knowledge from literature, reports, and traces. This turns unstructured sources into a queryable graph and telemetry into structured knowledge, and both the Graphs guide the LLM to deliver evidence-based, explainable analysis aligned with the domain vocabulary and constraints. Evaluation using real production traces shows that the KORAL delivers expert-level diagnosis and recommendations, supported by grounded explanations that improve reasoning transparency, guide operator decisions, reduce manual effort, and provide actionable insights to improve service quality. To our knowledge, this is the first end-to-end system that combines LLMs and KGs for full-spectrum SSD reasoning including Descriptive, Predictive, Prescriptive, and What-if analysis. We release the generated SSD-specific KG to advance reproducible research in knowledge-based storage system analysis. GitHub Repository: https://github.com/Damrl-lab/KORAL

</details>


### [15] [Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A](https://arxiv.org/abs/2602.10262)
*Aaron Jarmusch,Connor Vitz,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: 本文通过微基准测试对AMD MI300A APU的FP8矩阵执行、异步计算引擎并发性和结构化稀疏性进行执行中心化表征，量化了占用率阈值、公平性、并发执行下的吞吐量权衡以及上下文相关的稀疏性优势，为MI300A类统一节点的占用感知调度、并发决策和稀疏性启用提供实用指导。


<details>
  <summary>Details</summary>
Motivation: AMD MI300A APU集成了CDNA3 GPU、高带宽内存和先进加速器特性（FP8矩阵核心、异步计算引擎、2:4结构化稀疏性）。这些功能被现代HPC和HPC-AI工作负载日益依赖，但其执行特性和系统级影响仍未被充分理解。

Method: 使用针对性微基准测试对MI300A进行执行中心化表征，量化FP8矩阵执行、ACE并发性和结构化稀疏性的性能特征。评估代表性案例研究，包括transformer风格、并发和混合精度内核，以展示这些效应如何转化为应用级性能和可预测性。

Result: 量化了占用率阈值、公平性、并发执行下的吞吐量权衡以及上下文相关的稀疏性优势。通过案例研究展示了这些效应在实际应用中的表现，为MI300A类系统的性能优化提供了实证基础。

Conclusion: 研究结果为MI300A类统一节点的占用感知调度、并发决策和稀疏性启用提供了实用指导，有助于优化现代HPC和HPC-AI工作负载在先进APU架构上的性能表现。

Abstract: The AMD MI300A APU integrates CDNA3 GPUs with high-bandwidth memory and advanced accelerator features: FP8 matrix cores, asynchronous compute engines (ACE), and 2:4 structured sparsity. These capabilities are increasingly relied upon by modern HPC and HPC-AI workloads, yet their execution characteristics and system-level implications remain insufficiently understood. In this paper, we present an execution-centric characterization of FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A using targeted microbenchmarks. We quantify occupancy thresholds, fairness, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. We evaluate representative case studies - transformer-style, concurrent, and mixed-precision kernels - to show how these effects translate into application-level performance and predictability. Our results provide practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes.

</details>


### [16] [Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores](https://arxiv.org/abs/2602.10378)
*Elliot L. Epstein,Rajat Vadiraj Dwaraknath,John Winnicki*

Main category: cs.DC

TL;DR: Flash-SD-KDE通过重新组织计算结构利用Tensor Cores加速GPU实现，在32k样本16维问题上比基线快47倍，在1M样本任务上2.3秒完成，使分数去偏密度估计达到实用规模。


<details>
  <summary>Details</summary>
Motivation: 分数去偏核密度估计(SD-KDE)虽然比经典KDE有更好的渐近收敛率，但由于使用经验分数，实际计算速度显著较慢，限制了其在大规模问题上的应用。

Method: 通过重新组织SD-KDE计算以暴露矩阵乘法结构，利用GPU的Tensor Cores进行加速，开发了Flash-SD-KDE实现。

Result: 在32k样本16维问题上，比强SD-KDE GPU基线快47倍，比scikit-learn的KDE快3300倍；在1M样本16维任务上（131k查询），单GPU仅需2.3秒完成。

Conclusion: Flash-SD-KDE使分数去偏密度估计在以前不可行的规模上变得实用，显著提升了计算效率。

Abstract: Score-debiased kernel density estimation (SD-KDE) achieves improved asymptotic convergence rates over classical KDE, but its use of an empirical score has made it significantly slower in practice. We show that by re-ordering the SD-KDE computation to expose matrix-multiplication structure, Tensor Cores can be used to accelerate the GPU implementation. On a 32k-sample 16-dimensional problem, our approach runs up to $47\times$ faster than a strong SD-KDE GPU baseline and $3{,}300\times$ faster than scikit-learn's KDE. On a larger 1M-sample 16-dimensional task evaluated on 131k queries, Flash-SD-KDE completes in $2.3$ s on a single GPU, making score-debiased density estimation practical at previously infeasible scales.

</details>


### [17] [Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems](https://arxiv.org/abs/2602.10486)
*Vijay K. Garg,Rohan Garg*

Main category: cs.DC

TL;DR: 该论文提出了在并行和分布式环境中计算多个单调膨胀函数最小不动点的方法，针对覆盖语义、非原子更新和过时读取等现代计算特性，证明了三种渐进放松同步条件下的收敛定理。


<details>
  <summary>Details</summary>
Motivation: 经典Knaster-Tarski定理处理单函数顺序迭代，而现代计算系统需要并行执行，具有覆盖语义、非原子更新和过时读取等特点。现有方法如Cousot-Cousot的混沌迭代使用基于连接的合并操作，Bertsekas的异步方法假设收缩条件，都不适用于覆盖语义的并行更新场景。

Method: 采用坐标覆盖方法（而非连接合并），在三种渐进放松的同步条件下证明收敛性：1）交错语义与公平调度；2）仅当值变化时更新的并行执行；3）具有有限过时性（更新在T轮内传播）和i-局部性（每个进程只修改自身组件）的分布式执行。

Result: 证明了三种收敛定理，为覆盖语义的并行更新提供了首个精确的最小不动点收敛保证，无需连接操作或收缩假设。应用包括传递闭包、稳定婚姻、最短路径和带补贴的公平分配等并行分布式算法。

Conclusion: 该研究首次为基于覆盖的并行更新提供了精确的最小不动点收敛保证，填补了经典不动点理论与现代并行分布式计算需求之间的空白，为相关算法提供了理论基础。

Abstract: We present methods to compute least fixed points of multiple monotone inflationary functions in parallel and distributed settings. While the classic Knaster-Tarski theorem addresses a single function with sequential iteration, modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads. We prove three convergence theorems under progressively relaxed synchronization: (1) Interleaving semantics with fair scheduling, (2) Parallel execution with update-only-on-change semantics (processes write only on those coordinates whose values change), and (3) Distributed execution with bounded staleness (updates propagate within $T$ rounds) and $i$-locality (each process modifies only its own component).
  Our approach differs from prior work in fundamental ways: Cousot-Cousot's chaotic iteration uses join-based merges that preserve information. Instead, we use coordinate-wise overwriting. Bertsekas's asynchronous methods assume contractions. We use coordinate-wise overwriting with structural constraints (locality, bounded staleness) instead. Applications include parallel and distributed algorithms for the transitive closure, stable marriage, shortest paths, and fair division with subsidy problems. Our results provide the first exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.

</details>


### [18] [BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.10729)
*Youhe Jiang,Fangcheng Fu,Eiko Yoneki*

Main category: cs.DC

TL;DR: BOute是一个质量感知调度系统，通过联合利用异构模型和GPU能力，使用多目标贝叶斯优化框架共同优化路由策略和模型部署，实现成本高效的LLM服务。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型部署的快速增长，需要成本高效的服务系统。现有方法要么从算法角度利用异构模型能力进行查询路由，要么从系统角度利用异构GPU资源进行模型部署，但缺乏算法-系统协同设计来最大化整体系统性能。

Method: 提出BOute系统，采用多目标贝叶斯优化框架，共同优化路由策略和模型部署配置。系统需要确定在延迟和质量要求下的最优查询路由策略，在异构GPU上配置模型部署，并协同优化路由和部署决策。

Result: 在相同成本预算和质量要求下，BOute比最先进的LLM服务系统性能提升最高157%，平均59%；或在保持相同性能目标下，服务成本降低15%-61%，平均38%。

Conclusion: BOute通过算法-系统协同设计，有效实现了成本高效的LLM服务，验证了其在实际部署中的有效性。

Abstract: The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.

</details>


### [19] [Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots](https://arxiv.org/abs/2602.11125)
*Animesh Maiti,Abhinav Chakraborty,Bibhuti Das,Subhash Bhagat,Krishnendu Mukhopadhyaya*

Main category: cs.DC

TL;DR: 研究移动机器人在线段和圆上的最小总移动距离均匀覆盖问题，提出确定性分布式算法实现最优成本覆盖


<details>
  <summary>Details</summary>
Motivation: 研究自主、匿名、同质的移动机器人在异步调度下如何协调运动以达到均匀分布配置，同时最小化所有机器人移动的总距离。在线段和圆两种几何设置下，探索在有限通信和记忆约束下的最优覆盖问题。

Method: 采用Look-Compute-Move（LCM）模型，机器人具有非刚性运动能力，在公平异步调度下运行。提出确定性分布式算法：在线段设置中实现均匀间距放置；在圆设置中，首先确定不可解配置，然后为其他配置提供算法实现均匀分布形成正n边形。

Result: 在线段设置中，算法成功实现最小总移动成本的均匀覆盖。在圆设置中，完整刻画了确定性不可解的初始配置，并为所有其他配置提供了实现最小总移动距离的确定性分布式算法。

Conclusion: 该研究完整刻画了在考虑机器人模型下最小总距离覆盖问题的确定性可解性，并在可解情况下实现了最优成本。结果为无记忆、无通信的机器人在线段和圆上的均匀覆盖问题提供了理论保证和实用算法。

Abstract: We study the \textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \textit{Look-Compute-Move} (LCM) model with \textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \textbf{line-segment setting}, the \textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [ACE-RTL: When Agentic Context Evolution Meets RTL-Specialized LLMs](https://arxiv.org/abs/2602.10218)
*Chenhui Deng,Zhongzhi Yu,Guan-Ting Liu,Nathaniel Pinckney,Haoxing Ren*

Main category: cs.AR

TL;DR: ACE-RTL统一了硬件设计自动化的两种主流方法，通过Agentic Context Evolution结合专用RTL模型和前沿推理LLM，显著提升Verilog代码生成准确率。


<details>
  <summary>Details</summary>
Motivation: 现有硬件设计自动化方法存在两种独立路径：训练领域适应的RTL模型和基于仿真反馈的智能体系统，两者各有优缺点但缺乏统一框架。

Method: 提出ACE-RTL框架，集成在170万RTL样本上训练的专用LLM和前沿推理LLM，通过生成器、反射器和协调器三个协同组件迭代优化RTL代码，并引入并行扩展策略减少迭代次数。

Result: 在CVDP基准测试中，ACE-RTL相比14个竞争基线提升44.87%的通过率，平均仅需4次迭代即可达到正确解决方案。

Conclusion: ACE-RTL成功统一了硬件设计自动化的两种主流方法，通过智能体上下文演化显著提升了RTL代码生成的功能正确性和效率。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest in applying them to hardware design automation, particularly for accurate RTL code generation. Prior efforts follow two largely independent paths: (i) training domain-adapted RTL models to internalize hardware semantics, (ii) developing agentic systems that leverage frontier generic LLMs guided by simulation feedback. However, these two paths exhibit complementary strengths and weaknesses. In this work, we present ACE-RTL that unifies both directions through Agentic Context Evolution (ACE). ACE-RTL integrates an RTL-specialized LLM, trained on a large-scale dataset of 1.7 million RTL samples, with a frontier reasoning LLM through three synergistic components: the generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness. We further introduce a parallel scaling strategy that significantly reduces the number of iterations required to reach correct solutions. On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to a 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.

</details>


### [21] [Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching](https://arxiv.org/abs/2602.10254)
*Hanyuan Gao,Xiaoxuan Yang*

Main category: cs.AR

TL;DR: 提出了一种面向MoE变压器的面积高效存内计算架构，通过交叉开关级复用、专家分组调度和门输出缓存技术，显著提升了面积效率和生成性能。


<details>
  <summary>Details</summary>
Motivation: MoE层通过激活部分专家权重提升模型性能，特别适合存内计算架构部署，但现有PIM芯片存在面积开销大的问题，尤其是外围电路占用过多面积。

Method: 1) 提出交叉开关级复用策略，利用MoE稀疏性让多个交叉开关共享外围电路；2) 提出专家分组和组级调度方法缓解共享带来的负载不平衡和争用开销；3) 设计门输出缓存存储必要结果，避免生成阶段访问所有隐藏状态的开销。

Result: MoE部分面积效率相比SOTA架构提升2.2倍；生成8个token时，缓存使性能和能效分别提升4.2倍和10.1倍；总体性能密度达到15.6 GOPS/W/mm²。

Conclusion: 该工作为MoE变压器提供了一种面积高效的存内计算架构解决方案，通过创新的复用策略、调度方法和缓存机制，在保持高性能的同时显著降低了硬件开销。

Abstract: Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.

</details>


### [22] [DRAMPyML: A Formal Description of DRAM Protocols with Timed Petri Nets](https://arxiv.org/abs/2602.10654)
*Derek Christ,Thomas Zimmermann,Philippe Barbie,Dmitri Saberi,Yao Yin,Matthias Jung*

Main category: cs.AR

TL;DR: 该论文提出了一种基于时间Petri网和Python的DRAM协议建模方法，以解决JEDEC标准中简化状态机无法反映内存库并行操作的问题，使协议更易理解并可执行验证。


<details>
  <summary>Details</summary>
Motivation: JEDEC定义的DRAM标准协议日益复杂，时序图和命令表难以理解，特别是新特性和复杂设备层次结构。现有的简化状态机无法反映内存库的并行操作，需要更精确的建模方法。

Method: 采用基于时间Petri网和Python的建模方法，构建更准确的DRAM协议模型。该模型可直接执行，支持协议验证和性能评估。

Result: 提出的建模方法能够更准确地表示DRAM协议，使协议更易于理解，并支持直接执行验证控制器RTL模型、DRAM逻辑和内存模拟器，还能评估相关性能指标。

Conclusion: 基于时间Petri网和Python的建模方法为DRAM协议提供了更精确、可执行的表示，解决了传统状态机模型的局限性，有助于协议理解、验证和性能评估。

Abstract: The JEDEC committee defines various domain-specific DRAM standards. These standards feature increasingly complex and evolving protocol specifications, which are detailed in timing diagrams and command tables. Understanding these protocols is becoming progressively challenging as new features and complex device hierarchies are difficult to comprehend without an expressive model. While each JEDEC standard features a simplified state machine, this state machine fails to reflect the parallel operation of memory banks.
  In this paper, we present an evolved modeling approach based on timed Petri nets and Python. This model provides a more accurate representation of DRAM protocols, making them easier to understand and directly executable, which enables the evaluation of interesting metrics and the verification of controller RTL models, DRAM logic and memory simulators.

</details>


### [23] [Fault Tolerant Design of IGZO-based Binary Search ADCs](https://arxiv.org/abs/2602.10790)
*Paula Carolina Lozano Duarte,Sule Ozev,Mehdi Tahoori*

Main category: cs.AR

TL;DR: 提出分层故障注入框架分析IGZO柔性电子中ADC的缺陷敏感性，通过选择性冗余策略将故障覆盖率从60%提升至92%，面积开销仅4.2%


<details>
  <summary>Details</summary>
Motivation: IGZO等薄膜技术虽能实现柔性电子，但其单极性技术相比成熟CMOS具有更高的缺陷密度和工艺变化，而ADC作为关键传感器接口的制造缺陷敏感性尚未被充分理解

Method: 提出分层故障注入框架，结合晶体管级缺陷表征和系统级故障传播分析，高效探索转换层次中的单故障和多故障场景，识别关键故障敏感电路组件

Result: 缺陷容忍设计将单故障注入下的故障覆盖率从60%提升至92%，多故障注入下从34%提升至77.6%，仅带来4.2%的面积开销和6%的功耗增加

Conclusion: 该框架不仅适用于IGZO-TFTs验证，还可推广到所有新兴单极性技术，通过选择性冗余策略有效提升柔性电子系统中ADC的可靠性

Abstract: Thin-film technologies such as Indium Gallium Zinc Oxide (IGZO) enable Flexible Electronics (FE) for emerging applications in wearable sensing, personal health monitoring, and large-area systems. Analog-to-digital converters (ADCs) serve as critical sensor interfaces in these systems. Yet, their vulnerability to manufacturing defects remains poorly understood despite unipolar technologies' inherently high defect densities and process variations compared to mature CMOS technologies. We present a hierarchical fault injection framework to characterize defect sensitivity in Binary Search ADCs implemented in n-type only technologies. Our methodology combines transistor-level defect characterization with system-level fault propagation analysis, enabling efficient exploration of both single and multiple fault scenarios across the conversion hierarchy. The framework identifies critical fault-sensitive circuit components and enables selective redundancy strategies targeting only the most sensitive components. The resulting defect-tolerant designs improve fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, while incurring only 4.2% area overhead and 6% power increase. While validated on IGZO-TFTs, the methodology applies to all emerging unipolar technologies.

</details>


### [24] [From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design](https://arxiv.org/abs/2602.11016)
*Jinxin Yu,Yudong Pan,Mengdi Wang,Huawei Li,Yinhe Han,Xiaowei Li,Ying Wang*

Main category: cs.AR

TL;DR: 提出3D-Flow：一种基于3D堆叠的空间加速器，通过垂直分层的PE层间寄存器到寄存器通信，解决Transformer模型中片上SRAM访问成为新瓶颈的问题，显著降低能耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然主导现代AI工作负载，但其二次注意力复杂度和不断增长的模型尺寸加剧了内存瓶颈。现有加速器（如Groq和Cerebras）通过大容量片上缓存减少片外流量，算法创新（如FlashAttention）避免生成大型注意力矩阵。然而，随着片外流量减少，片上SRAM访问在长序列工作负载中占能耗60%以上，成为新的瓶颈。

Method: 提出3D-Flow：采用混合键合3D堆叠的空间加速器，支持垂直分层的处理单元（PE）层间寄存器到寄存器通信。相比受限于NoC路由器间传输的2D多阵列架构，3D-Flow利用亚10微米垂直TSV维持周期级算子流水线，开销最小。在此基础上设计3D-FlashAttention：一种细粒度调度方法，平衡各层延迟，形成无气泡垂直数据流，避免片上SRAM往返访问。

Result: 在Transformer工作负载（OPT和QWEN模型）评估中，3D空间加速器相比最先进的2D和3D设计，能耗降低46-93%，速度提升1.4-7.6倍。

Conclusion: 3D-Flow通过垂直堆叠架构和寄存器到寄存器通信，有效解决了Transformer加速器中片上SRAM访问成为主要能耗瓶颈的问题，为长序列AI工作负载提供了高效节能的硬件解决方案。

Abstract: Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [Reverse-Engineering Model Editing on Language Models](https://arxiv.org/abs/2602.10134)
*Zhiyu Sun,Minrui Luo,Yu Wang,Zhili Chen,Tianxing He*

Main category: cs.CR

TL;DR: 该论文揭示了主流模型编辑方法的一个关键安全漏洞：参数更新会无意中成为侧信道，使攻击者能够恢复被编辑的数据。作者提出了KSTER攻击方法，并设计了子空间伪装防御策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预训练过程中会记忆敏感信息，定位-编辑方法作为主流模型编辑范式，通过修改模型参数而不重新训练来保护隐私。然而，作者发现这种方法的参数更新会无意中成为侧信道，使攻击者能够恢复被编辑的数据，这构成了严重的安全威胁。

Method: 提出了两阶段逆向工程攻击KSTER：1）利用更新矩阵的低秩结构，通过谱分析从更新矩阵的行空间恢复编辑主题；2）基于熵的提示恢复攻击，重建编辑的语义上下文。同时提出了子空间伪装防御策略，通过语义诱饵混淆更新指纹。

Result: 在多个大型语言模型上的实验表明，KSTER攻击能够以高成功率恢复被编辑的数据。子空间伪装防御策略在不影响编辑效用的前提下，有效降低了重建风险。

Conclusion: 定位-编辑方法存在严重的安全漏洞，参数更新会泄露被编辑的数据。KSTER攻击展示了这种威胁的现实性，而子空间伪装提供了一种有效的防御机制，为安全模型编辑提供了新的研究方向。

Abstract: Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by modifying model parameters without retraining. However, in this work, we reveal a critical vulnerability of this paradigm: the parameter updates inadvertently serve as a side channel, enabling attackers to recover the edited data. We propose a two-stage reverse-engineering attack named \textit{KSTER} (\textbf{K}ey\textbf{S}paceRecons\textbf{T}ruction-then-\textbf{E}ntropy\textbf{R}eduction) that leverages the low-rank structure of these updates. First, we theoretically show that the row space of the update matrix encodes a ``fingerprint" of the edited subjects, enabling accurate subject recovery via spectral analysis. Second, we introduce an entropy-based prompt recovery attack that reconstructs the semantic context of the edit. Extensive experiments on multiple LLMs demonstrate that our attacks can recover edited data with high success rates. Furthermore, we propose \textit{subspace camouflage}, a defense strategy that obfuscates the update fingerprint with semantic decoys. This approach effectively mitigates reconstruction risks without compromising editing utility. Our code is available at https://github.com/reanatom/EditingAtk.git.

</details>


### [26] [Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise](https://arxiv.org/abs/2602.11088)
*Abhishek Saini,Haolin Jiang,Hang Liu*

Main category: cs.CR

TL;DR: 主流TEE加速设计中的静态密钥重用漏洞导致LLM知识产权保护系统被完全攻破，包括模型权重恢复和完整性检查绕过


<details>
  <summary>Details</summary>
Motivation: 第三方设备部署LLM需要保护模型知识产权，TEE虽然提供解决方案，但其性能限制导致设计中使用预计算的静态密钥基础来加速加密操作，这种主流设计模式引入了经典的密码学漏洞

Method: 通过两种不同的攻击方式：1）针对模型保密系统的攻击，通过恢复其秘密置换和模型权重实现完全保密破坏；2）完整性攻击，完全绕过Soter和TSQP等系统的完整性检查

Result: 攻击具有实际可行性，针对最先进的LLM，在约6分钟内恢复LLaMA-3 8B模型一层的秘密，并展示攻击可扩展到在各种配置下攻破405B参数的LLM

Conclusion: 主流TEE加速设计中的静态密钥重用模式存在严重安全漏洞，需要重新审视和设计更安全的LLM知识产权保护方案

Abstract: The deployment of large language models (LLMs) on third-party devices requires new ways to protect model intellectual property. While Trusted Execution Environments (TEEs) offer a promising solution, their performance limits can lead to a critical compromise: using a precomputed, static secret basis to accelerate cryptographic operations. We demonstrate that this mainstream design pattern introduces a classic cryptographic flaw, the reuse of secret keying material, into the system's protocol. We prove its vulnerability with two distinct attacks: First, our attack on a model confidentiality system achieves a full confidentiality break by recovering its secret permutations and model weights. Second, our integrity attack completely bypasses the integrity checks of systems like Soter and TSQP. We demonstrate the practicality of our attacks against state-of-the-art LLMs, recovering a layer's secrets from a LLaMA-3 8B model in about 6 minutes and showing the attack scales to compromise 405B-parameter LLMs across a variety of configurations.

</details>


### [27] [Privacy by Voice: Modeling Youth Privacy-Protective Behavior in Smart Voice Assistants](https://arxiv.org/abs/2602.10142)
*Molly Campbell,Ajay Kumar Shrestha*

Main category: cs.CR

TL;DR: 本研究通过结构方程模型分析加拿大青少年（16-24岁）与智能语音助手的隐私协商机制，发现隐私自我效能是隐私保护行为的最强预测因子，而算法透明度和信任通过自我效能完全中介影响保护行为。


<details>
  <summary>Details</summary>
Motivation: 智能语音助手已深度融入青少年生活，但青少年用户隐私保护行为的驱动机制仍不清楚。本研究旨在探究加拿大青少年如何与智能语音助手进行隐私协商，理解从隐私感知到保护行动的具体路径。

Method: 基于五个关键构念（感知隐私风险、感知利益、算法透明度和信任、隐私自我效能、隐私保护行为）构建结构模型，对N=469名青少年进行横断面调查，使用偏最小二乘结构方程模型进行分析。

Result: 隐私自我效能是隐私保护行为的最强预测因子；算法透明度和信任对隐私保护行为的影响完全由隐私自我效能中介；感知利益直接阻碍保护行动，但通过轻微提升自我效能间接促进保护行为。

Conclusion: 研究实证验证并扩展了早期定性工作，量化了政策过载和隐藏控制如何侵蚀保护行动所需的自我效能。研究贡献了从感知到行动的循证路径，并转化为设计原则，在不牺牲智能语音助手实用性的前提下赋能年轻数字公民。

Abstract: Smart Voice Assistants (SVAs) are deeply embedded in the lives of youth, yet the mechanisms driving the privacy-protective behaviors among young users remain poorly understood. This study investigates how Canadian youth (aged 16-24) negotiate privacy with SVAs by developing and testing a structural model grounded in five key constructs: perceived privacy risks (PPR), perceived benefits (PPBf), algorithmic transparency and trust (ATT), privacy self-efficacy (PSE), and privacy-protective behaviors (PPB). A cross-sectional survey of N=469 youth was analyzed using partial least squares structural equation modeling. Results reveal that PSE is the strongest predictor of PPB, while the effect of ATT on PPB is fully mediated by PSE. This identifies a critical efficacy gap, where youth's confidence must first be built up for them to act. The model confirms that PPBf directly discourages protective action, yet also indirectly fosters it by slightly boosting self-efficacy. These findings empirically validate and extend earlier qualitative work, quantifying how policy overload and hidden controls erode the self-efficacy necessary for protective action. This study contributes an evidence-based pathway from perception to action and translates it into design imperatives that empower young digital citizens without sacrificing the utility of SVAs.

</details>


### [28] [Red-teaming the Multimodal Reasoning: Jailbreaking Vision-Language Models via Cross-modal Entanglement Attacks](https://arxiv.org/abs/2602.10148)
*Yu Yan,Sheng Sun,Shengjia Cheng,Teli Liu,Mingfeng Li,Min Liu*

Main category: cs.CR

TL;DR: 提出CrossTALK攻击方法，通过跨模态信息纠缠来绕过视觉语言模型的安全对齐机制


<details>
  <summary>Details</summary>
Motivation: 现有黑盒越狱攻击依赖简单固定的图像-文本组合，缺乏攻击复杂度可扩展性，难以应对VLM不断发展的推理能力

Method: 1) 知识可扩展重构：将有害任务扩展为多跳链式指令；2) 跨模态线索纠缠：将可视化实体迁移到图像中构建多模态推理链接；3) 跨模态场景嵌套：使用多模态上下文指令引导VLM生成详细有害输出

Result: 实验表明CrossTALK达到了最先进的攻击成功率

Conclusion: 提出的跨模态纠缠攻击方法能够超越VLM训练和泛化的安全对齐模式，实现有效的越狱攻击

Abstract: Vision-Language Models (VLMs) with multimodal reasoning capabilities are high-value attack targets, given their potential for handling complex multimodal harmful tasks. Mainstream black-box jailbreak attacks on VLMs work by distributing malicious clues across modalities to disperse model attention and bypass safety alignment mechanisms. However, these adversarial attacks rely on simple and fixed image-text combinations that lack attack complexity scalability, limiting their effectiveness for red-teaming VLMs' continuously evolving reasoning capabilities. We propose \textbf{CrossTALK} (\textbf{\underline{Cross}}-modal en\textbf{\underline{TA}}ng\textbf{\underline{L}}ement attac\textbf{\underline{K}}), which is a scalable approach that extends and entangles information clues across modalities to exceed VLMs' trained and generalized safety alignment patterns for jailbreak. Specifically, {knowledge-scalable reframing} extends harmful tasks into multi-hop chain instructions, {cross-modal clue entangling} migrates visualizable entities into images to build multimodal reasoning links, and {cross-modal scenario nesting} uses multimodal contextual instructions to steer VLMs toward detailed harmful outputs. Experiments show our COMET achieves state-of-the-art attack success rate.

</details>


### [29] [Authenticated Workflows: A Systems Approach to Protecting Agentic AI](https://arxiv.org/abs/2602.10465)
*Mohan Rajagopalan,Vinay Rao*

Main category: cs.CR

TL;DR: 该论文提出了首个企业级智能体AI的完整信任层——认证工作流，通过保护提示、工具、数据和上下文四个边界，结合密码学消除攻击类别和运行时策略执行，提供确定性安全。


<details>
  <summary>Details</summary>
Motivation: 现有企业智能体AI系统的防御措施（护栏、语义过滤器）是概率性的，经常被绕过，缺乏可靠的安全保障。

Method: 引入认证工作流，在四个边界（提示、工具、数据、上下文）执行意图和完整性验证，结合密码学证明和运行时策略执行。提出MAPL AI原生策略语言，支持动态约束表达和层次化组合，通过薄适配器集成九大主流框架。

Result: 实现确定性安全：操作要么携带有效密码学证明被接受，要么被拒绝。在174个测试案例中达到100%召回率和零误报，防护9/10 OWASP Top 10风险，完全缓解两个高影响生产CVE。

Conclusion: 认证工作流为企业智能体AI提供了首个完整的信任层，通过密码学证明和动态策略执行实现了确定性安全，具有实际部署可行性。

Abstract: Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.

</details>


### [30] [Basic Legibility Protocols Improve Trusted Monitoring](https://arxiv.org/abs/2602.10153)
*Ashwin Sreevatsa,Sebastian Prasanna,Cody Rushing*

Main category: cs.CR

TL;DR: AI控制研究提出可读性协议，通过让不受信任的AI模型用注释详细记录代码，帮助较弱但可信的监控模型更好地评估其行为，在APPS编码环境中提高了安全性而不牺牲任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统可信监控方法在不受信任模型的行为超出监控模型理解能力时会失效。需要开发新的控制协议，使不受信任模型的行为对监控模型更易评估。

Method: 引入可读性协议，鼓励不受信任模型采取更易被监控模型评估的行动。在APPS编码环境中，允许不受信任模型用注释详细记录代码，而不是像先前工作那样删除注释以防止欺骗性内容。

Result: 1) 注释协议相比删除注释的基线方法，在不牺牲任务性能的情况下提高了安全性；2) 注释对诚实代码的益处更大，因为诚实代码通常有自然的解释可以消除监控怀疑，而后门代码往往缺乏容易的正当理由；3) 注释带来的收益随监控模型强度增加而增加，更强的监控模型能更好地区分真正的正当理由和表面合理的理由。

Conclusion: 可读性协议是有效的AI控制方法，通过鼓励不受信任模型提供可解释的行动，使较弱但可信的监控模型能够更可靠地评估其行为，在编码任务中实现了安全性和性能的良好平衡。

Abstract: The AI Control research agenda aims to develop control protocols: safety techniques that prevent untrusted AI systems from taking harmful actions during deployment. Because human oversight is expensive, one approach is trusted monitoring, where weaker, trusted models oversee stronger, untrusted models$\unicode{x2013}$but this often fails when the untrusted model's actions exceed the monitor's comprehension. We introduce legibility protocols, which encourage the untrusted model to take actions that are easier for a monitor to evaluate.
  We perform control evaluations in the APPS coding setting, where an adversarial agent attempts to write backdoored code without detection. We study legibility protocols that allow the untrusted model to thoroughly document its code with comments$\unicode{x2013}$in contrast to prior work, which removed comments to prevent deceptive ones. We find that: (i) commenting protocols improve safety without sacrificing task performance relative to comment-removal baselines; (ii) commenting disproportionately benefits honest code, which typically has a natural explanation that resolves monitor suspicion, whereas backdoored code frequently lacks an easy justification; (iii) gains from commenting increase with monitor strength, as stronger monitors better distinguish genuine justifications from only superficially plausible ones.

</details>


### [31] [MalMoE: Mixture-of-Experts Enhanced Encrypted Malicious Traffic Detection Under Graph Drift](https://arxiv.org/abs/2602.10157)
*Yunpeng Tan,Qingyang Li,Mingxin Yang,Yannan Hu,Lei Zhang,Xinggong Zhang*

Main category: cs.CR

TL;DR: MalMoE：基于专家混合的图辅助加密流量检测系统，通过选择最佳专家模型应对图漂移问题


<details>
  <summary>Details</summary>
Motivation: 加密流量在保障传输安全的同时，使得恶意流量检测面临挑战，因为无法查看数据包有效载荷。基于图的方法虽然能利用多主机交互提高检测精度，但大多数面临图漂移问题——图的流统计或拓扑信息随时间变化。

Method: 提出MalMoE系统，应用专家混合（MoE）选择最佳专家模型进行漂移感知分类。设计类似1跳GNN的专家模型，通过分析具有不同特征的图来处理不同的图漂移。重新设计门控模型根据实际漂移进行专家选择。采用带数据增强的两阶段稳定训练策略，有效指导门控进行路由选择。

Result: 在开源、合成和真实世界数据集上的实验表明，MalMoE能够进行精确且实时的检测。

Conclusion: MalMoE通过专家混合方法有效解决了加密流量检测中的图漂移问题，实现了精确的实时恶意流量检测。

Abstract: Encryption has been commonly used in network traffic to secure transmission, but it also brings challenges for malicious traffic detection, due to the invisibility of the packet payload. Graph-based methods are emerging as promising solutions by leveraging multi-host interactions to promote detection accuracy. But most of them face a critical problem: Graph Drift, where the flow statistics or topological information of a graph change over time. To overcome these drawbacks, we propose a graph-assisted encrypted traffic detection system, MalMoE, which applies Mixture of Experts (MoE) to select the best expert model for drift-aware classification. Particularly, we design 1-hop-GNN-like expert models that handle different graph drifts by analyzing graphs with different features. Then, the redesigned gate model conducts expert selection according to the actual drift. MalMoE is trained with a stable two-stage training strategy with data augmentation, which effectively guides the gate on how to perform routing. Experiments on open-source, synthetic, and real-world datasets show that MalMoE can perform precise and real-time detection.

</details>


### [32] [Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment](https://arxiv.org/abs/2602.10161)
*Kun Wang,Zherui Li,Zhenhong Zhou,Yitong Zhang,Yan Mi,Kun Yang,Yiming Zhang,Junhao Dong,Zhongxiang Sun,Qiankun Li,Yang Liu*

Main category: cs.CR

TL;DR: 本文针对全模态大语言模型的安全漏洞，提出了OmniSteer方法，通过提取黄金拒绝向量和轻量级适配器，显著提升了模型对有害输入的拒绝成功率，同时保持了多模态通用能力。


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型虽然扩展了多模态能力，但也引入了跨模态安全风险。目前缺乏对全模态交互中漏洞的系统性理解，需要填补这一研究空白。

Method: 1. 建立模态-语义解耦原则并构建AdvBench-Omni数据集；2. 通过机制分析发现中层解离现象和模态不变的纯拒绝方向；3. 使用奇异值分解提取黄金拒绝向量；4. 提出OmniSteer方法，利用轻量级适配器自适应调节干预强度。

Result: 实验表明，该方法将有害输入的拒绝成功率从69.9%提升到91.2%，同时有效保持了所有模态的通用能力。

Conclusion: OmniSteer方法通过系统性分析全模态模型的安全漏洞，提出了一种有效的安全增强方案，在提升安全性的同时保持了模型的多模态性能。

Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling principle and construct the AdvBench-Omni dataset, which reveals a significant vulnerability in OLLMs. Mechanistic analysis uncovers a Mid-layer Dissolution phenomenon driven by refusal vector magnitude shrinkage, alongside the existence of a modal-invariant pure refusal direction. Inspired by these insights, we extract a golden refusal vector using Singular Value Decomposition and propose OmniSteer, which utilizes lightweight adapters to modulate intervention intensity adaptively. Extensive experiments show that our method not only increases the Refusal Success Rate against harmful inputs from 69.9% to 91.2%, but also effectively preserves the general capabilities across all modalities. Our code is available at: https://github.com/zhrli324/omni-safety-research.

</details>


### [33] [Limits of Residual-Based Detection for Physically Consistent False Data Injection](https://arxiv.org/abs/2602.10162)
*Chenhan Xiao,Yang Weng*

Main category: cs.CR

TL;DR: 论文揭示了交流电力系统状态估计中基于残差的虚假数据注入攻击检测方法存在根本性局限：当攻击产生的操纵测量值保持在由交流潮流关系和测量冗余诱导的测量流形上时，残差检测器可能无法将其与正常数据区分开。


<details>
  <summary>Details</summary>
Motivation: 当前实践中主要依赖拓扑感知的残差检测来识别虚假数据注入攻击，这种方法假设恶意测量值可以通过物理不一致性反映在异常残差行为中被区分。但作者发现这一假设并不总是成立，需要揭示基于残差检测的根本局限性。

Method: 提出了一种数据驱动的构造机制，结合交流潮流的通用函数结构，生成物理一致、流形约束的扰动，为残差检测器如何被绕过提供具体证据。在多个交流测试系统上进行数值研究。

Result: 数值研究表明，当攻击产生的操纵测量值保持在测量流形上时，残差检测器无法有效检测虚假数据注入攻击。这种可检测性限制是测量流形本身的属性，不依赖于攻击者对物理系统模型的详细知识。

Conclusion: 研究结果突显了交流状态估计中基于残差检测的根本限制，表明需要超越测量一致性测试的补充防御措施。攻击者即使不了解详细的物理系统模型，也能构造出绕过残差检测的虚假数据。

Abstract: False data injection attacks (FDIAs) pose a persistent challenge to AC power system state estimation. In current practice, detection relies primarily on topology-aware residual-based tests that assume malicious measurements can be distinguished from normal operation through physical inconsistency reflected in abnormal residual behavior. This paper shows that this assumption does not always hold: when FDIA scenarios produce manipulated measurements that remain on the measurement manifold induced by AC power flow relations and measurement redundancy, residual-based detectors may fail to distinguish them from nominal data. The resulting detectability limitation is a property of the measurement manifold itself and does not depend on the attacker's detailed knowledge of the physical system model. To make this limitation observable in practice, we present a data-driven constructive mechanism that incorporates the generic functional structure of AC power flow to generate physically consistent, manifold-constrained perturbations, providing a concrete witness of how residual-based detectors can be bypassed. Numerical studies on multiple AC test systems characterize the conditions under which detection becomes challenging and illustrate its failure modes. The results highlight fundamental limits of residual-based detection in AC state estimation and motivate the need for complementary defenses beyond measurement consistency tests.

</details>


### [34] [MerkleSpeech: Public-Key Verifiable, Chunk-Localised Speech Provenance via Perceptual Fingerprints and Merkle Commitments](https://arxiv.org/abs/2602.10166)
*Tatsunori Ono*

Main category: cs.CR

TL;DR: MerkleSpeech：一个用于语音来源验证的双层系统，结合了鲁棒水印和严格的密码学完整性验证，支持片段级验证和时间线分析。


<details>
  <summary>Details</summary>
Motivation: 当前语音来源验证面临实际工作流中的拼接、引用、裁剪和平台级转换等挑战，现有方法缺乏第三方可验证的密码学证明，且C2PA等标准在重新编码或常规处理下会失效。

Method: 系统采用双层验证：1) 鲁棒水印归属层（WM-only）用于抵抗常见分发变换；2) 严格密码学完整性层（MSv1）通过Merkle树和发行者签名验证片段指纹。系统计算短语音片段的感知指纹，构建Merkle树并签名根节点，嵌入包含内容标识符和片段元数据的紧凑水印载荷。

Result: 系统能够提供拼接感知的时间线，显示哪些区域通过各层验证以及失败原因。实验针对重采样、带通滤波和加性噪声等场景，实现了极低的误报率，特别关注了神经编解码器对后处理水印的压力。

Conclusion: MerkleSpeech提供了一个公开可验证、片段本地化的语音来源验证系统，结合了鲁棒水印和密码学完整性验证，能够应对实际工作流中的各种处理变换。

Abstract: Speech provenance goes beyond detecting whether a watermark is present. Real workflows involve splicing, quoting, trimming, and platform-level transforms that may preserve some regions while altering others. Neural watermarking systems have made strides in robustness and localised detection, but most deployments produce outputs with no third-party verifiable cryptographic proof tying a time segment to an issuer-signed original. Provenance standards like C2PA adopt signed manifests and Merkle-based fragment validation, yet their bindings target encoded assets and break under re-encoding or routine processing.
  We propose MerkleSpeech, a system for public-key verifiable, chunk-localised speech provenance offering two tiers of assurance. The first, a robust watermark attribution layer (WM-only), survives common distribution transforms and answers "was this chunk issued by a known party?". The second, a strict cryptographic integrity layer (MSv1), verifies Merkle inclusion of the chunk's fingerprint under an issuer signature. The system computes perceptual fingerprints over short speech chunks, commits them in a Merkle tree whose root is signed with an issuer key, and embeds a compact in-band watermark payload carrying a random content identifier and chunk metadata sufficient to retrieve Merkle inclusion proofs from a repository. Once the payload is extracted, all subsequent verification steps (signature check, fingerprint recomputation, Merkle inclusion) use only public information. The result is a splice-aware timeline indicating which regions pass each tier and why any given region fails. We describe the protocol, provide pseudocode, and present experiments targeting very low false positive rates under resampling, bandpass filtering, and additive noise, informed by recent audits identifying neural codecs as a major stressor for post-hoc audio watermarks.

</details>


### [35] [Non-Fungible Blockchain Tokens for Traceable Online-Quality Assurance of Milled Workpieces](https://arxiv.org/abs/2602.10169)
*Nicolai Maisch,Shengjian Chen,Alexander Robertus,Samed Ajdinović,Armin Lechler,Alexander Verl,Oliver Riedel*

Main category: cs.CR

TL;DR: 提出基于NFT和区块链的铣削工件质量数据安全存储与传输方案，实现质量链自动化追溯


<details>
  <summary>Details</summary>
Motivation: 解决在线质量保证过程中铣削工件质量相关数据的安全存储和传输问题，减少耗时且成本高昂的重复人工质量检查

Method: 利用NFT在公共以太坊区块链上安全互操作地存储资产管理外壳格式的质量数据，通过自定义智能合约铸造NFT，引用IPFS中存储的元数据，支持灵活添加新处理步骤的数据

Result: 实现了质量数据的安全存储和传输系统，支持整个价值链的自动化追溯

Conclusion: 该概念为在线质量保证过程提供了安全、灵活的质量数据管理方案，显著减少了重复人工质量检查的需求

Abstract: This work presents a concept and implementation for the secure storage and transfer of quality-relevant data of milled workpieces from online-quality assurance processes enabled by real-time simulation models. It utilises Non-Fungible Tokens (NFT) to securely and interoperably store quality data in the form of an Asset Administration Shell (AAS) on a public Ethereum blockchain. Minted by a custom smart contract, the NFTs reference the metadata saved in the Interplanetary File System (IPFS), allowing new data from additional processing steps to be added in a flexible yet secure manner. The concept enables automated traceability throughout the value chain, minimising the need for time-consuming and costly repetitive manual quality checks.

</details>


### [36] [SecCodePRM: A Process Reward Model for Code Security](https://arxiv.org/abs/2602.10418)
*Weichen Yu,Ravi Mangal,Yinyi Luo,Kai Hu,Jingxuan He,Corina S. Pasareanu,Matt Fredrikson*

Main category: cs.CR

TL;DR: SecCodePRM是一个面向安全的流程奖励模型，为代码轨迹提供上下文感知的步骤级安全评分，支持实时漏洞检测和安全代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法要么依赖静态分析器，要么使用基于LLM/GNN的检测器，这些方法通常需要完整上下文、提供稀疏反馈，且随着代码长度增加性能下降，不适合交互式编码和流式生成中的实时前缀级评估。

Method: 提出SecCodePRM安全导向的流程奖励模型，从静态分析器和专家标注中获取步骤级监督标签，使模型能更精确关注与跨过程漏洞相关的细粒度区域。模型采用风险敏感聚合强调高风险步骤，支持推理时扩展通过排名候选继续并选择更高累积奖励。

Result: SecCodePRM在三个应用场景（完整代码漏洞检测、部分代码漏洞检测、安全代码生成）中均优于先前方法，同时保持代码功能正确性，表明在安全性和实用性之间没有权衡。

Conclusion: SecCodePRM提供了密集的实时反馈，可扩展到长时程生成，改善了代码安全性而不牺牲功能性，为现代软件开发工作流中的实时安全评估提供了有效解决方案。

Abstract: Large Language Models are rapidly becoming core components of modern software development workflows, yet ensuring code security remains challenging. Existing vulnerability detection pipelines either rely on static analyzers or use LLM/GNN-based detectors trained with coarse program-level supervision. Both families often require complete context, provide sparse end-of-completion feedback, and can degrade as code length grows, making them ill-suited for real-time, prefix-level assessment during interactive coding and streaming generation. We propose SecCodePRM, a security-oriented process reward model that assigns a context-aware, step-level security score along a code trajectory. To train the model, we derive step-level supervision labels from static analyzers and expert annotations, allowing the model to attend more precisely to fine-grained regions associated with inter-procedural vulnerabilities. SecCodePRM has three applications: full-code vulnerability detection (VD), partial-code VD, and secure code generation (CG). For VD, SecCodePRM uses risk-sensitive aggregation that emphasizes high-risk steps; for CG, SecCodePRM supports inference-time scaling by ranking candidate continuations and favoring higher cumulative reward. This design yields dense, real-time feedback that scales to long-horizon generation. Empirically, SecCodePRM outperforms prior approaches in all three settings, while preserving code functional correctness, suggesting improved security without a safety-utility tradeoff.

</details>


### [37] [GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks](https://arxiv.org/abs/2602.10478)
*Zihao Li,Hongyi Lu,Yanan Guo,Zhenkai Zhang,Shuai Wang,Fengwei Zhang*

Main category: cs.CR

TL;DR: GPU-Fuzz是一个针对深度学习框架GPU内存错误的模糊测试工具，通过形式化约束建模算子参数，使用约束求解器生成测试用例，在PyTorch、TensorFlow和PaddlePaddle中发现了13个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: GPU内存错误是深度学习框架的关键威胁，可能导致系统崩溃甚至安全问题。现有方法在高效定位这些错误方面存在不足。

Method: GPU-Fuzz将算子参数建模为形式化约束，利用约束求解器生成测试用例，系统性地探测GPU内核中容易出错的边界条件。

Result: 在PyTorch、TensorFlow和PaddlePaddle三个主流深度学习框架中发现了13个未知的内存错误漏洞。

Conclusion: GPU-Fuzz能够有效发现深度学习框架中的GPU内存错误，证明了其在实际应用中的有效性和实用性。

Abstract: GPU memory errors are a critical threat to deep learning (DL) frameworks, leading to crashes or even security issues. We introduce GPU-Fuzz, a fuzzer locating these issues efficiently by modeling operator parameters as formal constraints. GPU-Fuzz utilizes a constraint solver to generate test cases that systematically probe error-prone boundary conditions in GPU kernels. Applied to PyTorch, TensorFlow, and PaddlePaddle, we uncovered 13 unknown bugs, demonstrating the effectiveness of GPU-Fuzz in finding memory errors.

</details>


### [38] [Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI](https://arxiv.org/abs/2602.10481)
*Mohan Rajagopalan,Vinay Rao*

Main category: cs.CR

TL;DR: 该论文提出了一种结合密码学验证和形式化证明的LLM安全框架，通过认证提示和认证上下文确保工作流完整性，并提供预防性安全保证。


<details>
  <summary>Details</summary>
Motivation: 传统安全模型无法防止LLM应用中的提示注入和上下文操纵攻击，需要新的安全机制来提供可验证的完整性和预防性保护。

Method: 引入两个密码学原语：认证提示（提供自包含的溯源验证）和认证上下文（使用防篡改哈希链确保动态输入完整性）。基于这些原语构建形式化策略代数，提供协议级拜占庭容错，并设计了五层互补防御机制。

Result: 在涵盖6个类别的代表性攻击评估中，实现了100%检测率、零误报和可忽略的开销，首次结合了密码学强制的提示溯源、防篡改上下文和可证明的策略推理。

Conclusion: 该框架将LLM安全从被动检测转向预防性保证，通过密码学验证的完整性和形式化证明的策略执行，为LLM工作流提供了可验证的安全保障。

Abstract: Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across LLM workflows. Authenticated prompts enable self-contained lineage verification, while authenticated context uses tamper-evident hash chains to ensure integrity of dynamic inputs. Building on these primitives, we formalize a policy algebra with four proven theorems providing protocol-level Byzantine resistance--even adversarial agents cannot violate organizational policies. Five complementary defenses--from lightweight resource controls to LLM-based semantic validation--deliver layered, preventative security with formal guarantees. Evaluation against representative attacks spanning 6 exhaustive categories achieves 100% detection with zero false positives and nominal overhead. We demonstrate the first approach combining cryptographically enforced prompt lineage, tamper-evident context, and provable policy reasoning--shifting LLM security from reactive detection to preventative guarantees.

</details>


### [39] [Following Dragons: Code Review-Guided Fuzzing](https://arxiv.org/abs/2602.10487)
*Viet Hoang Luu,Amirmohammad Pasdar,Wachiraphan Charoenwet,Toby Murray,Shaanan Cohney,Van-Thuan Pham*

Main category: cs.CR

TL;DR: EyeQ系统利用代码审查中的开发者智能来指导模糊测试，通过提取安全相关信号、定位相关程序区域，并将这些洞察转化为基于注解的模糊测试指导，显著提升了漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试工具虽然能扩展到大型实际软件，但往往无法覆盖开发者认为最脆弱或安全关键的程序状态。这些状态通常深藏在执行空间中，受前置条件限制，或被低价值路径掩盖。同时，开发者在代码审查中经常发现风险相关的洞察，但这些信息在自动化测试工具中被忽视。

Method: EyeQ系统从代码审查讨论中提取安全相关信号，定位涉及的程序区域，并将这些洞察转化为基于注解的模糊测试指导。该方法基于现有的注解感知模糊测试，无需改变程序语义或开发者工作流程。首先通过人工指导的可行性研究验证方法，然后使用大型语言模型和精心设计的提示自动化工作流程。

Result: EyeQ显著改进了漏洞发现能力，在安全关键的PHP代码库中发现了40多个先前未知的漏洞，相比标准模糊测试配置有显著提升。

Conclusion: 通过利用代码审查中的开发者智能来指导模糊测试，EyeQ能够更有效地发现安全漏洞，证明了将人工审查洞察与自动化测试工具结合的价值。

Abstract: Modern fuzzers scale to large, real-world software but often fail to exercise the program states developers consider most fragile or security-critical. Such states are typically deep in the execution space, gated by preconditions, or overshadowed by lower-value paths that consume limited fuzzing budgets. Meanwhile, developers routinely surface risk-relevant insights during code review, yet this information is largely ignored by automated testing tools. We present EyeQ, a system that leverages developer intelligence from code reviews to guide fuzzing. EyeQ extracts security-relevant signals from review discussions, localizes the implicated program regions, and translates these insights into annotation-based guidance for fuzzing. The approach operates atop existing annotation-aware fuzzing, requiring no changes to program semantics or developer workflows. We first validate EyeQ through a human-guided feasibility study on a security-focused dataset of PHP code reviews, establishing a strong baseline for review-guided fuzzing. We then automate the workflow using a large language model with carefully designed prompts. EyeQ significantly improves vulnerability discovery over standard fuzzing configurations, uncovering more than 40 previously unknown bugs in the security-critical PHP codebase.

</details>


### [40] [SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration](https://arxiv.org/abs/2602.10750)
*Rumman Firdos,Aman Dangi*

Main category: cs.CR

TL;DR: SecureScan是一个AI驱动的三层检测框架，结合逻辑回归分类、启发式分析和外部威胁情报，用于URL、文件哈希和二进制文件的综合检测，在基准数据集上达到93.1%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件和钓鱼攻击日益复杂，传统基于签名的入侵检测系统效果下降，需要更智能的检测方法。

Method: 提出三层检测框架：1) 启发式分析过滤已知威胁；2) 机器学习（逻辑回归）分类不确定样本；3) 通过VirusTotal API验证边界案例。引入阈值校准和灰区逻辑(0.45-0.55)减少误报。

Result: 在基准数据集上达到93.1%准确率，精确度0.87，召回率0.92，表现出良好的泛化能力和减少过拟合。轻量级统计模型结合校准验证和外部情报，性能可与复杂深度学习系统媲美。

Conclusion: 轻量级统计模型通过校准验证和外部威胁情报增强，能够实现与复杂深度学习系统相当的可靠性和性能，为实际部署提供了高效解决方案。

Abstract: The growing sophistication of modern malware and phishing campaigns has diminished the effectiveness of traditional signature-based intrusion detection systems. This work presents SecureScan, an AI-driven, triple-layer detection framework that integrates logistic regression-based classification, heuristic analysis, and external threat intelligence via the VirusTotal API for comprehensive triage of URLs, file hashes, and binaries. The proposed architecture prioritizes efficiency by filtering known threats through heuristics, classifying uncertain samples using machine learning, and validating borderline cases with third-party intelligence. On benchmark datasets, SecureScan achieves 93.1 percent accuracy with balanced precision (0.87) and recall (0.92), demonstrating strong generalization and reduced overfitting through threshold-based decision calibration. A calibrated threshold and gray-zone logic (0.45-0.55) were introduced to minimize false positives and enhance real-world stability. Experimental results indicate that a lightweight statistical model, when augmented with calibrated verification and external intelligence, can achieve reliability and performance comparable to more complex deep learning systems.

</details>


### [41] [When Skills Lie: Hidden-Comment Injection in LLM Agents](https://arxiv.org/abs/2602.10498)
*Qianli Wang,Boyang Ma,Minghui Xu,Yue Zhang*

Main category: cs.CR

TL;DR: 研究发现LLM智能体技能文档中的隐藏注释存在提示注入风险，恶意指令可通过HTML注释隐藏，影响DeepSeek-V3.2和GLM-4.5-Air模型执行敏感工具调用


<details>
  <summary>Details</summary>
Motivation: LLM智能体通常依赖技能文档来描述可用工具和推荐流程，但Markdown技能文档在渲染为HTML时，HTML注释块可能对人类审查者不可见，而原始文本仍会完整提供给模型，这构成了潜在的安全风险

Method: 通过在合法技能文档中附加恶意指令的隐藏注释，测试DeepSeek-V3.2和GLM-4.5-Air模型是否会被影响，并设计防御性系统提示来防止恶意工具调用

Result: 实验发现DeepSeek-V3.2和GLM-4.5-Air模型确实会受到隐藏注释中恶意指令的影响，产生包含敏感工具意图的输出；而简短的防御性系统提示能有效阻止恶意工具调用并暴露可疑的隐藏指令

Conclusion: 技能文档层存在隐藏注释提示注入风险，需要将技能视为不可信来源并禁止敏感操作，防御性系统提示能有效缓解此安全威胁

Abstract: LLM agents often rely on Skills to describe available tools and recommended procedures. We study a hidden-comment prompt injection risk in this documentation layer: when a Markdown Skill is rendered to HTML, HTML comment blocks can become invisible to human reviewers, yet the raw text may still be supplied verbatim to the model. In experiments, we find that DeepSeek-V3.2 and GLM-4.5-Air can be influenced by malicious instructions embedded in a hidden comment appended to an otherwise legitimate Skill, yielding outputs that contain sensitive tool intentions. A short defensive system prompt that treats Skills as untrusted and forbids sensitive actions prevents these malicious tool calls and instead surfaces the suspicious hidden instructions.

</details>


### [42] [CryptoCatch: Cryptomining Hidden Nowhere](https://arxiv.org/abs/2602.10573)
*Ruisheng Shi,Ziding Lin,Haoran Sun,Qin Wang,Shihan Zhang,Lina Lan,Zhiyuan Peng,Chenfeng Wang*

Main category: cs.CR

TL;DR: 提出一种实用的加密加密货币挖矿流量检测机制，采用两阶段检测框架，结合机器学习和主动探测，有效降低误报率，实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法（如黑名单和深度包检测）对加密挖矿流量效果不佳，且误报率高，需要更有效的解决方案来应对加密货币挖矿带来的安全风险。

Method: 采用两阶段检测框架：第一阶段使用机器学习进行细粒度检测，第二阶段通过主动探测来减少分类器的误报。

Result: 系统达到F1分数0.99，识别特定加密货币的准确率达99.39%，在多个挖矿池的广泛测试中验证了方法的有效性。

Conclusion: 该方法提供了一种更精确可靠的加密货币挖矿活动识别解决方案，能够有效检测加密挖矿流量并显著降低误报率。

Abstract: Cryptomining poses significant security risks, yet traditional detection methods like blacklists and Deep Packet Inspection (DPI) are often ineffective against encrypted mining traffic and suffer from high false positive rates. In this paper, we propose a practical encrypted cryptomining traffic detection mechanism. It consists of a two-stage detection framework, which can effectively provide fine-grained detection results by machine learning and reduce false positives from classifiers through active probing. Our system achieves an F1-score of 0.99 and identifies specific cryptocurrencies with a 99.39\% accuracy rate. Extensive testing across various mining pools confirms the effectiveness of our approach, offering a more precise and reliable solution for identifying cryptomining activities.

</details>


### [43] [Invisible Trails? An Identity Alignment Scheme based on Online Tracking](https://arxiv.org/abs/2602.10626)
*Ruisheng Shi,Zhiyuan Peng,Tong Fu,Lina Lan,Qin Wang,Jiaqi Zeng*

Main category: cs.CR

TL;DR: 该论文揭示了匿名化用户数据仍存在隐私风险，攻击者可利用追踪数据进行身份对齐攻击，提出了被动和主动两种去匿名化攻击方法，并建立了首个在线追踪身份对齐评估框架。


<details>
  <summary>Details</summary>
Motivation: 尽管追踪公司声称通过匿名化保护用户隐私，但研究发现匿名化数据仍存在重大隐私风险，攻击者可以利用这些数据识别用户在其他网站上的账户并进行精准身份对齐，这暴露了当前隐私保护措施的不足。

Method: 开发了数据收集器获取必要数据集，设计了身份对齐算法，构建了两种去匿名化攻击：被动攻击（分析追踪数据对齐身份）和主动攻击（诱导用户在线互动以提高成功率），并首次引入了在线追踪身份对齐的评估框架。

Result: 成功实现了身份对齐方案，被动攻击和主动攻击都能有效识别目标用户，主动攻击成功率更高。建立了评估框架并分析了影响身份对齐效果的关键因素，提供了生成数据集的独立评估，并在加密货币用例中展示了完整系统原型。

Conclusion: 匿名化数据并不能完全保护用户隐私，身份对齐攻击具有显著威胁。需要更强大的隐私保护机制来应对此类攻击，论文提出的评估框架和攻击方法为理解和防御此类隐私威胁提供了重要工具。

Abstract: Many tracking companies collect user data and sell it to data markets and advertisers. While they claim to protect user privacy by anonymizing the data, our research reveals that significant privacy risks persist even with anonymized data. Attackers can exploit this data to identify users' accounts on other websites and perform targeted identity alignment. In this paper, we propose an effective identity alignment scheme for accurately identifying targeted users. We develop a data collector to obtain the necessary datasets, an algorithm for identity alignment, and, based on this, construct two types of de-anonymization attacks: the \textit{passive attack}, which analyzes tracker data to align identities, and the \textit{active attack}, which induces users to interact online, leading to higher success rates. Furthermore, we introduce, for the first time, a novel evaluation framework for online tracking-based identity alignment. We investigate the key factors influencing the effectiveness of identity alignment. Additionally, we provide an independent assessment of our generated dataset and present a fully functional system prototype applied to a cryptocurrency use case.

</details>


### [44] [CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data](https://arxiv.org/abs/2602.11015)
*Valery Khvatov,Alexey Neyman*

Main category: cs.CR

TL;DR: CVPL是一个几何框架，用于评估原始数据与保护后表格数据之间的链接风险，提供连续的风险估计而非二元合规判断。


<details>
  <summary>Details</summary>
Motivation: 形式化隐私度量通常只提供合规性保证，但无法量化发布数据集中实际的链接可能性。需要一种后验评估方法来衡量原始数据与保护后数据之间的链接风险。

Method: CVPL将链接分析建模为操作符流水线：分块、向量化、潜在投影和相似性评估。引入阈值感知风险曲面R(λ, τ)来捕捉保护强度和攻击者严格度的联合效应。建立具有单调性保证的渐进分块策略。

Result: 在10,000条记录和19种保护配置上的实证验证表明，形式化的k-匿名合规可能与实质性的经验链接性共存，其中很大一部分来自非准标识符的行为模式。CVPL能识别驱动链接可行性的特征。

Conclusion: CVPL提供了可解释的诊断工具，支持隐私影响评估、保护机制比较和效用-风险权衡分析，超越了传统的二元合规判断，能更准确地量化实际链接风险。

Abstract: Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL represents linkage analysis as an operator pipeline comprising blocking, vectorization, latent projection, and similarity evaluation, yielding continuous, scenario-dependent risk estimates rather than binary compliance verdicts. We formally define CVPL under an explicit threat model and introduce threshold-aware risk surfaces, R(lambda, tau), that capture the joint effects of protection strength and attacker strictness. We establish a progressive blocking strategy with monotonicity guarantees, enabling anytime risk estimation with valid lower bounds. We demonstrate that the classical Fellegi-Sunter linkage emerges as a special case of CVPL under restrictive assumptions, and that violations of these assumptions can lead to systematic over-linking bias. Empirical validation on 10,000 records across 19 protection configurations demonstrates that formal k-anonymity compliance may coexist with substantial empirical linkability, with a significant portion arising from non-quasi-identifier behavioral patterns. CVPL provides interpretable diagnostics identifying which features drive linkage feasibility, supporting privacy impact assessment, protection mechanism comparison, and utility-risk trade-off analysis.

</details>


### [45] [GoodVibe: Security-by-Vibe for LLM-Based Code Generation](https://arxiv.org/abs/2602.10778)
*Maximilian Thang,Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Jona te Lintelo,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GoodVibe是一个神经元级别的框架，通过识别安全相关神经元并进行选择性微调，显著提升代码语言模型的安全性，同时保持模型通用能力，训练成本远低于传统方法。


<details>
  <summary>Details</summary>
Motivation: LLM在快速非正式开发（vibe coding）中经常生成功能正确但不安全的代码，现有安全改进方法要么成本高、容易灾难性遗忘，要么粒度粗、可解释性和控制有限。

Method: 基于安全相关推理集中在少数神经元的洞察，使用梯度归因识别安全关键神经元，进行神经元选择性微调，并引入激活驱动的神经元聚类以减少训练成本。

Result: 在C++、Java、Swift、Go等安全关键编程语言的6个LLM上评估，安全性提升达2.5倍，匹配或超过全微调效果但可训练参数减少4700倍，训练计算比LoRA减少3.6倍以上。

Conclusion: 神经元级别优化为安全代码生成提供了有效且可扩展的方法，在不牺牲效率或通用性的前提下显著提升安全性。

Abstract: Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.
  We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.

</details>


### [46] [Mask-Based Window-Level Insider Threat Detection for Campaign Discovery](https://arxiv.org/abs/2602.11019)
*Jericho Cain,Hayden Beadles*

Main category: cs.CR

TL;DR: 该论文提出了一种双通道卷积自编码器，通过分离活动存在性和活动强度来改进用户和实体行为分析中的内部威胁检测，在CERT r4.2数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于固定时间窗口的用户和实体行为分析系统在检测短期行为异常方面有效，但对于持续时间较长的攻击活动，尚不清楚单个窗口内已包含多少信息，以及如何利用这些信息进行攻击活动发现。

Method: 提出了一种双通道卷积自编码器，同时重构二进制活动掩码和对应的活动数值，使模型能够将表示能力集中在稀疏的行为结构上，而不是密集的非活动基线。

Result: 在持续1-7天的多日攻击活动中，该方法实现了窗口级精确率-召回率AUC为0.71，显著超过标准无监督自编码器基线，并实现了零误报的高精度操作点。

Conclusion: 通过显式分离活动存在性和活动强度，可以显著提升无监督窗口级内部威胁检测性能，为发现长期攻击活动提供了有效方法。

Abstract: User and Entity Behavior Analytics (UEBA) systems commonly detect insider threats by scoring fixed time windows of user activity for anomalous behavior. While this window-level paradigm has proven effective for identifying sharp behavioral deviations, it remains unclear how much information about longer-running attack campaigns is already present within individual windows, and how such information can be leveraged for campaign discovery. In this work, we study unsupervised window-level insider threat detection on the CERT r4.2 dataset and show that explicitly separating activity presence from activity magnitude yields substantial performance gains. We introduce a dual-channel convolutional autoencoder that reconstructs both a binary activity mask and corresponding activity values, allowing the model to focus representational capacity on sparse behavioral structure rather than dense inactive baselines. Across multiday attack campaigns lasting between one and seven days, the proposed approach achieves a window-level precision-recall AUC of 0.71, substantially exceeding standard unsupervised autoencoder baselines and enabling high-precision operating points with zero false alarms.

</details>


### [47] [IU-GUARD: Privacy-Preserving Spectrum Coordination for Incumbent Users under Dynamic Spectrum Sharing](https://arxiv.org/abs/2602.11023)
*Shaoyu Li,Hexuan Yu,Shanghao Shi,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.CR

TL;DR: IU-GUARD是一个保护隐私的频谱共享框架，使用可验证凭证和零知识证明，让主要用户无需透露身份即可访问频谱，同时保护其操作隐私和任务机密性。


<details>
  <summary>Details</summary>
Motivation: 当前频谱共享框架中的主要用户保护机制存在严重缺陷：环境感知能力需要昂贵的传感器部署且易受干扰和安全风险；主要用户信息能力则需要主要用户向频谱协调系统披露身份和操作参数，这会创建可链接的记录，损害操作隐私和任务机密性。

Method: 提出IU-GUARD框架，利用可验证凭证和零知识证明技术，允许主要用户向频谱协调系统证明其授权，同时仅披露必要的操作参数，将主要用户身份与频谱访问解耦，防止跨请求链接，并减轻集中式频谱协调系统数据泄露风险。

Result: 实现了一个原型系统，评估显示IU-GUARD在提供强大隐私保证的同时，具有实用的计算和通信开销，适合实时动态频谱共享部署。

Conclusion: IU-GUARD是一个实用的隐私保护频谱共享解决方案，解决了当前主要用户保护机制的关键限制，平衡了频谱利用效率和主要用户隐私保护的需求。

Abstract: With the growing demand for wireless spectrum, dynamic spectrum sharing (DSS) frameworks such as the Citizens Broadband Radio Service (CBRS) have emerged as practical solutions to improve utilization while protecting incumbent users (IUs) such as military radars. However, current incumbent protection mechanisms face critical limitations. The Environmental Sensing Capability (ESC) requires costly sensor deployments and remains vulnerable to interference and security risks. Alternatively, the Incumbent Informing Capability (IIC) requires IUs to disclose their identities and operational parameters to the Spectrum Coordination System (SCS), creating linkable records that compromise operational privacy and mission secrecy. We propose IU-GUARD, a privacy-preserving spectrum sharing framework that enables IUs to access spectrum without revealing their identities. Leveraging verifiable credentials (VCs) and zero-knowledge proofs (ZKPs), IU-GUARD allows IUs to prove their authorization to the SCS while disclosing only essential operational parameters. This decouples IU identity from spectrum access, prevents cross-request linkage, and mitigates the risk of centralized SCS data leakage. We implement a prototype, and our evaluation shows that IU-GUARD achieves strong privacy guarantees with practical computation and communication overhead, making it suitable for real-time DSS deployment.

</details>
