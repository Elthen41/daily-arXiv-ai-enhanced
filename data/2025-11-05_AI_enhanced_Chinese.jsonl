{"id": "2511.02071", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02071", "abs": "https://arxiv.org/abs/2511.02071", "authors": ["Xinyi Lin", "Yuyang Zhang", "Yuanhang Gan", "Juntao Chen", "Hao Shen", "Yichun He", "Lijun Li", "Ze Yuan", "Shuang Wang", "Chaohao Wang", "Rui Zhang", "Na Li", "Jia Liu"], "title": "Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing", "comment": null, "summary": "Scientific experiment and manufacture rely on complex, multi-step procedures\nthat demand continuous human expertise for precise execution and\ndecision-making. Despite advances in machine learning and automation,\nconventional models remain confined to virtual domains, while real-world\nexperiment and manufacture still rely on human supervision and expertise. This\ngap between machine intelligence and physical execution limits reproducibility,\nscalability, and accessibility across scientific and manufacture workflows.\nHere, we introduce human-AI co-embodied intelligence, a new form of physical AI\nthat unites human users, agentic AI, and wearable hardware into an integrated\nsystem for real-world experiment and intelligent manufacture. In this paradigm,\nhumans provide precise execution and control, while agentic AI contributes\nmemory, contextual reasoning, adaptive planning, and real-time feedback. The\nwearable interface continuously captures the experimental and manufacture\nprocesses, facilitates seamless communication between humans and AI for\ncorrective guidance and interpretable collaboration. As a demonstration, we\npresent Agentic-Physical Experimentation (APEX) system, coupling agentic\nreasoning with physical execution through mixed-reality. APEX observes and\ninterprets human actions, aligns them with standard operating procedures,\nprovides 3D visual guidance, and analyzes every step. Implemented in a\ncleanroom for flexible electronics fabrication, APEX system achieves\ncontext-aware reasoning with accuracy exceeding general multimodal large\nlanguage models, corrects errors in real time, and transfers expertise to\nbeginners. These results establish a new class of agentic-physical-human\nintelligence that extends agentic reasoning beyond computation into the\nphysical domain, transforming scientific research and manufacturing into\nautonomous, traceable, interpretable, and scalable processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4eba\u7c7b-AI\u5171\u4f53\u73b0\u667a\u80fd\u7684\u65b0\u8303\u5f0f\uff0c\u5c06\u4eba\u7c7b\u7528\u6237\u3001\u667a\u80fdAI\u548c\u53ef\u7a7f\u6234\u786c\u4ef6\u96c6\u6210\u5230\u4e00\u4e2a\u7cfb\u7edf\u4e2d\uff0c\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u548c\u667a\u80fd\u5236\u9020\u3002\u901a\u8fc7APEX\u7cfb\u7edf\u5728\u67d4\u6027\u7535\u5b50\u5236\u9020\u4e2d\u7684\u6f14\u793a\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u5b9e\u65f6\u7ea0\u9519\u548c\u4e13\u4e1a\u77e5\u8bc6\u8f6c\u79fb\u3002", "motivation": "\u79d1\u5b66\u5b9e\u9a8c\u548c\u5236\u9020\u4f9d\u8d56\u590d\u6742\u591a\u6b65\u9aa4\u7a0b\u5e8f\uff0c\u9700\u8981\u6301\u7eed\u7684\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u8fdb\u884c\u7cbe\u786e\u6267\u884c\u548c\u51b3\u7b56\u3002\u4f20\u7edf\u6a21\u578b\u5c40\u9650\u4e8e\u865a\u62df\u9886\u57df\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u548c\u5236\u9020\u4ecd\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd9\u79cd\u673a\u5668\u667a\u80fd\u4e0e\u7269\u7406\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u9650\u5236\u4e86\u79d1\u5b66\u548c\u5236\u9020\u5de5\u4f5c\u6d41\u7a0b\u7684\u53ef\u91cd\u590d\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u5f15\u5165\u4eba\u7c7b-AI\u5171\u4f53\u73b0\u667a\u80fd\uff0c\u5c06\u4eba\u7c7b\u7528\u6237\u3001\u667a\u80fdAI\u548c\u53ef\u7a7f\u6234\u786c\u4ef6\u96c6\u6210\u5230\u4e00\u4e2a\u7cfb\u7edf\u4e2d\u3002\u4eba\u7c7b\u63d0\u4f9b\u7cbe\u786e\u6267\u884c\u548c\u63a7\u5236\uff0c\u667a\u80fdAI\u8d21\u732e\u8bb0\u5fc6\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u81ea\u9002\u5e94\u89c4\u5212\u548c\u5b9e\u65f6\u53cd\u9988\u3002\u901a\u8fc7APEX\u7cfb\u7edf\uff0c\u5c06\u667a\u80fd\u63a8\u7406\u4e0e\u7269\u7406\u6267\u884c\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u76f8\u7ed3\u5408\uff0c\u89c2\u5bdf\u548c\u89e3\u91ca\u4eba\u7c7b\u52a8\u4f5c\uff0c\u4e0e\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f\u5bf9\u9f50\uff0c\u63d0\u4f9b3D\u89c6\u89c9\u6307\u5bfc\u5e76\u5206\u6790\u6bcf\u4e00\u6b65\u3002", "result": "\u5728\u6d01\u51c0\u5ba4\u67d4\u6027\u7535\u5b50\u5236\u9020\u4e2d\u5b9e\u65bd\u7684APEX\u7cfb\u7edf\u5b9e\u73b0\u4e86\u8d85\u8d8a\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u7cbe\u5ea6\uff0c\u80fd\u591f\u5b9e\u65f6\u7ea0\u6b63\u9519\u8bef\uff0c\u5e76\u5c06\u4e13\u4e1a\u77e5\u8bc6\u8f6c\u79fb\u7ed9\u521d\u5b66\u8005\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5efa\u7acb\u4e86\u4e00\u7c7b\u65b0\u7684\u667a\u80fd-\u7269\u7406-\u4eba\u7c7b\u667a\u80fd\uff0c\u5c06\u667a\u80fd\u63a8\u7406\u4ece\u8ba1\u7b97\u6269\u5c55\u5230\u7269\u7406\u9886\u57df\uff0c\u5c06\u79d1\u5b66\u7814\u7a76\u548c\u5236\u9020\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u3001\u53ef\u8ffd\u6eaf\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u8fc7\u7a0b\u3002"}}
{"id": "2511.02094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02094", "abs": "https://arxiv.org/abs/2511.02094", "authors": ["Michel Ma", "Takuma Seno", "Kaushik Subramanian", "Peter R. Wurman", "Peter Stone", "Craig Sherstan"], "title": "Automated Reward Design for Gran Turismo", "comment": null, "summary": "When designing reinforcement learning (RL) agents, a designer communicates\nthe desired agent behavior through the definition of reward functions -\nnumerical feedback given to the agent as reward or punishment for its actions.\nHowever, mapping desired behaviors to reward functions can be a difficult\nprocess, especially in complex environments such as autonomous racing. In this\npaper, we demonstrate how current foundation models can effectively search over\na space of reward functions to produce desirable RL agents for the Gran Turismo\n7 racing game, given only text-based instructions. Through a combination of\nLLM-based reward generation, VLM preference-based evaluation, and human\nfeedback we demonstrate how our system can be used to produce racing agents\ncompetitive with GT Sophy, a champion-level RL racing agent, as well as\ngenerate novel behaviors, paving the way for practical automated reward design\nin real world applications.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u81ea\u52a8\u641c\u7d22\u5956\u52b1\u51fd\u6570\u6765\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5728Gran Turismo 7\u8d5b\u8f66\u6e38\u620f\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u51a0\u519b\u7ea7\u667a\u80fd\u4f53GT Sophy\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u5e76\u751f\u6210\u4e86\u65b0\u9896\u884c\u4e3a\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u6765\u4f20\u8fbe\u671f\u671b\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u662f\u4e00\u4e2a\u56f0\u96be\u7684\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u73af\u5883\u5982\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8eLLM\u7684\u5956\u52b1\u751f\u6210\u3001\u57fa\u4e8eVLM\u504f\u597d\u7684\u8bc4\u4f30\u548c\u4eba\u7c7b\u53cd\u9988\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u81ea\u52a8\u641c\u7d22\u5956\u52b1\u51fd\u6570\u7a7a\u95f4\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4ea7\u751f\u4e0e\u51a0\u519b\u7ea7RL\u8d5b\u8f66\u667a\u80fd\u4f53GT Sophy\u7ade\u4e89\u7684\u8d5b\u8f66\u667a\u80fd\u4f53\uff0c\u5e76\u80fd\u751f\u6210\u65b0\u9896\u7684\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u5956\u52b1\u8bbe\u8ba1\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u81ea\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.02109", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.02109", "abs": "https://arxiv.org/abs/2511.02109", "authors": ["Joshua Ashkinaze", "Hua Shen", "Sai Avula", "Eric Gilbert", "Ceren Budak"], "title": "Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences", "comment": "NeurIPS 2025 (Spotlight)", "summary": "We introduce the Deep Value Benchmark (DVB), an evaluation framework that\ndirectly tests whether large language models (LLMs) learn fundamental human\nvalues or merely surface-level preferences. This distinction is critical for AI\nalignment: Systems that capture deeper values are likely to generalize human\nintentions robustly, while those that capture only superficial patterns in\npreference data risk producing misaligned behavior. The DVB uses a novel\nexperimental design with controlled confounding between deep values (e.g.,\nmoral principles) and shallow features (e.g., superficial attributes). In the\ntraining phase, we expose LLMs to human preference data with deliberately\ncorrelated deep and shallow features -- for instance, where a user consistently\nprefers (non-maleficence, formal language) options over (justice, informal\nlanguage) alternatives. The testing phase then breaks these correlations,\npresenting choices between (justice, formal language) and (non-maleficence,\ninformal language) options. This design allows us to precisely measure a\nmodel's Deep Value Generalization Rate (DVGR) -- the probability of\ngeneralizing based on the underlying value rather than the shallow feature.\nAcross 9 different models, the average DVGR is just 0.30. All models generalize\ndeep values less than chance. Larger models have a (slightly) lower DVGR than\nsmaller models. We are releasing our dataset, which was subject to three\nseparate human validation experiments. DVB provides an interpretable measure of\na core feature of alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86Deep Value Benchmark (DVB)\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5b66\u4e60\u5230\u4eba\u7c7b\u6df1\u5c42\u4ef7\u503c\u89c2\u800c\u975e\u4ec5\u8868\u9762\u504f\u597d\u3002\u901a\u8fc7\u63a7\u5236\u6df1\u5c42\u4ef7\u503c\u89c2\u548c\u6d45\u5c42\u7279\u5f81\u7684\u6df7\u6dc6\uff0c\u6d4b\u91cf\u6a21\u578b\u7684\u6df1\u5c42\u4ef7\u503c\u6cdb\u5316\u7387(DVGR)\u3002", "motivation": "\u533a\u5206AI\u7cfb\u7edf\u662f\u5426\u5b66\u4e60\u5230\u4eba\u7c7b\u6df1\u5c42\u4ef7\u503c\u89c2\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u4ec5\u6355\u6349\u8868\u9762\u504f\u597d\u7684\u7cfb\u7edf\u53ef\u80fd\u4ea7\u751f\u4e0d\u534f\u8c03\u7684\u884c\u4e3a\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u7a33\u5065\u5730\u6cdb\u5316\u4eba\u7c7b\u610f\u56fe\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u8ba9LLMs\u63a5\u89e6\u6df1\u5c42\u4ef7\u503c\u89c2\u548c\u6d45\u5c42\u7279\u5f81\u76f8\u5173\u7684\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u6d4b\u8bd5\u9636\u6bb5\u6253\u7834\u8fd9\u4e9b\u76f8\u5173\u6027\uff0c\u6d4b\u91cf\u6a21\u578b\u57fa\u4e8e\u6df1\u5c42\u4ef7\u503c\u89c2\u800c\u975e\u6d45\u5c42\u7279\u5f81\u8fdb\u884c\u6cdb\u5316\u7684\u6982\u7387(DVGR)\u3002", "result": "\u57289\u4e2a\u4e0d\u540c\u6a21\u578b\u4e2d\uff0c\u5e73\u5747DVGR\u4ec5\u4e3a0.30\uff0c\u6240\u6709\u6a21\u578b\u7684\u6df1\u5c42\u4ef7\u503c\u6cdb\u5316\u7387\u90fd\u4f4e\u4e8e\u968f\u673a\u6c34\u5e73\u3002\u8f83\u5927\u6a21\u578b\u7684DVGR\u7565\u4f4e\u4e8e\u8f83\u5c0f\u6a21\u578b\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u6cdb\u5316\u6df1\u5c42\u4eba\u7c7b\u4ef7\u503c\u89c2\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cDVB\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6838\u5fc3\u7279\u5f81\u8861\u91cf\u6807\u51c6\u3002"}}
{"id": "2511.01898", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.01898", "abs": "https://arxiv.org/abs/2511.01898", "authors": ["Hanie Vatani", "Reza Ebrahimi Atani"], "title": "FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring", "comment": "10 pages, 4 figures, Accepted in 6th International Conference on Soft\n  Computing (CSC2025)", "summary": "Federated Learning (FL) enables collaborative model training without sharing\nraw data but suffers from limited scalability, high communication costs, and\nprivacy risks due to its centralized architecture. This paper proposes\nFedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability,\nsecurity, and energy efficiency. Multiple edge servers distribute workloads and\nperform score-based client selection, prioritizing participants based on\nutility, energy efficiency, and data sensitivity. Secure Aggregation with\nHomomorphic Encryption and Differential Privacy protects model updates from\nexposure and manipulation. Evaluated on the eICU healthcare dataset,\nFedSelect-ME achieves higher prediction accuracy, improved fairness across\nregions, and reduced communication overhead compared to FedAvg, FedProx, and\nFedSelect. The results demonstrate that the proposed framework effectively\naddresses the bottlenecks of conventional FL, offering a secure, scalable, and\nefficient solution for large-scale, privacy-sensitive healthcare applications.", "AI": {"tldr": "FedSelect-ME\u662f\u4e00\u4e2a\u5206\u5c42\u591a\u8fb9\u7f18\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8fb9\u7f18\u670d\u52a1\u5668\u3001\u57fa\u4e8e\u5206\u6570\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u4ee5\u53ca\u5b89\u5168\u805a\u5408\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u3001\u901a\u4fe1\u6210\u672c\u548c\u9690\u79c1\u98ce\u9669\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u96c6\u4e2d\u5f0f\u67b6\u6784\u5e26\u6765\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\u3001\u9ad8\u901a\u4fe1\u6210\u672c\u548c\u9690\u79c1\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u5065\u7b49\u9690\u79c1\u654f\u611f\u5e94\u7528\u4e2d\u8fd9\u4e9b\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u8fb9\u7f18\u67b6\u6784\uff0c\u591a\u4e2a\u8fb9\u7f18\u670d\u52a1\u5668\u5206\u5e03\u5de5\u4f5c\u8d1f\u8f7d\uff1b\u5b9e\u65bd\u57fa\u4e8e\u5206\u6570\u7684\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u7efc\u5408\u8003\u8651\u6548\u7528\u3001\u80fd\u6548\u548c\u6570\u636e\u654f\u611f\u6027\uff1b\u7ed3\u5408\u540c\u6001\u52a0\u5bc6\u548c\u5dee\u5206\u9690\u79c1\u7684\u5b89\u5168\u805a\u5408\u4fdd\u62a4\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5728eICU\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4FedAvg\u3001FedProx\u548cFedSelect\uff0cFedSelect-ME\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3001\u66f4\u597d\u7684\u533a\u57df\u95f4\u516c\u5e73\u6027\u548c\u66f4\u4f4e\u7684\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u9690\u79c1\u654f\u611f\u7684\u533b\u7597\u4fdd\u5065\u5e94\u7528\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02119", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02119", "abs": "https://arxiv.org/abs/2511.02119", "authors": ["Ziheng Geng", "Jiachen Liu", "Ran Cao", "Lu Cheng", "Dan M. Frangopol", "Minghui Cheng"], "title": "InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance", "comment": null, "summary": "Flood insurance is an effective strategy for individuals to mitigate\ndisaster-related losses. However, participation rates among at-risk populations\nin the United States remain strikingly low. This gap underscores the need to\nunderstand and model the behavioral mechanisms underlying insurance decisions.\nLarge language models (LLMs) have recently exhibited human-like intelligence\nacross wide-ranging tasks, offering promising tools for simulating human\ndecision-making. This study constructs a benchmark dataset to capture insurance\npurchase probabilities across factors. Using this dataset, the capacity of LLMs\nis evaluated: while LLMs exhibit a qualitative understanding of factors, they\nfall short in estimating quantitative probabilities. To address this\nlimitation, InsurAgent, an LLM-empowered agent comprising five modules\nincluding perception, retrieval, reasoning, action, and memory, is proposed.\nThe retrieval module leverages retrieval-augmented generation (RAG) to ground\ndecisions in empirical survey data, achieving accurate estimation of marginal\nand bivariate probabilities. The reasoning module leverages LLM common sense to\nextrapolate beyond survey data, capturing contextual information that is\nintractable for traditional models. The memory module supports the simulation\nof temporal decision evolutions, illustrated through a roller coaster life\ntrajectory. Overall, InsurAgent provides a valuable tool for behavioral\nmodeling and policy analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86InsurAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u6a21\u62df\u6d2a\u6c34\u4fdd\u9669\u8d2d\u4e70\u51b3\u7b56\u884c\u4e3a\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u63a8\u7406\u6a21\u5757\u89e3\u51b3LLM\u5728\u6982\u7387\u4f30\u8ba1\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7f8e\u56fd\u9ad8\u98ce\u9669\u4eba\u7fa4\u7684\u6d2a\u6c34\u4fdd\u9669\u53c2\u4e0e\u7387\u6781\u4f4e\uff0c\u9700\u8981\u7406\u89e3\u4fdd\u9669\u51b3\u7b56\u7684\u884c\u4e3a\u673a\u5236\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\u3002", "method": "\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u8bc4\u4f30LLM\u80fd\u529b\uff0c\u63d0\u51faInsurAgent\u667a\u80fd\u4f53\uff0c\u5305\u542b\u611f\u77e5\u3001\u68c0\u7d22\u3001\u63a8\u7406\u3001\u884c\u52a8\u548c\u8bb0\u5fc6\u4e94\u4e2a\u6a21\u5757\uff0c\u5176\u4e2d\u68c0\u7d22\u6a21\u5757\u4f7f\u7528RAG\u6280\u672f\u57fa\u4e8e\u8c03\u67e5\u6570\u636e\uff0c\u63a8\u7406\u6a21\u5757\u5229\u7528LLM\u5e38\u8bc6\u8fdb\u884c\u63a8\u65ad\u3002", "result": "LLM\u5bf9\u56e0\u7d20\u6709\u5b9a\u6027\u7406\u89e3\u4f46\u6982\u7387\u4f30\u8ba1\u4e0d\u8db3\uff0cInsurAgent\u901a\u8fc7RAG\u51c6\u786e\u4f30\u8ba1\u8fb9\u9645\u548c\u53cc\u53d8\u91cf\u6982\u7387\uff0c\u63a8\u7406\u6a21\u5757\u6355\u6349\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5904\u7406\u7684\u60c5\u5883\u4fe1\u606f\uff0c\u8bb0\u5fc6\u6a21\u5757\u652f\u6301\u65f6\u95f4\u51b3\u7b56\u6f14\u5316\u6a21\u62df\u3002", "conclusion": "InsurAgent\u4e3a\u884c\u4e3a\u5efa\u6a21\u548c\u653f\u7b56\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u80fd\u591f\u51c6\u786e\u6a21\u62df\u4fdd\u9669\u8d2d\u4e70\u51b3\u7b56\u884c\u4e3a\u3002"}}
{"id": "2511.02132", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.02132", "abs": "https://arxiv.org/abs/2511.02132", "authors": ["Mansi Choudhary", "Karthik Sangaiah", "Sonali Singh", "Muhammad Osama", "Lisa Wu Wills", "Ganesh Dasika"], "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects", "comment": "11 pages, 14 figures", "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5206\u89e3\u5f0fAI GPU\u7684NUMA\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\u2014\u2014Swizzled Head-first Mapping\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u5934\u4e0eGPU NUMA\u57df\u5bf9\u9f50\u6765\u5229\u7528\u7247\u5185\u7f13\u5b58\u91cd\u7528\uff0c\u5728AMD MI300X\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6ce8\u610f\u529b\u7b97\u6cd5\u9ad8\u8fbe50%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u5206\u89e3\u5f0fAI GPU\u7684\u5174\u8d77\uff0c\u591a\u82af\u7247\u8bbe\u8ba1\u4e2d\u7684\u975e\u5747\u5300\u5185\u5b58\u8bbf\u95ee(NUMA)\u6210\u4e3a\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5173\u952e\u74f6\u9888\u3002\u4f20\u7edfGPU\u5185\u6838\u8c03\u5ea6\u7b56\u7565\u5047\u8bbe\u5747\u5300\u5185\u5b58\u8bbf\u95ee\uff0c\u65e0\u6cd5\u5e94\u5bf9\u4e0d\u540c\u8ba1\u7b97\u533a\u57df\u5185\u5b58\u5ef6\u8fdf\u548c\u5e26\u5bbd\u7684\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86Swizzled Head-first Mapping\u7a7a\u95f4\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u8bc6\u522bNUMA\u6548\u5e94\u5bf9\u591a\u5934\u6ce8\u610f\u529b(MHA)\u5c40\u90e8\u6027\u7684\u5f71\u54cd\uff0c\u5c06\u6ce8\u610f\u529b\u5934\u4e0eGPU NUMA\u57df\u5bf9\u9f50\u4ee5\u5229\u7528\u7247\u5185\u7f13\u5b58\u91cd\u7528\u3002", "result": "\u5728AMD MI300X\u67b6\u6784\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f7f\u7528\u4f20\u7edf\u8c03\u5ea6\u6280\u672f\u7684\u6700\u5148\u8fdb\u6ce8\u610f\u529b\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe50%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u6301\u7eed\u4fdd\u630180-97%\u7684\u9ad8L2\u7f13\u5b58\u547d\u4e2d\u7387\u3002", "conclusion": "NUMA\u611f\u77e5\u8c03\u5ea6\u5bf9\u4e8e\u5728\u4e0b\u4e00\u4ee3\u5206\u89e3\u5f0fGPU\u4e0a\u5b9e\u73b0\u5b8c\u5168\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u53ef\u6269\u5c55AI\u8bad\u7ec3\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u524d\u8fdb\u8def\u5f84\u3002"}}
{"id": "2511.02130", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02130", "abs": "https://arxiv.org/abs/2511.02130", "authors": ["Renos Zabounidis", "Aditya Golatkar", "Michael Kleinman", "Alessandro Achille", "Wei Xia", "Stefano Soatto"], "title": "Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning", "comment": "Accepted at Efficient Reasoning Workshop at NeurIPS 2025", "summary": "We propose Re-FORC, an adaptive reward prediction method that, given a\ncontext, enables prediction of the expected future rewards as a function of the\nnumber of future thinking tokens. Re-FORC trains a lightweight adapter on\nreasoning models, demonstrating improved prediction with longer reasoning and\nlarger models. Re-FORC enables: 1) early stopping of unpromising reasoning\nchains, reducing compute by 26% while maintaining accuracy, 2) optimized model\nand thinking length selection that achieves 4% higher accuracy at equal compute\nand 55% less compute at equal accuracy compared to the largest model, 3)\nadaptive test-time scaling, which increases accuracy by 11% in high compute\nregime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with\nlength control via cost-per-token thresholds while estimating computation time\nupfront.", "AI": {"tldr": "Re-FORC\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5956\u52b1\u9884\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u672a\u6765\u601d\u8003token\u6570\u91cf\u9884\u6d4b\u9884\u671f\u672a\u6765\u5956\u52b1\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5728\u63a8\u7406\u6a21\u578b\u4e0a\u5b9e\u73b0\u66f4\u957f\u7684\u63a8\u7406\u548c\u66f4\u5927\u7684\u6a21\u578b\u5e26\u6765\u7684\u6539\u8fdb\u9884\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u548c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u63a7\u5236\u63a8\u7406\u957f\u5ea6\u3001\u4f18\u5316\u6a21\u578b\u9009\u62e9\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u51c6\u786e\u6027\u3002", "method": "\u5728\u63a8\u7406\u6a21\u578b\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u5b9e\u73b0\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u81ea\u9002\u5e94\u5956\u52b1\u9884\u6d4b\uff0c\u6839\u636e\u672a\u6765\u601d\u8003token\u6570\u91cf\u9884\u6d4b\u9884\u671f\u5956\u52b1\uff0c\u652f\u6301\u52a8\u6001\u63a8\u7406\u957f\u5ea6\u63a7\u5236\u548c\u8ba1\u7b97\u65f6\u95f4\u9884\u4f30\u3002", "result": "Re-FORC\u5b9e\u73b0\u4e86\uff1a1\uff09\u63d0\u524d\u7ec8\u6b62\u65e0\u671b\u63a8\u7406\u94fe\uff0c\u51cf\u5c1126%\u8ba1\u7b97\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff1b2\uff09\u4f18\u5316\u6a21\u578b\u548c\u601d\u8003\u957f\u5ea6\u9009\u62e9\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u4e0b\u63d0\u9ad84%\u51c6\u786e\u6027\uff0c\u5728\u76f8\u540c\u51c6\u786e\u6027\u4e0b\u51cf\u5c1155%\u8ba1\u7b97\uff1b3\uff09\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u5728\u9ad8\u8ba1\u7b97\u548c\u4f4e\u8ba1\u7b97\u573a\u666f\u4e0b\u5206\u522b\u63d0\u9ad811%\u548c7%\u51c6\u786e\u6027\u3002", "conclusion": "Re-FORC\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u52a8\u6001\u63a8\u7406\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u57fa\u4e8etoken\u6210\u672c\u9608\u503c\u7684\u7075\u6d3b\u63a8\u7406\u957f\u5ea6\u63a7\u5236\u3002"}}
{"id": "2511.01952", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01952", "abs": "https://arxiv.org/abs/2511.01952", "authors": ["Jinhua Yin", "Peiru Yang", "Chen Yang", "Huili Wang", "Zhiyang Hu", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing", "comment": null, "summary": "Large vision-language models (LVLMs) derive their capabilities from extensive\ntraining on vast corpora of visual and textual data. Empowered by large-scale\nparameters, these models often exhibit strong memorization of their training\ndata, rendering them susceptible to membership inference attacks (MIAs).\nExisting MIA methods for LVLMs typically operate under white- or gray-box\nassumptions, by extracting likelihood-based features for the suspected data\nsamples based on the target LVLMs. However, mainstream LVLMs generally only\nexpose generated outputs while concealing internal computational features\nduring inference, limiting the applicability of these methods. In this work, we\npropose the first black-box MIA framework for LVLMs, based on a prior\nknowledge-calibrated memory probing mechanism. The core idea is to assess the\nmodel memorization of the private semantic information embedded within the\nsuspected image data, which is unlikely to be inferred from general world\nknowledge alone. We conducted extensive experiments across four LVLMs and three\ndatasets. Empirical results demonstrate that our method effectively identifies\ntraining data of LVLMs in a purely black-box setting and even achieves\nperformance comparable to gray-box and white-box methods. Further analysis\nreveals the robustness of our method against potential adversarial\nmanipulations, and the effectiveness of the methodology designs. Our code and\ndata are available at https://github.com/spmede/KCMP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\u6846\u67b6KCMP\uff0c\u901a\u8fc7\u5148\u9a8c\u77e5\u8bc6\u6821\u51c6\u7684\u8bb0\u5fc6\u63a2\u6d4b\u673a\u5236\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\u7a0b\u5ea6\uff0c\u5728\u7eaf\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u6709\u6548\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u767d\u76d2\u6216\u7070\u76d2\u5047\u8bbe\uff0c\u800c\u4e3b\u6d41\u6a21\u578b\u5728\u63a8\u7406\u65f6\u53ea\u66b4\u9732\u751f\u6210\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002\u9700\u8981\u5f00\u53d1\u5728\u7eaf\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u6821\u51c6\u7684\u8bb0\u5fc6\u63a2\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5bf9\u7591\u4f3c\u56fe\u50cf\u6570\u636e\u4e2d\u5d4c\u5165\u7684\u79c1\u6709\u8bed\u4e49\u4fe1\u606f\u7684\u8bb0\u5fc6\u7a0b\u5ea6\u6765\u8fdb\u884c\u6210\u5458\u63a8\u7406\uff0c\u8fd9\u4e9b\u4fe1\u606f\u4e0d\u592a\u53ef\u80fd\u4ec5\u4ece\u4e00\u822c\u4e16\u754c\u77e5\u8bc6\u4e2d\u63a8\u65ad\u51fa\u6765\u3002", "result": "\u5728\u56db\u4e2a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7eaf\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u80fd\u6709\u6548\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\uff0c\u6027\u80fd\u751a\u81f3\u53ef\u4e0e\u7070\u76d2\u548c\u767d\u76d2\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u4e14\u5bf9\u6f5c\u5728\u5bf9\u6297\u64cd\u4f5c\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "KCMP\u662f\u9996\u4e2a\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u8bb0\u5fc6\u63a2\u6d4b\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u6709\u6548\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u6210\u5458\uff0c\u4e3a\u6a21\u578b\u9690\u79c1\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2511.01860", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01860", "abs": "https://arxiv.org/abs/2511.01860", "authors": ["Leszek Sliwko"], "title": "A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks", "comment": "This is the accepted author's version of the paper. The final\n  published version is available in Global Journal of Computer Science and\n  Technology, 2019", "summary": "This review analyzes deployed and actively used workload schedulers'\nsolutions and presents a taxonomy in which those systems are divided into\nseveral hierarchical groups based on their architecture and design. While other\ntaxonomies do exist, this review has focused on the key design factors that\naffect the throughput and scalability of a given solution, as well as the\nincremental improvements which bettered such an architecture. This review gives\nspecial attention to Google's Borg, which is one of the most advanced and\npublished systems of this kind.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5df2\u90e8\u7f72\u548c\u6b63\u5728\u4f7f\u7528\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u5f71\u54cd\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u4ee5\u53ca\u67b6\u6784\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u7c7b\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5173\u6ce8\u5f71\u54cd\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\uff0c\u4ee5\u53ca\u67b6\u6784\u7684\u6e10\u8fdb\u5f0f\u6539\u8fdb\u3002", "method": "\u5206\u6790\u5df2\u90e8\u7f72\u548c\u6b63\u5728\u4f7f\u7528\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u6784\u5efa\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u7279\u522b\u5173\u6ce8Google Borg\u7b49\u5148\u8fdb\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u5206\u7c7b\u6cd5\uff0c\u8bc6\u522b\u4e86\u5f71\u54cd\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u7684\u67b6\u6784\u6f14\u8fdb\u548c\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u7279\u522b\u5f3a\u8c03\u4e86Google Borg\u7b49\u5148\u8fdb\u7cfb\u7edf\u7684\u8bbe\u8ba1\u7ecf\u9a8c\u3002"}}
{"id": "2511.02194", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02194", "abs": "https://arxiv.org/abs/2511.02194", "authors": ["Yibo Zhao", "Yang Zhao", "Hongru Du", "Hao Frank Yang"], "title": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning", "comment": null, "summary": "Decision-making models for individuals, particularly in high-stakes scenarios\nlike vaccine uptake, often diverge from population optimal predictions. This\ngap arises from the uniqueness of the individual decision-making process,\nshaped by numerical attributes (e.g., cost, time) and linguistic influences\n(e.g., personal preferences and constraints). Developing upon Utility Theory\nand leveraging the textual-reasoning capabilities of Large Language Models\n(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric\nReasoning framework (ATHENA) to address the optimal information integration.\nATHENA uniquely integrates two stages: First, it discovers robust, group-level\nsymbolic utility functions via LLM-augmented symbolic discovery; Second, it\nimplements individual-level semantic adaptation, creating personalized semantic\ntemplates guided by the optimal utility to model personalized choices.\nValidated on real-world travel mode and vaccine choice tasks, ATHENA\nconsistently outperforms utility-based, machine learning, and other LLM-based\nmodels, lifting F1 score by at least 6.5% over the strongest cutting-edge\nmodels. Further, ablation studies confirm that both stages of ATHENA are\ncritical and complementary, as removing either clearly degrades overall\npredictive performance. By organically integrating symbolic utility modeling\nand semantic adaptation, ATHENA provides a new scheme for modeling\nhuman-centric decisions. The project page can be found at\nhttps://yibozh.github.io/Athena.", "AI": {"tldr": "\u63d0\u51fa\u4e86ATHENA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u6548\u7528\u5efa\u6a21\u548c\u8bed\u4e49\u9002\u5e94\u6765\u4e2a\u6027\u5316\u5efa\u6a21\u4eba\u7c7b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5728\u65c5\u884c\u65b9\u5f0f\u548c\u75ab\u82d7\u9009\u62e9\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4e2a\u4f53\u51b3\u7b56\u6a21\u578b\u4e0e\u7fa4\u4f53\u6700\u4f18\u9884\u6d4b\u5b58\u5728\u5dee\u8ddd\uff0c\u56e0\u4e3a\u4e2a\u4f53\u51b3\u7b56\u8fc7\u7a0b\u53d7\u5230\u6570\u503c\u5c5e\u6027\uff08\u6210\u672c\u3001\u65f6\u95f4\uff09\u548c\u8bed\u8a00\u56e0\u7d20\uff08\u4e2a\u4eba\u504f\u597d\u548c\u7ea6\u675f\uff09\u7684\u5171\u540c\u5f71\u54cd\u3002", "method": "ATHENA\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u901a\u8fc7LLM\u589e\u5f3a\u7684\u7b26\u53f7\u53d1\u73b0\u83b7\u5f97\u7fa4\u4f53\u7ea7\u7b26\u53f7\u6548\u7528\u51fd\u6570\uff1b2\uff09\u57fa\u4e8e\u6700\u4f18\u6548\u7528\u8fdb\u884c\u4e2a\u4f53\u7ea7\u8bed\u4e49\u9002\u5e94\uff0c\u521b\u5efa\u4e2a\u6027\u5316\u8bed\u4e49\u6a21\u677f\u6765\u5efa\u6a21\u4e2a\u6027\u5316\u9009\u62e9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u65c5\u884c\u6a21\u5f0f\u548c\u75ab\u82d7\u9009\u62e9\u4efb\u52a1\u4e2d\uff0cATHENA\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u6548\u7528\u3001\u673a\u5668\u5b66\u4e60\u548c\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u6a21\u578b\uff0cF1\u5206\u6570\u6bd4\u6700\u5f3a\u524d\u6cbf\u6a21\u578b\u81f3\u5c11\u63d0\u53476.5%\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e24\u4e2a\u9636\u6bb5\u90fd\u81f3\u5173\u91cd\u8981\u4e14\u4e92\u8865\u3002", "conclusion": "\u901a\u8fc7\u6709\u673a\u6574\u5408\u7b26\u53f7\u6548\u7528\u5efa\u6a21\u548c\u8bed\u4e49\u9002\u5e94\uff0cATHENA\u4e3a\u5efa\u6a21\u4ee5\u4eba\u4e3a\u672c\u7684\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2511.02269", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02269", "abs": "https://arxiv.org/abs/2511.02269", "authors": ["Takuto Ando", "Yu Eto", "Ayumu Takeuchi", "Yasuhiko Nakashima"], "title": "Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA", "comment": "This paper is accepted at The Thirteenth International Symposium on\n  Computing and Networking (CANDAR2025)", "summary": "The rise of generative AI for tasks like Automatic Speech Recognition (ASR)\nhas created a critical energy consumption challenge. While ASICs offer high\nefficiency, they lack the programmability to adapt to evolving algorithms. To\naddress this trade-off, we implement and evaluate Whisper's core computational\nkernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)\naccelerator. To our knowledge, this is the first work to execute a Whisper\nkernel on a CGRA and compare its performance against CPUs and GPUs. Using\nhardware/software co-design, we evaluate our system via an FPGA prototype and\nproject performance for a 28 nm ASIC. Our results demonstrate superior energy\nefficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA\nJetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This\nwork positions CGLA as a promising platform for sustainable ASR on\npower-constrained edge devices.", "AI": {"tldr": "\u672c\u6587\u5728IMAX CGLA\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0\u4e86Whisper\u8bed\u97f3\u8bc6\u522b\u7684\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5728FPGA\u539f\u578b\u4e0a\u8bc4\u4f30\u5e76\u9884\u6d4b28nm ASIC\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6bd4NVIDIA Jetson AGX Orin\u548cRTX 4090\u66f4\u9ad8\u7684\u80fd\u6548\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u4efb\u52a1\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u80fd\u8017\u6311\u6218\uff0c\u4e13\u7528\u96c6\u6210\u7535\u8def(ASIC)\u867d\u7136\u9ad8\u6548\u4f46\u7f3a\u4e4f\u7b97\u6cd5\u9002\u5e94\u6027\uff0c\u9700\u8981\u627e\u5230\u80fd\u6548\u4e0e\u53ef\u7f16\u7a0b\u6027\u7684\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u901a\u7528\u7c97\u7c92\u5ea6\u7ebf\u6027\u9635\u5217(CGLA)\u52a0\u901f\u5668IMAX\u4e0a\u5b9e\u73b0Whisper\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u91c7\u7528\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7FPGA\u539f\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u9884\u6d4b28nm ASIC\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u6295\u5f71\u7684ASIC\u5728Q8_0\u6a21\u578b\u4e0a\u6bd4NVIDIA Jetson AGX Orin\u80fd\u6548\u9ad81.90\u500d\uff0c\u6bd4NVIDIA RTX 4090\u9ad89.83\u500d\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u80fd\u6548\u4f18\u52bf\u3002", "conclusion": "CGLA\u67b6\u6784\u662f\u529f\u7387\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u53ef\u6301\u7eedASR\u7684\u6709\u524d\u666f\u5e73\u53f0\uff0c\u4e3a\u751f\u6210\u5f0fAI\u5e94\u7528\u63d0\u4f9b\u4e86\u80fd\u6548\u4e0e\u53ef\u7f16\u7a0b\u6027\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2511.01861", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01861", "abs": "https://arxiv.org/abs/2511.01861", "authors": ["Johan Messchendorp", "Mohammad Al-Turany", "Volker Friese", "Thorsten Kollegger", "Bastian Loeher", "Jochen Markert", "Andrew Mistry", "Thomas Neff", "Adrian Oeftiger", "Michael Papenbrock", "Stephane Pietri", "Shahab Sanjari", "Tobias Stockmanns"], "title": "Conceptual Design Report for FAIR Computing", "comment": "88 pages, Conceptual Design Report for FAIR Computing", "summary": "This Conceptual Design Report (CDR) presents the plans of the computing\ninfrastructure for research at FAIR, Darmstadt, Germany. It presents the\ncomputing requirements of the various research groups, the policies for the\ncomputing and storage infrastructure, the foreseen FAIR computing model\nincluding the open data, software and services policies and architecture for\nthe periods starting in 2028 with the \"first science (plus)\" phase to the\nmodularized start version of FAIR. The overall ambition is to create a\nfederated and centrally-orchestrated infrastructure serving the large diversity\nof the research lines present with sufficient scalability and flexibility to\ncope with future data challenges that will be present at FAIR.", "AI": {"tldr": "\u672c\u6982\u5ff5\u8bbe\u8ba1\u62a5\u544a\u4ecb\u7ecd\u4e86\u5fb7\u56fd\u8fbe\u59c6\u65bd\u5854\u7279FAIR\u7814\u7a76\u8bbe\u65bd\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u8ba1\u5212\uff0c\u6db5\u76d6\u4ece2028\u5e74'\u9996\u6b21\u79d1\u5b66+'\u9636\u6bb5\u5230\u6a21\u5757\u5316\u542f\u52a8\u7248\u672c\u7684\u8ba1\u7b97\u9700\u6c42\u3001\u653f\u7b56\u3001\u6a21\u578b\u548c\u67b6\u6784\u3002", "motivation": "\u4e3aFAIR\u7814\u7a76\u8bbe\u65bd\u521b\u5efa\u7edf\u4e00\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff0c\u6ee1\u8db3\u591a\u6837\u5316\u7814\u7a76\u7ebf\u8def\u7684\u9700\u6c42\uff0c\u5e76\u5e94\u5bf9\u672a\u6765\u7684\u6570\u636e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8054\u90a6\u5316\u548c\u4e2d\u592e\u534f\u8c03\u7684\u57fa\u7840\u8bbe\u65bd\u67b6\u6784\uff0c\u5305\u62ec\u5f00\u653e\u6570\u636e\u3001\u8f6f\u4ef6\u548c\u670d\u52a1\u653f\u7b56\uff0c\u786e\u4fdd\u8db3\u591f\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002", "result": "\u5236\u5b9a\u4e86\u6db5\u76d6\u8ba1\u7b97\u9700\u6c42\u3001\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u653f\u7b56\u3001\u8ba1\u7b97\u6a21\u578b\u548c\u67b6\u6784\u7684\u7efc\u5408\u8ba1\u5212\u3002", "conclusion": "\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u670d\u52a1\u591a\u6837\u5316\u7814\u7a76\u7ebf\u8def\u3001\u5177\u6709\u8db3\u591f\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u7684\u8054\u90a6\u5316\u4e2d\u592e\u534f\u8c03\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2511.02083", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.02083", "abs": "https://arxiv.org/abs/2511.02083", "authors": ["Avi Bagchi", "Akhil Bhimaraju", "Moulik Choraria", "Daniel Alabi", "Lav R. Varshney"], "title": "Watermarking Discrete Diffusion Language Models", "comment": null, "summary": "Watermarking has emerged as a promising technique to track AI-generated\ncontent and differentiate it from authentic human creations. While prior work\nextensively studies watermarking for autoregressive large language models\n(LLMs) and image diffusion models, none address discrete diffusion language\nmodels, which are becoming popular due to their high inference throughput. In\nthis paper, we introduce the first watermarking method for discrete diffusion\nmodels by applying the distribution-preserving Gumbel-max trick at every\ndiffusion step and seeding the randomness with the sequence index to enable\nreliable detection. We experimentally demonstrate that our scheme is reliably\ndetectable on state-of-the-art diffusion language models and analytically prove\nthat it is distortion-free with an exponentially decaying probability of false\ndetection in the token sequence length.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u6269\u6563\u6b65\u9aa4\u5e94\u7528\u4fdd\u6301\u5206\u5e03\u7684Gumbel-max\u6280\u5de7\uff0c\u5e76\u4f7f\u7528\u5e8f\u5217\u7d22\u5f15\u4f5c\u4e3a\u968f\u673a\u79cd\u5b50\u6765\u5b9e\u73b0\u53ef\u9760\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u6280\u672f\u4e3b\u8981\u7814\u7a76\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u65e5\u76ca\u6d41\u884c\u7684\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e9b\u6a21\u578b\u56e0\u9ad8\u63a8\u7406\u541e\u5410\u91cf\u800c\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u5728\u6bcf\u4e2a\u6269\u6563\u6b65\u9aa4\u5e94\u7528\u5206\u5e03\u4fdd\u6301\u7684Gumbel-max\u6280\u5de7\uff0c\u5e76\u4f7f\u7528\u5e8f\u5217\u7d22\u5f15\u4f5c\u4e3a\u968f\u673a\u79cd\u5b50\u6765\u751f\u6210\u6c34\u5370\uff0c\u786e\u4fdd\u53ef\u9760\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6700\u65b0\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e0a\u53ef\u53ef\u9760\u68c0\u6d4b\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u5b83\u662f\u65e0\u5931\u771f\u7684\uff0c\u4e14\u8bef\u68c0\u6982\u7387\u968f\u4ee4\u724c\u5e8f\u5217\u957f\u5ea6\u5448\u6307\u6570\u8870\u51cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6c34\u5370\u65b9\u6848\u6210\u529f\u586b\u8865\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u6c34\u5370\u6280\u672f\u7684\u7a7a\u767d\uff0c\u4e3aAI\u751f\u6210\u5185\u5bb9\u7684\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02285", "categories": ["cs.AR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02285", "abs": "https://arxiv.org/abs/2511.02285", "authors": ["Zhuorui Zhao", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "title": "VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning", "comment": "accepted by SOCC 2025", "summary": "Large Language Models (LLMs) have shown impressive potential in generating\nVerilog codes, but ensuring functional correctness remains a challenge.\nExisting approaches often rely on self-consistency or simulation feedback to\nselect the best candidate, but they miss opportunities to focus LLM reasoning\non the most informative parts of the design. We propose VFocus, a three-stage\nframework that enhances Verilog generation by sharpening the focus of LLM\nreasoning onto critical decision points in the code generation process. In the\n\\textbf{pre-ranking stage}, VFocus generates multiple code candidates through\nLLM prompting, retries for syntactically valid outputs, and introduces a\n\\textit{Density-guided Filtering} to retain candidates that fall within the\n\"reasoning sweet spot\" for functional correctness. In the \\textbf{ranking\nstage}, we simulate each code candidate using an automatically generated\ntestbench and apply self-consistency-based clustering to identify the most\nconsistent outputs. Finally, in the \\textbf{post-ranking refinement stage},\nVFocus performs inconsistency mining on top-ranked candidates and invokes\nreasoning-augmented LLM prompts for candidate refinement. Experiments on the\nVerilogEval-Human benchmark show that VFocus significantly improves the pass@1\ncorrectness across multiple reasoning LLMs, demonstrating its effectiveness in\nenhancing Verilog generation for complex hardware design tasks.", "AI": {"tldr": "VFocus\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLM\u63a8\u7406\u805a\u7126\u4e8e\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u51b3\u7b56\u70b9\u6765\u589e\u5f3aVerilog\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u81ea\u4e00\u81f4\u6027\u6216\u4eff\u771f\u53cd\u9988\u6765\u9009\u62e9\u6700\u4f73\u5019\u9009\u4ee3\u7801\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u63a8\u7406\u805a\u7126\u4e8e\u8bbe\u8ba1\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u90e8\u5206\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u9884\u6392\u540d\u9636\u6bb5\u751f\u6210\u591a\u4e2a\u4ee3\u7801\u5019\u9009\u5e76\u5e94\u7528\u5bc6\u5ea6\u5f15\u5bfc\u8fc7\u6ee4\uff1b\u6392\u540d\u9636\u6bb5\u4f7f\u7528\u81ea\u52a8\u751f\u6210\u7684\u6d4b\u8bd5\u53f0\u8fdb\u884c\u4eff\u771f\u5e76\u57fa\u4e8e\u81ea\u4e00\u81f4\u6027\u805a\u7c7b\uff1b\u540e\u6392\u540d\u7ec6\u5316\u9636\u6bb5\u5bf9\u6392\u540d\u9760\u524d\u7684\u5019\u9009\u8fdb\u884c\u4e0d\u4e00\u81f4\u6027\u6316\u6398\u5e76\u4f7f\u7528\u63a8\u7406\u589e\u5f3a\u7684LLM\u63d0\u793a\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u5728VerilogEval-Human\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVFocus\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4e2a\u63a8\u7406LLM\u7684pass@1\u6b63\u786e\u6027\u3002", "conclusion": "VFocus\u901a\u8fc7\u805a\u7126LLM\u63a8\u7406\u4e8e\u5173\u952e\u51b3\u7b56\u70b9\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u590d\u6742\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u7684Verilog\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2511.01862", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01862", "abs": "https://arxiv.org/abs/2511.01862", "authors": ["Vanessa Sochat", "Daniel Milroy"], "title": "Possible Futures for Cloud Cost Models", "comment": "10 pages", "summary": "Cloud is now the leading software and computing hardware innovator, and is\nchanging the landscape of compute to one that is optimized for artificial\nintelligence and machine learning (AI/ML). Computing innovation was initially\ndriven to meet the needs of scientific computing. As industry and consumer\nusage of computing proliferated, there was a shift to satisfy a multipolar\ncustomer base. Demand for AI/ML now dominates modern computing and innovation\nhas centralized on cloud. As a result, cost and resource models designed to\nserve AI/ML use cases are not currently well suited for science. If resource\ncontention resulting from a unipole consumer makes access to contended\nresources harder for scientific users, a likely future is running scientific\nworkloads where they were not intended. In this article, we discuss the past,\ncurrent, and possible futures of cloud cost models for the continued support of\ndiscovery and science.", "AI": {"tldr": "\u4e91\u8ba1\u7b97\u5df2\u6210\u4e3aAI/ML\u521b\u65b0\u7684\u4e3b\u5bfc\u529b\u91cf\uff0c\u4f46\u5176\u6210\u672c\u6a21\u578b\u4e0d\u9002\u5408\u79d1\u5b66\u8ba1\u7b97\u9700\u6c42\uff0c\u53ef\u80fd\u5bfc\u81f4\u79d1\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u5728\u4e0d\u9002\u5408\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "motivation": "\u5206\u6790\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u4ece\u79d1\u5b66\u8ba1\u7b97\u5230AI/ML\u4e3b\u5bfc\u7684\u6f14\u53d8\uff0c\u63a2\u8ba8\u5982\u4f55\u7ee7\u7eed\u652f\u6301\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u56de\u987e\u548c\u73b0\u72b6\u5206\u6790\uff0c\u8ba8\u8bba\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u7684\u8fc7\u53bb\u3001\u73b0\u5728\u548c\u672a\u6765\u53ef\u80fd\u6027\u3002", "result": "\u53d1\u73b0AI/ML\u9700\u6c42\u4e3b\u5bfc\u7684\u4e91\u8ba1\u7b97\u521b\u65b0\u4f7f\u6210\u672c\u6a21\u578b\u4e0d\u518d\u9002\u5408\u79d1\u5b66\u8ba1\u7b97\u9700\u6c42\uff0c\u8d44\u6e90\u7ade\u4e89\u53ef\u80fd\u963b\u788d\u79d1\u5b66\u7528\u6237\u8bbf\u95ee\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\uff0c\u4ee5\u786e\u4fdd\u79d1\u5b66\u53d1\u73b0\u5de5\u4f5c\u8d1f\u8f7d\u5f97\u5230\u6301\u7eed\u652f\u6301\u3002"}}
{"id": "2511.02408", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02408", "abs": "https://arxiv.org/abs/2511.02408", "authors": ["Takuto Ando", "Yusuke Inoue"], "title": "Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA", "comment": "This paper was published in the proceedings of the 2024 Twelfth\n  International Symposium on Computing and Networking Workshops (CANDARW)", "summary": "In this paper, we implement a stand-alone facial expression recognition\nsystem on an SoC FPGA with multi-threading using a Deep learning Processor Unit\n(DPU). The system consists of two steps: one for face detection step and one\nfor facial expression recognition. In the previous work, the Haar Cascade\ndetector was run on a CPU in the face detection step due to FPGA resource\nlimitations, but this detector is less accurate for profile and variable\nillumination condition images. Moreover, the previous work used a dedicated\ncircuit accelerator, so running a second DNN inference for face detection on\nthe FPGA would require the addition of a new accelerator. As an alternative to\nthis approach, we run the two inferences by DNN on a DPU, which is a\ngeneral-purpose CNN accelerator of the systolic array type. Our method for face\ndetection using DenseBox and facial expression recognition using CNN on the\nsame DPU enables the efficient use of FPGA resources while maintaining a small\ncircuit size. We also developed a multi-threading technique that improves the\noverall throughput while increasing the DPU utilization efficiency. With this\napproach, we achieved an overall system throughput of 25 FPS and a throughput\nper power consumption of 2.4 times.", "AI": {"tldr": "\u5728SoC FPGA\u4e0a\u5b9e\u73b0\u57fa\u4e8eDPU\u7684\u591a\u7ebf\u7a0b\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528DenseBox\u8fdb\u884c\u4eba\u8138\u68c0\u6d4b\u548cCNN\u8fdb\u884c\u8868\u60c5\u8bc6\u522b\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u5de5\u4f5c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u89e3\u51b3\u4e4b\u524d\u5de5\u4f5c\u4e2dHaar Cascade\u68c0\u6d4b\u5668\u5728\u4fa7\u8138\u548c\u53d8\u5149\u7167\u6761\u4ef6\u4e0b\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u907f\u514d\u4e3a\u7b2c\u4e8c\u4e2aDNN\u63a8\u7406\u6dfb\u52a0\u65b0\u52a0\u901f\u5668\u5e26\u6765\u7684\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u5728\u540c\u4e00\u4e2aDPU\u4e0a\u8fd0\u884cDenseBox\u4eba\u8138\u68c0\u6d4b\u548cCNN\u8868\u60c5\u8bc6\u522b\u4e24\u4e2aDNN\u63a8\u7406\uff0c\u5f00\u53d1\u591a\u7ebf\u7a0b\u6280\u672f\u63d0\u9ad8\u541e\u5410\u91cf\u548cDPU\u5229\u7528\u7387\u3002", "result": "\u7cfb\u7edf\u6574\u4f53\u541e\u5410\u91cf\u8fbe\u523025 FPS\uff0c\u6bcf\u529f\u8017\u541e\u5410\u91cf\u63d0\u9ad8\u4e862.4\u500d\u3002", "conclusion": "\u4f7f\u7528DPU\u4f5c\u4e3a\u901a\u7528CNN\u52a0\u901f\u5668\u53ef\u4ee5\u6709\u6548\u5229\u7528FPGA\u8d44\u6e90\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u7ebf\u7a0b\u6280\u672f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.02219", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02219", "abs": "https://arxiv.org/abs/2511.02219", "authors": ["Changjiang Jiang", "Fengchang Yu", "Haihua Chen", "Wei Lu", "Jin Zeng"], "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Complex reasoning over tabular data is crucial in real-world data analysis,\nyet large language models (LLMs) often underperform due to complex queries,\nnoisy data, and limited numerical capabilities. To address these issues, we\npropose \\method, a framework consisting of: (1) a query decomposer that breaks\ndown complex questions, (2) a table sanitizer that cleans and filters noisy\ntables, and (3) a program-of-thoughts (PoT)-based reasoner that generates\nexecutable code to derive the final answer from the sanitized table. To ensure\nunbiased evaluation and mitigate data leakage, we introduce a new dataset,\nCalTab151, specifically designed for complex numerical reasoning over tables.\nExperimental results demonstrate that \\method consistently outperforms existing\nmethods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and\n19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively.\nMoreover, our framework integrates seamlessly with mainstream LLMs, providing a\nrobust solution for complex tabular numerical reasoning. These findings\nhighlight the effectiveness of our framework in enhancing LLM performance for\ncomplex tabular numerical reasoning. Data and code are available upon request.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\\method\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8868\u683c\u6570\u636e\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\uff0c\u8be5\u6846\u67b6\u5305\u542b\u67e5\u8be2\u5206\u89e3\u5668\u3001\u8868\u683c\u6e05\u7406\u5668\u548c\u57fa\u4e8e\u7a0b\u5e8f\u601d\u7ef4\u7684\u63a8\u7406\u5668\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6CalTab151\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8868\u683c\u6570\u636e\u63a8\u7406\u4e2d\u56e0\u590d\u6742\u67e5\u8be2\u3001\u566a\u58f0\u6570\u636e\u548c\u6709\u9650\u6570\u503c\u80fd\u529b\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1)\u67e5\u8be2\u5206\u89e3\u5668\u5206\u89e3\u590d\u6742\u95ee\u9898\uff0c(2)\u8868\u683c\u6e05\u7406\u5668\u6e05\u7406\u548c\u8fc7\u6ee4\u566a\u58f0\u8868\u683c\uff0c(3)\u57fa\u4e8e\u7a0b\u5e8f\u601d\u7ef4\u7684\u63a8\u7406\u5668\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u4ece\u6e05\u7406\u540e\u7684\u8868\u683c\u4e2d\u63a8\u5bfc\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5728TAT-QA\u3001TableBench\u548c\\method\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e868.79%\u30016.08%\u548c19.87%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8868\u683c\u6570\u503c\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u4e0e\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u4e3a\u590d\u6742\u8868\u683c\u6570\u503c\u63a8\u7406\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02176", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02176", "abs": "https://arxiv.org/abs/2511.02176", "authors": ["Fuyi Wang", "Fangyuan Sun", "Mingyuan Fan", "Jianying Zhou", "Jin Ma", "Chao Chen", "Jiangang Shu", "Leo Yu Zhang"], "title": "FLAME: Flexible and Lightweight Biometric Authentication Scheme in Malicious Environments", "comment": "Accepted to ACSAC'25", "summary": "Privacy-preserving biometric authentication (PPBA) enables client\nauthentication without revealing sensitive biometric data, addressing privacy\nand security concerns. Many studies have proposed efficient cryptographic\nsolutions to this problem based on secure multi-party computation, typically\nassuming a semi-honest adversary model, where all parties follow the protocol\nbut may try to learn additional information. However, this assumption often\nfalls short in real-world scenarios, where adversaries may behave maliciously\nand actively deviate from the protocol.\n  In this paper, we propose, implement, and evaluate $\\sysname$, a\n\\underline{F}lexible and \\underline{L}ightweight biometric\n\\underline{A}uthentication scheme designed for a \\underline{M}alicious\n\\underline{E}nvironment. By hybridizing lightweight secret-sharing-family\nprimitives within two-party computation, $\\sysname$ carefully designs a line of\nsupporting protocols that incorporate integrity checks with rationally extra\noverhead. Additionally, $\\sysname$ enables server-side authentication with\nvarious similarity metrics through a cross-metric-compatible design, enhancing\nflexibility and robustness without requiring any changes to the server-side\nprocess. A rigorous theoretical analysis validates the correctness, security,\nand efficiency of $\\sysname$. Extensive experiments highlight $\\sysname$'s\nsuperior efficiency, with a communication reduction by {$97.61\\times \\sim\n110.13\\times$} and a speedup of {$ 2.72\\times \\sim 2.82\\times$ (resp. $\n6.58\\times \\sim 8.51\\times$)} in a LAN (resp. WAN) environment, when compared\nto the state-of-the-art work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFLAME\u7684\u7075\u6d3b\u8f7b\u91cf\u7ea7\u751f\u7269\u8ba4\u8bc1\u65b9\u6848\uff0c\u4e13\u4e3a\u6076\u610f\u73af\u5883\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6df7\u5408\u8f7b\u91cf\u7ea7\u79d8\u5bc6\u5171\u4eab\u539f\u8bed\u548c\u53cc\u65b9\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u751f\u7269\u8ba4\u8bc1\u65b9\u6848\u5927\u591a\u57fa\u4e8e\u534a\u8bda\u5b9e\u5bf9\u624b\u6a21\u578b\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5bf9\u624b\u53ef\u80fd\u6076\u610f\u504f\u79bb\u534f\u8bae\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u79d8\u5bc6\u5171\u4eab\u539f\u8bed\u4e0e\u53cc\u65b9\u8ba1\u7b97\u6df7\u5408\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u5305\u542b\u5b8c\u6574\u6027\u68c0\u67e5\u7684\u652f\u6301\u534f\u8bae\uff0c\u5e76\u5b9e\u73b0\u4e86\u8de8\u5ea6\u91cf\u517c\u5bb9\u7684\u670d\u52a1\u5668\u7aef\u8ba4\u8bc1\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u65b9\u6848\u76f8\u6bd4\uff0c\u901a\u4fe1\u91cf\u51cf\u5c1197.61-110.13\u500d\uff0cLAN\u73af\u5883\u4e0b\u901f\u5ea6\u63d0\u53472.72-2.82\u500d\uff0cWAN\u73af\u5883\u4e0b\u63d0\u53476.58-8.51\u500d\u3002", "conclusion": "FLAME\u65b9\u6848\u5728\u6076\u610f\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u9690\u79c1\u4fdd\u62a4\u751f\u7269\u8ba4\u8bc1\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.02494", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02494", "abs": "https://arxiv.org/abs/2511.02494", "authors": ["Raul Murillo", "Julio Villalba-Moreno", "Alberto A. Del Barrio", "Guillermo Botella"], "title": "Digit-Recurrence Posit Division", "comment": "11 pages, 9 figures", "summary": "Posit arithmetic has emerged as a promising alternative to IEEE 754\nfloating-point representation, offering enhanced accuracy and dynamic range.\nHowever, division operations in posit systems remain challenging due to their\ninherent hardware complexity. In this work, we present posit division units\nbased on the digit-recurrence algorithm, marking the first implementation of\nradix-4 digit-recurrence techniques within this context. Our approach\nincorporates hardware-centric optimizations including redundant arithmetic,\non-the-fly quotient conversion, and operand scaling to streamline the division\nprocess while mitigating latency, area, and power overheads. Comprehensive\nsynthesis evaluations across multiple posit configurations demonstrate\nsignificant performance improvements, including more than 80% energy reduction\nwith small area overhead compared to existing methods, and a substantial\ndecrease in the number of iterations. These results underscore the potential of\nour adapted algorithm to enhance the efficiency of posit-based arithmetic\nunits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u5b57\u9012\u5f52\u7b97\u6cd5\u7684posit\u9664\u6cd5\u5355\u5143\uff0c\u9996\u6b21\u5728\u8be5\u9886\u57df\u5b9e\u73b0\u4e86radix-4\u6570\u5b57\u9012\u5f52\u6280\u672f\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u8fed\u4ee3\u6b21\u6570\u3002", "motivation": "posit\u7b97\u672f\u4f5c\u4e3aIEEE 754\u6d6e\u70b9\u8868\u793a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u52a8\u6001\u8303\u56f4\uff0c\u4f46\u5176\u9664\u6cd5\u64cd\u4f5c\u7531\u4e8e\u786c\u4ef6\u590d\u6742\u6027\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6570\u5b57\u9012\u5f52\u7b97\u6cd5\u7684posit\u9664\u6cd5\u5355\u5143\uff0c\u7ed3\u5408\u5197\u4f59\u7b97\u672f\u3001\u5728\u7ebf\u5546\u8f6c\u6362\u548c\u64cd\u4f5c\u6570\u7f29\u653e\u7b49\u786c\u4ef6\u4f18\u5316\u6280\u672f\u6765\u7b80\u5316\u9664\u6cd5\u8fc7\u7a0b\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684\u80fd\u8017\u964d\u4f4e\u548c\u8f83\u5c0f\u7684\u9762\u79ef\u5f00\u9500\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8fed\u4ee3\u6b21\u6570\u3002", "conclusion": "\u8be5\u9002\u914d\u7b97\u6cd5\u6709\u6f5c\u529b\u663e\u8457\u63d0\u5347\u57fa\u4e8eposit\u7684\u7b97\u672f\u5355\u5143\u7684\u6548\u7387\u3002"}}
{"id": "2511.01866", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.01866", "abs": "https://arxiv.org/abs/2511.01866", "authors": ["Benjamin Kubwimana", "Qijing Huang"], "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs", "comment": "Published in the Proceedings of the 2025 IEEE International Symposium\n  on Workload Characterization (IISWC 2025)", "summary": "Edge intelligence paradigm is increasingly demanded by the emerging\nautonomous systems, such as robotics. Beyond ensuring privacy-preserving\noperation and resilience in connectivity-limited environments, edge deployment\noffers significant energy and cost advantages over cloud-based solutions.\nHowever, deploying large language models (LLMs) for reasoning tasks on edge\nGPUs faces critical challenges from strict latency constraints and limited\ncomputational resources. To navigate these constraints, developers must balance\nmultiple design factors - choosing reasoning versus non-reasoning\narchitectures, selecting appropriate model sizes, allocating token budgets, and\napplying test-time scaling strategies - to meet target latency and optimize\naccuracy. Yet guidance on optimal combinations of these variables remains\nscarce. In this work, we present EdgeReasoning, a comprehensive study\ncharacterizing the deployment of reasoning LLMs on edge GPUs. We systematically\nquantify latency-accuracy tradeoffs across various LLM architectures and model\nsizes. We systematically evaluate prompt-based and model-tuning-based\ntechniques for reducing reasoning token length while maintaining performance\nquality. We further profile test-time scaling methods with varying degrees of\nparallelism to maximize accuracy under strict latency budgets. Through these\nanalyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency\nconfigurations, offering systematic guidance for optimal edge deployment of\nreasoning LLMs.", "AI": {"tldr": "EdgeReasoning\u662f\u4e00\u4e2a\u9488\u5bf9\u8fb9\u7f18GPU\u90e8\u7f72\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u7814\u7a76\uff0c\u7cfb\u7edf\u91cf\u5316\u4e86\u4e0d\u540c\u67b6\u6784\u548c\u6a21\u578b\u5c3a\u5bf8\u4e0b\u7684\u5ef6\u8fdf-\u51c6\u786e\u7387\u6743\u8861\uff0c\u8bc4\u4f30\u4e86\u51cf\u5c11\u63a8\u7406token\u957f\u5ea6\u7684\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u6700\u4f18\u914d\u7f6e\u6307\u5bfc\u3002", "motivation": "\u8fb9\u7f18\u667a\u80fd\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u9700\u6c42\u589e\u957f\uff0c\u4f46\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u5230\u8fb9\u7f18GPU\u9762\u4e34\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u7684\u6311\u6218\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5728\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u67b6\u6784\u3001\u6a21\u578b\u5c3a\u5bf8\u3001token\u9884\u7b97\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u4e4b\u95f4\u5e73\u8861\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8fd9\u4e9b\u53d8\u91cf\u6700\u4f18\u7ec4\u5408\u7684\u6307\u5bfc\u3002", "method": "\u7cfb\u7edf\u91cf\u5316\u4e0d\u540cLLM\u67b6\u6784\u548c\u6a21\u578b\u5c3a\u5bf8\u7684\u5ef6\u8fdf-\u51c6\u786e\u7387\u6743\u8861\uff1b\u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u548c\u6a21\u578b\u8c03\u4f18\u7684\u51cf\u5c11\u63a8\u7406token\u957f\u5ea6\u6280\u672f\uff1b\u5206\u6790\u4e0d\u540c\u5e76\u884c\u5ea6\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff1b\u7ed8\u5236\u53ef\u5b9e\u73b0\u7684\u51c6\u786e\u7387-\u5ef6\u8fdf\u914d\u7f6e\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\u3002", "result": "\u901a\u8fc7\u5168\u9762\u5206\u6790\uff0cEdgeReasoning\u7ed8\u5236\u4e86\u53ef\u5b9e\u73b0\u7684\u51c6\u786e\u7387-\u5ef6\u8fdf\u914d\u7f6e\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\uff0c\u4e3a\u63a8\u7406\u578bLLM\u7684\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u8fb9\u7f18GPU\u4e0a\u90e8\u7f72\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u914d\u7f6e\u6307\u5bfc\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.02530", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02530", "abs": "https://arxiv.org/abs/2511.02530", "authors": ["Takuto Ando", "Yu Eto", "Yasuhiko Nakashima"], "title": "Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator", "comment": "This paper is accepted at 2025 IEEE 18th International Symposium on\n  Embedded Multicore/Many-core Systems-on-Chip (MCSoC)", "summary": "This paper presents the first implementation and in-depth evaluation of the\nprimary computational kernels from the stable-diffusion.cpp image generation\nframework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array\n(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,\nand this work assesses its capabilities by executing a demanding image\ngeneration workload. We evaluate its performance on a current\nField-Programmable Gate Array (FPGA) prototype to establish a baseline and\nproject its potential for a future Application-Specific Integrated Circuit\n(ASIC) implementation. Our results demonstrate that, despite its\ngeneral-purpose architecture, IMAX3 achieves promising performance and power\nefficiency, particularly in its projected ASIC form. This work provides\nconcrete guidelines for future IMAX architectural designs and establishes a\nfoundation for developing next-generation, AI-specialized Coarse-Grained Linear\nArray (CGLA) accelerators by refining this versatile platform. Ultimately, this\nachievement contributes to the realization of energy-efficient, on-device,\nmulti-modal AI platforms.", "AI": {"tldr": "\u672c\u6587\u5728IMAX3 CGRA\u52a0\u901f\u5668\u4e0a\u9996\u6b21\u5b9e\u73b0\u4e86stable-diffusion.cpp\u56fe\u50cf\u751f\u6210\u6846\u67b6\u7684\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u4e3a\u672a\u6765ASIC\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u8bc4\u4f30\u901a\u7528CGRA\u52a0\u901f\u5668IMAX3\u5728\u6267\u884c\u9ad8\u8981\u6c42\u56fe\u50cf\u751f\u6210\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u7684\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3AI\u4e13\u7528CGLA\u52a0\u901f\u5668\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u5728FPGA\u539f\u578b\u4e0a\u5b9e\u73b0stable-diffusion.cpp\u7684\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u5efa\u7acb\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u9884\u6d4b\u672a\u6765ASIC\u5b9e\u73b0\u7684\u6f5c\u529b\u3002", "result": "\u5c3d\u7ba1\u91c7\u7528\u901a\u7528\u67b6\u6784\uff0cIMAX3\u5728\u6027\u80fd\u548c\u80fd\u6548\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u7684ASIC\u5f62\u5f0f\u4e2d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765IMAX\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\uff0c\u5e76\u4e3a\u5f00\u53d1\u9ad8\u6548\u80fd\u3001\u8bbe\u5907\u7aef\u591a\u6a21\u6001AI\u5e73\u53f0\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2511.01871", "categories": ["cs.DC", "68M15, 93B12"], "pdf": "https://arxiv.org/pdf/2511.01871", "abs": "https://arxiv.org/abs/2511.01871", "authors": ["S. Tsiramua", "H. Meladze", "T. Davitashvili", "J. M. Sanchez", "F. Criado-Aldeanueva"], "title": "Structural Analysis of Multi-Core Processor and Reliability Evaluation Model", "comment": null, "summary": "In the present paper, the models of structural analysis and evaluation of\nefficiency indicators (reliability, fault tolerance, viability, and\nflexibility) of a multi core processor with variable structure, equipped with\nmulti functional cores, are considered. Using logical probabilistic methods,\nthe following has been developed: models for evaluating the reliability and\nfault tolerance of processor cores as multi functional elements; logical\nprobabilistic models of the shortest paths, flexibility, and performance\nconditions for successful operation of multi core processors based on multi\nfunctional cores; and models for estimating the reliability, fault tolerance,\nand lifetime of multi core processors considering all possible states of\nperformance. The results of the structural analysis of two core and four core\nprocessors and the trends of increasing the efficiency indicators of multi core\nprocessors are presented.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u591a\u6838\u5904\u7406\u5668\u7ed3\u6784\u5206\u6790\u548c\u6548\u7387\u6307\u6807\u8bc4\u4f30\u6a21\u578b\uff0c\u5305\u62ec\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u3001\u751f\u5b58\u6027\u548c\u7075\u6d3b\u6027\u8bc4\u4f30\uff0c\u4f7f\u7528\u903b\u8f91\u6982\u7387\u65b9\u6cd5\u5206\u6790\u591a\u6838\u5904\u7406\u5668\u7684\u6027\u80fd\u72b6\u6001\u548c\u6548\u7387\u63d0\u5347\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u591a\u6838\u5904\u7406\u5668\uff08\u7279\u522b\u662f\u5177\u6709\u53ef\u53d8\u7ed3\u6784\u548c\u591a\u529f\u80fd\u6838\u5fc3\u7684\u5904\u7406\u5668\uff09\u7684\u7ed3\u6784\u5206\u6790\u548c\u6548\u7387\u6307\u6807\u8bc4\u4f30\uff0c\u4ee5\u63d0\u5347\u5176\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u6982\u7387\u65b9\u6cd5\u5f00\u53d1\u4e86\u591a\u79cd\u6a21\u578b\uff1a\u8bc4\u4f30\u591a\u529f\u80fd\u6838\u5fc3\u53ef\u9760\u6027\u548c\u5bb9\u9519\u6027\u7684\u6a21\u578b\u3001\u57fa\u4e8e\u591a\u529f\u80fd\u6838\u5fc3\u7684\u591a\u6838\u5904\u7406\u5668\u6700\u77ed\u8def\u5f84\u3001\u7075\u6d3b\u6027\u548c\u6027\u80fd\u6761\u4ef6\u7684\u903b\u8f91\u6982\u7387\u6a21\u578b\u3001\u4ee5\u53ca\u8003\u8651\u6240\u6709\u53ef\u80fd\u6027\u80fd\u72b6\u6001\u7684\u591a\u6838\u5904\u7406\u5668\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u548c\u5bff\u547d\u4f30\u8ba1\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u53cc\u6838\u548c\u56db\u6838\u5904\u7406\u5668\u7684\u7ed3\u6784\u5206\u6790\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6838\u5904\u7406\u5668\u6548\u7387\u6307\u6807\u63d0\u5347\u7684\u8d8b\u52bf\u3002", "conclusion": "\u901a\u8fc7\u903b\u8f91\u6982\u7387\u65b9\u6cd5\u6210\u529f\u5f00\u53d1\u4e86\u591a\u6838\u5904\u7406\u5668\u7ed3\u6784\u5206\u6790\u548c\u6548\u7387\u8bc4\u4f30\u7684\u7efc\u5408\u6a21\u578b\uff0c\u4e3a\u591a\u6838\u5904\u7406\u5668\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2511.02243", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02243", "abs": "https://arxiv.org/abs/2511.02243", "authors": ["Zhuoran Zhang", "Tengyue Wang", "Xilin Gong", "Yang Shi", "Haotian Wang", "Di Wang", "Lijie Hu"], "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs", "comment": "19 pages", "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u6001\u8ddf\u968f\u884c\u4e3a\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u6a21\u6001\u8ddf\u968f\u5206\u89e3\u4e3a\u76f8\u5bf9\u63a8\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u56fa\u6709\u6a21\u6001\u504f\u597d\u4e24\u4e2a\u56e0\u7d20\uff0c\u63ed\u793a\u4e86\u6a21\u6001\u8ddf\u968f\u6982\u7387\u968f\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u5355\u8c03\u4e0b\u964d\u7684\u666e\u904d\u89c4\u5f8b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u7528\u7c97\u7c92\u5ea6\u7684\u6570\u636e\u96c6\u7ea7\u7edf\u8ba1\u6765\u8861\u91cf\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u6001\u8ddf\u968f\u884c\u4e3a\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u5728\u5355\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u5f71\u54cd\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6846\u67b6\u6765\u7406\u89e3\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u6a21\u6001\u95f4\u51b2\u7a81\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u53ef\u63a7\u6570\u636e\u96c6\u7cfb\u7edf\u53d8\u5316\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u7684\u63a8\u7406\u96be\u5ea6\uff0c\u4f7f\u7528\u71b5\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u901a\u8fc7\u5c42\u95f4\u9884\u6d4b\u5206\u6790\u63ed\u793a\u5185\u90e8\u673a\u5236\u3002", "result": "\u53d1\u73b0\u6a21\u6001\u8ddf\u968f\u6982\u7387\u968f\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u5355\u8c03\u4e0b\u964d\u7684\u666e\u904d\u89c4\u5f8b\uff0c\u5728\u5e73\u8861\u70b9\u5904\u6a21\u578b\u5bf9\u4e24\u79cd\u6a21\u6001\u7684\u8ddf\u968f\u6982\u7387\u76f8\u5f53\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6a21\u7cca\u533a\u57df\u8de8\u5c42\u632f\u8361\u7684\u5185\u90e8\u673a\u5236\u3002", "conclusion": "\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u548c\u56fa\u6709\u6a21\u6001\u504f\u597d\u662f\u63a7\u5236\u6a21\u6001\u8ddf\u968f\u7684\u4e24\u4e2a\u57fa\u672c\u539f\u5219\uff0c\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u51b2\u7a81\u4fe1\u606f\u63d0\u4f9b\u4e86\u5b9a\u91cf\u6846\u67b6\u548c\u673a\u5236\u6027\u89c1\u89e3\u3002"}}
{"id": "2511.02303", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02303", "abs": "https://arxiv.org/abs/2511.02303", "authors": ["Zhiwei Zhang", "Xiaomin Li", "Yudi Lin", "Hui Liu", "Ramraj Chandradevan", "Linlin Wu", "Minhua Lin", "Fali Wang", "Xianfeng Tang", "Qi He", "Suhang Wang"], "title": "Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation", "comment": null, "summary": "Large Language Models (LLMs) trained with reinforcement learning and\nverifiable rewards have achieved strong results on complex reasoning tasks.\nRecent work extends this paradigm to a multi-agent setting, where a\nmeta-thinking agent proposes plans and monitors progress while a reasoning\nagent executes subtasks through sequential conversational turns. Despite\npromising performance, we identify a critical limitation: lazy agent behavior,\nin which one agent dominates while the other contributes little, undermining\ncollaboration and collapsing the setup to an ineffective single agent. In this\npaper, we first provide a theoretical analysis showing why lazy behavior\nnaturally arises in multi-agent reasoning. We then introduce a stable and\nefficient method for measuring causal influence, helping mitigate this issue.\nFinally, as collaboration intensifies, the reasoning agent risks getting lost\nin multi-turn interactions and trapped by previous noisy responses. To counter\nthis, we propose a verifiable reward mechanism that encourages deliberation by\nallowing the reasoning agent to discard noisy outputs, consolidate\ninstructions, and restart its reasoning process when necessary. Extensive\nexperiments demonstrate that our framework alleviates lazy agent behavior and\nunlocks the full potential of multi-agent framework for complex reasoning\ntasks.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u61d2\u60f0\u884c\u4e3a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u56e0\u679c\u5f71\u54cd\u6d4b\u91cf\u65b9\u6cd5\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u6765\u6539\u5584\u534f\u4f5c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7cfb\u7edf\u4e2d\u5b58\u5728\u61d2\u60f0\u884c\u4e3a\u95ee\u9898\uff0c\u5373\u4e00\u4e2a\u667a\u80fd\u4f53\u4e3b\u5bfc\u800c\u53e6\u4e00\u4e2a\u8d21\u732e\u5f88\u5c11\uff0c\u5bfc\u81f4\u534f\u4f5c\u5931\u6548\uff0c\u9650\u5236\u4e86\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u9996\u5148\u8fdb\u884c\u7406\u8bba\u5206\u6790\u89e3\u91ca\u61d2\u60f0\u884c\u4e3a\u7684\u6210\u56e0\uff1b\u7136\u540e\u5f15\u5165\u7a33\u5b9a\u9ad8\u6548\u7684\u56e0\u679c\u5f71\u54cd\u6d4b\u91cf\u65b9\u6cd5\uff1b\u6700\u540e\u63d0\u51fa\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u5141\u8bb8\u63a8\u7406\u667a\u80fd\u4f53\u4e22\u5f03\u566a\u58f0\u8f93\u51fa\u3001\u6574\u5408\u6307\u4ee4\u5e76\u5728\u5fc5\u8981\u65f6\u91cd\u542f\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u61d2\u60f0\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u91ca\u653e\u4e86\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5168\u90e8\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u534f\u4f5c\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u591a\u667a\u80fd\u4f53\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02365", "categories": ["cs.CR", "math.QA", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.02365", "abs": "https://arxiv.org/abs/2511.02365", "authors": ["Gautier-Edouard Filardo", "Thibaut Heckmann"], "title": "Enhancing NTRUEncrypt Security Using Markov Chain Monte Carlo Methods: Theory and Practice", "comment": null, "summary": "This paper presents a novel framework for enhancing the quantum resistance of\nNTRUEncrypt using Markov Chain Monte Carlo (MCMC) methods. We establish formal\nbounds on sampling efficiency and provide security reductions to lattice\nproblems, bridging theoretical guarantees with practical implementations. Key\ncontributions include: a new methodology for exploring private key\nvulnerabilities while maintaining quantum resistance, provable mixing time\nbounds for high-dimensional lattices, and concrete metrics linking MCMC\nparameters to lattice hardness assumptions. Numerical experiments validate our\napproach, demonstrating improved security guarantees and computational\nefficiency. These findings advance the theoretical understanding and practical\nadoption of NTRU- Encrypt in the post-quantum era.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u589e\u5f3aNTRUEncrypt\u91cf\u5b50\u62b5\u6297\u6027\u7684\u65b0\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u91c7\u6837\u6548\u7387\u7684\u5f62\u5f0f\u754c\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u5230\u683c\u95ee\u9898\u7684\u5b89\u5168\u5f52\u7ea6\u3002", "motivation": "\u5728\u91cf\u5b50\u8ba1\u7b97\u65f6\u4ee3\uff0c\u589e\u5f3aNTRUEncrypt\u7684\u91cf\u5b50\u62b5\u6297\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u63a2\u7d22\u79c1\u94a5\u6f0f\u6d1e\uff0c\u5efa\u7acb\u53ef\u8bc1\u660e\u7684\u6df7\u5408\u65f6\u95f4\u754c\u9650\uff0c\u5e76\u5c06MCMC\u53c2\u6570\u4e0e\u683c\u786c\u5ea6\u5047\u8bbe\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u5b89\u5168\u4fdd\u8bc1\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63a8\u8fdb\u4e86NTRUEncrypt\u5728\u540e\u91cf\u5b50\u65f6\u4ee3\u7684\u7406\u8bba\u7406\u89e3\u548c\u5b9e\u9645\u91c7\u7528\u3002"}}
{"id": "2511.02340", "categories": ["cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2511.02340", "abs": "https://arxiv.org/abs/2511.02340", "authors": ["Yohan Lee", "DongGyun Kang", "SeHoon Park", "Sa-Yoon Park", "Kwangsoo Kim"], "title": "Chronic Kidney Disease Prognosis Prediction Using Transformer", "comment": "5 pages, 2 figures, 2 tables", "summary": "Chronic Kidney Disease (CKD) affects nearly 10\\% of the global population and\noften progresses to end-stage renal failure. Accurate prognosis prediction is\nvital for timely interventions and resource optimization. We present a\ntransformer-based framework for predicting CKD progression using multi-modal\nelectronic health records (EHR) from the Seoul National University Hospital\nOMOP Common Data Model. Our approach (\\textbf{ProQ-BERT}) integrates\ndemographic, clinical, and laboratory data, employing quantization-based\ntokenization for continuous lab values and attention mechanisms for\ninterpretability. The model was pretrained with masked language modeling and\nfine-tuned for binary classification tasks predicting progression from stage 3a\nto stage 5 across varying follow-up and assessment periods. Evaluated on a\ncohort of 91,816 patients, our model consistently outperformed CEHR-BERT,\nachieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.\nThese results highlight the effectiveness of transformer architectures and\ntemporal design choices in clinical prognosis modeling, offering a promising\ndirection for personalized CKD care.", "AI": {"tldr": "\u63d0\u51faProQ-BERT\u6846\u67b6\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u57fa\u4e8e\u591a\u6a21\u6001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u9884\u6d4b\u6162\u6027\u80be\u75c5\u8fdb\u5c55\uff0c\u572891,816\u60a3\u8005\u961f\u5217\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cROC-AUC\u8fbe0.995\u3002", "motivation": "\u6162\u6027\u80be\u75c5\u5f71\u54cd\u5168\u7403\u8fd110%\u4eba\u53e3\uff0c\u51c6\u786e\u9884\u6d4b\u75be\u75c5\u8fdb\u5c55\u5bf9\u53ca\u65f6\u5e72\u9884\u548c\u8d44\u6e90\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u6574\u5408\u4eba\u53e3\u7edf\u8ba1\u3001\u4e34\u5e8a\u548c\u5b9e\u9a8c\u5ba4\u6570\u636e\uff0c\u91c7\u7528\u57fa\u4e8e\u91cf\u5316\u7684\u6807\u8bb0\u5316\u5904\u7406\u8fde\u7eed\u5b9e\u9a8c\u5ba4\u503c\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u9884\u8bad\u7ec3\u5e76\u9488\u5bf9\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u5fae\u8c03\u3002", "result": "\u572891,816\u60a3\u8005\u961f\u5217\u8bc4\u4f30\u4e2d\uff0c\u6a21\u578b\u6301\u7eed\u4f18\u4e8eCEHR-BERT\uff0c\u77ed\u671f\u9884\u6d4bROC-AUC\u8fbe0.995\uff0cPR-AUC\u8fbe0.989\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86Transformer\u67b6\u6784\u548c\u65f6\u95f4\u8bbe\u8ba1\u9009\u62e9\u5728\u4e34\u5e8a\u9884\u540e\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e2a\u6027\u5316\u6162\u6027\u80be\u75c5\u62a4\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2511.02600", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02600", "abs": "https://arxiv.org/abs/2511.02600", "authors": ["Patrick Karlsen", "Even Eilertsen"], "title": "On The Dangers of Poisoned LLMs In Security Automation", "comment": "5 pages, 1 figure", "summary": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the\nintentional or unintentional introduction of malicious or biased data during\nmodel training. We demonstrate how a seemingly improved LLM, fine-tuned on a\nlimited dataset, can introduce significant bias, to the extent that a simple\nLLM-based alert investigator is completely bypassed when the prompt utilizes\nthe introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we\ndemonstrate how a targeted poisoning attack can bias the model to consistently\ndismiss true positive alerts originating from a specific user. Additionally, we\npropose some mitigation and best-practices to increase trustworthiness,\nrobustness and reduce risk in applied LLMs in security applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u6295\u6bd2\u98ce\u9669\uff0c\u5c55\u793a\u6709\u9650\u6570\u636e\u96c6\u5fae\u8c03\u53ef\u80fd\u5f15\u5165\u663e\u8457\u504f\u89c1\uff0c\u5bfc\u81f4\u57fa\u4e8eLLM\u7684\u8b66\u62a5\u8c03\u67e5\u7cfb\u7edf\u88ab\u7ed5\u8fc7\uff0c\u5e76\u63d0\u51fa\u7f13\u89e3\u63aa\u65bd\u3002", "motivation": "\u7814\u7a76LLM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6076\u610f\u6216\u504f\u89c1\u6570\u636e\u5f15\u5165\u7684\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5e94\u7528\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u4e25\u91cd\u540e\u679c\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684Llama3.1 8B\u548cQwen3 4B\u6a21\u578b\uff0c\u6f14\u793a\u9488\u5bf9\u6027\u6295\u6bd2\u653b\u51fb\u5982\u4f55\u4f7f\u6a21\u578b\u6301\u7eed\u5ffd\u7565\u7279\u5b9a\u7528\u6237\u7684\u771f\u5b9e\u9633\u6027\u8b66\u62a5\u3002", "result": "\u53d1\u73b0\u770b\u4f3c\u6539\u8fdb\u7684LLM\u5728\u6709\u9650\u6570\u636e\u96c6\u5fae\u8c03\u540e\u53ef\u80fd\u5f15\u5165\u663e\u8457\u504f\u89c1\uff0c\u5bfc\u81f4\u57fa\u4e8eLLM\u7684\u8b66\u62a5\u8c03\u67e5\u7cfb\u7edf\u88ab\u5b8c\u5168\u7ed5\u8fc7\u3002", "conclusion": "\u63d0\u51fa\u7f13\u89e3\u63aa\u65bd\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u5e94\u7528\u4e2dLLM\u7684\u53ef\u4fe1\u5ea6\u3001\u9c81\u68d2\u6027\u5e76\u964d\u4f4e\u98ce\u9669\u3002"}}
{"id": "2511.01888", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01888", "abs": "https://arxiv.org/abs/2511.01888", "authors": ["Cynthia Marcelino", "Thomas Pusztai", "Stefan Nastic"], "title": "Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions", "comment": "26th International Middleware Conference (MIDDLEWARE 25)", "summary": "Serverless computing provides infrastructure management and elastic\nauto-scaling, therefore reducing operational overhead. By design serverless\nfunctions are stateless, which means they typically leverage external remote\nservices to store and exchange data. Transferring data over a network typically\ninvolves serialization and deserialization. These operations usually require\nmultiple data copies and transitions between user and kernel space, resulting\nin overhead from context switching and memory allocation, contributing\nsignificantly to increased latency and resource consumption. To address these\nissues, we present Roadrunner, a sidecar shim that enables near-zero copy and\nserialization-free data transfer between WebAssembly-based serverless\nfunctions. Roadrunner reduces the multiple copies between user space and kernel\nspace by mapping the function memory and moving the data along a dedicated\nvirtual data hose, bypassing the costly processes of serialization and\ndeserialization. This approach reduces data movement overhead and context\nswitching, achieving near-native latency performance for WebAssembly-based\nserverless functions. Our experimental results demonstrate that Roadrunner\nsignificantly improves the inter-function communication latency from 44% up to\n89%, reducing the serialization overhead in 97% of data transfer, and\nincreasing throughput by 69 times compared to state-of-the-art\nWebAssembly-based serverless functions.", "AI": {"tldr": "Roadrunner\u662f\u4e00\u4e2a\u8fb9\u8f66shim\uff0c\u901a\u8fc7\u8fd1\u4e4e\u96f6\u62f7\u8d1d\u548c\u65e0\u5e8f\u5217\u5316\u7684\u6570\u636e\u4f20\u8f93\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u57fa\u4e8eWebAssembly\u7684\u65e0\u670d\u52a1\u5668\u51fd\u6570\u95f4\u901a\u4fe1\u6027\u80fd\uff0c\u51cf\u5c11\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\u51fd\u6570\u95f4\u6570\u636e\u4f20\u8f93\u901a\u5e38\u6d89\u53ca\u5e8f\u5217\u5316/\u53cd\u5e8f\u5217\u5316\u64cd\u4f5c\uff0c\u5bfc\u81f4\u591a\u6b21\u6570\u636e\u62f7\u8d1d\u548c\u7528\u6237/\u5185\u6838\u7a7a\u95f4\u5207\u6362\uff0c\u589e\u52a0\u4e86\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u901a\u8fc7\u6620\u5c04\u51fd\u6570\u5185\u5b58\u5e76\u6cbf\u4e13\u7528\u865a\u62df\u6570\u636e\u7ba1\u9053\u79fb\u52a8\u6570\u636e\uff0c\u7ed5\u8fc7\u5e8f\u5217\u5316\u548c\u53cd\u5e8f\u5217\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u62f7\u8d1d\u7684\u6570\u636e\u4f20\u8f93\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRoadrunner\u5c06\u51fd\u6570\u95f4\u901a\u4fe1\u5ef6\u8fdf\u964d\u4f4e\u4e8644%\u81f389%\uff0c\u51cf\u5c11\u4e8697%\u7684\u6570\u636e\u4f20\u8f93\u5e8f\u5217\u5316\u5f00\u9500\uff0c\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6848\u63d0\u9ad8\u4e8669\u500d\u3002", "conclusion": "Roadrunner\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u4f20\u8f93\u673a\u5236\uff0c\u4e3a\u57fa\u4e8eWebAssembly\u7684\u65e0\u670d\u52a1\u5668\u51fd\u6570\u5b9e\u73b0\u4e86\u63a5\u8fd1\u539f\u751f\u6027\u80fd\u7684\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2511.02392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02392", "abs": "https://arxiv.org/abs/2511.02392", "authors": ["Muhammad Sheharyar Liaqat"], "title": "Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients", "comment": null, "summary": "Breast cancer remains one of the leading causes of mortality among women\nworldwide, with early diagnosis being critical for effective treatment and\nimproved survival rates. However, timely detection continues to be a challenge\ndue to the complex nature of the disease and variability in patient risk\nfactors. This study presents a fuzzy soft set theory-based expert system\ndesigned to assess the risk of breast cancer in patients using measurable\nclinical and physiological parameters. The proposed system integrates Body Mass\nIndex, Insulin Level, Leptin Level, Adiponectin Level, and age as input\nvariables to estimate breast cancer risk through a set of fuzzy inference rules\nand soft set computations. These parameters can be obtained from routine blood\nanalyses, enabling a non-invasive and accessible method for preliminary\nassessment. The dataset used for model development and validation was obtained\nfrom the UCI Machine Learning Repository. The proposed expert system aims to\nsupport healthcare professionals in identifying high-risk patients and\ndetermining the necessity of further diagnostic procedures such as biopsies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u8f6f\u96c6\u7406\u8bba\u7684\u4e13\u5bb6\u7cfb\u7edf\uff0c\u7528\u4e8e\u901a\u8fc7\u53ef\u6d4b\u91cf\u7684\u4e34\u5e8a\u548c\u751f\u7406\u53c2\u6570\u8bc4\u4f30\u4e73\u817a\u764c\u98ce\u9669\u3002\u8be5\u7cfb\u7edf\u6574\u5408BMI\u3001\u80f0\u5c9b\u7d20\u6c34\u5e73\u3001\u7626\u7d20\u6c34\u5e73\u3001\u8102\u8054\u7d20\u6c34\u5e73\u548c\u5e74\u9f84\u4f5c\u4e3a\u8f93\u5165\u53d8\u91cf\uff0c\u901a\u8fc7\u6a21\u7cca\u63a8\u7406\u89c4\u5219\u548c\u8f6f\u96c6\u8ba1\u7b97\u6765\u4f30\u8ba1\u4e73\u817a\u764c\u98ce\u9669\u3002", "motivation": "\u4e73\u817a\u764c\u662f\u5168\u7403\u5973\u6027\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u65e9\u671f\u8bca\u65ad\u5bf9\u6709\u6548\u6cbb\u7597\u548c\u63d0\u9ad8\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002\u4f46\u7531\u4e8e\u75be\u75c5\u590d\u6742\u6027\u548c\u60a3\u8005\u98ce\u9669\u56e0\u7d20\u7684\u53d8\u5f02\u6027\uff0c\u53ca\u65f6\u68c0\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u6a21\u7cca\u8f6f\u96c6\u7406\u8bba\u7684\u4e13\u5bb6\u7cfb\u7edf\uff0c\u4f7f\u7528BMI\u3001\u80f0\u5c9b\u7d20\u6c34\u5e73\u3001\u7626\u7d20\u6c34\u5e73\u3001\u8102\u8054\u7d20\u6c34\u5e73\u548c\u5e74\u9f84\u4f5c\u4e3a\u8f93\u5165\u53d8\u91cf\uff0c\u901a\u8fc7\u6a21\u7cca\u63a8\u7406\u89c4\u5219\u548c\u8f6f\u96c6\u8ba1\u7b97\u8fdb\u884c\u98ce\u9669\u8bc4\u4f30\u3002\u6570\u636e\u96c6\u6765\u81eaUCI\u673a\u5668\u5b66\u4e60\u5e93\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u5e38\u89c4\u8840\u6db2\u5206\u6790\u83b7\u5f97\u53c2\u6570\uff0c\u63d0\u4f9b\u975e\u4fb5\u5165\u6027\u548c\u53ef\u8bbf\u95ee\u7684\u521d\u6b65\u8bc4\u4f30\u65b9\u6cd5\uff0c\u652f\u6301\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u8bc6\u522b\u9ad8\u98ce\u9669\u60a3\u8005\u3002", "conclusion": "\u8be5\u4e13\u5bb6\u7cfb\u7edf\u65e8\u5728\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u8bc6\u522b\u9ad8\u98ce\u9669\u60a3\u8005\uff0c\u5e76\u786e\u5b9a\u662f\u5426\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u8bca\u65ad\u7a0b\u5e8f\uff08\u5982\u6d3b\u68c0\uff09\uff0c\u4e3a\u4e73\u817a\u764c\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2511.02620", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02620", "abs": "https://arxiv.org/abs/2511.02620", "authors": ["Roy Rinberg", "Adam Karvonen", "Alex Hoover", "Daniel Reuter", "Keri Warr"], "title": "Verifying LLM Inference to Prevent Model Weight Exfiltration", "comment": null, "summary": "As large AI models become increasingly valuable assets, the risk of model\nweight exfiltration from inference servers grows accordingly. An attacker\ncontrolling an inference server may exfiltrate model weights by hiding them\nwithin ordinary model outputs, a strategy known as steganography. This work\ninvestigates how to verify model responses to defend against such attacks and,\nmore broadly, to detect anomalous or buggy behavior during inference. We\nformalize model exfiltration as a security game, propose a verification\nframework that can provably mitigate steganographic exfiltration, and specify\nthe trust assumptions associated with our scheme. To enable verification, we\ncharacterize valid sources of non-determinism in large language model inference\nand introduce two practical estimators for them. We evaluate our detection\nframework on several open-weight models ranging from 3B to 30B parameters. On\nMOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with\nfalse-positive rate of 0.01%, corresponding to a >200x slowdown for\nadversaries. Overall, this work further establishes a foundation for defending\nagainst model weight exfiltration and demonstrates that strong protection can\nbe achieved with minimal additional cost to inference providers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u6a21\u578b\u6743\u91cd\u6cc4\u9732\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\u6765\u9632\u6b62\u653b\u51fb\u8005\u901a\u8fc7\u9690\u5199\u672f\u4ece\u63a8\u7406\u670d\u52a1\u5668\u4e2d\u7a83\u53d6\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u968f\u7740\u5927\u578bAI\u6a21\u578b\u6210\u4e3a\u91cd\u8981\u8d44\u4ea7\uff0c\u4ece\u63a8\u7406\u670d\u52a1\u5668\u7a83\u53d6\u6a21\u578b\u6743\u91cd\u7684\u98ce\u9669\u589e\u52a0\u3002\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u9690\u85cf\u6743\u91cd\u5728\u666e\u901a\u6a21\u578b\u8f93\u51fa\u4e2d\u8fdb\u884c\u9690\u5199\u5f0f\u6cc4\u9732\u3002", "method": "\u5c06\u6a21\u578b\u6cc4\u9732\u5f62\u5f0f\u5316\u4e3a\u5b89\u5168\u6e38\u620f\uff0c\u63d0\u51fa\u53ef\u8bc1\u660e\u7f13\u89e3\u9690\u5199\u6cc4\u9732\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u5e76\u6307\u5b9a\u4fe1\u4efb\u5047\u8bbe\u3002\u901a\u8fc7\u8868\u5f81\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u6709\u6548\u975e\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5f15\u5165\u4e24\u4e2a\u5b9e\u7528\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u591a\u4e2a3B\u523030B\u53c2\u6570\u7684\u5f00\u653e\u6743\u91cd\u6a21\u578b\u4e0a\u8bc4\u4f30\u68c0\u6d4b\u6846\u67b6\u3002\u5728MOE-Qwen-30B\u4e0a\uff0c\u68c0\u6d4b\u5668\u5c06\u53ef\u6cc4\u9732\u4fe1\u606f\u51cf\u5c11\u5230<0.5%\uff0c\u8bef\u62a5\u7387\u4e3a0.01%\uff0c\u76f8\u5f53\u4e8e\u4f7f\u653b\u51fb\u8005\u901f\u5ea6\u964d\u4f4e200\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u9632\u5fa1\u6a21\u578b\u6743\u91cd\u6cc4\u9732\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u53ef\u4ee5\u901a\u8fc7\u6700\u5c0f\u7684\u989d\u5916\u6210\u672c\u5b9e\u73b0\u5f3a\u5927\u7684\u4fdd\u62a4\u3002"}}
{"id": "2511.01893", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01893", "abs": "https://arxiv.org/abs/2511.01893", "authors": ["Bin Ma", "Viktor Nikitin", "Xi Wang", "Tekin Bicer", "Dong Li"], "title": "mLR: Scalable Laminography Reconstruction based on Memoization", "comment": null, "summary": "ADMM-FFT is an iterative method with high reconstruction accuracy for\nlaminography but suffers from excessive computation time and large memory\nconsumption. We introduce mLR, which employs memoization to replace the\ntime-consuming Fast Fourier Transform (FFT) operations based on an unique\nobservation that similar FFT operations appear in iterations of ADMM-FFT. We\nintroduce a series of techniques to make the application of memoization to\nADMM-FFT performance-beneficial and scalable. We also introduce variable\noffloading to save CPU memory and scale ADMM-FFT across GPUs within and across\nnodes. Using mLR, we are able to scale ADMM-FFT on an input problem of\n2Kx2Kx2K, which is the largest input problem laminography reconstruction has\never worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%\nperformance improvement on average (up to 65.4%), compared to the original\nADMM-FFT.", "AI": {"tldr": "mLRI\u901a\u8fc7\u8bb0\u5fc6\u5316\u6280\u672f\u4f18\u5316ADMM-FFT\u7b97\u6cd5\uff0c\u7528\u7f13\u5b58\u66ff\u4ee3\u91cd\u590d\u7684FFT\u8ba1\u7b97\uff0c\u5f15\u5165\u53d8\u91cf\u5378\u8f7d\u6280\u672f\u8282\u7701CPU\u5185\u5b58\uff0c\u5b9e\u73b0\u8de8GPU\u6269\u5c55\uff0c\u57282Kx2Kx2K\u89c4\u6a21\u95ee\u9898\u4e0a\u5e73\u5747\u63d0\u534752.8%\u6027\u80fd\u3002", "motivation": "ADMM-FFT\u7b97\u6cd5\u5728\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u4e2d\u7cbe\u5ea6\u9ad8\u4f46\u8ba1\u7b97\u65f6\u95f4\u957f\u3001\u5185\u5b58\u6d88\u8017\u5927\uff0c\u9700\u8981\u4f18\u5316\u5176\u6027\u80fd\u74f6\u9888\u3002", "method": "\u5229\u7528\u8bb0\u5fc6\u5316\u6280\u672f\u7f13\u5b58\u91cd\u590d\u7684FFT\u64cd\u4f5c\uff0c\u5f15\u5165\u53d8\u91cf\u5378\u8f7d\u6280\u672f\u8282\u7701\u5185\u5b58\uff0c\u5b9e\u73b0\u8de8GPU\u6269\u5c55\u3002", "result": "\u57282Kx2Kx2K\u89c4\u6a21\u95ee\u9898\u4e0a\uff0cmLRI\u76f8\u6bd4\u539f\u59cbADMM-FFT\u5e73\u5747\u63d0\u534752.8%\u6027\u80fd\uff08\u6700\u9ad865.4%\uff09\uff0c\u5e76\u6210\u529f\u6269\u5c55\u5230\u6700\u5927\u8f93\u5165\u95ee\u9898\u89c4\u6a21\u3002", "conclusion": "mLRI\u901a\u8fc7\u8bb0\u5fc6\u5316\u548c\u53d8\u91cf\u5378\u8f7d\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86ADMM-FFT\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.02414", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02414", "abs": "https://arxiv.org/abs/2511.02414", "authors": ["Benjamin Sykes", "Lo\u00efc Simon", "Julien Rabin", "Jalal Fadili"], "title": "A New Perspective on Precision and Recall for Generative Models", "comment": null, "summary": "With the recent success of generative models in image and text, the question\nof their evaluation has recently gained a lot of attention. While most methods\nfrom the state of the art rely on scalar metrics, the introduction of Precision\nand Recall (PR) for generative model has opened up a new avenue of research.\nThe associated PR curve allows for a richer analysis, but their estimation\nposes several challenges. In this paper, we present a new framework for\nestimating entire PR curves based on a binary classification standpoint. We\nconduct a thorough statistical analysis of the proposed estimates. As a\nbyproduct, we obtain a minimax upper bound on the PR estimation risk. We also\nshow that our framework extends several landmark PR metrics of the literature\nwhich by design are restrained to the extreme values of the curve. Finally, we\nstudy the different behaviors of the curves obtained experimentally in various\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e8c\u5143\u5206\u7c7b\u89c6\u89d2\u7684\u751f\u6210\u6a21\u578b\u7cbe\u786e\u7387-\u53ec\u56de\u7387(PR)\u66f2\u7ebf\u4f30\u8ba1\u65b0\u6846\u67b6\uff0c\u8fdb\u884c\u4e86\u7edf\u8ba1\u5206\u6790\u548c\u98ce\u9669\u4e0a\u754c\u63a8\u5bfc\uff0c\u6269\u5c55\u4e86\u73b0\u6709PR\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u66f2\u7ebf\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u548c\u6587\u672c\u9886\u57df\u7684\u6210\u529f\uff0c\u5176\u8bc4\u4f30\u65b9\u6cd5\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u6807\u91cf\u6307\u6807\uff0c\u4f46PR\u66f2\u7ebf\u4e3a\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u5206\u6790\u7ef4\u5ea6\uff0c\u7136\u800c\u5176\u4f30\u8ba1\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u4e8c\u5143\u5206\u7c7b\u89c6\u89d2\u6784\u5efaPR\u66f2\u7ebf\u4f30\u8ba1\u6846\u67b6\uff0c\u8fdb\u884c\u7edf\u8ba1\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u6700\u5c0f\u6700\u5927\u98ce\u9669\u4e0a\u754c\uff0c\u5e76\u6269\u5c55\u73b0\u6709\u4ec5\u5173\u6ce8\u66f2\u7ebf\u6781\u503c\u7684PR\u6307\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684PR\u66f2\u7ebf\u4f30\u8ba1\u6846\u67b6\uff0c\u83b7\u5f97\u4e86PR\u4f30\u8ba1\u98ce\u9669\u7684\u6700\u5c0f\u6700\u5927\u4e0a\u754c\uff0c\u6210\u529f\u6269\u5c55\u4e86\u6587\u732e\u4e2d\u7684\u591a\u4e2a\u91cd\u8981PR\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u66f2\u7ebf\u884c\u4e3a\u7684\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u6210\u6a21\u578b\u7684PR\u66f2\u7ebf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u6807\u91cf\u6307\u6807\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.02034", "categories": ["cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02034", "abs": "https://arxiv.org/abs/2511.02034", "authors": ["Shashank Motepalli", "Naman Garg", "Gengrui Zhang", "Hans-Arno Jacobsen"], "title": "GPoS: Geospatially-aware Proof of Stake", "comment": "Published in ACM TWEB", "summary": "Geospatial decentralization is essential for blockchains, ensuring regulatory\nresilience, robustness, and fairness. We empirically analyze five major Proof\nof Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,\nrevealing that a few geographic regions dominate consensus voting power,\nresulting in limited geospatial decentralization. To address this, we propose\nGeospatially aware Proof of Stake (GPoS), which integrates geospatial diversity\nwith stake-based voting power. Experimental evaluation demonstrates an average\n45% improvement in geospatial decentralization, as measured by the Gini\ncoefficient of Eigenvector centrality, while incurring minimal performance\noverhead in BFT protocols, including HotStuff and CometBFT. These results\ndemonstrate that GPoS can improve geospatial decentralization {while, in our\nexperiments, incurring minimal overhead} to consensus performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u4e94\u4e2a\u4e3b\u8981PoS\u533a\u5757\u94fe\u7684\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u5c11\u6570\u5730\u533a\u4e3b\u5bfc\u5171\u8bc6\u6295\u7968\u6743\uff0c\u5e76\u63d0\u51faGPoS\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u5730\u7406\u7a7a\u95f4\u591a\u6837\u6027\u4e0e\u6743\u76ca\u6295\u7968\u76f8\u7ed3\u5408\uff0c\u5b9e\u9a8c\u663e\u793a\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\u5e73\u5747\u63d0\u534745%\uff0c\u4e14\u5bf9BFT\u534f\u8bae\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "motivation": "\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\u5bf9\u533a\u5757\u94fe\u7684\u76d1\u7ba1\u5f39\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709PoS\u533a\u5757\u94fe\u5b58\u5728\u5c11\u6570\u5730\u7406\u533a\u57df\u4e3b\u5bfc\u5171\u8bc6\u6295\u7968\u6743\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5730\u7406\u7a7a\u95f4\u611f\u77e5\u6743\u76ca\u8bc1\u660e\uff08GPoS\uff09\uff0c\u5c06\u5730\u7406\u7a7a\u95f4\u591a\u6837\u6027\u4e0e\u57fa\u4e8e\u6743\u76ca\u7684\u6295\u7968\u6743\u76f8\u7ed3\u5408\uff0c\u5e76\u5728HotStuff\u548cCometBFT\u7b49BFT\u534f\u8bae\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u901a\u8fc7\u7279\u5f81\u5411\u91cf\u4e2d\u5fc3\u6027\u7684\u57fa\u5c3c\u7cfb\u6570\u8861\u91cf\uff0c\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\u5e73\u5747\u63d0\u5347\u4e8645%\uff0c\u540c\u65f6\u5bf9\u5171\u8bc6\u6027\u80fd\u7684\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "GPoS\u80fd\u591f\u6709\u6548\u6539\u5584\u533a\u5757\u94fe\u7684\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\uff0c\u4e14\u5728\u5b9e\u8df5\u4e2d\u4ec5\u5e26\u6765\u6700\u5c0f\u7684\u6027\u80fd\u5f00\u9500\u3002"}}
{"id": "2511.02168", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02168", "abs": "https://arxiv.org/abs/2511.02168", "authors": ["Octavian Alexandru Trifan", "Karthik Sangaiah", "Muhammad Awad", "Muhammad Osama", "Sumanth Gudaparthi", "Alexandru Nicolau", "Alexander Veidenbaum", "Ganesh Dasika"], "title": "Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs", "comment": null, "summary": "As large language models (LLMs) continue to scale, their workloads\nincreasingly rely on distributed execution across multiple GPUs. However, the\nconventional bulk synchronous parallel~(BSP) model used in such settings\nintroduces significant performance inefficiencies. To characterize these\nbottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel\nData Locality, and Kernel Launch Overhead) as an analytical framework. We\npropose moving beyond the rigid BSP model to address key inefficiencies in\ndistributed GPU execution. By exploiting libraries like Iris for Triton, we\ngain access to in-kernel communication primitives that enable the design of\nnovel fine-grained programming patterns, offering greater flexibility and\nperformance than traditional BSP-based approaches. These patterns\nsystematically eliminate the three taxes by creating direct, tile-level\nproducer-consumer pipelines and replacing global barriers with fine-grained\ndataflow synchronization. Applying this methodology to critical kernels, from\nthe foundational All-Gather + general matrix multiplication operation to the\ncomplex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end\nlatency over BSP-based approaches, establishing a more programmable and\nefficient paradigm for distributed LLM workloads.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8d85\u8d8a\u4f20\u7edfBSP\u6a21\u578b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7f16\u7a0b\u6a21\u5f0f\u6d88\u9664\u5206\u5e03\u5f0fGPU\u6267\u884c\u4e2d\u7684\u4e09\u5927\u6027\u80fd\u7a0e\uff08\u6279\u91cf\u540c\u6b65\u3001\u5185\u6838\u95f4\u6570\u636e\u5c40\u90e8\u6027\u3001\u5185\u6838\u542f\u52a8\u5f00\u9500\uff09\uff0c\u5728\u5173\u952e\u5185\u6838\u4e0a\u5b9e\u73b010-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u591aGPU\u5206\u5e03\u5f0f\u6267\u884c\u4e2d\u7684\u4f20\u7edfBSP\u6a21\u578b\u5b58\u5728\u663e\u8457\u6027\u80fd\u4f4e\u6548\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u6267\u884c\u8303\u5f0f\u6765\u89e3\u51b3\u74f6\u9888\u3002", "method": "\u5229\u7528Iris for Triton\u7b49\u5e93\u8bbf\u95ee\u5185\u6838\u5185\u901a\u4fe1\u539f\u8bed\uff0c\u8bbe\u8ba1\u65b0\u9896\u7684\u7ec6\u7c92\u5ea6\u7f16\u7a0b\u6a21\u5f0f\uff0c\u521b\u5efa\u76f4\u63a5\u7684\u74e6\u7247\u7ea7\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u7ba1\u9053\uff0c\u7528\u7ec6\u7c92\u5ea6\u6570\u636e\u6d41\u540c\u6b65\u66ff\u4ee3\u5168\u5c40\u5c4f\u969c\u3002", "result": "\u5728\u5173\u952e\u5185\u6838\uff08\u4eceAll-Gather + \u901a\u7528\u77e9\u9635\u4e58\u6cd5\u5230\u590d\u6742Flash Decode\u7b97\u6cd5\uff09\u4e0a\uff0c\u76f8\u6bd4\u57fa\u4e8eBSP\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e8610-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u66f4\u53ef\u7f16\u7a0b\u548c\u9ad8\u6548\u7684\u5206\u5e03\u5f0fLLM\u5de5\u4f5c\u8d1f\u8f7d\u8303\u5f0f\uff0c\u7cfb\u7edf\u6027\u5730\u6d88\u9664\u4e86\u4e09\u5927\u6027\u80fd\u7a0e\u3002"}}
{"id": "2511.02463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02463", "abs": "https://arxiv.org/abs/2511.02463", "authors": ["Mengyu Zhang", "Xubo Liu", "Siyu Ding", "Weichong Yin", "Yu Sun", "Hua Wu", "Wenya Guo", "Ying Zhang"], "title": "Auditable-choice reframing unlocks RL-based verification for open-ended tasks", "comment": "9 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great\npotential in enhancing the reasoning capabilities of large language models\n(LLMs), achieving remarkable progress in domains such as mathematics and\nprogramming where standard answers are available. However, for open-ended tasks\nlacking ground-truth solutions (e.g., creative writing and instruction\nfollowing), existing studies typically regard them as non-reasoning scenarios,\nthereby overlooking the latent value of reasoning capabilities. This raises a\nkey question: Can strengthening reasoning improve performance in open-ended\ntasks? To address this, we explore the transfer of the RLVR paradigm to the\nopen domain. Yet, since RLVR fundamentally relies on verifiers that presuppose\nthe existence of standard answers, it cannot be directly applied to open-ended\ntasks. To overcome this challenge, we introduce Verifiable Multiple-Choice\nReformulation (VMR), a novel training strategy that restructures open-ended\ndata into verifiable multiple-choice formats, enabling effective training even\nin the absence of explicit ground truth. Experimental results on multiple\nbenchmarks validate the effectiveness of our method in improving LLM\nperformance on open-ended tasks. Notably, across eight open-ended benchmarks,\nour VMR-based training delivers an average gain of 5.99 points over the\nbaseline. Code will be released upon acceptance to facilitate reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9a8c\u8bc1\u591a\u9009\u62e9\u91cd\u6784\uff08VMR\uff09\u65b9\u6cd5\uff0c\u5c06\u5f00\u653e\u57df\u4efb\u52a1\u8f6c\u6362\u4e3a\u53ef\u9a8c\u8bc1\u7684\u591a\u9009\u62e9\u683c\u5f0f\uff0c\u4ece\u800c\u5728\u7f3a\u4e4f\u6807\u51c6\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8303\u5f0f\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u867d\u7136RLVR\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u6709\u6807\u51c6\u7b54\u6848\u7684\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7f3a\u4e4f\u6807\u51c6\u7b54\u6848\u7684\u5f00\u653e\u57df\u4efb\u52a1\uff08\u5982\u521b\u610f\u5199\u4f5c\u548c\u6307\u4ee4\u9075\u5faa\uff09\u4e2d\uff0c\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u975e\u63a8\u7406\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u80fd\u529b\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06RLVR\u8303\u5f0f\u8fc1\u79fb\u5230\u5f00\u653e\u57df\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u53ef\u9a8c\u8bc1\u591a\u9009\u62e9\u91cd\u6784\uff08VMR\uff09\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u5f00\u653e\u57df\u6570\u636e\u91cd\u65b0\u6784\u5efa\u4e3a\u53ef\u9a8c\u8bc1\u7684\u591a\u9009\u62e9\u683c\u5f0f\uff0c\u4ece\u800c\u5728\u7f3a\u4e4f\u660e\u786e\u6807\u51c6\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u57df\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u5728\u516b\u4e2a\u5f00\u653e\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eVMR\u7684\u8bad\u7ec3\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u63d0\u5347\u4e865.99\u5206\u3002", "conclusion": "VMR\u65b9\u6cd5\u6210\u529f\u5730\u5c06RLVR\u8303\u5f0f\u6269\u5c55\u5230\u5f00\u653e\u57df\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u7f3a\u4e4f\u6807\u51c6\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u6570\u636e\u91cd\u6784\u4e5f\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u6539\u5584\u5f00\u653e\u57df\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.02248", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02248", "abs": "https://arxiv.org/abs/2511.02248", "authors": ["Xingqi Cui", "Chieh-Jan Mike Liang", "Jiarong Xing", "Haoran Qiu"], "title": "From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models", "comment": "16 pages, 13 figures", "summary": "Serving large generative models such as LLMs and multi- modal transformers\nrequires balancing user-facing SLOs (e.g., time-to-first-token,\ntime-between-tokens) with provider goals of efficiency and cost reduction.\nExisting solutions rely on static provisioning or model-level autoscaling, both\nof which treat the model as a monolith. This coarse-grained resource management\nleads to degraded performance or significant resource underutilization due to\npoor adaptability to dynamic inference traffic that is common online.\n  The root cause of this inefficiency lies in the internal structure of\ngenerative models: they are executed as graphs of interconnected operators.\nThrough detailed characterization and systematic analysis, we find that\noperators are heterogeneous in their compute and memory footprints and exhibit\ndiverse sensitivity to workload and resource factors such as batch size,\nsequence length, and traffic rate. This heterogeneity suggests that the\noperator, rather than the entire model, is the right granularity for scaling\ndecisions.\n  We propose an operator-level autoscaling framework, which allocates resources\nat finer (operator)-granularity, optimizing the scaling, batching, and\nplacement based on individual operator profiles. Evaluated on production-scale\ntraces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less\nenergy, or under fixed resources achieves 1.6x higher throughput with 5% less\nenergy. These results show that the operator, rather than the model, is\nfundamentally a more effective unit for scaling large generative workloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u5b50\u7ea7\u522b\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8d44\u6e90\u7ba1\u7406\u4f18\u5316\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u670d\u52a1\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u7ea7\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u91c7\u7528\u9759\u6001\u8d44\u6e90\u914d\u7f6e\u6216\u6a21\u578b\u7ea7\u81ea\u52a8\u6269\u7f29\u5bb9\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u6574\u4f53\u8fdb\u884c\u7c97\u7c92\u5ea6\u8d44\u6e90\u7ba1\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u6216\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u63a8\u7406\u6d41\u91cf\u7684\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u7279\u5f81\u5206\u6790\u548c\u7cfb\u7edf\u5206\u6790\u53d1\u73b0\u751f\u6210\u6a21\u578b\u4e2d\u7b97\u5b50\u7684\u5f02\u6784\u6027\uff0c\u63d0\u51fa\u7b97\u5b50\u7ea7\u81ea\u52a8\u6269\u7f29\u5bb9\u6846\u67b6\uff0c\u57fa\u4e8e\u5355\u4e2a\u7b97\u5b50\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u6269\u7f29\u5bb9\u3001\u6279\u5904\u7406\u548c\u653e\u7f6e\u4f18\u5316\u3002", "result": "\u5728\u751f\u4ea7\u89c4\u6a21\u8ddf\u8e2a\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301SLO\u7684\u524d\u63d0\u4e0b\u51cf\u5c1140% GPU\u548c35%\u80fd\u8017\uff0c\u6216\u5728\u56fa\u5b9a\u8d44\u6e90\u4e0b\u5b9e\u73b01.6\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c5%\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "\u7b97\u5b50\u800c\u975e\u6a21\u578b\u662f\u6269\u5c55\u5927\u578b\u751f\u6210\u5de5\u4f5c\u8d1f\u8f7d\u7684\u66f4\u6709\u6548\u5355\u5143\uff0c\u7ec6\u7c92\u5ea6\u8d44\u6e90\u7ba1\u7406\u80fd\u663e\u8457\u63d0\u5347\u670d\u52a1\u6548\u7387\u3002"}}
{"id": "2511.02257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02257", "abs": "https://arxiv.org/abs/2511.02257", "authors": ["Oguz Selvitopi", "Emin Ozturk", "Jie Chen", "Ponnuswamy Sadayappan", "Robert G. Edwards", "Ayd\u0131n Bulu\u00e7"], "title": "Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators", "comment": null, "summary": "Computation of correlation functions is a key operation in Lattice quantum\nchromodynamics (LQCD) simulations to extract nuclear physics observables. These\nfunctions involve many binary batch tensor contractions, each tensor possibly\noccupying hundreds of MBs of memory. Performing these contractions on GPU\naccelerators poses the challenge of scheduling them as to optimize tensor reuse\nand reduce data traffic. In this work we propose two fast novel scheduling\nalgorithms that reorder contractions to increase temporal locality via\ninput/intermediate tensor reuse. Our schedulers take advantage of\napplication-specific features, such as contractions being binary and locality\nwithin contraction trees, to optimize the objective of minimizing peak memory.\nWe integrate them into the LQCD analysis software suite Redstar and improve\ntime-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,\nwhich is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data\ntraffic, resulting in upto 1.9x faster correlation function computation time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u8c03\u5ea6\u7b97\u6cd5\u6765\u4f18\u5316\u683c\u70b9\u91cf\u5b50\u8272\u52a8\u529b\u5b66\u4e2d\u7684\u76f8\u5173\u51fd\u6570\u8ba1\u7b97\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u5f20\u91cf\u6536\u7f29\u64cd\u4f5c\u6765\u63d0\u9ad8\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u51cf\u5c11\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u548c\u6570\u636e\u4f20\u8f93\uff0c\u4ece\u800c\u52a0\u901f\u8ba1\u7b97\u3002", "motivation": "\u683c\u70b9\u91cf\u5b50\u8272\u52a8\u529b\u5b66\u4e2d\u7684\u76f8\u5173\u51fd\u6570\u8ba1\u7b97\u6d89\u53ca\u5927\u91cf\u4e8c\u8fdb\u5236\u6279\u91cf\u5f20\u91cf\u6536\u7f29\uff0c\u6bcf\u4e2a\u5f20\u91cf\u53ef\u80fd\u5360\u7528\u6570\u767eMB\u5185\u5b58\u3002\u5728GPU\u52a0\u901f\u5668\u4e0a\u6267\u884c\u8fd9\u4e9b\u6536\u7f29\u65f6\uff0c\u9762\u4e34\u8c03\u5ea6\u6311\u6218\uff0c\u9700\u8981\u4f18\u5316\u5f20\u91cf\u91cd\u7528\u5e76\u51cf\u5c11\u6570\u636e\u6d41\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5feb\u901f\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5229\u7528\u5e94\u7528\u7279\u5b9a\u7279\u5f81\uff08\u5982\u4e8c\u8fdb\u5236\u6536\u7f29\u548c\u6536\u7f29\u6811\u5185\u7684\u5c40\u90e8\u6027\uff09\u6765\u91cd\u65b0\u6392\u5e8f\u6536\u7f29\u64cd\u4f5c\uff0c\u901a\u8fc7\u8f93\u5165/\u4e2d\u95f4\u5f20\u91cf\u91cd\u7528\u589e\u52a0\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u4f18\u5316\u6700\u5c0f\u5316\u5cf0\u503c\u5185\u5b58\u7684\u76ee\u6807\u3002", "result": "\u8c03\u5ea6\u5668\u5728\u5cf0\u503c\u5185\u5b58\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.1\u500d\u7684\u6539\u8fdb\uff0c\u8fd9\u53cd\u6620\u5728\u9a71\u9010\u6b21\u6570\u51cf\u5c11\u9ad8\u8fbe4.2\u500d\uff0c\u6570\u636e\u6d41\u91cf\u51cf\u5c11\u9ad8\u8fbe1.8\u500d\uff0c\u5bfc\u81f4\u76f8\u5173\u51fd\u6570\u8ba1\u7b97\u65f6\u95f4\u52a0\u5feb\u9ad8\u8fbe1.9\u500d\u3002", "conclusion": "\u5c06\u8c03\u5ea6\u5668\u96c6\u6210\u5230LQCD\u5206\u6790\u8f6f\u4ef6\u5957\u4ef6Redstar\u4e2d\uff0c\u6539\u5584\u4e86\u6c42\u89e3\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u4f18\u5316\u683c\u70b9\u91cf\u5b50\u8272\u52a8\u529b\u5b66\u8ba1\u7b97\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.02534", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02534", "abs": "https://arxiv.org/abs/2511.02534", "authors": ["Enhong Mu", "Jinyu Cai", "Yijun Lu", "Mingyue Zhang", "Kenji Tei", "Jialong Li"], "title": "Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting", "comment": null, "summary": "The rapid iteration and frequent updates of modern video games pose\nsignificant challenges to the efficiency and specificity of testing. Although\nautomated playtesting methods based on Large Language Models (LLMs) have shown\npromise, they often lack structured knowledge accumulation mechanisms, making\nit difficult to conduct precise and efficient testing tailored for incremental\ngame updates. To address this challenge, this paper proposes a KLPEG framework.\nThe framework constructs and maintains a Knowledge Graph (KG) to systematically\nmodel game elements, task dependencies, and causal relationships, enabling\nknowledge accumulation and reuse across versions. Building on this foundation,\nthe framework utilizes LLMs to parse natural language update logs, identify the\nscope of impact through multi-hop reasoning on the KG, enabling the generation\nof update-tailored test cases. Experiments in two representative game\nenvironments, Overcooked and Minecraft, demonstrate that KLPEG can more\naccurately locate functionalities affected by updates and complete tests in\nfewer steps, significantly improving both playtesting effectiveness and\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KLPEG\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u6765\u7cfb\u7edf\u5efa\u6a21\u6e38\u620f\u5143\u7d20\u3001\u4efb\u52a1\u4f9d\u8d56\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u5229\u7528LLM\u89e3\u6790\u66f4\u65b0\u65e5\u5fd7\u5e76\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u751f\u6210\u9488\u5bf9\u66f4\u65b0\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e38\u620f\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u4ee3\u89c6\u9891\u6e38\u620f\u7684\u5feb\u901f\u8fed\u4ee3\u548c\u9891\u7e41\u66f4\u65b0\u5bf9\u6d4b\u8bd5\u6548\u7387\u548c\u7279\u5f02\u6027\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\u7f3a\u4e4f\u7ed3\u6784\u5316\u77e5\u8bc6\u79ef\u7d2f\u673a\u5236\uff0c\u96be\u4ee5\u9488\u5bf9\u589e\u91cf\u6e38\u620f\u66f4\u65b0\u8fdb\u884c\u7cbe\u786e\u9ad8\u6548\u7684\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faKLPEG\u6846\u67b6\uff1a1\uff09\u6784\u5efa\u548c\u7ef4\u62a4\u77e5\u8bc6\u56fe\u8c31\u6765\u7cfb\u7edf\u5efa\u6a21\u6e38\u620f\u5143\u7d20\u3001\u4efb\u52a1\u4f9d\u8d56\u548c\u56e0\u679c\u5173\u7cfb\uff1b2\uff09\u5229\u7528LLM\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u66f4\u65b0\u65e5\u5fd7\uff1b3\uff09\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u591a\u8df3\u63a8\u7406\u8bc6\u522b\u5f71\u54cd\u8303\u56f4\uff1b4\uff09\u751f\u6210\u9488\u5bf9\u66f4\u65b0\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728Overcooked\u548cMinecraft\u4e24\u4e2a\u4ee3\u8868\u6027\u6e38\u620f\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKLPEG\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u53d7\u66f4\u65b0\u5f71\u54cd\u7684\u529f\u80fd\uff0c\u5e76\u4ee5\u66f4\u5c11\u7684\u6b65\u9aa4\u5b8c\u6210\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6e38\u620f\u6d4b\u8bd5\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "conclusion": "KLPEG\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548cLLM\u7684\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u6e38\u620f\u589e\u91cf\u66f4\u65b0\u6d4b\u8bd5\u4e2d\u7684\u77e5\u8bc6\u79ef\u7d2f\u548c\u91cd\u7528\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316\u6e38\u620f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02589", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02589", "abs": "https://arxiv.org/abs/2511.02589", "authors": ["Claudia Herambourg", "Dawid Siuda", "Anna Szczepanek", "Julia Kopczy\u0144ska", "Joao R. L. Santos", "Wojciech Sas", "Joanna \u015amieta\u0144ska-Nowak"], "title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models", "comment": null, "summary": "We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics.", "AI": {"tldr": "ORCA\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLMs\u5728\u591a\u9886\u57df\u771f\u5b9e\u5b9a\u91cf\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u91d1\u878d\u3001\u7269\u7406\u3001\u5065\u5eb7\u3001\u7edf\u8ba1\u7b49\u9886\u57df\u3002\u4e94\u4e2a\u5148\u8fdb\u6a21\u578b\u5728500\u4e2a\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4ec5\u4e3a45-63%\uff0c\u4e3b\u8981\u9519\u8bef\u4e3a\u820d\u5165\u9519\u8bef(35%)\u548c\u8ba1\u7b97\u9519\u8bef(33%)\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u6570\u636e\u96c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4e16\u754c\u591a\u9886\u57df\u5b9a\u91cf\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u9010\u6b65\u63a8\u7406\u3001\u6570\u503c\u7cbe\u5ea6\u548c\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528Omni\u8ba1\u7b97\u5f15\u64ce\u9a8c\u8bc1\u8f93\u51fa\uff0c\u6784\u5efa\u5305\u542b500\u4e2a\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u91d1\u878d\u3001\u7269\u7406\u3001\u5065\u5eb7\u3001\u7edf\u8ba1\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u9010\u6b65\u63a8\u7406\u548c\u6570\u503c\u7cbe\u5ea6\u3002", "result": "\u4e94\u4e2a\u5148\u8fdb\u6a21\u578b(\u5305\u62ecChatGPT-5\u3001Gemini 2.5 Flash\u7b49)\u5728ORCA\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u4e3a45-63%\uff0c\u4e3b\u8981\u9519\u8bef\u7c7b\u578b\u4e3a\u820d\u5165\u9519\u8bef(35%)\u548c\u8ba1\u7b97\u9519\u8bef(33%)\u3002\u6a21\u578b\u5728\u6570\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u7269\u7406\u548c\u81ea\u7136\u79d1\u5b66\u9886\u57df\u8f83\u5f31\u3002", "conclusion": "LLMs\u5728\u771f\u5b9e\u4e16\u754c\u5b9a\u91cf\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u6709\u663e\u8457\u5c40\u9650\u6027\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u9519\u8bef\u7c7b\u578b\u4e0a\u5177\u6709\u90e8\u5206\u4e92\u8865\u6027\u800c\u975e\u5b8c\u5168\u5197\u4f59\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6570\u503c\u7cbe\u5ea6\u548c\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.02647", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02647", "abs": "https://arxiv.org/abs/2511.02647", "authors": ["Xiumei Deng", "Zehui Xiong", "Binbin Chen", "Dong In Kim", "Merouane Debbah", "H. Vincent Poor"], "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks", "comment": null, "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8054\u90a6\u6ce8\u610f\u529b\uff08FedAttn\uff09\u6846\u67b6\uff0c\u5c06\u8054\u90a6\u5b66\u4e60\u8303\u5f0f\u96c6\u6210\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3001\u901a\u4fe1\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5206\u5e03\u5f0fLLM\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u8fb9\u7f18\u534f\u4f5c\u573a\u666f\u4e2d\u9762\u4e34\u7684\u9690\u79c1\u6f0f\u6d1e\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u74f6\u9888\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u672c\u5730\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u5468\u671f\u6027\u7684KV\u77e9\u9635\u4ea4\u6362\u805a\u5408\uff0c\u5728\u591a\u4e2aTransformer\u5757\u95f4\u534f\u4f5c\u751f\u6210LLM\u54cd\u5e94\uff0c\u540c\u65f6\u4fdd\u62a4\u79c1\u6709\u63d0\u793a\u4e0d\u88ab\u66b4\u9732\u3002", "result": "\u7406\u8bba\u5206\u6790\u4e86\u9519\u8bef\u4f20\u64ad\u52a8\u6001\u548c\u54cd\u5e94\u8d28\u91cf\u4e0e\u901a\u4fe1/\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u5e76\u63ed\u793a\u4e86\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94KV\u805a\u5408\u7684\u4f18\u5316\u673a\u4f1a\u3002", "conclusion": "FedAttn\u6846\u67b6\u4e3a\u73b0\u5b9e\u4e16\u754c\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u7684\u6f5c\u529b\uff0c\u5efa\u7acb\u4e86\u5c06\u8054\u90a6\u4f18\u5316\u6280\u672f\u7cfb\u7edf\u6027\u5730\u79fb\u690d\u5230\u534f\u4f5cLLM\u63a8\u7406\u7684\u539f\u5219\u57fa\u7840\u3002"}}
{"id": "2511.02605", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02605", "abs": "https://arxiv.org/abs/2511.02605", "authors": ["Tiberiu-Andrei Georgescu", "Alexander W. Goodall", "Dalal Alrajeh", "Francesco Belardinelli", "Sebastian Uchitel"], "title": "Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning", "comment": null, "summary": "Shielding is widely used to enforce safety in reinforcement learning (RL),\nensuring that an agent's actions remain compliant with formal specifications.\nClassical shielding approaches, however, are often static, in the sense that\nthey assume fixed logical specifications and hand-crafted abstractions. While\nthese static shields provide safety under nominal assumptions, they fail to\nadapt when environment assumptions are violated. In this paper, we develop the\nfirst adaptive shielding framework - to the best of our knowledge - based on\nGeneralized Reactivity of rank 1 (GR(1)) specifications, a tractable and\nexpressive fragment of Linear Temporal Logic (LTL) that captures both safety\nand liveness properties. Our method detects environment assumption violations\nat runtime and employs Inductive Logic Programming (ILP) to automatically\nrepair GR(1) specifications online, in a systematic and interpretable way. This\nensures that the shield evolves gracefully, ensuring liveness is achievable and\nweakening goals only when necessary. We consider two case studies: Minepump and\nAtari Seaquest; showing that (i) static symbolic controllers are often severely\nsuboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped\nwith our adaptive shield maintain near-optimal reward and perfect logical\ncompliance compared with static shields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8eGR(1)\u89c4\u8303\u7684\u81ea\u9002\u5e94\u5c4f\u853d\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u68c0\u6d4b\u73af\u5883\u5047\u8bbe\u8fdd\u89c4\u5e76\u4f7f\u7528\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u81ea\u52a8\u5728\u7ebf\u4fee\u590d\u89c4\u8303\uff0c\u786e\u4fdd\u5c4f\u853d\u5668\u4f18\u96c5\u6f14\u5316\u5e76\u4fdd\u6301\u6700\u4f18\u5956\u52b1\u548c\u5b8c\u7f8e\u903b\u8f91\u5408\u89c4\u6027\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u5c4f\u853d\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u7684\u903b\u8f91\u89c4\u8303\u548c\u624b\u5de5\u5236\u4f5c\u7684\u62bd\u8c61\uff0c\u5728\u73af\u5883\u5047\u8bbe\u88ab\u8fdd\u53cd\u65f6\u65e0\u6cd5\u9002\u5e94\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u52a8\u6001\u8c03\u6574\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u5c4f\u853d\u6846\u67b6\u3002", "method": "\u57fa\u4e8eGR(1)\u89c4\u8303\u5f00\u53d1\u81ea\u9002\u5e94\u5c4f\u853d\u6846\u67b6\uff0c\u8fd0\u884c\u65f6\u68c0\u6d4b\u73af\u5883\u5047\u8bbe\u8fdd\u89c4\uff0c\u4f7f\u7528\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u81ea\u52a8\u5728\u7ebf\u4fee\u590dGR(1)\u89c4\u8303\uff0c\u786e\u4fdd\u5c4f\u853d\u5668\u6f14\u5316\u5e76\u4fdd\u6301\u6d3b\u6027\u76ee\u6807\u3002", "result": "\u5728Minepump\u548cAtari Seaquest\u6848\u4f8b\u7814\u7a76\u4e2d\u663e\u793a\uff1a(i)\u9759\u6001\u7b26\u53f7\u63a7\u5236\u5668\u5728\u4f18\u5316\u8f85\u52a9\u5956\u52b1\u65f6\u901a\u5e38\u4e25\u91cd\u6b21\u4f18\uff1b(ii)\u914d\u5907\u81ea\u9002\u5e94\u5c4f\u853d\u7684RL\u667a\u80fd\u4f53\u76f8\u6bd4\u9759\u6001\u5c4f\u853d\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u5956\u52b1\u548c\u5b8c\u7f8e\u903b\u8f91\u5408\u89c4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5c4f\u853d\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u73af\u5883\u5047\u8bbe\u8fdd\u89c4\uff0c\u786e\u4fdd\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u5408\u89c4\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02655", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2511.02655", "abs": "https://arxiv.org/abs/2511.02655", "authors": ["Johansell Villalobos", "Josef Ruzicka", "Silvio Rizzi"], "title": "Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks", "comment": null, "summary": "Scientific computing in the exascale era demands increased computational\npower to solve complex problems across various domains. With the rise of\nheterogeneous computing architectures the need for vendor-agnostic, performance\nportability frameworks has been highlighted. Libraries like Kokkos have become\nessential for enabling high-performance computing applications to execute\nefficiently across different hardware platforms with minimal code changes. In\nthis direction, this paper presents preliminary time-to-solution results for\ntwo representative scientific computing applications: an N-body simulation and\na structured grid simulation. Both applications used a distributed memory\napproach and hardware acceleration through four performance portability\nframeworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single\nnode of the Polaris supercomputer using four NVIDIA A100 GPUs revealed\nsignificant performance variability among frameworks. OCCA demonstrated faster\nexecution times for small-scale validation problems, likely due to JIT\ncompilation, however its lack of optimized reduction algorithms may limit\nscalability for larger simulations while using its out of the box API. OpenMP\nperformed poorly in the structured grid simulation most likely due to\ninefficiencies in inter-node data synchronization and communication. These\nfindings highlight the need for further optimization to maximize each\nframework's capabilities. Future work will focus on enhancing reduction\nalgorithms, data communication, memory management, as wells as performing\nscalability studies, and a comprehensive statistical analysis to evaluate and\ncompare framework performance.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6846\u67b6\uff08Kokkos\u3001OpenMP\u3001RAJA\u3001OCCA\uff09\u5728\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u6846\u67b6\u5728N\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0cOCCA\u5728\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u95ee\u9898\u4e0a\u8868\u73b0\u6700\u4f73\u4f46\u7f3a\u4e4f\u4f18\u5316\u7684\u5f52\u7ea6\u7b97\u6cd5\uff0cOpenMP\u5728\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u4e2d\u6027\u80fd\u8f83\u5dee\u3002", "motivation": "\u968f\u7740\u5f02\u6784\u8ba1\u7b97\u67b6\u6784\u7684\u5174\u8d77\uff0c\u79d1\u5b66\u8ba1\u7b97\u5728\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u65f6\u4ee3\u9700\u8981\u8de8\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u9ad8\u6548\u6267\u884c\u7684\u4f9b\u5e94\u5546\u65e0\u5173\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6846\u67b6\u3002Kokkos\u7b49\u5e93\u5bf9\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u4ee5\u6700\u5c0f\u4ee3\u7801\u66f4\u6539\u5b9e\u73b0\u9ad8\u6548\u6267\u884c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u5185\u5b58\u65b9\u6cd5\u548c\u786c\u4ef6\u52a0\u901f\uff0c\u901a\u8fc7\u56db\u79cd\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6846\u67b6\uff08Kokkos\u3001OpenMP\u3001RAJA\u3001OCCA\uff09\u5bf9\u4e24\u4e2a\u4ee3\u8868\u6027\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\uff08N\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728Polaris\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u5355\u4e2a\u8282\u70b9\u4e0a\u4f7f\u7528\u56db\u4e2aNVIDIA A100 GPU\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6846\u67b6\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff1aOCCA\u5728\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u95ee\u9898\u4e0a\u6267\u884c\u65f6\u95f4\u66f4\u5feb\uff08\u53ef\u80fd\u7531\u4e8eJIT\u7f16\u8bd1\uff09\uff0c\u4f46\u5176\u7f3a\u4e4f\u4f18\u5316\u7684\u5f52\u7ea6\u7b97\u6cd5\u53ef\u80fd\u9650\u5236\u5927\u89c4\u6a21\u6a21\u62df\u7684\u53ef\u6269\u5c55\u6027\uff1bOpenMP\u5728\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f88\u53ef\u80fd\u7531\u4e8e\u8282\u70b9\u95f4\u6570\u636e\u540c\u6b65\u548c\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u6700\u5927\u5316\u6bcf\u4e2a\u6846\u67b6\u7684\u80fd\u529b\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u4e13\u6ce8\u4e8e\u589e\u5f3a\u5f52\u7ea6\u7b97\u6cd5\u3001\u6570\u636e\u901a\u4fe1\u3001\u5185\u5b58\u7ba1\u7406\uff0c\u4ee5\u53ca\u8fdb\u884c\u53ef\u6269\u5c55\u6027\u7814\u7a76\u548c\u5168\u9762\u7684\u7edf\u8ba1\u5206\u6790\u6765\u8bc4\u4f30\u548c\u6bd4\u8f83\u6846\u67b6\u6027\u80fd\u3002"}}
{"id": "2511.02743", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02743", "abs": "https://arxiv.org/abs/2511.02743", "authors": ["Fedor Ryabinin", "Alexey Gotsman", "Pierre Sutra"], "title": "Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)", "comment": "Extended version of a paper in OPODIS'25: International Conference on\n  Principles of Distributed Systems", "summary": "Classical state-machine replication protocols, such as Paxos, rely on a\ndistinguished leader process to order commands. Unfortunately, this approach\nmakes the leader a single point of failure and increases the latency for\nclients that are not co-located with it. As a response to these drawbacks,\nEgalitarian Paxos introduced an alternative, leaderless approach, that allows\nreplicas to order commands collaboratively. Not relying on a single leader\nallows the protocol to maintain non-zero throughput with up to $f$ crashes of\nany processes out of a total of $n = 2f+1$. The protocol furthermore allows any\nprocess to execute a command $c$ fast, in $2$ message delays, provided no more\nthan $e = \\lceil\\frac{f+1}{2}\\rceil$ other processes fail, and all concurrently\nsubmitted commands commute with $c$; the latter condition is often satisfied in\npractical systems.\n  Egalitarian Paxos has served as a foundation for many other replication\nprotocols. But unfortunately, the protocol is very complex, ambiguously\nspecified and suffers from nontrivial bugs. In this paper, we present EPaxos*\n-- a simpler and correct variant of Egalitarian Paxos. Our key technical\ncontribution is a simpler failure-recovery algorithm, which we have rigorously\nproved correct. Our protocol also generalizes Egalitarian Paxos to cover the\nwhole spectrum of failure thresholds $f$ and $e$ such that $n \\ge \\max\\{2e+f-1,\n2f+1\\}$ -- the number of processes that we show to be optimal.", "AI": {"tldr": "EPaxos*\u662f\u4e00\u4e2a\u66f4\u7b80\u5355\u4e14\u6b63\u786e\u7684Egalitarian Paxos\u53d8\u4f53\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\u89e3\u51b3\u4e86\u539f\u534f\u8bae\u7684\u590d\u6742\u6027\u548c\u9519\u8bef\u95ee\u9898\uff0c\u5e76\u5c06\u534f\u8bae\u63a8\u5e7f\u5230\u6700\u4f18\u8fdb\u7a0b\u6570\u8303\u56f4\u3002", "motivation": "\u7ecf\u5178\u7684\u72b6\u6001\u673a\u590d\u5236\u534f\u8bae\uff08\u5982Paxos\uff09\u4f9d\u8d56\u9886\u5bfc\u8005\u8fdb\u7a0b\u6765\u6392\u5e8f\u547d\u4ee4\uff0c\u8fd9\u5bfc\u81f4\u9886\u5bfc\u8005\u6210\u4e3a\u5355\u70b9\u6545\u969c\u5e76\u589e\u52a0\u5ef6\u8fdf\u3002Egalitarian Paxos\u5f15\u5165\u4e86\u65e0\u9886\u5bfc\u8005\u7684\u65b9\u6cd5\uff0c\u4f46\u534f\u8bae\u590d\u6742\u3001\u89c4\u8303\u6a21\u7cca\u4e14\u5b58\u5728\u4e25\u91cd\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e86EPaxos*\uff0c\u5173\u952e\u6280\u672f\u521b\u65b0\u662f\u66f4\u7b80\u5355\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u4e25\u683c\u6b63\u786e\u6027\u8bc1\u660e\u3002\u534f\u8bae\u5c06Egalitarian Paxos\u63a8\u5e7f\u5230\u6ee1\u8db3n \u2265 max{2e+f-1, 2f+1}\u7684\u6574\u4e2a\u6545\u969c\u9608\u503cf\u548ce\u8303\u56f4\u3002", "result": "EPaxos*\u89e3\u51b3\u4e86Egalitarian Paxos\u7684\u590d\u6742\u6027\u548c\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u6b63\u786e\u7684\u534f\u8bae\u5b9e\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u534f\u8bae\u7684\u4f18\u52bf\u7279\u6027\u3002", "conclusion": "EPaxos*\u662fEgalitarian Paxos\u7684\u6539\u8fdb\u7248\u672c\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\u548c\u66f4\u4f18\u7684\u8fdb\u7a0b\u6570\u914d\u7f6e\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u5b9e\u7528\u7684\u65e0\u9886\u5bfc\u8005\u72b6\u6001\u673a\u590d\u5236\u534f\u8bae\u3002"}}
{"id": "2511.02627", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02627", "abs": "https://arxiv.org/abs/2511.02627", "authors": ["Lachlan McPheat", "Navdeep Kaur", "Robert Blackwell", "Alessandra Russo", "Anthony G. Cohn", "Pranava Madhyastha"], "title": "DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning", "comment": null, "summary": "We introduce DecompSR, decomposed spatial reasoning, a large benchmark\ndataset (over 5m datapoints) and generation framework designed to analyse\ncompositional spatial reasoning ability. The generation of DecompSR allows\nusers to independently vary several aspects of compositionality, namely:\nproductivity (reasoning depth), substitutivity (entity and linguistic\nvariability), overgeneralisation (input order, distractors) and systematicity\n(novel linguistic elements). DecompSR is built procedurally in a manner which\nmakes it is correct by construction, which is independently verified using a\nsymbolic solver to guarantee the correctness of the dataset. DecompSR is\ncomprehensively benchmarked across a host of Large Language Models (LLMs) where\nwe show that LLMs struggle with productive and systematic generalisation in\nspatial reasoning tasks whereas they are more robust to linguistic variation.\nDecompSR provides a provably correct and rigorous benchmarking dataset with a\nnovel ability to independently vary the degrees of several key aspects of\ncompositionality, allowing for robust and fine-grained probing of the\ncompositional reasoning abilities of LLMs.", "AI": {"tldr": "DecompSR\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u7ec4\u5408\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u548c\u751f\u6210\u6846\u67b6\uff0c\u5305\u542b\u8d85\u8fc7500\u4e07\u4e2a\u6570\u636e\u70b9\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u6784\u5efa\u786e\u4fdd\u6b63\u786e\u6027\uff0c\u5e76\u72ec\u7acb\u9a8c\u8bc1\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u72ec\u7acb\u53d8\u5316\u7ec4\u5408\u6027\u591a\u4e2a\u5173\u952e\u65b9\u9762\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u4e25\u683c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u7a0b\u5e8f\u5316\u751f\u6210\u6570\u636e\u96c6\uff0c\u5141\u8bb8\u72ec\u7acb\u53d8\u5316\u751f\u4ea7\u529b\uff08\u63a8\u7406\u6df1\u5ea6\uff09\u3001\u53ef\u66ff\u6362\u6027\uff08\u5b9e\u4f53\u548c\u8bed\u8a00\u53d8\u5f02\u6027\uff09\u3001\u8fc7\u5ea6\u6cdb\u5316\uff08\u8f93\u5165\u987a\u5e8f\u3001\u5e72\u6270\u9879\uff09\u548c\u7cfb\u7edf\u6027\uff08\u65b0\u8bed\u8a00\u5143\u7d20\uff09\u7b49\u7ec4\u5408\u6027\u65b9\u9762\uff0c\u5e76\u4f7f\u7528\u7b26\u53f7\u6c42\u89e3\u5668\u9a8c\u8bc1\u6570\u636e\u96c6\u6b63\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cLLMs\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u96be\u4ee5\u8fdb\u884c\u751f\u4ea7\u6027\u548c\u7cfb\u7edf\u6027\u6cdb\u5316\uff0c\u4f46\u5bf9\u8bed\u8a00\u53d8\u5f02\u6027\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "DecompSR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bc1\u660e\u6b63\u786e\u4e14\u4e25\u683c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u80fd\u591f\u72ec\u7acb\u53d8\u5316\u7ec4\u5408\u6027\u7684\u591a\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u4e3aLLMs\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7a33\u5065\u548c\u7ec6\u7c92\u5ea6\u7684\u63a2\u6d4b\u5de5\u5177\u3002"}}
{"id": "2511.02687", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02687", "abs": "https://arxiv.org/abs/2511.02687", "authors": ["Tim R. Davidson", "Adam Fourney", "Saleema Amershi", "Robert West", "Eric Horvitz", "Ece Kamar"], "title": "The Collaboration Gap", "comment": null, "summary": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u4f5c\u8ff7\u5bab\u6c42\u89e3\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8632\u4e2a\u9886\u5148\u7684AI\u6a21\u578b\u5728\u5355\u72ec\u3001\u540c\u8d28\u548c\u5f02\u8d28\u914d\u5bf9\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u53d1\u73b0\u5b58\u5728'\u534f\u4f5c\u5dee\u8ddd'\uff1a\u5355\u72ec\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u5728\u534f\u4f5c\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u968f\u7740AI\u53d1\u5c55\uff0c\u6211\u4eec\u5c06\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u7531\u72ec\u7acb\u5f00\u53d1\u7684\u5f02\u6784\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6210\u529f\u5173\u952e\u5728\u4e8e\u6709\u6548\u534f\u4f5c\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bc4\u4f30\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u534f\u4f5c\u8ff7\u5bab\u6c42\u89e3\u57fa\u51c6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\uff1a(i)\u9694\u79bb\u534f\u4f5c\u80fd\u529b\uff0c(ii)\u8c03\u8282\u95ee\u9898\u590d\u6742\u5ea6\uff0c(iii)\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u8bc4\u5206\uff0c(iv)\u4fdd\u6301\u751f\u6001\u5408\u7406\u6027\u3002\u4f7f\u7528\u8be5\u6846\u67b6\u8bc4\u4f30\u4e8632\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u4e0d\u540c\u914d\u5bf9\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u4e86\u660e\u663e\u7684'\u534f\u4f5c\u5dee\u8ddd'\uff1a\u5355\u72ec\u8868\u73b0\u597d\u7684\u6a21\u578b\u5728\u534f\u4f5c\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u5c0f\u578b\u84b8\u998f\u6a21\u578b\u5728\u67d0\u4e9b\u914d\u5bf9\u4e2d\u51e0\u4e4e\u5b8c\u5168\u5931\u8d25\u3002\u7814\u7a76\u53d1\u73b0\u4ece\u8f83\u5f3a\u667a\u80fd\u4f53\u5f00\u59cb\u5f80\u5f80\u80fd\u6539\u5584\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86'\u63a5\u529b\u63a8\u7406'\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u7f29\u5c0f\u534f\u4f5c\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u4e3b\u5f20\uff1a(1)\u534f\u4f5c\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c(2)\u589e\u5f3a\u534f\u4f5c\u80fd\u529b\u7684\u8bad\u7ec3\u7b56\u7565\uff0c(3)\u53ef\u9760\u6fc0\u53d1\u667a\u80fd\u4f53\u6f5c\u5728\u6280\u80fd\u7684\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u8fd9\u4e9b\u6307\u5bfc\u539f\u5219\u9002\u7528\u4e8eAI-AI\u548c\u4eba\u7c7b-AI\u534f\u4f5c\u3002"}}
{"id": "2511.02749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02749", "abs": "https://arxiv.org/abs/2511.02749", "authors": ["Paul Castro", "Nick Mitchell", "Nathan Ordonez", "Thomas Parnell", "Mudhakar Srivatsa", "Antoni Viros i Martin"], "title": "Using Span Queries to Optimize for Cache and Attention Locality", "comment": "12 pages, 17 figures", "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86span query\u6982\u5ff5\uff0c\u5c06\u63a8\u7406\u670d\u52a1\u5668\u63a5\u53e3\u6cdb\u5316\uff0c\u652f\u6301\u804a\u5929\u3001RAG\u3001\u63a8\u7406\u65f6\u7f29\u653e\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u901a\u8fc7\u4ea4\u6362\u6027\u7ea6\u675f\u4f18\u5316KV\u7f13\u5b58\u548c\u6ce8\u610f\u529b\u5c40\u90e8\u6027\uff0c\u5728vLLM\u4e0a\u5b9e\u73b010-20\u500dTTFT\u964d\u4f4e\u3002", "motivation": "\u5ba2\u6237\u7aef\u6b63\u5728\u8d85\u8d8a\u804a\u5929\u5b8c\u6210\u529f\u80fd\uff0c\u5305\u542b\u5404\u79cd\u521b\u65b0\u7684\u63a8\u7406\u65f6\u7f29\u653e\u548c\u6df1\u5ea6\u63a8\u7406\u6280\u672f\uff0c\u800c\u63a8\u7406\u670d\u52a1\u5668\u4ecd\u4e3b\u8981\u9488\u5bf9\u804a\u5929\u5b8c\u6210\u4f18\u5316\u3002\u73b0\u6709\u5de5\u4f5c\u867d\u7136\u5c55\u793a\u4e86KV\u7f13\u5b58\u547d\u4e2d\u7387\u7684\u5927\u5e45\u63d0\u5347\u53ef\u80fd\uff0c\u4f46\u4ec5\u9488\u5bf9\u5355\u4e00\u7528\u4f8bRAG\u3002", "method": "\u5f15\u5165span query\u4f5c\u4e3a\u63a8\u7406\u670d\u52a1\u5668\u63a5\u53e3\u7684\u6cdb\u5316\uff0c\u5c06\u5176\u5b9a\u4e49\u4e3a\u5e26\u6709\u4ea4\u6362\u6027\u7ea6\u675f\u7684\u63a8\u7406\u8c03\u7528\u8868\u8fbe\u5f0f\u6811\u3002\u63cf\u8ff0\u4e86span query\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u81ea\u52a8\u4f18\u5316\u4ee5\u63d0\u9ad8KV\u7f13\u5b58\u5c40\u90e8\u6027\uff0c\u5e76\u5bf9vLLM\u8fdb\u884c\u5c0f\u89c4\u6a21\u4fee\u6539\uff08\u4ec5492\u884c\u4ee3\u7801\uff09\u4ee5\u652f\u6301\u9ad8\u6027\u80fdspan query\u6267\u884c\u3002", "result": "span query\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u975e\u804a\u5929\u7528\u4f8b\u4e2d\u5b9e\u73b0\u4e8610-20\u500d\u7684TTFT\u964d\u4f4e\u3002\u6ce8\u610f\u529b\u4f18\u5316\u7684span query\u57282b\u53c2\u6570\u6a21\u578b\u4e0a\u7684\u51c6\u786e\u7387\u8fdc\u8d85\u4f7f\u75288b\u6a21\u578b\u7684\u6807\u51c6\u63a8\u7406\u670d\u52a1\u5668\u3002", "conclusion": "span query\u4e3a\u63a8\u7406\u670d\u52a1\u5668\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u63a5\u53e3\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u79cd\u65b0\u5174\u7684\u975e\u804a\u5929\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u901a\u8fc7\u4ea4\u6362\u6027\u7ea6\u675f\u4f18\u5316\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4e2d\u95f4\u4e22\u5931\u95ee\u9898\u3002"}}
{"id": "2511.02759", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02759", "abs": "https://arxiv.org/abs/2511.02759", "authors": ["Julius Fiedler", "Carsten Knoll", "Klaus R\u00f6benack"], "title": "LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer", "comment": "4 pages, 2 figures", "summary": "The rapid growth of research output in control engineering calls for new\napproaches to structure and formalize domain knowledge. This paper briefly\ndescribes an LLM-supported method for semi-automated generation of formal\nknowledge representations that combine human readability with machine\ninterpretability and increased expressiveness. Based on the Imperative\nRepresentation of Knowledge (PyIRK) framework, we demonstrate how language\nmodels can assist in transforming natural-language descriptions and\nmathematical definitions (available as LaTeX source code) into a formalized\nknowledge graph. As a first application we present the generation of an\n``interactive semantic layer'' to enhance the source documents in order to\nfacilitate knowledge transfer. From our perspective this contributes to the\nvision of easily accessible, collaborative, and verifiable knowledge bases for\nthe control engineering domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u63a7\u5236\u5de5\u7a0b\u9886\u57df\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548c\u6570\u5b66\u5b9a\u4e49\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u589e\u5f3a\u77e5\u8bc6\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "motivation": "\u63a7\u5236\u5de5\u7a0b\u7814\u7a76\u4ea7\u51fa\u7684\u5feb\u901f\u589e\u957f\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7ed3\u6784\u5316\u548c\u5f62\u5f0f\u5316\u9886\u57df\u77e5\u8bc6\uff0c\u89e3\u51b3\u77e5\u8bc6\u53ef\u8bbf\u95ee\u6027\u3001\u534f\u4f5c\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8eImperative Representation of Knowledge (PyIRK)\u6846\u67b6\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548cLaTeX\u6570\u5b66\u5b9a\u4e49\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u751f\u6210\u4ea4\u4e92\u5f0f\u8bed\u4e49\u5c42\u6765\u589e\u5f3a\u6e90\u6587\u6863\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5982\u4f55\u901a\u8fc7LLM\u652f\u6301\u7684\u65b9\u6cd5\u751f\u6210\u7ed3\u5408\u4eba\u7c7b\u53ef\u8bfb\u6027\u548c\u673a\u5668\u53ef\u89e3\u91ca\u6027\u7684\u5f62\u5f0f\u5316\u77e5\u8bc6\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u6784\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u63a7\u5236\u5de5\u7a0b\u9886\u57df\u6613\u4e8e\u8bbf\u95ee\u3001\u534f\u4f5c\u548c\u53ef\u9a8c\u8bc1\u7684\u77e5\u8bc6\u5e93\u613f\u666f\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5c55\u793a\u4e86LLM\u5728\u77e5\u8bc6\u5f62\u5f0f\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.02794", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.02794", "abs": "https://arxiv.org/abs/2511.02794", "authors": ["Chenyu Zhang", "Minsol Kim", "Shohreh Ghorbani", "Jingyao Wu", "Rosalind Picard", "Patricia Maes", "Paul Pu Liang"], "title": "When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning", "comment": "Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop,\n  NeurIPS 2025", "summary": "Despite rapid growth in multimodal large language models (MLLMs), their\nreasoning traces remain opaque: it is often unclear which modality drives a\nprediction, how conflicts are resolved, or when one stream dominates. In this\npaper, we introduce modality sabotage, a diagnostic failure mode in which a\nhigh-confidence unimodal error overrides other evidence and misleads the fused\nresult. To analyze such dynamics, we propose a lightweight, model-agnostic\nevaluation layer that treats each modality as an agent, producing candidate\nlabels and a brief self-assessment used for auditing. A simple fusion mechanism\naggregates these outputs, exposing contributors (modalities supporting correct\noutcomes) and saboteurs (modalities that mislead). Applying our diagnostic\nlayer in a case study on multimodal emotion recognition benchmarks with\nfoundation models revealed systematic reliability profiles, providing insight\ninto whether failures may arise from dataset artifacts or model limitations.\nMore broadly, our framework offers a diagnostic scaffold for multimodal\nreasoning, supporting principled auditing of fusion dynamics and informing\npossible interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u5c42\u6765\u8bca\u65ad\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u7834\u574f\u95ee\u9898\uff0c\u5373\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5355\u6a21\u6001\u9519\u8bef\u4f1a\u8986\u76d6\u5176\u4ed6\u8bc1\u636e\u5e76\u8bef\u5bfc\u878d\u5408\u7ed3\u679c\u3002\u8be5\u6846\u67b6\u5c06\u6bcf\u4e2a\u6a21\u6001\u89c6\u4e3a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u878d\u5408\u673a\u5236\u8bc6\u522b\u8d21\u732e\u8005\u548c\u7834\u574f\u8005\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u8bca\u65ad\u652f\u67b6\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u4ecd\u4e0d\u900f\u660e\uff1a\u4e0d\u6e05\u695a\u54ea\u4e2a\u6a21\u6001\u9a71\u52a8\u9884\u6d4b\u3001\u5982\u4f55\u89e3\u51b3\u51b2\u7a81\u6216\u4f55\u65f6\u67d0\u4e2a\u6a21\u6001\u5360\u4e3b\u5bfc\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u52a8\u6001\u7279\u6027\uff0c\u7279\u522b\u662f\u6a21\u6001\u7834\u574f\u8fd9\u79cd\u8bca\u65ad\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u5c42\uff0c\u5c06\u6bcf\u4e2a\u6a21\u6001\u89c6\u4e3a\u667a\u80fd\u4f53\uff0c\u751f\u6210\u5019\u9009\u6807\u7b7e\u548c\u7b80\u8981\u81ea\u8bc4\u4f30\u7528\u4e8e\u5ba1\u8ba1\u3002\u901a\u8fc7\u7b80\u5355\u7684\u878d\u5408\u673a\u5236\u805a\u5408\u8fd9\u4e9b\u8f93\u51fa\uff0c\u66b4\u9732\u8d21\u732e\u8005\uff08\u652f\u6301\u6b63\u786e\u7ed3\u679c\u7684\u6a21\u6001\uff09\u548c\u7834\u574f\u8005\uff08\u8bef\u5bfc\u7684\u6a21\u6001\uff09\u3002", "result": "\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5e94\u7528\u8be5\u8bca\u65ad\u5c42\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u53ef\u9760\u6027\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5931\u8d25\u662f\u6e90\u4e8e\u6570\u636e\u96c6\u4f2a\u5f71\u8fd8\u662f\u6a21\u578b\u9650\u5236\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u8bca\u65ad\u652f\u67b6\uff0c\u652f\u6301\u5bf9\u878d\u5408\u52a8\u6001\u8fdb\u884c\u539f\u5219\u6027\u5ba1\u8ba1\uff0c\u5e76\u4e3a\u53ef\u80fd\u7684\u5e72\u9884\u63aa\u65bd\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2511.02823", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02823", "abs": "https://arxiv.org/abs/2511.02823", "authors": ["Chloe Loughridge", "Paul Colognese", "Avery Griffin", "Tyler Tracy", "Jon Kutasov", "Joe Benton"], "title": "Optimizing AI Agent Attacks With Synthetic Data", "comment": null, "summary": "As AI deployments become more complex and high-stakes, it becomes\nincreasingly important to be able to estimate their risk. AI control is one\nframework for doing so. However, good control evaluations require eliciting\nstrong attack policies. This can be challenging in complex agentic environments\nwhere compute constraints leave us data-poor. In this work, we show how to\noptimize attack policies in SHADE-Arena, a dataset of diverse realistic control\nenvironments. We do this by decomposing attack capability into five constituent\nskills -- suspicion modeling, attack selection, plan synthesis, execution and\nsubtlety -- and optimizing each component individually. To get around the\nconstraint of limited data, we develop a probabilistic model of attack\ndynamics, optimize our attack hyperparameters using this simulation, and then\nshow that the results transfer to SHADE-Arena. This results in a substantial\nimprovement in attack strength, reducing safety score from a baseline of 0.87\nto 0.41 using our scaffold.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u590d\u6742AI\u63a7\u5236\u73af\u5883\u4e2d\u4f18\u5316\u653b\u51fb\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u653b\u51fb\u80fd\u529b\u5206\u89e3\u4e3a\u4e94\u4e2a\u6280\u80fd\u7ec4\u4ef6\u5e76\u5206\u522b\u4f18\u5316\uff0c\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u6982\u7387\u6a21\u578b\u6a21\u62df\u653b\u51fb\u52a8\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u5f3a\u5ea6\u3002", "motivation": "\u968f\u7740AI\u90e8\u7f72\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u548c\u9ad8\u98ce\u9669\uff0c\u51c6\u786e\u8bc4\u4f30\u5176\u98ce\u9669\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002AI\u63a7\u5236\u662f\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u4f46\u9700\u8981\u5f3a\u5927\u7684\u653b\u51fb\u7b56\u7565\uff0c\u8fd9\u5728\u8ba1\u7b97\u53d7\u9650\u7684\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5c06\u653b\u51fb\u80fd\u529b\u5206\u89e3\u4e3a\u4e94\u4e2a\u7ec4\u6210\u6280\u80fd\uff1a\u6000\u7591\u5efa\u6a21\u3001\u653b\u51fb\u9009\u62e9\u3001\u8ba1\u5212\u5408\u6210\u3001\u6267\u884c\u548c\u9690\u853d\u6027\uff0c\u5e76\u5206\u522b\u4f18\u5316\u6bcf\u4e2a\u7ec4\u4ef6\u3002\u4e3a\u89e3\u51b3\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u653b\u51fb\u52a8\u6001\u7684\u6982\u7387\u6a21\u578b\uff0c\u5728\u6a21\u62df\u4e2d\u4f18\u5316\u653b\u51fb\u8d85\u53c2\u6570\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u8f6c\u79fb\u5230SHADE-Arena\u73af\u5883\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u5f3a\u5ea6\uff0c\u5c06\u5b89\u5168\u5206\u6570\u4ece\u57fa\u7ebf0.87\u964d\u4f4e\u52300.41\uff0c\u4f7f\u7528\u63d0\u51fa\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u653b\u51fb\u80fd\u529b\u7684\u5b9e\u8d28\u6027\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u6280\u80fd\u5206\u89e3\u548c\u6982\u7387\u5efa\u6a21\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6570\u636e\u6709\u9650\u7684\u590d\u6742AI\u63a7\u5236\u73af\u5883\u4e2d\u6709\u6548\u4f18\u5316\u653b\u51fb\u7b56\u7565\uff0c\u4e3aAI\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
