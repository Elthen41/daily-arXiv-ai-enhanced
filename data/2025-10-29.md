<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 33]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing](https://arxiv.org/abs/2510.23911)
*Arno Uhlig,Iris Braun,Matthias Wählisch*

Main category: cs.DC

TL;DR: 本文分析了SAP云平台中虚拟机调度和放置问题，基于1800个管理程序和48000个VM的30天数据，发现了CPU资源争用超过40%、CPU就绪时间达220秒、主机负载不平衡等问题，并提出了改进调度算法的需求。


<details>
  <summary>Details</summary>
Motivation: 分析分布式环境中资源分配的基本挑战，特别是在SAP云平台中优化虚拟机调度和放置，以提高资源利用效率。

Method: 基于30天观察期内约1800个管理程序和48000个VM的细粒度时间序列遥测数据，使用可观测性工具跟踪整个基础设施的资源使用和性能指标。

Result: 发现多个次优调度情况：CPU资源争用超过40%、CPU就绪时间高达220秒、计算主机负载显著不平衡（最大CPU利用率达99%）、CPU和内存资源过度配置导致超过80%的VM使用不到70%的分配资源。

Conclusion: 基于研究发现，提出了新型放置和调度算法的设计需求，并为优化资源分配提供了指导。同时公开了完整数据集，以支持未来大规模云基础设施调度方法的数据驱动评估。

Abstract: Allocating resources in a distributed environment is a fundamental challenge.
In this paper, we analyze the scheduling and placement of virtual machines
(VMs) in the cloud platform of SAP, the world's largest enterprise resource
planning software vendor. Based on data from roughly 1,800 hypervisors and
48,000 VMs within a 30-day observation period, we highlight potential
improvements for workload management. The data was measured through
observability tooling that tracks resource usage and performance metrics across
the entire infrastructure. In contrast to existing datasets, ours uniquely
offers fine-grained time-series telemetry data of fully virtualized
enterprise-level workloads from both long-running and memory-intensive SAP
S/4HANA and diverse, general-purpose applications. Our key findings include
several suboptimal scheduling situations, such as CPU resource contention
exceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced
compute hosts with a maximum CPU~utilization on intra-building block hosts of
up to 99%, and overprovisioned CPU and memory resources resulting into over 80%
of VMs using less than 70% of the provided resources. Bolstered by these
findings, we derive requirements for the design and implementation of novel
placement and scheduling algorithms and provide guidance to optimize resource
allocations. We make the full dataset used in this study publicly available to
enable data-driven evaluations of scheduling approaches for large-scale cloud
infrastructures in future research.

</details>


### [2] [A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales](https://arxiv.org/abs/2510.23993)
*Anthony Carreon,Jagmohan Singh,Shivank Sharma,Shuzhi Zhang,Venkat Raman*

Main category: cs.DC

TL;DR: 开发了一个基于AMReX框架的高性能可压缩反应流求解器，针对多GPU环境优化，解决了GPU性能瓶颈问题，在燃烧应用中实现了2-5倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 高速化学反应流存在显著的计算挑战，现有的GPU可压缩燃烧求解器在内存管理、负载平衡和处理化学反应的高度局部化特性方面存在关键限制。

Method: 采用列优先存储优化内存访问模式，通过批量稀疏积分策略处理化学动力学计算负载变化，为自适应网格细化应用优化多GPU负载分布，并将现有基于矩阵的化学动力学公式适应多网格环境。

Result: 在氢-空气爆轰和超声速横流射流等代表性燃烧应用中，相比初始GPU实现实现了2-5倍的性能提升，在1-96个NVIDIA H100 GPU上实现了接近理想的弱扩展性。屋顶线分析显示对流（约10倍）和化学（约4倍）例程的算术强度显著提高。

Conclusion: 该求解器有效利用了GPU内存带宽和计算资源，为高速化学反应流的高性能计算提供了有效的解决方案。

Abstract: High-speed chemically active flows present significant computational
challenges due to their disparate space and time scales, where stiff chemistry
often dominates simulation time. While modern supercomputing scientific codes
achieve exascale performance by leveraging graphics processing units (GPUs),
existing GPU-based compressible combustion solvers face critical limitations in
memory management, load balancing, and handling the highly localized nature of
chemical reactions. To this end, we present a high-performance compressible
reacting flow solver built on the AMReX framework and optimized for multi-GPU
settings. Our approach addresses three GPU performance bottlenecks: memory
access patterns through column-major storage optimization, computational
workload variability via a bulk-sparse integration strategy for chemical
kinetics, and multi-GPU load distribution for adaptive mesh refinement
applications. The solver adapts existing matrix-based chemical kinetics
formulations to multigrid contexts. Using representative combustion
applications including hydrogen-air detonations and jet in supersonic crossflow
configurations, we demonstrate $2-5\times$ performance improvements over
initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA
H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic
intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4
\times$) routines, confirming efficient utilization of GPU memory bandwidth and
computational resources.

</details>


### [3] [Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System](https://arxiv.org/abs/2510.24175)
*Nitin Shukla,Alessandro Romeo,Caterina Caravita,Michael Redenti,Radim Vavrik,Lubomir Riha,Andrea Mignone,Marco Rossazza,Stefano Truzzi,Luca Tornatore,Antonio Ragagnin,Tiago Castro,Geray S. Karademir,Klaus Dolag,Pranab J. Deka,Fabio Bacchini,Rostislav-Paul Wilhelm,Daniele Gregori,Elisabetta Boella*

Main category: cs.DC

TL;DR: SPACE-CoE中心通过优化gPLUTO、OpenGadget3和iPIC3D三个旗舰代码，在Leonardo系统上实现了高达1,024个GPU的80%可扩展性，为大规模天体物理模拟提供支持。


<details>
  <summary>Details</summary>
Motivation: 为现有和下一代加速器开发和重新设计天体物理、宇宙学和空间等离子体数值代码，以支持大规模模拟，应对exascale计算时代的挑战。

Method: 通过SPACE-CoE中心促进科学家、代码开发者和高性能计算专家之间的合作，使用性能分析工具在CINECA的Leonardo系统上对三个旗舰代码进行单节点和多节点性能分析。

Result: 初步测试显示所有三个代码都能高效扩展，在1,024个GPU上达到80%的可扩展性。

Conclusion: SPACE-CoE中心的协作策略成功优化了天体物理模拟代码在exascale系统上的性能，为大规模科学计算提供了有效解决方案。

Abstract: Developing and redesigning astrophysical, cosmological, and space plasma
numerical codes for existing and next-generation accelerators is critical for
enabling large-scale simulations. To address these challenges, the SPACE Center
of Excellence (SPACE-CoE) fosters collaboration between scientists, code
developers, and high-performance computing experts to optimize applications for
the exascale era. This paper presents our strategy and initial results on the
Leonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3
and iPIC3D, using profiling tools to analyze performance on single and multiple
nodes. Preliminary tests show all three codes scale efficiently, reaching 80%
scalability up to 1,024 GPUs.

</details>


### [4] [CoMPSeT: A Framework for Comparing Multiparty Session Types](https://arxiv.org/abs/2510.24205)
*Telmo Ribeiro,José Proença,Mário Florido*

Main category: cs.DC

TL;DR: CoMPSeT是一个开源工具，用于分析和比较多参与方会话类型(MPST)的不同特性，帮助研究者和教师更好地理解全局编排协议。


<details>
  <summary>Details</summary>
Motivation: 并发系统设计复杂，现有的MPST变体各有特定功能和特点，缺乏统一的比较工具来清晰展示不同特性。

Method: 选择代表性MPST示例，提供机制来组合不同特性，动画化并比较具体示例的语义，工具开源且可在浏览器中直接执行。

Result: 开发了CoMPSeT工具，能够直观展示不同MPST特性的语义差异，支持交互式学习和比较。

Conclusion: CoMPSeT为MPST研究者和教师提供了有效的工具，有助于更好地理解全局编排协议的不同特性。

Abstract: Concurrent systems are often complex and difficult to design. Choreographic
languages, such as Multiparty Session Types (MPST), allow the description of
global protocols of interactions by capturing valid patterns of interactions
between participants. Many variations of MPST exist, each one with its rather
specific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that
provides clearer insights over different features in existing MPST. We select a
representative set of MPST examples and provide mechanisms to combine different
features and to animate and compare the semantics of concrete examples. CoMPSeT
is open-source, compiled into JavaScript, and can be directly executed from any
browser, becoming useful both for researchers who want to better understand the
landscape of MPST and for teachers who want to explain global choreographies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [5] [Short Ticketing Detection Framework Analysis Report](https://arxiv.org/abs/2510.23619)
*Yuyang Miao,Huijun Xing,Danilo P. Mandic,Tony G. Constantinides*

Main category: cs.CR

TL;DR: 该研究提出了一个无监督多专家机器学习框架，用于检测铁路系统中的短票欺诈，通过A/B/C/D车站分类系统识别了30个高风险车站的可疑模式，并发现了5种不同的短票模式。


<details>
  <summary>Details</summary>
Motivation: 铁路系统中的短票欺诈造成重大经济损失，需要有效的检测方法来识别这种欺诈行为并减少损失。

Method: 采用四种互补算法：隔离森林、局部异常因子、一类支持向量机和马氏距离，构建无监督多专家机器学习框架，并建立A/B/C/D车站分类系统。

Result: 成功识别了30个高风险车站的可疑模式，发现了5种不同的短票欺诈模式，并展示了短票回收的潜力。

Conclusion: 该无监督多专家机器学习框架能有效检测铁路系统中的短票欺诈，为交通系统的欺诈检测和损失恢复提供了实用解决方案。

Abstract: This report presents a comprehensive analysis of an unsupervised multi-expert
machine learning framework for detecting short ticketing fraud in railway
systems. The study introduces an A/B/C/D station classification system that
successfully identifies suspicious patterns across 30 high-risk stations. The
framework employs four complementary algorithms: Isolation Forest, Local
Outlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include
the identification of five distinct short ticketing patterns and potential for
short ticketing recovery in transportation systems.

</details>


### [6] [SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection](https://arxiv.org/abs/2510.23643)
*Zhixin Pan,Ziyu Shu,Linh Nguyen,Amberbir Alemayoh*

Main category: cs.CR

TL;DR: SAND是一个自监督和自适应NAS驱动的硬件木马检测框架，通过自监督学习实现自动特征提取，利用神经架构搜索动态优化分类器，显著提高了检测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 全球化半导体供应链使硬件木马成为嵌入式系统的重大安全威胁，现有基于机器学习的检测技术存在特征选择随意和缺乏自适应性的问题，限制了其在不同木马攻击中的有效性。

Method: 提出SAND框架：1）利用自监督学习实现自动特征提取，消除对人工设计特征的依赖；2）集成神经架构搜索动态优化下游分类器，只需少量微调即可适应未见过的基准测试。

Result: 实验结果显示，SAND相比最先进方法检测精度显著提升（最高18.3%），对规避性木马具有高弹性，并表现出强大的泛化能力。

Conclusion: SAND框架通过自监督学习和神经架构搜索的结合，为硬件木马检测提供了高效且自适应的解决方案，显著提升了检测性能和泛化能力。

Abstract: The globalized semiconductor supply chain has made Hardware Trojans (HT) a
significant security threat to embedded systems, necessitating the design of
efficient and adaptable detection mechanisms. Despite promising machine
learning-based HT detection techniques in the literature, they suffer from ad
hoc feature selection and the lack of adaptivity, all of which hinder their
effectiveness across diverse HT attacks. In this paper, we propose SAND, a
selfsupervised and adaptive NAS-driven framework for efficient HT detection.
Specifically, this paper makes three key contributions. (1) We leverage
self-supervised learning (SSL) to enable automated feature extraction,
eliminating the dependency on manually engineered features. (2) SAND integrates
neural architecture search (NAS) to dynamically optimize the downstream
classifier, allowing for seamless adaptation to unseen benchmarks with minimal
fine-tuning. (3) Experimental results show that SAND achieves a significant
improvement in detection accuracy (up to 18.3%) over state-of-the-art methods,
exhibits high resilience against evasive Trojans, and demonstrates strong
generalization.

</details>


### [7] [Attack on a PUF-based Secure Binary Neural Network](https://arxiv.org/abs/2510.24422)
*Bijeet Basak,Nupur Patil,Kurian Polachan,Srinivas Vivek*

Main category: cs.CR

TL;DR: 本文展示了对基于PUF保护的BNN模型的攻击方法，能够通过差分密码分析恢复PUF密钥和模型参数。


<details>
  <summary>Details</summary>
Motivation: 由于忆阻器非易失性，部署在忆阻交叉阵列上的BNN容易受到物理攻击。虽然已有PUF方案保护BNN，但本文发现该方案存在漏洞。

Method: 采用差分密码分析方法，通过观察模型准确率变化来逐位恢复PUF密钥，最终获取BNN模型参数。

Result: 在MNIST数据集上测试，攻击能够恢复85%的PUF密钥，恢复的BNN模型达到93%的分类准确率（原模型96%）。攻击效率高，仅需几分钟即可完成。

Conclusion: 现有的PUF保护方案存在安全漏洞，需要更强的安全机制来保护BNN模型。

Abstract: Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays
provide energy-efficient solutions for edge computing but are susceptible to
physical attacks due to memristor nonvolatility. Recently, Rajendran et al.
(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function
(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the
weight and bias matrices of the BNN layers were secured by swapping columns
based on device's PUF key bits.
  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable
to PUF-key recovery attack. As a consequence of our attack, we recover the
secret weight and bias matrices of the BNN. Our approach is motivated by
differential cryptanalysis and reconstructs the PUF key bit-by-bit by observing
the change in model accuracy, and eventually recovering the BNN model
parameters. Evaluated on a BNN trained on the MNIST dataset, our attack could
recover 85% of the PUF key, and recover the BNN model up to 93% classification
accuracy compared to the original model's 96% accuracy. Our attack is very
efficient and it takes a couple of minutes to recovery the PUF key and the
model parameters.

</details>


### [8] [EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet](https://arxiv.org/abs/2510.23847)
*Joel Poncha Lemayian,Ghyslain Gagnon,Kaiwen Zhang,Pascal Giard*

Main category: cs.CR

TL;DR: EthVault是首个用于以太坊分层确定性冷钱包的硬件架构，通过硬件实现关键算法来安全生成密钥，并提出抗侧信道和时序攻击的ECC架构，同时最小化资源使用。


<details>
  <summary>Details</summary>
Motivation: 现有的加密货币钱包通常是基于软件的微控制器应用，容易受到恶意软件和侧信道攻击，攻击者可以通过针对ECC等关键算法提取私钥。

Method: 提出EthVault硬件架构，包括硬件实现的密钥生成算法、抗侧信道和时序攻击的ECC架构，以及子密钥派生函数架构，并在FPGA上实现验证。

Result: FPGA实现验证了方法的可行性，ECC架构在不同输入下表现出统一的执行行为，完整设计在Xilinx Zynq UltraScale+ FPGA上仅使用了27%的LUT、7%的寄存器和6%的RAM块。

Conclusion: EthVault提供了一种安全、资源高效的硬件钱包解决方案，能够有效抵御侧信道和时序攻击，满足市场对小型便携式加密货币钱包的需求。

Abstract: Cryptocurrency blockchain networks safeguard digital assets using
cryptographic keys, with wallets playing a critical role in generating,
storing, and managing these keys. Wallets, typically categorized as hot and
cold, offer varying degrees of security and convenience. However, they are
generally software-based applications running on microcontrollers.
Consequently, they are vulnerable to malware and side-channel attacks, allowing
perpetrators to extract private keys by targeting critical algorithms, such as
ECC, which processes private keys to generate public keys and authorize
transactions. To address these issues, this work presents EthVault, the first
hardware architecture for an Ethereum hierarchically deterministic cold wallet,
featuring hardware implementations of key algorithms for secure key generation.
Also, an ECC architecture resilient to side-channel and timing attacks is
proposed. Moreover, an architecture of the child key derivation function, a
fundamental component of cryptocurrency wallets, is proposed. The design
minimizes resource usage, meeting market demand for small, portable
cryptocurrency wallets. FPGA implementation results validate the feasibility of
the proposed approach. The ECC architecture exhibits uniform execution behavior
across varying inputs, while the complete design utilizes only 27%, 7%, and 6%
of LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+
FPGA.

</details>


### [9] [PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs](https://arxiv.org/abs/2510.23891)
*Jiaqi Xue,Yifei Zhao,Mansour Al Ghanim,Shangqian Gao,Ruimin Sun,Qian Lou,Mengxin Zheng*

Main category: cs.CR

TL;DR: PRO是一种针对开源大语言模型的精确鲁棒文本水印方法，通过联合训练水印策略模型与LLM，解决了开源模型水印检测性差和对下游修改脆弱的问题。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型缺乏实用的文本来源验证手段，现有方法难以在模型权重中直接嵌入水印而不损害检测性，且蒸馏方法存在检测性差和对下游修改脆弱的问题。

Method: PRO联合训练水印策略模型与LLM，生成易于模型学习且与检测标准一致的模式，通过正则化项模拟下游扰动并惩罚水印检测性退化，确保模型编辑下的鲁棒性。

Result: 在LLaMA-3.2、LLaMA-3、Phi-2等开源LLM上的实验表明，PRO显著提高了水印检测性和对模型修改的韧性。

Conclusion: PRO为开源大语言模型提供了一种精确且鲁棒的文本水印解决方案，有效解决了检测性差和模型修改脆弱性的核心挑战。

Abstract: Text watermarking for large language models (LLMs) enables model owners to
verify text origin and protect intellectual property. While watermarking
methods for closed-source LLMs are relatively mature, extending them to
open-source models remains challenging, as developers cannot control the
decoding process. Consequently, owners of open-source LLMs lack practical means
to verify whether text was generated by their models. A core difficulty lies in
embedding watermarks directly into model weights without hurting detectability.
A promising idea is to distill watermarks from a closed-source model into an
open one, but this suffers from (i) poor detectability due to mismatch between
learned and predefined patterns, and (ii) fragility to downstream modifications
such as fine-tuning or model merging. To overcome these limitations, we propose
PRO, a Precise and Robust text watermarking method for open-source LLMs. PRO
jointly trains a watermark policy model with the LLM, producing patterns that
are easier for the model to learn and more consistent with detection criteria.
A regularization term further simulates downstream perturbations and penalizes
degradation in watermark detectability, ensuring robustness under model edits.
Experiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO
substantially improves both watermark detectability and resilience to model
modifications.

</details>


### [10] [Victim as a Service: Designing a System for Engaging with Interactive Scammers](https://arxiv.org/abs/2510.23927)
*Daniel Spokoyny,Nikolai Vogler,Xin Gao,Tianyi Zheng,Yufei Weng,Jonghyun Park,Jiajun Jiao,Geoffrey M. Voelker,Stefan Savage,Taylor Berg-Kirkpatrick*

Main category: cs.CR

TL;DR: 本文介绍了CHATTERBOX系统，这是一个基于LLM的系统，用于自动化与在线诈骗者进行长期互动，以大规模调查他们的策略。


<details>
  <summary>Details</summary>
Motivation: 杀猪盘等在线诈骗通过长期对话建立信任来降低受害者防御，造成巨大经济损失（至少750亿美元），但由于其长期对话性质，难以大规模调查。

Method: 开发了CHATTERBOX系统，使用LLM技术自动化与诈骗者进行长期互动，包括吸引诈骗尝试的技术、系统设计和LLM工程，以及满足或规避诈骗者工作流程中"里程碑"的必要能力。

Result: 该系统使得大规模调查诈骗者策略成为可能。

Conclusion: CHATTERBOX系统为解决长期在线诈骗调查的挑战提供了有效方案。

Abstract: Pig butchering, and similar interactive online scams, lower their victims'
defenses by building trust over extended periods of conversation - sometimes
weeks or months. They have become increasingly public losses (at least $75B by
one recent study). However, because of their long-term conversational nature,
they are extremely challenging to investigate at scale. In this paper, we
describe the motivation, design, implementation, and experience with
CHATTERBOX, an LLM-based system that automates long-term engagement with online
scammers, making large-scale investigations of their tactics possible. We
describe the techniques we have developed to attract scam attempts, the system
and LLM-engineering required to convincingly engage with scammers, and the
necessary capabilities required to satisfy or evade "milestones" in scammers'
workflow.

</details>


### [11] [Scalable GPU-Based Integrity Verification for Large Machine Learning Models](https://arxiv.org/abs/2510.23938)
*Marcin Spoczynski,Marcela S. Melara*

Main category: cs.CR

TL;DR: 提出一个安全框架，通过在CPU和GPU平台上标准化完整性保护并显著减少验证开销来加强分布式机器学习。该方法将完整性验证直接与GPU加速器上的大型ML模型执行协同定位，解决了传统CPU验证与GPU计算不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型机器学习工作负载主要在GPU上运行，而安全验证传统上在单独的基于CPU的进程上运行之间的根本不匹配问题，提供即时性能优势和长期架构一致性。

Method: 使用专用计算单元（如Intel Arc的XMX单元、NVIDIA的Tensor Cores）在GPU上本地执行加密操作，利用GPU的高内存带宽和并行处理原语，确保完整性检查与模型执行同步。

Result: 消除了传统基于CPU的验证系统在处理大型模型时可能出现的架构瓶颈，即使对于超过100GB的大规模模型，完整性检查也能跟上模型执行速度。

Conclusion: 该框架建立了一个通用的完整性验证机制，可在不同的GPU供应商和硬件配置中一致工作，为企业团队提供了一个硬件无关的基础，无论其底层CPU和GPU基础设施如何都可以部署。

Abstract: We present a security framework that strengthens distributed machine learning
by standardizing integrity protections across CPU and GPU platforms and
significantly reducing verification overheads. Our approach co-locates
integrity verification directly with large ML model execution on GPU
accelerators, resolving the fundamental mismatch between how large ML workloads
typically run (primarily on GPUs) and how security verifications traditionally
operate (on separate CPU-based processes), delivering both immediate
performance benefits and long-term architectural consistency. By performing
cryptographic operations natively on GPUs using dedicated compute units (e.g.,
Intel Arc's XMX units, NVIDIA's Tensor Cores), our solution eliminates the
potential architectural bottlenecks that could plague traditional CPU-based
verification systems when dealing with large models. This approach leverages
the same GPU-based high-memory bandwidth and parallel processing primitives
that power ML workloads ensuring integrity checks keep pace with model
execution even for massive models exceeding 100GB. This framework establishes a
common integrity verification mechanism that works consistently across
different GPU vendors and hardware configurations. By anticipating future
capabilities for creating secure channels between trusted execution
environments and GPU accelerators, we provide a hardware-agnostic foundation
that enterprise teams can deploy regardless of their underlying CPU and GPU
infrastructures.

</details>


### [12] [Traceable Signatures from Lattices](https://arxiv.org/abs/2510.24101)
*Nam Tran,Khoa Nguyen,Dongxi Liu,Josef Pieprzyk,Willy Susilo*

Main category: cs.CR

TL;DR: 提出了首个基于格的、在量子随机预言机模型下可证明安全的可追踪签名方案，解决了现有基于数论/配对假设的方案在量子计算机面前可能不安全的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的可追踪签名方案都基于数论/配对假设，在量子计算机存在的情况下可能不安全，因此需要开发基于格的抗量子安全方案。

Method: 基于格密码学构建可追踪签名方案，并在量子随机预言机模型下进行安全性证明。

Result: 成功构建了首个基于格的可追踪签名方案，并在QROM下证明了其安全性。

Conclusion: 该工作为可追踪签名提供了抗量子安全的替代方案，填补了现有方案在量子安全方面的空白。

Abstract: Traceable signatures (Kiayas et al., EUROCRYPT 2004) is an anonymous digital
signature system that extends the tracing power of the opening authority in
group signatures. There are many known constructions of traceable signatures,
but all are based on number-theoretic/pairing assumptions. For such reason,
they may not be secure in the presence of quantum computers. This work revisits
the notion of traceable signatures and presents a lattice-based construction
provably secure in the quantum random oracle model (QROM).

</details>


### [13] [Demystifying Cookie Sharing Risks in WebView-based Mobile App-in-app Ecosystems](https://arxiv.org/abs/2510.24141)
*Miao Zhang,Shenao Wang,Guilin Zheng,Yanjie Zhao,Haoyu Wang*

Main category: cs.CR

TL;DR: 本文发现了一种名为跨小程序Cookie共享（CMCS）的新漏洞，该漏洞源于小程序中web-view组件的共享环境，允许不同小程序之间未经授权地交换数据，破坏了隔离机制。


<details>
  <summary>Details</summary>
Motivation: 小程序采用web-view组件破坏了原有的隔离机制，暴露出新的攻击面和漏洞，需要研究这些安全风险的实际影响。

Method: 分析了四大平台（微信、支付宝、抖音、百度）的web-view机制，开发了MiCoScan静态分析工具，通过web-view上下文建模和跨web-view数据流分析来检测受CMCS漏洞影响的小程序。

Result: 在351,483个小程序的大规模分析中，发现45,448个共享web-view域的集群，7,965个特权数据传输实例，9,877个小程序易受合谋攻击。

Conclusion: CMCS漏洞在小程序生态中广泛存在且带来重大安全风险，迫切需要改进隔离机制。

Abstract: Mini-programs, an emerging mobile application paradigm within super-apps,
offer a seamless and installation-free experience. However, the adoption of the
web-view component has disrupted their isolation mechanisms, exposing new
attack surfaces and vulnerabilities. In this paper, we introduce a novel
vulnerability called Cross Mini-program Cookie Sharing (CMCS), which arises
from the shared web-view environment across mini-programs. This vulnerability
allows unauthorized data exchange across mini-programs by enabling one
mini-program to access cookies set by another within the same web-view context,
violating isolation principles. As a preliminary step, we analyzed the web-view
mechanisms of four major platforms, including WeChat, AliPay, TikTok, and
Baidu, and found that all of them are affected by CMCS vulnerabilities.
Furthermore, we demonstrate the collusion attack enabled by CMCS, where
privileged mini-programs exfiltrate sensitive user data via cookies accessible
to unprivileged mini-programs. To measure the impact of collusion attacks
enabled by CMCS vulnerabilities in the wild, we developed MiCoScan, a static
analysis tool that detects mini-programs affected by CMCS vulnerabilities.
MiCoScan employs web-view context modeling to identify clusters of
mini-programs sharing the same web-view domain and cross-webview data flow
analysis to detect sensitive data transmissions to/from web-views. Using
MiCoScan, we conducted a large-scale analysis of 351,483 mini-programs,
identifying 45,448 clusters sharing web-view domains, 7,965 instances of
privileged data transmission, and 9,877 mini-programs vulnerable to collusion
attacks. Our findings highlight the widespread prevalence and significant
security risks posed by CMCS vulnerabilities, underscoring the urgent need for
improved isolation mechanisms in mini-program ecosystems.

</details>


### [14] [Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers](https://arxiv.org/abs/2510.24393)
*Yan Meng,Jiachun Li,Matthew Pillari,Arjun Deopujari,Liam Brennan,Hafsah Shamsie,Haojin Zhu,Yuan Tian*

Main category: cs.CR

TL;DR: 该论文提出了一种名为阵列指纹的新型活体检测特征，利用智能音箱内置的麦克风阵列来区分真实人声和重放语音攻击，并开发了ARRAYID轻量级检测方案，在包含32,780个音频样本的数据集上达到99.84%的准确率。


<details>
  <summary>Details</summary>
Motivation: 智能音箱在智能家居系统中扮演重要角色，但容易受到语音欺骗攻击。现有的被动活体检测方法面临环境因素变化导致性能下降以及固定用户姿势要求的挑战。

Method: 提出阵列指纹特征，利用智能音箱固有的圆形布局麦克风阵列来确定音频身份；开发ARRAYID轻量级被动检测方案，结合一系列与阵列指纹协同工作的特征。

Result: 在包含32,780个音频样本和14种欺骗设备的数据集上评估，ARRAYID达到99.84%的准确率，优于现有的被动活体检测方案。

Conclusion: 阵列指纹特征在环境变化和用户移动情况下比现有方案具有更强的鲁棒性，ARRAYID检测方案在语音欺骗攻击检测方面表现出卓越性能。

Abstract: Though playing an essential role in smart home systems, smart speakers are
vulnerable to voice spoofing attacks. Passive liveness detection, which
utilizes only the collected audio rather than the deployed sensors to
distinguish between live-human and replayed voices, has drawn increasing
attention. However, it faces the challenge of performance degradation under the
different environmental factors as well as the strict requirement of the fixed
user gestures.
  In this study, we propose a novel liveness feature, array fingerprint, which
utilizes the microphone array inherently adopted by the smart speaker to
determine the identity of collected audios. Our theoretical analysis
demonstrates that by leveraging the circular layout of microphones, compared
with existing schemes, array fingerprint achieves a more robust performance
under the environmental change and user's movement. Then, to leverage such a
fingerprint, we propose ARRAYID, a lightweight passive detection scheme, and
elaborate a series of features working together with array fingerprint. Our
evaluation on the dataset containing 32,780 audio samples and 14 spoofing
devices shows that ARRAYID achieves an accuracy of 99.84%, which is superior to
existing passive liveness detection schemes.

</details>


### [15] [Uncovering Gaps Between RFC Updates and TCP/IP Implementations: LLM-Facilitated Differential Checks on Intermediate Representations](https://arxiv.org/abs/2510.24408)
*Yifan Wu,Xuewei Feng,Yuxiang Yang,Ke Xu*

Main category: cs.CR

TL;DR: 提出基于LLM和差分模型的自动化分析框架，用于检测TCP/IP协议栈实现与RFC标准之间的不一致性，识别潜在安全漏洞。


<details>
  <summary>Details</summary>
Motivation: TCP/IP协议栈实现与RFC标准之间存在不一致性，可能导致功能差异和严重安全漏洞。现有方法依赖预定义模式或基于规则的方法，无法跨不同协议规范泛化，自动化检测仍面临挑战。

Method: 基于LLM和差分模型的自动化分析框架，通过建模协议的迭代关系，基于RFC标准的迭代更新关系，对不同版本内核代码实现进行增量代码功能分析，自动执行代码检测和漏洞分析。

Result: 通过广泛评估验证了框架的有效性，证明其在识别由RFC代码不一致性引起的潜在漏洞方面的有效性。

Conclusion: 提出的基于LLM和差分模型的自动化分析框架能够有效检测协议栈实现与RFC标准之间的不一致性，识别潜在安全漏洞。

Abstract: As the core of the Internet infrastructure, the TCP/IP protocol stack
undertakes the task of network data transmission. However, due to the
complexity of the protocol and the uncertainty of cross-layer interaction,
there are often inconsistencies between the implementation of the protocol
stack code and the RFC standard. This inconsistency may not only lead to
differences in protocol functions but also cause serious security
vulnerabilities. At present, with the continuous expansion of protocol stack
functions and the rapid iteration of RFC documents, it is increasingly
important to detect and fix these inconsistencies. With the rise of large
language models, researchers have begun to explore how to extract protocol
specifications from RFC documents through these models, including protocol
stack modeling, state machine extraction, text ambiguity analysis, and other
related content. However, existing methods rely on predefined patterns or
rule-based approaches that fail to generalize across different protocol
specifications. Automated and scalable detection of these inconsistencies
remains a significant challenge. In this study, we propose an automated
analysis framework based on LLM and differential models. By modeling the
iterative relationship of the protocol and based on the iterative update
relationship of the RFC standard, we perform incremental code function analysis
on different versions of kernel code implementations to automatically perform
code detection and vulnerability analysis. We conduct extensive evaluations to
validate the effectiveness of our framework, demonstrating its effectiveness in
identifying potential vulnerabilities caused by RFC code inconsistencies.

</details>


### [16] [Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference](https://arxiv.org/abs/2510.24498)
*Tejaswini Bollikonda*

Main category: cs.CR

TL;DR: 提出了一个云原生同态加密框架，用于隐私保护的机器学习推理，通过容器化和Kubernetes编排实现分布式加密计算，优化策略使推理速度提升3.2倍，内存使用减少40%。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在云基础设施中的部署，用户数据在推理过程中的机密性成为重要安全挑战。同态加密虽然能实现加密数据上的计算，但在大规模云原生管道中的集成仍受限于高计算开销、编排复杂性和模型兼容性问题。

Method: 提出了系统化框架，集成容器化HE模块与Kubernetes编排，实现弹性扩展和分布式加密计算。采用密文打包、多项式模数调整和算子融合等优化策略来减少延迟和资源消耗。

Result: 实验结果显示，相比传统HE管道，该系统实现了高达3.2倍的推理加速和40%的内存使用减少。

Conclusion: 这些发现为在零信任云条件下部署保证数据机密性的安全MLaaS系统提供了实用路径。

Abstract: As machine learning (ML) models become increasingly deployed through cloud
infrastructures, the confidentiality of user data during inference poses a
significant security challenge. Homomorphic Encryption (HE) has emerged as a
compelling cryptographic technique that enables computation on encrypted data,
allowing predictions to be generated without decrypting sensitive inputs.
However, the integration of HE within large scale cloud native pipelines
remains constrained by high computational overhead, orchestration complexity,
and model compatibility issues.
  This paper presents a systematic framework for the design and optimization of
cloud native homomorphic encryption workflows that support privacy-preserving
ML inference. The proposed architecture integrates containerized HE modules
with Kubernetes-based orchestration, enabling elastic scaling and parallel
encrypted computation across distributed environments. Furthermore,
optimization strategies including ciphertext packing, polynomial modulus
adjustment, and operator fusion are employed to minimize latency and resource
consumption while preserving cryptographic integrity. Experimental results
demonstrate that the proposed system achieves up to 3.2times inference
acceleration and 40% reduction in memory utilization compared to conventional
HE pipelines. These findings illustrate a practical pathway for deploying
secure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality
under zero-trust cloud conditions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems](https://arxiv.org/abs/2510.24112)
*Junchi Wu,Xinfei Wan,Zhuoran Li,Yuyang Jin,Guangyu Sun,Yun Liang,Diyu Zhou,Youwei Zhuo*

Main category: cs.AR

TL;DR: SlowPoke是一个轻量级硬件感知框架，用于在多核架构上进行片上故障慢速检测，通过编译器插桩、在线跟踪压缩和拓扑感知排名算法，显著降低存储开销并提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 多核架构对高性能计算至关重要，但其性能受到广泛存在的故障慢速问题的严重影响。现有的分布式系统方法由于严格的内存限制和无法跟踪硬件拓扑中的故障，不适合片上检测。

Method: 结合编译器插桩进行低开销监控，使用在线跟踪压缩在千字节内存内运行，并采用新颖的拓扑感知排名算法来定位故障的根本原因。

Result: 在代表性多核工作负载上的评估显示，SlowPoke将检测跟踪的存储开销平均降低了115.9倍，实现了86.77%的平均故障慢速检测准确率和12.11%的误报率。

Conclusion: SlowPoke在不同多核架构上都能有效扩展，使其适用于大规模部署，为片上故障慢速检测提供了实用的解决方案。

Abstract: Many-core architectures are essential for high-performance computing, but
their performance is undermined by widespread fail-slow failures. Detecting
such failures on-chip is challenging, as prior methods from distributed systems
are unsuitable due to strict memory limits and their inability to track
failures across the hardware topology. This paper introduces SlowPoke, a
lightweight, hardware-aware framework for practical on-chip fail-slow
detection. SlowPoke combines compiler-based instrumentation for low-overhead
monitoring, on-the-fly trace compression to operate within kilobytes of memory,
and a novel topology-aware ranking algorithm to pinpoint a failure's root
cause. We evaluate SlowPoke on a wide range of representative many-core
workloads, and the results demonstrate that SlowPoke reduces the storage
overhead of detection traces by an average of 115.9$\times$, while achieving an
average fail-slow detection accuracy of 86.77% and a false positive rate (FPR)
of 12.11%. More importantly, SlowPoke scales effectively across different
many-core architectures, making it practical for large-scale deployments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS是一个通用游戏智能体，使用统一、可扩展的动作空间进行训练，基于人类对齐的键盘鼠标输入。通过大规模持续预训练，在多种领域（操作系统、网页、模拟游戏）中表现出色，在多个游戏基准测试中超越现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够跨异构领域（操作系统、网页、模拟游戏）工作的通用游戏智能体，通过统一、可扩展的动作空间实现大规模持续预训练，解决现有API或GUI方法的局限性。

Method: 使用基于人类对齐键盘鼠标输入的统一动作空间；进行超过500B token的大规模预训练，包含多样化轨迹和多模态数据；采用衰减持续损失减少因果混淆；使用高效的稀疏思考策略平衡推理深度和推理成本。

Result: 在开放世界Minecraft任务中成功率比之前最先进模型提高约2倍；在未见过的网页3D游戏中接近新手的通用性；在FPS基准测试中超越GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet；训练时间和测试时间的扩展结果证实统一动作空间在跨游戏和多模态数据扩展时持续改进。

Conclusion: 简单、可扩展的动作表示与大规模预训练相结合，为实现具有广泛计算机使用能力的通用智能体提供了一条有前景的路径。

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [19] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 本文探讨人工智能在科学问题解决中的作用，重点关注其对学科创造力的影响。通过区分创造性方法和创造性产品，提出学科创造力的概念，并通过数学案例说明AI可能取代而非扩展学科创造力。


<details>
  <summary>Details</summary>
Motivation: 研究AI在科学问题解决中的角色，特别关注其对学科创造力的潜在影响，探讨AI可能如何改变科学追求的价值。

Method: 基于创造力哲学的最新研究，区分创造性方法和创造性产品，引入学科创造力的概念，并通过两个数学案例进行分析。

Result: 研究发现计算可以扩展学科创造力，但某些涉及AI的方法可能取代学科创造力，这种取代有可能改变科学追求的价值。

Conclusion: AI在科学问题解决中的应用需要谨慎，因为它可能通过取代学科创造力而改变科学追求的本质和价值。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [20] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: 本文提出多环境POMDPs（ME-POMDPs）及其扩展形式AB-POMDPs，用于处理具有离散模型不确定性的POMDP问题，旨在寻找对环境中所有可能变化都具有鲁棒性的单一策略。


<details>
  <summary>Details</summary>
Motivation: 当多个领域专家对问题建模存在分歧时，需要一种能够处理模型不确定性的框架。ME-POMDPs扩展了标准POMDP，通过考虑一组可能的环境模型来寻找鲁棒策略。

Method: 1）将ME-POMDPs推广为具有初始信念集合的AB-POMDPs；2）证明任意ME-POMDP可简化为仅变化转移和奖励函数或仅变化观测和奖励函数的形式；3）开发精确和近似（基于点）的算法来计算鲁棒策略。

Result: 成功为标准POMDP基准测试的多环境扩展版本计算了策略，验证了所提方法的有效性。

Conclusion: ME-POMDPs和AB-POMDPs为处理模型不确定性提供了有效的框架，所开发的算法能够在多环境设置下计算鲁棒策略。

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [21] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: 提出了一种基于测试时调优的框架，通过增强预训练transformer模型直接从串联质谱和分子式进行端到端从头分子结构生成，无需依赖数据库匹配或中间步骤，在两个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前串联质谱分析方法依赖数据库匹配或需要中间片段预测的多步骤流程，难以识别参考数据库中不存在的化合物，限制了在代谢组学、天然产物发现等领域的应用。

Method: 利用测试时调优增强预训练transformer模型，直接从串联质谱和分子式进行端到端分子结构生成，无需人工标注和中间步骤，能够动态适应新的质谱数据。

Result: 在NPLIB1和MassSpecGym两个基准测试中分别超越DiffMS方法100%和20%，测试时调优相比传统微调在MassSpecGym上获得62%的相对性能提升。

Conclusion: 该框架能够生成结构准确的分子候选物，即使预测偏离真实值，仍能为人工解释提供有价值指导，实现更可靠的化合物识别。

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [22] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 该研究探讨了生成式AI在象棋谜题领域的创造力，开发了一个能生成具有美学吸引力、新颖性、反直觉和独特解法的AI系统，并由三位国际象棋专家评估其创造性。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI是否能产生创造性和新颖的输出，特别是在象棋谜题这一需要高度创造力的领域。

Method: 开发了一个AI系统来生成象棋谜题，然后将精选的AI生成谜题提交给三位世界知名象棋专家（国际象棋排局大师Amatzia Avni、特级大师Jonathan Levitt和Matthew Sadler）进行评估。

Result: 三位专家从创造力、挑战水平和美学设计等角度评估了AI生成的谜题，并选择了他们最喜欢的谜题。

Conclusion: 该研究表明生成式AI在象棋谜题领域能够产生具有创造性和美学价值的输出，得到了专业象棋专家的认可。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [23] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 病理学基础模型在癌症诊断和预后方面表现不佳，存在诊断准确率低、鲁棒性差、几何不稳定、计算需求大和安全漏洞等问题，主要源于通用基础模型假设与人体组织内在复杂性之间的概念不匹配。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在计算机视觉和语言处理领域取得了突破，但在计算病理学中的快速应用并未带来预期的癌症诊断和预后突破，反而暴露出系统性弱点。

Method: 通过系统评估现有病理学基础模型，识别其根本弱点，并分析导致这些问题的七个相互关联的原因。

Result: 发现当前病理学基础模型存在诊断准确率低、鲁棒性差、几何不稳定、计算需求大和安全漏洞等系统性缺陷。

Conclusion: 病理学基础模型在概念上与组织形态学本质不匹配，需要对范式本身进行根本性重新思考。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [24] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: ReCAP是一个用于大语言模型的分层推理规划框架，通过计划分解、父计划结构化重注入和内存高效执行机制，解决长时程任务中的上下文漂移和目标信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在需要多步推理和动态重规划的长时程任务中面临的挑战，包括顺序提示方法的上下文漂移、目标信息丢失和重复失败循环，以及分层提示方法的跨级连续性减弱和运行时开销问题。

Method: 结合三个关键机制：(i) 计划提前分解：生成完整子任务列表，执行第一项并优化剩余部分；(ii) 结构化重注入父计划：在递归返回时保持一致的多级上下文；(iii) 内存高效执行：限制活动提示，使成本随任务深度线性扩展。

Result: 在多个长时程推理基准测试中显著提高了子目标对齐和成功率，在同步Robotouille上获得32%的提升，在异步Robotouille上获得29%的改进（严格pass@1协议）。

Conclusion: ReCAP框架通过将高层目标与低层动作对齐、减少冗余提示和保持跨递归的连贯上下文更新，有效提升了长时程任务的推理和规划能力。

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [25] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: 该研究提出了一种去中心化的多智能体路径规划目标分配方法，智能体基于环境结构化表示独立生成目标偏好排序，通过固定冲突解决规则进行分配，无需协商或迭代协调。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化条件下多智能体在共享环境中的协调挑战，探索语言模型在去中心化目标分配中的潜力。

Method: 智能体基于网格可视化和场景数据独立生成目标偏好排序，交换排序后通过固定确定性冲突解决规则（如智能体索引排序）进行目标分配。

Result: LLM智能体在精心设计的提示和相关定量信息下，能够实现接近最优的完工时间，并持续优于传统启发式方法。

Conclusion: 语言模型在去中心化多智能体路径规划目标分配中具有巨大潜力，信息结构在此类系统中至关重要。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [26] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的国际象棋谜题生成方法，通过设计基于象棋引擎搜索统计的新型奖励函数，显著提升了谜题的反直觉性、独特性和多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在各个领域快速发展，但在生成真正具有创造性、美学价值和反直觉性的输出方面仍面临挑战，特别是在国际象棋谜题领域。

Method: 首先对生成式AI架构进行基准测试，然后引入基于象棋引擎搜索统计的新型奖励函数的强化学习框架，旨在增强谜题的独特性、反直觉性、多样性和真实性。

Result: 强化学习方法将反直觉谜题生成率从0.22%（监督学习）大幅提升至2.5%，超过了现有数据集（2.1%）和最佳Lichess训练模型（0.4%）。生成的谜题满足新颖性和多样性标准，保留了美学主题，并被人类专家评为比传统书籍谜题更具创造性、趣味性和反直觉性，甚至接近经典作品水平。

Conclusion: 最终成果是一本经过精心筛选的AI生成谜题集，获得了三位世界知名专家的创造力认可，证明了该方法在生成高质量创造性内容方面的有效性。

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [27] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文调查了基于大型语言模型的智能AI系统所面临的安全威胁，提出了威胁分类法，回顾了评估方法，并讨论了技术和治理层面的防御策略。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统在网页、软件和物理环境中自主执行任务的能力带来了新的安全风险，这些风险既不同于传统AI安全，也不同于常规软件安全，需要专门研究。

Method: 采用调查分析方法，构建了智能AI特有的威胁分类法，回顾了最近的基准测试和评估方法，并从技术和治理两个角度分析防御策略。

Result: 系统梳理了当前研究现状，识别了智能AI系统的独特安全威胁，并提出了相应的评估框架和防御方法。

Conclusion: 智能AI系统需要采用安全设计原则，当前研究为开发安全的智能系统提供了基础，但仍存在许多开放挑战需要解决。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [28] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 本文提出了一种基于变分推理的LVLM训练算法，通过稀疏奖励函数和贝叶斯推理扩展策略，在七个推理基准上提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练算法（如SFT、PPO、GRPO）在未见推理任务上泛化能力不足，且过度依赖有偏奖励模型，需要更鲁棒的推理训练方法。

Method: 将LVLM推理重新表述为后验推理问题，采用摊销变分推理的扩展训练算法，引入稀疏奖励函数鼓励多样化高似然潜在CoT，并使用贝叶斯推理扩展策略替代昂贵的搜索方法。

Result: 在七个推理基准上实证表明，该方法在有效性、泛化性和可解释性方面提升了最先进的LVLM模型。

Conclusion: 提出的基于变分推理的训练算法能够有效解决现有方法的局限性，显著提升LVLM在推理任务上的性能表现。

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [29] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出了一种基于直觉主义去中心化框架的因果发现理论——柔道演算，使用j-稳定因果推理和j-do-演算在层拓扑中形式化处理因果效应的环境依赖性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中因果效应依赖于具体环境（如年龄、国家、剂量、基因型等），需要形式化处理这种环境依赖性。

Method: 结合层理论和Lawvere-Tierney模态算子j，开发了柔道演算算法框架，并与基于分数、约束和梯度的标准因果发现方法相结合。

Result: 在合成和真实数据集上的实验显示，基于层理论的去中心化因果发现具有计算效率优势，性能优于经典因果发现方法。

Conclusion: 柔道演算为处理环境依赖的因果发现提供了形式化框架，在计算效率和性能方面表现出优势。

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [30] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: 提出了一种名为符号估计器的新方法，通过将交叉熵损失替换为二元分类损失，解决了传统LLM对齐方法在人类偏好异质性下的不一致性问题，实现了可证明的一致性和多项式有限样本误差界。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法对人类偏好的异质性很脆弱，拟合简单的概率模型到成对比较数据会产生不一致的群体平均效用估计，这是社会福利的规范度量。

Method: 提出符号估计器方法，在聚合步骤中用二元分类损失替换交叉熵损失，在温和假设下恢复一致的有序对齐，并实现该设置下的首个多项式有限样本误差界。

Result: 在基于数字孪生的LLM对齐现实模拟中，符号估计器显著减少了模拟人物面板上的偏好失真，将（角度）估计误差降低了近35%，与真实群体偏好的分歧从12%降至8%，优于标准RLHF。

Conclusion: 符号估计器在保持现有LLM对齐管道实现简单性的同时，优于明确建模用户异质性并需要跟踪个体级偏好数据的面板数据启发式方法。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [31] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 本研究提出了一种结合个体社会基础设施韧性(SIR)的条件深度学习模型，用于预测破坏性事件后个体移动模式的变化。


<details>
  <summary>Details</summary>
Motivation: 预测破坏性事件前的个体移动模式变化具有挑战性，因为缺乏衡量个体异质性社会基础设施韧性的方法，且个体移动模式与空间环境的复杂交互关系未被充分捕捉。

Method: 将个体的SIR纳入条件深度学习模型，利用大规模稀疏个体级数据捕捉个体移动模式与局部空间环境的复杂关系。

Result: 实验表明，结合个体SIR和空间环境可以增强模型预测事件后个体移动模式的能力。条件模型能够捕捉具有相似事件前模式但SIR不同的个体在移动模式上的差异变化。

Conclusion: 该研究证明了将个体社会基础设施韧性纳入预测模型的重要性，为预测破坏性事件后的个体移动行为提供了有效方法。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [32] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast是一个结构化、模块化的跨域时间序列预测框架，通过将时间序列分解为季节性和趋势分量，分别使用定制化生成路径进行建模，在多个领域实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨域时间序列预测中面临领域特定趋势变化和不一致周期性模式的挑战，现有方法将时间序列视为未分化的序列，未能显式解耦其固有结构组件。

Method: 提出OneCast框架：1）将时间序列分解为季节性和趋势分量；2）季节性分量通过轻量级投影模块使用可解释基函数重建周期性模式；3）趋势分量通过语义感知分词器编码为分段级离散token，并通过掩码离散扩散机制进行推断；4）两个分支输出结合生成最终预测。

Result: 在八个领域上的广泛实验表明，OneCast在大多数情况下优于最先进的基线方法。

Conclusion: OneCast通过结构化分解和模块化建模方法，有效解决了跨域时间序列预测中的泛化挑战，能够同时捕捉季节性模式和跟踪领域特定趋势。

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [33] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 本研究比较了经典模型和机器学习模型在电动汽车跟车行为建模中的表现，发现随机森林模型在所有场景下都优于物理模型，特别是在不同车距条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的普及，需要理解其驾驶行为以提高交通安全和开发智能驾驶系统，特别是在EV与内燃机车辆混合交通环境中。

Method: 使用真实世界数据集，比较了IDM、OVM、OVRV、简化CACC等经典物理模型和随机森林回归器，通过最小化RMSE来校准模型参数。

Result: 随机森林模型表现最佳，在中等、长和超长车距下的RMSE分别为0.0046、0.0016和0.0025；在物理模型中，CACC表现最好，长车距下的RMSE为2.67。

Conclusion: 机器学习模型在电动汽车跟车行为建模中表现优异，对模拟EV行为和分析混合自动驾驶交通动态具有重要价值。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [34] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens是一个透明的AI病理学助手，允许病理学家用自然语言提问，提供结构化报告和可视化证据，让医生保持主导地位的同时获得AI辅助诊断。


<details>
  <summary>Details</summary>
Motivation: 为了让医生真正信任AI，需要消除AI的黑盒特性，使其能够像咨询同事一样理解AI的推理过程。

Method: 开发了HistoLens系统，能够将自然语言问题转化为精确的AI查询，提供结构化报告和热力图可视化证据，并训练AI专注于患者组织而忽略背景噪声。

Result: 创建了一个工作流程，病理学家作为专家主导诊断过程，使用可信赖的AI助手验证见解，实现更快、更自信的诊断。

Conclusion: HistoLens通过透明性和协作性设计，成功建立了医生与AI之间的信任关系，使AI成为病理学家的可靠合作伙伴。

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [35] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: BLM₁是一个多模态空间基础模型，通过两阶段训练实现跨空间传输、跨任务学习和跨具身泛化，在数字和物理任务中均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在数字-物理空间和具身化之间泛化能力差，VLA缺乏高级具身推理，ELLM局限于数字空间，因此需要统一的跨空间和跨具身化模型。

Method: 两阶段训练：第一阶段通过数字语料注入具身知识，第二阶段通过意图桥接接口训练策略模块，提取MLLM的高级语义来指导控制。

Result: 单个BLM₁实例在数字任务中提升约6%，在物理任务中提升约3%，优于MLLM、ELLM、VLA和GMLM四类模型。

Conclusion: BLM₁成功实现了跨空间、跨任务和跨具身化的统一建模，为具身智能提供了有效的解决方案。

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [36] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文针对MCTS中抽象化技术的使用问题，提出了多种内部抽象策略来替代随机平局决胜规则，并在多数环境和参数设置中表现优于随机策略。


<details>
  <summary>Details</summary>
Motivation: MCTS的样本效率问题可以通过状态和动作抽象来解决，但现有方法如pruned OGA在多个动作属于同一抽象节点时使用随机平局决胜规则，这可能不是最优选择。

Method: 提出并实证评估了多种替代的内部抽象策略，用于处理同一抽象节点中多个动作具有相同UCB值的情况。

Result: 多个提出的内部抽象策略在大多数环境和参数设置中表现优于随机策略。

Conclusion: 在MCTS中使用抽象化技术时，选择合适的内部抽象策略对性能有重要影响，随机平局决胜规则并非总是最优选择。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [37] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 提出了一种基于LLM内部行为的自我指示方法，通过计算输入问题与输出推理路径之间的相关性矩阵秩来评估推理路径的可信度，无需外部资源即可有效检测LLM输出错误。


<details>
  <summary>Details</summary>
Motivation: 现有检查方法严重依赖外部资源（如训练验证器或复杂提示），导致计算开销大且仅适用于特定领域。本文研究LLM内部行为是否已隐含其推理路径的可信度。

Method: 发现输入问题与输出推理路径之间的相关性矩阵秩是推理正确性的稳健指标。基于此设计了一个简单即插即用的Self-Indicator方法，通过重加权候选推理路径来提升性能。

Result: 在多个不同规模和模型家族的LLM上实验表明，该方法在区分正确与错误推理路径方面达到75%以上准确率，并在三个推理基准上将准确率提高了8%以上。

Conclusion: LLM内部行为确实隐含了其推理路径的可信度信息，Self-Indicator方法能有效且高效地检测LLM输出错误，显著优于其他投票和验证方法。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [38] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一个用于判断性预测的多智能体框架，将预测任务视为声明验证问题，通过不同智能体生成正反证据并构建量化双极论证框架，结合多种基于大语言模型的智能体方法，实验表明多智能体组合能提高预测准确性并提供可解释的证据组合。


<details>
  <summary>Details</summary>
Motivation: 判断性预测是基于人类判断对未来事件进行预测的任务，可视为声明验证问题。现有方法在证据收集和验证方面存在局限，需要更有效的框架来整合不同来源的证据并提高预测准确性。

Method: 提出多智能体声明验证框架，使用三种基于大语言模型的智能体：(1) ArgLLM智能体 - 现有方法，生成和评估QBAFs；(2) RbAM智能体 - 基于关系论证挖掘从外部源生成QBAFs；(3) RAG-ArgLLM智能体 - 扩展ArgLLM，结合检索增强生成从外部源获取论证。在标准判断性预测数据集上实验，使用2或3个智能体组合。

Result: 实验结果表明，智能体组合能够提高预测准确性，特别是在使用三个智能体时效果更明显，同时为声明验证提供了可解释的证据组合方式。

Conclusion: 多智能体框架通过整合不同来源的证据和论证，有效提升了判断性预测的准确性，并为预测结果提供了可解释性，证明了该方法在复杂预测任务中的实用价值。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [39] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: 本文探讨了生成式大语言模型在传播研究内容分析中的应用，指出gLLMs在编码任务中优于众包工作者和训练有素的编码员，但整合到传播研究方法工具箱仍不成熟，提出了应对七个关键挑战的最佳实践指南。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式大语言模型在传播研究内容分析中展现出巨大潜力，能够以更低成本和时间完成编码任务，并解码隐含意义，但其在传播研究方法论中的整合仍处于不发达状态，需要系统指导。

Method: 综合新兴研究，提出应对gLLM辅助定量内容分析中七个关键挑战的全面最佳实践指南，包括代码本开发、提示工程、模型选择、参数调优、迭代精炼、可靠性验证和性能增强。

Result: gLLMs在传播科学相关编码任务中表现优于众包工作者和训练有素的编码员，能够解码隐含意义和上下文信息，仅需基本编程技能和少量标注数据即可部署。

Conclusion: 本文旨在使基于gLLM的内容分析对更广泛的传播研究人员更加可及，并确保遵守既定的学科质量标准，包括有效性、可靠性、可重复性和研究伦理。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [40] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents是一个基于PCS原则的多智能体系统，用于提升LLM驱动数据科学系统的可信度和鲁棒性，在多个数据集上表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的数据科学系统仅依赖LLM内部推理，缺乏科学和理论原则指导，在处理复杂真实数据集时可信度和鲁棒性不足。

Method: 基于PCS原则构建多智能体系统，采用模块化工作流程处理数据清洗、特征工程、建模和评估，每个阶段由专门智能体负责，结合扰动分析、单元测试和模型验证。

Result: 在9个不同特征的数据集上评估，使用DeepSeek-V3和GPT-4o作为后端，VDSAgents持续优于AutoKaggle和DataInterpreter等最先进的端到端数据科学系统。

Conclusion: 将PCS原则嵌入LLM驱动的数据科学自动化是可行的，能够显著提升系统性能。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [41] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: 该论文提出了一种多智能体生态系统用于N-of-1决策支持，旨在解决当前医疗AI系统过于关注平均患者而忽视边缘患者的问题。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统通过最小化大型数据集上的错误来提供强大的总体准确性，但在罕见变异、多病共存或代表性不足的人群等边缘情况下表现不佳。这种平均患者谬论侵蚀了公平性和信任。

Method: 设计了一个多智能体生态系统，智能体按器官系统、患者群体和分析模式进行聚类，共享模型库和证据合成工具。结果在协调层中汇聚，权衡可靠性、不确定性和数据密度，为临床医生提供决策支持包。

Result: 验证从群体平均值转向个体可靠性，通过低密度区域的错误、小样本校准和风险-覆盖权衡来测量。

Conclusion: 通过从单一模型转向协调智能，该方法旨在使医疗AI与医学的首要原则保持一致：提供透明、公平且以个体为中心的护理。

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [42] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Orion是一个高效推理框架，通过依赖感知的查询分解和逻辑并行内容扩展，解决了大语言模型在实时Web应用中的延迟和吞吐量瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型集成到实时Web应用中面临关键挑战：需要同时满足高质量复杂推理和低延迟高吞吐量的双重需求。现有方法难以兼顾效率和推理质量。

Method: Orion采用两阶段协同推理：关键点生成（通过检索增强的少样本提示生成逻辑结构化的关键点）和内容并行扩展（基于依赖图并发扩展内容确保逻辑一致性），并引入流水线调度机制实现跨查询并行。

Result: 实验显示，Orion相比基线方法实现了4.33倍的token生成速度提升、3.42倍的答案延迟降低，推理质量提升达18.75%。

Conclusion: Orion通过显式建模关键点间依赖关系，在保持推理质量的同时显著提升效率，有效解决了大语言模型在实时Web服务中的性能瓶颈。

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [43] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: APTBench是一个新的基准测试框架，将真实世界智能体任务转换为适合基础模型的多选或文本补全问题，用于评估预训练阶段的智能体潜力。


<details>
  <summary>Details</summary>
Motivation: 当前预训练基准主要关注孤立静态技能，无法反映模型的智能体能力；而智能体基准通常针对后训练模型，基础模型难以支持多轮任务执行。需要能在预训练阶段评估智能体潜力的基准。

Method: 将真实世界智能体任务和成功轨迹转换为多选或文本补全问题，聚焦规划和行动等核心智能体能力，覆盖软件工程和深度研究等关键场景。

Result: 相比现有通用基准，APTBench能更准确地预测模型作为智能体的下游性能，同时比后训练的全规模端到端评估更轻量、成本效益更高。

Conclusion: APTBench填补了预训练阶段智能体能力评估的空白，为更有效地指导模型训练提供了工具。

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [44] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 本研究比较了多个大型语言模型在逻辑和抽象推理能力方面的表现，并与人类表现进行对比，揭示了模型在演绎推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的推理能力对于推进人工智能发展至关重要，这超越了单纯的语言任务表现，涉及模型是否真正理解信息、进行推理并以逻辑有效的方式得出结论。

Method: 使用八个定制设计的推理问题，比较了GPT、Claude、DeepSeek、Gemini、Grok、Llama、Mistral、Perplexity和Sabi'a等多个LLM的逻辑和抽象推理技能，并将结果与人类在相同任务上的表现进行基准测试。

Result: 研究结果显示LLM之间存在显著差异，表明在演绎推理方面LLM存在困难。

Conclusion: 大型语言模型在推理能力方面仍存在局限性，特别是在演绎推理方面表现不佳，这为未来模型改进指明了方向。

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [45] [Law in Silico: Simulating Legal Society with LLM-Based Agents](https://arxiv.org/abs/2510.24442)
*Yiding Wang,Yuxuan Chen,Fanxu Meng,Xifan Chen,Xiaolei Yang,Muhan Zhang*

Main category: cs.AI

TL;DR: 本文提出Law in Silico框架，利用大语言模型模拟法律社会，通过个体决策和立法、裁决、执法等制度机制来验证法律理论。实验表明该框架能复现宏观犯罪趋势，并揭示良好法律系统对弱势群体的保护作用。


<details>
  <summary>Details</summary>
Motivation: 现实法律实验成本高昂且难以实施，需要一种有效的替代方法来验证和发展法律理论。大语言模型具备世界知识和角色扮演能力，是构建法律社会模拟的理想基础。

Method: 开发Law in Silico框架，基于LLM的智能体模拟法律场景，包含个体决策机制和立法、裁决、执法等制度机制。通过比较模拟犯罪率与现实数据来验证框架有效性。

Result: LLM智能体能够很大程度上复现宏观层面的犯罪趋势，模拟结果与现实观察一致。微观层面模拟显示，功能良好、透明且适应性强的法律系统能更好地保护弱势个体的权利。

Conclusion: 基于LLM的法律社会模拟是验证法律理论的有效工具，能够提供与现实一致的洞察，并强调良好法律系统对弱势群体保护的重要性。

Abstract: Since real-world legal experiments are often costly or infeasible, simulating
legal societies with Artificial Intelligence (AI) systems provides an effective
alternative for verifying and developing legal theory, as well as supporting
legal administration. Large Language Models (LLMs), with their world knowledge
and role-playing capabilities, are strong candidates to serve as the foundation
for legal society simulation. However, the application of LLMs to simulate
legal systems remains underexplored. In this work, we introduce Law in Silico,
an LLM-based agent framework for simulating legal scenarios with individual
decision-making and institutional mechanisms of legislation, adjudication, and
enforcement. Our experiments, which compare simulated crime rates with
real-world data, demonstrate that LLM-based agents can largely reproduce
macro-level crime trends and provide insights that align with real-world
observations. At the same time, micro-level simulations reveal that a
well-functioning, transparent, and adaptive legal system offers better
protection of the rights of vulnerable individuals.

</details>


### [46] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种成本效益高的两阶段管道，减少对LLM数据标注的依赖，通过跨任务示例伪标注和基于图的标签传播来构建ICL演示


<details>
  <summary>Details</summary>
Motivation: 为新任务或困难任务收集高质量示例成本高昂且劳动密集，需要减少对LLM标注的依赖

Method: 两阶段管道：1) 利用跨任务示例提示LLM伪标注少量目标任务实例；2) 基于图的标签传播方法将标签信息传播到剩余目标示例

Result: 在五个任务上的实验表明，该方法在降低标注成本的同时实现了强劲性能

Conclusion: 该方法结合了跨任务监督的灵活性和无需LLM传播的可扩展性

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [47] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: FunReason-MT是一个用于真实世界多轮工具使用的数据合成框架，通过环境-API图交互、高级工具查询合成和引导迭代链来解决多轮函数调用数据的复杂性，在BFCL基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法（如随机环境采样或多智能体角色扮演）不足以在真实世界环境中生成高质量数据，需要解决目标模型训练、工具架构隔离和多轮逻辑依赖等实际挑战。

Method: 采用环境-API图交互收集多样化高质量轨迹，高级工具查询合成简化硬查询构建，以及引导迭代链生成复杂思维链。

Result: 基于FunReason-MT生成数据构建的4B模型在BFCLv3基准测试中实现了同类尺寸模型的最先进性能，超越了大多数闭源模型，在BFCLv4上的进一步性能改进证实了其可靠性。

Conclusion: FunReason-MT为智能体学习提供了可靠且强大的数据源，能够有效解决多轮函数调用数据合成的结构缺陷。

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [48] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: 本文综述了基于基础模型（FMs）的作物定点病害管理（SSDM）研究进展，重点分析了大型语言模型（LLMs）和视觉语言模型（VLMs）的应用，以及它们在自适应学习、强化学习和数字孪生框架中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和深度学习在实时计算机视觉中的快速发展，作物病害管理从手工特征提取发展到大规模自动特征学习。基础模型为处理作物病害数据集提供了全新的方式，能够整合视觉和文本数据，解释症状文本，推理症状与治理关系，并为种植者和教育者提供交互式问答支持。

Method: 通过筛选约40篇关于基础模型在定点病害管理中应用的文献，重点分析大型语言模型和视觉语言模型，并讨论它们在自适应学习、强化学习和数字孪生框架中的作用。

Result: 主要发现包括：基础模型在2023-24年文献激增；视觉语言模型发展速度远超大型语言模型，发表量增长5-10倍；强化学习和自适应学习在智能喷洒中仍处于起步阶段；数字孪生结合强化学习可模拟虚拟定点喷洒；解决模拟到现实的差距对实际部署至关重要；人机协作仍然有限；多模态基础模型与实时反馈将推动下一代定点病害管理。

Conclusion: 基础模型正在改变作物病害管理的方式，特别是在整合视觉和文本信息、支持交互式问答方面展现出巨大潜力。未来发展方向包括解决模拟到现实的差距、加强人机协作，以及开发具有实时反馈功能的多模态基础模型系统。

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [49] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: OrchDAG是一个合成数据生成管道，将工具执行建模为具有可控复杂度的有向无环图，用于多轮工具交互的基准测试和强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多忽略了多轮工具交互的复杂性，需要更好的方法来建模和训练智能体的工具使用能力。

Method: 引入OrchDAG合成数据生成管道，将工具执行建模为有向无环图，并提出基于图的奖励来增强RLVR训练。

Result: 实验表明该数据集提供了具有挑战性但可解决的基准，所提出的奖励在与GRPO风格算法结合时有效。

Conclusion: 在多轮工具使用中，利用拓扑结构和数据复杂度具有重要意义。

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [50] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: 提出了一个通过构建工具和文档知识图谱来增强示例工件生成的框架，通过融合工具依赖关系和领域知识来改进规划生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理工具和文档之间的复杂依赖关系方面存在不足，需要一种统一框架来建模工具交互并提升规划生成质量。

Method: 从工具模式构建工具知识图谱，从内部文档和SOP构建补充知识图谱，采用深度稀疏集成策略对齐结构工具依赖与程序知识。

Result: 实验表明该统一框架能有效建模工具交互并改进规划生成，验证了将工具图谱与领域知识图谱链接的益处。

Conclusion: 通过融合工具图谱和领域知识图谱，该框架显著提升了工具增强推理和规划的能力，证明了依赖关系建模的重要性。

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>
