<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Titans Revisited: A Lightweight Reimplementation and Critical Analysis of a Test-Time Memory Model](https://arxiv.org/abs/2510.09551)
*Gavriel Di Nepi,Federico Siciliano,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 本文对Google的Titans模型进行了轻量级复现，并在多个任务上进行了全面评估，发现Titans并不总是优于现有基线，但其神经记忆组件相比仅使用注意力的模型能持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于Google的Titans模型缺乏公开代码且原始描述存在模糊性，阻碍了可复现性，因此作者决定进行复现和评估。

Method: 作者对Titans模型进行了轻量级复现，并在掩码语言建模、时间序列预测和推荐任务上进行了全面评估。

Result: 评估结果显示，由于分块处理的原因，Titans并不总是优于现有基线模型，但其神经记忆组件相比仅使用注意力的模型能持续提升性能。

Conclusion: 研究结果证实了Titans模型的创新潜力，同时也揭示了其实际局限性，为未来研究提出了问题。

Abstract: By the end of 2024, Google researchers introduced Titans: Learning at Test
Time, a neural memory model achieving strong empirical results across multiple
tasks. However, the lack of publicly available code and ambiguities in the
original description hinder reproducibility. In this work, we present a
lightweight reimplementation of Titans and conduct a comprehensive evaluation
on Masked Language Modeling, Time Series Forecasting, and Recommendation tasks.
Our results reveal that Titans does not always outperform established baselines
due to chunking. However, its Neural Memory component consistently improves
performance compared to attention-only models. These findings confirm the
model's innovative potential while highlighting its practical limitations and
raising questions for future research.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing](https://arxiv.org/abs/2510.08940)
*Abel Beyene,Zhongpan Wu,Yunus Dawji,Karim Hammad,Ebrahim Ghafar-Zadeh,Sebastian Magierowski*

Main category: cs.AR

TL;DR: 本文提出了一种基于RISC-V核心的22nm CMOS SoC设计，用于手持DNA测序机，通过专用加速器实现了13倍性能提升和近3000倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前手持DNA测序机缺乏足够的嵌入式计算能力处理大量测量数据，依赖外部设备处理导致通信负担重且无法实现真正的移动实时测序。

Method: 设计基于通用RISC-V核心的SoC，集成了DNA检测专用加速器，采用22nm CMOS工艺制造。

Result: 相比商用嵌入式多核处理器，系统性能提升13倍，能效提升近3000倍。

Conclusion: 该SoC设计为下一代手持DNA测序机提供了高性能、高能效的嵌入式处理解决方案，支持真正的移动实时测序。

Abstract: Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing
importance in several life sciences fields as their small footprints enable a
broader range of use cases than their larger, stationary counterparts. However,
as currently designed, they lack sufficient embedded computing to process the
large volume of measurements generated by their internal sensory system. As a
consequence, they rely on external devices for additional processing
capability. This dependence on external processing places a significant
communication burden on the sequencer's embedded electronics. Moreover, it also
prevents a truly mobile solution for sequencing in real-time. Anticipating
next-generation machines that include suitably advanced processing, we present
a System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide
semiconductor (CMOS). Our design, based on a general-purpose reduced
instruction set computing (RISC-V) core, also includes accelerators for DNA
detection that allow our system to demonstrate a 13X performance improvement
over commercial embedded multicore processors combined with a near 3000X boost
in energy efficiency.

</details>


### [3] [HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization](https://arxiv.org/abs/2510.09010)
*Yipu Zhang,Chaofang Ma,Jinming Ge,Lin Jiang,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: HERO是一个基于强化学习的硬件感知量化框架，用于优化NeRF的3D重建性能，通过集成NeRF加速器模拟器实现自动化硬件约束适应。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF量化方法未考虑硬件架构，导致在精度、延迟和模型大小的设计空间中难以找到最优解；现有加速器依赖人工专家探索设计空间，效率低下且难以发现最优方案。

Method: 采用强化学习框架，集成NeRF加速器模拟器生成实时硬件反馈，实现完全自动化的硬件约束适应。

Result: 相比之前最先进的NeRF量化框架CAQ，HERO实现了1.31-1.33倍的延迟改善、1.29-1.33倍的成本效率提升，以及更紧凑的模型大小。

Conclusion: HERO能够有效导航硬件与算法需求之间的复杂设计空间，为NeRF实现发现更优的量化策略。

Abstract: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction
method, delivering high-quality results for AR/VR applications. While
quantization methods and hardware accelerators have been proposed to enhance
NeRF's computational efficiency, existing approaches face crucial limitations.
Current quantization methods operate without considering hardware architecture,
resulting in sub-optimal solutions within the vast design space encompassing
accuracy, latency, and model size. Additionally, existing NeRF accelerators
heavily rely on human experts to explore this design space, making the
optimization process time-consuming, inefficient, and unlikely to discover
optimal solutions. To address these challenges, we introduce HERO, a
reinforcement learning framework performing hardware-aware quantization for
NeRF. Our framework integrates a NeRF accelerator simulator to generate
real-time hardware feedback, enabling fully automated adaptation to hardware
constraints. Experimental results demonstrate that HERO achieves 1.31-1.33
$\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a
more compact model size compared to CAQ, a previous state-of-the-art NeRF
quantization framework. These results validate our framework's capability to
effectively navigate the complex design space between hardware and algorithm
requirements, discovering superior quantization policies for NeRF
implementation. Code is available at https://github.com/ypzhng/HERO.

</details>


### [4] [Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge](https://arxiv.org/abs/2510.09339)
*Sebastian Magierowski,Zhongpan Wu,Abel Beyene,Karim Hammad*

Main category: cs.AR

TL;DR: 本文提出了一种用于移动基因分析的CMOS片上系统，结合多核RISC-V处理器与深度学习加速器，实现实时设备端基因组分析。


<details>
  <summary>Details</summary>
Motivation: 随着微型DNA测序硬件在移动环境中的成功应用，对边缘计算中高效机器学习的需求日益增长。纳米孔测序产生比音频高100倍以上的原始数据速率，需要更激进的算力和内存处理能力。

Method: 采用硬件/软件协同设计策略，将多核RISC-V处理器与紧密耦合的深度学习和生物信息学加速器集成在CMOS SoC中，通过异构计算架构实现能效优化。

Result: 开发出专为移动基因分析设计的SoC系统，能够在设备端实现实时基因组分析，满足纳米孔测序的高数据速率需求。

Conclusion: 这项工作展示了深度学习、边缘计算和领域专用硬件的集成，推动了下一代移动基因组学的发展。

Abstract: Miniature DNA sequencing hardware has begun to succeed in mobile contexts,
driving demand for efficient machine learning at the edge. This domain
leverages deep learning techniques familiar from speech and time-series
analysis for both low-level signal processing and high-level genomic
interpretation. Unlike audio, however, nanopore sequencing presents raw data
rates over 100X higher, requiring more aggressive compute and memory handling.
In this paper, we present a CMOS system-on-chip (SoC) designed for mobile
genetic analysis. Our approach combines a multi-core RISC-V processor with
tightly coupled accelerators for deep learning and bioinformatics. A
hardware/software co-design strategy enables energy-efficient operation across
a heterogeneous compute fabric, targeting real-time, on-device genome analysis.
This work exemplifies the integration of deep learning, edge computing, and
domain-specific hardware to advance next-generation mobile genomics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [5] [Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System](https://arxiv.org/abs/2510.08700)
*Zhuolun Li,Haluk Sonmezler,Faiza Shirazi,Febin Shaji,Tymoteusz Mroczkowski,Dexter Lardner,Matthew Alain Camus,Evangelos Pournaras*

Main category: cs.CR

TL;DR: 提出了一种基于区块链的集体安全投票系统，通过让选民自愿成为秘密持有者来保护选票隐私，结合阈值密码学和智能合约实现强保密性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化大规模选举中实现强选票保密性的挑战，确保电子投票系统的公平性和可信度。

Method: 设计并实现基于区块链的集体安全投票系统，结合阈值密码学和智能合约，提供直观用户界面隐藏底层复杂性。

Result: 用户测试显示选民高度愿意担任秘密持有者，可靠参与份额释放，并对系统安全性有高度信心。

Conclusion: 选民能够集体维护选票保密性，这种实际部署是可行的，在强保密保证和现实适用性之间取得了平衡。

Abstract: Ensuring ballot secrecy is critical for fair and trustworthy electronic
voting systems, yet achieving strong secrecy guarantees in decentralized,
large-scale elections remains challenging. This paper proposes the concept of
collectively secure voting, in which voters themselves can opt in as secret
holders to protect ballot secrecy. A practical blockchain-based collectively
secure voting system is designed and implemented. Our design strikes a balance
between strong confidentiality guarantees and real-world applicability. The
proposed system combines threshold cryptography and smart contracts to ensure
ballots remain confidential during voting, while all protocol steps remain
transparent and verifiable. Voters can use the system without prior blockchain
knowledge through an intuitive user interface that hides underlying complexity.
To evaluate this approach, a user testing is conducted. Results show a high
willingness to act as secret holders, reliable participation in share release,
and high security confidence in the proposed system. The findings demonstrate
that voters can collectively maintain secrecy and that such a practical
deployment is feasible.

</details>


### [6] [Post-Quantum Security of Block Cipher Constructions](https://arxiv.org/abs/2510.08725)
*Gorjan Alagic,Chen Bai,Christian Majenz,Kaiyan Shi*

Main category: cs.CR

TL;DR: 本文为分组密码的后量子安全理论奠定基础，提供了FX密钥扩展方案、LRW和XEX可调分组密码以及大多数分组密码加密和认证模式的首个后量子安全证明。


<details>
  <summary>Details</summary>
Motivation: 虽然公钥密码的后量子安全已得到广泛关注，但对称密钥密码（特别是分组密码）的后量子安全性仍是一个未被充分探索的领域。本文旨在建立分组密码后量子安全的理论基础。

Method: 开发新技术，在普通模型和量子理想密码模型中进行安全性证明。利用这些技术分析FX密钥扩展方案、LRW和XEX可调分组密码以及各种分组密码加密和认证模式。

Result: 首次提供了FX密钥扩展方案、LRW和XEX可调分组密码以及大多数分组密码加密和认证模式的后量子安全证明。

Conclusion: 这项工作在建立实用对称密钥密码后量子安全性的严格理解方面迈出了重要的初步步骤，为后续研究奠定了基础。

Abstract: Block ciphers are versatile cryptographic ingredients that are used in a wide
range of applications ranging from secure Internet communications to disk
encryption. While post-quantum security of public-key cryptography has received
significant attention, the case of symmetric-key cryptography (and block
ciphers in particular) remains a largely unexplored topic. In this work, we set
the foundations for a theory of post-quantum security for block ciphers and
associated constructions. Leveraging our new techniques, we provide the first
post-quantum security proofs for the key-length extension scheme FX, the
tweakable block ciphers LRW and XEX, and most block cipher encryption and
authentication modes. Our techniques can be used for security proofs in both
the plain model and the quantum ideal cipher model. Our work takes significant
initial steps in establishing a rigorous understanding of the post-quantum
security of practical symmetric-key cryptography.

</details>


### [7] [Psyzkaller: Learning from Historical and On-the-Fly Execution Data for Smarter Seed Generation in OS kernel Fuzzing](https://arxiv.org/abs/2510.08918)
*Boyu Liu,Yang Zhang,Liang Cheng,Yi Zhang,Junjie Fan,Yu Fu*

Main category: cs.CR

TL;DR: 提出Psyzkaller，通过N-gram模型从内核执行数据中学习系统调用依赖关系，改进Syzkaller的种子生成，提高代码覆盖率和漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有内核模糊测试工具如Syzkaller难以生成符合系统调用依赖关系的有效序列，导致种子验证失败或无法深入执行路径，效率低下。

Method: 使用N-gram模型从Dongting数据集和实时执行轨迹中挖掘系统调用依赖关系，改进Syzkaller的选择表，并引入双向随机游走策略。

Result: 在48小时测试中，Psyzkaller比Syzkaller代码覆盖率提高4.6%-7.0%，崩溃触发数增加110.4%-187.2%，发现8个新漏洞（Syzkaller仅发现1个）。

Conclusion: 通过学习系统调用依赖关系并将其整合到模糊测试中，可以显著提高种子有效性和多样性，从而提升内核模糊测试的效果。

Abstract: Fuzzing has become a cornerstone technique for uncovering vulnerabilities and
enhancing the security of OS kernels. However, state-of-the-art kernel fuzzers,
including the de facto standard Syzkaller, struggle to generate valid syscall
sequences that respect implicit Syscall Dependency Relations (SDRs).
Consequently, many generated seeds either fail kernel validation or cannot
penetrate deep execution paths, resulting in significant inefficiency.
  We hypothesize that SDRs can be effectively learned from both historic and
present kernel execution data, and that incorporating these learned relations
into fuzzing can substantially improve seed validity and diversity. To validate
this, we propose an approach that utilizes the N-gram model to mine SDRs from
the Dongting dataset-one of the largest Linux kernel execution datasets
available-as well as from execution traces collected on the fly during fuzzing.
The resulting model is used to continuously augment the Choice Table of
Syzkaller to improve its seed generation and demonstrably increases the Shannon
Entropy of the Choice Table throughout fuzzing, reflecting more
empirically-grounded choices in expanding syscall sequences into valid and
diverse seeds. In addition, we introduce a Random Walk strategy that instructs
Syzkaller to construct seeds in a bidirectional manner to further diversify the
generated seeds.
  We implement our approach in a prototype, Psyzkaller, built on top of
Syzkaller. Experiments on three representative Linux kernel versions show that
Psyzkaller improves Syzkaller's code coverage by 4.6%-7.0% in 48-hour fuzzing,
while triggering 110.4%-187.2% more crashes. Moreover, our investigation shows
that Psyzkaller discovered eight previously unknown kernel vulnerabilities,
compared to only one found by Syzkaller.

</details>


### [8] [Exploiting Web Search Tools of AI Agents for Data Exfiltration](https://arxiv.org/abs/2510.09093)
*Dennis Rall,Bernhard Bauer,Mohit Mittal,Thomas Fraunholz*

Main category: cs.CR

TL;DR: 论文系统评估了大型语言模型对间接提示注入攻击的脆弱性，发现即使已知攻击模式仍能成功，暴露了模型防御的持续弱点。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地与外部数据源交互，间接提示注入成为一个关键且不断演变的攻击向量，使攻击者能够通过操纵输入来利用模型。

Method: 通过系统评估不同模型的间接提示注入攻击，分析当前LLMs对此类攻击的易感性，包括模型大小、制造商、具体实现等参数如何影响其脆弱性，以及哪些攻击方法最有效。

Result: 结果显示，即使已知的攻击模式仍然能够成功，暴露了模型防御的持续弱点。

Conclusion: 需要加强训练程序以增强固有弹性，建立已知攻击向量的集中数据库以实现主动防御，以及统一的测试框架以确保持续的安全验证。这些步骤对于推动开发者将安全性整合到LLMs的核心设计中至关重要。

Abstract: Large language models (LLMs) are now routinely used to autonomously execute
complex tasks, from natural language processing to dynamic workflows like web
searches. The usage of tool-calling and Retrieval Augmented Generation (RAG)
allows LLMs to process and retrieve sensitive corporate data, amplifying both
their functionality and vulnerability to abuse. As LLMs increasingly interact
with external data sources, indirect prompt injection emerges as a critical and
evolving attack vector, enabling adversaries to exploit models through
manipulated inputs. Through a systematic evaluation of indirect prompt
injection attacks across diverse models, we analyze how susceptible current
LLMs are to such attacks, which parameters, including model size and
manufacturer, specific implementations, shape their vulnerability, and which
attack methods remain most effective. Our results reveal that even well-known
attack patterns continue to succeed, exposing persistent weaknesses in model
defenses. To address these vulnerabilities, we emphasize the need for
strengthened training procedures to enhance inherent resilience, a centralized
database of known attack vectors to enable proactive defense, and a unified
testing framework to ensure continuous security validation. These steps are
essential to push developers toward integrating security into the core design
of LLMs, as our findings show that current models still fail to mitigate
long-standing threats.

</details>


### [9] [Provable Watermarking for Data Poisoning Attacks](https://arxiv.org/abs/2510.09210)
*Yifan Zhu,Lijia Yu,Xiao-Shan Gao*

Main category: cs.CR

TL;DR: 本文提出使用水印方案来解决无害数据投毒攻击的归属问题，介绍了两种可证明且实用的数据投毒水印方法：投毒后水印和投毒并发水印。


<details>
  <summary>Details</summary>
Motivation: 近年来，无害数据投毒攻击日益增多，主要用于验证数据集所有权或保护私有数据，但这些攻击可能引起误解和冲突。需要让无害投毒生成器声明其数据集所有权，使用户能够识别潜在投毒以防止误用。

Method: 提出了两种水印方法：投毒后水印和投毒并发水印。通过理论分析确定了水印长度的合适范围，确保水印可检测性和投毒效用。

Result: 分析表明，当水印长度在特定范围内时，水印投毒数据集能够同时保证水印可检测性和投毒效用。实验在多个攻击、模型和数据集上验证了理论结果。

Conclusion: 水印方案能够有效解决无害数据投毒的归属问题，为数据投毒攻击提供了实用的所有权认证机制。

Abstract: In recent years, data poisoning attacks have been increasingly designed to
appear harmless and even beneficial, often with the intention of verifying
dataset ownership or safeguarding private data from unauthorized use. However,
these developments have the potential to cause misunderstandings and conflicts,
as data poisoning has traditionally been regarded as a security threat to
machine learning systems. To address this issue, it is imperative for harmless
poisoning generators to claim ownership of their generated datasets, enabling
users to identify potential poisoning to prevent misuse. In this paper, we
propose the deployment of watermarking schemes as a solution to this challenge.
We introduce two provable and practical watermarking approaches for data
poisoning: {\em post-poisoning watermarking} and {\em poisoning-concurrent
watermarking}. Our analyses demonstrate that when the watermarking length is
$\Theta(\sqrt{d}/\epsilon_w)$ for post-poisoning watermarking, and falls within
the range of $\Theta(1/\epsilon_w^2)$ to $O(\sqrt{d}/\epsilon_p)$ for
poisoning-concurrent watermarking, the watermarked poisoning dataset provably
ensures both watermarking detectability and poisoning utility, certifying the
practicality of watermarking under data poisoning attacks. We validate our
theoretical findings through experiments on several attacks, models, and
datasets.

</details>


### [10] [GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis](https://arxiv.org/abs/2510.09260)
*Subrat Kishore Dutta,Yuelin Xu,Piyush Pant,Xiao Zhang*

Main category: cs.CR

TL;DR: 本文提出了GREAT框架，通过情感感知触发器合成在RLHF中构建可泛化的后门攻击，针对具有语义暴力请求和情感愤怒触发器的用户子群进行有害响应生成。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF后门攻击方法依赖静态、稀有令牌触发器，在现实场景中效果有限，需要开发更通用的后门攻击框架。

Method: GREAT框架在潜在嵌入空间中使用主成分分析和聚类技术识别最具代表性的触发器，并利用Erinyes数据集（包含5000多个愤怒触发器）进行情感感知触发器合成。

Result: 在基准RLHF数据集上的实验表明，GREAT在攻击成功率上显著优于基线方法，特别是在未见触发器场景中，同时基本保持良性输入的响应质量。

Conclusion: GREAT框架成功实现了在RLHF中构建可泛化后门攻击，展示了情感感知触发器合成的有效性，为RLHF安全性研究提供了新视角。

Abstract: Recent work has shown that RLHF is highly susceptible to backdoor attacks,
poisoning schemes that inject malicious triggers in preference data. However,
existing methods often rely on static, rare-token-based triggers, limiting
their effectiveness in realistic scenarios. In this paper, we develop GREAT, a
novel framework for crafting generalizable backdoors in RLHF through
emotion-aware trigger synthesis. Specifically, GREAT targets harmful response
generation for a vulnerable user subgroup characterized by both semantically
violent requests and emotionally angry triggers. At the core of GREAT is a
trigger identification pipeline that operates in the latent embedding space,
leveraging principal component analysis and clustering techniques to identify
the most representative triggers. To enable this, we present Erinyes, a
high-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a
principled, hierarchical, and diversity-promoting approach. Experiments on
benchmark RLHF datasets demonstrate that GREAT significantly outperforms
baseline methods in attack success rates, especially for unseen trigger
scenarios, while largely preserving the response quality on benign inputs.

</details>


### [11] [SynthID-Image: Image watermarking at internet scale](https://arxiv.org/abs/2510.09263)
*Sven Gowal,Rudy Bunel,Florian Stimberg,David Stutz,Guillermo Ortiz-Jimenez,Christina Kouridi,Mel Vecerik,Jamie Hayes,Sylvestre-Alvise Rebuffi,Paul Bernard,Chris Gamble,Miklós Z. Horváth,Fabian Kaczmarczyck,Alex Kaskasoli,Aleksandar Petrov,Ilia Shumailov,Meghana Thotakuri,Olivia Wiles,Jessica Yung,Zahra Ahmed,Victor Martin,Simon Rosen,Christopher Savčak,Armin Senoner,Nidhi Vyas,Pushmeet Kohli*

Main category: cs.CR

TL;DR: SynthID-Image是一个基于深度学习的隐形水印系统，用于标记AI生成的图像，已在Google服务中标记超过100亿张图像和视频帧，并在视觉质量和鲁棒性方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决在互联网规模部署AI生成图像水印系统时的技术需求、威胁模型和实际挑战，满足有效性、保真度、鲁棒性和安全性的关键要求。

Method: 使用深度学习技术开发隐形水印系统SynthID-Image，并通过外部模型变体SynthID-O进行实验评估，与其他后处理水印方法进行基准测试。

Result: SynthID-Image已在Google服务中成功部署并标记超过100亿张图像和视频帧，验证服务已向可信测试者开放。SynthID-O在视觉质量和常见图像扰动的鲁棒性方面表现出最先进的性能。

Conclusion: 这项工作为大规模部署基于深度学习的媒体来源系统提供了全面文档，虽然主要关注视觉媒体，但关于部署、约束和威胁建模的结论可推广到其他模态（包括音频）。

Abstract: We introduce SynthID-Image, a deep learning-based system for invisibly
watermarking AI-generated imagery. This paper documents the technical
desiderata, threat models, and practical challenges of deploying such a system
at internet scale, addressing key requirements of effectiveness, fidelity,
robustness, and security. SynthID-Image has been used to watermark over ten
billion images and video frames across Google's services and its corresponding
verification service is available to trusted testers. For completeness, we
present an experimental evaluation of an external model variant, SynthID-O,
which is available through partnerships. We benchmark SynthID-O against other
post-hoc watermarking methods from the literature, demonstrating
state-of-the-art performance in both visual quality and robustness to common
image perturbations. While this work centers on visual media, the conclusions
on deployment, constraints, and threat modeling generalize to other modalities,
including audio. This paper provides a comprehensive documentation for the
large-scale deployment of deep learning-based media provenance systems.

</details>


### [12] [Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects](https://arxiv.org/abs/2510.09269)
*Zirun Zhou,Zhengyang Xiao,Haochuan Xu,Jing Sun,Di Wang,Jingfeng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种针对视觉-语言-动作模型的目标导向后门攻击方法，通过在训练数据中注入物理对象作为触发器，使模型在遇到特定物理触发时执行预定义的目标动作，而正常输入下表现正常。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型依赖未筛选的训练数据集存在安全隐患，现有后门攻击大多假设白盒访问且仅导致任务失败而非执行特定动作。本文旨在揭示更实际的威胁：通过物理对象作为触发器操纵VLA模型。

Method: 基于LIBERO基准提出BadLIBERO数据集，包含多样化的物理触发器和目标导向后门动作。采用三级评估方法对受害VLA在攻击下的行为进行分类。

Result: 实验表明，当物理触发器存在时，受害VLA在97%的输入中成功实现后门目标，而在干净输入上性能零下降。动作轨迹和触发器颜色显著影响攻击性能，触发器大小影响较小。

Conclusion: GoBA攻击展示了VLA模型对物理触发器后门攻击的脆弱性，强调了训练数据安全的重要性，并为防御此类攻击提供了见解。

Abstract: Recent advances in vision-language-action (VLA) models have greatly improved
embodied AI, enabling robots to follow natural language instructions and
perform diverse tasks. However, their reliance on uncurated training datasets
raises serious security concerns. Existing backdoor attacks on VLAs mostly
assume white-box access and result in task failures instead of enforcing
specific actions. In this work, we reveal a more practical threat: attackers
can manipulate VLAs by simply injecting physical objects as triggers into the
training dataset. We propose goal-oriented backdoor attacks (GoBA), where the
VLA behaves normally in the absence of physical triggers but executes
predefined and goal-oriented actions in the presence of physical triggers.
Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO
that incorporates diverse physical triggers and goal-oriented backdoor actions.
In addition, we propose a three-level evaluation that categorizes the victim
VLA's actions under GoBA into three states: nothing to do, try to do, and
success to do. Experiments show that GoBA enables the victim VLA to
successfully achieve the backdoor goal in 97 percentage of inputs when the
physical trigger is present, while causing zero performance degradation on
clean inputs. Finally, by investigating factors related to GoBA, we find that
the action trajectory and trigger color significantly influence attack
performance, while trigger size has surprisingly little effect. The code and
BadLIBERO dataset are accessible via the project page at
https://goba-attack.github.io/.

</details>


### [13] [Assessing the Impact of Post-Quantum Digital Signature Algorithms on Blockchains](https://arxiv.org/abs/2510.09271)
*Alison Gonçalves Schemitt,Henrique Fan da Silva,Roben Castagna Lunardi,Diego Kreutz,Rodrigo Brandão Mansilha,Avelino Francisco Zorzo*

Main category: cs.CR

TL;DR: 本文提出了在后量子密码学（PQC）背景下，对区块链环境中加密算法性能进行基准测试的方法论，评估了多种PQC数字签名方案与传统ECDSA的性能对比。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展威胁传统加密算法安全，NIST在2024年标准化了多个PQC算法。区块链系统依赖的ECDSA等算法易受量子攻击，但PQC在区块链环境中的计算开销尚未充分研究。

Method: 提出在区块链环境中对PQC和传统加密算法进行基准测试的方法论，测量签名生成和验证时间，并在不同计算环境中进行大规模模拟评估。

Result: PQC算法在安全级别1仅引入轻微性能开销，在某些场景下高级别安全性能甚至显著优于ECDSA。例如ML-DSA在ARM笔记本上安全级别5的验证时间为0.14ms，而ECDSA为0.88ms。

Conclusion: PQC算法在区块链环境中具有可行性，提供了开源实现以确保可复现性并鼓励进一步研究。

Abstract: The advent of quantum computing threatens the security of traditional
encryption algorithms, motivating the development of post-quantum cryptography
(PQC). In 2024, the National Institute of Standards and Technology (NIST)
standardized several PQC algorithms, marking an important milestone in the
transition toward quantum-resistant security. Blockchain systems fundamentally
rely on cryptographic primitives to guarantee data integrity and transaction
authenticity. However, widely used algorithms such as ECDSA, employed in
Bitcoin, Ethereum, and other networks, are vulnerable to quantum attacks.
Although adopting PQC is essential for long-term security, its computational
overhead in blockchain environments remains largely unexplored. In this work,
we propose a methodology for benchmarking both PQC and traditional
cryptographic algorithms in blockchain contexts. We measure signature
generation and verification times across diverse computational environments and
simulate their impact at scale. Our evaluation focuses on PQC digital signature
schemes (ML-DSA, Dilithium, Falcon, Mayo, SLH-DSA, SPHINCS+, and Cross) across
security levels 1 to 5, comparing them to ECDSA, the current standard in
Bitcoin and Ethereum. Our results indicate that PQC algorithms introduce only
minor performance overhead at security level 1, while in some scenarios they
significantly outperform ECDSA at higher security levels. For instance, ML-DSA
achieves a verification time of 0.14 ms on an ARM-based laptop at level 5,
compared to 0.88 ms for ECDSA. We also provide an open-source implementation to
ensure reproducibility and encourage further research.

</details>


### [14] [Modern iOS Security Features -- A Deep Dive into SPTM, TXM, and Exclaves](https://arxiv.org/abs/2510.09272)
*Moritz Steffin,Jiska Classen*

Main category: cs.CR

TL;DR: 本文对苹果XNU内核的新安全机制SPTM进行了全面分析，揭示了其通过内存重类型和映射规则集引入信任域，将不同功能隔离，特别是将TXM等关键组件移出XNU直接控制范围，提高了系统安全性。


<details>
  <summary>Details</summary>
Motivation: 苹果XNU内核虽然标榜为混合内核，但实际上以单体方式运行，存在安全隐患。近年来苹果采取了更隔离的内核架构和微内核设计，但SPTM及相关安全机制缺乏科学讨论，对其理解有限。

Method: 通过分析SPTM作为内存重类型的唯一权威，研究其基于帧重类型和内存映射规则集的信任域机制，以及Exclaves安全特性的通信机制，包括xnuproxy和Tightbeam IPC框架。

Result: SPTM在系统中引入了信任域，有效隔离了不同功能，包括负责代码签名和权限验证的TXM。这些架构变化提高了系统安全性，关键敏感组件不再直接受XNU控制。

Conclusion: 新的安全机制显著提升了系统安全，即使内核被攻破也不再构成最高信任级别的直接威胁，为系统提供了额外的安全保障。

Abstract: The XNU kernel is the basis of Apple's operating systems. Although labeled as
a hybrid kernel, it is found to generally operate in a monolithic manner by
defining a single privileged trust zone in which all system functionality
resides. This has security implications, as a kernel compromise has immediate
and significant effects on the entire system. Over the past few years, Apple
has taken steps towards a more compartmentalized kernel architecture and a more
microkernel-like design. To date, there has been no scientific discussion of
SPTM and related security mechanisms. Therefore, the understanding of the
system and the underlying security mechanisms is minimal. In this paper, we
provide a comprehensive analysis of new security mechanisms and their
interplay, and create the first conclusive writeup considering all current
mitigations. SPTM acts as the sole authority regarding memory retyping. Our
analysis reveals that, through SPTM domains based on frame retyping and memory
mapping rule sets, SPTM introduces domains of trust into the system,
effectively gapping different functionalities from one another. Gapped
functionality includes the TXM, responsible for code signing and entitlement
verification. We further demonstrate how this introduction lays the groundwork
for the most recent security feature of Exclaves, and conduct an in-depth
analysis of its communication mechanisms. We discover multifold ways of
communication, most notably xnuproxy as a secure world request handler, and the
Tightbeam IPC framework. The architecture changes are found to increase system
security, with key and sensitive components being moved out of XNU's direct
reach. This also provides additional security guarantees in the event of a
kernel compromise, which is no longer an immediate threat at the highest trust
level.

</details>


### [15] [Clustering Deposit and Withdrawal Activity in Tornado Cash: A Cross-Chain Analysis](https://arxiv.org/abs/2510.09433)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: 该研究对Tornado Cash在以太坊、BNB智能链和Polygon上的活动进行了首次跨链实证分析，通过三种聚类启发式方法（地址复用、交易链接和FIFO时间匹配）成功将大量提现与存款重新关联，暴露了实际匿名性的显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管Tornado Cash使用加密技术切断存款者和提现者之间的链上痕迹，但用户行为和操作特性可能削弱其匿名性。研究旨在通过实证分析揭示实际使用中的匿名性漏洞。

Method: 引入三种聚类启发式方法：(i)地址复用启发式，(ii)交易链接启发式，(iii)新颖的先进先出(FIFO)时间匹配规则。这些方法共同用于重新关联存款和提现。

Result: 仅通过地址复用和交易链接启发式就能追踪5.1-12.6%的提现到原始存款，加入FIFO时间匹配启发式后关联率再提高15-22个百分点。统计测试证实这些FIFO匹配极不可能是偶然发生。总计关联了超过23亿美元的Tornado Cash提现到可识别存款。

Conclusion: 研究结果表明加密保证在日常使用中可能迅速失效，强调了规范用户行为和隐私感知协议设计的必要性。跨链可比较的泄露表明这是链无关的用户不当行为，而非特定链的协议缺陷。

Abstract: Tornado Cash is a decentralised mixer that uses cryptographic techniques to
sever the on-chain trail between depositors and withdrawers. In practice,
however, its anonymity can be undermined by user behaviour and operational
quirks. We conduct the first cross-chain empirical study of Tornado Cash
activity on Ethereum, BNB Smart Chain, and Polygon, introducing three
clustering heuristics-(i) address-reuse, (ii) transactional-linkage, and (iii)
a novel first-in-first-out (FIFO) temporal-matching rule. Together, these
heuristics reconnect deposits to withdrawals and deanonymise a substantial
share of recipients. Our analysis shows that 5.1 - 12.6% of withdrawals can
already be traced to their originating deposits through address reuse and
transactional linkage heuristics. Adding our novel First-In-First-Out (FIFO)
temporal-matching heuristic lifts the linkage rate by a further 15 - 22
percentage points. Statistical tests confirm that these FIFO matches are highly
unlikely to occur by chance. Comparable leakage across Ethereum, BNB Smart
Chain, and Polygon indicates chain-agnostic user misbehaviour, rather than
chain-specific protocol flaws. These results expose how quickly cryptographic
guarantees can unravel in everyday use, underscoring the need for both
disciplined user behaviour and privacy-aware protocol design. In total, our
heuristics link over $2.3 billion in Tornado Cash withdrawals to identifiable
deposits, exposing significant cracks in practical anonymity.

</details>


### [16] [The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash](https://arxiv.org/abs/2510.09443)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: 本文研究了美国对Tornado Cash的制裁影响，发现在2022年8月制裁实施后，该隐私协议的交易量、用户多样性和整体使用率显著下降，尽管2025年3月制裁部分解除后活动有所恢复，但反弹有限。


<details>
  <summary>Details</summary>
Motivation: 研究监管干预对去中心化协议的影响，特别是美国财政部对Tornado Cash的制裁如何影响该隐私增强协议的运作。

Method: 基于以太坊、BNB智能链和Polygon三大区块链的交易数据进行分析，追踪制裁前后的平台活动变化。

Result: 制裁导致Tornado Cash的交易量、用户多样性和协议使用率显著且持续下降；制裁部分解除后活动仅部分恢复，反弹有限。

Conclusion: Tornado Cash案例表明监管干预能够影响去中心化协议，但也凸显了在去中心化环境中完全执行此类措施的挑战。

Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart
contract protocol designed to enhance transaction privacy. Following the U.S.
Department of the Treasury's sanctions against Tornado Cash in August 2022,
platform activity declined sharply. We document a significant and sustained
reduction in transaction volume, user diversity, and overall protocol
utilization after the sanctions were imposed. Our analysis draws on transaction
data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We
further examine developments following the partial lifting and eventual removal
of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.
Although activity partially recovered, the rebound remained limited. The
Tornado Cash case illustrates how regulatory interventions can affect
decentralized protocols, while also highlighting the challenges of fully
enforcing such measures in decentralized environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Optimizing delivery for quick commerce factoring qualitative assessment of generated routes](https://arxiv.org/abs/2510.08671)
*Milon Bhattacharya,Milan Kumar*

Main category: cs.AI

TL;DR: 该研究提出使用大语言模型(LLMs)来评估车辆路径规划(VRP)生成的配送路线，通过政策标准来识别路由问题，提高印度等发展中国家最后一公里物流的效率。


<details>
  <summary>Details</summary>
Motivation: 印度电商市场快速增长，最后一公里配送占运营成本近一半。传统VRP求解器在现实场景中效果有限，因为存在非结构化地址、不完整地图和距离估算计算约束等问题。

Method: 提出一个框架，使用LLMs根据政策标准来评估VRP生成的路由，物流运营商可以评估和优先选择更高效的配送计划。生成了400个案例进行标注和评估。

Result: 开源LLMs识别路由问题的准确率达到79%，专有推理模型最高达到86%。LLM基于的路由评估可以作为超越传统距离和时间指标的有效可扩展评估层。

Conclusion: 基于LLM的VRP生成路线评估可以成为改善最后一公里物流成本效率、配送可靠性和可持续性的有效方法，特别适用于印度等发展中国家。

Abstract: Indias e-commerce market is projected to grow rapidly, with last-mile
delivery accounting for nearly half of operational expenses. Although vehicle
routing problem (VRP) based solvers are widely used for delivery planning,
their effectiveness in real-world scenarios is limited due to unstructured
addresses, incomplete maps, and computational constraints in distance
estimation. This study proposes a framework that employs large language models
(LLMs) to critique VRP-generated routes against policy-based criteria, allowing
logistics operators to evaluate and prioritise more efficient delivery plans.
As a illustration of our approach we generate, annotate and evaluated 400 cases
using large language models. Our study found that open-source LLMs identified
routing issues with 79% accuracy, while proprietary reasoning models achieved
reach upto 86%. The results demonstrate that LLM-based evaluation of
VRP-generated routes can be an effective and scalable layer of evaluation which
goes beyond beyond conventional distance and time based metrics. This has
implications for improving cost efficiency, delivery reliability, and
sustainability in last-mile logistics, especially for developing countries like
India.

</details>


### [18] [Robust Heuristic Algorithm Design with LLMs](https://arxiv.org/abs/2510.08755)
*Pantea Karimi,Dany Rouhana,Pooria Namyar,Siva Kesava Reddy Kakarla,Venkat Arun,Behnaz Arzani*

Main category: cs.AI

TL;DR: 通过向LLM提供启发式算法表现不佳的实例、解释原因并针对输入空间特定区域进行专门设计，可以生成更鲁棒和性能更好的启发式算法。


<details>
  <summary>Details</summary>
Motivation: 现有使用LLM设计启发式算法的方法缺乏对算法表现不佳原因的解释和改进建议，导致生成的算法鲁棒性不足。

Method: 提出三个简单但有效的改进：(1)向LLM展示启发式算法表现不佳的实例；(2)解释表现不佳的原因；(3)针对输入空间的特定区域进行专门设计。

Result: 生成的启发式算法在最坏情况下性能比FunSearch好约28倍，平均性能也有所提升，同时保持运行时间不变。

Conclusion: 通过向LLM提供表现不佳实例的解释和改进建议，可以显著提升生成启发式算法的鲁棒性和性能。

Abstract: We posit that we can generate more robust and performant heuristics if we
augment approaches using LLMs for heuristic design with tools that explain why
heuristics underperform and suggestions about how to fix them. We find even
simple ideas that (1) expose the LLM to instances where the heuristic
underperforms; (2) explain why they occur; and (3) specialize design to regions
in the input space, can produce more robust algorithms compared to existing
techniques~ -- ~the heuristics we produce have a $\sim28\times$ better
worst-case performance compared to FunSearch, improve average performance, and
maintain the runtime.

</details>


### [19] [Everyone prefers human writers, including AI](https://arxiv.org/abs/2510.08831)
*Wouter Haverals,Meredith Martin*

Main category: cs.AI

TL;DR: 该研究通过控制实验比较人类和AI对文学风格的评估偏见，发现AI系统不仅复制而且放大了人类对AI生成内容的偏见，表现出更强的反AI偏见。


<details>
  <summary>Details</summary>
Motivation: 随着AI写作工具的普及，需要理解人类和机器如何评估文学风格这一主观领域，特别是评估中的归因偏见问题。

Method: 研究1比较556名人类参与者和13个AI模型对Queneau原作与GPT-4生成版本的评估；研究2测试14×14矩阵的AI评估者和创建者之间的偏见泛化。

Result: 人类表现出+13.7百分点的偏见，AI模型表现出+34.3百分点的偏见（是人类的2.5倍）。AI系统在标记为'AI生成'时系统性地贬低创意内容。

Conclusion: AI模型在训练过程中吸收了人类对人工创造力的文化偏见，不仅复制而且放大了这种偏见，导致评估标准基于感知作者身份而反转。

Abstract: As AI writing tools become widespread, we need to understand how both humans
and machines evaluate literary style, a domain where objective standards are
elusive and judgments are inherently subjective. We conducted controlled
experiments using Raymond Queneau's Exercises in Style (1947) to measure
attribution bias across evaluators. Study 1 compared human participants (N=556)
and AI models (N=13) evaluating literary passages from Queneau versus
GPT-4-generated versions under three conditions: blind, accurately labeled, and
counterfactually labeled. Study 2 tested bias generalization across a
14$\times$14 matrix of AI evaluators and creators. Both studies revealed
systematic pro-human attribution bias. Humans showed +13.7 percentage point
(pp) bias (Cohen's h = 0.28, 95% CI: 0.21-0.34), while AI models showed +34.3
percentage point bias (h = 0.70, 95% CI: 0.65-0.76), a 2.5-fold stronger effect
(P$<$0.001). Study 2 confirmed this bias operates across AI architectures
(+25.8pp, 95% CI: 24.1-27.6%), demonstrating that AI systems systematically
devalue creative content when labeled as "AI-generated" regardless of which AI
created it. We also find that attribution labels cause evaluators to invert
assessment criteria, with identical features receiving opposing evaluations
based solely on perceived authorship. This suggests AI models have absorbed
human cultural biases against artificial creativity during training. Our study
represents the first controlled comparison of attribution bias between human
and artificial evaluators in aesthetic judgment, revealing that AI systems not
only replicate but amplify this human tendency.

</details>


### [20] [What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment](https://arxiv.org/abs/2510.08847)
*Allison Sihan Jia,Daniel Huang,Nikhil Vytla,Nirvika Choudhury,John C Mitchell,Anupam Datta*

Main category: cs.AI

TL;DR: 提出了Agent GPA（目标-计划-行动）评估框架，包含五个评估指标：目标实现、逻辑一致性、执行效率、计划质量和计划遵循。该框架能系统性地覆盖广泛的智能体失败情况，支持LLM评估器与人工标注高度一致，并能准确定位错误以改进智能体性能。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统性的评估范式来评估智能体的操作循环（设定目标、制定计划、执行行动），以全面覆盖智能体的各种失败模式。

Method: 基于智能体的目标-计划-行动操作循环构建评估框架，包含五个核心评估指标，并在TRAIL/GAIA公开数据集和生产级数据智能体的内部数据集上进行实验验证。

Result: 实验结果显示：该框架能覆盖TRAIL/GAIA基准数据集中的所有智能体错误；LLM评估器与人工标注的一致性达到80%-95%以上；错误定位准确率达到86%。

Conclusion: Agent GPA框架提供了一种系统化的方法来评估和诊断智能体性能，能够有效识别和定位各种类型的智能体失败，为智能体性能的针对性改进提供了有力支持。

Abstract: We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation
paradigm based on an agent's operational loop of setting goals, devising plans,
and executing actions. The framework includes five evaluation metrics: Goal
Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan
Adherence. Logical Consistency checks that an agent's actions are consistent
with its prior actions. Execution Efficiency checks whether the agent executes
in the most efficient way to achieve its goal. Plan Quality checks whether an
agent's plans are aligned with its goals; Plan Adherence checks if an agent's
actions are aligned with its plan; and Goal Fulfillment checks that agent's
final outcomes match the stated goals. Our experimental results on two
benchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for
a production-grade data agent - show that this framework (a) provides a
systematic way to cover a broad range of agent failures, including all agent
errors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that
exhibit strong agreement with human annotation, covering 80% to over 95%
errors; and (c) localizes errors with 86% agreement to enable targeted
improvement of agent performance.

</details>


### [21] [ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review](https://arxiv.org/abs/2510.08867)
*Gaurav Sahu,Hugo Larochelle,Laurent Charlin,Christopher Pal*

Main category: cs.AI

TL;DR: ReviewerToo是一个模块化框架，用于研究和部署AI辅助同行评审，在ICLR 2025数据集上达到81.8%的接受/拒绝分类准确率，接近人类评审员的83.9%。


<details>
  <summary>Details</summary>
Motivation: 解决传统同行评审存在的不一致性、评审员主观性和可扩展性挑战，通过AI辅助来补充人类判断。

Method: 开发ReviewerToo框架，支持专门的评审员角色和结构化评估标准，在ICLR 2025的1,963篇论文提交上进行验证，使用gpt-oss-120b模型。

Result: AI评审员在事实核查和文献覆盖方面表现出色，但在评估方法新颖性和理论贡献方面仍有困难；AI生成的评审质量高于人类平均水平，但仍落后于最强专家。

Conclusion: AI可以增强同行评审的一致性、覆盖范围和公平性，但复杂评估仍需领域专家；提出了将AI整合到同行评审流程的指南，为混合评审系统奠定基础。

Abstract: Peer review is the cornerstone of scientific publishing, yet it suffers from
inconsistencies, reviewer subjectivity, and scalability challenges. We
introduce ReviewerToo, a modular framework for studying and deploying
AI-assisted peer review to complement human judgment with systematic and
consistent assessments. ReviewerToo supports systematic experiments with
specialized reviewer personas and structured evaluation criteria, and can be
partially or fully integrated into real conference workflows. We validate
ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR
2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy
for the task of categorizing a paper as accept/reject compared to 83.9% for the
average human reviewer. Additionally, ReviewerToo-generated reviews are rated
as higher quality than the human average by an LLM judge, though still trailing
the strongest expert contributions. Our analysis highlights domains where AI
reviewers excel (e.g., fact-checking, literature coverage) and where they
struggle (e.g., assessing methodological novelty and theoretical
contributions), underscoring the continued need for human expertise. Based on
these findings, we propose guidelines for integrating AI into peer-review
pipelines, showing how AI can enhance consistency, coverage, and fairness while
leaving complex evaluative judgments to domain experts. Our work provides a
foundation for systematic, hybrid peer-review systems that scale with the
growth of scientific publishing.

</details>


### [22] [GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare](https://arxiv.org/abs/2510.08872)
*Siqi Zhu,David Zhang,Pedro Cisneros-Velarde,Jiaxuan You*

Main category: cs.AI

TL;DR: GTAlign是一个基于博弈论的对齐框架，通过在推理过程中将用户-LLM交互建模为战略游戏，并在训练中引入相互福利奖励，来优化LLM的响应质量和社会效率。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法假设最大化模型奖励等同于最大化用户福利，但实践中LLM可能产生过于冗长或次优的响应，类似于囚徒困境中的社会次优结果。

Method: 在推理过程中构建收益矩阵评估双方福利，选择互利行动；在训练中引入相互福利奖励强化合作响应；还包含基于博弈论推理的动态适应技术。

Result: 实验表明GTAlign在多样化任务中显著提升了推理效率、答案质量和相互福利。

Conclusion: GTAlign通过博弈论决策机制有效解决了LLM对齐中的社会效率问题，实现了用户和模型的双赢。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in reasoning,
yet sometimes produce responses that are suboptimal for users in tasks such as
writing, information seeking, or providing practical guidance. Conventional
alignment practices typically assume that maximizing model reward also
maximizes user welfare, but this assumption frequently fails in practice:
models may over-clarify or generate overly verbose reasoning when users prefer
concise answers. Such behaviors resemble the prisoner's dilemma, where
individually rational choices lead to socially suboptimal outcomes. The
fundamental challenge is the lack of a principled decision making mechanism
that mutually benefits both the LLM and the user. We propose Game-Theoretic
Alignment (GTAlign), an alignment framework that integrates game-theoretic
decision making into both reasoning and training. During reasoning, the model
explicitly treats user-LLM interaction as a strategic game: it constructs
payoff matrices within its reasoning chain to estimate welfare for both itself
and the user, and then selects actions that are mutually beneficial. During
training, we introduce a mutual welfare reward that reinforces cooperative
responses, aligning model behavior with socially efficient outcomes. In
addition, we introduce an inference technique that leverages game-theoretic
reasoning to dynamically adapt LLM's response when pricing policies of LLM
service change. Extensive experiments demonstrate that GTAlign substantially
improves reasoning efficiency, answer quality, and mutual welfare compared to
baselines across diverse tasks. The code is available at
https://github.com/ulab-uiuc/GTAlign .

</details>


### [23] [RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation](https://arxiv.org/abs/2510.08931)
*Ashish Kattamuri,Harshwardhan Fartale,Arpita Vats,Rahul Raja,Ishita Prasad*

Main category: cs.AI

TL;DR: RADAR是一个通过机制可解释性检测LLM数据污染的新框架，通过区分基于记忆和基于推理的模型响应，使用37个特征实现93%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 数据污染对可靠的LLM评估构成重大挑战，模型可能通过记忆训练数据而非展示真正推理能力来获得高性能表现。

Method: RADAR提取37个特征，涵盖表面级置信度轨迹和深层机制特性，包括注意力专业化、电路动态和激活流模式，使用这些特征的分类器集成进行检测。

Result: RADAR在多样化评估集上达到93%的准确率，在清晰案例上表现完美，在具有挑战性的模糊示例上达到76.7%的准确率。

Conclusion: 这项工作展示了机制可解释性在超越传统表面级指标推进LLM评估方面的潜力。

Abstract: Data contamination poses a significant challenge to reliable LLM evaluation,
where models may achieve high performance by memorizing training data rather
than demonstrating genuine reasoning capabilities. We introduce RADAR (Recall
vs. Reasoning Detection through Activation Representation), a novel framework
that leverages mechanistic interpretability to detect contamination by
distinguishing recall-based from reasoning-based model responses. RADAR
extracts 37 features spanning surface-level confidence trajectories and deep
mechanistic properties including attention specialization, circuit dynamics,
and activation flow patterns. Using an ensemble of classifiers trained on these
features, RADAR achieves 93\% accuracy on a diverse evaluation set, with
perfect performance on clear cases and 76.7\% accuracy on challenging ambiguous
examples. This work demonstrates the potential of mechanistic interpretability
for advancing LLM evaluation beyond traditional surface-level metrics.

</details>


### [24] [FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation](https://arxiv.org/abs/2510.08945)
*Samuel Hildebrand,Curtis Taylor,Sean Oesch,James M Ghawaly Jr,Amir Sadovnik,Ryan Shivers,Brandon Schreiber,Kevin Kurian*

Main category: cs.AI

TL;DR: 提出了一个评估检索增强生成（RAG）管道的基准，包括多模态信息处理能力评估、短语级召回指标、幻觉检测方法，并对开源和闭源系统进行了比较评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注检索等特定方面，缺乏对RAG管道整体能力的评估，特别是多模态信息处理能力。

Method: 创建了93个问题的数据集评估文本、表格、图像等多模态信息处理；提出短语级召回指标和最近邻嵌入分类器检测幻觉；比较了2个开源和4个闭源系统。

Result: 闭源管道在正确性和幻觉检测方面显著优于开源管道，特别是在多模态和跨文档问题上；人工评估显示指标与人类判断高度一致（4.62/5和4.53/5）。

Conclusion: 提出的基准能够有效评估RAG管道的整体性能，闭源系统在多模态信息处理方面表现更优，评估指标与人类判断具有良好一致性。

Abstract: Retrieval-augmented generation (RAG) has emerged as a promising paradigm for
improving factual accuracy in large language models (LLMs). We introduce a
benchmark designed to evaluate RAG pipelines as a whole, evaluating a
pipeline's ability to ingest, retrieve, and reason about several modalities of
information, differentiating it from existing benchmarks that focus on
particular aspects such as retrieval. We present (1) a small, human-created
dataset of 93 questions designed to evaluate a pipeline's ability to ingest
textual data, tables, images, and data spread across these modalities in one or
more documents; (2) a phrase-level recall metric for correctness; (3) a
nearest-neighbor embedding classifier to identify potential pipeline
hallucinations; (4) a comparative evaluation of 2 pipelines built with
open-source retrieval mechanisms and 4 closed-source foundation models; and (5)
a third-party human evaluation of the alignment of our correctness and
hallucination metrics. We find that closed-source pipelines significantly
outperform open-source pipelines in both correctness and hallucination metrics,
with wider performance gaps in questions relying on multimodal and
cross-document information. Human evaluation of our metrics showed average
agreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5
Likert scale (5 indicating "strongly agree").

</details>


### [25] [EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory](https://arxiv.org/abs/2510.08958)
*Zirui Liao*

Main category: cs.AI

TL;DR: EcphoryRAG是一个基于实体中心知识图谱的RAG框架，通过提取和存储核心实体及其元数据，在索引阶段减少94%的token消耗。检索时通过提取查询中的线索实体，在知识图谱中进行可扩展的多跳关联搜索，并动态推断实体间的隐含关系来填充上下文，实现深度推理。


<details>
  <summary>Details</summary>
Motivation: 受人类认知神经科学启发，人类利用线索激活实体中心记忆痕迹进行复杂多跳回忆。作者希望开发一个类似的实体中心知识图谱RAG框架，通过轻量级索引和动态关系推断来提升复杂问答性能。

Method: 1. 索引阶段仅提取和存储核心实体及其元数据；2. 检索阶段提取查询中的线索实体；3. 在知识图谱中进行多跳关联搜索；4. 动态推断实体间的隐含关系来填充上下文。

Result: 在2WikiMultiHop、HotpotQA和MuSiQue基准测试中，EcphoryRAG实现了新的最先进水平，将平均精确匹配分数从0.392提升到0.474，相比HippoRAG等强KG-RAG方法有显著改进。

Conclusion: 实体-线索-多跳检索范式在复杂问答任务中具有显著效果，验证了该方法的有效性。

Abstract: Cognitive neuroscience research indicates that humans leverage cues to
activate entity-centered memory traces (engrams) for complex, multi-hop
recollection. Inspired by this mechanism, we introduce EcphoryRAG, an
entity-centric knowledge graph RAG framework. During indexing, EcphoryRAG
extracts and stores only core entities with corresponding metadata, a
lightweight approach that reduces token consumption by up to 94\% compared to
other structured RAG systems. For retrieval, the system first extracts cue
entities from queries, then performs a scalable multi-hop associative search
across the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit
relations between entities to populate context, enabling deep reasoning without
exhaustive pre-enumeration of relationships. Extensive evaluations on the
2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG
sets a new state-of-the-art, improving the average Exact Match (EM) score from
0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate
the efficacy of the entity-cue-multi-hop retrieval paradigm for complex
question answering.

</details>


### [26] [DualResearch: Entropy-Gated Dual-Graph Retrieval for Answer Reconstruction](https://arxiv.org/abs/2510.08959)
*Jinxin Shi,Zongsheng Cao,Runmin Ma,Yusong Hu,Jie Zhou,Xin Li,Lei Bai,Liang He,Bo Zhang*

Main category: cs.AI

TL;DR: DualResearch是一个检索和融合框架，通过联合建模广度语义图和深度因果图来解决深度研究框架中的上下文污染、证据支持薄弱和执行路径脆弱等问题，显著提升了科学推理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度研究框架虽然能够协调外部工具进行复杂的多步骤科学推理，但仍存在上下文污染、证据支持薄弱和执行路径脆弱等问题，需要更有效的解决方案。

Method: 提出DualResearch框架，联合建模两个互补图：广度语义图编码稳定的背景知识，深度因果图捕获执行来源。每个图都有层原生相关性函数，广度使用种子锚定的语义扩散，深度使用因果语义路径匹配和可靠性加权。通过熵门控规则在log空间融合答案分布。

Result: 在科学推理基准HLE和GPQA上表现出竞争力。使用开源系统InternAgent的日志文件，在HLE上的准确率提高了7.7%，在GPQA上提高了6.06%。

Conclusion: DualResearch作为深度研究系统的补充，能够将冗长的多工具执行日志压缩为简洁的推理图，稳定有效地重构答案，显著提升了科学推理性能。

Abstract: The deep-research framework orchestrates external tools to perform complex,
multi-step scientific reasoning that exceeds the native limits of a single
large language model. However, it still suffers from context pollution, weak
evidentiary support, and brittle execution paths. To address these issues, we
propose DualResearch, a retrieval and fusion framework that matches the
epistemic structure of tool-intensive reasoning by jointly modeling two
complementary graphs: a breadth semantic graph that encodes stable background
knowledge, and a depth causal graph that captures execution provenance. Each
graph has a layer-native relevance function, seed-anchored semantic diffusion
for breadth, and causal-semantic path matching with reliability weighting for
depth. To reconcile their heterogeneity and query-dependent uncertainty,
DualResearch converts per-layer path evidence into answer distributions and
fuses them in log space via an entropy-gated rule with global calibration. The
fusion up-weights the more certain channel and amplifies agreement. As a
complement to deep-research systems, DualResearch compresses lengthy multi-tool
execution logs into a concise reasoning graph, and we show that it can
reconstruct answers stably and effectively. On the scientific reasoning
benchmarks HLE and GPQA, DualResearch achieves competitive performance. Using
log files from the open-source system InternAgent, its accuracy improves by
7.7% on HLE and 6.06% on GPQA.

</details>


### [27] [Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging](https://arxiv.org/abs/2510.08987)
*Qixiang Yin,Huanjin Yao,Jianghao Chen,Jiaxing Huang,Zhicheng Zhao,Fei Su*

Main category: cs.AI

TL;DR: Tiny-R1V是一个轻量级的3B参数多模态大语言模型，通过两阶段优化实现更快的推理速度和更高的准确性，统一了多任务多模态推理，同时使用更少的token。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在推理效率方面面临诸多挑战，如模型规模大、过度思考以及在轻量级场景下准确性受损。然而，关于轻量级MLLM推理能力的研究相当缺乏。

Method: 采用两阶段优化方法：第一阶段引入LIPO（长度感知相对策略优化）强化学习方法，动态调整组内响应优势，优先考虑简洁高质量响应；第二阶段提出AMM（自适应模型合并），一种无需训练的方法，将多个专家模型合并为统一架构，通过梯度投影正则化损失函数自适应调整任务向量权重。

Result: 在十个广泛使用的推理基准测试（涵盖数学、结构化数据、OCR和通用能力）上进行了广泛评估，展示了Tiny-R1V的卓越性能。

Conclusion: Tiny-R1V使轻量级模型能够在多样化的多模态推理任务中表现出色，实现了更快的推理速度和更高的准确性。

Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated
remarkable capabilities across diverse tasks, they encounter numerous
challenges in terms of reasoning efficiency, such as large model size,
overthinking, and compromised accuracy in lightweight scenarios. However,
research on the reasoning capabilities of lightweight MLLMs is quite lacking.
To this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves
faster inference and higher accuracy via a two-stage optimization, while
unifying multimodal reasoning across multiple tasks and using fewer tokens. In
the first stage, Tiny-R1V introduces Length-Informed Relative Policy
Optimization (LIPO), a novel reinforcement learning method, to train each
reasoning model. The LIPO is designed to dynamically adjusts advantages of
responses within groups, that is, by prioritizing concise yet high-quality
responses to encourage the generation of shorter and more accurate response. In
the second stage, we propose Adaptive Model Merging (AMM), a training-free
model merging method that merges multiple specialist models into a unified
architecture. Specifically, AMM adaptively adjusts the weights of task vectors
and robustly optimizes the merged vectors via a novel gradient projection
regularization loss function, thus mitigating redundant conflicts between them.
Extensive evaluations on ten widely-used reasoning benchmarks covering
mathematics, structured data (charts, tables, documents), OCR, and general
capabilities showcase the superior performance of Tiny-R1V, enabling
lightweight models to excel in diverse multimodal reasoning tasks.

</details>


### [28] [TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation](https://arxiv.org/abs/2510.09011)
*Yincen Qu,Huan Xiao,Feng Li,Hui Zhou,Xiangying Dai*

Main category: cs.AI

TL;DR: 该论文提出了一个统一的旅行规划基准，通过将细粒度标准整合为单一奖励来评估LLM的规划能力，并发布了包含4870个查询的大规模数据集。实验表明，强化学习在提高行程可行性方面优于提示工程和监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估LLM旅行规划能力时，往往无法充分评估计划的可行性、可靠性和参与度，需要更全面的评估框架。

Method: 开发了统一的旅行规划基准，将细粒度标准整合为单一奖励；创建了包含4870个查询的大规模数据集；使用多种方法（测试时计算、神经符号方法、监督微调、GRPO强化学习）进行实验。

Result: 评估器与旅行专家注释达到了60.75%的一致性，优于多个LLM作为评判基准；强化学习在提高行程可行性方面优于提示工程和监督学习，获得了更高的统一奖励分数。

Conclusion: 提出的统一基准能够有效评估LLM的旅行规划能力，强化学习方法在提高规划质量方面表现最佳，为未来旅行规划系统的开发提供了有价值的评估工具。

Abstract: Travel planning is a valuable yet complex task that poses significant
challenges even for advanced large language models (LLMs). While recent
benchmarks have advanced in evaluating LLMs' planning capabilities, they often
fall short in evaluating feasibility, reliability, and engagement of travel
plans. We introduce a comprehensive benchmark for travel planning that unifies
fine-grained criteria into a single reward, enabling direct comparison of plan
quality and seamless integration with reinforcement learning (RL). Our
evaluator achieves moderate agreement with travel-expert annotations (60.75\%)
and outperforms multiple LLM-as-judge baselines. We further release a
large-scale dataset of 4,870 queries including 219 real-world, free-form
requests for generalization to authentic user intent. Using this benchmark, we
conduct extensive experiments across diverse methods and LLMs, including
test-time computation, neuro-symbolic approaches, supervised fine-tuning, and
RL via GRPO. Across base models, RL generally improves itinerary feasibility
over prompt-only and supervised baselines, yielding higher unified reward
scores.

</details>


### [29] [RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows](https://arxiv.org/abs/2510.09021)
*Hamed Mahdavi,Pouria Mahdavinia,Samira Malek,Pegah Mohammadipour,Alireza Hashemi,Majid Daliri,Alireza Farhadi,Amir Khasahmadi,Niloofar Mireshghallah,Vasant Honavar*

Main category: cs.AI

TL;DR: 该论文评估了先进LLMs在数学证明评分方面的能力，发现模型能可靠识别错误证明但在部分得分分配上存在校准差距，为此提出了基于智能体工作流程的多步骤评分方法，显著提高了与人类评分的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着SOTA LLMs在解决奥林匹克数学问题方面取得显著进展（能解决IMO 2025的大部分问题），需要评估这些模型在证明评分方面的能力，包括错误检测、严重性判断和公平分数分配，而不仅仅是二元正确性判断。

Method: 使用包含90个Gemini 2.5 Pro生成解决方案的语料库（按1-4分制评分并带有详细错误标注）和MathArena的IMO/USAMO 2025解决方案集（按0-7分制评分），引入智能体工作流程来提取和分析参考解决方案，自动推导问题特定的评分标准，实现多步骤评分过程。

Result: 分析显示模型能可靠标记错误（包括细微错误）解决方案，但在部分得分分配上存在校准差距。提出的工作流程在标注语料库和MathArena上都实现了与人类评分更高的协议一致性，并在部分得分处理上更加一致。

Conclusion: 智能体工作流程能有效提高LLMs在数学证明评分方面的性能，特别是在部分得分分配的一致性方面，为未来研究提供了代码、数据和提示/日志资源。

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [30] [Repairing Regex Vulnerabilities via Localization-Guided Instructions](https://arxiv.org/abs/2510.09037)
*Sicheol Sung,Joonghyuk Hahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: 提出了一种混合框架LRR，结合符号模块和LLM来修复正则表达式拒绝服务漏洞，在保持语义正确性的同时提高修复率


<details>
  <summary>Details</summary>
Motivation: 正则表达式在现代计算中广泛使用，但存在ReDoS漏洞。现有方法存在权衡：基于符号规则的方法精确但无法修复复杂模式，LLM具有泛化能力但缺乏可靠性

Method: LRR混合框架：首先使用确定性符号模块定位易受攻击的子模式，然后调用LLM为隔离的片段生成语义等效的修复

Result: 成功解决了基于规则修复无法处理的复杂修复案例，同时避免了纯LLM方法的语义错误，修复率比现有最优方法提高了15.4%

Conclusion: 该工作为自动修复问题提供了经过验证的方法论，通过解耦问题识别和修复过程，在利用LLM泛化能力的同时确保可靠性

Abstract: Regular expressions (regexes) are foundational to modern computing for
critical tasks like input validation and data parsing, yet their ubiquity
exposes systems to regular expression denial of service (ReDoS), a
vulnerability requiring automated repair methods. Current approaches, however,
are hampered by a trade-off. Symbolic, rule-based system are precise but fails
to repair unseen or complex vulnerability patterns. Conversely, large language
models (LLMs) possess the necessary generalizability but are unreliable for
tasks demanding strict syntactic and semantic correctness. We resolve this
impasse by introducing a hybrid framework, localized regex repair (LRR),
designed to harness LLM generalization while enforcing reliability. Our core
insight is to decouple problem identification from the repair process. First, a
deterministic, symbolic module localizes the precise vulnerable subpattern,
creating a constrained and tractable problem space. Then, the LLM invoked to
generate a semantically equivalent fix for this isolated segment. This combined
architecture successfully resolves complex repair cases intractable for
rule-based repair while avoiding the semantic errors of LLM-only approaches.
Our work provides a validated methodology for solving such problems in
automated repair, improving the repair rate by 15.4%p over the
state-of-the-art. Our code is available at https://github.com/cdltlehf/LRR.

</details>


### [31] [MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction](https://arxiv.org/abs/2510.09049)
*Joonghyuk Hahn,Soohan Lim,Yo-Sub Han*

Main category: cs.AI

TL;DR: MEC³O是一个多专家共识系统，通过将LLMs分配到特定复杂度类别并让它们进行结构化辩论，结合加权共识机制来预测代码时间复杂度，在CodeComplex数据集上表现优于开源基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在代码时间复杂度预测中表现不均衡，没有单一模型在所有复杂度类别上都表现优异，需要一种能够整合不同模型优势的方法。

Method: 提出MEC³O多专家共识系统：1）基于性能将LLMs分配到特定复杂度类别；2）提供类别专业化指令；3）专家进行结构化辩论；4）通过加权共识机制整合预测结果。

Result: 在CodeComplex数据集上，MEC³O比开源基线模型准确率和macro-F1分数至少提高10%，在macro-F1分数上平均超过GPT-4o-mini，与GPT-4o和GPT-o4-mini的F1分数相当。

Conclusion: 多专家辩论和加权共识策略能有效生成最终预测，证明了该方法在代码复杂度预测任务中的有效性。

Abstract: Predicting the complexity of source code is essential for software
development and algorithm analysis. Recently, Baik et al. (2025) introduced
CodeComplex for code time complexity prediction. The paper shows that LLMs
without fine-tuning struggle with certain complexity classes. This suggests
that no single LLM excels at every class, but rather each model shows
advantages in certain classes. We propose MEC$^3$O, a multi-expert consensus
system, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs
to complexity classes based on their performance and provides them with
class-specialized instructions, turning them into experts. These experts engage
in structured debates, and their predictions are integrated through a weighted
consensus mechanism. Our expertise assignments to LLMs effectively handle
Degeneration-of-Thought, reducing reliance on a separate judge model, and
preventing convergence to incorrect majority opinions. Experiments on
CodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving
at least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini
in macro-F1 scores on average and demonstrates competitive on-par F1 scores to
GPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of
multi-expert debates and weight consensus strategy to generate the final
predictions. Our code and data is available at
https://github.com/suhanmen/MECO.

</details>


### [32] [PAC Reasoning: Controlling the Performance Loss for Efficient Reasoning](https://arxiv.org/abs/2510.09133)
*Hao Zeng,Jianguo Huang,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出PAC推理方法，通过置信上界控制性能损失，在用户指定的性能损失容忍度下动态切换思考与非思考模式以节省计算成本


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂问题解决中表现出色但部署成本高，现有动态切换方法存在额外推理错误且缺乏性能损失统计保证，难以应用于高风险场景

Method: 构建性能损失的单调函数置信上界，确定切换到非思考模型的阈值，以分布无关方式确保有界性能损失

Result: 在推理基准测试中，该方法能节省计算预算并控制用户指定的性能损失

Conclusion: PAC推理方法为高效推理提供了统计保证，能在保证性能的前提下显著降低计算成本

Abstract: Large reasoning models (LRMs) have achieved remarkable progress in complex
problem-solving tasks. Despite this success, LRMs typically suffer from high
computational costs during deployment, highlighting a need for efficient
inference. A popular direction of efficiency improvement is to switch the LRM
between thinking and nonthinking modes dynamically. However, such approaches
often introduce additional reasoning errors and lack statistical guarantees for
the performance loss, which are critical for high-stakes applications. In this
work, we propose Probably Approximately Correct (PAC) reasoning that controls
the performance loss under the user-specified performance loss tolerance. In
particular, we construct an upper confidence bound on the performance loss,
formulated as a monotone function of the uncertainty score, and subsequently
determine a threshold for switching to the nonthinking model. Theoretically,
using the threshold to switch between the thinking and nonthinking modes
ensures bounded performance loss in a distribution-free manner. Our
comprehensive experiments on reasoning benchmarks show that the proposed method
can save computational budgets and control the user-specified performance loss.

</details>


### [33] [Dr. Bias: Social Disparities in AI-Powered Medical Guidance](https://arxiv.org/abs/2510.09162)
*Emma Kondrup,Anne Imouza*

Main category: cs.AI

TL;DR: 本文探索性分析了LLM在生成医疗建议时对不同社会群体（基于性别、年龄、种族）的系统性差异，发现土著和双性患者获得的建议可读性更差、更复杂，交叉群体中这种趋势更加明显。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗领域的广泛应用，需要评估其是否考虑医疗的社会性本质，特别是健康差异和社会偏见如何影响LLM生成的医疗建议。

Method: 通过模拟不同患者档案（性别、年龄、种族）向LLM提出一系列医疗问题，比较生成回答的自然语言特征。

Result: LLM生成的医疗建议在不同社会群体间存在系统性差异，土著和双性患者获得的建议可读性更低、更复杂，交叉群体中差异更加明显。

Conclusion: 鉴于用户对LLM的信任度增加，需要提高AI素养，并呼吁AI开发者紧急调查和缓解这些系统性差异，确保公平的患者支持。

Abstract: With the rapid progress of Large Language Models (LLMs), the general public
now has easy and affordable access to applications capable of answering most
health-related questions in a personalized manner. These LLMs are increasingly
proving to be competitive, and now even surpass professionals in some medical
capabilities. They hold particular promise in low-resource settings,
considering they provide the possibility of widely accessible, quasi-free
healthcare support. However, evaluations that fuel these motivations highly
lack insights into the social nature of healthcare, oblivious to health
disparities between social groups and to how bias may translate into
LLM-generated medical advice and impact users. We provide an exploratory
analysis of LLM answers to a series of medical questions spanning key clinical
domains, where we simulate these questions being asked by several patient
profiles that vary in sex, age range, and ethnicity. By comparing natural
language features of the generated responses, we show that, when LLMs are used
for medical advice generation, they generate responses that systematically
differ between social groups. In particular, Indigenous and intersex patients
receive advice that is less readable and more complex. We observe these trends
amplify when intersectional groups are considered. Considering the increasing
trust individuals place in these models, we argue for higher AI literacy and
for the urgent need for investigation and mitigation by AI developers to ensure
these systemic differences are diminished and do not translate to unjust
patient support. Our code is publicly available on GitHub.

</details>


### [34] [Comparing Knowledge Source Integration Methods for Optimizing Healthcare Knowledge Fusion in Rescue Operation](https://arxiv.org/abs/2510.09223)
*Mubaris Nadeem,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出了基于知识图谱结构的医学知识融合概念模型，旨在整合多种医疗知识源以支持精准的患者驱动决策制定。


<details>
  <summary>Details</summary>
Motivation: 医疗领域需要统一方法来收集、分析和利用现有医学知识，以支持关键决策制定。医疗知识的复杂性和多样性要求融合多种知识源，为医疗专业人员提供多情境对齐的知识选择。

Method: 基于知识图谱结构开发多个概念模型，评估知识融合的实现方式，展示如何将各种知识源整合到知识图谱中用于救援操作。

Result: 提出了支持知识融合的概念模型框架，能够集成多种医疗知识源，为医疗决策提供支持。

Conclusion: 知识图谱为基础的融合方法为医疗领域提供了有效的知识整合途径，有助于提升医疗决策的准确性和效率。

Abstract: In the field of medicine and healthcare, the utilization of medical
expertise, based on medical knowledge combined with patients' health
information is a life-critical challenge for patients and health professionals.
The within-laying complexity and variety form the need for a united approach to
gather, analyze, and utilize existing knowledge of medical treatments, and
medical operations to provide the ability to present knowledge for the means of
accurate patient-driven decision-making. One way to achieve this is the fusion
of multiple knowledge sources in healthcare. It provides health professionals
the opportunity to select from multiple contextual aligned knowledge sources
which enables the support for critical decisions. This paper presents multiple
conceptual models for knowledge fusion in the field of medicine, based on a
knowledge graph structure. It will evaluate, how knowledge fusion can be
enabled and presents how to integrate various knowledge sources into the
knowledge graph for rescue operations.

</details>


### [35] [RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems](https://arxiv.org/abs/2510.09227)
*Hyundong Jin,Joonghyuk Hahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: 该论文提出了一个基于PSPACE完全正则表达式问题的新基准，用于评估大语言模型和大推理模型的空间计算限制，揭示了它们在处理需要大规模搜索空间探索问题时的常见失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注NP复杂度类内的问题，但大语言模型和大推理模型在空间复杂度方面的计算限制仍未被充分理解，特别是受限于有限上下文窗口的问题。PSPACE完全问题作为更严格的标准，能够更好地评估模型的计算能力。

Method: 通过双重指数空间探索构建了包含超过100万个正则表达式实例的标注数据集，并采用严格过滤过程建立基准。对6个大语言模型和5个大推理模型进行了广泛评估。

Result: 评估揭示了模型在处理PSPACE完全正则表达式问题时的常见失败模式，如冗长和重复。这是首次对大语言模型和大推理模型空间计算限制的实证研究。

Conclusion: 该工作提供了一个评估大语言模型和大推理模型高级推理能力的新框架，通过PSPACE完全问题揭示了它们的空间计算局限性，为未来模型开发提供了重要参考。

Abstract: Large language models (LLMs) show strong performance across natural language
processing (NLP), mathematical reasoning, and programming, and recent large
reasoning models (LRMs) further emphasize explicit reasoning. Yet their
computational limits, particularly spatial complexity constrained by finite
context windows, remain poorly understood. While recent works often focus on
problems within the NP complexity class, we push the boundary by introducing a
novel benchmark grounded in two PSPACE-complete regular expression (regex)
problems: equivalence decision (RegexEQ) and minimization (RegexMin).
PSPACE-complete problems serve as a more rigorous standard for assessing
computational capacity, as their solutions require massive search space
exploration. We perform a double-exponential space exploration to construct a
labeled dataset of over a million regex instances with a sound filtering
process to build the benchmark. We conduct extensive evaluations on 6 LLMs and
5 LRMs of varying scales, revealing common failure patterns such as verbosity
and repetition. With its well-defined structure and quantitative evaluation
metrics, this work presents the first empirical investigation into the spatial
computational limitations of LLMs and LRMs, offering a new framework for
evaluating their advanced reasoning capabilities. Our code is available at
https://github.com/hyundong98/RegexPSPACE .

</details>


### [36] [Localist LLMs -- A Mathematical Framework for Dynamic Locality Control](https://arxiv.org/abs/2510.09338)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 提出了一种训练大型语言模型的新框架，通过可调节的局部性参数在局部化（可解释）和分布式（高效）表示之间连续切换，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在需要透明度的受监管领域中，同时满足可解释性和高性能的需求。

Method: 使用组稀疏惩罚、信息论锚点设计和动态规则注入，通过局部性参数动态控制表示的局部化程度。

Result: 提供了严格的数学证明，建立了注意力集中在语义相关块上的明确阈值条件，具有指数级的注意力熵和指针保真度界限。

Conclusion: 该框架使从业者能够在可解释和高性能模式之间连续插值，支持需要透明度和能力的受监管领域应用。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovation is a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining. This is achieved
through group sparsity penalties on attention mechanisms, information-theoretic
anchor design, and dynamic rule injection. We provide rigorous mathematical
proofs establishing explicit threshold conditions under which attention
provably concentrates on semantically relevant blocks, with exponential bounds
on attention entropy and pointer fidelity. Specifically, we prove that when
group sparsity penalties exceed certain threshold values, the model's attention
mechanisms concentrate on semantically relevant blocks, achieving low entropy
and high fidelity with negligible error. This framework enables practitioners
to continuously interpolate between interpretable and high-performance modes,
supporting applications in regulated domains requiring both transparency and
capability.

</details>


### [37] [Toward Mechanistic Explanation of Deductive Reasoning in Language Models](https://arxiv.org/abs/2510.09340)
*Davide Maltoni,Matteo Ferrara*

Main category: cs.AI

TL;DR: 小型语言模型通过学习底层规则（而非统计学习）解决演绎推理任务，研究发现归纳头在规则完成和规则链式推理中起核心作用。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型解决逻辑推理问题的内部机制，目前相关研究较少。

Method: 使用小型语言模型解决演绎推理任务，分析其内部表示和计算电路，重点关注归纳头的作用。

Result: 发现归纳头在实现逻辑推理所需的规则完成和规则链式步骤中发挥核心作用。

Conclusion: 研究揭示了语言模型解决逻辑推理任务的内部工作机制，特别是归纳头在规则学习中的关键作用。

Abstract: Recent large language models have demonstrated relevant capabilities in
solving problems that require logical reasoning; however, the corresponding
internal mechanisms remain largely unexplored. In this paper, we show that a
small language model can solve a deductive reasoning task by learning the
underlying rules (rather than operating as a statistical learner). A low-level
explanation of its internal representations and computational circuits is then
provided. Our findings reveal that induction heads play a central role in the
implementation of the rule completion and rule chaining steps involved in the
logical inference required by the task.

</details>


### [38] [Sequence Variables: A Constraint Programming Computational Domain for Routing and Sequencing](https://arxiv.org/abs/2510.09373)
*Augustin Delecluse,Pierre Schaus,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: 本文在约束编程中形式化了序列变量，以解决经典后继变量模型在处理可选访问和插入启发式方面的局限性，为车辆路径问题提供了更直观的建模框架。


<details>
  <summary>Details</summary>
Motivation: 经典约束编程模型基于后继变量，无法有效处理可选访问或基于插入的启发式方法，这限制了在车辆路径问题中的应用。

Method: 形式化序列变量的定义、域、更新操作和一致性级别；实现底层数据结构以集成到现有CP求解器中；设计专门用于序列变量和车辆路径的全局约束。

Result: 序列变量简化了问题建模，并在Dial-a-Ride问题上实现了具有竞争力的计算性能。

Conclusion: 序列变量为约束编程中的车辆路径问题提供了更强大的建模能力，能够处理可选访问并支持插入启发式，在实际应用中表现出良好效果。

Abstract: Constraint Programming (CP) offers an intuitive, declarative framework for
modeling Vehicle Routing Problems (VRP), yet classical CP models based on
successor variables cannot always deal with optional visits or insertion based
heuristics. To address these limitations, this paper formalizes sequence
variables within CP. Unlike the classical successor models, this computational
domain handle optional visits and support insertion heuristics, including
insertion-based Large Neighborhood Search. We provide a clear definition of
their domain, update operations, and introduce consistency levels for
constraints on this domain. An implementation is described with the underlying
data structures required for integrating sequence variables into existing
trail-based CP solvers. Furthermore, global constraints specifically designed
for sequence variables and vehicle routing are introduced. Finally, the
effectiveness of sequence variables is demonstrated by simplifying problem
modeling and achieving competitive computational performance on the Dial-a-Ride
Problem.

</details>


### [39] [Safe, Untrusted, "Proof-Carrying" AI Agents: toward the agentic lakehouse](https://arxiv.org/abs/2510.09567)
*Jacopo Tagliabue,Ciro Greco*

Main category: cs.AI

TL;DR: 本文提出API优先、可编程的数据湖仓为AI驱动的自动化工作流提供安全设计抽象，通过数据分支和声明式环境实现可复现性和可观测性，减少攻击面。


<details>
  <summary>Details</summary>
Motivation: 数据湖仓运行敏感工作负载，AI驱动的自动化引发了关于信任、正确性和治理的担忧。

Method: 使用Bauplan作为案例研究，展示数据分支和声明式环境如何自然扩展到智能体，实现可复现性和可观测性。提出基于证明携带代码思想的正确性检查概念验证，让智能体修复数据管道。

Result: 原型演示表明不受信任的AI智能体可以在生产数据上安全操作，并勾勒出完全智能体化湖仓的实现路径。

Conclusion: API优先、可编程的湖仓架构为安全设计的智能体工作流提供了合适的抽象，能够实现生产环境中AI智能体的安全操作。

Abstract: Data lakehouses run sensitive workloads, where AI-driven automation raises
concerns about trust, correctness, and governance. We argue that API-first,
programmable lakehouses provide the right abstractions for safe-by-design,
agentic workflows. Using Bauplan as a case study, we show how data branching
and declarative environments extend naturally to agents, enabling
reproducibility and observability while reducing the attack surface. We present
a proof-of-concept in which agents repair data pipelines using correctness
checks inspired by proof-carrying code. Our prototype demonstrates that
untrusted AI agents can operate safely on production data and outlines a path
toward a fully agentic lakehouse.

</details>


### [40] [GraphMERT: Efficient and Scalable Distillation of Reliable Knowledge Graphs from Unstructured Data](https://arxiv.org/abs/2510.09580)
*Margarita Belova,Jiaxin Xiao,Shikhar Tuli,Niraj K. Jha*

Main category: cs.AI

TL;DR: GraphMERT是一个小型图形编码器模型，通过从非结构化文本语料库及其内部表示中提取高质量知识图谱，解决了神经符号AI的可扩展性和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI领域近三十年来未能实现其潜力，主要因为现有框架难以扩展，且神经方法的隐式表示和近似推理限制了可解释性和可信度。知识图谱作为显式语义知识的黄金标准表示可以解决符号方面的问题，但从文本语料库自动推导可靠的知识图谱仍是一个开放问题。

Method: 引入GraphMERT，一个微小的图形编码器模型，从非结构化文本语料库及其内部表示中提取高质量知识图谱。GraphMERT及其等效知识图谱形成一个模块化的神经符号堆栈：神经学习抽象；符号知识图谱用于可验证推理。

Result: 在PubMed糖尿病论文文本上，80M参数的GraphMERT生成的知识图谱达到69.8%的FActScore，而32B参数的基线LLM仅达到40.2%。GraphMERT知识图谱还获得68.8%的ValidityScore，而LLM基线为43.0%。

Conclusion: GraphMERT + KG是第一个高效且可扩展的神经符号模型，在实现最先进基准精度的同时，相对于基线具有优越的符号表示能力。

Abstract: Researchers have pursued neurosymbolic artificial intelligence (AI)
applications for nearly three decades because symbolic components provide
abstraction while neural components provide generalization. Thus, a marriage of
the two components can lead to rapid advancements in AI. Yet, the field has not
realized this promise since most neurosymbolic AI frameworks fail to scale. In
addition, the implicit representations and approximate reasoning of neural
approaches limit interpretability and trust. Knowledge graphs (KGs), a
gold-standard representation of explicit semantic knowledge, can address the
symbolic side. However, automatically deriving reliable KGs from text corpora
has remained an open problem. We address these challenges by introducing
GraphMERT, a tiny graphical encoder-only model that distills high-quality KGs
from unstructured text corpora and its own internal representations. GraphMERT
and its equivalent KG form a modular neurosymbolic stack: neural learning of
abstractions; symbolic KGs for verifiable reasoning. GraphMERT + KG is the
first efficient and scalable neurosymbolic model to achieve state-of-the-art
benchmark accuracy along with superior symbolic representations relative to
baselines.
  Concretely, we target reliable domain-specific KGs that are both (1) factual
(with provenance) and (2) valid (ontology-consistent relations with
domain-appropriate semantics). When a large language model (LLM), e.g.,
Qwen3-32B, generates domain-specific KGs, it falls short on reliability due to
prompt sensitivity, shallow domain expertise, and hallucinated relations. On
text obtained from PubMed papers on diabetes, our 80M-parameter GraphMERT
yields a KG with a 69.8% FActScore; a 32B-parameter baseline LLM yields a KG
that achieves only 40.2% FActScore. The GraphMERT KG also attains a higher
ValidityScore of 68.8%, versus 43.0% for the LLM baseline.

</details>


### [41] [LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?](https://arxiv.org/abs/2510.09595)
*Kaijian Zou,Aaron Xiong,Yunxiang Zhang,Frederick Zhang,Yueqi Ren,Jirong Yang,Ayoung Lee,Shitanshu Bhushan,Lu Wang*

Main category: cs.AI

TL;DR: LiveOIBench是一个包含403个奥林匹克级编程竞赛问题的基准测试，具有高质量测试用例和人类表现对比，评估显示GPT-5达到81.76百分位但仍落后于顶尖人类选手。


<details>
  <summary>Details</summary>
Motivation: 当前编程基准测试存在缺乏高难度问题、测试用例不足、依赖在线平台API等问题，需要更全面的评估工具来测试大型语言模型的编程能力。

Method: 从72个官方信息学奥林匹克竞赛中收集403个专家策划的问题，每个问题平均60个测试用例，建立包含详细子任务评分和私有测试用例的评估系统。

Result: GPT-5在基准测试中达到81.76百分位，而开源模型GPT-OSS-120B仅达到60百分位，显示与前沿闭源模型的显著能力差距。

Conclusion: 强大的推理模型应优先精确的问题分析而非过度探索，未来模型应强调结构化分析并减少不必要的探索。

Abstract: Competitive programming problems increasingly serve as valuable benchmarks to
evaluate the coding capabilities of large language models (LLMs) due to their
complexity and ease of verification. Yet, current coding benchmarks face
limitations such as lack of exceptionally challenging problems, insufficient
test case coverage, reliance on online platform APIs that limit accessibility.
To address these issues, we introduce LiveOIBench, a comprehensive benchmark
featuring 403 expert-curated Olympiad-level competitive programming problems,
each with an average of 60 expert-designed test cases. The problems are sourced
directly from 72 official Informatics Olympiads in different regions conducted
between 2023 and 2025. LiveOIBench distinguishes itself through four key
features: (1) meticulously curated high-quality tasks with detailed subtask
rubrics and extensive private test cases; (2) direct integration of elite
contestant performance data to enable informative comparison against
top-performing humans; (3) planned continuous, contamination-free updates from
newly released Olympiad problems; and (4) a self-contained evaluation system
facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular
general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable
81.76th percentile, a strong result that nonetheless falls short of top human
contestant performance, who usually place above 90th. In contrast, among
open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile,
underscoring significant capability disparities from frontier closed models.
Detailed analyses indicate that robust reasoning models prioritize precise
problem analysis over excessive exploration, suggesting future models should
emphasize structured analysis and minimize unnecessary exploration. All data,
code, and leaderboard results will be made publicly available on our website.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [Maple: A Multi-agent System for Portable Deep Learning across Clusters](https://arxiv.org/abs/2510.08842)
*Molang Wu,Zhao Zhang*

Main category: cs.DC

TL;DR: Maple是一个多智能体系统，通过自然语言输入生成正确的深度学习命令，解决了在异构GPU集群上训练深度学习模型时命令行配置的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在GPU集群上训练深度学习模型面临技术挑战，用户需要适应异构启动器、调度器、亲和性选项、框架参数和环境变量等，手动编写命令行容易出错且耗时，阻碍研究进展和浪费资源。

Method: Maple采用四智能体架构，包括信息提取、模板检索、命令行验证和错误修正功能，利用多个语言模型（总计10B参数）生成正确的深度学习命令。

Result: 在9个美国国家计算中心的GPU集群、5个代表性深度学习模型家族和4种常用并行训练范式的567个测试案例中，Maple实现了92.0%的命令行生成准确率，性能与GPT-5、Claude和Gemini等最先进模型相当。

Conclusion: Maple在异构高性能计算环境中实现了可移植和可扩展的分布式深度学习，具有重要的实用价值。

Abstract: Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.

</details>


### [43] [Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication](https://arxiv.org/abs/2510.08874)
*Benjamin Brock,Renato Golin*

Main category: cs.DC

TL;DR: 提出一种通用的单边分布式矩阵乘法算法，支持所有分区和复制因子组合，通过切片计算重叠瓦片的乘法集合，性能与PyTorch DTensor相当。


<details>
  <summary>Details</summary>
Motivation: 现有分布式矩阵乘法算法仅支持部分分区方式，需要多个算法实现来支持全部分区组合，当没有对应算法时需重新分布操作数，增加通信成本。

Method: 使用切片（索引算术）计算需要相乘的重叠瓦片集合，然后直接执行或重新排序并降低到优化IR以最大化重叠。基于高级C++ PGAS编程框架实现，使用GPU到GPU直接通信。

Result: 在各种分区和复制因子配置下评估性能，发现与针对AI模型优化的PyTorch DTensor性能相当。

Conclusion: 提出的通用单边算法能够支持所有分区和复制因子组合，避免了操作数重新分布的需求，在性能上具有竞争力。

Abstract: Many important applications across science, data analytics, and AI workloads
depend on distributed matrix multiplication. Prior work has developed a large
array of algorithms suitable for different problem sizes and partitionings
including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is
that existing algorithms are limited to a subset of partitionings. Multiple
algorithm implementations are required to support the full space of possible
partitionings. If no algorithm implementation is available for a particular set
of partitionings, one or more operands must be redistributed, increasing
communication costs. This paper presents a universal one-sided algorithm for
distributed matrix multiplication that supports all combinations of
partitionings and replication factors. Our algorithm uses slicing (index
arithmetic) to compute the sets of overlapping tiles that must be multiplied
together. This list of local matrix multiplies can then either be executed
directly, or reordered and lowered to an optimized IR to maximize overlap. We
implement our algorithm using a high-level C++-based PGAS programming framework
that performs direct GPU-to-GPU communication using intra-node interconnects.
We evaluate performance for a wide variety of partitionings and replication
factors, finding that our work is competitive with PyTorch DTensor, a highly
optimized distributed tensor library targeting AI models.

</details>


### [44] [Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors](https://arxiv.org/abs/2510.09163)
*Alessandro Ottaviano,Andrino Meli,Paul Scheffler,Giovanni Bambini,Robert Balas,Davide Rossi,Andrea Bartolini,Luca Benini*

Main category: cs.DC

TL;DR: 提出了一种硬件-软件协同设计的轻量级MPC控制器，用于多核HPC处理器的能量和热管理，通过算子分裂二次规划求解器和嵌入式RISC-V控制器实现高效控制。


<details>
  <summary>Details</summary>
Motivation: 传统MPC方法在PE上执行控制器会因操作系统开销产生抖动并限制控制带宽，而专用片上控制器又面临面积和功耗开销的担忧。

Method: 基于算子分裂二次规划求解器和嵌入式多核RISC-V控制器，通过剪枝弱热耦合减少模型内存，并采用提前调度技术高效并行执行稀疏三角系统。

Result: 在500MHz频率下控制144个PE时实现亚毫秒延迟，相比单核基准延迟降低33倍，能效提高7.9倍，内存占用小于1MiB，功耗仅325mW，占用典型HPC处理器芯片面积小于1.5%。

Conclusion: 该硬件-软件协同设计方法成功解决了多核HPC处理器能量热管理的控制挑战，实现了高效、低延迟、低功耗的控制方案。

Abstract: Managing energy and thermal profiles is critical for many-core HPC processors
with hundreds of application-class processing elements (PEs). Advanced model
predictive control (MPC) delivers state-of-the-art performance but requires
solving an online optimization problem over a thousand times per second (1 kHz
control bandwidth), with computational and memory demands scaling with PE
count. Traditional MPC approaches execute the controller on the PEs, but
operating system overheads create jitter and limit control bandwidth. Running
MPC on dedicated on-chip controllers enables fast, deterministic control but
raises concerns about area and power overhead. In this work, we tackle these
challenges by proposing a hardware-software codesign of a lightweight MPC
controller, based on an operator-splitting quadratic programming solver and an
embedded multi-core RISC-V controller. Key innovations include pruning weak
thermal couplings to reduce model memory and ahead-of-time scheduling for
efficient parallel execution of sparse triangular systems arising from the
optimization problem. The proposed controller achieves sub-millisecond latency
when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x
higher energy efficiency than a single-core baseline. Operating within a
compact less than 1 MiB memory footprint, it consumes as little as 325 mW while
occupying less than 1.5% of a typical HPC processor's die area.

</details>
