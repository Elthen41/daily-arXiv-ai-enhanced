<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Pending Conflicts Make Progress Impossible](https://arxiv.org/abs/2602.04013)
*Petr Kuznetsov,Pierre Sutra,Guillermo Toyos-Marfurt*

Main category: cs.DC

TL;DR: 研究共享对象的可交换感知线性化实现中的进展条件，提出冲突-阻塞自由概念，证明在异步读写共享内存模型中无法实现冲突-阻塞自由的通用构造


<details>
  <summary>Details</summary>
Motivation: 观察到可交换操作可以并行执行，希望通过利用操作间的可交换性来改进并发实现的进展条件

Method: 引入冲突-阻塞自由概念：只要进程在足够长时间内没有遇到与非可交换操作的步骤争用，就能保证完成操作。这推广了阻塞自由和等待自由条件

Result: 证明在异步读写共享内存模型中，冲突-阻塞自由的通用构造是不可能的实现。这揭示了冲突感知通用构造的基本限制：冲突操作的调用本身就会带来同步成本

Conclusion: 进展需要最终解决待处理的冲突，冲突-阻塞自由虽然理论上扩展了进展条件，但在异步读写共享内存模型中存在根本性实现限制

Abstract: In this work, we study progress conditions for commutativity-aware, linearizable implementations of shared objects. Motivated by the observation that commuting operations can be executed in parallel, we introduce conflict-obstruction-freedom: a process is guaranteed to complete its operation if it runs for long enough without encountering step contention with conflicting (non-commuting) operations. This condition generalizes obstruction-freedom and wait-freedom by allowing progress as long as step contention is only induced by commuting operations. We prove that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model. This result exposes a fundamental limitation of conflict-aware universal constructions: the mere invocation of conflicting operations imposes a synchronization cost. Progress requires eventual resolution of pending conflicts.

</details>


### [2] [Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN](https://arxiv.org/abs/2602.04652)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.DC

TL;DR: 本文实证研究了将5G NR LDPC解码从Grace CPU卸载到Blackwell GB10 GPU的性能优势，在DGX Spark平台上实现了约6倍的吞吐量加速，GPU解码延迟保持在时隙限制内，而CPU解码已超时。


<details>
  <summary>Details</summary>
Motivation: 5G NR物理层中的LDPC解码是计算密集型任务，必须在0.5ms传输时间间隔内完成。许多现有系统仍在通用CPU上执行LDPC解码，随着带宽、调制阶数和用户复用的增加，存在错过时隙事件和可扩展性受限的问题。

Method: 使用NVIDIA Sionna PHY/SYS在TensorFlow上构建NR类链路级链，包含LDPC5G编码/解码器、16-QAM调制和AWGN信道。通过扫描并行解码码字数、置信传播迭代次数，测量解码阶段的时间，同时记录CPU和GPU利用率及功耗。

Result: 观察到平均GPU/CPU吞吐量加速约6倍。CPU解码延迟在20次迭代时达到约0.71ms（超过0.5ms时隙限制），而GB10 GPU在相同工作负载下保持在时隙的6-24%范围内。CPU解码消耗约10个Grace核心，而GPU解码仅比GPU空闲时增加10-15W功耗。

Conclusion: GPU卸载能显著提升LDPC解码性能并满足时隙要求，同时释放CPU资源用于更高层任务。基于高层Sionna层的实现代表了可实现的加速器性能保守下界，为未来平台评估物理层内核提供了可重用、可脚本化的方法。

Abstract: Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\times$, with per-codeword CPU latency reaching $\approx 0.71$\,ms at 20 iterations (exceeding the 0.5\,ms slot), while the GB10 GPU remains within 6--24\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\approx10-15$\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.

</details>


### [3] [A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources](https://arxiv.org/abs/2602.04697)
*Davide Basile,Valerio Goretti,Luca Barbaro,Hajo A. Reijers,Claudio Di Ciccio*

Main category: cs.DC

TL;DR: CONFINE提出了一种基于可信执行环境(TEE)的保密性保护跨组织流程挖掘方法，通过四阶段协议安全处理多方事件日志，避免组织间敏感数据泄露。


<details>
  <summary>Details</summary>
Motivation: 跨组织流程挖掘面临重大挑战，特别是保密性问题：数据分析可能揭示参与组织不愿相互披露或向第三方流程挖掘服务提供商披露的信息。现有方法难以在保护数据机密性的同时进行有效的跨组织流程分析。

Method: CONFINE利用可信执行环境(TEE)部署可信应用程序，通过四阶段协议保护数据交换和处理，支持跨组织边界的未更改流程数据的受保护传输和聚合。为避免TEE容量限制导致的内存溢出错误，采用基于分段的策略，将事件日志分批传输到TEE。

Result: 通过形式化验证正确性和TEE核心提供的安全保证分析，在真实世界和合成数据上评估实现，表明该方法能够处理实际工作负载。结果显示内存使用随事件日志大小呈对数增长，随供应组织数量呈线性增长，突出了可扩展性和进一步优化的机会。

Conclusion: CONFINE提供了一种可行的保密性保护跨组织流程挖掘方法，利用TEE技术解决了多方数据共享中的敏感信息保护问题，为跨组织业务流程分析提供了安全可靠的技术方案。

Abstract: Process mining techniques enable organizations to gain insights into their business processes through the analysis of execution records (event logs) stored by information systems. While most process mining efforts focus on intra-organizational scenarios, many real-world business processes span multiple independent organizations. Inter-organizational process mining, though, faces significant challenges, particularly regarding confidentiality guarantees: The analysis of data can reveal information that the participating organizations may not consent to disclose to one another, or to a third party hosting process mining services. To overcome this issue, this paper presents CONFINE, an approach for secrecy-preserving inter-organizational process mining. CONFINE leverages Trusted Execution Environments (TEEs) to deploy trusted applications that are capable of securely mining multi-party event logs while preserving data secrecy. We propose an architecture supporting a four-stage protocol to secure data exchange and processing, allowing for protected transfer and aggregation of unaltered process data across organizational boundaries. To avoid out-of-memory errors due to the limited capacity of TEEs, our protocol employs a segmentation-based strategy, whereby event logs are transmitted to TEEs in smaller batches. We conduct a formal verification of correctness and a security analysis of the guarantees provided by the TEE core. We evaluate our implementation on real-world and synthetic data, showing that the proposed approach can handle realistic workloads. The results indicate logarithmic memory growth with respect to the event log size and linear growth with the number of provisioning organizations, highlighting scalability properties and opportunities for further optimization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [SPPAM: Signature Pattern Prediction and Access-Map Prefetcher](https://arxiv.org/abs/2602.04100)
*Maccoy Merrell,Lei Wang,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: SPPAM是一种新的缓存预取方法，结合了SPP和AMPM的优点，通过在线学习构建访问映射模式，使用置信度调节的推测性前瞻机制，在二级缓存上显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 处理器速度与内存系统性能之间的差距持续限制许多工作负载的性能。现有预取技术如SPP容易受到高级缓存和乱序核心的引用重排序影响，而AMPM虽然对重排序有抵抗力但无法进行区域外推测。需要一种能克服这些限制的新预取方法。

Method: SPPAM采用在线学习构建一组访问映射模式，这些模式用于置信度调节的推测性前瞻机制。该方法针对二级缓存，结合了SPP的推测能力和AMPM的重排序抵抗特性。

Result: SPPAM与最先进的预取器Berti和Bingo结合，相比无预取系统性能提升31.4%，相比Berti和Pythia基线提升6.2%。

Conclusion: SPPAM成功解决了现有预取技术的局限性，通过结合在线学习的访问映射模式和置信度调节的推测机制，在二级缓存上实现了显著的性能提升。

Abstract: The discrepancy between processor speed and memory system performance continues to limit the performance of many workloads. To address the issue, one effective and well studied technique is cache prefetching. Many prefetching designs have been proposed, with varying approaches and effectiveness. For example, SPP is a popular prefetcher that leverages confidence throttled recursion to speculate on the future path of program's references, however it is very susceptible to the reference reordering of higher-level caches and the OoO core. Orthogonally, AMPM is another popular approach to prefetching which uses reordering-resistant access maps to identify patterns within a region, but is unable to speculate beyond that region. In this paper, we propose SPPAM, a new approach to prefetching, inspired by prior works such as SPP and AMPM, while addressing their limitations. SPPAM utilizes online-learning to build a set of access-map patterns. These patterns are used in a speculative lookahead which is throttled by a confidence metric. Targeting the second-level cache, SPPAM alongside state-of-the-art prefetchers Berti and Bingo improve system performance by 31.4% over no prefetching and 6.2% over the baseline of Berti and Pythia.

</details>


### [5] [Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference](https://arxiv.org/abs/2602.04595)
*Xinyu Wang,Jieyu Li,Yanan Sun,Weifeng He*

Main category: cs.AR

TL;DR: Harmonia是一个算法-硬件协同设计框架，通过全层BFP激活、非对称位分配策略和混合离线-在线异常值平滑技术，在保持精度的同时显著提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大但存在高内存和计算成本问题。现有量化方法在线性层使用BFP激活，但无法扩展到注意力层，限制了整体效率提升。

Method: 1. 系统探索BFP配置以实现所有层精度与激活压缩的更好权衡；2. 引入非对称位分配策略和混合离线-在线异常值平滑技术，将KV缓存从FP16压缩到4位尾数BFP；3. 设计专用硬件组件，包括支持混合数据格式的可重构PE、实时FP16到BFP转换器和分块感知数据流。

Result: 在八个广泛使用的LLM上评估，相比先前工作，Harmonia平均实现3.84倍（最高5.05倍）面积效率提升、2.03倍（最高3.90倍）能效提升和3.08倍（最高4.62倍）加速。

Conclusion: Harmonia通过算法-硬件协同设计成功解决了BFP激活在注意力层的精度退化问题，实现了全层BFP激活，显著提升了LLM推理的效率和性能。

Abstract: Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [6] [Boost+: Equitable, Incentive-Compatible Block Building](https://arxiv.org/abs/2602.04007)
*Mengqian Zhang,Sen Yang,Kartik Nayak,Fan Zhang*

Main category: cs.CR

TL;DR: Boost+ 是一个去中心化的区块构建系统，通过将交易收集和排序解耦，解决 MEV-Boost 的中心化问题，确保所有参与者公平访问交易并实现经济效率。


<details>
  <summary>Details</summary>
Motivation: 当前以太坊的区块构建生态系统 MEV-Boost 高度中心化，这扭曲了竞争、降低了区块空间效率，并模糊了 MEV 流动的透明度。需要保证区块构建的公平性和经济效率。

Method: 提出 Boost+ 系统，将区块构建过程解耦为交易收集和排序两个阶段。核心是 M_Boost+ 机制，围绕默认算法构建，确保搜索者和构建者的激励相容。搜索者真实报告是主导策略，构建者真实出价是主导策略。还实现了基于真实交易数据经验分析的具体默认算法。

Result: M_Boost+ 机制为所有构建者提供了真实出价的主导策略。对于搜索者，当默认算法优于竞争构建者时，真实报告是主导策略；对于所有无冲突交易，即使构建者可能获胜，真实报告仍然是主导策略。即使搜索者技术上可以与构建者集成，对于无冲突交易，非集成结合真实出价仍优于任何偏离策略。

Conclusion: Boost+ 通过解耦交易收集和排序，解决了 MEV-Boost 的中心化问题，确保了区块构建的公平性和经济效率。该系统为构建者和搜索者提供了激励相容的机制，并通过基于真实数据的默认算法实现了有效实施。

Abstract: Block space on the blockchain is scarce and must be allocated efficiently through block building. However, Ethereum's current block-building ecosystem, MEV-Boost, has become highly centralized due to integration, which distorts competition, reduces blockspace efficiency, and obscures MEV flow transparency. To guarantee equitability and economic efficiency in block building, we propose $\mathrm{Boost+}$, a system that decouples the process into collecting and ordering transactions, and ensures equal access to all collected transactions.
  The core of $\mathrm{Boost+}$ is the mechanism $\mathit{M}_{\mathrm{Boost+}}$, built around a default algorithm. $\mathit{M}_{\mathrm{Boost+}}$ aligns incentives for both searchers (intermediaries that generate or route transactions) and builders: Truthful bidding is a dominant strategy for all builders. For searchers, truthful reporting is dominant whenever the default algorithm dominates competing builders, and it remains dominant for all conflict-free transactions, even when builders may win. We further show that even if a searcher can technically integrate with a builder, non-integration combined with truthful bidding still dominates any deviation for conflict-free transactions. We also implement a concrete default algorithm informed by empirical analysis of real-world transactions and evaluate its efficacy using historical transaction data.

</details>


### [7] [Evaluating the Vulnerability Landscape of LLM-Generated Smart Contracts](https://arxiv.org/abs/2602.04039)
*Hoang Long Do,Nasrin Sohrabi,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 对ChatGPT、Gemini和Sonnet等先进大语言模型生成的Solidity智能合约进行系统性安全分析，发现尽管语法正确且功能完整，但这些合约经常存在严重安全漏洞，可能在实际环境中被利用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在智能合约开发中的广泛应用，由于已部署的智能合约不可修改，其正确性和安全性至关重要。然而，LLM生成的智能合约的安全影响尚未得到充分理解，特别是在金融和治理等高风险领域。

Method: 对ChatGPT、Gemini和Sonnet等先进LLM生成的Solidity智能合约进行系统性安全分析，针对广泛的已知智能合约漏洞进行评估，分析漏洞模式并分类。

Result: 实验研究表明，LLM生成的智能合约尽管语法正确且功能完整，但经常表现出严重的安全缺陷，这些缺陷可能在现实环境中被利用。研究识别了不同模型间重复出现的弱点模式。

Conclusion: 提出了实用的对策和开发指南以减轻这些风险，为开发者和研究人员提供可操作的见解。研究旨在支持LLM安全集成到智能合约开发工作流中，并加强区块链生态系统对未来安全故障的整体安全性。

Abstract: Large language models (LLMs) have been widely adopted in modern software development lifecycles, where they are increasingly used to automate and assist code generation, significantly improving developer productivity and reducing development time. In the blockchain domain, developers increasingly rely on LLMs to generate and maintain smart contracts, the immutable, self-executing components of decentralized applications. Because deployed smart contracts cannot be modified, correctness and security are paramount, particularly in high-stakes domains such as finance and governance. Despite this growing reliance, the security implications of LLM-generated smart contracts remain insufficiently understood.
  In this work, we conduct a systematic security analysis of Solidity smart contracts generated by state-of-the-art LLMs, including ChatGPT, Gemini, and Sonnet. We evaluate these contracts against a broad set of known smart contract vulnerabilities to assess their suitability for direct deployment in production environments. Our extensive experimental study shows that, despite their syntactic correctness and functional completeness, LLM-generated smart contracts frequently exhibit severe security flaws that could be exploited in real-world settings. We further analyze and categorize these vulnerabilities, identifying recurring weakness patterns across different models. Finally, we discuss practical countermeasures and development guidelines to help mitigate these risks, offering actionable insights for both developers and researchers. Our findings aim to support safe integration of LLMs into smart contract development workflows and to strengthen the overall security of the blockchain ecosystem against future security failures.

</details>


### [8] [ZKBoost: Zero-Knowledge Verifiable Training for XGBoost](https://arxiv.org/abs/2602.04113)
*Nikolas Melissaris,Jiayi Xu,Antigoni Polychroniadou,Akira Takahashi,Chenkai Weng*

Main category: cs.CR

TL;DR: ZKBoost是首个用于XGBoost的零知识训练证明协议，允许模型所有者在不泄露数据或参数的情况下证明在承诺数据集上的正确训练。


<details>
  <summary>Details</summary>
Motivation: 随着梯度提升决策树（特别是XGBoost）在敏感场景中的部署增加，需要密码学保证模型完整性，确保模型训练的正确性而不泄露隐私数据。

Method: 提出了三个关键贡献：1）兼容算术电路的定点XGBoost实现；2）通用的XGBoost零知识训练证明模板；3）基于向量不经意线性评估的实例化，解决非线性定点运算的证明挑战。

Result: 定点XGBoost实现与标准XGBoost的准确率差异在1%以内，同时能够在真实数据集上实现实用的零知识训练证明。

Conclusion: ZKBoost为XGBoost提供了首个零知识训练证明协议，在保持模型准确性的同时实现了密码学安全保证，适用于敏感场景的部署需求。

Abstract: Gradient boosted decision trees, particularly XGBoost, are among the most effective methods for tabular data. As deployment in sensitive settings increases, cryptographic guarantees of model integrity become essential. We present ZKBoost, the first zero-knowledge proof of training (zkPoT) protocol for XGBoost, enabling model owners to prove correct training on a committed dataset without revealing data or parameters. We make three key contributions: (1) a fixed-point XGBoost implementation compatible with arithmetic circuits, enabling instantiation of efficient zkPoT, (2) a generic template of zkPoT for XGBoost, which can be instantiated with any general-purpose ZKP backend, and (3) vector oblivious linear evaluation (VOLE)-based instantiation resolving challenges in proving nonlinear fixed-point operations. Our fixed-point implementation matches standard XGBoost accuracy within 1\% while enabling practical zkPoT on real-world datasets.

</details>


### [9] [Post-Quantum Identity-Based TLS for 5G Service-Based Architecture and Cloud-Native Infrastructure](https://arxiv.org/abs/2602.04238)
*Vipin Kumar Rathi,Lakshya Chopra,Nikhil Kumar Rajput*

Main category: cs.CR

TL;DR: 提出基于后量子身份基加密的无证书认证框架，替代传统PKI和证书验证，用于私有分布式系统、云原生平台和5G核心网


<details>
  <summary>Details</summary>
Motivation: 传统基于证书的PKI和mTLS在云原生平台和5G核心网等延迟敏感系统中存在显著的运维和性能开销，在后量子场景下证书体积大、签名验证成本高的问题更加突出

Method: 设计基于后量子身份基加密的无证书认证框架，用身份派生密钥和基于身份的密钥封装替代证书和签名认证，实现无需证书传输和验证的相互认证TLS连接；提出基于阈值私钥生成器的IBE替代私有PKI方案，包括身份生命周期管理

Result: 将框架应用于云原生应用部署和5G核心网，展示了基于身份的TLS如何与5G服务化架构集成并保持安全语义和3GPP要求，同时证明同一架构可以替代Kubernetes中的私有PKI而不破坏现有信任域或部署模型

Conclusion: 基于后量子IBE的无证书认证框架能够有效解决传统PKI在云原生和5G系统中的运维和性能问题，特别是在后量子安全场景下具有显著优势，为私有分布式系统提供了更高效的认证解决方案

Abstract: Cloud-native application platforms and latency-sensitive systems such as 5G Core networks rely heavily on certificate-based Public Key Infrastructure (PKI) and mutual TLS to secure service-to-service communication. While effective, this model introduces significant operational and performance overhead, which is further amplified in the post-quantum setting due to large certificates and expensive signature verification. In this paper, we present a certificate-free authentication framework for private distributed systems based on post-quantum Identity-Based Encryption(IBE). Our design replaces certificate and signature based authentication with identity-derived keys and identity-based key encapsulation, enabling mutually authenticated TLS connections without certificate transmission or validation. We describe an IBE-based replacement for private PKI, including identity lifecycle management, and show how it can be instantiated using a threshold Private Key Generator (T-PKG). We apply this framework to cloud-native application deployments and latency-sensitive 5G Core networks. In particular, we demonstrate how identity-based TLS integrates with the 5G Service-Based Architecture while preserving security semantics and 3GPP requirements, and we show how the same architecture can replace private PKI in Kubernetes, including its control plane, without disrupting existing trust domains or deployment models.

</details>


### [10] [Optimal conversion from Rényi Differential Privacy to $f$-Differential Privacy](https://arxiv.org/abs/2602.04562)
*Anneliese Riess,Juan Felipe Gomez,Flavio du Pin Calmon,Julia Anne Schnabel,Georgios Kaissis*

Main category: cs.CR

TL;DR: 该论文证明了将Rényi差分隐私(RDP)轮廓转换为假设检验权衡函数的最优转换规则：基于单阶RDP隐私区域交集的规则对所有RDP轮廓和所有I类错误水平α都是最优的。


<details>
  <summary>Details</summary>
Motivation: 解决Zhu等人(2022)附录F.3中提出的猜想：在所有将RDP轮廓映射到有效假设检验权衡函数的转换规则中，确定基于单阶RDP隐私区域交集的规则是否最优。

Method: 通过精确的几何特征分析RDP隐私区域，利用其凸性以及边界完全由伯努利机制决定的事实。证明在权衡函数空间中，最紧的边界是f_{ρ(·)}(α) = sup_{τ≥0.5} f_{τ,ρ(τ)}(α)，即每个RDP隐私区域的单阶边界的逐点最大值。

Result: 证明了"RDP隐私区域交集"规则不仅有效，而且是最优的：没有其他黑盒转换能在Blackwell意义上一致地优于它，这标志着仅从RDP保证推断机制隐私性的基本极限。

Conclusion: 该研究统一并强化了Balle等人(2019)、Asoodeh等人(2021)和Zhu等人(2022)的见解，建立了RDP到假设检验隐私转换的基本最优性理论，为差分隐私分析提供了理论基础。

Abstract: We prove the conjecture stated in Appendix F.3 of [Zhu et al. (2022)]: among all conversion rules that map a Rényi Differential Privacy (RDP) profile $τ\mapsto ρ(τ)$ to a valid hypothesis-testing trade-off $f$, the rule based on the intersection of single-order RDP privacy regions is optimal. This optimality holds simultaneously for all valid RDP profiles and for all Type I error levels $α$. Concretely, we show that in the space of trade-off functions, the tightest possible bound is $f_{ρ(\cdot)}(α) = \sup_{τ\geq 0.5} f_{τ,ρ(τ)}(α)$: the pointwise maximum of the single-order bounds for each RDP privacy region. Our proof unifies and sharpens the insights of [Balle et al. (2019)], [Asoodeh et al. (2021)], and [Zhu et al. (2022)]. Our analysis relies on a precise geometric characterization of the RDP privacy region, leveraging its convexity and the fact that its boundary is determined exclusively by Bernoulli mechanisms. Our results establish that the "intersection-of-RDP-privacy-regions" rule is not only valid, but optimal: no other black-box conversion can uniformly dominate it in the Blackwell sense, marking the fundamental limit of what can be inferred about a mechanism's privacy solely from its RDP guarantees.

</details>


### [11] [Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653)
*Ariel Fogel,Omer Hofman,Eilon Cohen,Roman Vainshtein*

Main category: cs.CR

TL;DR: 通过恶意修改聊天模板实现无需修改模型权重、训练数据或控制运行时基础设施的推理时后门攻击


<details>
  <summary>Details</summary>
Motivation: 随着开源权重语言模型在生产环境中的广泛应用，安全挑战日益突出。后门攻击是主要威胁之一，传统攻击假设攻击者能访问训练管道或部署基础设施。本文提出一种新型攻击面，利用聊天模板这一特权位置实现无需传统攻击条件的后门攻击。

Method: 利用聊天模板作为攻击面，聊天模板是每次推理调用时执行的Jinja2程序，位于用户输入和模型处理之间的特权位置。攻击者通过分发带有恶意修改模板的模型，在不修改模型权重、不污染训练数据、不控制运行时基础设施的情况下植入推理时后门。

Result: 在18个模型（涵盖7个模型家族和4个推理引擎）上评估了两种目标的后门攻击：降低事实准确性和诱导发射攻击者控制的URL。触发条件下，事实准确性从90%平均降至15%，攻击者控制URL的发射成功率超过80%；良性输入无显著性能下降。后门在不同推理运行时上具有通用性，并能逃避最大开源权重分发平台的所有自动安全扫描。

Conclusion: 聊天模板是LLM供应链中可靠且目前无防御的攻击面，需要新的安全措施来防范此类模板后门攻击。

Abstract: Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK框架显著提升LLM在符号规划任务上的表现，在Blocksworld随机版本中准确率从31.5%提升至97.3%，帮助模型从语言模式转向形式化推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有提示技术如CoT在LLM推理能力上存在局限，需要更有效的框架来提升模型在复杂规划任务中的表现。TMK框架因其能够捕捉因果、目的论和层次化推理结构，且提供明确的任务分解机制，被认为可能解决语言模型推理缺陷。

Method: 采用Task-Method-Knowledge（TMK）框架进行结构化提示，在PlanBench基准的Blocksworld领域进行实验，测试TMK是否能帮助语言模型更好地将复杂规划问题分解为可管理的子任务。

Result: TMK提示显著提升了推理模型的性能，在PlanBench中Blocksworld的随机版本（不透明符号任务）上，准确率从31.5%提升至97.3%。研究还发现TMK促使推理模型从默认的语言模式转向形式化、代码执行路径。

Conclusion: TMK框架不仅提供上下文，更是一种引导机制，能够帮助语言模型克服符号操作困难，在语义近似和符号操作之间建立桥梁，显著提升复杂规划任务的推理能力。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [13] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构造方法，通过结合执行反馈和LLM的思维链能力，提升数学推理的准确性和可修正性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统在数学推理方面仍缺乏可靠可修正的推理过程表示，要么采用僵化的顺序流程无法修正早期步骤，要么依赖可能无法识别和修复错误的启发式自我评估，同时程序化上下文可能分散语言模型的注意力并降低准确性。

Method: IIPC（迭代改进的程序构造）方法迭代地精炼程序化推理链，将执行反馈与基础LLM的原生思维链能力相结合，以保持高层次上下文焦点。

Result: IIPC在多个基础LLM上的大多数推理基准测试中超越了竞争方法。

Conclusion: IIPC通过迭代改进程序构造，结合执行反馈和思维链能力，有效提升了数学推理的可靠性和准确性，代码已开源发布。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [14] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk框架将多智能体系统的动态交互蒸馏到单个模型的权重中，将显式的测试时交互转化为隐式的模型能力，使单个智能体具备多智能体系统的智能同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多智能体系统通过迭代辩论实现了优越的推理性能，但实际部署受到高计算成本和错误传播的限制。需要一种方法既能保留多智能体系统的推理优势，又能降低计算开销。

Method: 提出了AgentArk框架，研究三种分层蒸馏策略：推理增强微调、基于轨迹的增强和过程感知蒸馏。通过将计算负担从推理转移到训练，将多智能体动态蒸馏到单个模型的权重中。

Result: 蒸馏后的模型在保持单个智能体效率的同时，展现出多智能体的强大推理和自我纠正性能。进一步证明了在各种推理任务中具有增强的鲁棒性和泛化能力。

Conclusion: AgentArk框架成功地将多智能体系统的动态交互蒸馏到单个模型中，为高效和鲁棒的多智能体开发提供了新的研究方向，代码已开源。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [15] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出一种状态级选择性验证框架，在验证成本受限的情况下优化验证资源分配，相比传统方法减少44%验证调用同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理中的测试时计算越来越受限于昂贵的验证成本，许多验证调用浪费在冗余或无前景的中间假设上，需要研究如何在验证成本受限的情况下优化验证资源分配。

Method: 提出状态级选择性验证框架，包含三个组件：1) 结构化移动接口上的确定性可行性门控；2) 结合学习状态距离和残差评分的预验证排序；3) 基于局部不确定性的自适应验证调用分配。

Result: 在MATH基准测试中，该方法相比best-of-N、多数投票和束搜索实现了更高的准确率，同时减少了44%的验证调用。

Conclusion: 通过状态级选择性验证框架，可以在验证成本受限的情况下更有效地分配验证资源，相比传统方法在减少验证开销的同时提升推理性能。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [16] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: RLVR训练早期阶段，CoT可监控性看似"免费获得"，但实际效果受数据多样性和指令遵循数据影响，且与模型能力正交


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型部署增加，审计其思维链轨迹的安全性变得至关重要。近期研究发现，在强化学习可验证奖励的早期阶段，可监控性（思维链忠实反映内部计算的程度）可能表现为"免费礼物"，需要系统评估这一现象

Method: 通过跨模型家族和训练领域的系统评估，分析数据多样性、指令遵循数据在RLVR训练中的关键作用，进行机制分析关注响应分布锐化和对提示的关注度

Result: 可监控性改进具有强烈数据依赖性，与模型能力正交；机制分析显示可监控性增益主要来自响应分布锐化和对提示关注度增加，而非对推理轨迹的因果依赖增强

Conclusion: 可监控性在RLVR下的出现机制复杂，增益发生与否取决于数据条件，为理解模型透明度提供了整体视角

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [17] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 该论文首次系统性地研究了对抗性解释攻击，即通过操纵LLM生成解释的框架来调节用户对错误输出的信任，揭示了AI与用户之间认知通道的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统越来越多地融入人类决策循环，用户依赖模型推荐并解释其输出。大语言模型生成的自然语言解释会影响用户对AI输出的感知和信任，这为攻击者提供了新的攻击面——AI与用户之间的认知通道。目前大多数对抗性威胁针对模型的计算行为，而非依赖模型的用户。

Method: 引入对抗性解释攻击概念，通过信任校准差距指标量化威胁。进行控制实验（n=205），系统性地操纵解释框架的四个维度：推理模式、证据类型、沟通风格和呈现格式，测量用户对对抗性和良性解释的信任差异。

Result: 用户对对抗性和良性解释报告的信任几乎相同，对抗性解释在错误情况下仍能保留大部分良性信任。最脆弱的场景出现在：对抗性解释类似专家沟通、结合权威证据、中性语气和领域适当推理时；困难任务、事实驱动领域；以及教育程度较低、年轻或高度信任AI的参与者。

Conclusion: 这是首个将解释视为对抗性认知通道并量化其对AI辅助决策中人类信任影响的系统性安全研究，揭示了LLM生成解释可能被滥用来操纵人类信任的新安全威胁。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [18] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文提出一个反事实解释的axiomatic框架，通过公理系统定义了五种不同类型的反事实解释器，包括局部和全局解释，并建立了公理子集与解释器家族之间的一一对应关系。


<details>
  <summary>Details</summary>
Motivation: 当前大多数反事实解释器只关注单一类型的局部解释，缺乏对替代反事实类型的系统研究，也没有考虑全局反事实解释。本文旨在填补这两个研究空白。

Method: 建立基于期望属性的公理框架，证明不可能性定理，建立表示定理揭示公理子集与解释器家族的一一对应关系，从而识别五种根本不同的反事实类型。

Result: 发现了五种不同类型的反事实解释器家族，包括局部和全局解释类型。框架将现有解释器纳入分类体系，形式化描述其行为，并分析了生成此类解释的计算复杂度。

Conclusion: 提出的公理框架系统性地分类了反事实解释器，揭示了五种根本不同的反事实类型，为理解和设计更全面的可解释AI系统提供了理论基础。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [19] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: ORBIT框架通过元强化学习训练LLMs，使其能够在上下文中从交互中学习，显著提升在线决策性能，匹配GPT-5.2水平


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要在线交互、延迟反馈和探索-利用权衡的决策任务中表现不佳，无法可靠利用上下文交互经验

Method: 提出ORBIT：多任务、多回合的元强化学习框架，训练LLMs在上下文中从交互中学习，无需权重更新

Result: 经过元训练后，相对较小的开源模型(Qwen3-14B)在全新环境中表现出显著提升的在线学习能力，性能匹配GPT-5.2，大幅优于标准RL微调

Conclusion: 通过训练可以解决LLMs在线学习能力的局限性，模型规模扩展实验显示持续增益，表明推理时学习决策智能体仍有很大提升空间

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [20] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个将LLM应用视为上下文构建与执行问题的系统，而非仅选择单一模型。它通过异构DNN堆栈、上下文构建层和动作层处理复杂任务，仅将提炼后的上下文传递给用户选择的LLM生成最终响应。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用不应仅依赖单一大型模型，而应通过构建和操作上下文来解决问题。传统方法使用单一transformer模型处理所有任务效率低下且成本高昂，需要将计算负担从昂贵的大型模型转移到小型模型和工具栈上。

Method: 系统包含三个核心组件：(1) 异构DNN堆栈配合小型语言模型作为感知模块，处理复杂PDF、图表、多语言ASR等；(2) 上下文构建层爬取、索引、解析外部资源（网页、代码、PDF）为结构化状态；(3) 动作层支持浏览、检索、沙箱代码执行、驱动无头浏览器。顶层控制器决定运行哪些小型模型和动作，并将提炼的上下文传递给用户选择的LLM。

Result: Interfaze-Beta在多个基准测试中表现优异：MMLU-Pro 83.6%、MMLU 91.4%、GPQA-Diamond 81.3%、LiveCodeBench v5 57.8%、AIME-2025 90.0%。多模态任务上：MMMU(val) 77.3%、AI2D 91.5%、ChartQA 90.9%、Common Voice v16 90.8%。大部分查询由小型模型和工具栈处理，大型LLM仅操作提炼后的上下文。

Conclusion: Interfaze系统通过将LLM应用重构为上下文构建与执行问题，实现了在保持竞争力的准确率的同时，将大部分计算从昂贵的大型模型转移到小型模型和工具栈上，为高效、可扩展的LLM应用架构提供了新范式。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [21] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: OMG-Agent是一个新型的多模态生成框架，通过解耦语义规划和细节合成来解决数据不完整问题，相比现有方法在极端缺失情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有多模态系统面临数据不完整问题，传统重建方法存在瓶颈：参数化/生成模型容易产生幻觉（过度依赖内部记忆），检索增强框架存在检索僵化问题。这些端到端架构受到"语义-细节纠缠"的结构性限制，即逻辑推理和信号合成之间的冲突损害了保真度。

Method: 提出OMG-Agent框架，采用动态粗到细的智能体工作流程，模仿"先思考后行动"的认知过程。包含三个阶段：1) MLLM驱动的语义规划器，通过渐进上下文推理解决输入歧义，创建确定性结构化语义计划；2) 非参数证据检索器，将抽象语义锚定到外部知识；3) 检索注入执行器，利用检索证据作为灵活特征提示来克服僵化并合成高保真细节。

Result: 在多个基准测试上的广泛实验表明，OMG-Agent始终优于最先进的方法，在极端缺失情况下保持鲁棒性，例如在70%缺失率下在CMU-MOSI上获得2.6分的提升。

Conclusion: OMG-Agent通过将任务解耦为语义规划、证据检索和细节执行的协同阶段，有效解决了多模态数据不完整问题，克服了传统方法的局限性，实现了更高的保真度和鲁棒性。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [22] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 论文提出可扩展交互式监督框架，通过将复杂意图分解为递归决策树来增强人类监督能力，使非专家能生成专家级产品需求文档，对齐度提升54%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在复杂长时任务（如氛围编程）中日益自动化，出现了监督缺口。模型擅长执行，但用户因领域专业知识不足、难以精确表达意图、无法可靠验证复杂输出而难以有效指导模型，这构成了可扩展监督中的关键挑战。

Method: 提出可扩展交互式监督框架，将复杂意图分解为递归决策树，在节点处收集低负担反馈，递归聚合为精确全局指导，而非依赖开放式提示。通过强化学习仅使用在线用户反馈进行优化。

Result: 在网页开发任务中验证，使非专家能生成专家级产品需求文档，对齐度提升54%。框架可通过强化学习仅用在线用户反馈进行优化。

Conclusion: 该框架为解决AI规模化过程中的人类控制问题提供了实用路径，通过递归分解复杂意图和低负担反馈收集，使人类能有效监督超越自身能力的AI任务。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [23] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本文提出了一种零配置AI管道方法，通过数字孪生技术实现工业CPS中AI功能的模块化集成，解决了异构系统集成难题。


<details>
  <summary>Details</summary>
Motivation: 工业CPS系统日益复杂，物联网和工业物联网技术碎片化导致通信协议、数据格式和设备能力多样化，在物理层和智能功能层之间形成了巨大鸿沟。现有方法通常是孤立的、紧密耦合的，限制了AI功能的可扩展性和重用性。

Method: 提出模块化、可互操作的解决方案，通过最小化配置和解耦数字孪生与AI组件角色，实现AI管道在CPS中的无缝集成。引入零配置AI管道概念，由数字孪生协调数据管理和智能增强。

Result: 在微工厂场景中演示了该方法，展示了支持并发ML模型和动态数据处理的能力，有效加速了复杂工业环境中智能服务的部署。

Conclusion: 提出的零配置AI管道方法通过数字孪生技术解决了工业CPS中AI集成的关键挑战，为复杂工业环境中的智能服务部署提供了有效解决方案。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [24] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个基于置信度的智能体框架，通过Solver-Critic-Selector架构实现动态计算分配，在专家级科学推理任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在专家级科学推理任务（如Humanity's Last Exam）上表现有限，主要受限于僵化的工具管道、脆弱的多智能体协调和低效的测试时扩展。

Method: 提出ReThinker框架，采用Solver-Critic-Selector三阶段架构，基于模型置信度动态分配计算资源，实现自适应工具调用、引导式多维反思和鲁棒的置信度加权选择。同时提出反向数据合成管道和自适应轨迹回收策略，将成功推理轨迹转化为高质量监督数据。

Result: 在HLE、GAIA和XBench基准测试中，ReThinker持续优于现有最先进的基于工具的基础模型和深度研究系统，在专家级推理任务上取得了最先进的结果。

Conclusion: ReThinker通过置信度感知的智能体框架和创新的数据合成方法，有效解决了专家级科学推理的挑战，为复杂推理任务提供了可扩展的解决方案。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [25] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 研究提出一个序列交互框架来解决生成式AI系统与问答论坛之间的悖论关系，通过数据驱动的模拟展示了激励错配问题，但证明双方仍能实现约一半的理想效用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统一方面将用户从问答论坛吸引走，另一方面又依赖这些论坛产生的数据来提升性能，这种悖论关系需要解决。研究旨在探索AI系统与人类知识平台之间可持续协作的可能性。

Method: 提出了一个序列交互框架，让生成式AI系统向论坛提出问题，论坛可以选择发布部分问题。该框架考虑了非货币交换、信息不对称和激励错配等复杂因素。使用真实的Stack Exchange数据和常用LLM进行全面的数据驱动模拟。

Result: 实证证明了激励错配的存在，但显示参与者仍能实现理想全信息场景下约一半的效用。结果强调了AI系统与人类知识平台之间保持有效知识共享的可持续协作潜力。

Conclusion: 生成式AI系统与人类知识平台之间存在激励错配，但通过适当的协作框架，双方仍能实现显著效用，为可持续的知识共享协作提供了可能性。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [26] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 论文提出Vibe AIGC新范式，通过智能体编排解决当前生成式AI的意图-执行差距问题，将用户从提示工程师转变为提供"氛围"的指挥官，由元规划器分解为可执行的智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI受模型中心范式主导，虽然视觉保真度显著提升，但遇到了"可用性天花板"，表现为意图-执行差距——创作者的高层意图与当前单次模型的随机性、黑盒性质之间的根本差异。

Method: 受Vibe Coding启发，提出Vibe AIGC范式，通过智能体编排实现内容生成。用户作为指挥官提供"氛围"（高层审美偏好、功能逻辑等），中央元规划器作为系统架构师，将"氛围"分解为可执行、可验证、自适应的智能体流水线。

Result: Vibe AIGC通过从随机推理转向逻辑编排，弥合了人类想象力与机器执行之间的差距。这一转变将重新定义人机协作经济，将AI从脆弱的推理引擎转变为稳健的系统级工程伙伴。

Conclusion: Vibe AIGC范式将民主化复杂、长视野数字资产的创建，使生成式AI能够更好地服务于人类创意意图，实现更高效、可控的内容生成。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [27] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出WideSeek-R1框架，通过多智能体宽度扩展解决广泛信息搜索任务，使用4B参数模型达到与671B单智能体模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要关注深度扩展（单智能体解决长时程问题），但随着任务范围扩大，瓶颈从个体能力转向组织能力。现有多智能体系统依赖手工工作流和顺序交互，无法有效并行化工作。

Method: 提出WideSeek-R1框架，采用领导智能体-子智能体架构，通过多智能体强化学习训练，实现可扩展的编排和并行执行。使用共享LLM但隔离上下文和专用工具，在2万个广泛信息搜索任务数据集上进行联合优化。

Result: WideSeek-R1-4B在WideSearch基准测试中达到40.0%的item F1分数，性能与单智能体DeepSeek-R1-671B相当。随着并行子智能体数量增加，性能持续提升，验证了宽度扩展的有效性。

Conclusion: 宽度扩展是多智能体系统的重要方向，WideSeek-R1框架通过并行执行和可扩展编排，在广泛信息搜索任务上实现了高效性能，为多智能体系统设计提供了新思路。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [28] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 本文提出一个七维分类法来系统评估医疗领域LLM智能体研究现状，通过对49项研究的定量分析揭示了能力实现的不对称性


<details>
  <summary>Details</summary>
Motivation: 现有医疗LLM智能体研究缺乏统一框架，多为宽泛综述或单一能力探讨，需要系统分类法来评估研究现状和识别差距

Method: 建立七维分类法（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型、核心任务与子任务），包含29个子维度；使用明确纳入排除标准，对49项研究进行标注（完全实现、部分实现、未实现），并进行定量分析和共现模式统计

Result: 分析揭示了明显的不对称性：外部知识整合普遍实现（76%完全实现），而事件触发激活（92%未实现）和漂移检测与缓解（98%未实现）严重缺失；多智能体设计是主导模式（82%完全实现），编排层大多部分实现；信息中心能力领先，而治疗规划与处方等行动导向领域存在显著差距（59%未实现）

Conclusion: 研究为医疗LLM智能体提供了系统评估框架，揭示了当前研究的重点和空白，为未来研究方向提供了指导，特别是在行动导向任务和适应性能力方面需要更多关注

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [29] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 本文反驳了METR报告中关于AI能力呈指数增长的结论，认为数据不支持指数增长，并提出了更复杂的模型来预测AI能力将在近期出现拐点。


<details>
  <summary>Details</summary>
Motivation: 针对METR报告声称AI能力自2019年以来呈指数增长的观点，作者认为现有数据并不支持这一结论，旨在揭示现有指数增长预测的脆弱性。

Method: 1) 对METR的现有数据拟合S型曲线，发现拐点已经过去；2) 提出更复杂的模型，将AI能力分解为基础能力和推理能力，分别考察其改进速率。

Result: 分析显示AI能力增长并非指数型，S型曲线拟合表明拐点已经过去，复杂模型支持AI能力将在近期出现拐点的假设。

Conclusion: 现有关于AI能力指数增长的预测缺乏稳健性，需要更谨慎的建模方法来准确预测AI能力的发展轨迹。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [30] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: GEA（群体进化智能体）是一种新的开放式自我改进范式，将智能体群体作为基本进化单元，通过群体内显式经验共享和重用，更有效地将早期探索多样性转化为长期性能提升，在编码任务上显著优于现有自进化方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放式自进化范式采用树状结构进化，存在进化分支隔离导致的探索多样性利用效率低的问题。需要一种新范式来更有效地利用探索多样性，实现持续的性能提升。

Method: 提出GEA（群体进化智能体）范式，将智能体群体作为基本进化单元，在进化过程中实现群体内显式经验共享和重用，克服传统树状进化中分支隔离的局限性。

Result: 在编码基准测试中，GEA显著优于最先进的自进化方法（SWE-bench Verified：71.0% vs. 56.7%；Polyglot：88.3% vs. 68.3%），匹配或超越顶级人工设计智能体框架。GEA更有效地将早期探索多样性转化为长期进步，在相同进化智能体数量下表现更强，且具有更好的跨模型迁移性和鲁棒性。

Conclusion: GEA通过群体作为进化单元和显式经验共享，解决了传统自进化方法中探索多样性利用效率低的问题，在编码任务上实现了显著性能提升，展示了开放式自我改进的新方向。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [31] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 论文发现推理语言模型通过上下文精炼token表示（流体推理表示）来提升抽象问题解决能力，QwQ-32B模型在推理过程中逐步改进对动作和概念的内部表示，形成关注结构而非具体动作名称的抽象编码。


<details>
  <summary>Details</summary>
Motivation: 虽然推理语言模型在抽象问题上表现显著优于非推理模型，但其内部机制仍不清楚。本研究旨在揭示QwQ-32B这类专门训练用于生成扩展推理轨迹的模型如何处理抽象结构信息。

Method: 在语义混淆的规划领域Mystery Blocksworld上，通过机制分析研究QwQ-32B的内部表示变化。使用引导实验建立因果证据：注入成功轨迹中的精炼表示来提升准确性，用符号表示替换混淆编码来测试性能变化。

Result: QwQ-32B在推理过程中逐步改进对动作和概念的内部表示，形成关注结构而非具体动作名称的抽象编码。引导实验证实这些适应改进了解题能力：注入精炼表示能提升准确率，符号表示替换混淆编码仅导致最小性能损失。

Conclusion: 推理模型性能的关键驱动因素之一是上下文精炼token表示（流体推理表示），模型在推理过程中动态优化内部表示，形成结构化的抽象编码，这解释了推理模型在抽象问题上的优越表现。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>
