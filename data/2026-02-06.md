<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 23]
- [cs.AI](#cs.AI) [Total: 47]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [System-Level Isolation for Mixed-Criticality RISC-V SoCs: A "World" Reality Check](https://arxiv.org/abs/2602.05002)
*Luis Cunha,Jose Martins,Manuel Rodriguez,Tiago Gomes,Sandro Pinto,Uwe Moslehner,Kai Dieffenbach,Glenn Farrall,Kajetan Nuernberger,Thomas Roecker*

Main category: cs.CR

TL;DR: 本文对RISC-V硬件隔离原语（IOPMP、World-based checker及其改进版本）进行了比较分析，评估了它们在异构SoC中的安全性、性能和面积权衡，发现World-based checker具有更低的固定延迟和更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V在汽车、物联网和工业控制等领域的加速采用，这些领域对尺寸、重量、功耗和成本有严格限制，推动了异构SoC的发展。然而，这种集成带来了安全和隔离挑战，现有RISC-V硬件隔离原语的互操作性、可扩展性和实时系统适用性尚未得到充分理解。

Method: 本文从实际异构SoC设计的角度，对RISC-V Worlds、IOPMP和SmMTT等硬件隔离原语进行对比分析。实现了IOPMP、基于World的检查器，以及一个改进的RISC-V World检查器来解决基线规范的关键限制，并评估它们在安全保证和功耗-性能-面积方面的权衡。

Result: 结果显示，基于World的检查器引入了固定、配置无关的访问延迟，相比评估的替代方案具有更低的延迟，并能随着系统规模可预测地扩展。在宏观层面，提出的改进相比基线设计可减少约5%的SoC面积。

Conclusion: 所有成果将作为开源发布，这些发现有望直接促进RISC-V规范的演进和标准化，以及未来RISC-V SoC的设计。World-based checker在混合关键性环境中提供了更好的系统级隔离解决方案。

Abstract: As RISC-V adoption accelerates, domains such as automotive, the Internet of Things (IoT), and industrial control are attracting growing attention. These domains are subject to stringent Size, Weight, Power, and Cost (SWaP-C) constraints, which have driven a shift toward heterogeneous Systems-on-Chip (SoCs) integrating general-purpose CPUs, tightly coupled accelerators, and diverse I/O devices with different integrity levels. While such integration improves cost efficiency and performance, it introduces a fundamental safety and security challenge: enforcing system-level isolation in mixed-criticality environments. Although RISC-V International has proposed several hardware isolation primitives, including RISC-V Worlds, IOPMP, and SmMTT, their interoperability, scalability, and suitability for real-time systems remain insufficiently understood. In this paper, we present a comparative analysis of these primitives from the perspective of practical heterogeneous SoC designs. We implement an IOPMP, a World-based checker, and a modified RISC-V World checker that addresses key limitations of the baseline specification, and evaluate their trade-offs in terms of security guarantees and power-performance-area (PPA). Our results show that the World-based checker introduces a fixed, configuration-independent access latency, achieving lower worst-case delay than the evaluated alternatives while scaling predictably with system size. At the macro level, we estimate that the proposed modifications reduce SoC area by up to approximately 5% compared to a baseline design. All artifacts will be released as open source, and we expect these findings to directly contribute to the evolution and ratification of RISC-V specifications, as well as to the design of future RISC-V SoCs.

</details>


### [2] [Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software](https://arxiv.org/abs/2602.04894)
*Tomer Kordonsky,Maayan Yamin,Noam Benzimra,Amit LeVi,Avi Mendelson*

Main category: cs.CR

TL;DR: 该论文研究了LLM生成代码中的漏洞持续性，提出了Feature-Security Table（FSTab）框架，包含基于前端特征预测后端漏洞的黑盒攻击和评估模型漏洞一致性的方法。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常包含重复模板，这些模板可能引入可预测的漏洞。现有研究未充分探索LLM生成软件中漏洞的持续性和可预测性，这构成了新的攻击面。

Method: 提出了FSTab框架：1）黑盒攻击：通过观察前端特征和了解源LLM来预测后端漏洞，无需访问后端代码或源代码；2）模型中心评估：量化模型在不同程序、语义保持重述和应用领域中重现相同漏洞的一致性。

Result: 在GPT-5.2、Claude-4.5 Opus、Gemini-3 Pro等先进代码LLM上评估，结果显示强跨域迁移能力：即使目标域未包含在训练中，FSTab在Internal Tools（Claude-4.5 Opus）上达到94%攻击成功率和93%漏洞覆盖率。

Conclusion: LLM生成软件存在未充分探索的攻击面，代码生成存在安全风险。FSTab暴露了LLM生成代码中漏洞的持续性和可预测性，强调了需要更安全的代码生成方法。

Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.

</details>


### [3] [Privacy Amplification Persists under Unlimited Synthetic Data Release](https://arxiv.org/abs/2602.04895)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.CR

TL;DR: 该研究探讨了通过合成数据发布实现隐私放大的现象，发现在有界参数假设下，即使发布无限数量的合成记录，隐私放大仍然存在，这改进了先前工作中仅适用于模型维度远大于合成记录数量的渐近结果。


<details>
  <summary>Details</summary>
Motivation: 先前Pierquin等人的工作虽然首次为线性生成器建立了形式化的隐私放大保证，但仅适用于模型维度远大于合成记录数量的渐近情况，限制了实际应用价值。本研究旨在探索更实用的隐私放大场景。

Method: 在有限参数假设下，分析通过合成数据发布而非私有生成模型本身来实现隐私放大的机制，研究即使在发布无限数量合成记录时隐私放大仍然存在的条件。

Result: 研究发现了一个令人惊讶的结果：在有界参数假设下，即使发布无限数量的合成记录，隐私放大现象仍然持续存在，这改进了Pierquin等人先前工作的界限。

Conclusion: 该分析提供了结构性的见解，可能指导为更复杂的发布机制开发更严格的隐私保证，推动了合成数据发布隐私放大理论的实际应用。

Abstract: We study privacy amplification by synthetic data release, a phenomenon in which differential privacy guarantees are improved by releasing only synthetic data rather than the private generative model itself. Recent work by Pierquin et al. (2025) established the first formal amplification guarantees for a linear generator, but they apply only in asymptotic regimes where the model dimension far exceeds the number of released synthetic records, limiting their practical relevance. In this work, we show a surprising result: under a bounded-parameter assumption, privacy amplification persists even when releasing an unbounded number of synthetic records, thereby improving upon the bounds of Pierquin et al. (2025). Our analysis provides structural insights that may guide the development of tighter privacy guarantees for more complex release mechanisms.

</details>


### [4] [Semantic-level Backdoor Attack against Text-to-Image Diffusion Models](https://arxiv.org/abs/2602.04898)
*Tianxin Chen,Wenbo Jiang,Hongqiao Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CR

TL;DR: 本文提出SemBD（语义级后门攻击），通过将触发器定义为连续语义区域而非离散文本模式，在表示层面植入后门，有效规避现有基于枚举和注意力一致性的防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的后门攻击通常依赖固定文本触发器和单实体后门目标，容易被基于枚举的输入防御和注意力一致性检测发现。需要更隐蔽、更鲁棒的后门攻击方法。

Method: 1. 在表示层面植入后门，将触发器定义为连续语义区域而非离散文本模式；2. 通过蒸馏编辑交叉注意力层中的键值投影矩阵；3. 引入语义正则化防止不完整语义下的意外激活；4. 采用多实体后门目标避免高度一致的交叉注意力模式。

Result: 实验表明SemBD实现了100%的攻击成功率，同时对最先进的输入级防御保持强鲁棒性，有效规避了现有防御方法的检测。

Conclusion: SemBD通过语义级后门攻击，在保持高攻击成功率的同时显著提升了隐蔽性和鲁棒性，揭示了扩散模型在表示层面的安全漏洞，为防御设计提供了新视角。

Abstract: Text-to-image (T2I) diffusion models are widely adopted for their strong generative capabilities, yet remain vulnerable to backdoor attacks. Existing attacks typically rely on fixed textual triggers and single-entity backdoor targets, making them highly susceptible to enumeration-based input defenses and attention-consistency detection. In this work, we propose Semantic-level Backdoor Attack (SemBD), which implants backdoors at the representation level by defining triggers as continuous semantic regions rather than discrete textual patterns. Concretely, SemBD injects semantic backdoors by distillation-based editing of the key and value projection matrices in cross-attention layers, enabling diverse prompts with identical semantic compositions to reliably activate the backdoor attack. To further enhance stealthiness, SemBD incorporates a semantic regularization to prevent unintended activation under incomplete semantics, as well as multi-entity backdoor targets that avoid highly consistent cross-attention patterns. Extensive experiments demonstrate that SemBD achieves a 100% attack success rate while maintaining strong robustness against state-of-the-art input-level defenses.

</details>


### [5] [Phantom Transfer: Data-level Defences are Insufficient Against Data Poisoning](https://arxiv.org/abs/2602.04899)
*Andrew Draganov,Tolga H. Dur,Anandmayi Bhongade,Mary Phuong*

Main category: cs.CR

TL;DR: 提出名为"Phantom Transfer"的数据投毒攻击，即使知道投毒方式也无法过滤，攻击对GPT-4等模型有效，完全改写数据集也无法阻止攻击


<details>
  <summary>Details</summary>
Motivation: 当前数据层面的防御措施对复杂的数据投毒攻击可能不足，需要探索更有效的安全防护方法

Method: 修改潜意识学习技术使其适用于现实场景，利用密码触发行为植入模型，同时绕过现有防御机制

Result: 攻击在多种模型上有效（包括GPT-4），即使完全改写数据集也无法阻止攻击，能够植入密码触发行为并绕过防御

Conclusion: 数据层面的防御措施不足以阻止复杂的数据投毒攻击，未来工作应关注模型审计和白盒安全方法

Abstract: We present a data poisoning attack -- Phantom Transfer -- with the property that, even if you know precisely how the poison was placed into an otherwise benign dataset, you cannot filter it out. We achieve this by modifying subliminal learning to work in real-world contexts and demonstrate that the attack works across models, including GPT-4.1. Indeed, even fully paraphrasing every sample in the dataset using a different model does not stop the attack. We also discuss connections to steering vectors and show that one can plant password-triggered behaviours into models while still beating defences.
  This suggests that data-level defences are insufficient for stopping sophisticated data poisoning attacks. We suggest that future work should focus on model audits and white-box security methods.

</details>


### [6] [PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM](https://arxiv.org/abs/2602.04927)
*Gautam Savaliya,Robert Aufschläger,Abhishek Subedi,Michael Heigl,Martin Schramm*

Main category: cs.CR

TL;DR: PriMod4AI提出了一种混合隐私威胁建模框架，结合LINDDUN传统隐私分类和AI特有的模型中心隐私攻击，通过知识库向量化和LLM增强分析，为AI系统提供全面的隐私威胁评估。


<details>
  <summary>Details</summary>
Motivation: AI系统在处理敏感或高维数据时面临复杂隐私风险，传统LINDDUN框架无法覆盖模型中心隐私攻击（如成员推断和模型反转），需要新的威胁建模方法同时处理传统和AI特有的隐私威胁。

Method: PriMod4AI采用混合方法：1）整合LINDDUN知识库和模型中心隐私攻击知识库；2）将知识库嵌入向量数据库进行语义检索；3）结合数据流图元数据；4）使用检索增强和数据流特定的提示生成指导LLM识别、解释和分类隐私威胁。

Result: 在两个AI系统上的评估表明，PriMod4AI能广泛覆盖传统隐私类别，同时识别模型中心隐私威胁。框架在不同LLM上产生一致、基于知识的输出，观察到的协议分数反映了这种一致性。

Conclusion: PriMod4AI提供了一个统一的隐私威胁建模框架，有效整合传统LINDDUN威胁和AI特有隐私攻击，为AI系统生命周期各阶段提供全面、有依据的隐私威胁评估。

Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy attacks such as membership inference and model inversion, which LINDDUN does not cover. To address both classical LINDDUN threats and additional AI-driven privacy attacks, PriMod4AI introduces a hybrid privacy threat modeling approach that unifies two structured knowledge sources, a LINDDUN knowledge base representing the established taxonomy, and a model-centric privacy attack knowledge base capturing threats outside LINDDUN. These knowledge bases are embedded into a vector database for semantic retrieval and combined with system level metadata derived from Data Flow Diagram. PriMod4AI uses retrieval-augmented and Data Flow specific prompt generation to guide large language models (LLMs) in identifying, explaining, and categorizing privacy threats across lifecycle stages. The framework produces justified and taxonomy-grounded threat assessments that integrate both classical and AI-driven perspectives. Evaluation on two AI systems indicates that PriMod4AI provides broad coverage of classical privacy categories while additionally identifying model-centric privacy threats. The framework produces consistent, knowledge-grounded outputs across LLMs, as reflected in agreement scores in the observed range.

</details>


### [7] [Attack Selection Reduces Safety in Concentrated AI Control Settings against Trusted Monitoring](https://arxiv.org/abs/2602.04930)
*Joachim Schaeffer,Arjun Khandelwal,Tyler Tracy*

Main category: cs.CR

TL;DR: 研究AI如何通过对抗性选择攻击来规避监控系统，发现让攻击模型思考监控器会显著降低安全性


<details>
  <summary>Details</summary>
Motivation: 未来AI部署将受到监控以防止恶意行为，但AI可能通过对抗性选择攻击来规避监控，这种攻击选择能力特别令人担忧

Method: 在BigCodeBench后门设置中，让红队创建攻击策略，将攻击选择分解为两个问题：将攻击映射到质量分数，将质量分数映射到提交概率。将攻击选择构建为分类问题，并提示攻击模型思考监控器

Result: 安全性对假阳性率(FPR)比真阳性率(TPR)更敏感。当提示攻击模型思考监控器并谨慎选择攻击时，安全性从基线99%降至59%（在0.5%审计预算下）

Conclusion: 激发模型的攻击选择能力对于避免在控制评估中获得过于乐观的安全分数至关重要，攻击选择是AI安全评估中需要认真考虑的重要方面

Abstract: Future AI deployments will likely be monitored for malicious behaviour. The ability of these AIs to subvert monitors by adversarially selecting against them - attack selection - is particularly concerning. To study this, we let a red team create attack policies that attempt to insert attacks into code without being caught by a monitor in the concentrated BigCodeBench backdooring setting. We decompose attack selection into two problems: mapping attacks to a quality score and mapping quality scores to submission probabilities. We frame attack selection as a classification problem and show that safety is significantly more sensitive to FPR than TPR. We find that prompting the attacker model to reason about the monitor while being cautious with attack selection reduces safety from a baseline of 99% to 59% at 0.5% auditing budget, emphasizing that eliciting attack selection capabilities of models is vital to avoid overly optimistic safety scores in control evaluations.

</details>


### [8] [The Birthmark Standard: Privacy-Preserving Photo Authentication via Hardware Roots of Trust and Consortium Blockchain](https://arxiv.org/abs/2602.04933)
*Sam Ryan*

Main category: cs.CR

TL;DR: 该论文提出了一种名为"Birthmark Standard"的新型图像认证架构，利用相机传感器固有的制造缺陷特征（NUC映射和PRNU模式）生成硬件根认证密钥，通过联盟区块链存储认证记录，解决现有C2PA方案易受元数据剥离和依赖企业控制基础设施的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统快速发展导致摄影证据可信度崩溃，现有认证方法（如C2PA）存在两个关键缺陷：1）元数据在社交媒体重新处理过程中易被剥离；2）依赖企业控制的验证基础设施，商业利益可能与公共利益冲突。

Method: 提出Birthmark Standard认证架构：利用相机传感器制造缺陷（非均匀性校正映射和PRNU模式）生成硬件根认证密钥；采用密钥表架构创建匿名认证证书，保持超过1000台设备的匿名集；将认证记录存储在由新闻机构运营的联盟区块链上；使用ProVerif形式化验证隐私属性。

Result: 原型系统在树莓派4硬件上实现完整密码学管道；性能分析显示相机开销低于100毫秒，验证延迟在每日100万次认证规模下低于500毫秒；形式化验证证明在Dolev-Yao对手假设下满足制造商非关联性和区块链观察者非识别性。

Conclusion: Birthmark Standard提供了一种硬件根、隐私保护且不依赖企业基础设施的图像认证方案，能够抵抗元数据剥离，为新闻和公共话语提供可靠的证据基础，同时保护设备匿名性。

Abstract: The rapid advancement of generative AI systems has collapsed the credibility landscape for photographic evidence. Modern image generation models produce photorealistic images undermining the evidentiary foundation upon which journalism and public discourse depend. Existing authentication approaches, such as the Coalition for Content Provenance and Authenticity (C2PA), embed cryptographically signed metadata directly into image files but suffer from two critical failures: technical vulnerability to metadata stripping during social media reprocessing, and structural dependency on corporate-controlled verification infrastructure where commercial incentives may conflict with public interest. We present the Birthmark Standard, an authentication architecture leveraging manufacturing-unique sensor entropy from non-uniformity correction (NUC) maps and PRNU patterns to generate hardware-rooted authentication keys. During capture, cameras create anonymized authentication certificates proving sensor authenticity without exposing device identity via a key table architecture maintaining anonymity sets exceeding 1,000 devices. Authentication records are stored on a consortium blockchain operated by journalism organizations rather than commercial platforms, enabling verification that survives all metadata loss. We formally verify privacy properties using ProVerif, proving observational equivalence for Manufacturer Non-Correlation and Blockchain Observer Non-Identification under Dolev-Yao adversary assumptions. The architecture is validated through prototype implementation using Raspberry Pi 4 hardware, demonstrating the complete cryptographic pipeline. Performance analysis projects camera overhead below 100ms and verification latency below 500ms at scale of one million daily authentications.

</details>


### [9] [SynthForensics: A Multi-Generator Benchmark for Detecting Synthetic Video Deepfakes](https://arxiv.org/abs/2602.04939)
*Roberto Leotta,Salvatore Alfio Sambataro,Claudio Vittorio Ragaglia,Mirko Casu,Yuri Petralia,Francesco Guarnera,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CR

TL;DR: SynthForensics是首个面向纯合成视频深度伪造检测的人类中心化基准，包含来自5种不同架构开源T2V模型的6815个高质量视频，揭示了现有检测器在合成视频检测上的脆弱性和泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 随着开源文本到视频（T2V）模型在消费级硬件上的普及，高保真合成视频的创建能力正在民主化，这使得现有的以人脸为中心和基于操纵的检测基准变得过时，迫切需要新的基准来应对这一威胁。

Method: 构建了SynthForensics基准，包含来自5种架构不同的最先进开源T2V模型的6815个独特视频，采用两阶段人工参与验证确保语义和视觉质量，每个视频提供原始、无损、轻度和重度压缩四个版本以测试现实世界鲁棒性。

Result: 实验表明，最先进的检测器在这一新领域既脆弱又泛化能力有限：平均AUC性能下降29.19%，某些方法表现甚至比随机猜测更差，顶级模型在重度压缩下损失超过30个点。通过在SynthForensics上训练可以缓解性能差距，实现对新生成器的鲁棒泛化（93.81% AUC），但会降低与传统基于操纵的深度伪造的向后兼容性。

Conclusion: SynthForensics基准填补了纯合成视频深度伪造检测的空白，揭示了现有检测方法的局限性，为开发更鲁棒的检测器提供了重要资源，同时强调了在合成媒体时代需要重新思考深度伪造检测方法。

Abstract: The landscape of synthetic media has been irrevocably altered by text-to-video (T2V) models, whose outputs are rapidly approaching indistinguishability from reality. Critically, this technology is no longer confined to large-scale labs; the proliferation of efficient, open-source generators is democratizing the ability to create high-fidelity synthetic content on consumer-grade hardware. This makes existing face-centric and manipulation-based benchmarks obsolete. To address this urgent threat, we introduce SynthForensics, to the best of our knowledge the first human-centric benchmark for detecting purely synthetic video deepfakes. The benchmark comprises 6,815 unique videos from five architecturally distinct, state-of-the-art open-source T2V models. Its construction was underpinned by a meticulous two-stage, human-in-the-loop validation to ensure high semantic and visual quality. Each video is provided in four versions (raw, lossless, light, and heavy compression) to enable real-world robustness testing. Experiments demonstrate that state-of-the-art detectors are both fragile and exhibit limited generalization when evaluated on this new domain: we observe a mean performance drop of $29.19\%$ AUC, with some methods performing worse than random chance, and top models losing over 30 points under heavy compression. The paper further investigates the efficacy of training on SynthForensics as a means to mitigate these observed performance gaps, achieving robust generalization to unseen generators ($93.81\%$ AUC), though at the cost of reduced backward compatibility with traditional manipulation-based deepfakes. The complete dataset and all generation metadata, including the specific prompts and inference parameters for every video, will be made publicly available at [link anonymized for review].

</details>


### [10] [Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?](https://arxiv.org/abs/2602.05023)
*Ruixin Yang,Ethan Mendes,Arthur Wang,James Hays,Sauvik Das,Wei Xu,Alan Ritter*

Main category: cs.CR

TL;DR: 该论文提出了VLM-GEOPRIVACY基准，用于评估视觉语言模型在图像地理位置隐私保护方面的表现，发现现有模型虽然能精确定位，但与人类隐私期望不符，容易过度披露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型和前沿多模态大推理模型在图像地理位置识别方面表现出色，这带来了严重的隐私风险，因为这些广泛可访问的模型可能被滥用来从随意分享的照片中推断敏感位置，甚至达到街道级别的精度，超出了分享者同意或意图披露的详细程度。现有方法采用一刀切的限制措施，无法区分合法的地理位置使用和恶意行为。

Method: 提出了VLM-GEOPRIVACY基准，挑战视觉语言模型解释真实世界图像中的潜在社会规范和上下文线索，并确定适当的位置信息披露级别。该基准用于评估模型在尊重上下文完整性方面的表现。

Result: 评估了14个领先的视觉语言模型，结果显示尽管这些模型能够精确地定位图像，但它们与人类隐私期望的对齐程度很差。模型经常在敏感上下文中过度披露信息，并且容易受到基于提示的攻击。

Conclusion: 研究结果呼吁在多模态系统中采用新的设计原则，以纳入上下文条件的隐私推理，使模型能够根据图像内容推理适当的隐私保护级别，平衡隐私和实用性。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.

</details>


### [11] [VEXA: Evidence-Grounded and Persona-Adaptive Explanations for Scam Risk Sensemaking](https://arxiv.org/abs/2602.05056)
*Heajun An,Connor Ng,Sandesh Sharma Dulal,Junghwan Kim,Jin-Hee Cho*

Main category: cs.CR

TL;DR: VEXA是一个基于证据和角色自适应的诈骗检测解释框架，通过结合GradientSHAP归因和理论驱动的漏洞角色，为学习者生成可理解的诈骗解释。


<details>
  <summary>Details</summary>
Motivation: 在线诈骗日益复杂，生成式AI使欺骗更加流畅和上下文感知。虽然基于Transformer的检测器有很强的预测性能，但其解释对非专家不透明或与模型决策不一致。

Method: 提出VEXA框架，整合GradientSHAP归因（基于梯度的SHAP方法）和理论驱动的漏洞角色，生成面向学习者的诈骗解释。证据基础确保语义可靠性，角色适应引入可解释的风格变化。

Result: 在多渠道数据集上的评估显示：基于检测器证据的解释提高了语义可靠性而不增加语言复杂性；角色适应引入可解释的风格变化而不破坏证据对齐。证据基础控制语义正确性，角色适应在忠实性约束下操作于呈现层面。

Conclusion: VEXA证明了角色自适应、证据基础解释的可行性，为非正式场景下可信赖的、面向学习者的安全解释提供了设计指导。

Abstract: Online scams across email, short message services, and social media increasingly challenge everyday risk assessment, particularly as generative AI enables more fluent and context-aware deception. Although transformer-based detectors achieve strong predictive performance, their explanations are often opaque to non-experts or misaligned with model decisions. We propose VEXA, an evidence-grounded and persona-adaptive framework for generating learner-facing scam explanations by integrating GradientSHAP-based attribution with theory-informed vulnerability personas. Evaluation across multi-channel datasets shows that grounding explanations in detector-derived evidence improves semantic reliability without increasing linguistic complexity, while persona conditioning introduces interpretable stylistic variation without disrupting evidential alignment. These results reveal a key design insight: evidential grounding governs semantic correctness, whereas persona-based adaptation operates at the level of presentation under constraints of faithfulness. Together, VEXA demonstrates the feasibility of persona-adaptive, evidence-grounded explanations and provides design guidance for trustworthy, learner-facing security explanations in non-formal contexts.

</details>


### [12] [Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning](https://arxiv.org/abs/2602.05089)
*Ethan Rathbun,Wo Wei Lin,Alina Oprea,Christopher Amato*

Main category: cs.CR

TL;DR: 本文提出了一种针对强化学习模拟器的新型后门攻击"Daze"，能够在无需修改或观察奖励信号的情况下，在RL智能体中植入动作级后门，并在真实机器人硬件上实现了攻击转移。


<details>
  <summary>Details</summary>
Motivation: 模拟器是强化学习成功的关键，但也是安全盲点。攻击者可能通过修改模拟器动态来植入恶意后门。传统后门攻击假设攻击者几乎完全控制训练流程，这在模拟器环境中不现实，因此需要一种更隐蔽的攻击方法。

Method: 提出"Daze"攻击方法，通过利用模拟器动态而非修改奖励信号来植入后门。攻击者只需在特定"触发器"出现时操纵环境动态，就能使智能体可靠地执行目标动作。提供了形式化证明和实证评估。

Result: Daze攻击能够在离散和连续动作空间任务中可靠且隐蔽地植入后门。首次展示了RL后门攻击能够转移到真实机器人硬件上，证明了攻击的实际威胁性。

Conclusion: 模拟器安全是RL训练流程中的重要薄弱环节。Daze攻击展示了即使不控制奖励信号也能植入后门的可能性，强调了保护RL训练流程所有组件的必要性，以防范恶意攻击。

Abstract: Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger'', leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent's training pipeline, enabling them to both alter and observe agent's rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze'' which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze's effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.

</details>


### [13] [Crypto-asset Taxonomy for Investors and Regulators](https://arxiv.org/abs/2602.05098)
*Xiao Zhang,Juan Ignacio Ibañez,Jiahua Xu*

Main category: cs.CR

TL;DR: 该论文开发了一个多维度的加密货币资产分类框架，将技术设计与市场结构和监管联系起来，填补了该领域缺乏统一分类体系的空白。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场交易量和市场份额不断增长，但缺乏统一全面的资产级别分类框架，这阻碍了对该市场的系统性研究和监管。

Method: 基于既有的分类指南和现有模型，从理论、监管框架和案例研究中推导维度，然后对前100种资产进行映射，并提供详细的案例研究。

Result: 分类框架涵盖技术标准、关键资源集中度、资产功能、法律分类以及铸造、收益、赎回等机制设计；资产映射揭示了重复的设计模式，捕捉了现有分类边界上的边缘案例特征，并记录了名义去中心化资产中的中心化控制。

Conclusion: 该框架为加密货币市场的系统性研究提供了基础，支持监管机构评估代币风险，并为投资者和数字平台设计者提供了在构建或参与电子市场时比较资产的工具。

Abstract: Crypto-assets are a main segment of electronic markets, with growing trade volume and market share, yet there's no unified and comprehensive asset level taxonomy framework. This paper develops a multidimensional taxonomy for crypto-assets that connects technical design to market structure and regulation. Building on established taxonomy guideline and existing models, we derive dimensions from theory, regulatory frameworks, and case studies. We then map top 100 assets within the structure and provide several detailed case studies. The taxonomy covers technology standard, centralisation of critical resources, asset function, legal classification and mechanism designs of minting, yield, redemption. The asset mapping and case studies reveal recurring design patterns, capture features of edge cases that sit on boundaries of current categorisations, and document centralised control of nominal decentralised assets. This paper provides framework for systematic study for crypto markets, supports regulators in assessing token risks, and offers investors and digital platform designers a tool to compare assets when building or participate in electronic markets.

</details>


### [14] [SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions](https://arxiv.org/abs/2602.05329)
*Ziyou Jiang,Lin Shi,Guowei Yang,Xuyan Ma,Fenglong Li,Qing Wang*

Main category: cs.CR

TL;DR: SynAT是一种从在线安全讨论帖中自动合成攻击树的方法，利用LLM和提示学习筛选相关句子，通过事件关系提取模型获取攻击信息，最后使用启发式规则构建攻击树。


<details>
  <summary>Details</summary>
Motivation: 官方安全信息发布存在时间延迟，导致安全知识库维护不足，难以应对新兴安全风险。而在线知识共享平台上的安全讨论帖包含大量众包安全讨论，可用于增强安全知识库。

Method: 1) 使用大型语言模型和提示学习筛选可能包含攻击信息的句子范围；2) 采用基于转换的事件和关系提取模型同时从筛选范围内提取事件和关系；3) 应用启发式规则，利用提取的事件和关系合成攻击树。

Result: 在5,070个Stack Overflow安全帖子上进行评估，SynAT在事件和关系提取方面优于所有基线方法，在攻击树合成中实现了最高的树相似度。已成功应用于增强华为安全知识库以及公共安全知识库CVE和CAPEC。

Conclusion: SynAT能够有效从众包安全讨论中自动合成攻击树，具有实际应用价值，可用于增强企业安全知识库和公共安全数据库。

Abstract: Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.

</details>


### [15] [BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Guowen Xu*

Main category: cs.CR

TL;DR: BadTemplate是一种无需训练的LLM后门攻击方法，通过操纵聊天模板在系统提示中注入恶意指令，实现高达100%的攻击成功率


<details>
  <summary>Details</summary>
Motivation: 聊天模板作为LLM训练和推理的常用技术，其可定制性可能被攻击者利用，在用户不知情的情况下向系统提示注入任意字符串，形成新的攻击面

Method: BadTemplate攻击通过精心设计的恶意指令插入到高优先级的系统提示中，无需模型重新训练，利用聊天模板的可定制性实现后门攻击

Result: 在5个基准数据集、6个开源和3个闭源LLM上的实验表明，BadTemplate攻击成功率高达100%，显著优于传统基于提示的后门攻击，且HuggingFace和LLM-as-a-judge等主流检测平台对其效果有限

Conclusion: 该研究揭示了聊天模板在LLM供应链中潜在的安全风险，BadTemplate攻击的简单性、可扩展性和高有效性使其在现实系统中易于广泛部署，可能造成快速传播、经济损失和大规模错误信息等严重后果

Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.

</details>


### [16] [LTRAS: A Linkable Threshold Ring Adaptor Signature Scheme for Efficient and Private Cross-Chain Transactions](https://arxiv.org/abs/2602.05431)
*Yi Liang,Jinguang Han*

Main category: cs.CR

TL;DR: 本文提出了一种可链接阈值环适配器签名方案(LTRAS)，结合了适配器签名的条件绑定、阈值环签名的多账户支付特性以及可链接性防止双花，显著降低了大规模环和多账户支付场景下的计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 区块链技术面临可扩展性和吞吐量限制，支付通道等链下解决方案成为研究重点。适配器签名因其支持原子性而被视为构建支付通道的有前景原语，但标准适配器签名可能泄露签名者身份，存在隐私问题。环签名虽能提供匿名性，但在UTXO区块链（如Monero）常用的多账户支付场景中通常引入高通信开销。

Method: 提出可链接阈值环适配器签名(LTRAS)方案，整合了适配器签名的条件绑定特性、阈值环签名的多账户支付能力以及可链接性以防止双花。提供了LTRAS的形式化定义、安全模型和具体构造，并通过理论分析和实验实现评估其性能。

Result: 实验结果表明，在大规模环和多账户支付场景下，该方案相比现有方案显著降低了计算和通信开销。同时讨论了该方案在跨链原子交换中的应用，展示了其在增强区块链交易隐私和效率方面的潜力。

Conclusion: LTRAS方案成功解决了适配器签名隐私泄露和环签名高通信开销的问题，为区块链支付通道提供了一种既保护隐私又高效的解决方案，特别适用于UTXO区块链的多账户支付场景，在跨链原子交换等应用中具有重要价值。

Abstract: Despite the advantages of decentralization and immutability, blockchain technology faces significant scalability and throughput limitations, which has prompted the exploration of off-chain solutions like payment channels. Adaptor signatures have been considered a promising primitive for constructing such channels due to their support for atomicity, offering an alternative to traditional hash-timelock contracts. However, standard adaptor signatures may reveal signer identity, raising potential privacy concerns. While ring signatures can mitigate this issue by providing anonymity, they often introduce high communication overhead, particularly in multi-account payment settings commonly used in UTXO-based blockchains like Monero. To address these limitations, we propose a Linkable Threshold Ring Adaptor Signature (LTRAS) scheme, which integrates the conditional binding of adaptor signatures, the multi-account payment of threshold ring signatures, and the linkability for preventing double-spending. The formal definition, security model and concrete construction of LTRAS are provided. We also analyze its security and evaluate its performance through theoretical analysis and experimental implementation. Experimental results demonstrate that our scheme achieve significantly lower computation and communication overhead compared to existing schemes in large ring sizes and multi-account payment scenarios. Finally, we discuss its application in cross-chain atomic swaps, demonstrating its potential for enhancing privacy and efficiency in blockchain transactions.

</details>


### [17] [Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection](https://arxiv.org/abs/2602.05484)
*Takashi Koide,Hiroki Nakano,Daiki Chiba*

Main category: cs.CR

TL;DR: 该论文首次全面评估了针对多模态LLM钓鱼检测的提示注入攻击，提出了攻击分类框架并开发了InjectDefuser防御方案，显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站数量和复杂性持续增长，虽然基于LLM的检测方法有前景，但LLM固有的提示注入漏洞被攻击者利用。攻击者可以完全控制钓鱼网站元素，利用LLM与人类感知不对称性进行隐蔽攻击，而现有研究对此风险及防御策略探索不足。

Method: 提出二维分类法（攻击技术和攻击面）描述现实攻击策略；实现多样化攻击并实证研究多个代表性LLM检测系统；开发InjectDefuser防御框架，结合提示强化、基于白名单的检索增强和输出验证。

Result: 实验表明，即使是GPT-5等最先进模型在钓鱼检测中仍易受提示注入攻击；InjectDefuser防御框架显著降低了多种模型的攻击成功率，提高了检测可靠性。

Conclusion: 该研究明确了提示注入在钓鱼检测中的风险格局，提供了实用的防御方案，增强了下一代钓鱼对抗措施的可靠性，为构建更安全的LLM安全应用提供了重要参考。

Abstract: Phishing sites continue to grow in volume and sophistication. Recent work leverages large language models (LLMs) to analyze URLs, HTML, and rendered content to decide whether a website is a phishing site. While these approaches are promising, LLMs are inherently vulnerable to prompt injection (PI). Because attackers can fully control various elements of phishing sites, this creates the potential for PI that exploits the perceptual asymmetry between LLMs and humans: instructions imperceptible to end users can still be parsed by the LLM and can stealthily manipulate its judgment. The specific risks of PI in phishing detection and effective mitigation strategies remain largely unexplored. This paper presents the first comprehensive evaluation of PI against multimodal LLM-based phishing detection. We introduce a two-dimensional taxonomy, defined by Attack Techniques and Attack Surfaces, that captures realistic PI strategies. Using this taxonomy, we implement diverse attacks and empirically study several representative LLM-based detection systems. The results show that phishing detection with state-of-the-art models such as GPT-5 remains vulnerable to PI. We then propose InjectDefuser, a defense framework that combines prompt hardening, allowlist-based retrieval augmentation, and output validation. Across multiple models, InjectDefuser significantly reduces attack success rates. Our findings clarify the PI risk landscape and offer practical defenses that improve the reliability of next-generation phishing countermeasures.

</details>


### [18] [GNSS SpAmming: a spoofing-based GNSS denial-of-service attack](https://arxiv.org/abs/2602.05517)
*Sergio Angulo Cosín,Javier Junquera-Sánchez,Carlos Hernando-Ramiro,José-Antonio Gómez-Sánchez*

Main category: cs.CR

TL;DR: 本文提出了一种新型GNSS攻击"SpAmming"，结合了干扰和欺骗两种攻击方式，通过CDMA复用技术使接收器失去对合法卫星信号的访问，现有反制措施难以应对。


<details>
  <summary>Details</summary>
Motivation: 现有GNSS系统主要面临干扰和欺骗两种攻击，虽然已有大量反制措施，但作者发现结合这两种攻击的新型攻击方式"SpAmming"能够更隐蔽地达到相同效果，现有防御机制难以应对。

Method: 利用大多数GNSS中的CDMA复用技术，通过欺骗攻击使接收器失去对合法卫星信号的访问。使用SDR系统开发实验验证，针对冷启动、温启动和已获取PVT解算的接收器进行测试，并尝试不同攻击配置（原始虚假信号发射、精确多普勒效应配置、码偏移等）。

Result: 实验证明SpAmming攻击对冷启动接收器特别有效，但在其他场景下也表现出有效性，尤其是与其他攻击结合时。攻击效果取决于接收器的先前状态。

Conclusion: SpAmming攻击结合了干扰和欺骗的特点，现有反制措施难以应对。需要开发新的检测和对抗方法，特别是对于OSNMA等认证服务，需要进一步研究以更好地理解其影响并改进对类似攻击的响应。

Abstract: GNSSs are vulnerable to attacks of two kinds: jamming (i.e. denying access to the signal) and spoofing (i.e. impersonating a legitimate satellite). These attacks have been extensively studied, and we have a myriad of countermeasures to mitigate them. In this paper we expose a new type of attack: SpAmming, which combines both approaches to achieve the same effects in a more subtle way.
  Exploiting the CDMA multiplexing present in most GNSSs, and through a spoofing attack, this approach leads the receiver to lose access to the signal of a legitimate satellite, which would be equivalent to a denial of service; but in this case the existing countermeasures against jamming or spoofing would not allow safeguarding its effectiveness, as it is neither of them.
  An experimental proof-of-concept is presented in which its impact is evaluated as a function of the previous state of the receiver. Using an SDR-based system developed at the Space Security Centre, the attack is executed against a cold-started receiver, a warm-started receiver, and a receiver that has already acquired the PVT solution and is navigating. Different attack configurations are also tested, starting from a raw emission of the false signal, to surgical Doppler effect configuration, code offset, etc. Although it is shown to be particularly successful against cold-started receivers, the results show that it is also effective in other scenarios, especially if accompanied by other attacks. We will conclude the article by outlining possible countermeasures to detect and, eventually, counteract it; and possible avenues of research to better understand its impact, especially for authenticated services such as OSNMA, and to characterize it in order to improve the response to similar attacks.

</details>


### [19] [ADCA: Attention-Driven Multi-Party Collusion Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2602.05612)
*Jiayao Wang,Yiping Zhang,Jiale Zhang,Wenliang Yuan,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: ADCA是一种针对联邦自监督学习的注意力驱动多方共谋攻击方法，通过分解全局触发器、恶意客户端共谋和注意力机制动态聚合，显著提高了攻击成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦自监督学习中的后门攻击存在局限性：使用全局统一触发器容易被检测、在聚合过程中被稀释、在异构客户端环境中缺乏鲁棒性。需要设计更隐蔽、更鲁棒的攻击方法。

Method: 提出注意力驱动的多方共谋攻击（ADCA）：1）恶意客户端在本地预训练中分解全局触发器寻找最优本地模式；2）恶意客户端形成恶意联盟并建立协作优化机制；3）使用注意力机制动态聚合模型更新，探索最佳协作策略；4）聚合参数作为联盟内下一轮训练的初始状态。

Result: 在多个联邦自监督学习场景和四个数据集上的实验表明，ADCA在攻击成功率（ASR）和持久性方面显著优于现有方法，证明了其有效性和鲁棒性。

Conclusion: ADCA通过创新的注意力驱动多方共谋机制，有效解决了现有后门攻击在联邦自监督学习中的局限性，实现了更隐蔽、更鲁棒的攻击效果。

Abstract: Federated Self-Supervised Learning (FSSL) integrates the privacy advantages of distributed training with the capability of self-supervised learning to leverage unlabeled data, showing strong potential across applications. However, recent studies have shown that FSSL is also vulnerable to backdoor attacks. Existing attacks are limited by their trigger design, which typically employs a global, uniform trigger that is easily detected, gets diluted during aggregation, and lacks robustness in heterogeneous client environments. To address these challenges, we propose the Attention-Driven multi-party Collusion Attack (ADCA). During local pre-training, malicious clients decompose the global trigger to find optimal local patterns. Subsequently, these malicious clients collude to form a malicious coalition and establish a collaborative optimization mechanism within it. In this mechanism, each submits its model updates, and an attention mechanism dynamically aggregates them to explore the best cooperative strategy. The resulting aggregated parameters serve as the initial state for the next round of training within the coalition, thereby effectively mitigating the dilution of backdoor information by benign updates. Experiments on multiple FSSL scenarios and four datasets show that ADCA significantly outperforms existing methods in Attack Success Rate (ASR) and persistence, proving its effectiveness and robustness.

</details>


### [20] [Time-Complexity Characterization of NIST Lightweight Cryptography Finalists](https://arxiv.org/abs/2602.05641)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.CR

TL;DR: 该论文提出了一个符号模型来分析NIST轻量级密码学项目中10个最终候选算法的时间复杂度，为受限环境下的算法选择提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着数字身份系统和物联网验证等新兴技术的发展，需要在处理能力、内存和能源资源有限的设备上实现强密码保证。虽然NIST的轻量级密码学项目提供了10个最终算法的经验评估，但缺乏对其时间复杂度的统一理论理解。

Method: 引入一个符号模型，将每个方案分解为初始化、数据处理和最终化三个阶段，从而能够为所有10个最终候选算法推导出形式化的时间复杂度。

Result: 结果阐明了设计参数如何在受限的移动和嵌入式环境中影响计算扩展性。该框架为区分算法效率提供了基础。

Conclusion: 该框架为区分算法效率提供了必要的基础，并指导选择能够在受限环境中支持安全系统的密码原语。

Abstract: Lightweight cryptography is becoming essential as emerging technologies in digital identity systems and Internet of Things verification continue to demand strong cryptographic assurance on devices with limited processing power, memory, and energy resources. As these technologies move into routine use, they demand cryptographic primitives that maintain strong security and deliver predictable performance through clear theoretical models of time complexity. Although NIST's lightweight cryptography project provides empirical evaluations of the ten finalist algorithms, a unified theoretical understanding of their time-complexity behavior remains absent. This work introduces a symbolic model that decomposes each scheme into initialization, data-processing, and finalization phases, enabling formal time-complexity derivation for all ten finalists. The results clarify how design parameters shape computational scaling on constrained mobile and embedded environments. The framework provides a foundation needed to distinguish algorithmic efficiency and guides the choice of primitives capable of supporting security systems in constrained environments.

</details>


### [21] [FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation](https://arxiv.org/abs/2602.05838)
*Mayank Kumar,Qian Lou,Paulo Barreto,Martine De Cock,Sikha Pentyala*

Main category: cs.CR

TL;DR: FHAIM是首个基于全同态加密的框架，用于在加密的表格数据上训练基于边际分布的合成数据生成器，解决了隐私保护与数据可用性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和法规限制，许多最有价值的数据（如医疗、教育、金融领域）被锁在数据孤岛中，导致AI在这些关键领域利用不足。现有的合成数据生成服务需要数据持有者信任服务提供商访问私有数据，存在隐私风险。

Method: FHAIM将广泛使用的AIM算法适配到全同态加密环境中，使用新颖的FHE协议，确保私有数据在整个训练过程中保持加密状态，并且仅在满足差分隐私保证的情况下释放数据。

Result: 实证分析表明，FHAIM在保持可行运行时间的同时，能够保留AIM算法的性能，实现了隐私保护与数据可用性的平衡。

Conclusion: FHAIM为在隐私敏感领域安全利用AI提供了有前景的解决方案，通过全同态加密技术实现了在加密数据上训练合成数据生成器，同时提供差分隐私保证。

Abstract: Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.

</details>


### [22] [Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection](https://arxiv.org/abs/2602.05868)
*Ehsan Firouzi,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 论文研究发现静态分析工具在评估LLM生成代码安全性时存在显著偏差，提出基于人类反馈的动态检索增强生成框架


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖静态分析工具评估LLM的代码安全生成和漏洞检测能力，但这些工具的可靠性尚未得到充分验证

Method: 审查1080个LLM生成代码样本，建立人工验证的基准事实，比较CodeQL和Semgrep两种静态安全工具的输出结果

Result: 61%样本实际安全，但Semgrep和CodeQL分别报告60%和80%安全；工具报告与基准事实的匹配率仅为65%和61%，存在显著差异

Conclusion: 静态分析工具不能作为代码安全性的唯一评估标准，需要专家反馈；提出动态检索增强生成框架，持久存储人类反馈供LLM复用

Abstract: Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep's and 61% of CodeQL's reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.

</details>


### [23] [Characterizing and Modeling the GitHub Security Advisories Review Pipeline](https://arxiv.org/abs/2602.06009)
*Claudio Segal,Paulo Segal,Carlos Eduardo de Schuller Banjar,Felipe Paixão,Hudson Silva Borges,Paulo Silveira Neto,Eduardo Santana de Almeida,Joanna C. S. Santos,Anton Kocheturov,Gaurav Kumar Srivastava,Daniel Sadoc Menasché*

Main category: cs.CR

TL;DR: GitHub安全公告（GHSA）已成为开源漏洞披露的核心组件，但只有部分公告经过GitHub审查。本研究通过分析2019-2025年超过28.8万条公告，揭示了审查机制、审查延迟特征，并识别出快速路径（GRA主导）和慢速路径（NVD优先）两种审查延迟模式。


<details>
  <summary>Details</summary>
Motivation: GitHub安全公告（GHSA）在开源漏洞披露中扮演核心角色，但只有部分公告经过GitHub审查，且审查机制不明确。需要理解哪些公告更可能被审查、审查延迟如何，以及审查流程的结构特征。

Method: 对2019-2025年超过288,000条GHSA公告进行大规模实证研究，分析审查模式、量化审查延迟，并识别不同的审查延迟机制。进一步基于公告处理管道的结构开发排队模型来解释这种二分现象。

Result: 研究发现：1）识别了哪些公告更可能被审查；2）量化了审查延迟；3）发现了两种明显的审查延迟机制：快速路径（由GitHub仓库公告主导）和慢速路径（由NVD优先公告主导）；4）开发了能够解释这种二分现象的排队模型。

Conclusion: GHSA审查过程存在明显的二分结构，GitHub仓库公告和NVD优先公告遵循不同的审查延迟机制。这种理解有助于改进开源漏洞披露流程，并为安全工具和开发者提供更好的指导。

Abstract: GitHub Security Advisories (GHSA) have become a central component of open-source vulnerability disclosure and are widely used by developers and security tools. A distinctive feature of GHSA is that only a fraction of advisories are reviewed by GitHub, while the mechanisms associated with this review process remain poorly understood. In this paper, we conduct a large-scale empirical study of GHSA review processes, analyzing over 288,000 advisories spanning 2019--2025. We characterize which advisories are more likely to be reviewed, quantify review delays, and identify two distinct review-latency regimes: a fast path dominated by GitHub Repository Advisories (GRAs) and a slow path dominated by NVD-first advisories. We further develop a queueing model that accounts for this dichotomy based on the structure of the advisory processing pipeline.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 论文批评AI进步的线性模型，提出"熟悉智能"和"陌生智能"概念，认为AI智能更可能是陌生智能，具有超人类和亚人类能力的混合特征，并发展非线性智能模型。


<details>
  <summary>Details</summary>
Motivation: 批评当前流行的AI进步线性模型，指出该模型无法准确描述AI智能的本质特征，特别是AI系统表现出的能力不均衡现象。

Method: 提出并扩展Susan Schneider对线性模型的批评，引入"熟悉智能"和"陌生智能"两个新概念，发展非线性智能模型，将"通用智能"定义为在广泛环境中实现广泛目标的能力。

Result: AI智能更可能是陌生智能，表现出超人类和亚人类能力的混合特征，有时在看似简单的任务中失败，而在某些领域具有超人类洞察力。通用智能不是单一线性量，而是非线性能力组合。

Conclusion: AI智能的陌生性意味着即使最先进的系统也可能在看似明显的任务中失败，这种错误本身不能证明系统缺乏出色的通用智能。同时，在单一任务（如IQ测试）上的优异表现也不能保证在任务领域之外的广泛能力。

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [25] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: LLM在已解决的图论问题上表现良好，能正确理解定义、回忆相关结果并构建有效证明，但在开放问题上仅能提供合理解释和探索策略，无法推进解决，表明其在支持概念探索方面有用但缺乏新颖数学洞察力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被学生用于探索计算机科学高级内容（如图论），需要了解这些工具如何可靠地支持数学严谨性思维。研究旨在评估LLM在已解决和未解决的图论问题上的表现，以指导其在计算教育中的合理使用。

Method: 采用八阶段评估协议，反映真实的数学探究过程，包括解释、探索、策略形成和证明构建。研究测试LLM在两个相关图论问题上的表现：一个已解决的线图优美性问题和一个当前未知解的开放问题。

Result: 在已解决问题上，LLM表现强劲：正确生成定义、识别相关结构、无幻觉地回忆适当结果，并构建了图论专家确认的有效证明。在开放问题上，LLM能生成连贯解释和合理探索策略，但未推进解决方案，且未捏造结果而是承认不确定性。

Conclusion: LLM能够支持已建立材料的探索，但在需要新颖数学洞察或关键结构推理的任务上仍然有限。对于计算教育，这一区别强调了指导学生使用LLM进行概念探索，同时依赖独立验证和严谨论证进行正式问题解决的重要性。

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [26] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: 本文提出LLM智能体不确定性量化新框架，强调从传统问答场景转向交互式智能体环境，提出条件不确定性减少过程的新视角。


<details>
  <summary>Details</summary>
Motivation: 当前不确定性量化研究主要集中于单轮问答场景，而实际LLM智能体应用日益复杂，需要面向交互式智能体的新UQ框架。

Method: 提出首个通用的智能体UQ形式化框架，涵盖现有UQ设置；提出条件不确定性减少过程新视角，强调动作的"交互性"；构建概念框架指导LLM智能体UQ设计。

Result: 揭示先前工作隐含地将LLM UQ视为不确定性累积过程，这在开放世界交互式智能体中失效；提出条件不确定性减少过程能显式建模智能体轨迹中的可减少不确定性。

Conclusion: 智能体UQ对前沿LLM开发和领域应用具有实际意义，但仍存在开放问题需要解决，需要研究范式从问答场景向交互式智能体转变。

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [27] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 提出基于强化学习的自适应碰撞规避框架，用于多碎片主动清除任务中的小型卫星任务规划


<details>
  <summary>Details</summary>
Motivation: 随着地球轨道环境日益拥挤，主动碎片清除任务面临确保安全操作同时降低碰撞风险的挑战，需要更智能的自适应解决方案

Method: 采用掩码近端策略优化算法，集成加油策略、高效任务规划和自适应碰撞规避，优化航天器交会操作

Result: 相比传统启发式方法，该强化学习框架降低了碰撞风险并提高了任务效率，在多种轨道配置和碎片分布场景中表现出鲁棒性和适应性

Conclusion: 为复杂多碎片主动清除任务提供了可扩展解决方案，适用于自主空间任务规划中的其他多目标交会问题

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [28] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 比较三种自主任务规划方法用于低地球轨道多碎片交会问题：固定参数训练的PPO、领域随机化PPO和蒙特卡洛树搜索，评估其在燃料和时间约束变化下的表现


<details>
  <summary>Details</summary>
Motivation: 自主碎片清除任务规划需要在效率、适应性和严格的燃料与任务时间约束之间取得平衡，需要评估不同规划方法在约束条件变化下的表现

Method: 比较三种规划器：1）在固定任务参数下训练的掩码PPO策略；2）在不同任务约束下训练的领域随机化掩码PPO策略；3）蒙特卡洛树搜索基线。在包含加油、真实转移动力学和随机碎片场的高保真轨道模拟中进行300个测试案例评估

Result: 名义PPO在训练匹配条件下表现最佳但在分布偏移时急剧退化；领域随机化PPO在名义性能适度损失下表现出更好的适应性；MCTS在约束变化时处理最好但计算时间高出几个数量级

Conclusion: 学习策略的速度与基于搜索方法的适应性之间存在权衡，结合训练时多样性和在线规划可能是未来弹性ADR任务规划的有前景路径

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [29] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: DemPO框架通过算法抽签构建代表性人类评分者小组，用于AI偏好对齐训练，相比传统便利样本方法能更好地反映多元人口价值观。


<details>
  <summary>Details</summary>
Motivation: 当前基于偏好的AI对齐方法（如RLHF）依赖人类评分者，但这些评分者通常是便利样本，存在人口统计学上的系统性偏差，无法代表全体人口的价值观。

Method: 提出民主偏好优化（DemPO）框架，采用公民大会的算法抽签机制构建代表性评分小组。提供两种训练方案：硬面板（仅使用抽签选出的代表性小组数据）和软面板（保留所有数据但按抽签包含概率重新加权）。

Result: 在包含人类评分者人口统计数据的公开偏好数据集上，使用1B到8B参数的Llama模型进行实验。硬面板在六种聚合方法中始终排名第一，软面板始终优于未加权基线，且随着模型容量增加效果更明显。

Conclusion: 在偏好收集阶段强制执行人口代表性（而非事后校正）能产生更好地反映代表性公众价值观的模型行为，算法抽签是解决AI对齐中代表性偏差的有效方法。

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [30] [CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction](https://arxiv.org/abs/2602.05133)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Alpha Alimamy Kamara,Zhongyi Zhang*

Main category: cs.AI

TL;DR: CAST-CKT是一个用于数据稀缺跨城市交通预测的混沌感知时空知识转移框架，通过混沌分析量化交通可预测性机制，实现机制自适应建模和跨城市知识对齐。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺的跨城市交通预测面临复杂非线性动力学和领域偏移的挑战，现有方法难以捕捉交通固有的混沌特性进行有效的少样本学习。

Method: 提出CAST-CKT框架：1）高效混沌分析器量化交通可预测性机制；2）混沌感知注意力机制实现机制自适应时间建模；3）自适应拓扑学习动态空间依赖；4）基于混沌一致性的跨城市对齐进行知识转移；5）提供具有不确定性量化的特定时间范围预测。

Result: 在四个基准数据集上的跨城市少样本实验中，CAST-CKT在MAE和RMSE指标上显著优于现有最先进方法，同时提供可解释的机制分析。

Conclusion: CAST-CKT通过混沌感知建模有效解决了跨城市少样本交通预测问题，理论分析显示改进的泛化边界，框架具有可解释性和不确定性量化能力。

Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.

</details>


### [31] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG是一个基于因果门控的层次化图RAG框架，通过显式建模因果关系来抑制虚假相关性，实现大规模知识图上的可扩展推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法过度依赖表面节点匹配，缺乏显式因果建模，导致答案不可靠或虚假。先前尝试融入因果关系的方法通常局限于局部或单文档上下文，且受模块化图结构导致的信息隔离问题影响，阻碍了可扩展性和跨模块因果推理。

Method: 提出HugRAG框架，通过跨层次模块的因果门控重新思考基于图的RAG知识组织方式。该框架显式建模因果关系以抑制虚假相关性，同时支持在大规模知识图上进行可扩展推理。

Result: 在多个数据集和评估指标上的广泛实验表明，HugRAG始终优于竞争性的基于图的RAG基线方法。

Conclusion: 该工作为结构化、可扩展且基于因果基础的RAG系统建立了原则性基础。

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [32] [First Proof](https://arxiv.org/abs/2602.05192)
*Mohammed Abouzaid,Andrew J. Blumberg,Martin Hairer,Joe Kileel,Tamara G. Kolda,Paul D. Nelson,Daniel Spielman,Nikhil Srivastava,Rachel Ward,Shmuel Weinberger,Lauren Williams*

Main category: cs.AI

TL;DR: 作者分享了10个在研究过程中自然产生的数学问题，用于评估当前AI系统回答研究级数学问题的能力。这些问题之前未公开，答案暂时加密。


<details>
  <summary>Details</summary>
Motivation: 评估当前人工智能系统回答研究级数学问题的能力，通过提供真实的研究过程中产生的数学问题来测试AI的数学推理水平。

Method: 作者分享了10个在研究过程中自然产生的数学问题，这些问题之前未公开，答案暂时加密，用于测试AI系统的数学问题解决能力。

Result: 论文提出了一个测试集，但未提供具体的测试结果，因为这是一个评估框架的提出，而不是实际的评估报告。

Conclusion: 通过分享研究级数学问题来评估AI系统的数学能力是一个有价值的测试方法，有助于了解当前AI在高级数学推理方面的表现。

Abstract: To assess the ability of current AI systems to correctly answer research-level mathematics questions, we share a set of ten math questions which have arisen naturally in the research process of the authors. The questions had not been shared publicly until now; the answers are known to the authors of the questions but will remain encrypted for a short time.

</details>


### [33] [Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering](https://arxiv.org/abs/2602.05195)
*Fengxian Chen,Zhilong Tao,Jiaxuan Li,Yunlong Li,Qingguo Zhou*

Main category: cs.AI

TL;DR: 本文针对多知识库检索增强生成中的权威性偏差问题，提出DAKS路由预算检索和对齐图证据融合方法，在藏医药领域提升跨知识库问答的溯源性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在藏医药等专业领域，多知识库检索增强生成面临挑战：百科全书条目通常密集易匹配，可能主导检索结果，而经典文献和临床论文等更具权威性的证据反而被忽视。需要解决密度驱动的检索偏差问题，提升跨知识库问答的权威性和可追溯性。

Method: 提出两种互补方法：1) DAKS进行知识库路由和预算检索，缓解密度驱动偏差，在适当时优先考虑权威来源；2) 使用对齐图指导证据融合和覆盖感知打包，改进跨知识库证据覆盖，避免简单拼接。所有答案由轻量级生成器openPangu-Embedded-7B生成。

Result: 实验在包含500个查询的基准测试（截断K=5）上进行，涵盖单知识库和跨知识库问题。结果显示：路由质量和跨知识库证据覆盖持续提升，完整系统在保持强忠实性和引用正确性的同时，实现了最佳CrossEv@5指标。

Conclusion: 提出的DAKS路由预算检索和对齐图证据融合方法有效解决了多知识库检索增强生成中的权威性偏差问题，显著提升了跨知识库问答的溯源能力、减少幻觉，并支持跨知识库验证，在藏医药领域具有实用价值。

Abstract: Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

</details>


### [34] [Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://arxiv.org/abs/2602.05228)
*Guozhi Liu,Weiwei Lin,Tiansheng Huang,Ruichao Mo,Qi Mu,Xiumin Wang,Li Shen*

Main category: cs.AI

TL;DR: 提出Surgery方法，利用注意力汇聚机制中的汇聚散度指标来防御有害微调，通过正则化抑制正汇聚散度，减少模型学习有害模式


<details>
  <summary>Details</summary>
Motivation: 有害微调会破坏大语言模型的安全对齐，带来严重安全风险。现有方法需要大量计算资源或依赖外部数据，需要更高效的防御方法

Method: 提出汇聚散度指标衡量注意力头的安全状态，发现正汇聚散度头数量随模型有害性增加而增加。基于可分离汇聚散度假设，设计Surgery方法，使用汇聚散度抑制正则化器，引导注意力头向负汇聚散度组移动

Result: 在BeaverTails、HarmBench和SorryBench基准测试上，Surgery分别提升防御性能5.90%、11.25%和9.55%，有效减少模型学习有害模式的倾向

Conclusion: Surgery方法通过汇聚散度机制有效防御有害微调，提供了一种计算高效、无需外部数据的微调阶段防御方案，显著提升大语言模型的安全性

Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \emph{sink divergence} for each attention head and observe that \emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\%, 11.25\%, and 9.55\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.

</details>


### [35] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: 提出TEA方法，通过动态原位任务生成在未见3D环境中评估智能体，包含交互和演化两阶段，自动生成大量物理合理任务，揭示现有模型在基本感知和3D交互方面存在严重缺陷


<details>
  <summary>Details</summary>
Motivation: 随着通用智能体即将在多样化家庭环境中广泛部署，针对每个独特未见3D环境的评估变得至关重要。现有基准存在严重的数据污染问题，缺乏场景特异性，不足以评估智能体在未见环境中的能力。

Method: 提出TEA（面向具身智能体的两阶段交互-演化任务生成系统）：1）交互阶段：智能体主动与环境交互，在任务执行和生成之间形成循环，实现连续任务生成；2）演化阶段：通过任务图建模，重新组合和重用现有任务生成新任务，无需外部数据。使用结构化图表示定义任务。

Result: 在10个未见场景中，TEA在两个周期内自动生成了87,876个任务，经人工验证这些任务物理合理且涵盖基本日常认知能力。将SOTA模型与人类在生成任务上对比发现：模型虽然在公共基准上表现优异，但在基本感知任务上表现惊人地差，严重缺乏3D交互意识，且对任务类型的推理高度敏感。

Conclusion: 这些令人警醒的发现强调了在将智能体部署到真实世界人类环境之前进行原位评估的必要性。TEA方法为未见3D环境中的智能体评估提供了有效解决方案。

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [36] [Beyond Cosine Similarity](https://arxiv.org/abs/2602.05266)
*Xinbo Ai*

Main category: cs.AI

TL;DR: 本文提出了一种新的语义相似度度量方法recos，它通过推导比经典柯西-施瓦茨不等式更紧的上界，用排序向量分量对点积进行归一化，从而超越了传统余弦相似度的线性限制。


<details>
  <summary>Details</summary>
Motivation: 余弦相似度基于柯西-施瓦茨不等式，只能捕捉线性关系，无法建模现实世界语义空间的复杂非线性结构。需要一种更强大的相似度度量方法来克服这一限制。

Method: 推导了比经典柯西-施瓦茨界更紧的点积上界，基于此提出了recos相似度度量方法。该方法通过排序向量分量对点积进行归一化，将完美相似度的条件从严格线性依赖放宽为序数一致性。

Result: 在11种嵌入模型（包括静态、上下文化和通用类型）上的广泛实验表明，recos在标准语义文本相似度基准测试中始终优于传统余弦相似度，与人类判断的相关性更高。

Conclusion: recos作为一种数学原理严谨且经验上优越的替代方案，为复杂嵌入空间中的语义分析提供了更高的准确性。

Abstract: Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.

</details>


### [37] [Hallucination-Resistant Security Planning with a Large Language Model](https://arxiv.org/abs/2602.05279)
*Kim Hammar,Tansu Alpcan,Emil Lupu*

Main category: cs.AI

TL;DR: 本文提出了一种将LLM集成到安全管理决策支持中的框架，通过一致性检查和数字孪生反馈来控制幻觉风险，在事件响应场景中相比前沿LLM将恢复时间减少达30%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在安全管理任务（如事件响应规划）中具有潜力，但其不可靠性和幻觉倾向仍然是重大挑战。需要一种原则性框架来解决这些问题。

Method: 提出一个将LLM集成到迭代循环中的框架：LLM生成候选行动，检查其与系统约束和前瞻预测的一致性；当一致性低时，通过数字孪生等外部反馈收集信息，然后通过上下文学习（ICL）细化候选行动。

Result: 证明了该设计可以通过调整一致性阈值来控制幻觉风险，并在某些假设下建立了ICL的遗憾边界。在四个公共数据集上的实验表明，该框架相比前沿LLM将恢复时间减少达30%。

Conclusion: 提出的框架为在安全管理中使用LLM作为决策支持提供了一种原则性方法，通过一致性检查和外部反馈有效控制了幻觉风险，显著提高了事件响应效率。

Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.

</details>


### [38] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: GPT-5在PieArena谈判基准测试中表现达到或超过经过专业培训的商学院学生，展示了前沿语言模型在高风险经济场景中的潜力，但鲁棒性和可信赖性仍需改进。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在谈判这一核心商业任务中的能力，谈判需要战略推理、心智理论和经济价值创造能力。现有基准测试主要关注交易结果，缺乏对谈判行为多维度特征的全面评估。

Method: 引入PieArena大规模谈判基准，基于精英商学院MBA谈判课程中的真实场景构建多智能体交互环境。评估GPT-5等模型与经过一学期谈判培训和针对性指导的商学院学生的表现对比，并研究联合意向性智能体脚手架的影响。

Result: GPT-5在谈判表现上达到或超过经过专业培训的商学院学生。联合意向性脚手架对中低层模型有显著提升，但对前沿模型收益递减。PieArena揭示了模型在欺骗、计算准确性、指令遵从性和感知声誉等方面的异质性，这些在仅关注交易结果的基准测试中被掩盖。

Conclusion: 前沿语言智能体在智力和心理层面已具备在高风险经济环境中部署的能力，但在鲁棒性和可信赖性方面仍存在不足，需要进一步研究和改进。

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [39] [ProAct: Agentic Lookahead in Interactive Environments](https://arxiv.org/abs/2602.05327)
*Yangbin Yu,Mingyu Yang,Junyou Li,Yiming Gao,Feiyu Liu,Yijun Yang,Zichuan Lin,Jiafei Lyu,Yicheng Liu,Zhicong Lu,Deheng Ye,Jie Jiang*

Main category: cs.AI

TL;DR: ProAct框架通过两阶段训练让LLM智能体学习前瞻推理：第一阶段使用GLAD进行监督微调，将复杂搜索树压缩为因果推理链；第二阶段使用MC-Critic增强策略梯度算法，通过轻量环境模拟校准价值估计，显著提升长期规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型智能体在需要长期规划的交互环境中表现不佳，主要原因是模拟未来状态时会产生复合错误。需要一种方法让智能体能够内化准确的前瞻推理能力，同时避免推理时的高计算成本搜索。

Method: 提出ProAct框架，包含两个阶段：1) Grounded LookAhead Distillation (GLAD)：通过基于环境的搜索轨迹进行监督微调，将复杂搜索树压缩为简洁的因果推理链；2) Monte-Carlo Critic (MC-Critic)：轻量级环境模拟的价值估计器，增强PPO和GRPO等策略梯度算法，提供低方差信号稳定策略优化。

Result: 在随机环境（如2048）和确定性环境（如Sokoban）上的实验表明，ProAct显著提高了规划准确性。4B参数模型使用ProAct训练后，超越了所有开源基线，并与最先进的闭源模型相媲美，同时在未见环境中表现出强大的泛化能力。

Conclusion: ProAct框架通过将环境搜索知识蒸馏到模型中，并利用轻量级模拟增强策略优化，使LLM智能体能够内化准确的前瞻推理能力，在长期规划任务中实现显著性能提升，同时保持计算效率。

Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct

</details>


### [40] [RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs](https://arxiv.org/abs/2602.05367)
*Youngcheon You,Banseok Lee,Minseop Choi,Seonyoung Kim,Hyochan Chong,Changdong Kim,Youngmin Kim,Dongkyu Kim*

Main category: cs.AI

TL;DR: RaBiT提出了一种新颖的残差二值化量化框架，通过算法强制残差层次结构解决并行二值路径间的特征共适应问题，在保持硬件友好的矩阵乘法自由推理的同时，重新定义了2位精度-效率边界。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的高效部署需要极端量化，但低比特效率与性能之间存在关键权衡。残差二值化虽然能实现硬件友好的无矩阵乘法推理，但存在病态特征共适应问题，特别是并行残差二值路径学习冗余特征，这限制了模型的表达能力。

Method: 提出RaBiT量化框架，核心机制是从单个共享全精度权重顺序推导每个二值路径，确保每个路径纠正前一个路径的误差。通过算法强制残差层次结构解决共适应问题，并采用优先功能保持而非单纯权重近似的鲁棒初始化来稳定训练过程。

Result: RaBiT重新定义了2位精度-效率边界：达到最先进的性能，甚至能与硬件密集的向量量化方法相媲美，在RTX 4090上相比全精度模型实现了4.49倍的推理加速。

Conclusion: RaBiT通过解决残差二值化中的特征共适应问题，提供了一种高效的量化框架，在保持硬件友好推理的同时显著提升了极端量化下的模型性能，为大语言模型的高效部署提供了新的解决方案。

Abstract: Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\times$ inference speed-up over full-precision models on an RTX 4090.

</details>


### [41] [Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation](https://arxiv.org/abs/2602.05381)
*Ting Fang Tan,Kabilan Elangovan,Andreas Pollreisz,Kevin Bryan Dy,Wei Yan Ng,Joy Le Yi Wong,Jin Liyuan,Chrystie Quek Wan Ning,Ashley Shuen Ying Hong,Arun James Thirunavukarasu,Shelley Yin-His Chang,Jie Yao,Dylan Hong,Wang Zhaoran,Amrita Gupta,Daniel SW Ting*

Main category: cs.AI

TL;DR: 本研究评估了四个小型医学LLM在眼科患者咨询回答中的表现，发现Meerkat-7B表现最佳，MedLLaMA3-v20表现最差且存在幻觉问题，GPT-4-Turbo评分与临床医生评估高度一致。


<details>
  <summary>Details</summary>
Motivation: 随着领域特定大语言模型在眼科患者教育、分诊和临床决策支持中的应用日益增多，为确保安全性和准确性，需要进行严格的评估。本研究旨在评估小型医学LLM在回答眼科患者咨询方面的表现，并探索基于LLM的评估方法的可行性。

Method: 采用横断面研究设计，选取四个参数规模小于100亿的小型医学LLM（Meerkat-7B、BioMistral-7B、OpenBioLLM-8B、MedLLaMA3-v20），让每个模型回答180个眼科患者咨询问题，共生成2160个回答。由三位不同资历的眼科医生和GPT-4-Turbo使用S.C.O.R.E.框架（评估安全性、共识与上下文、客观性、可重复性和可解释性）进行评分，采用五点李克特量表。使用Spearman等级相关、Kendall tau统计和核密度估计分析评估LLM评分与临床医生评分的一致性。

Result: Meerkat-7B表现最佳，分别获得高级顾问3.44分、顾问4.08分和住院医师4.18分的平均分。MedLLaMA3-v20表现最差，25.5%的回答包含幻觉或临床误导性内容，包括编造术语。GPT-4-Turbo评分与临床医生评估整体高度一致（Spearman rho=0.80，Kendall tau=0.67），但高级顾问评分更为保守。

Conclusion: 医学LLM在眼科问题回答方面显示出安全潜力，但在临床深度和共识方面仍存在差距。研究支持基于LLM的大规模基准评估的可行性，并表明需要结合自动化和临床医生审查的混合框架来指导安全的临床部署。

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

</details>


### [42] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: H-AdminSim：一个结合真实数据生成和多智能体模拟的医院行政工作流端到端仿真框架，用于系统评估LLM驱动的行政自动化


<details>
  <summary>Details</summary>
Motivation: 医院行政部门处理大量运营任务（大型医院每天处理超1万条请求），现有研究主要关注医患交互或孤立的行政子任务，未能捕捉真实行政工作流的复杂性

Method: 提出H-AdminSim框架，结合真实数据生成和多智能体模拟医院行政工作流，通过详细评分标准进行定量评估，并通过FHIR集成提供统一可互操作环境

Result: 该框架为评估LLM驱动行政自动化的可行性和性能提供了标准化测试平台，能够系统比较不同LLM的表现

Conclusion: H-AdminSim填补了现有研究的空白，为医院行政工作流的复杂自动化评估提供了全面、可互操作的仿真环境

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [43] [THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs](https://arxiv.org/abs/2602.05424)
*Weijian Yu,Yuhuan Lu,Dingqi Yang*

Main category: cs.AI

TL;DR: THOR是一种用于超关系知识图谱的归纳式链接预测方法，通过构建关系基础图和实体基础图来建模跨图谱的结构不变性，支持完全归纳推理。


<details>
  <summary>Details</summary>
Motivation: 现有超关系知识图谱的链接预测方法主要关注传导式设置，只能在同一词汇表内进行预测，缺乏对未见词汇表的泛化能力，限制了方法的通用性。

Method: 提出THOR方法：1) 构建关系基础图和实体基础图，建模超关系知识图谱中事实间的基本交互模式；2) 使用两个并行图编码器学习基础图表示，后接Transformer解码器，支持高效掩码训练和完全归纳推理。

Result: 在12个数据集上的评估显示，THOR显著优于现有方法：比最佳规则方法提升66.1%，比最佳半归纳方法提升55.9%，比最佳完全归纳方法提升20.4%。消融研究验证了捕捉跨图谱结构不变性的关键设计因素。

Conclusion: THOR通过建模超关系知识图谱中的基础交互模式，实现了有效的归纳式链接预测，显著提升了方法对未见词汇表的泛化能力。

Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.

</details>


### [44] [Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy](https://arxiv.org/abs/2602.05430)
*Kritchanat Ponyuenyong,Pengyu Tu,Jia Wei Tan,Wei Soon Cheong,Jamie Ng Suat Ling,Lianlian Jiang*

Main category: cs.AI

TL;DR: 该论文提出了一种尖峰正则化策略，并评估了多种时间序列基础模型在波动性电力市场中的日前电价预测性能，结果显示TSFMs相比传统方法在MAPE指标上最高提升37.4%。


<details>
  <summary>Details</summary>
Motivation: 电力价格预测对能源市场参与者至关重要，但由于价格信号固有的波动性和非线性，传统统计和深度学习模型难以有效捕捉复杂的时间依赖关系并整合异构数据。虽然时间序列基础模型在一般时间序列预测任务中表现良好，但它们在波动性市场中的日前电价预测效果尚未得到充分探索。

Method: 提出尖峰正则化策略，评估多种时间序列基础模型（包括Tiny Time Mixers、MOIRAI、MOMENT、TimesFM），并与传统统计模型（ARIMA）和深度学习模型（LSTM、CNN-LSTM）进行比较。使用新加坡具有波动趋势的半小时批发市场数据，并在适用模型中纳入外生因素（如天气和日历变量）。

Result: 时间序列基础模型在所有评估设置中始终优于传统方法，在MAPE指标上最高实现了37.4%的改进。这表明TSFMs在波动性电力市场中具有显著优势。

Conclusion: 研究结果为提高波动性电力市场中的预测准确性和决策制定提供了实用指导，证明了时间序列基础模型在日前电价预测任务中的优越性能。

Abstract: Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.

</details>


### [45] [Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning](https://arxiv.org/abs/2602.05464)
*Jiaquan Wang,Yan Lyu,Chen Li,Yuheng Jia*

Main category: cs.AI

TL;DR: OD-CRL框架通过自适应正交基优化和零空间去噪投影，解决了条件表示学习中基向量敏感性和子空间干扰问题，在多种定制化任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有条件表示学习方法存在两个关键限制：对子空间基向量的敏感性，以及子空间间干扰的脆弱性。这些限制影响了条件特征提取的质量和鲁棒性。

Method: 提出OD-CRL框架，包含自适应正交基优化（AOBO）和零空间去噪投影（NSDP）。AOBO通过奇异值分解和基于曲率的截断构建正交语义基；NSDP通过将嵌入投影到无关子空间的零空间来抑制非目标语义干扰。

Result: 在定制化聚类、分类和检索任务上的大量实验表明，OD-CRL实现了新的最先进性能，并具有优越的泛化能力。

Conclusion: OD-CRL通过正交基优化和零空间投影有效解决了条件表示学习中的基向量敏感性和子空间干扰问题，为定制化任务提供了更鲁棒和有效的解决方案。

Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.

</details>


### [46] [ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation](https://arxiv.org/abs/2602.05472)
*Yiwen Duan,Jing Ye,Xinpei Zhao*

Main category: cs.AI

TL;DR: ALIVE框架通过对抗学习和指导性语言反馈，让LLM内部化推理逻辑，解决传统强化学习依赖标量奖励的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖的标量奖励存在三个主要问题：扩展成本高、跨领域脆弱、无法理解解决方案的内在逻辑。这种对外部贫乏信号的依赖阻碍了模型发展对推理原则的深层、自包含理解。

Method: ALIVE框架基于"认知协同"原则，将问题提出、解决和评判统一在单一策略模型中，通过对抗学习和指导性语言反馈，让模型直接从原始语料中内部化评估标准，将外部批评转化为内生的推理能力。

Result: 在数学推理、代码生成和一般逻辑推理基准测试中，ALIVE持续缓解了奖励信号限制。在相同数据和计算条件下，实现了准确率提升、跨领域泛化能力显著改善以及更高的自我纠正率。

Conclusion: 推理三位一体（问题提出、解决、评判）促进了能力增长的自我维持轨迹，使ALIVE成为无需人工监督的通用推理对齐的可扩展基础。

Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.

</details>


### [47] [Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction](https://arxiv.org/abs/2602.05479)
*Zhe Wang,Zijing Liu,Chencheng Xu,Yuan Yao*

Main category: cs.AI

TL;DR: Phi-former是一种用于预测化合物-蛋白质相互作用的分层表示学习方法，通过原子-原子、基序-基序和原子-基序三个层次建模分子相互作用，提高了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽然能在原子层面建模化合物-蛋白质相互作用，但忽略了分子片段（基序或功能基团）作为生物识别和结合主要单元的现实化学原理，导致模型与化学现实不完全一致。

Method: 提出Phi-former方法：1）对化合物和蛋白质进行分层表示；2）采用成对预训练框架，系统建模原子-原子、基序-基序、原子-基序三个层次的相互作用；3）设计层内和层间学习管道，使不同相互作用层次相互促进。

Result: Phi-former在CPI相关任务上表现出优越性能。案例研究表明，该方法能准确识别在CPI中被激活的特定原子或基序，提供可解释的模型解释。

Conclusion: Phi-former通过整合基序在CPI中的生物学作用，提供了更符合化学现实的化合物-蛋白质相互作用预测方法，其可解释性洞察有助于指导理性药物设计和精准医疗应用。

Abstract: Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.

</details>


### [48] [SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration](https://arxiv.org/abs/2602.05499)
*Hanyu Wei,Zunhai Su,Peng Lu,Chao Li,Spandan Tiwari,Ashish Sirasao,Yuhan Dong*

Main category: cs.AI

TL;DR: SDFP是一种无需训练、即插即用的推测解码框架，通过基于Fisher信息迹的层剪枝从现有LLM构建轻量级草稿模型，实现1.32x-1.5x的解码加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法需要额外的训练、调优或维护专门的草稿模型，部署成本高且复杂。需要一种无需训练、即插即用的轻量级草稿模型构建方法，以降低延迟并支持多媒体应用。

Method: 提出SDFP框架，基于Fisher信息迹评估各层对输出扰动的影响，剪除低影响层，从原始LLM构建紧凑的草稿模型。该方法无需额外训练、超参数调优或单独维护草稿模型。

Result: 在基准测试中，SDFP实现了1.32倍到1.5倍的解码加速，且不改变目标模型的输出分布，支持低延迟多媒体应用。

Conclusion: SDFP提供了一种部署友好、无需训练的草稿模型构建方法，有效降低LLM解码延迟，适用于需要实时交互的多媒体应用场景。

Abstract: Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.

</details>


### [49] [A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma](https://arxiv.org/abs/2602.05515)
*Ajo Babu George,Anna Mariam John,Athul Anoop,Balu Bhasuran*

Main category: cs.AI

TL;DR: 研究人员创建了一个专门针对成釉细胞瘤的多模态数据集，并开发了多模态深度学习模型，用于变体分类、复发风险评估和手术规划支持，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助颌面病理诊断缺乏结构化、高质量的多模态数据集，现有资源对成釉细胞瘤覆盖有限且格式不一致，无法直接用于模型训练。

Method: 创建了专门针对成釉细胞瘤的多模态数据集，整合了标注的放射学、组织病理学和口腔临床图像，以及从病例报告中提取的结构化数据。使用自然语言处理技术从文本报告中提取临床相关特征，对图像数据进行领域特定的预处理和增强。基于该数据集开发了多模态深度学习模型，可接受临床输入（如主诉、年龄、性别）以增强个性化推理。

Result: 定量评估显示显著改进：变体分类准确率从46.2%提升至65.9%，异常组织检测F1分数从43.0%提升至90.3%。与MultiCaRe等资源相比，该工作通过提供稳健的数据集和适应性强的多模态AI框架，推进了患者特异性决策支持。

Conclusion: 本研究提供了一个专门针对成釉细胞瘤的多模态数据集和AI框架，显著提升了诊断性能，为患者特异性决策支持提供了有力工具。

Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.

</details>


### [50] [Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities](https://arxiv.org/abs/2602.05532)
*Florian Dietz,William Wale,Oscar Gilg,Robert McCarthy,Felix Michalak,Gustavo Ewbank Rodrigues Danon,Miguelito de Guzman,Dietrich Klakow*

Main category: cs.AI

TL;DR: SPT方法通过在LoRA参数中训练"诚实人格"来检测大语言模型中的错位行为，当激活该人格时能审查主模型的响应并访问其潜在状态，在Anthropic审计游戏基准测试中达到96%准确率。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型中的错位行为具有挑战性，因为模型可能在训练中学会隐藏不当行为。标准审计技术存在不足：黑盒方法难以区分错位输出与良性输出，而机制可解释性方法无法随模型能力扩展。

Method: 引入分裂人格训练(SPT)，在LoRA参数中微调第二个"诚实人格"，该人格在正常操作中保持不活跃。主模型响应后，激活LoRA适配器并插入触发字符串，使诚实人格能够审查响应同时访问主模型的潜在状态。

Result: 在Anthropic审计游戏模型生物基准测试中，SPT达到96%的整体准确率，而Anthropic报告接近0%准确率。诚实人格揭示了外部观察者无法访问的潜在知识，如受损模型训练时所使用的虚构偏见。

Conclusion: SPT提供了一种有效检测大语言模型错位行为的方法，通过训练辅助诚实人格来审查主模型响应，能够揭示隐藏的错位行为，在基准测试中表现出色。

Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

</details>


### [51] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: 研究探讨大语言模型能否在个体层面预测认知偏差，并在交互对话情境中模拟人类决策偏差行为，发现LLMs能精确复现人类偏差


<details>
  <summary>Details</summary>
Motivation: 虽然已知大语言模型能复现常见认知偏差，但更关键的问题是LLMs能否在个体层面预测偏差，并在认知负荷等情境因素与偏差交互时模拟人类决策的动态行为

Method: 将三个经典决策场景改编为对话设置，进行人类实验(N=1100)，参与者与促进决策的聊天机器人互动；使用参与者人口统计数据和对话记录，基于GPT-4和GPT-5模拟类似交互条件

Result: 人类实验显示显著的认知偏差；LLMs能精确复现人类偏差，不同模型在模拟人类行为对齐方面存在明显差异

Conclusion: LLMs能够有效模拟人类决策偏差，这对设计和评估交互情境中适应性、偏差感知的LLM系统具有重要意义

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [52] [BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages](https://arxiv.org/abs/2602.05599)
*Subhadip Maji,Arnab Bhattacharya*

Main category: cs.AI

TL;DR: GETR方法通过图增强标记表示实现跨语言知识迁移，在低资源语言任务上显著超越现有基线方法


<details>
  <summary>Details</summary>
Motivation: 低资源语言由于数据稀缺和语言资源不足，其NLP系统性能远落后于高资源语言，需要有效的跨语言知识迁移方法来解决这一挑战

Method: 提出GETR（图增强标记表示）方法用于跨语言知识迁移，同时采用两种基线方法：隐藏层增强和通过标记翻译的标记嵌入迁移

Result: GNN方法显著优于现有多语言和跨语言基线，在真正低资源语言（Mizo、Khasi）的词性标注任务上提升13个百分点，在模拟低资源语言（Marathi、Bangla、Malayalam）的情感分类和NER任务上分别提升20和27个百分点的宏F1值

Conclusion: GETR方法在低资源语言任务上表现出色，论文还详细分析了迁移机制并确定了在此语言背景下成功知识迁移的关键因素

Abstract: Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.

</details>


### [53] [Reactive Knowledge Representation and Asynchronous Reasoning](https://arxiv.org/abs/2602.05625)
*Simon Kohaut,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.AI

TL;DR: 提出Resin概率编程语言和Reactive Circuits结构，通过异步推理和基于信号变化频率的计算分区，实现动态环境中实时概率推理的指数级加速


<details>
  <summary>Details</summary>
Motivation: 复杂概率模型中的精确推理计算成本过高，特别是在动态环境中需要频繁实时更新的自主智能体场景。现有方法效率低下，每次变化都重新评估整个模型，未能利用现实世界信息流具有不同更新频率的特性。

Method: 1. 提出Resin（Reactive Signal Inference）概率编程语言，将概率逻辑与反应式编程结合；2. 提出Reactive Circuits（RCs）作为Resin的高效精确语义，基于代数电路和异步数据流构建，形成时间动态有向无环图，能根据输入信号波动性自主适应；3. 基于异步输入变化频率估计对计算进行分区，将大型推理任务分解为可单独记忆的子问题。

Result: 在高保真无人机群模拟中，相比频率无关的推理方法实现了几个数量级的加速。RCs的结构适应成功捕捉环境动态，显著降低延迟并促进反应式实时推理。通过基于变化频率的计算分区，确保只有受新信息影响的模型特定组件被重新评估，大幅减少流式上下文中的冗余计算。

Conclusion: 通过结合概率逻辑与反应式编程，以及基于信号变化频率的自适应计算分区，能够实现动态环境中高效、精确的实时概率推理，为自主智能体在复杂环境中的决策提供可行解决方案。

Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.

</details>


### [54] [Generative Ontology: When Structured Knowledge Learns to Create](https://arxiv.org/abs/2602.05636)
*Benny Cheung*

Main category: cs.AI

TL;DR: Generative Ontology框架结合传统本体论的结构化优势和LLM的创造力，通过可执行的Pydantic模式约束LLM生成，应用于游戏设计等领域。


<details>
  <summary>Details</summary>
Motivation: 传统本体论擅长描述领域结构但无法生成新内容，而大型语言模型能流畅生成但缺乏结构有效性，经常产生幻觉输出。需要结合两者的互补优势。

Method: 将领域知识编码为可执行的Pydantic模式，通过DSPy签名约束LLM生成。采用多智能体管道，为不同本体领域分配专门角色（如机制架构师、主题编织者、平衡批评家），每个智能体带有防止浅层输出的"专业焦虑"。使用检索增强生成和迭代验证确保机制与组件的一致性。

Result: 通过GameGrammar系统演示，给定主题提示（如"洞穴生态系统中发光的真菌竞争"），管道能生成结构完整、可玩的桌面游戏规范，包括机制、组件、胜利条件和设置说明，既满足本体约束又保持创造性。

Conclusion: 该模式可推广到游戏以外的领域，任何具有专业词汇、有效性约束和积累范例的领域（音乐创作、软件架构、烹饪艺术）都适合使用Generative Ontology。约束不是限制创造力，而是使其成为可能。

Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.

</details>


### [55] [Graph-based Agent Memory: Taxonomy, Techniques, and Applications](https://arxiv.org/abs/2602.05665)
*Chang Yang,Chuang Zhou,Yilin Xiao,Su Dong,Luyao Zhuang,Yujing Zhang,Zhu Wang,Zijin Hong,Zheng Yuan,Zhishang Xiang,Shengyuan Chen,Huachi Zhou,Qinggang Zhang,Ninghao Liu,Jinsong Su,Xinrun Wang,Yi Chang,Xiao Huang*

Main category: cs.AI

TL;DR: 本文综述了基于图的智能体记忆系统，分析了记忆分类、生命周期关键技术、开源资源及应用，旨在推动更高效可靠的图记忆系统发展。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在复杂长程任务中的应用，记忆成为智能体的核心模块。图结构因其建模关系依赖、组织层次信息和高效检索的内在能力，成为构建智能体记忆的有力工具。本文旨在系统梳理基于图的智能体记忆研究现状，为社区提供可操作的见解。

Method: 采用综述研究方法：1）提出智能体记忆的分类体系；2）按照记忆生命周期系统分析图记忆关键技术；3）总结开源库和基准测试；4）探索应用场景；5）识别挑战和未来方向。

Result: 建立了完整的图记忆分类体系，系统分析了记忆提取、存储、检索和演化的关键技术，收集整理了相关研究论文、开源数据和项目资源，为社区提供了全面的研究参考。

Conclusion: 图结构为智能体记忆提供了强大的组织框架，能够有效支持知识积累、迭代推理和自我演化。未来需要进一步解决记忆效率、可靠性等关键挑战，推动图记忆系统的发展。

Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.

</details>


### [56] [Determining Energy Efficiency Sweet Spots in Production LLM Inference](https://arxiv.org/abs/2602.05695)
*Hiari Pizzini Cavagna,Andrea Proia,Giacomo Madella,Giovanni B. Esposito,Francesco Antici,Daniele Cesarini,Zeynep Kiziltan,Andrea Bartolini*

Main category: cs.AI

TL;DR: 本文提出了一个基于Transformer架构计算和内存访问复杂度的分析模型，能够准确预测LLM推理的能耗曲线，发现存在能效"甜点区"，合理调整序列长度可显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: LLM推理在现代AI应用中至关重要，但现有能耗估计方法通常基于输入输出序列长度的简单线性函数，而实际观测显示能效存在明显的非线性依赖关系，需要更准确的能耗模型来指导节能优化。

Method: 从Transformer架构的计算和内存访问复杂度推导出分析模型，使用TensorRT-LLM在NVIDIA H100 GPU上评估多种LLM（1B-9B参数），包括OPT、LLaMA、Gemma、Falcon、Qwen2和Granite，测试范围从64到4096个token。

Result: 模型平均MAPE为1.79%，发现能效存在"甜点区"：中等长度输入和中等长度输出时能效最高，而长输入或极短输出时能效显著下降。通过将序列长度与这些"甜点区"对齐，可以大幅降低能耗。

Conclusion: 提出的分析模型能够准确预测LLM推理能耗，揭示了能效的非线性特性，为生产系统中的截断、摘要和自适应生成策略提供了理论依据，有助于实现更节能的LLM部署。

Abstract: Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency "Sweet Spots" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.

</details>


### [57] [Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions](https://arxiv.org/abs/2602.05709)
*Yihao Ouyang,Shiwei Li,Haozhao Wang,Xiandi Luo,Zhuoqi Hu,Yuetong Song,Qiyu Qin,Yichen Li,Ruixuan Li*

Main category: cs.AI

TL;DR: GenLoRA提出用非线性函数生成低秩矩阵的基础向量，替代显式存储，实现更高的参数效率


<details>
  <summary>Details</summary>
Motivation: 标准LoRA采用显式秩范式，增加模型容量需要添加更多行或列（基础向量），导致参数大幅增长。研究发现这些基础向量存在显著参数冗余，可以用轻量级非线性函数紧凑表示。

Method: 提出生成式低秩适配器（GenLoRA），用非线性基础向量生成替代显式基础向量存储。具体为每个低秩矩阵维护一个潜在向量，并使用一组轻量级径向基函数（RBFs）合成基础向量。每个RBF需要的参数远少于显式基础向量。

Result: 在多个数据集和架构上的广泛实验表明，GenLoRA在较小参数预算下获得更高的有效LoRA秩，从而实现更优的微调性能。

Conclusion: GenLoRA通过用非线性函数生成基础向量替代显式存储，显著提高了LoRA的参数效率，在有限参数预算下实现更好的微调效果。

Abstract: Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.

</details>


### [58] [Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification](https://arxiv.org/abs/2602.05717)
*Tianyi Wang,Long Li,Hongcan Guo,Yibiao Chen,Yixia Li,Yong Wang,Yun Chen,Guanhua Chen*

Main category: cs.AI

TL;DR: APO提出了一种新的强化学习优化方法，通过支持覆盖而非形状匹配来解决递归空间收缩问题，在保持准确性的同时恢复多样性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在递归空间收缩（RSC）的系统性病理问题，即采样概率的不可逆崩溃。KL正则化虽然试图缓解此问题，但强加了刚性的形状匹配约束，导致与正确性所需的锐化产生梯度冲突。

Method: 提出锚定策略优化（APO），将范式从全局形状匹配转向支持覆盖。通过基于参考模型高置信度支持定义安全流形，APO允许激进锐化以提高效率，同时在错误修正时选择性调用恢复力以防止崩溃。

Result: 在数学基准测试中，APO打破了准确性与多样性的权衡，显著提高了Pass@1指标，同时恢复了标准策略梯度方法通常丢失的Pass@K多样性。

Conclusion: APO作为一种梯度对齐机制，通过最大化支持覆盖实现弹性恢复，有效解决了RLVR中的递归空间收缩问题，为强化学习优化提供了新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.

</details>


### [59] [Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification](https://arxiv.org/abs/2602.05723)
*Taoye Yin,Haoyuan Hu,Yaxin Fan,Xinhao Chen,Xinya Wu,Kai Deng,Kezun Zhang,Feng Wang*

Main category: cs.AI

TL;DR: 提出RLFKV框架，通过细粒度知识验证和强化学习减少金融RAG系统中的幻觉问题，提高回答与检索文档的一致性


<details>
  <summary>Details</summary>
Motivation: 金融RAG系统虽然依赖检索文档来生成准确回答，但由于金融领域的时间敏感性，模型生成的回答仍然存在与检索信息矛盾的幻觉问题，需要解决这种不一致性

Method: 提出强化学习框架RLFKV，将金融回答分解为原子知识单元，评估每个单元的正确性来计算细粒度忠实度奖励；同时引入信息丰富度奖励防止奖励黑客攻击，鼓励策略模型保留至少与基础模型相同数量的知识单元

Result: 在公开的FDD任务和新提出的FDD-ANT数据集上的实验显示了一致的改进，证实了该方法的有效性

Conclusion: RLFKV框架通过细粒度知识验证和强化学习，有效减少了金融RAG系统中的幻觉问题，提高了回答与检索文档的一致性

Abstract: In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.

</details>


### [60] [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765)
*Zhong Guan,Haoran Sun,Yongjian Guo,Shuai Di,Xiaodong Bai,Jing Long,Tianyun Zhao,Mingxi Luo,Chen Zhou,Yucheng Guo,Qiming Yang,Wanting Xu,Wen Huang,Yunxuan Ma,Hongke Zhao,Likang Wu,Xiaotie Deng,Xi Xiao,Sheng Wen,Yicheng Gong,Junwu Xiong*

Main category: cs.AI

TL;DR: 本文提出首个完全异步的VLA模型训练框架，通过环境交互、策略生成和模型更新的全流程异步化，显著提升训练效率，在LIBERO基准上实现最高126.67%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型训练框架（如RLinf）虽然能提升模型泛化能力，但仍采用同步执行方式，导致环境交互、策略生成和模型更新阶段的资源利用率低下和吞吐量受限，成为训练效率的关键瓶颈。

Method: 提出完全异步策略训练框架，采用多层次解耦架构：1）环境交互和轨迹收集的异步并行化；2）策略生成的流式执行；3）训练更新的解耦调度。从大模型RL的异步优化思想中系统汲取灵感。

Result: 在LIBERO基准上，相比现有同步策略，吞吐量提升最高达59.25%；深度优化分离策略后，吞吐量提升最高达126.67%。消融实验验证了各异步组件的有效性，8-256 GPU的扩展性验证显示在多数条件下具有优秀可扩展性。

Conclusion: 首次实现从环境交互、轨迹收集到策略更新的全流程异步训练框架，显著提升VLA模型训练效率，为通用具身智能的发展提供了高效的训练解决方案。

Abstract: In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.

</details>


### [61] [FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem](https://arxiv.org/abs/2602.05794)
*Aboli Kathar,Aman Kumar,Anusha Kamath,Araveeti Srujan,Ashish Sharma,Chandra Bhushan,Dilip Asbe,Divya Sorate,Duddu Prasanth Kumar,Evan Acharya,Harsh Sharma,Hrithik Kadam,Kanishk Singla,Keyur Doshi,Kiran Praveen,Kolisetty Krishna SK,Krishanu Adhikary,Lokesh MPT,Mayurdeep Sonowal,Nadeem Shaikh,Navya Prakash,Nimit Kothari,Nitin Kukreja,Prashant Devadiga,Rakesh Paul,Ratanjeet Pratap Chauhan,Raunak Kalani,Raviraj Joshi,Shamanth MH,Shantanu Pandey,Shubham Soni,Siddharth Dixit,Smriti Jopat,Sunil Patel,Suraj Singh,Suvradip Paul,Tulasi Pilla,Utkarsh Vaidya,Vineeth Nambiar,Vishal Kanvaty,Yatharth Dedhia*

Main category: cs.AI

TL;DR: FiMI是专门为印度数字支付系统开发的金融语言模型，包含Base和Instruct两个变体，基于Mistral Small 24B架构，通过多阶段训练在金融推理和工具调用方面显著优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对印度数字支付系统的金融语言模型，解决现有通用模型在印度金融领域特定需求（如多语言支持、真实工作流程建模）方面的不足。

Method: 基于Mistral Small 24B架构，采用多阶段训练流程：1) 在680亿个金融、多语言（英语、印地语、Hinglish）和合成数据上进行持续预训练；2) 指令微调；3) 针对多轮工具驱动对话的领域特定监督微调。

Result: FiMI Base在金融推理基准测试中比Mistral Small 24B Base提升20%；FiMI Instruct在领域特定工具调用方面比Mistral Small 24B Instruct提升87%；同时在通用基准测试中保持与同规模模型相当的性能。

Conclusion: FiMI成功开发了专门针对印度数字支付系统的金融语言模型，在保持通用能力的同时，在金融领域特定任务上取得了显著性能提升，证明了领域专业化模型的有效性。

Abstract: We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.

</details>


### [62] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker：一种具有自主规划和自适应检索能力的智能体，通过动态多轮交互和双重训练策略（SFT+RL）在时序知识图谱上进行推理，解决了LLM在TKGQA中的推理幻觉和静态提示限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的时序知识图谱问答方法存在两个主要限制：1）在复杂时序约束下容易产生推理幻觉；2）静态提示限制了模型的自主性和泛化能力，缺乏与时序知识图谱环境的动态交互优化。

Method: 提出TKG-Thinker智能体，具备自主规划和自适应检索能力。采用双重训练策略：首先使用思维链数据进行监督微调（SFT）培养核心规划能力，然后通过强化学习（RL）阶段利用多维奖励在复杂时序约束下优化推理策略。

Result: 在基准数据集上使用三个开源LLM进行实验，TKG-Thinker取得了最先进的性能，并在复杂TKGQA设置中表现出强大的泛化能力。

Conclusion: TKG-Thinker通过动态多轮交互和双重训练策略有效解决了LLM在时序知识图谱问答中的推理幻觉和泛化限制问题，为时序知识推理提供了新的解决方案。

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [63] [OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention](https://arxiv.org/abs/2602.05847)
*Zhangquan Chen,Jiale Tao,Ruihuang Li,Yihao Hu,Ruitao Chen,Zhantao Yang,Xinlei Yu,Haodong Jing,Manyuan Zhang,Shuai Shao,Biao Wang,Qinglin Lu,Ruqi Huang*

Main category: cs.AI

TL;DR: OmniVideo-R1是一个增强的多模态视频理解框架，通过自监督学习和对比学习策略提升音视频理解能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类通过多种模态协同感知世界，但现有的全视频模型在音视频理解任务上仍面临挑战，需要更好的多模态推理能力。

Method: 采用两种关键策略：1）基于自监督学习的查询密集型定位；2）基于对比学习的模态注意力融合。

Result: 在多个基准测试中，OmniVideo-R1始终优于强基线模型，显示出其有效性和强大的泛化能力。

Conclusion: OmniVideo-R1通过增强的多模态推理框架，显著提升了音视频理解任务的性能，为全视频理解提供了有效的解决方案。

Abstract: While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.

</details>


### [64] [BABE: Biology Arena BEnchmark](https://arxiv.org/abs/2602.05857)
*Junting Zhou,Jin Chen,Linfeng Hao,Denghui Cao,Zheyu Wang,Qiguang Chen,Chaoyou Fu,Jiaze Chen,Yuchen Wu,Ge Zhang,Mingxuan Wang,Wenhao Huang,Tong Yang*

Main category: cs.AI

TL;DR: BABE是一个评估生物AI系统实验推理能力的基准，基于同行评议研究论文和真实生物研究构建，挑战模型进行因果推理和跨尺度推断。


<details>
  <summary>Details</summary>
Motivation: 现有生物学基准往往无法评估研究者所需的关键技能：将实验结果与背景知识整合以得出有意义结论的能力。大型语言模型能力已从基础对话扩展到高级科学推理，但缺乏评估实验推理能力的基准。

Method: 基于同行评议研究论文和真实世界生物研究构建BABE基准，确保任务反映实际科学探究的复杂性和跨学科性质。基准设计挑战模型进行因果推理和跨尺度推断。

Result: BABE提供了一个稳健的框架，用于评估AI系统如何像实践科学家一样进行推理，为评估AI在生物学研究中的潜力提供了更真实的衡量标准。

Conclusion: BABE基准填补了现有生物学评估的空白，能够更真实地评估AI系统的实验推理能力，为衡量AI对生物学研究的潜在贡献提供了更好的工具。

Abstract: The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.

</details>


### [65] [Beyond Manual Planning: Seating Allocation for Large Organizations](https://arxiv.org/abs/2602.05875)
*Anton Ipsen,Michael Cashmore,Kirsty Fielding,Nicolas Marchesotti,Parisa Zehtabi,Daniele Magazzeni,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文提出了分层座位分配问题(HSAP)，通过概率路线图和快速探索随机树计算座位距离，结合启发式搜索和动态规划，使用整数规划解决分层组织的座位分配问题。


<details>
  <summary>Details</summary>
Motivation: 大型组织需要将具有紧密层次关系的团队安排在邻近座位，如确保研究小组占据连续区域。目前这个问题是手动管理的，导致重新规划频率低且效果不佳。

Method: 提出了端到端框架解决HSAP：使用概率路线图(PRM)和快速探索随机树(RRT)计算座位间距离，结合启发式搜索和动态规划，采用整数规划方法求解。

Result: 在不同规模实例下评估了PRM框架和后续分配方案，进行了定量和定性评估，证明了方法的有效性。

Conclusion: 提出的框架能够自动解决分层座位分配问题，减少了手动规划的需求，提高了座位分配的效率和优化程度。

Abstract: We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.

</details>


### [66] [A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges](https://arxiv.org/abs/2602.05883)
*Philippe J. Giabbanelli*

Main category: cs.AI

TL;DR: 论文分析了在建模与仿真(M&S)中使用大语言模型(LLMs)的常见误区，提供了实用指南，强调原则性设计选择、诊断策略和实证评估。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在M&S工作流中越来越普及，但看似简单的实践可能引入微妙问题、不必要的复杂性甚至导致结果劣化。需要为模型开发者提供全面实用的指导，帮助他们做出明智决策。

Method: 讨论LLMs在M&S应用中的常见困惑来源，包括非确定性、知识增强（RAG和LoRA）、M&S数据分解和超参数设置。强调原则性设计选择、诊断策略和实证评估方法。

Result: 识别了多个关键问题：增加数据可能适得其反（模型崩溃或消除现有防护），未经评估的微调可能不必要，温度设为0不能保证确定性，大量M&S数据输入可能过度但简化又可能丢失信息。

Conclusion: 需要为M&S从业者提供系统指导，帮助他们理解何时、如何以及是否依赖LLMs，避免常见陷阱，做出基于证据的决策，从而提高LLMs在建模与仿真应用中的有效性和可靠性。

Abstract: Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.

</details>


### [67] [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920)
*Eva Andrés*

Main category: cs.AI

TL;DR: 比较经典与量子强化学习方法解决带容量约束的车辆路径问题，量子增强模型在距离、紧凑度和路线重叠方面表现更优


<details>
  <summary>Details</summary>
Motivation: 探索量子增强的强化学习方法在复杂组合优化问题（特别是带容量约束的车辆路径问题）中的应用潜力，比较经典与量子方法的性能差异

Method: 实现经典、全量子和混合三种变体的优势演员-评论家（A2C）智能体，集成transformer架构通过自注意力和交叉注意力机制捕捉车辆、客户和仓库之间的关系，在20个客户和4辆车的多车辆场景中进行实验

Result: 所有三种方法都能学习有效的路由策略，但量子增强模型优于经典基线，产生更稳健的路线组织，混合架构在距离、紧凑度和路线重叠方面取得最佳整体性能

Conclusion: 量子增强模型在CVRP问题上表现出优越性能，特别是混合量子-经典强化学习模型在处理复杂组合优化问题方面具有显著潜力

Abstract: This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.

</details>


### [68] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: 该研究探索了预训练ASR模型Whisper在语音情感识别中的应用，提出了两种基于注意力的池化方法来降维并保留情感特征，在英语和波斯语数据集上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别研究面临标准大规模数据集缺乏的局限，现有研究利用预训练模型提取特征。本研究探索Whisper这一预训练ASR系统在语音情感识别中的能力，旨在提供轻量高效的替代方案。

Method: 提出了两种基于注意力的池化方法：多头注意力平均池化和QKV池化，用于高效降低Whisper表示的维度同时保留情感特征。在英语IEMOCAP和波斯语ShEMO数据集上实验，使用Whisper Tiny和Small模型，比较不同编码器层的性能。

Result: 多头QKV架构在ShEMO数据集上取得了SOTA结果，未加权准确率提升了2.47%。发现中间层在波斯语数据集上表现更好，为HuBERT X-Large等大型模型提供了轻量高效的替代方案。

Conclusion: 研究突出了Whisper作为语音情感识别表示提取器的潜力，并证明了基于注意力的池化在维度减少方面的有效性，为多语言语音情感识别提供了高效解决方案。

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


### [69] [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008)
*Xianyang Liu,Shangding Gu,Dawn Song*

Main category: cs.AI

TL;DR: AgenticPay是一个用于评估多智能体买卖谈判的基准框架，通过自然语言驱动，包含110多个任务，揭示了大语言模型在谈判策略推理方面的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏评估多智能体之间语言驱动经济互动的原则性设置，而基于大语言模型的智能体越来越多地需要自主协商、协调和交易。

Method: 开发了AgenticPay基准和模拟框架，模拟买卖双方具有私有约束和产品依赖估值的市场，通过多轮语言谈判达成协议，支持从双边议价到多对多市场的110多个任务。

Result: 对最先进的专有和开源大语言模型进行基准测试，揭示了谈判性能的显著差距，突显了长时程策略推理的挑战。

Conclusion: AgenticPay为研究智能体商业和基于语言的市场互动奠定了基础，代码和数据集已开源。

Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.

</details>


### [70] [Learning Event-Based Shooter Models from Virtual Reality Experiments](https://arxiv.org/abs/2602.06023)
*Christopher A. McClurg,Alan R. Wagner*

Main category: cs.AI

TL;DR: 开发数据驱动的离散事件模拟器，用于评估学校安全干预措施，特别是机器人射击手干预策略，通过VR数据训练模拟器实现可扩展评估


<details>
  <summary>Details</summary>
Motivation: VR是评估学校安全措施的有力工具，但需要为每个条件招募新参与者，使得大规模或迭代评估困难，尤其在学习有效干预策略时需要大量训练时

Method: 开发数据驱动的离散事件模拟器（DES），将射击手移动和区域内行动建模为从VR研究中参与者行为学习的随机过程，用于评估机器人射击手干预策略

Result: 模拟器能够重现关键经验模式，实现干预策略的可扩展评估和学习，这些策略直接使用人类受试者训练是不可行的

Conclusion: 这项工作展示了一个高到中保真度的模拟工作流程，为开发和评估自主学校安全干预提供了可扩展的替代方案

Abstract: Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [71] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: 首次在RISC-V架构上实现标准控制流完整性扩展Zicfiss和Zicfilp的设计、集成与评估，通过硬件单元提供前后向控制流保护，仅产生1.0%面积开销和最高15.6%性能开销。


<details>
  <summary>Details</summary>
Motivation: 保护易受攻击程序免受控制流劫持攻击，通过实现基于影子栈和着陆垫原语的安全机制，在RISC-V架构上首次实现标准控制流完整性扩展。

Method: 设计两个独立可配置的硬件单元，分别实现前向边和后向边控制流保护，完全集成到开源CVA6核心中，使用22nm FDX技术合成。

Result: 在22nm FDX技术中仅产生1.0%的面积开销，使用MiBench汽车基准测试子集评估显示最高15.6%的性能开销，完整实现已开源发布。

Conclusion: 成功实现了RISC-V控制流完整性扩展的首次硬件设计，以较低的面积和性能开销提供了有效的控制流保护，为RISC-V生态系统的安全增强做出了贡献。

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [72] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: COFFEE是首个针对HZO基FeFET eNVM的碳建模框架，涵盖硬件制造（体现碳）到使用（运行碳）的全生命周期分析。评估显示HZO-FeFET在单位面积体现碳比CMOS基线高11%，但每MB体现碳比SRAM低4.3倍。在边缘ML加速器中应用可减少42.3%体现碳和高达70%运行碳。


<details>
  <summary>Details</summary>
Motivation: 信息通信技术在全球环境影响中的占比日益增长，新兴非易失性存储器（eNVM）虽能提供能效计算解决方案，但其端到端碳足迹尚未被充分理解。理解硬件系统全生命周期的环境影响是实现可持续计算的第一步。

Method: 本研究以HZO基铁电场效应晶体管（FeFET）为例，开发了COFFEE碳建模框架。该框架基于真实半导体工厂数据和设备制造配方估算体现碳，并利用架构级eNVM设计空间探索工具量化使用阶段性能和能耗。

Result: 在2MB容量下，HZO-FeFET的单位面积体现碳比CMOS基线高11%，但每MB体现碳始终比SRAM低4.3倍。在边缘ML加速器案例中，用HZO基FeFET eNVM替换SRAM权重缓冲区可将体现碳减少42.3%，运行碳减少高达70%。

Conclusion: HZO基FeFET eNVM在可持续计算方面具有显著潜力，通过COFFEE框架的系统分析表明，尽管制造阶段碳足迹略高，但在全生命周期中能大幅降低碳影响，特别是在边缘计算应用中。

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [73] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: 本文提出了一种灵活的FP8数字计算内存加速器，通过动态位宽预测、FIFO输入对齐单元和可扩展INT MAC阵列三项创新，解决了现有DCIM架构在处理可变FP8对齐尾数位宽时的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数字计算内存（DCIM）架构难以支持可变FP8对齐尾数位宽，因为统一的校准策略和固定精度的乘累加（MAC）单元无法有效处理具有不同分布的输入数据。

Method: 提出三项创新技术：1）动态移位感知位宽预测（DSBP），通过实时输入预测自适应调整权重（2/4/6/8b）和输入（2∼12b）对齐尾数精度；2）FIFO输入对齐单元（FIAU），用基于指针的控制替代复杂的桶形移位器；3）精度可扩展的INT MAC阵列，以最小开销实现灵活的权重精度。

Result: 在28nm CMOS工艺中实现64×96 CIM阵列，固定E5M7格式下达到20.4 TFLOPS/W的能效，比先前工作提高2.8倍，同时支持所有FP8格式。在Llama-7b模型上，DSBP在BoolQ和Winogrande数据集上相比固定位宽模式在相同精度水平下获得更高效率。

Conclusion: 该灵活的FP8 DCIM加速器通过创新的动态位宽预测和硬件设计，有效解决了可变FP8格式支持问题，在Transformer推理和训练中实现了显著的能效提升和灵活性。

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [74] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 2025年科学工作流社区峰会总结：工作流在科学研究中面临采用障碍，包括系统通用性与领域特定性的矛盾、可持续性担忧、开发者认可不足、标准化和培训缺失等问题，并提出技术、政策和社区层面的解决方案。


<details>
  <summary>Details</summary>
Motivation: 科学工作流已成为现代研究中协调复杂计算过程、管理大数据集和确保可重复性的关键工具。然而，工作流采用面临多重挑战，需要社区共同努力解决。

Method: 通过2025年6月6日在阿姆斯特丹举办的工作流社区峰会，汇集国际专家，识别工作流采用的关键障碍，并提出跨技术、政策和社区维度的行动方案。

Result: 识别出四大关键障碍：1) 系统通用性与领域特定效用之间的张力；2) 工作流系统和服务长期可持续性担忧；3) 开发和维护可重复工作流的人员认可不足；4) 标准化、资金、培训和跨学科合作方面的差距。

Conclusion: 提出四方面行动路线：1) 从原始计算性能转向衡量真实科学影响的评估指标；2) 形式化工作流模式和社区驱动的基准测试；3) 培养凝聚的国际工作流社区；4) 通过专门角色、职业路径和教育整合投资人力资本。

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [75] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: ORACL是一个利用大语言模型进行微服务资源自动扩展的框架，通过思维链推理诊断性能问题并推荐资源分配，无需特定部署的重新训练。


<details>
  <summary>Details</summary>
Motivation: 随着应用从单体架构转向微服务和Serverless架构，现有的自动扩展策略存在局限性：基于学习的模型需要大量部署特定训练，而手动调优的规则又缺乏泛化能力。需要一种能够适应快速演变的微服务部署的通用资源分配方法。

Method: ORACL框架将运行时遥测数据（包括pod、副本、CPU和内存使用率、延迟、SLO和故障信号）转换为语义自然语言状态描述，然后调用LLM生成可解释的中间推理轨迹。该推理过程识别可能的根本原因，修剪动作空间，并在策略约束下发出安全的分配决策。

Result: 在代表性开源微服务工作负载上的实验表明，ORACL将根本原因识别准确率提高了15%，训练加速高达24倍，在短期场景中服务质量提高了6%，且无需部署特定的重新训练。

Conclusion: 大语言模型可以作为通用的少样本资源分配器，适应快速演变的微服务部署。ORACL通过结合先验知识和思维链推理，提供了一种可解释、高效的自动扩展解决方案。

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [76] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus是一个新的分布式共识协议，通过将拜占庭容错协议嵌入到崩溃容错协议中，谨慎信任TEE硬件安全，在保持性能的同时确保TEE平台被攻破时的数据完整性。


<details>
  <summary>Details</summary>
Motivation: 分布式账本依赖硬件可信执行环境（TEE）提供安全保证，但TEE可能受到攻击，这会破坏分布式账本精心设计的保障机制。需要一种既能利用TEE优势，又能在TEE被攻破时仍能保证完整性的解决方案。

Method: Proteus通过精心重构CFT和BFT协议，使它们的结构对齐，将BFT协议嵌入到CFT协议中，无需额外消息。这种设计谨慎信任TEE的保证，在TEE平台被攻破时仍能保证数据完整性。

Result: Proteus实现了与常规TEE增强共识协议相当的性能，同时确保在TEE平台被攻破时仍能保证数据完整性，提供了更强的弹性保障。

Conclusion: Proteus协议通过创新的架构设计，在保持高性能的同时解决了TEE可能被攻破的安全隐患，为分布式账本提供了更可靠的完整性保护。

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [77] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: 该研究挑战了Dolev-Reischuk下界对拜占庭协议通信复杂性的传统理解，证明达成一致性（univalency）本身不需要二次通信，二次成本主要来自结果传播过程。


<details>
  <summary>Details</summary>
Motivation: 传统Dolev-Reischuk下界表明确定性拜占庭协议需要Ω(f²+n)消息，但研究者质疑这个二次成本到底是为了达成一致性还是仅仅为了传播结果。他们想探究拜占庭协议的核心困难究竟在哪里。

Method: 提出ε-BA松弛协议，允许ε比例的正确处理器输出错误结果，证明当f < n(1/3 - ε)时可以用O(n log n)通信复杂度解决。任何ε-BA协议都可以作为完整BA协议的第一阶段，之后通过一轮全对全交换和多数投票完成BA。

Result: 证明达成一致性（univalency）不需要二次通信，ε-BA可以在O(n log n)通信复杂度下解决。在认证设置中，可提取BA可以在O(f log f)通信复杂度下解决。这表明Dolev-Reischuk下界的二次成本完全来自结果传播，而不是达成一致性。

Conclusion: 拜占庭协议的二次通信成本主要源于结果传播阶段，而不是达成一致性的过程。这一发现重新解释了Dolev-Reischuk下界的本质，为设计更高效的拜占庭协议提供了新思路。

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [78] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze通过建模流水线调度为有向无环图并求解线性规划，在精度约束下计算最优参数冻结比例，减少流水线气泡，提升训练吞吐量40%


<details>
  <summary>Details</summary>
Motivation: 流水线并行虽然能训练超出单设备内存的模型，但实际吞吐量受限于流水线气泡。现有参数冻结方法虽然能通过自适应跳过反向计算提高训练吞吐量，但往往过度冻结参数，导致不必要的精度下降。

Method: 将流水线调度建模为有向无环图，通过求解线性规划问题来计算最优的冻结比例，在精度约束下最小化批次执行时间。

Result: 实验表明，TimelyFreeze在LLaMA-8B模型上实现了高达40%的训练吞吐量提升，同时保持可比较的精度。

Conclusion: TimelyFreeze能够在不影响收敛的情况下实现更快的大规模模型训练，并且能够泛化到各种流水线并行设置中。

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>
