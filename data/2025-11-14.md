<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.CR](#cs.CR) [Total: 11]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services](https://arxiv.org/abs/2511.09688)
*Hiroshi Nakano,Hiroaki Nishi*

Main category: cs.AR

TL;DR: 本文提出了一种新颖的基于历史感知的轨迹k-匿名化方法，并开发了相应的FPGA硬件架构，通过结合并行历史轨迹搜索和传统最短路径算法，提高了匿名化数据的实用性和行为准确性。


<details>
  <summary>Details</summary>
Motivation: 先前基于最短路径的轨迹匿名化方法无法捕捉真实的旅行行为，降低了匿名化数据的实用性。本文旨在解决这一局限性，通过引入历史数据来提高匿名化质量。

Method: 提出了一种历史感知的轨迹k-匿名化方法，集成了并行历史轨迹搜索和传统最短路径查找，使用自定义定点计数模块来准确加权历史数据的贡献。

Result: FPGA实现显示新架构达到超过6,000条记录/秒的实时吞吐量，数据保留率比先前仅使用最短路径的设计提高了1.2%，并更有效地保留了主要干道。

Conclusion: 这一成果标志着在LBS严格延迟约束下实现高保真、历史感知的匿名化的重要进展，既能保护隐私又能保持行为准确性。

Abstract: Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS.

</details>


### [2] [AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs](https://arxiv.org/abs/2511.10007)
*Hongqin Lyu,Yonghao Wang,Jiaxin Zhou,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertMiner是一个模块级断言生成框架，利用AST静态信息辅助LLM挖掘模块级断言，解决现有方法只生成顶层断言而忽略微架构级验证需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于设计规范的断言生成方法通常只产生顶层断言，忽视了在微架构级模块中更频繁出现设计错误的实现细节验证需求。

Method: 通过AST结构提取获取模块调用图、I/O表和数据流图，指导LLM生成模块级规范并挖掘模块级断言。

Result: 评估显示AssertMiner在生成模块高质量断言方面优于AssertLLM和Spec2Assertion等方法，集成后可显著提高结构覆盖率和错误检测能力。

Conclusion: AssertMiner能够实现更全面高效的验证过程，通过模块级断言生成提升验证质量。

Abstract: Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process.

</details>


### [3] [Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations](https://arxiv.org/abs/2511.10563)
*Seyed Hadi Mirfarshbafan,Christoph Studer*

Main category: cs.AR

TL;DR: 该论文提出了一种用于毫米波大规模MIMO系统的波束空间数据检测算法CSPADE及其VLSI架构，相比天线域均衡器可节省高达66%的功耗，同时实现最高的吞吐量和更好的能效。


<details>
  <summary>Details</summary>
Motivation: 大规模多用户MIMO和毫米波通信是未来无线系统的关键技术，但其部署会带来高昂的基带处理硬件成本和功耗。波束空间处理利用毫米波频段的信道稀疏性来降低基带处理复杂度。

Method: 提出了新的波束空间数据检测算法CSPADE（复杂稀疏自适应均衡器）及相应的VLSI架构，包括全并行实现和基于顺序MAC的架构。

Result: 在22nm FDSOI工艺中的VLSI实现结果显示，全并行CSPADE相比天线域均衡可节省54%功耗，基于MAC的架构可节省66%功耗，同时实现了现有大规模MIMO数据检测器中最高的吞吐量。

Conclusion: 所提出的波束空间处理方法和VLSI架构能够显著降低大规模MIMO系统的功耗，同时保持高性能，为未来无线系统的实际部署提供了可行的解决方案。

Abstract: Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [An Efficient and Almost Optimal Solver for the Joint Routing-Assignment Problem via Partial JRA and Large-α Optimization](https://arxiv.org/abs/2511.09563)
*Qilong Yuan*

Main category: cs.AI

TL;DR: 本文提出了一种新颖高效的Partial Path Reconstruction (PPR)求解器，用于解决联合路由分配(JRA)优化问题，能够在大型实例中实现接近最优的解，平均偏差为0.00%。


<details>
  <summary>Details</summary>
Motivation: 现有的精确混合整数规划(MIP)求解器虽然能保证最优性，但在大规模实例中计算效率低下。启发式方法虽然效率高，但与最优解存在约1%的偏差。需要开发一种既能保持高精度又具有计算效率的新方法。

Method: 提出Partial Path Reconstruction (PPR)求解器，首先识别关键物品-占位符对形成简化子问题，然后高效求解以优化全局解。结合PJAR框架，通过迭代优化路径进一步精化解，并引入全局Large-α约束增强最优性。

Result: 在n=300、500和1000的基准数据集上的实验评估表明，该方法始终提供几乎最优的解，平均偏差为0.00%，同时保持高计算效率。

Conclusion: 所提出的PPR方法能够高效解决大规模JRA问题，实现接近最优的解。该框架和方法论在TSP及相关优化问题中具有广泛的应用潜力。

Abstract: The Joint Routing-Assignment (JRA) optimization problem simultaneously determines the assignment of items to placeholders and a Hamiltonian cycle that visits each node pair exactly once, with the objective of minimizing total travel cost. Previous studies introduced an exact mixed-integer programming (MIP) solver, along with datasets and a Gurobi implementation, showing that while the exact approach guarantees optimality, it becomes computationally inefficient for large-scale instances. To overcome this limitation, heuristic methods based on merging algorithms and shaking procedures were proposed, achieving solutions within approximately 1% deviation from the optimum. This work presents a novel and more efficient approach that attains high-accuracy, near-optimal solutions for large-scale JRA problems. The proposed method introduces a Partial Path Reconstructon (PPR) solver that first identifies key item-placeholder pairs to form a reduced subproblem, which is solved efficiently to refine the global solution. Using this PJAR framework, the initial heuristic merging solutions can be further improved, reducing the deviation by half. Moreover, the solution can be iteratively polished with PPR based solver along the optimization path to yield highly accurate tours. Additionally, a global Large-α constraint is incorporated into the JRA model to further enhance solution optimality. Experimental evaluations on benchmark datasets with n = 300, 500, and 1000 demonstrate that the proposed method consistently delivers almost optimal solutions, achieving an average deviation of 0.00% from the ground truth while maintaining high computational efficiency. Beyond the JRA problem, the proposed framework and methodologies exhibit strong potential for broader applications. The Framework can be applied to TSP and related optimization problems.

</details>


### [5] [Variable Neighborhood Search for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2511.09570)
*David Woller,Viktor Kozák,Miroslav Kulich,Libor Přeučil*

Main category: cs.AI

TL;DR: 本文介绍了在2020年IEEE世界计算智能大会上CEC-12竞赛中获胜的电动汽车路径问题解决方案，基于变邻域搜索元启发式算法，在竞赛数据集上取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 由于电动汽车路径问题文献中考虑的各种约束条件多样，比较不同问题变体的方法具有挑战性。CGVRP作为EVRP的最小化变体，为算法比较提供了标准基准。

Method: 采用变邻域搜索（VNS）元启发式算法来解决容量约束绿色车辆路径问题（CGVRP）。

Result: 该方法在完整竞赛数据集上取得了最佳结果，并且超越了后来发表的更近期算法。

Conclusion: 基于变邻域搜索的方法在电动汽车路径问题的标准基准测试中表现出色，证明了其有效性。

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the classical Vehicle Routing Problem (VRP) to reflect the growing use of electric and hybrid vehicles in logistics. Due to the variety of constraints considered in the literature, comparing approaches across different problem variants remains challenging. A minimalistic variant of the EVRP, known as the Capacitated Green Vehicle Routing Problem (CGVRP), was the focus of the CEC-12 competition held during the 2020 IEEE World Congress on Computational Intelligence. This paper presents the competition-winning approach, based on the Variable Neighborhood Search (VNS) metaheuristic. The method achieves the best results on the full competition dataset and also outperforms a more recent algorithm published afterward.

</details>


### [6] [Proceedings of the Second International Workshop on Next-Generation Language Models for Knowledge Representation and Reasoning (NeLaMKRR 2025)](https://arxiv.org/abs/2511.09575)
*Ha-Thanh Nguyen,Ken Satoh,Francesca Toni,Randy Goebel,Kostas Stathis*

Main category: cs.AI

TL;DR: 该研讨会旨在探索如何协调基于transformer的语言模型与基于逻辑的知识表示之间的推理能力，分析语言模型的推理能力，向语言模型注入知识表示风格的推理能力，并形式化语言模型所进行的推理类型。


<details>
  <summary>Details</summary>
Motivation: 推理是人类智能的核心组成部分，在批判性思维、决策支持和问题解决中起基础作用。虽然基于transformer的语言模型在自然语言处理方面取得了巨大进展，但这些模型是否真正具备推理能力以及如何将逻辑推理与语言模型相结合仍需深入探讨。

Method: 通过跨学科研究平台，分析语言模型推理能力与知识表示方法的对比，采用神经符号方法向语言模型注入知识表示风格的推理能力，并形式化语言模型所进行的推理类型。

Result: 该研讨会旨在揭示语言模型如何有效整合和利用知识与推理，从而在精度和可靠性要求较高的应用领域中提升其应用价值和实用性。

Conclusion: 通过协调语言模型与逻辑推理方法，可以增强语言模型在需要精确推理的应用场景中的能力，推动人工智能推理能力的发展。

Abstract: Reasoning is an essential component of human intelligence in that it plays a fundamental role in our ability to think critically, support responsible decisions, and solve challenging problems. Traditionally, AI has addressed reasoning in the context of logic-based representations of knowledge. However, the recent leap forward in natural language processing, with the emergence of language models based on transformers, is hinting at the possibility that these models exhibit reasoning abilities, particularly as they grow in size and are trained on more and more data. Still, despite ongoing discussions about what reasoning is in language models, it is still not easy to articulate to what extent these models are actually capable of reasoning.
  The goal of this workshop is to create a platform for researchers from different disciplines and/or AI perspectives to explore approaches and techniques with the aim to reconcile reasoning between language models using transformers and logic-based representations. The specific objectives include analysing the reasoning abilities of language models measured alongside KR methods, injecting KR-style reasoning abilities into language models (including by neuro-symbolic means), and formalising the kind of reasoning language models carry out. This exploration aims to uncover how language models can effectively integrate and leverage knowledge and reasoning with it, thus improving their application and utility in areas where precision and reliability are key requirements.

</details>


### [7] [Cogent argument extensions are weakly admissible but not vice versa](https://arxiv.org/abs/2511.09600)
*Gustavo Bodanza*

Main category: cs.AI

TL;DR: 本文研究了非可接受论证框架语义中cogent语义与弱可接受语义之间的关系，证明了cogent扩展是弱可接受的，但反之不成立。


<details>
  <summary>Details</summary>
Motivation: 探索两种非可接受论证框架语义（cogent和弱可接受语义）之间的理论关系，澄清它们之间的包含关系。

Method: 通过理论证明的方法，分析cogent扩展与弱可接受扩展之间的逻辑关系。

Result: 证明了cogent扩展一定是弱可接受的，但弱可接受扩展不一定是cogent的，即存在弱可接受但不是cogent的扩展。

Conclusion: cogent语义是弱可接受语义的严格子集，两者之间存在单向包含关系。

Abstract: In this research note, we show the relationship between two non-admissible argumentation framework semantics: cogent and weakly admissible semantics. We prove that, while cogent extensions are weakly admissible, the converse is not true.

</details>


### [8] [Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models](https://arxiv.org/abs/2511.09682)
*Tiansheng Huang,Virat Shejwalkar,Oscar Chang,Milad Nasr,Ling Liu*

Main category: cs.AI

TL;DR: 论文研究了音频推理模型（ARMs）的安全性，发现标准推理训练能抵御普通音频越狱攻击，但无法抵御作者提出的高级攻击。作者提出了Rebellion方法，通过训练ARMs对最坏情况表示漂移具有鲁棒性，显著提升了安全性与性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着音频推理模型的流行，目前尚无研究关注其对抗越狱攻击的安全性。作者旨在填补这一空白，研究ARMs在面对音频越狱攻击时的脆弱性。

Method: 作者首先展示了标准推理训练的局限性，然后提出了Rebellion方法——一种鲁棒的推理训练方法，训练ARMs对最坏情况的表示漂移具有鲁棒性。

Result: 在Qwen2-Audio上的实验表明，Rebellion能够有效防御高级音频越狱攻击，同时不影响良性任务的性能，显著改善了准确性与安全性之间的权衡。

Conclusion: Rebellion方法为音频推理模型提供了有效的安全防护，能够在不牺牲性能的前提下抵御复杂的越狱攻击，为ARMs的安全部署提供了重要保障。

Abstract: Instilling reasoning capabilities in large models (LMs) using reasoning training (RT) significantly improves LMs' performances. Thus Audio Reasoning Models (ARMs), i.e., audio LMs that can reason, are becoming increasingly popular. However, no work has studied the safety of ARMs against jailbreak attacks that aim to elicit harmful responses from target models. To this end, first, we show that standard RT with appropriate safety reasoning data can protect ARMs from vanilla audio jailbreaks, but cannot protect them against our proposed simple yet effective jailbreaks. We show that this is because of the significant representation drift between vanilla and advanced jailbreaks which forces the target ARMs to emit harmful responses. Based on this observation, we propose Rebellion, a robust RT that trains ARMs to be robust to the worst-case representation drift. All our results are on Qwen2-Audio; they demonstrate that Rebellion: 1) can protect against advanced audio jailbreaks without compromising performance on benign tasks, and 2) significantly improves accuracy-safety trade-off over standard RT method.

</details>


### [9] [Echoing: Identity Failures when LLM Agents Talk to Each Other](https://arxiv.org/abs/2511.09710)
*Sarath Shekkizhar,Romain Cosentino,Adam Earle,Silvio Savarese*

Main category: cs.AI

TL;DR: 本文研究了多智能体对话中的回音现象，即智能体放弃自身角色而模仿对话伙伴，导致目标失效。通过大规模实验发现这种现象普遍存在于主流LLM中，且随着对话轮次增加而加剧，最后提出了结构化响应的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体自主交互，出现了一类无法从单智能体性能预测的新失败模式——智能体间对话中的行为漂移。与人类-智能体交互不同，智能体间对话缺乏稳定信号，导致独特的失败模式。

Method: 通过60种智能体配置、3个领域、2000+对话的实验，研究了回音现象在不同LLM提供商中的表现，分析了提示影响和对话动态，并引入了结构化响应的协议级缓解方法。

Result: 实验显示回音现象在三大主流LLM提供商中普遍存在，回音率从5%到70%不等，即使在高级推理模型中也有32.8%的回音率，且不会因增加推理努力而减少。回音随着交互增长（实验中7+轮次）而出现，结构化响应可将回音率降至9%。

Conclusion: 智能体间对话中的回音现象是一个普遍且持久的问题，需要专门的协议级干预来缓解，结构化响应是有效的缓解策略。

Abstract: As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $60$ AxA configurations, $3$ domains, and $2000+$ conversations, we demonstrate that echoing occurs across three major LLM providers, with echoing rates from $5\%$ to $70\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\%$) that are not reduced by increased reasoning efforts. We analyze prompt impacts, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ turns in experiments) and is not merely an artifact of sub-optimal prompting. Finally, we introduce a protocol-level mitigation in which targeted use of structured responses reduces echoing to $9\%$.

</details>


### [10] [AI Annotation Orchestration: Evaluating LLM verifiers to Improve the Quality of LLM Annotations in Learning Analytics](https://arxiv.org/abs/2511.09785)
*Bakhtawar Ahtisham,Kirk Vanacore,Jinsook Lee,Zhuqian Zhou,Doug Pietrzak,Rene F. Kizilcec*

Main category: cs.AI

TL;DR: 该研究测试了通过自我验证和交叉验证的编排策略来提高LLM在辅导对话标注中的可靠性，结果显示编排策略使Cohen's kappa提高了58%，其中自我验证效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM越来越多地用于标注学习交互，但可靠性问题限制了其实际应用。研究旨在验证通过编排策略（自我验证和交叉验证）是否能提高LLM在辅导话语定性编码中的可靠性。

Method: 使用30个一对一数学辅导会话的转录文本，比较三种前沿LLM（GPT、Claude、Gemini）在三种条件下的表现：无验证标注、自我验证和交叉验证。输出结果与盲审的人类裁决进行基准测试，使用Cohen's kappa评估一致性。

Result: 编排策略使kappa提高了58%。自我验证相对于无验证基线几乎使一致性翻倍，在具有挑战性的辅导动作上收益最大。交叉验证平均提高了37%，但存在配对和结构依赖效应：某些验证器-标注器组合优于自我验证，而其他组合则降低了一致性。

Conclusion: 验证是一种有原则的设计杠杆，可用于实现可靠、可扩展的LLM辅助标注，特别是在学习分析领域。研究贡献包括灵活的编排框架、在真实辅导数据上的实证比较以及标准化的报告符号。

Abstract: Large Language Models (LLMs) are increasingly used to annotate learning interactions, yet concerns about reliability limit their utility. We test whether verification-oriented orchestration-prompting models to check their own labels (self-verification) or audit one another (cross-verification)-improves qualitative coding of tutoring discourse. Using transcripts from 30 one-to-one math sessions, we compare three production LLMs (GPT, Claude, Gemini) under three conditions: unverified annotation, self-verification, and cross-verification across all orchestration configurations. Outputs are benchmarked against a blinded, disagreement-focused human adjudication using Cohen's kappa. Overall, orchestration yields a 58 percent improvement in kappa. Self-verification nearly doubles agreement relative to unverified baselines, with the largest gains for challenging tutor moves. Cross-verification achieves a 37 percent improvement on average, with pair- and construct-dependent effects: some verifier-annotator pairs exceed self-verification, while others reduce alignment, reflecting differences in verifier strictness. We contribute: (1) a flexible orchestration framework instantiating control, self-, and cross-verification; (2) an empirical comparison across frontier LLMs on authentic tutoring data with blinded human "gold" labels; and (3) a concise notation, verifier(annotator) (e.g., Gemini(GPT) or Claude(Claude)), to standardize reporting and make directional effects explicit for replication. Results position verification as a principled design lever for reliable, scalable LLM-assisted annotation in Learning Analytics.

</details>


### [11] [Why Open Small AI Models Matter for Interactive Art](https://arxiv.org/abs/2511.09788)
*Mar Canet Sola,Varvara Guljajeva*

Main category: cs.AI

TL;DR: 本文主张开放小型AI模型对互动艺术创作独立性的重要性，这些模型可在本地部署，为艺术家提供对基础设施和代码的控制权，与大型封闭式企业系统形成对比。


<details>
  <summary>Details</summary>
Motivation: 大型封闭式企业AI系统作为不透明的黑箱，对互动艺术作品施加严重限制，包括内容过滤、保存问题和技术挑战。艺术家需要更多自主权和控制力来支持长期艺术创作。

Method: 通过使用可在本地部署的开放小型AI模型，艺术家可以获得对基础设施和代码的完全控制，能够根据需要修改代码、集成新接口，或通过重新训练和微调创建自定义模型。

Result: 小型AI模型赋予创作者更多自主权、控制力和可持续性，支持技术自决，减少对不适合互动艺术需求的企业AI的依赖。

Conclusion: 开放小型AI模型方法能够增强艺术家的能力，支持包含AI组件的艺术作品的长期保存和展览，为互动艺术实践提供更可持续的解决方案。

Abstract: This position paper argues for the importance of open small AI models in creative independence for interactive art practices. Deployable locally, these models offer artists vital control over infrastructure and code, unlike dominant large, closed-source corporate systems. Such centralized platforms function as opaque black boxes, imposing severe limitations on interactive artworks, including restrictive content filters, preservation issues, and technical challenges such as increased latency and limited interfaces. In contrast, small AI models empower creators with more autonomy, control, and sustainability for these artistic processes. They enable the ability to use a model as long as they want, create their own custom model, either by making code changes to integrate new interfaces, or via new datasets by re-training or fine-tuning the model. This fosters technological self-determination, offering greater ownership and reducing reliance on corporate AI ill-suited for interactive art's demands. Critically, this approach empowers the artist and supports long-term preservation and exhibition of artworks with AI components. This paper explores the practical applications and implications of using open small AI models in interactive art, contrasting them with closed-source alternatives.

</details>


### [12] [Massively Parallel Proof-Number Search for Impartial Games and Beyond](https://arxiv.org/abs/2511.10339)
*Tomáš Čížek,Martin Balko,Martin Schmid*

Main category: cs.AI

TL;DR: 本文提出了首个可在大规模CPU核心上高效扩展的并行证明数搜索算法，应用于Sprouts游戏验证，在1024核心上实现332.9倍加速，性能提升4个数量级，并验证了Sprouts猜想的新位置。


<details>
  <summary>Details</summary>
Motivation: 现有并行证明数搜索算法在多CPU核心上扩展性差，随着大规模计算集群的普及，需要开发能够高效利用大量CPU核心的并行算法。

Method: 采用两层并行化架构和工作者间信息共享机制，结合Grundy数进行游戏树约简，开发了大规模并行证明数搜索算法。

Result: 在1024核心上实现332.9倍加速，性能比现有最优Sprouts求解器GLOP快4个数量级，生成证明复杂度提高1000倍，验证了Sprouts猜想的42个新位置。

Conclusion: 该大规模并行证明数搜索算法成功解决了现有并行算法扩展性差的问题，显著提升了游戏求解性能，为复杂游戏树分析提供了有效工具。

Abstract: Proof-Number Search is a best-first search algorithm with many successful applications, especially in game solving. As large-scale computing clusters become increasingly accessible, parallelization is a natural way to accelerate computation. However, existing parallel versions of Proof-Number Search are known to scale poorly on many CPU cores. Using two parallelized levels and shared information among workers, we present the first massively parallel version of Proof-Number Search that scales efficiently even on a large number of CPUs. We apply our solver, enhanced with Grundy numbers for reducing game trees, to the Sprouts game, a case study motivated by the long-standing Sprouts Conjecture. Our solver achieves a significantly improved 332.9$\times$ speedup when run on 1024 cores, enabling it to outperform the state-of-the-art Sprouts solver GLOP by four orders of magnitude in runtime and to generate proofs 1,000$\times$ more complex. Despite exponential growth in game tree size, our solver verified the Sprouts Conjecture for 42 new positions, nearly doubling the number of known outcomes.

</details>


### [13] [Thermally Activated Dual-Modal Adversarial Clothing against AI Surveillance Systems](https://arxiv.org/abs/2511.09829)
*Jiahuan Long,Tingsong Jiang,Hanqing Liu,Chao Ma,Wen Yao*

Main category: cs.AI

TL;DR: 提出一种热激活对抗可穿戴设备，通过热致变色染料和柔性加热单元在衣物表面产生动态对抗图案，在默认状态下为普通黑色T恤，加热后激活隐藏的对抗图案，能在可见光和红外模态下有效躲避检测。


<details>
  <summary>Details</summary>
Motivation: 传统对抗补丁外观显眼，难以在实际场景中部署，需要开发适应性更强、更隐蔽的隐私保护方法以应对无处不在的AI监控。

Method: 集成热致变色染料与柔性加热单元，在衣物表面诱导视觉动态对抗图案，通过嵌入式热单元加热激活隐藏图案。

Result: 物理实验显示对抗可穿戴设备在50秒内实现快速纹理激活，在多样化真实监控环境中保持80%以上的对抗成功率。

Conclusion: 这项工作展示了物理基础、用户可控的反AI系统的新途径，强调了在AI监控无处不在时代主动对抗技术对隐私保护日益增长的重要性。

Abstract: Adversarial patches have emerged as a popular privacy-preserving approach for resisting AI-driven surveillance systems. However, their conspicuous appearance makes them difficult to deploy in real-world scenarios. In this paper, we propose a thermally activated adversarial wearable designed to ensure adaptability and effectiveness in complex real-world environments. The system integrates thermochromic dyes with flexible heating units to induce visually dynamic adversarial patterns on clothing surfaces. In its default state, the clothing appears as an ordinary black T-shirt. Upon heating via an embedded thermal unit, hidden adversarial patterns on the fabric are activated, allowing the wearer to effectively evade detection across both visible and infrared modalities. Physical experiments demonstrate that the adversarial wearable achieves rapid texture activation within 50 seconds and maintains an adversarial success rate above 80\% across diverse real-world surveillance environments. This work demonstrates a new pathway toward physically grounded, user-controllable anti-AI systems, highlighting the growing importance of proactive adversarial techniques for privacy protection in the age of ubiquitous AI surveillance.

</details>


### [14] [EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services](https://arxiv.org/abs/2511.09894)
*Keshara Weerasinghe,Xueren Ge,Tessa Heick,Lahiru Nuwan Wijayasingha,Anthony Cortez,Abhishek Satpathy,John Stankovic,Homa Alemzadeh*

Main category: cs.AI

TL;DR: EgoEMS是首个端到端、高保真、多模态、多人员的EMS数据集，包含233个模拟紧急场景的20+小时数据，由62名参与者（包括46名EMS专业人员）从自我中心视角采集，用于开发AI认知助手支持急救决策。


<details>
  <summary>Details</summary>
Motivation: 急救医疗服务中急救人员面临高认知负荷，AI认知助手可作为虚拟伙伴减轻负担，支持实时数据收集和决策制定。

Method: 开发了开源、低成本、可复现的数据采集系统，采集了233个模拟EMS场景的自我中心视角数据，包含关键步骤、时间戳音频转录、动作质量指标、边界框和分割掩码等标注。

Result: 创建了包含20+小时高保真EMS活动数据的EgoEMS数据集，并提供了实时多模态关键步骤识别和动作质量评估的基准测试套件。

Conclusion: EgoEMS数据集旨在推动智能EMS系统的研究边界，最终改善患者预后。

Abstract: Emergency Medical Services (EMS) are critical to patient survival in emergencies, but first responders often face intense cognitive demands in high-stakes situations. AI cognitive assistants, acting as virtual partners, have the potential to ease this burden by supporting real-time data collection and decision making. In pursuit of this vision, we introduce EgoEMS, the first end-to-end, high-fidelity, multimodal, multiperson dataset capturing over 20 hours of realistic, procedural EMS activities from an egocentric view in 233 simulated emergency scenarios performed by 62 participants, including 46 EMS professionals. Developed in collaboration with EMS experts and aligned with national standards, EgoEMS is captured using an open-source, low-cost, and replicable data collection system and is annotated with keysteps, timestamped audio transcripts with speaker diarization, action quality metrics, and bounding boxes with segmentation masks. Emphasizing realism, the dataset includes responder-patient interactions reflecting real-world emergency dynamics. We also present a suite of benchmarks for real-time multimodal keystep recognition and action quality estimation, essential for developing AI support tools for EMS. We hope EgoEMS inspires the research community to push the boundaries of intelligent EMS systems and ultimately contribute to improved patient outcomes.

</details>


### [15] [Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search](https://arxiv.org/abs/2511.09900)
*Yaodong Yang,Yang Wang,Jinpeng Li,Pei Guo,Da Han,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.AI

TL;DR: AlphaDE是一个利用蛋白质语言模型指导蛋白质序列进化的新框架，通过微调预训练模型和蒙特卡洛树搜索，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质定向进化算法主要关注搜索策略设计，但忽略了利用编码丰富进化模式的蛋白质语言模型来指导搜索过程。

Method: 1. 使用同源蛋白质序列通过掩码语言建模微调预训练蛋白质语言模型；2. 基于蒙特卡洛树搜索进行测试时推理，利用微调后的模型指导蛋白质进化。

Result: 广泛的基准实验表明，AlphaDE即使使用少样本微调也能显著优于之前的最先进方法。案例研究显示其支持通过计算进化压缩蛋白质序列空间。

Conclusion: AlphaDE成功地将蛋白质语言模型的创新范式应用于蛋白质序列进化，为蛋白质工程提供了有效的计算进化工具。

Abstract: Protein evolution through amino acid sequence mutations is a cornerstone of life sciences. While current in-silicon directed evolution algorithms focus on designing search strategies, they overlook how to utilize the transformative protein language models, which encode rich evolutionary patterns, to guide search. To bridge this gap, we propose AlphaDE, a novel framework to evolve protein sequences by harnessing the innovative paradigms of large language models. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility for the interested protein class. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. An interesting case study further shows that AlphaDE supports condensing the protein sequence space through computational evolution.

</details>


### [16] [Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models](https://arxiv.org/abs/2511.09907)
*Yongxian Wei,Yilin Zhao,Li Shen,Xinrui Chen,Runxi Cheng,Sinan Du,Hao Yu,Gang Liu,Jiahong Yan,Chun Yuan,Dian Li*

Main category: cs.AI

TL;DR: 本文提出了一种能够推理规划问题方向并适应求解器能力的问题生成器，通过构建相关问题对和中间问题设计链式思维，利用求解器反馈作为奖励信号来校准难度，在10个数学和通用推理基准上平均提升2.5%。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法存在两个主要问题：(i) 不考虑求解器能力的盲目生成导致低价值问题，或依赖复杂数据管道来平衡难度；(ii) 问题生成缺乏推理，导致问题变体浅层化。

Method: 构建相关问题对并增强中间问题设计CoT，从生成器中引导问题设计策略；将求解器对合成问题的反馈作为奖励信号，使生成器能够校准难度并在求解器能力边缘产生补充问题。

Result: 在10个数学和通用推理基准上平均提升2.5%，并能泛化到语言和视觉语言模型；通过协同进化，进一步获得0.7%的性能提升。

Conclusion: 该方法通过显式推理和难度适应机制，有效解决了现有数据合成方法的局限性，实现了生成器与求解器的协同进化，显著提升了推理模型的性能。

Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.

</details>


### [17] [OIDA-QA: A Multimodal Benchmark for Analyzing the Opioid Industry Documents Archive](https://arxiv.org/abs/2511.09914)
*Xuan Shen,Brian Wingenroth,Zichao Wang,Jason Kuen,Wanrong Zhu,Ruiyi Zhang,Yiwei Wang,Lichun Ma,Anqi Liu,Hongfu Liu,Tong Sun,Kevin S. Hawkins,Kate Tasker,G. Caleb Alexander,Jiuxiang Gu*

Main category: cs.AI

TL;DR: 本文构建了一个针对阿片类药物行业文档的多模态AI分析系统，包括大规模数据集、领域专用多模态大语言模型，以及结合历史问答对和页面引用的增强方法，显著提升了文档信息提取和问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 阿片类药物危机暴露了监管、医疗、企业治理和公共政策等多系统的系统性缺陷，需要创新分析方法来处理UCSF-JHU阿片类药物行业文档档案中的大量数据和文档。这些医疗相关法律和企业文档的复杂性、多模态特性和专业性要求开发更先进的方法和模型。

Method: 1) 按文档属性组织原始数据集，构建包含40万训练文档和1万测试文档的基准；2) 从每个文档提取文本内容、视觉元素和布局结构等多模态信息；3) 使用多个AI模型生成包含36万训练问答对和1万测试问答对的大规模数据集；4) 开发领域专用多模态大语言模型；5) 结合历史问答对作为上下文基础；6) 在答案中纳入页面引用并引入基于重要性的页面分类器。

Result: 初步结果表明，我们的AI助手在文档信息提取和问答任务方面取得了改进。数据集和模型已在HuggingFace平台公开可用。

Conclusion: 通过构建专门针对阿片类药物行业文档的多模态AI分析系统，结合大规模数据集、领域专用模型和增强技术，有效提升了复杂医疗法律文档分析的准确性和相关性。

Abstract: The opioid crisis represents a significant moment in public health that reveals systemic shortcomings across regulatory systems, healthcare practices, corporate governance, and public policy. Analyzing how these interconnected systems simultaneously failed to protect public health requires innovative analytic approaches for exploring the vast amounts of data and documents disclosed in the UCSF-JHU Opioid Industry Documents Archive (OIDA). The complexity, multimodal nature, and specialized characteristics of these healthcare-related legal and corporate documents necessitate more advanced methods and models tailored to specific data types and detailed annotations, ensuring the precision and professionalism in the analysis. In this paper, we tackle this challenge by organizing the original dataset according to document attributes and constructing a benchmark with 400k training documents and 10k for testing. From each document, we extract rich multimodal information-including textual content, visual elements, and layout structures-to capture a comprehensive range of features. Using multiple AI models, we then generate a large-scale dataset comprising 360k training QA pairs and 10k testing QA pairs. Building on this foundation, we develop domain-specific multimodal Large Language Models (LLMs) and explore the impact of multimodal inputs on task performance. To further enhance response accuracy, we incorporate historical QA pairs as contextual grounding for answering current queries. Additionally, we incorporate page references within the answers and introduce an importance-based page classifier, further improving the precision and relevance of the information provided. Preliminary results indicate the improvements with our AI assistant in document information extraction and question-answering tasks. The dataset and models are publicly available at: https://huggingface.co/opioidarchive

</details>


### [18] [Adaptive Hyperbolic Kernels: Modulated Embedding in de Branges-Rovnyak Spaces](https://arxiv.org/abs/2511.09921)
*Leping Si,Meimei Yang,Hui Xue,Shipeng Zhu,Pengfei Fang*

Main category: cs.AI

TL;DR: 本文提出了一种曲率感知的de Branges-Rovnyak空间和自适应双曲核，通过可调节乘子选择合适曲率的双曲空间，在视觉和语言基准测试中优于现有双曲核方法。


<details>
  <summary>Details</summary>
Motivation: 双曲空间在嵌入层次结构方面具有优势，但现有双曲核存在几何失真或缺乏适应性的问题，需要开发更有效的双曲核方法。

Method: 引入曲率感知的de Branges-Rovnyak空间（与庞加莱球等距的RKHS），设计可调节乘子自适应选择合适曲率的双曲空间，构建包括自适应双曲径向核在内的自适应双曲核族。

Result: 在视觉和语言基准测试上的大量实验表明，所提出的核在建模层次依赖关系方面优于现有双曲核。

Conclusion: 通过曲率感知的RKHS和自适应双曲核，能够更有效地建模层次结构，提升双曲表示能力。

Abstract: Hierarchical data pervades diverse machine learning applications, including natural language processing, computer vision, and social network analysis. Hyperbolic space, characterized by its negative curvature, has demonstrated strong potential in such tasks due to its capacity to embed hierarchical structures with minimal distortion. Previous evidence indicates that the hyperbolic representation capacity can be further enhanced through kernel methods. However, existing hyperbolic kernels still suffer from mild geometric distortion or lack adaptability. This paper addresses these issues by introducing a curvature-aware de Branges-Rovnyak space, a reproducing kernel Hilbert space (RKHS) that is isometric to a Poincare ball. We design an adjustable multiplier to select the appropriate RKHS corresponding to the hyperbolic space with any curvature adaptively. Building on this foundation, we further construct a family of adaptive hyperbolic kernels, including the novel adaptive hyperbolic radial kernel, whose learnable parameters modulate hyperbolic features in a task-aware manner. Extensive experiments on visual and language benchmarks demonstrate that our proposed kernels outperform existing hyperbolic kernels in modeling hierarchical dependencies.

</details>


### [19] [ChEmREF: Evaluating Language Model Readiness for Chemical Emergency Response](https://arxiv.org/abs/2511.10027)
*Risha Surana,Qinyuan Ye,Swabha Swayamdipta*

Main category: cs.AI

TL;DR: 该论文提出了ChEmREF基准测试，评估语言模型在危险化学品应急响应中的能力，包括化学表示转换、应急响应建议生成和领域知识问答三个任务。


<details>
  <summary>Details</summary>
Motivation: 应急响应人员在处理危险化学品事故时需要快速准确地获取关键信息，传统方法依赖人工查阅大量化学指南，效率低下且易出错。

Method: 构建ChEmREF基准测试，包含1,035种危险化学品的三个任务：化学表示转换、应急响应生成和领域知识问答。

Result: 最佳模型在化学表示转换任务中准确率为68.0%，应急响应建议任务中LLM评分52.7%，危险化学品考试中多选题准确率63.9%。

Conclusion: 语言模型在协助应急响应方面具有潜力，但由于当前局限性，需要谨慎的人工监督。

Abstract: Emergency responders managing hazardous material HAZMAT incidents face critical, time-sensitive decisions, manually navigating extensive chemical guidelines. We investigate whether today's language models can assist responders by rapidly and reliably understanding critical information, identifying hazards, and providing recommendations.We introduce the Chemical Emergency Response Evaluation Framework (ChEmREF), a new benchmark comprising questions on 1,035 HAZMAT chemicals from the Emergency Response Guidebook and the PubChem Database. ChEmREF is organized into three tasks: (1) translation of chemical representation between structured and unstructured forms (e.g., converting C2H6O to ethanol), (2) emergency response generation (e.g., recommending appropriate evacuation distances) and (3) domain knowledge question answering from chemical safety and certification exams. Our best evaluated models received an exact match of 68.0% on unstructured HAZMAT chemical representation translation, a LLM Judge score of 52.7% on incident response recommendations, and a multiple-choice accuracy of 63.9% on HAMZAT examinations.These findings suggest that while language models show potential to assist emergency responders in various tasks, they require careful human oversight due to their current limitations.

</details>


### [20] [Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning](https://arxiv.org/abs/2511.10037)
*Xiaolong Wei,Yuehu Dong,Xingliang Wang,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.AI

TL;DR: 提出了一种新的Planner-centric Plan-Execute范式，通过全局DAG规划解决现有工具增强LLM在复杂查询处理中的局部优化陷阱问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强大语言模型在处理复杂查询时面临显著挑战，如ReAct等框架因依赖增量决策过程而容易陷入局部优化陷阱。

Method: 采用Planner-centric Plan-Execute架构，核心是执行全局DAG规划的Planner模型；开发两阶段训练方法，结合SFT和GRPO优化工具选择和全局规划能力；创建ComplexTool-Plan大规模基准数据集。

Result: 在StableToolBench基准测试中实现了最先进的性能，展示了优越的端到端执行能力和对复杂多工具工作流的鲁棒处理。

Conclusion: 该框架通过架构创新从根本上解决了局部优化瓶颈，为复杂查询处理提供了有效的解决方案。

Abstract: Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Planner's tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.

</details>


### [21] [Efficient Thought Space Exploration through Strategic Intervention](https://arxiv.org/abs/2511.10038)
*Ziheng Li,Hengyi Cai,Xiaochi Wei,Yuchen Li,Shuaiqiang Wang,Zhi-Hong Deng,Dawei Yin*

Main category: cs.AI

TL;DR: 提出Hint-Practice Reasoning (HPR)框架，通过分布不一致性减少(DIR)指标动态识别关键决策点，让强大LLM提供概率指导，小型模型执行主要推理步骤，在保持准确性的同时大幅减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理时扩展方法计算成本过高，通过分析解码轨迹发现大多数token预测准确，只有少数关键token导致偏差，需要更高效的推理方法。

Method: HPR框架包含两个组件：hinter(强大LLM)在关键决策点提供概率指导，practitioner(高效小模型)执行主要推理步骤，使用DIR指标动态识别干预点。

Result: 在算术和常识推理基准测试中，HPR达到与自一致性和MCTS基线相当的性能，同时只解码1/5的token，在保持相似或更低FLOPs的情况下比其他方法准确率最高提升5.1%。

Conclusion: HPR框架通过智能识别关键决策点，实现了效率与准确性的最佳平衡，为LLM推理提供了更实用的解决方案。

Abstract: While large language models (LLMs) demonstrate emerging reasoning capabilities, current inference-time expansion methods incur prohibitive computational costs by exhaustive sampling. Through analyzing decoding trajectories, we observe that most next-token predictions align well with the golden output, except for a few critical tokens that lead to deviations. Inspired by this phenomenon, we propose a novel Hint-Practice Reasoning (HPR) framework that operationalizes this insight through two synergistic components: 1) a hinter (powerful LLM) that provides probabilistic guidance at critical decision points, and 2) a practitioner (efficient smaller model) that executes major reasoning steps. The framework's core innovation lies in Distributional Inconsistency Reduction (DIR), a theoretically-grounded metric that dynamically identifies intervention points by quantifying the divergence between practitioner's reasoning trajectory and hinter's expected distribution in a tree-structured probabilistic space. Through iterative tree updates guided by DIR, HPR reweights promising reasoning paths while deprioritizing low-probability branches. Experiments across arithmetic and commonsense reasoning benchmarks demonstrate HPR's state-of-the-art efficiency-accuracy tradeoffs: it achieves comparable performance to self-consistency and MCTS baselines while decoding only 1/5 tokens, and outperforms existing methods by at most 5.1% absolute accuracy while maintaining similar or lower FLOPs.

</details>


### [22] [Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning for Medical Report Generation](https://arxiv.org/abs/2511.10065)
*Bodong Du,Honglong Yang,Xiaomeng Li*

Main category: cs.AI

TL;DR: RadFlow是一个分层工作流引导的强化优化框架，通过模拟放射科医生结构化报告工作流程，解决现有医疗报告生成系统忽视报告层次结构的问题，提升诊断一致性和报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成系统将报告视为扁平序列，忽视了放射科医生描述视觉发现、总结为印象并精炼关键病例陈述的结构化工作流程，导致描述性和诊断性内容不一致。

Method: 提出RadFlow框架，包括临床基础奖励层次结构：全局层面整合语言流畅性、医学领域正确性和发现与印象间的跨部分一致性；局部层面强调印象质量；关键感知策略优化机制自适应正则化高风险病例学习。

Result: 在胸部X光和颈动脉超声数据集上的实验表明，RadFlow相比最先进基线方法持续改善了诊断一致性和整体报告质量。

Conclusion: RadFlow将结构化报告范式转化为强化微调过程，使模型能够生成既语言一致又临床对齐的报告，更好地模拟放射科医生的实际工作流程。

Abstract: Radiologists compose diagnostic reports through a structured workflow: they describe visual findings, summarize them into impressions, and carefully refine statements in clinically critical cases. However, most existing medical report generation (MRG) systems treat reports as flat sequences, overlooking this hierarchical organization and leading to inconsistencies between descriptive and diagnostic content. To align model behavior with real-world reporting practices, we propose RadFlow, a hierarchical workflow-guided reinforcement optimization framework that explicitly models the structured nature of clinical reporting. RadFlow introduces a clinically grounded reward hierarchy that mirrors the organization of radiological reports. At the global level, the reward integrates linguistic fluency, medical-domain correctness, and cross-sectional consistency between Finding and Impression, promoting coherent and clinically faithful narratives. At the local level, a section-specific reward emphasizes Impression quality, reflecting its central role in diagnostic accuracy. Furthermore, a critical-aware policy optimization mechanism adaptively regularizes learning for high-risk or clinically sensitive cases, emulating the cautious refinement behavior of radiologists when documenting critical findings. Together, these components translate the structured reporting paradigm into the reinforcement fine-tuning process, enabling the model to generate reports that are both linguistically consistent and clinically aligned. Experiments on chest X-ray and carotid ultrasound datasets demonstrate that RadFlow consistently improves diagnostic coherence and overall report quality compared with state-of-the-art baselines.

</details>


### [23] [Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning](https://arxiv.org/abs/2511.10067)
*Yuxuan Zhou,Yubin Wang,Bin Wang,Chen Ning,Xien Liu,Ji Wu,Jianye Hao*

Main category: cs.AI

TL;DR: MuSeR是一种数据驱动方法，通过自评估和精炼增强LLM在医疗领域的上下文感知能力，在HealthBench基准上显著提升性能，特别是上下文感知方面。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域表现出色，但在真实医疗场景中仍表现不佳，需要更强的上下文感知能力来识别缺失或关键细节并提供安全、有帮助且上下文适当的响应。

Method: 提出多层面自精炼方法，通过属性条件查询生成器模拟多样化真实用户上下文，让LLM响应查询、自评估答案并精炼响应，最后使用监督微调增强模型上下文感知能力。

Result: 在HealthBench数据集上显著提升LLM性能，特别是在上下文感知维度。通过知识蒸馏，较小模型性能超越教师模型，在HealthBench上达到63.8%的新SOTA。

Conclusion: MuSeR方法有效提升了LLM在医疗领域的上下文感知能力，通过自精炼和知识蒸馏实现了性能的显著提升，为医疗AI应用提供了更可靠的解决方案。

Abstract: Large language models (LLMs) have shown great promise in the medical domain, achieving strong performance on several benchmarks. However, they continue to underperform in real-world medical scenarios, which often demand stronger context-awareness, i.e., the ability to recognize missing or critical details (e.g., user identity, medical history, risk factors) and provide safe, helpful, and contextually appropriate responses. To address this issue, we propose Multifaceted Self-Refinement (MuSeR), a data-driven approach that enhances LLMs' context-awareness along three key facets (decision-making, communication, and safety) through self-evaluation and refinement. Specifically, we first design a attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity. An LLM then responds to these queries, self-evaluates its answers along three key facets, and refines its responses to better align with the requirements of each facet. Finally, the queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness ability. Evaluation results on the latest HealthBench dataset demonstrate that our method significantly improves LLM performance across multiple aspects, with particularly notable gains in the context-awareness axis. Furthermore, by incorporating knowledge distillation with the proposed method, the performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its teacher model, achieving a new SOTA across all open-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at https://muser-llm.github.io.

</details>


### [24] [Balancing Centralized Learning and Distributed Self-Organization: A Hybrid Model for Embodied Morphogenesis](https://arxiv.org/abs/2511.10101)
*Takehiro Ishikawa*

Main category: cs.AI

TL;DR: 该研究探索如何将可学习的类脑控制器与类细胞的Gray-Scott底物耦合，以最小努力引导模式形成。通过混合耦合方法，在约165步内实现100%严格收敛，同时显著降低控制能耗。


<details>
  <summary>Details</summary>
Motivation: 研究动机是构建可操控、稳健且节能的具身系统，探索集中式学习与分布式自组织之间的最优分工，量化形态计算的价值。

Method: 在可微分的PyTorch反应-扩散模拟器中嵌入紧凑的卷积策略，采用warm-hold-decay增益调度产生空间平滑的有界调制，训练优化图灵带谱目标同时惩罚控制努力和不稳定性。

Result: 混合耦合方法在约165步内实现100%严格收敛，匹配细胞仅有的谱选择性(0.436 vs. 0.434)，同时使用比NN主导控制少约15倍的ℓ1努力和>200倍的ℓ2功率。发现非单调的Goldilocks区域(A≈0.03-0.045)在94-96步内实现100%准收敛。

Conclusion: 控制器提供短暂、稀疏的推动将系统置于正确的吸引盆中，之后局部物理维持模式，展示了形态计算：控制器播种然后让步。研究为构建利用集中学习与分布式自组织最优分工的可操控、稳健、节能具身系统提供了实用方案。

Abstract: We investigate how to couple a learnable brain-like'' controller to a cell-like'' Gray--Scott substrate to steer pattern formation with minimal effort. A compact convolutional policy is embedded in a differentiable PyTorch reaction--diffusion simulator, producing spatially smooth, bounded modulations of the feed and kill parameters ($ΔF$, $ΔK$) under a warm--hold--decay gain schedule. Training optimizes Turing-band spectral targets (FFT-based) while penalizing control effort ($\ell_1/\ell_2$) and instability. We compare three regimes: pure reaction--diffusion, NN-dominant, and a hybrid coupling. The hybrid achieves reliable, fast formation of target textures: 100% strict convergence in $\sim 165$ steps, matching cell-only spectral selectivity (0.436 vs.\ 0.434) while using $\sim 15\times$ less $\ell_1$ effort and $>200\times$ less $\ell_2$ power than NN-dominant control. An amplitude sweep reveals a non-monotonic Goldilocks'' zone ($A \approx 0.03$--$0.045$) that yields 100\% quasi convergence in 94--96 steps, whereas weaker or stronger gains fail to converge or degrade selectivity. These results quantify morphological computation: the controller seeds then cedes,'' providing brief, sparse nudges that place the system in the correct basin of attraction, after which local physics maintains the pattern. The study offers a practical recipe for building steerable, robust, and energy-efficient embodied systems that exploit an optimal division of labor between centralized learning and distributed self-organization.

</details>


### [25] [RAGFort: Dual-Path Defense Against Proprietary Knowledge Base Extraction in Retrieval-Augmented Generation](https://arxiv.org/abs/2511.10128)
*Qinfeng Li,Miao Pan,Ke Xiong,Ge Su,Zhiqiang Shen,Yan Liu,Bing Sun,Hao Peng,Xuhong Zhang*

Main category: cs.AI

TL;DR: RAG系统面临知识库重构攻击威胁，现有防御只针对单一攻击路径。本文提出RAGFort防御框架，通过对比重索引和约束级联生成实现双路径保护，有效降低重构成功率并保持回答质量。


<details>
  <summary>Details</summary>
Motivation: RAG系统面临的知识库重构攻击通过类内和类间路径逐步提取知识，现有防御方法只保护单一路径，无法提供全面保护。

Method: 提出RAGFort防御框架，包含对比重索引实现类间隔离和约束级联生成实现类内保护的双模块结构。

Result: 实验验证RAGFort在安全性、性能和鲁棒性方面显著降低重构攻击成功率，同时保持回答质量。

Conclusion: RAGFort为知识库提取攻击提供了全面有效的防御方案，证明联合保护类内和类间路径的必要性。

Abstract: Retrieval-Augmented Generation (RAG) systems deployed over proprietary knowledge bases face growing threats from reconstruction attacks that aggregate model responses to replicate knowledge bases. Such attacks exploit both intra-class and inter-class paths, progressively extracting fine-grained knowledge within topics and diffusing it across semantically related ones, thereby enabling comprehensive extraction of the original knowledge base. However, existing defenses target only one path, leaving the other unprotected. We conduct a systematic exploration to assess the impact of protecting each path independently and find that joint protection is essential for effective defense. Based on this, we propose RAGFort, a structure-aware dual-module defense combining "contrastive reindexing" for inter-class isolation and "constrained cascade generation" for intra-class protection. Experiments across security, performance, and robustness confirm that RAGFort significantly reduces reconstruction success while preserving answer quality, offering comprehensive defense against knowledge base extraction attacks.

</details>


### [26] [DenoGrad: Deep Gradient Denoising Framework for Enhancing the Performance of Interpretable AI Models](https://arxiv.org/abs/2511.10161)
*J. Javier Alonso-Ramos,Ignacio Aguilera-Martos,Andrés Herrera-Poyatos,Francisco Herrera*

Main category: cs.AI

TL;DR: 提出DenoGrad框架，利用深度学习模型的梯度来检测和修正噪声样本，在保持原始数据分布的同时提升可解释AI模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有去噪方法会改变原始数据分布，导致不现实的场景和有偏模型，这在可解释AI中尤为问题，因为其可解释性依赖于底层数据模式的保真度。

Method: DenoGrad框架利用在目标数据上训练的准确深度学习模型的梯度来动态检测和调整噪声样本，不依赖于特定任务。

Result: 在表格和时间序列数据集的各种噪声设置下，DenoGrad优于现有最先进的去噪策略，是唯一能保持原始数据分布的高质量方法。

Conclusion: DenoGrad通过任务特定的高质量解决方案作为参考，提供了更精确和适应性强的噪声定义，显著提升了可解释AI模型的鲁棒性和性能。

Abstract: The performance of Machine Learning (ML) models, particularly those operating within the Interpretable Artificial Intelligence (Interpretable AI) framework, is significantly affected by the presence of noise in both training and production data. Denoising has therefore become a critical preprocessing step, typically categorized into instance removal and instance correction techniques. However, existing correction approaches often degrade performance or oversimplify the problem by altering the original data distribution. This leads to unrealistic scenarios and biased models, which is particularly problematic in contexts where interpretable AI models are employed, as their interpretability depends on the fidelity of the underlying data patterns. In this paper, we argue that defining noise independently of the solution may be ineffective, as its nature can vary significantly across tasks and datasets. Using a task-specific high quality solution as a reference can provide a more precise and adaptable noise definition. To this end, we propose DenoGrad, a novel Gradient-based instance Denoiser framework that leverages gradients from an accurate Deep Learning (DL) model trained on the target data -- regardless of the specific task -- to detect and adjust noisy samples. Unlike conventional approaches, DenoGrad dynamically corrects noisy instances, preserving problem's data distribution, and improving AI models robustness. DenoGrad is validated on both tabular and time series datasets under various noise settings against the state-of-the-art. DenoGrad outperforms existing denoising strategies, enhancing the performance of interpretable IA models while standing out as the only high quality approach that preserves the original data distribution.

</details>


### [27] [Two Constraint Compilation Methods for Lifted Planning](https://arxiv.org/abs/2511.10164)
*Periklis Mantenoglou,Luigi Bonassi,Enrico Scala,Pedro Zuidberg Dos Martires*

Main category: cs.AI

TL;DR: 该论文提出了两种无需基础化（grounding）的约束编译方法，用于处理包含定性状态轨迹约束的大规模规划问题，相比现有方法能生成更简洁的规划规范且保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现实世界规划问题常包含安全性要求、任务排序条件和中间子目标等定性状态轨迹约束。现有编译器需要先对问题进行基础化处理，这在对象数量多、动作元数高的情况下无法扩展。

Method: 提出了两种无需基础化的约束编译方法，将约束编译为支持现代规划器的规划问题，并证明了编译器的正确性和最坏情况时间复杂度。

Result: 在国际规划竞赛领域上的实验表明，该方法高效且生成的规划规范比基础化编译器简洁数个数量级，同时在使用现代规划器时保持竞争力。

Conclusion: 提出的无基础化约束编译方法能有效处理大规模规划问题，显著减少规划规范规模，为现实世界复杂规划问题提供了可行的解决方案。

Abstract: We study planning in a fragment of PDDL with qualitative state-trajectory constraints, capturing safety requirements, task ordering conditions, and intermediate sub-goals commonly found in real-world problems. A prominent approach to tackle such problems is to compile their constraints away, leading to a problem that is supported by state-of-the-art planners. Unfortunately, existing compilers do not scale on problems with a large number of objects and high-arity actions, as they necessitate grounding the problem before compilation. To address this issue, we propose two methods for compiling away constraints without grounding, making them suitable for large-scale planning problems. We prove the correctness of our compilers and outline their worst-case time complexity. Moreover, we present a reproducible empirical evaluation on the domains used in the latest International Planning Competition. Our results demonstrate that our methods are efficient and produce planning specifications that are orders of magnitude more succinct than the ones produced by compilers that ground the domain, while remaining competitive when used for planning with a state-of-the-art planner.

</details>


### [28] [MTP: Exploring Multimodal Urban Traffic Profiling with Modality Augmentation and Spectrum Fusion](https://arxiv.org/abs/2511.10218)
*Haolong Xiang,Peisi Wang,Xiaolong Xu,Kun Yi,Xuyun Zhang,Quanzheng Sheng,Amin Beheshti,Wei Fan*

Main category: cs.AI

TL;DR: 本文提出了一种名为MTP的多模态城市交通信号分析框架，通过数值、视觉和文本三个视角学习多模态特征，在频域中进行交通信号学习，并通过层次对比学习融合三种模态信息。


<details>
  <summary>Details</summary>
Motivation: 现有交通信号建模方法主要依赖传感器原始数值数据，忽略了多模态异构城市数据中的语义信息，这限制了全面理解交通信号和准确预测复杂交通动态的能力。

Method: MTP框架包含三个分支：1）视觉分支将交通信号转换为频域图像和周期性图像；2）文本分支基于特定主题、背景信息和项目描述生成描述性文本；3）数值分支使用频域多层感知机处理原始数据。通过层次对比学习融合三种模态。

Result: 在六个真实世界数据集上的广泛实验表明，该方法相比最先进方法具有优越性能。

Conclusion: MTP框架通过多模态学习有效提升了城市交通信号分析能力，证明了融合数值、视觉和文本信息在交通信号建模中的重要性。

Abstract: With rapid urbanization in the modern era, traffic signals from various sensors have been playing a significant role in monitoring the states of cities, which provides a strong foundation in ensuring safe travel, reducing traffic congestion and optimizing urban mobility. Most existing methods for traffic signal modeling often rely on the original data modality, i.e., numerical direct readings from the sensors in cities. However, this unimodal approach overlooks the semantic information existing in multimodal heterogeneous urban data in different perspectives, which hinders a comprehensive understanding of traffic signals and limits the accurate prediction of complex traffic dynamics. To address this problem, we propose a novel \textit{M}ultimodal framework, \textit{MTP}, for urban \textit{T}raffic \textit{P}rofiling, which learns multimodal features through numeric, visual, and textual perspectives. The three branches drive for a multimodal perspective of urban traffic signal learning in the frequency domain, while the frequency learning strategies delicately refine the information for extraction. Specifically, we first conduct the visual augmentation for the traffic signals, which transforms the original modality into frequency images and periodicity images for visual learning. Also, we augment descriptive texts for the traffic signals based on the specific topic, background information and item description for textual learning. To complement the numeric information, we utilize frequency multilayer perceptrons for learning on the original modality. We design a hierarchical contrastive learning on the three branches to fuse the spectrum of three modalities. Finally, extensive experiments on six real-world datasets demonstrate superior performance compared with the state-of-the-art approaches.

</details>


### [29] [Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation](https://arxiv.org/abs/2511.10233)
*Jianghan Zhu,Yaoxin Wu,Zhuoyi Lin,Zhengyuan Zhang,Haiyan Yin,Zhiguang Cao,Senthilnath Jayavelu,Xiaoli Li*

Main category: cs.AI

TL;DR: EvoReal方法通过进化模块和LLM指导生成结构多样的合成实例，显著提升了神经组合优化模型在真实世界VRP场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在从均匀分布的合成训练数据泛化到真实世界VRP场景时存在困难，需要弥合这一泛化差距。

Method: 使用进化模块和LLM指导生成结构属性统计上模拟真实实例的合成实例，然后通过两阶段微调：先对齐结构丰富的合成分布，再在真实基准实例上直接微调。

Result: EvoReal显著提升了最先进神经求解器的泛化能力，在TSPLib和CVRPLib基准上分别将性能差距缩小到1.05%和2.71%。

Conclusion: EvoReal通过生成结构多样的合成实例和渐进式微调，有效弥合了神经组合优化模型在真实世界VRP场景中的泛化差距。

Abstract: Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales.

</details>


### [30] [ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs](https://arxiv.org/abs/2511.10240)
*Minbae Park,Hyemin Yang,Jeonghyun Kim,Kunsoo Park,Hyunjoon Kim*

Main category: cs.AI

TL;DR: ProgRAG是一个多跳知识图谱问答框架，通过将复杂问题分解为子问题并逐步扩展推理路径来解决LLM在知识图谱增强推理中的检索和推理失败问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理方面表现强大，但存在幻觉和透明度有限的问题。现有的知识图谱增强LLM方法虽然能提高推理性能，但仍面临检索不准确和推理失败等挑战，特别是在长输入上下文和复杂逻辑方向捕捉方面。

Method: 提出ProgRAG框架：将复杂问题分解为子问题，逐步扩展部分推理路径；在每一步使用外部检索器收集候选证据，通过LLM进行不确定性感知剪枝；优化LLM推理上下文，通过组织和重新排列子问题答案获得的部分推理路径。

Result: 在三个知名数据集上的实验表明，ProgRAG在多跳知识图谱问答任务中优于现有基线方法，提供了更好的可靠性和推理质量。

Conclusion: ProgRAG通过渐进式推理路径扩展和不确定性感知证据剪枝，有效解决了知识图谱增强LLM中的检索和推理失败问题，在多跳KGQA任务中表现出优越性能。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.

</details>


### [31] [Beyond Single-Step Updates: Reinforcement Learning of Heuristics with Limited-Horizon Search](https://arxiv.org/abs/2511.10264)
*Gal Hadar,Forest Agostinelli,Shahaf S. Shperberg*

Main category: cs.AI

TL;DR: 本文提出了一种增强启发式搜索的广义方法，通过有限范围搜索和基于到搜索边界最短路径的启发式更新，改进状态采样和启发式更新过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖单步Bellman更新，仅基于最佳邻居和对应边成本来更新状态的启发式。本文旨在通过更全面的搜索和更新策略来提升启发式学习效果。

Method: 提出广义方法，执行有限范围搜索，基于到搜索边界的最短路径（包含边成本和边界状态启发式值）来更新每个状态的启发式。

Result: 该方法增强了状态采样和启发式更新过程，能够更有效地学习启发式函数。

Conclusion: 通过有限范围搜索和基于最短路径的启发式更新，可以显著改进启发式搜索在顺序决策问题中的性能。

Abstract: Many sequential decision-making problems can be formulated as shortest-path problems, where the objective is to reach a goal state from a given starting state. Heuristic search is a standard approach for solving such problems, relying on a heuristic function to estimate the cost to the goal from any given state. Recent approaches leverage reinforcement learning to learn heuristics by applying deep approximate value iteration. These methods typically rely on single-step Bellman updates, where the heuristic of a state is updated based on its best neighbor and the corresponding edge cost. This work proposes a generalized approach that enhances both state sampling and heuristic updates by performing limited-horizon searches and updating each state's heuristic based on the shortest path to the search frontier, incorporating both edge costs and the heuristic values of frontier states.

</details>


### [32] [Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention](https://arxiv.org/abs/2511.10268)
*Zhe Xu,Zhicai Wang,Junkang Wu,Jinda Lu,Xiang Wang*

Main category: cs.AI

TL;DR: 该论文提出大型视觉语言模型存在物体幻觉问题，主要源于训练中高度共现物体之间的虚假相关性。作者引入因果分析建立结构因果模型，开发了Causal-HalBench基准来量化虚假相关性的影响，并评估主流模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型经常出现物体幻觉问题，错误判断图像中物体的存在。作者认为这主要源于训练过程中高度共现物体之间形成的虚假相关性，导致模型受到视觉上下文影响而产生幻觉物体。

Method: 1. 将因果分析引入LVLMs的物体识别场景，建立结构因果模型
2. 正式定义由共现偏差引起的虚假相关性
3. 开发Causal-HalBench基准，包含反事实样本和综合因果指标
4. 提出可扩展的管道用于生成反事实样本，利用专有LVLMs和文本到图像模型

Result: 对主流LVLMs使用Causal-HalBench的评估表明，这些模型都表现出对虚假相关性的敏感性，尽管程度有所不同。

Conclusion: 大型视觉语言模型确实容易受到虚假相关性的影响，Causal-HalBench提供了一个有效的框架来量化这种影响并评估模型的鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) often suffer from object hallucination, making erroneous judgments about the presence of objects in images. We propose this primar- ily stems from spurious correlations arising when models strongly associate highly co-occurring objects during train- ing, leading to hallucinated objects influenced by visual con- text. Current benchmarks mainly focus on hallucination de- tection but lack a formal characterization and quantitative evaluation of spurious correlations in LVLMs. To address this, we introduce causal analysis into the object recognition scenario of LVLMs, establishing a Structural Causal Model (SCM). Utilizing the language of causality, we formally de- fine spurious correlations arising from co-occurrence bias. To quantify the influence induced by these spurious correla- tions, we develop Causal-HalBench, a benchmark specifically constructed with counterfactual samples and integrated with comprehensive causal metrics designed to assess model ro- bustness against spurious correlations. Concurrently, we pro- pose an extensible pipeline for the construction of these coun- terfactual samples, leveraging the capabilities of proprietary LVLMs and Text-to-Image (T2I) models for their genera- tion. Our evaluations on mainstream LVLMs using Causal- HalBench demonstrate these models exhibit susceptibility to spurious correlations, albeit to varying extents.

</details>


### [33] [Bidirectional Bounded-Suboptimal Heuristic Search with Consistent Heuristics](https://arxiv.org/abs/2511.10272)
*Shahaf S. Shperberg,Natalie Morad,Lior Siag,Ariel Felner,Dor Atzmon*

Main category: cs.AI

TL;DR: 本文提出了几种基于BAE*算法的有界次优双向搜索变体，用于解决指定解成本次优性界限的搜索问题。


<details>
  <summary>Details</summary>
Motivation: 现有双向启发式搜索研究主要集中在最优搜索方法上，而本文关注有界次优双向搜索问题，即在指定解成本次优性界限的情况下进行搜索。

Method: 基于最先进的最优双向搜索算法BAE*（适用于一致启发式），开发了多个专门针对有界次优场景的BAE*变体算法。

Result: 通过实验评估，比较了新变体与其他有界次优双向算法以及标准加权A*算法的性能，发现每种算法在不同条件下表现优异。

Conclusion: 不同算法各有优缺点，各自在特定条件下表现最佳，这突出了各种方法的优势和局限性。

Abstract: Recent advancements in bidirectional heuristic search have yielded significant theoretical insights and novel algorithms. While most previous work has concentrated on optimal search methods, this paper focuses on bounded-suboptimal bidirectional search, where a bound on the suboptimality of the solution cost is specified. We build upon the state-of-the-art optimal bidirectional search algorithm, BAE*, designed for consistent heuristics, and introduce several variants of BAE* specifically tailored for the bounded-suboptimal context. Through experimental evaluation, we compare the performance of these new variants against other bounded-suboptimal bidirectional algorithms as well as the standard weighted A* algorithm. Our results demonstrate that each algorithm excels under distinct conditions, highlighting the strengths and weaknesses of each approach.

</details>


### [34] [FactGuard: Event-Centric and Commonsense-Guided Fake News Detection](https://arxiv.org/abs/2511.10281)
*Jing He,Han Zhang,Yuanhui Xiao,Wei Guo,Shaowen Yao,Renyang Liu*

Main category: cs.AI

TL;DR: FactGuard是一个利用大语言模型提取事件中心内容来减少写作风格影响的假新闻检测框架，通过动态可用性机制和知识蒸馏技术提高检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着攻击者模仿真实新闻风格，基于写作风格的假新闻检测方法效果逐渐下降，而现有LLM方法存在功能探索浅、可用性模糊和推理成本高等问题。

Method: 提出FactGuard框架：1) 利用LLM提取事件中心内容；2) 引入动态可用性机制识别矛盾案例；3) 通过知识蒸馏得到FactGuard-D用于冷启动和资源受限场景。

Result: 在两个基准数据集上的综合实验表明，该方法在鲁棒性和准确性方面持续优于现有方法。

Conclusion: FactGuard有效解决了假新闻检测中的风格敏感性和LLM可用性挑战，为实际部署提供了可行方案。

Abstract: Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.

</details>


### [35] [Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage](https://arxiv.org/abs/2511.10284)
*Belona Sonna,Alban Grastien,Claire Benn*

Main category: cs.AI

TL;DR: 提出了一个基于溯因解释的形式化框架来审计AI决策过程中的隐私泄露问题，通过识别最小充分证据来检测敏感信息是否被泄露，并在德国信用数据集上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: AI决策过程中的隐私泄露风险日益严重，特别是当敏感信息可能被推断时，需要一种能够提供严格隐私保证且易于理解的审计工具。

Method: 使用溯因解释框架，识别最小充分证据来证明模型决策，引入潜在适用解释(PAE)概念来识别那些结果可以保护具有敏感特征的个体的个体。

Result: 实验结果表明，敏感字面量在模型决策过程中的重要性会影响隐私泄露程度，溯因推理能够实现可解释的隐私审计。

Conclusion: 尽管存在计算挑战和简化假设，溯因推理为在AI决策中协调透明度、模型可解释性和隐私保护提供了实用途径。

Abstract: Privacy leakage in AI-based decision processes poses significant risks, particularly when sensitive information can be inferred. We propose a formal framework to audit privacy leakage using abductive explanations, which identifies minimal sufficient evidence justifying model decisions and determines whether sensitive information disclosed. Our framework formalizes both individual and system-level leakage, introducing the notion of Potentially Applicable Explanations (PAE) to identify individuals whose outcomes can shield those with sensitive features. This approach provides rigorous privacy guarantees while producing human understandable explanations, a key requirement for auditing tools. Experimental evaluation on the German Credit Dataset illustrates how the importance of sensitive literal in the model decision process affects privacy leakage. Despite computational challenges and simplifying assumptions, our results demonstrate that abductive reasoning enables interpretable privacy auditing, offering a practical pathway to reconcile transparency, model interpretability, and privacy preserving in AI decision-making.

</details>


### [36] [SITA: A Framework for Structure-to-Instance Theorem Autoformalization](https://arxiv.org/abs/2511.10356)
*Chenyi Li,Wanli Ma,Zichen Wang,Zaiwen Wen*

Main category: cs.AI

TL;DR: 本文提出了SITA框架，用于自动形式化从抽象结构实例化产生的数学定理，通过将形式化抽象结构作为模块化模板，指导具体实例的形式化，并利用LLM生成和反馈引导优化确保自动化和形式正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面虽有进展，但在形式化从抽象结构实例化产生的定理时仍面临挑战，目标是实现研究级数学结果的自动形式化。

Method: 开发SITA框架，将形式化抽象结构作为包含定义、假设、操作和定理的模块化模板，利用Lean类型类机制整合具体实例，结合LLM生成和反馈引导优化确保正确性。

Result: 在优化问题数据集上的实验表明，SITA能有效形式化基于抽象结构的多样化实例。

Conclusion: SITA框架成功桥接了抽象数学理论与具体应用之间的差距，实现了研究级数学定理的自动形式化。

Abstract: While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures.

</details>


### [37] [Explaining Decentralized Multi-Agent Reinforcement Learning Policies](https://arxiv.org/abs/2511.10409)
*Kayla Boggess,Sarit Kraus,Lu Feng*

Main category: cs.AI

TL;DR: 本文提出了为去中心化多智能体强化学习生成策略总结和基于查询的解释方法，解决了现有方法无法处理去中心化设置中的不确定性和非确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法主要关注中心化多智能体强化学习，无法有效处理去中心化设置中固有的不确定性和非确定性，需要开发专门针对去中心化MARL的解释方法。

Method: 提出生成策略总结的方法来捕捉任务排序和智能体协作，同时开发基于查询的解释系统，支持When、Why Not和What类型的用户查询，涵盖特定智能体行为的时间、原因和内容解释。

Result: 在四个MARL领域和两种去中心化MARL算法上评估，证明方法具有通用性和计算效率。用户研究表明，总结和解释显著提高了用户问答性能，并提升了理解和满意度等主观评分。

Conclusion: 所提出的方法为去中心化多智能体强化学习提供了有效的解释框架，能够生成有意义的策略总结和针对性解释，显著改善了用户对复杂MARL系统的理解和满意度。

Abstract: Multi-Agent Reinforcement Learning (MARL) has gained significant interest in recent years, enabling sequential decision-making across multiple agents in various domains. However, most existing explanation methods focus on centralized MARL, failing to address the uncertainty and nondeterminism inherent in decentralized settings. We propose methods to generate policy summarizations that capture task ordering and agent cooperation in decentralized MARL policies, along with query-based explanations for When, Why Not, and What types of user queries about specific agent behaviors. We evaluate our approach across four MARL domains and two decentralized MARL algorithms, demonstrating its generalizability and computational efficiency. User studies show that our summarizations and explanations significantly improve user question-answering performance and enhance subjective ratings on metrics such as understanding and satisfaction.

</details>


### [38] [Generalizing Analogical Inference from Boolean to Continuous Domains](https://arxiv.org/abs/2511.10416)
*Francisco Cunha,Yves Lepage,Zied Bouraoui,Miguel Couceiro*

Main category: cs.AI

TL;DR: 本文重新审视类比推理的理论基础，提出了一个统一框架来处理实值域中的类比推理，该框架包含布尔分类和回归任务，并支持连续函数的类比推理。


<details>
  <summary>Details</summary>
Motivation: 现有类比推理框架主要针对布尔域，无法扩展到回归任务或连续域，且现有泛化边界在布尔设置下也存在问题。

Method: 引入基于广义均值定义的参数化类比统一框架，表征类比保持函数的类别，并在平滑性假设下推导最坏情况和平均情况误差边界。

Result: 建立了涵盖离散和连续域的一般类比推理理论，证明了该框架能够统一处理布尔分类和回归任务。

Conclusion: 提出的统一框架为类比推理提供了跨离散和连续域的一般理论，解决了现有方法在连续域中的局限性。

Abstract: Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains.

</details>


### [39] [Using Certifying Constraint Solvers for Generating Step-wise Explanations](https://arxiv.org/abs/2511.10428)
*Ignace Bleukx,Maarten Flippo,Bart Bogaerts,Emir Demirović,Tias Guns*

Main category: cs.AI

TL;DR: 本文提出了一种基于约束求解器生成的证明来快速计算逐步解释序列的方法，通过抽象证明框架和修剪简化技术，显著提高了解释生成效率。


<details>
  <summary>Details</summary>
Motivation: 在可解释约束求解领域，向用户解释问题不可满足性时，计算逐步解释序列计算成本高昂，限制了其应用范围。

Method: 定义抽象证明框架，将证明和逐步解释统一表示；提出多种将证明转换为逐步解释序列的方法，特别关注修剪和简化技术以保持序列简洁。

Result: 该方法显著加快了逐步解释序列的生成速度，同时生成解释的质量与当前最先进方法相当。

Conclusion: 利用约束求解器生成的证明作为起点，可以有效提高逐步解释序列的计算效率，为更大范围的问题提供可解释性支持。

Abstract: In the field of Explainable Constraint Solving, it is common to explain to a user why a problem is unsatisfiable. A recently proposed method for this is to compute a sequence of explanation steps. Such a step-wise explanation shows individual reasoning steps involving constraints from the original specification, that in the end explain a conflict. However, computing a step-wise explanation is computationally expensive, limiting the scope of problems for which it can be used. We investigate how we can use proofs generated by a constraint solver as a starting point for computing step-wise explanations, instead of computing them step-by-step. More specifically, we define a framework of abstract proofs, in which both proofs and step-wise explanations can be represented. We then propose several methods for converting a proof to a step-wise explanation sequence, with special attention to trimming and simplification techniques to keep the sequence and its individual steps small. Our results show our method significantly speeds up the generation of step-wise explanation sequences, while the resulting step-wise explanation has a quality similar to the current state-of-the-art.

</details>


### [40] [Preference Elicitation for Step-Wise Explanations in Logic Puzzles](https://arxiv.org/abs/2511.10436)
*Marco Foschini,Marianne Defresne,Emilio Gamba,Bart Bogaerts,Tias Guns*

Main category: cs.AI

TL;DR: 本文研究了如何通过交互式偏好学习来优化逻辑谜题的分步解释质量，提出了动态归一化技术和MACHOP查询生成策略，在数独和逻辑网格谜题上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 分步解释需要用户定义目标函数来量化解释质量，但定义好的目标函数具有挑战性。需要从机器学习社区引入交互式偏好学习方法来学习用户偏好。

Method: 提出两种动态归一化技术来重新缩放特征并稳定学习过程，引入MACHOP查询生成策略，结合非支配约束和基于上置信界的多样化方法。

Result: 在数独和逻辑网格谜题上使用人工用户进行评估，并通过真实用户验证。MACHOP在两种设置下始终产生比标准方法更高质量的解释。

Conclusion: 交互式偏好学习方法可以有效提升分步解释的质量，MACHOP策略在多样化查询生成方面表现优异，能够稳定地学习用户偏好并生成更易理解的解释。

Abstract: Step-wise explanations can explain logic puzzles and other satisfaction problems by showing how to derive decisions step by step. Each step consists of a set of constraints that derive an assignment to one or more decision variables. However, many candidate explanation steps exist, with different sets of constraints and different decisions they derive. To identify the most comprehensible one, a user-defined objective function is required to quantify the quality of each step. However, defining a good objective function is challenging. Here, interactive preference elicitation methods from the wider machine learning community can offer a way to learn user preferences from pairwise comparisons. We investigate the feasibility of this approach for step-wise explanations and address several limitations that distinguish it from elicitation for standard combinatorial problems. First, because the explanation quality is measured using multiple sub-objectives that can vary a lot in scale, we propose two dynamic normalization techniques to rescale these features and stabilize the learning process. We also observed that many generated comparisons involve similar explanations. For this reason, we introduce MACHOP (Multi-Armed CHOice Perceptron), a novel query generation strategy that integrates non-domination constraints with upper confidence bound-based diversification. We evaluate the elicitation techniques on Sudokus and Logic-Grid puzzles using artificial users, and validate them with a real-user evaluation. In both settings, MACHOP consistently produces higher-quality explanations than the standard approach.

</details>


### [41] [Non-Monotonic S4F Standpoint Logic](https://arxiv.org/abs/2511.10449)
*Piotr Gorczyca,Hannes Strass*

Main category: cs.AI

TL;DR: 本文提出了一种新的S4F立场逻辑，统一了S4F模态逻辑和立场命题逻辑，能够表达多视角非单调语义承诺，其计算复杂度不高于组成逻辑。


<details>
  <summary>Details</summary>
Motivation: 统一表示多个异构视角的立场逻辑与使用模态逻辑（特别是S4F）捕获非单调推理框架的需求，希望开发一个能同时表达多视角和非单调语义承诺的形式化方法。

Method: 提出了S4F立场逻辑，定义了其语法和语义，分析了计算复杂度，并概述了轻信和怀疑接受机制。

Result: S4F立场逻辑在计算复杂度上不高于其组成逻辑，无论是单调还是非单调形式，并提供了接受机制和示例说明。

Conclusion: S4F立场逻辑成功统一了S4F和立场命题逻辑，为多视角非单调推理提供了有效的形式化框架，且保持了可接受的计算复杂度。

Abstract: Standpoint logics offer unified modal logic-based formalisms for representing multiple heterogeneous viewpoints. At the same time, many non-monotonic reasoning frameworks can be naturally captured using modal logics, in particular using the modal logic S4F. In this work, we propose a novel formalism called S4F Standpoint Logic, which generalises both S4F and standpoint propositional logic and is therefore capable of expressing multi-viewpoint, non-monotonic semantic commitments. We define its syntax and semantics and analyze its computational complexity, obtaining the result that S4F Standpoint Logic is not computationally harder than its constituent logics, whether in monotonic or non-monotonic form. We also outline mechanisms for credulous and sceptical acceptance and illustrate the framework with an example.

</details>


### [42] [Proceedings of The third international workshop on eXplainable AI for the Arts (XAIxArts)](https://arxiv.org/abs/2511.10482)
*Corey Ford,Elizabeth Wilson,Shuoyang Zheng,Gabriel Vigliensoni,Jeba Rezwana,Lanxi Xiao,Michael Clemens,Makayla Lewis,Drew Hemment,Alan Chamberlain,Helen Kennedy,Nick Bryan-Kinns*

Main category: cs.AI

TL;DR: 第三届XAIxArts国际研讨会汇集了HCI、交互设计、AI、可解释AI和数字艺术领域的研究者，探讨可解释AI在艺术中的作用。


<details>
  <summary>Details</summary>
Motivation: 将可解释AI技术应用于艺术领域，促进艺术创作与AI技术的融合。

Method: 通过国际研讨会形式，汇集多学科研究者进行交流和探讨。

Result: 成功举办了第三届XAIxArts研讨会，建立了艺术与可解释AI交叉领域的研究社区。

Conclusion: 可解释AI在艺术领域具有重要应用价值，需要继续推动这一交叉领域的研究和发展。

Abstract: This third international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts. Workshop held at the 17th ACM Conference on Creativity and Cognition (C&C 2025), online.

</details>


### [43] [Rethinking Science in the Age of Artificial Intelligence](https://arxiv.org/abs/2511.10524)
*Maksim E. Eren,Dorianis M. Perez*

Main category: cs.AI

TL;DR: AI正在从计算工具转变为科学研究的积极合作者，重塑研究流程，但需要谨慎整合和治理，确保AI增强而非取代人类判断。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何改变研究工作的各个阶段，从信息管理到实验设计，以及这种转变带来的挑战和机遇。

Method: 通过评论性分析，考察AI在研究工作流程中的应用现状和发展趋势。

Result: AI系统已能帮助研究人员管理信息过载、筛选文献、发现跨学科联系、生成假设以及设计和执行实验。

Conclusion: 需要在科学实践中审慎采用AI，通过促进透明度、可重复性和问责制的政策来实现AI的负责任整合。

Abstract: Artificial intelligence (AI) is reshaping how research is conceived, conducted, and communicated across fields from chemistry to biomedicine. This commentary examines how AI is transforming the research workflow. AI systems now help researchers manage the information deluge, filtering the literature, surfacing cross-disciplinary links for ideas and collaborations, generating hypotheses, and designing and executing experiments. These developments mark a shift from AI as a mere computational tool to AI as an active collaborator in science. Yet this transformation demands thoughtful integration and governance. We argue that at this time AI must augment but not replace human judgment in academic workflows such as peer review, ethical evaluation, and validation of results. This paper calls for the deliberate adoption of AI within the scientific practice through policies that promote transparency, reproducibility, and accountability.

</details>


### [44] [Regular Games -- an Automata-Based General Game Playing Language](https://arxiv.org/abs/2511.10593)
*Radosław Miernik,Marek Szykuła,Jakub Kowalski,Jakub Cieśluk,Łukasz Galas,Wojciech Pawlik*

Main category: cs.AI

TL;DR: 提出了一个新的通用游戏系统Regular Games，旨在实现计算高效和游戏设计便利。该系统包含多级语言，核心是使用有限自动机定义规则的低级语言，适用于所有有限回合制不完全信息游戏。


<details>
  <summary>Details</summary>
Motivation: 开发一个既计算高效又便于游戏设计的通用游戏系统，解决现有系统在效率和便利性方面的不足。

Method: 采用多级语言架构：核心是使用有限自动机定义规则的低级语言，支持高级语言用于游戏设计，最终翻译为低级语言。系统还包括编辑器、可视化工具、基准测试工具和调试器。

Result: RG系统生成的前向模型比现有最先进系统更快，在效率上超越了Regular Boardgames和Ludii等其他GGP系统。

Conclusion: RG系统成功实现了计算高效和游戏设计便利的目标，为通用游戏开发提供了有效的解决方案。

Abstract: We propose a new General Game Playing (GGP) system called Regular Games (RG). The main goal of RG is to be both computationally efficient and convenient for game design. The system consists of several languages. The core component is a low-level language that defines the rules by a finite automaton. It is minimal with only a few mechanisms, which makes it easy for automatic processing (by agents, analysis, optimization, etc.). The language is universal for the class of all finite turn-based games with imperfect information. Higher-level languages are introduced for game design (by humans or Procedural Content Generation), which are eventually translated to a low-level language. RG generates faster forward models than the current state of the art, beating other GGP systems (Regular Boardgames, Ludii) in terms of efficiency. Additionally, RG's ecosystem includes an editor with LSP, automaton visualization, benchmarking tools, and a debugger of game description transformations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [45] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 本文评估了Ksurf方法在云资源编排任务中的性能，该方法是针对高变异性云数据的方差最小化估计器，与Drone编排器结合使用，在高度可变的工作负载下显著降低了延迟方差并节省了成本。


<details>
  <summary>Details</summary>
Motivation: 云数据中心中容器基础设施的资源编排和配置参数搜索面临巨大配置空间和云不确定性的挑战，现有方法如Drone编排器使用上下文多臂老虎机技术，但在云环境复杂性增加时决策准确性下降。

Method: 将Ksurf方差最小化估计器方法作为上下文多臂老虎机的目标函数模型，应用于Drone编排器中，用于处理高度可变的云工作负载和资源指标。

Result: Ksurf显著降低了延迟方差（p95降低41%，p99降低47%），CPU使用率降低4%，主节点内存使用减少7MB，在VarBench Kubernetes基准测试中平均工作pod数量节省7%成本。

Conclusion: Ksurf方法在高度可变云环境下能够有效优化资源估计和编排决策，显著改善性能指标并实现成本节约。

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [46] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: MoFa是一个新颖的预训练性能建模框架，统一了多维优化特征和容错机制，通过增强的成本模型准确捕捉关键优化效果，并基于历史集群可靠性数据集成容错模型，为LLM预训练系统提供先验指导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模指数级增长，分布式预训练面临巨大优化挑战。传统手动调优成本高昂，现有性能建模方法无法全面考虑优化特征和容错机制开销。

Method: 提出MoFa框架，包含增强的成本模型准确捕捉关键优化效果，集成基于历史集群可靠性数据的容错模型，并开发基于MoFa的调优系统探索最优预训练性能。

Result: 广泛的建模评估表明MoFa在各种场景下都能实现高预测精度，调优实验系统揭示了不同配置下影响预训练性能的关键因素。

Conclusion: MoFa框架为LLM预训练系统设计和部署提供了可靠的先验指导，能够有效解决大规模分布式预训练中的性能优化挑战。

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [47] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: GPU系统在数据中心存在性能变异问题，特别是多GPU节点中热不平衡导致的"Lit Silicon"效应，即热引发的慢GPU拖累快GPU性能。通过检测和缓解技术，结合三种功率管理方案，可实现最高6%性能提升和4%功耗改善。


<details>
  <summary>Details</summary>
Motivation: GPU系统在现代数据中心中广泛应用，但存在节点和集群级别的性能变异问题，严重影响高性能计算和AI工作负载（如大语言模型训练）的效率。

Method: 分析单节点多GPU系统运行LLM训练的性能，提出Lit Silicon效应模型，设计检测和缓解技术，评估三种功率管理方案：GPU热设计功耗优化、节点级GPU功率封顶优化、节点级CPU功率转移优化。

Result: 在两个AMD Instinct MI300X GPU系统和两个LLM训练框架上进行实验，观察到最高6%的性能提升和4%的功耗改善，可为数据中心节省数亿美元成本。

Conclusion: 提出的解决方案几乎零成本，可作为新的节点级功率管理层在数据中心轻松部署，有效解决Lit Silicon问题。

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [48] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: CacheX通过在虚拟机内使用驱逐集探测缓存抽象信息，无需硬件或hypervisor支持，实现了LLC竞争感知的任务调度和虚拟颜色感知的页面缓存管理，有效提升了公共云虚拟机中各种工作负载的缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 云虚拟机中的缓存优化往往无效，因为虚拟机无法了解和控制分配的缓存资源。在公有云中，CPU缓存可能被分区或共享，但虚拟机无法获知缓存配置细节，也无法通过页面放置策略影响缓存使用。

Method: 提出CacheX解决方案，使用驱逐集在虚拟机内探测准确细粒度的缓存抽象信息，无需硬件或hypervisor支持，并开发了两种新技术：LLC竞争感知的任务调度和虚拟颜色感知的页面缓存管理。

Result: 在x86 Linux内核中实现的CacheX评估表明，它能够有效提高公共云虚拟机中各种工作负载的缓存利用率。

Conclusion: CacheX通过探测虚拟机内的缓存抽象信息，解决了云环境中缓存优化无效的问题，为云虚拟机提供了有效的缓存管理机制。

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [49] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: 提出基于监督学习的稀疏矩阵重排序算法选择模型，能够根据矩阵特征自动选择最优重排序算法，在佛罗里达稀疏矩阵数据集上实现55.37%的求解时间减少和1.45倍的平均加速比。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏矩阵排序算法选择方法依赖暴力搜索或经验知识，无法适应不同稀疏矩阵结构，需要智能化的自动选择方法。

Method: 开发基于监督学习的模型，学习矩阵特征与常用重排序算法之间的关联，实现稀疏矩阵重排序算法的自动智能选择。

Result: 在佛罗里达稀疏矩阵数据集上，模型能准确预测各种矩阵的最优重排序算法，相比仅使用AMD算法，求解时间减少55.37%，平均加速比为1.45。

Conclusion: 监督学习方法能有效解决稀疏矩阵重排序算法选择问题，显著提升求解效率，为大规模稀疏矩阵求解提供智能化优化方案。

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [50] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种现代工作负载调度器的分类新方法，描述了操作系统进程调度器、集群系统作业调度器和大数据调度器三类调度器，分析它们从早期采用到现代实现的演变过程，并讨论各类调度器之间的差异和时序发展。


<details>
  <summary>Details</summary>
Motivation: 为现代工作负载调度器提供一个系统的分类框架，理解不同类型调度器的演变历程和算法特征，揭示本地和分布式系统调度策略设计的共性。

Method: 通过描述三类调度器（操作系统进程调度器、集群系统作业调度器、大数据调度器）的演变过程，分析其算法使用和功能特性，比较各类调度器的差异和发展时序。

Result: 建立了现代工作负载调度器的分类体系，揭示了不同类型调度器从早期到现代的演变路径，识别了各类调度器的关键特征和算法应用。

Conclusion: 本地和分布式系统的调度策略设计在关注点上存在相似性，现代调度器的发展体现了算法和功能特性的持续演进。

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [51] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: STAGE是一个生成高保真执行轨迹的框架，用于准确建模大语言模型工作负载，支持全面的并行化策略，可扩展到32K GPU规模。


<details>
  <summary>Details</summary>
Motivation: 由于大规模AI训练和推理系统的访问受限，且现有平台的执行轨迹难以适应未来更大规模系统配置的研究需求，需要一种可扩展且表达力强的机制来建模分布式工作负载执行。

Method: 引入Symbolic Tensor grAph GEnerator(STAGE)框架，通过合成高保真执行轨迹来建模LLM工作负载，支持全面的并行化策略，允许用户系统性地探索广泛的LLM架构和系统配置。

Result: STAGE展示了其可扩展性，能够合成覆盖32K GPU的高保真LLM轨迹，同时在计算、内存和通信方面保持张量级精度。

Conclusion: STAGE将公开发布，以促进分布式机器学习系统的进一步研究。

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [52] [Revisit to the Bai-Galbraith signature scheme](https://arxiv.org/abs/2511.09582)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: 本文介绍了与Dilithium不同的Bai-Galbraith签名方案，该方案基于LWE问题且没有公钥压缩。


<details>
  <summary>Details</summary>
Motivation: 比较Dilithium与Bai-Galbraith签名方案的差异，特别关注公钥压缩方面。

Method: 基于Learning with Errors (LWE)问题的格基签名方案，采用Bai-Galbraith在BG14中提出的方法。

Result: 描述了一个与Dilithium不同的签名方案，该方案没有公钥压缩特性。

Conclusion: Bai-Galbraith签名方案是Dilithium的替代方案，在公钥处理方面有所不同。

Abstract: Dilithium is one of the NIST approved lattice-based signature schemes. In this short note we describe the Bai-Galbraith signature scheme proposed in BG14, which differs to Dilithium, due to the fact that there is no public key compression. This lattice-based signature scheme is based on Learning with Errors (LWE).

</details>


### [53] [How Can We Effectively Use LLMs for Phishing Detection?: Evaluating the Effectiveness of Large Language Model-based Phishing Detection Models](https://arxiv.org/abs/2511.09606)
*Fujiao Ji,Doowon Kim*

Main category: cs.CR

TL;DR: 本研究探索了如何有效利用大语言模型进行钓鱼检测和品牌识别，通过测试不同输入模态、温度设置和提示策略，发现商业LLM在钓鱼检测中表现优于开源模型，而截图输入在品牌识别中效果最佳。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习钓鱼检测器存在泛化能力差和缺乏可解释性的问题，而LLM作为有前景的检测机制，其有效性尚未被充分探索。

Method: 使用19,131个真实钓鱼网站和243个良性网站数据集，评估7个LLM（2个商业模型和5个开源模型）及2个深度学习基线模型，考察输入模态（截图、logo、HTML、URL）、温度设置和提示工程策略的影响。

Result: 商业LLM在钓鱼检测中表现更好，而DL模型在良性样本上表现更佳；截图输入在品牌识别中达到93-95%准确率；多模态输入或单样本提示不能持续提升性能；较高温度会降低性能。

Conclusion: 推荐使用截图输入和零温度设置来最大化LLM检测器的准确性，当截图信息不足时可将HTML作为辅助上下文。

Abstract: Large language models (LLMs) have emerged as a promising phishing detection mechanism, addressing the limitations of traditional deep learning-based detectors, including poor generalization to previously unseen websites and a lack of interpretability. However, LLMs' effectiveness for phishing detection remains unexplored. This study investigates how to effectively leverage LLMs for phishing detection (including target brand identification) by examining the impact of input modalities (screenshots, logos, HTML, and URLs), temperature settings, and prompt engineering strategies. Using a dataset of 19,131 real-world phishing websites and 243 benign sites, we evaluate seven LLMs -- two commercial models (GPT 4.1 and Gemini 2.0 flash) and five open-source models (Qwen, Llama, Janus, DeepSeek-VL2, and R1) -- alongside two deep learning (DL)-based baselines (PhishIntention and Phishpedia).
  Our findings reveal that commercial LLMs generally outperform open-source models in phishing detection, while DL models demonstrate better performance on benign samples. For brand identification, screenshot inputs achieve optimal results, with commercial LLMs reaching 93-95% accuracy and open-source models, particularly Qwen, achieving up to 92%. However, incorporating multiple input modalities simultaneously or applying one-shot prompts does not consistently enhance performance and may degrade results. Furthermore, higher temperature values reduce performance. Based on these results, we recommend using screenshot inputs with zero temperature to maximize accuracy for LLM-based detectors with HTML serving as auxiliary context when screenshot information is insufficient.

</details>


### [54] [Cooperative Local Differential Privacy: Securing Time Series Data in Distributed Environments](https://arxiv.org/abs/2511.09696)
*Bikash Chandra Singh,Md Jakir Hossain,Rafael Diaz,Sandip Roy,Ravi Mukkamala,Sachin Shetty*

Main category: cs.CR

TL;DR: 提出了一种协作式本地差分隐私（CLDP）机制，通过在多用户间分配噪声向量来增强隐私保护，在聚合时噪声相互抵消，既保护个体隐私又保持整体统计特性。


<details>
  <summary>Details</summary>
Motivation: 传统本地差分隐私方法在时间窗口内添加用户特定噪声存在漏洞，攻击者可通过聚合过程消除噪声来推断个体统计信息，导致隐私保护失效。

Method: 采用协作式噪声生成和分配机制，多个用户共同生成噪声向量，在数据扰动过程中分配噪声，使得聚合时噪声相互抵消。

Result: CLDP机制能够有效对抗基于时间窗口的攻击方法，在保护个体隐私的同时保持数据效用，适用于大规模实时数据集。

Conclusion: 协作式本地差分隐私机制在多用户环境中实现了更好的数据效用与隐私保护平衡，解决了传统LDP方法在时间序列数据中的安全漏洞。

Abstract: The rapid growth of smart devices such as phones, wearables, IoT sensors, and connected vehicles has led to an explosion of continuous time series data that offers valuable insights in healthcare, transportation, and more. However, this surge raises significant privacy concerns, as sensitive patterns can reveal personal details. While traditional differential privacy (DP) relies on trusted servers, local differential privacy (LDP) enables users to perturb their own data. However, traditional LDP methods perturb time series data by adding user-specific noise but exhibit vulnerabilities. For instance, noise applied within fixed time windows can be canceled during aggregation (e.g., averaging), enabling adversaries to infer individual statistics over time, thereby eroding privacy guarantees.
  To address these issues, we introduce a Cooperative Local Differential Privacy (CLDP) mechanism that enhances privacy by distributing noise vectors across multiple users. In our approach, noise is collaboratively generated and assigned so that when all users' perturbed data is aggregated, the noise cancels out preserving overall statistical properties while protecting individual privacy. This cooperative strategy not only counters vulnerabilities inherent in time-window-based methods but also scales effectively for large, real-time datasets, striking a better balance between data utility and privacy in multiuser environments.

</details>


### [55] [Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization](https://arxiv.org/abs/2511.09775)
*Dilli Prasad Sharma,Xiaowei Sun,Liang Xue,Xiaodong Lin,Pulei Xiong*

Main category: cs.CR

TL;DR: 提出基于SHAP熵正则化的隐私保护方法，通过惩罚低熵SHAP归因分布来减少可解释AI在AIoT应用中的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: AIoT在智能家居中的广泛应用增加了对透明可解释模型的需求，但现有XAI方法（如SHAP、LIME）可能无意中暴露用户敏感信息，带来新的隐私风险。

Method: 采用SHAP熵正则化方法，在训练过程中引入基于熵的正则化目标，惩罚低熵的SHAP归因分布，促进特征贡献的均匀分布。

Result: 实验结果表明，SHAP熵正则化相比基线模型显著减少隐私泄露，同时保持高预测准确性和解释保真度。

Conclusion: 该方法为安全可信的AIoT应用开发隐私保护的可解释AI技术做出了贡献。

Abstract: The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.

</details>


### [56] [DP-GENG : Differentially Private Dataset Distillation Guided by DP-Generated Data](https://arxiv.org/abs/2511.09876)
*Shuo Shi,Jinghuai Zhang,Shijie Jiang,Chunyi Zhou,Yuyuan Li,Mengying Zhu,Yangyang Wu,Tianyu Du*

Main category: cs.CR

TL;DR: 本文提出了一个名为Libn的新框架，通过利用差分隐私生成的数据来解决当前差分隐私数据集蒸馏方法的关键限制，显著提升了数据集效用和对抗成员推理攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私数据集蒸馏方法虽然通过注入噪声来降低隐私风险，但往往无法充分利用原始数据集，导致生成的蒸馏数据集在真实性和效用上表现不佳。

Method: Libn框架首先使用差分隐私生成的数据初始化蒸馏数据集以增强真实性，然后通过改进的差分隐私特征匹配技术在小隐私预算下蒸馏原始数据集，并训练专家模型使蒸馏样本与其类别分布对齐，同时设计了隐私预算分配策略。

Result: 大量实验表明，Libn在数据集效用和对抗成员推理攻击的鲁棒性方面显著优于最先进的差分隐私数据集蒸馏方法。

Conclusion: Libn建立了一个隐私保护数据集蒸馏的新范式，通过结合差分隐私生成的数据和优化的蒸馏技术，在保证隐私的同时显著提升了数据集的实用价值。

Abstract: Dataset distillation (DD) compresses large datasets into smaller ones while preserving the performance of models trained on them. Although DD is often assumed to enhance data privacy by aggregating over individual examples, recent studies reveal that standard DD can still leak sensitive information from the original dataset due to the lack of formal privacy guarantees. Existing differentially private (DP)-DD methods attempt to mitigate this risk by injecting noise into the distillation process. However, they often fail to fully leverage the original dataset, resulting in degraded realism and utility. This paper introduces \libn, a novel framework that addresses the key limitations of current DP-DD by leveraging DP-generated data. Specifically, \lib initializes the distilled dataset with DP-generated data to enhance realism. Then, generated data refines the DP-feature matching technique to distill the original dataset under a small privacy budget, and trains an expert model to align the distilled examples with their class distribution. Furthermore, we design a privacy budget allocation strategy to determine budget consumption across DP components and provide a theoretical analysis of the overall privacy guarantees. Extensive experiments show that \lib significantly outperforms state-of-the-art DP-DD methods in terms of both dataset utility and robustness against membership inference attacks, establishing a new paradigm for privacy-preserving dataset distillation.

</details>


### [57] [Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code](https://arxiv.org/abs/2511.09879)
*Catherine Xia,Manar H. Alalfi*

Main category: cs.CR

TL;DR: 通过使用静态分析工具过滤训练数据中的安全漏洞，构建安全数据集来训练AI编程助手，结果显示该方法能减少生成代码的安全问题，同时保持功能正确性。


<details>
  <summary>Details</summary>
Motivation: AI编程助手生成的代码经常包含基本安全漏洞，主要原因是训练数据集中存在漏洞。为了提高生成代码的内在质量，需要构建无漏洞的训练数据集。

Method: 使用静态分析工具过滤现有Python语料库，构建仅包含无漏洞函数的安全数据集，然后分别在原始数据集和过滤后数据集上训练两个基于transformer的模型。

Result: 在安全数据集上训练的模型生成的代码安全问题更少，同时保持了与原始模型相当的功能正确性。

Conclusion: 安全训练数据对于提高AI编程助手的可靠性至关重要，但还需要进一步改进模型架构和评估方法来强化这些结果。

Abstract: AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes.

</details>


### [58] [Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems](https://arxiv.org/abs/2511.10050)
*Go Tsuruoka,Takami Sato,Qi Alfred Chen,Kazuki Nomoto,Ryunosuke Kobayashi,Yuna Tanaka,Tatsuya Mori*

Main category: cs.CR

TL;DR: 提出了一种新型对抗性攻击方法ARP，利用逆反射材料在车辆头灯照射下激活攻击，结合了贴片攻击的高部署性和激光投影的隐蔽性，在动态场景中达到93.4%成功率，并开发了DPR Shield防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有交通标志识别系统的对抗攻击方法存在视觉可检测性或实施限制的问题，表明TSR系统存在未探索的漏洞表面，需要开发更隐蔽且易部署的攻击向量。

Method: 开发了逆反射模拟方法，使用黑盒优化来最大化攻击效果，利用逆反射材料仅在受害者头灯照射下激活的特性。

Result: ARP在35米动态场景中达到≥93.4%成功率，在真实世界条件下对商业TSR系统达到≥60%成功率，隐蔽性得分比先前贴片攻击高≥1.9%。

Conclusion: ARP攻击展示了结合高部署性和隐蔽性的新型攻击向量，同时提出的DPR Shield防御方法使用偏振滤光片对微棱镜贴片达到≥75%防御成功率。

Abstract: Traffic sign recognition plays a critical role in ensuring safe and efficient transportation of autonomous vehicles but remain vulnerable to adversarial attacks using stickers or laser projections. While existing attack vectors demonstrate security concerns, they suffer from visual detectability or implementation constraints, suggesting unexplored vulnerability surfaces in TSR systems. We introduce the Adversarial Retroreflective Patch (ARP), a novel attack vector that combines the high deployability of patch attacks with the stealthiness of laser projections by utilizing retroreflective materials activated only under victim headlight illumination. We develop a retroreflection simulation method and employ black-box optimization to maximize attack effectiveness. ARP achieves $\geq$93.4\% success rate in dynamic scenarios at 35 meters and $\geq$60\% success rate against commercial TSR systems in real-world conditions. Our user study demonstrates that ARP attacks maintain near-identical stealthiness to benign signs while achieving $\geq$1.9\% higher stealthiness scores than previous patch attacks. We propose the DPR Shield defense, employing strategically placed polarized filters, which achieves $\geq$75\% defense success rates for stop signs and speed limit signs against micro-prism patches.

</details>


### [59] [Enhanced Anonymous Credentials for E-Voting Systems](https://arxiv.org/abs/2511.10265)
*Tomasz Truderung*

Main category: cs.CR

TL;DR: 提出了一种增强匿名凭证机制的简单方法，使用完美隐藏承诺来链接凭证与选民身份，在保持永久隐私的同时加强选民与凭证的绑定关系。


<details>
  <summary>Details</summary>
Motivation: 匿名凭证方法虽然简单实用，但在结合其他安全特性（如使用第二设备的按意图投票验证和第二因素认证）时可能存在挑战，需要加强选民与凭证的绑定关系。

Method: 在匿名凭证机制基础上使用完美隐藏承诺来链接凭证与选民身份，确保已发布选票与选民身份不可链接，同时支持必要的选票投递和审计一致性检查。

Result: 该方法在保持选票永久隐私的同时，加强了选民与凭证的绑定关系，解决了简单匿名凭证方法与其他安全特性结合时的挑战。

Conclusion: 通过完美隐藏承诺增强匿名凭证机制，可以在不依赖高级密码技术的情况下实现永久隐私，同时确保必要的安全验证功能。

Abstract: A simple and practical method for achieving everlasting privacy in e-voting systems, without relying on advanced cryptographic techniques, is to use anonymous voter credentials. The simplicity of this approach may, however, create some challenges, when combined with other security features, such as cast-as-intended verifiability with second device and second-factor authentication.
  This paper considers a simple augmentation to the anonymous credential mechanism, using perfectly hiding commitments to link such credentials to the voter identities. This solution strengthens the binding between voters and their credentials while preserving everlasting privacy. It ensures that published ballots remain unlinkable to voter identities, yet enables necessary consistency checks during ballot casting and ballot auditing

</details>


### [60] [Enhanced Privacy Leakage from Noise-Perturbed Gradients via Gradient-Guided Conditional Diffusion Models](https://arxiv.org/abs/2511.10423)
*Jiayang Meng,Tao Huang,Hong Chen,Chen Hou,Guolong Zheng*

Main category: cs.CR

TL;DR: 本文提出了一种基于梯度引导条件扩散模型（GG-CDMs）的新方法，用于从泄露的梯度中重建私有图像，即使梯度受到噪声扰动防御机制的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过梯度传输和聚合同步模型，但这些梯度包含敏感训练数据，存在隐私风险。现有的梯度反演攻击在梯度被噪声扰动时重建性能显著下降，而噪声是常见的防御机制。

Method: 利用扩散模型固有的去噪能力绕过噪声扰动提供的部分保护，提出梯度引导条件扩散模型（GG-CDMs），无需目标数据分布的先验知识即可重建私有图像。

Result: 大量实验证明该方法在具有高斯噪声扰动的梯度下具有优越的重建性能，并验证了理论分析结果。

Conclusion: 该方法显著提高了在噪声防御机制下的攻击性能，为联邦学习中的隐私保护提供了新的挑战和启示。

Abstract: Federated learning synchronizes models through gradient transmission and aggregation. However, these gradients pose significant privacy risks, as sensitive training data is embedded within them. Existing gradient inversion attacks suffer from significantly degraded reconstruction performance when gradients are perturbed by noise-a common defense mechanism. In this paper, we introduce Gradient-Guided Conditional Diffusion Models (GG-CDMs) for reconstructing private images from leaked gradients without prior knowledge of the target data distribution. Our approach leverages the inherent denoising capability of diffusion models to circumvent the partial protection offered by noise perturbation, thereby improving attack performance under such defenses. We further provide a theoretical analysis of the reconstruction error bounds and the convergence properties of attack loss, characterizing the impact of key factors-such as noise magnitude and attacked model architecture-on reconstruction quality. Extensive experiments demonstrate our attack's superior reconstruction performance with Gaussian noise-perturbed gradients, and confirm our theoretical findings.

</details>


### [61] [On the Detectability of Active Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2511.10502)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: 本文对联邦学习中的主动梯度反转攻击进行了首次全面分析，提出了基于统计异常权重结构和损失梯度动态的轻量级客户端检测方法，能够在不修改FL训练协议的情况下有效检测攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然承诺保护客户端数据隐私，但梯度反转攻击能够重建客户端本地数据，特别是新型主动攻击声称比以往方法更隐蔽。本文旨在验证这些声称并开发有效的检测方法。

Method: 提出了基于统计异常权重结构和异常损失梯度动态的轻量级客户端检测技术，对四种最先进的梯度反转攻击进行了全面分析。

Result: 在多种配置下的广泛评估表明，所提出的方法能够使客户端有效检测主动梯度反转攻击，且无需修改联邦学习训练协议。

Conclusion: 本文证明了即使是最先进的主动梯度反转攻击也是可检测的，为联邦学习中的隐私保护提供了实用的客户端检测解决方案。

Abstract: One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol.

</details>


### [62] [How Worrying Are Privacy Attacks Against Machine Learning?](https://arxiv.org/abs/2511.10516)
*Josep Domingo-Ferrer*

Main category: cs.CR

TL;DR: 本文探讨了机器学习模型发布与个人数据隐私风险的关系，分析了针对预测性和生成性ML的主要隐私攻击类型，发现这些攻击在现实世界中的有效性低于文献表面显示的水平。


<details>
  <summary>Details</summary>
Motivation: 随着监管框架将个人数据保护扩展到机器学习领域，需要澄清披露训练好的ML模型是否真的等同于直接发布训练数据所带来的隐私风险。

Method: 作为概念性论文，通过分析针对预测性和生成性机器学习的主要隐私攻击家族，包括成员推断攻击、属性推断攻击和重建攻击。

Result: 分析表明，大多数隐私攻击在现实世界中的有效性低于文献表面显示的水平，攻击者需要主动发起攻击才能从训练好的模型中推断训练数据。

Conclusion: 直接披露训练好的机器学习模型所带来的隐私风险可能被高估，大多数隐私攻击在现实场景中的实际威胁有限。

Abstract: In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest.

</details>
