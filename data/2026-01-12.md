<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization](https://arxiv.org/abs/2601.05955)
*Yuliang Chen,Xi Lin,Jun Wu,Xiangrui Cai,Qiaolun Zhang,Xichun Fan,Jiapeng Xu,Xiu Su*

Main category: cs.DC

TL;DR: FaST-PT是一个联邦域泛化框架，通过多模态风格转移进行本地特征增强，使用双提示模块（全局提示和域提示）捕获知识，并通过域感知提示生成实现高效未见域适应。


<details>
  <summary>Details</summary>
Motivation: 现有联邦域泛化方法面临跨客户端数据异构性挑战，且通信和计算开销较大，需要更高效的分布式训练框架来提升模型在未见域上的泛化能力。

Method: 1. 轻量级多模态风格转移方法，在文本监督下转换图像嵌入以扩展训练数据分布；2. 双提示模块分解为全局提示（捕获跨客户端增强嵌入的通用知识）和域提示（捕获本地数据的域特定知识）；3. 域感知提示生成自适应地为每个样本生成合适提示，通过知识融合促进未见域适应。

Result: 在PACS和DomainNet等四个跨域基准数据集上的实验表明，FaST-PT优于FedDG-GA和DiPrompt等最先进的联邦域泛化方法。消融研究进一步验证了其有效性和效率。

Conclusion: FaST-PT通过本地特征增强和高效未见域适应，有效解决了联邦域泛化中的数据异构性和计算开销问题，为分布式跨域学习提供了新框架。

Abstract: Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing](https://arxiv.org/abs/2601.05298)
*Yeongbin Cha,Namjung Kim*

Main category: cs.AI

TL;DR: 本文提出了一种本体引导、方程中心的框架，将大语言模型与增材制造数学知识图谱相结合，实现可靠的知识提取和外推建模，通过形式化本体编码方程、变量和语义关系，并引入置信感知外推评估。


<details>
  <summary>Details</summary>
Motivation: 增材制造领域现有的数据驱动方法受限于碎片化的知识表示和稀疏数据条件下的不可靠外推，需要一种能够整合领域知识、支持结构化推理和稳定外推的框架。

Method: 提出本体引导、方程中心的框架，将LLMs与增材制造数学知识图谱（AM-MKG）紧密集成，通过形式化本体编码方程、变量、假设及其语义关系，将非结构化文献转化为机器可解释表示，并基于知识图谱子图条件化LLM方程生成，引入置信感知外推评估整合外推距离、统计稳定性和物理一致性。

Result: 本体引导提取显著提高了提取知识的结构连贯性和定量可靠性，子图条件化的方程生成相比无引导的LLM输出产生了更稳定和物理一致的外推结果，展示了知识图谱增强的LLMs作为增材制造外推建模可靠工具的潜力。

Conclusion: 该工作建立了一个统一的流程，用于本体驱动的知识表示、方程中心推理和基于置信度的外推评估，突出了知识图谱增强的LLMs在增材制造外推建模中的可靠工具潜力。

Abstract: Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.

</details>


### [3] [Effects of personality steering on cooperative behavior in Large Language Model agents](https://arxiv.org/abs/2601.05302)
*Mizuki Sakai,Mizuki Yokoyama,Wakaba Tateishi,Genki Ichinose*

Main category: cs.AI

TL;DR: 研究通过重复囚徒困境实验，探究人格引导对LLM智能体合作行为的影响，发现宜人性是促进合作的主导因素，人格引导更多是行为偏差而非确定性控制机制。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明为人格特质可以影响LLM行为，但人格引导在受控条件下如何影响合作行为仍不清楚。本研究旨在探究人格引导对LLM智能体在战略互动中合作行为的影响。

Method: 基于大五人格框架，首先测量GPT-3.5-turbo、GPT-4o和GPT-5三种模型的基本人格特征，然后在重复囚徒困境游戏中比较基线条件和人格引导条件下的行为，并分析独立操纵每个人格维度到极端值的影响。

Result: 宜人性是所有模型中促进合作的主导因素，其他人格特质影响有限。明确的人格信息能增加合作，但也可能增加被剥削的脆弱性，特别是在早期模型中。后期模型表现出更具选择性的合作。

Conclusion: 人格引导更多是作为行为偏差而非确定性控制机制，不同代际的LLM模型对人格引导的反应存在差异，后期模型表现出更复杂的合作策略。

Abstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

</details>


### [4] [Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings](https://arxiv.org/abs/2601.05330)
*Tengwei Song,Long Yin,Zhen Han,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出Hyper-Enz模型，通过知识图谱嵌入和超图transformer预测酶-底物相互作用，相比传统模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有酶-底物相互作用预测方法依赖稀疏、人工维护的数据库，训练数据不足限制了模型的泛化能力。化学反应方程数据更易获取且更丰富，但多化合物与同一酶的复杂关系模式传统模型难以捕捉。

Method: 将化学反应方程表示为(底物、酶、产物)三元组构建知识图谱，提出Hyper-Enz模型：集成超图transformer和KGE模型学习涉及多个底物和产物的超边表示，引入多专家范式指导酶-底物相互作用学习。

Result: 实验结果显示显著改进：酶检索准确率相对提升达88%，配对级别预测提升30%，证明方法的有效性。

Conclusion: Hyper-Enz模型通过知识图谱和超图表示有效解决了酶-底物相互作用预测中数据稀疏和复杂关系建模的问题，为生物化学和代谢工程提供了更准确的预测工具。

Abstract: Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.

</details>


### [5] [The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models](https://arxiv.org/abs/2601.05376)
*Tassallah Abdullahi,Shrestha Ghosh,Hamish S Fraser,Daniel León Tramontini,Adeel Abbasi,Ghada Bourjeily,Carsten Eickhoff,Ritambhara Singh*

Main category: cs.AI

TL;DR: 研究发现：在临床决策中，人物角色条件化对LLM性能的影响是系统性的、情境依赖的、非单调的，不能简单保证安全性或专业性提升


<details>
  <summary>Details</summary>
Motivation: 探讨人物角色条件化在临床决策中的实际影响，挑战其单调提升安全性和专业性的假设

Method: 系统评估基于角色的控制，考察专业角色（急诊医生、护士）和互动风格（大胆vs谨慎）在不同模型和医疗任务中的影响，使用多维评估指标

Result: 医疗角色在重症监护任务中提升性能（准确性和校准度提升约20%），但在初级护理中降低性能；互动风格影响风险倾向但高度依赖模型；人类临床医生对安全合规性有中等一致性但推理质量信心低

Conclusion: 人物角色作为行为先验引入情境依赖的权衡，而非安全或专业性的保证，需要更细致的评估框架

Abstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

</details>


### [6] [On the Effect of Cheating in Chess](https://arxiv.org/abs/2601.05386)
*Daniel Keren*

Main category: cs.AI

TL;DR: 研究评估在棋局中有限次数作弊（使用强大软件建议）可能带来的表现提升，开发算法并在常用国际象棋引擎上测试


<details>
  <summary>Details</summary>
Motivation: 国际象棋中使用强大软件建议作弊已成为严重问题，甚至影响到最高级别比赛。与以往主要关注作弊检测的研究不同，本文旨在评估在比赛中有限次数作弊可能带来的表现提升

Method: 开发算法并在常用国际象棋引擎上进行测试，量化有限次数作弊对棋局表现的影响

Result: 通过算法测试，量化了在棋局中有限次数使用软件建议作弊可能带来的表现提升效果

Conclusion: 评估作弊效果对于有效遏制和检测作弊行为至关重要，本文的研究目标不是协助作弊者，而是为反作弊工作提供重要依据

Abstract: Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}

</details>


### [7] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: ART是一种用于声明验证的分层方法，通过构建支持与反对论据树，使用LLM作为裁判进行成对比较，实现透明、可争议的决策过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂决策中具有潜力，但其不透明性阻碍了在高风险环境中的应用。现有方法缺乏忠实解释和错误纠正机制，影响了可信度。

Method: 提出ART（自适应推理树）方法：从根声明开始，分支为支持和反对的子论据。通过LLM裁判对子论据进行成对比较，自底向上确定论据强度，系统化推导最终透明且可争议的裁决。

Result: 在多个数据集上验证ART，分析不同论据生成器和比较策略。结果显示ART的结构化推理优于强基线方法，为可解释的声明验证设立了新基准。

Conclusion: ART通过分层推理树结构解决了LLM决策的透明度问题，提供了更可靠、清晰的决策过程，为高风险环境中的可信AI决策提供了新方法。

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [8] [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)
*Zixuan Xiao,Jun Ma,Siwei Zhang*

Main category: cs.AI

TL;DR: MMUEChange是一个多模态智能体框架，通过模块化工具包和模态控制器实现异构城市数据的灵活集成，用于复杂城市环境变化分析，相比最佳基线方法任务成功率提升46.7%。


<details>
  <summary>Details</summary>
Motivation: 当前城市环境变化分析方法（特别是遥感变化检测）通常依赖僵化的单模态分析，无法有效处理复杂的城市变化场景，需要更灵活的多模态集成方法。

Method: 提出MMUEChange多模态智能体框架，包含模块化工具包和核心模块——模态控制器，实现跨模态和模态内对齐，能够灵活整合异构城市数据。

Result: 案例研究显示：纽约小型社区公园增加反映本地绿地建设；香港跨区域水污染扩散指向协调水管理需求；深圳露天垃圾场减少，夜间经济活动与不同类型垃圾关联模式差异反映城市压力差异。相比最佳基线，任务成功率提升46.7%，有效缓解幻觉问题。

Conclusion: MMUEChange框架能够支持具有现实政策意义的复杂城市变化分析任务，为可持续城市发展提供有力工具。

Abstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.

</details>


### [9] [The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm](https://arxiv.org/abs/2601.05500)
*Aparna Elangovan,Lei Xu,Mahsa Elyasi,Ismail Akdulum,Mehmet Aksakal,Enes Gurun,Brian Hur,Saab Mansour,Ravid Shwartz Ziv,Karin Verspoor,Dan Roth*

Main category: cs.AI

TL;DR: 论文提出概率评估范式，强调医学AI基准测试中忽略标注者不确定性会导致误导性结论，引入期望准确率和期望F1来估计专家在标注变异性下的表现，建议按标注确定性分层评估系统能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统基准测试（特别是医学领域）通常忽略专家标注答案中的不确定性，而医学领域不确定性普遍存在。忽略这种不确定性可能导致错误结论，例如认为非专家与专家表现相似。

Method: 提出概率评估范式，引入期望准确率和期望F1概念，从理论上分析标注确定性对评估结果的影响。建议按标注答案概率（通常通过专家标注一致性率衡量）对结果进行分层评估。

Result: 研究表明：高确定性标注对专家获得高分几乎总是必要的；在标注变异性高的数据集中，随机标注者与专家之间可能几乎没有差异。当总体性能低于80%阈值时，分层评估变得尤为关键。

Conclusion: 在评估AI系统能力时，应通过分层评估来考虑标注不确定性，特别是在高确定性区间进行性能比较更为可靠，这能减轻不确定性这一关键混杂因素的影响。

Abstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.
  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.

</details>


### [10] [Explainable AI: Learning from the Learners](https://arxiv.org/abs/2601.05525)
*Ricardo Vinuesa,Steven L. Brunton,Gianmarco Mengaldo*

Main category: cs.AI

TL;DR: 本文提出将可解释人工智能（XAI）与因果推理结合，通过"向学习者学习"的方式，从基础模型中提取因果机制，指导稳健设计与控制，支持高风险应用中的信任与问责。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在科学和工程任务中已超越人类表现，但其内部表示往往不透明。需要解决AI黑箱问题，通过可解释性促进人类与AI在科学和工程领域的协作。

Method: 将可解释人工智能（XAI）与因果推理相结合，利用基础模型和可解释性方法提取因果机制，指导稳健设计与控制，支持高风险应用中的信任与问责。

Result: 展示了XAI与因果推理结合在发现、优化和认证三个方面的应用潜力：提取因果机制、指导稳健设计与控制、支持高风险应用中的信任与问责。

Conclusion: XAI可作为人类-AI协作的统一框架，但面临解释的忠实性、泛化性和可用性等挑战。通过结合因果推理，能够实现"向学习者学习"，促进科学和工程领域的人机协作。

Abstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.

</details>


### [11] [Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making](https://arxiv.org/abs/2601.05529)
*Jua Han,Jaeyoon Seo,Jungbin Min,Jean Oh,Jihie Kim*

Main category: cs.AI

TL;DR: 该研究系统评估了大型语言模型在安全关键场景中的表现，发现即使99%的准确率在机器人应用中也可能导致灾难性后果，因为1%的错误就足以危及生命。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被整合到机器人决策中，物理风险维度增加，单个错误指令可能直接危及人类安全。研究旨在系统评估LLMs在即使微小错误也是灾难性的场景中的表现。

Method: 通过火灾疏散场景的定性评估识别关键失败案例，设计7个定量评估任务，分为：完整信息任务（使用ASCII地图最小化解释歧义）、不完整信息任务（需要推断缺失上下文）、安全导向空间推理任务（使用自然语言评估生命威胁情境下的安全决策）。对多种LLMs和视觉语言模型进行基准测试。

Result: 结果显示严重漏洞：多个模型在ASCII导航中达到0%成功率；在模拟消防演习中，模型指示机器人向危险区域移动而非紧急出口。研究强调1%失败率如何升级为灾难性结果，99%准确率在机器人应用中具有误导性。

Conclusion: 当前LLMs尚未准备好直接部署到安全关键系统中。即使最先进的模型也无法保证安全，绝对依赖它们会带来不可接受的风险。99%的准确率在机器人应用中具有危险性，因为每百次执行中可能有一次导致灾难性伤害。

Abstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how "rare" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.

</details>


### [12] [WildSci: Advancing Scientific Reasoning from In-the-Wild Literature](https://arxiv.org/abs/2601.05567)
*Tengxiao Liu,Deepak Nathani,Zekun Li,Kevin Yang,William Yang Wang*

Main category: cs.AI

TL;DR: WildSci是一个从同行评审文献自动合成的领域特定科学问题数据集，涵盖9个科学学科和26个子领域，通过将复杂科学推理任务转化为多项选择题格式，支持可扩展训练和强化学习微调。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学和编程等领域的推理进展显著，但在医学和材料科学等科学领域的进展有限，主要原因是数据集覆盖不足和开放科学问题的复杂性。需要解决科学领域高质量数据和客观评估指标缺乏的问题。

Method: 1. 从同行评审文献自动合成领域特定科学问题数据集WildSci，涵盖9个科学学科和26个子领域；2. 将复杂科学推理任务转化为多项选择题格式，提供明确的奖励信号；3. 应用强化学习对模型进行微调，并分析训练动态，包括领域特定性能变化、响应行为和泛化趋势。

Result: 在一系列科学基准测试上的实验证明了数据集和方法的有效性。WildSci数据集已公开发布，支持科学推理研究的可扩展和可持续发展。

Conclusion: WildSci数据集通过自动合成同行评审文献中的科学问题，解决了科学领域高质量训练数据缺乏的问题，为科学推理模型的可扩展训练提供了有效解决方案，推动了科学领域大语言模型推理能力的发展。

Abstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.

</details>


### [13] [Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection](https://arxiv.org/abs/2601.05578)
*Cooper Lin,Yanting Zhang,Maohao Ran,Wei Xue,Hongwei Fan,Yibo Xu,Zhenglin Wan,Sirui Han,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 本文提出了一种使用强化学习对轻量级语言模型进行后训练的方法，专门用于电子商务欺诈检测任务，仅使用原始交易数据，在真实交易数据集上取得了显著的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台和支付解决方案提供商面临日益复杂的欺诈方案，但尽管大型语言模型在理论上具有潜力，其在真实金融欺诈检测中的应用仍未被充分探索，处理特定领域电子商务交易数据的实际效果也尚未得到实证验证。

Method: 采用强化学习框架，使用组序列策略优化算法结合基于规则的奖励系统，对多种规模的轻量级语言模型进行后训练，专门用于欺诈检测任务，仅使用原始交易数据，鼓励模型探索文本交易数据中嵌入的多样信任和风险信号。

Result: 实验结果表明该方法有效，后训练的语言模型在保留测试数据上实现了显著的F1分数提升。性能改进主要归因于强化学习固有的探索机制，使模型能够发现超越传统工程特征的新型欺诈指标。

Conclusion: 该方法成功弥合了传统机器学习局限性与大型语言模型在欺诈检测中未开发潜力之间的差距，为电子商务欺诈检测提供了一种新颖有效的方法。

Abstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.

</details>


### [14] [PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility](https://arxiv.org/abs/2601.05739)
*G M Shahariar,Zabir Al Nazi,Md Olid Hasan Bhuiyan,Zhouxing Shi*

Main category: cs.AI

TL;DR: 本文提出了PII-VisBench基准，用于评估视觉语言模型在个人身份信息泄露方面的安全性，特别关注个体在线存在程度对隐私对齐的影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要将隐私视为静态提取任务，忽略了个人在线存在程度对视觉语言模型隐私对齐的影响。随着VLMs越来越多地应用于隐私关键领域，需要更全面的评估框架来理解模型在不同可见度个体上的隐私保护表现。

Method: 构建了包含4000个独特探针的PII-VisBench基准，将200个研究对象根据在线信息可用性分为高、中、低、零可见度四个类别。评估了18个开源VLMs（0.3B-32B），使用两个关键指标：拒绝率（PII探查询问被拒绝的百分比）和条件PII泄露率（非拒绝响应中包含PII的比例）。

Result: 模型表现出一致模式：随着主体可见度降低，拒绝率增加，PII泄露率从高可见度的9.10%降至低可见度的5.34%。模型更倾向于泄露高可见度主体的PII，存在显著的模型家族异质性和PII类型差异。改写和越狱式提示暴露了攻击和模型相关的失败。

Conclusion: 研究揭示了在线存在程度对VLM隐私保护的重要影响，强调了需要基于可见度的安全性评估和训练干预，以改善模型在不同隐私场景下的表现。

Abstract: Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.

</details>


### [15] [A Causal Information-Flow Framework for Unbiased Learning-to-Rank](https://arxiv.org/abs/2601.05590)
*Haoming Gong,Qingyao Ai,Zhihao Tao,Yongfeng Zhang*

Main category: cs.AI

TL;DR: 提出基于因果学习的新型排序框架，结合结构因果模型与信息论工具，通过互信息泄漏度量偏差并作为正则化项，使用双重稳健估计器提升风险估计可靠性，有效处理点击数据中的多种偏差问题。


<details>
  <summary>Details</summary>
Motivation: 在网页搜索和推荐系统中，用户点击数据存在严重偏差（位置偏差、选择偏差、信任偏差），现有无偏学习排序方法主要纠正位置偏差且依赖倾向性估计，无法测量剩余偏差、提供风险保证或联合处理多种偏差源。

Method: 结合结构因果模型与信息论工具，使用条件互信息度量偏差泄漏到学习相关性估计中的程度，将其作为正则化项加入模型训练以减少偏差，同时引入双重稳健估计器确保更可靠的风险估计。

Result: 在标准学习排序基准测试中，该方法持续减少测量的偏差泄漏并提高排序性能，特别是在位置偏差和信任偏差等多种偏差强烈交互的现实场景中表现更优。

Conclusion: 提出的因果学习排序框架通过结构因果模型和信息论工具，有效识别和减少点击数据中的多种偏差，提高相关性估计的准确性和排序性能，为无偏学习排序提供了更全面的解决方案。

Abstract: In web search and recommendation systems, user clicks are widely used to train ranking models. However, click data is heavily biased, i.e., users tend to click higher-ranked items (position bias), choose only what was shown to them (selection bias), and trust top results more (trust bias). Without explicitly modeling these biases, the true relevance of ranked items cannot be correctly learned from clicks. Existing Unbiased Learning-to-Rank (ULTR) methods mainly correct position bias and rely on propensity estimation, but they cannot measure remaining bias, provide risk guarantees, or jointly handle multiple bias sources. To overcome these challenges, this paper introduces a novel causal learning-based ranking framework that extends ULTR by combining Structural Causal Models (SCMs) with information-theoretic tools. SCMs specify how clicks are generated and help identify the true relevance signal from click data, while conditional mutual information, measures how much bias leaks into the
  learned relevance estimates. We use this leakage measure to define a rigorous notion of disentanglement and include it as a regularizer during model training to reduce bias. In addition, we incorporate a causal inference estimator, i.e., doubly robust estimator, to ensure more reliable risk estimation. Experiments on standard Learning-to-Rank benchmarks show that our method consistently reduces measured bias leakage and improves ranking performance, especially in realistic scenarios where multiple biases-such as position and trust bias-interact strongly.

</details>


### [16] [Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion](https://arxiv.org/abs/2601.05629)
*Jiapu Wang,Xinghe Cheng,Zezheng Wu,Ruiqi Ma,Rui Wang,Zhichao Yan,Haoran Luo,Yuhao Jiang,Kai Sun*

Main category: cs.AI

TL;DR: 本文提出CPSR框架，通过查询依赖掩码模块和全局语义评分模块，同时捕获知识图谱的结构和语义信息，以解决归纳式知识图谱补全中的噪声结构信息和长距离依赖问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱补全方法在处理新兴实体时效果不佳，而现有的归纳式KGC方法虽然能处理新兴实体和关系，但仍面临两个主要挑战：1）在推理过程中容易受到噪声结构信息的影响；2）难以捕捉推理路径中的长距离依赖关系。

Method: 提出了CPSR框架，包含两个核心模块：1）查询依赖掩码模块：自适应地掩码噪声结构信息，同时保留与目标密切相关的关键信息；2）全局语义评分模块：评估推理路径中各个节点的个体贡献以及节点集合的整体影响。

Result: 实验结果表明，CPSR在归纳式知识图谱补全任务中取得了最先进的性能。

Conclusion: CPSR框架通过同时捕获知识图谱的结构和语义信息，有效解决了归纳式KGC中的噪声结构信息和长距离依赖问题，提升了推理性能。

Abstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.

</details>


### [17] [GenCtrl -- A Formal Controllability Toolkit for Generative Models](https://arxiv.org/abs/2601.05637)
*Emily Cheng,Carmen Amo Alonso,Federico Danieli,Arno Blaas,Luca Zappella,Pau Rodriguez,Xavier Suau*

Main category: cs.AI

TL;DR: 本文提出了一个理论框架来评估生成模型的可控性，通过对话场景下的可控集估计算法，为模型控制提供了分布无关的误差保证，揭示了模型可控性的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的普及，对生成过程的细粒度控制需求日益增长。然而，尽管从提示到微调的各种控制方法不断涌现，一个根本问题仍未解决：这些模型是否真正可控？本文旨在通过理论框架正式回答这个问题。

Method: 将人机交互建模为控制过程，提出了一种新颖的算法来估计对话场景下模型的可控集。该算法提供形式化的估计误差保证，具有分布无关性，仅假设输出有界，适用于任何黑盒非线性控制系统（即任何生成模型）。

Result: 在语言模型和文本到图像生成的不同对话控制任务上进行实证验证。结果显示模型可控性出人意料地脆弱，且高度依赖实验设置。

Conclusion: 研究强调了进行严格可控性分析的必要性，应将重点从单纯尝试控制转向首先理解其基本限制。模型可控性具有脆弱性，需要更深入的理论分析。

Abstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.

</details>


### [18] [HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation](https://arxiv.org/abs/2601.05656)
*Rongxin Chen,Tianyu Wu,Bingbing Xu,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: HAG是一个分层智能体生成框架，通过两阶段决策过程解决智能体初始化问题，实现宏观分布对齐和微观一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能体初始化方法存在局限性：基于静态数据检索的方法无法适应未见主题，而基于LLM生成的方法缺乏宏观分布意识，导致微观属性与现实不一致。需要一种既能适应新主题又能保持宏观分布一致性的框架。

Method: 提出HAG分层智能体生成框架，将群体生成形式化为两阶段决策过程：1) 使用世界知识模型推断分层条件概率构建主题自适应树，实现宏观分布对齐；2) 基于真实世界数据进行实例化和智能体增强，确保微观一致性。

Result: 实验表明HAG显著优于代表性基线方法，平均减少群体对齐误差37.7%，提升社会学一致性18.8%。建立了多领域基准和全面的PACE评估框架。

Conclusion: HAG框架通过分层方法有效解决了智能体初始化中的主题适应性和一致性挑战，为基于智能体建模提供了高质量的初始化解决方案。

Abstract: High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.

</details>


### [19] [Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models](https://arxiv.org/abs/2601.05693)
*Zenghao Duan,Liang Pang,Zihao Wei,Wenbin Duan,Yuxin Tian,Shicheng Xu,Jingcheng Deng,Zhiyi Yin,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出"循环推理"这一新的失败模式，开发LoopBench数据集进行分析，发现循环由推理障碍触发并通过自增强注意力机制维持，使用CUSUM算法实现早期预测。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时缩放技术取得成功，但大型推理模型(LRMs)经常遇到重复循环，导致计算浪费和推理失败。论文旨在识别和分析这种独特的"循环推理"失败模式。

Method: 1. 引入LoopBench数据集捕捉数值循环和陈述循环两种类型；2. 从机制上表征循环推理为具有明显边界的状态崩溃；3. 使用累积和(CUSUM)算法捕捉循环前兆进行早期预测。

Result: 实验验证了CUSUM算法在多种LRMs上的准确性，并阐明了长链推理的稳定性。揭示了循环推理由推理障碍触发，随后通过自增强的V形注意力机制持续存在。

Conclusion: 循环推理是一种独特的失败模式，表现为自增强陷阱，生成内容作为自身重复的逻辑前提。通过CUSUM算法可以早期预测循环，提高大型推理模型的效率和稳定性。

Abstract: Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.

</details>


### [20] [Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.05705)
*Ali Farjami,Luca Redondi,Marco Valentino*

Main category: cs.AI

TL;DR: 该论文提出了一种逻辑参数化的神经符号推理框架，将逻辑形式作为可控组件而非固定背景，通过高阶逻辑嵌入多种经典和非经典形式化方法，在规范推理领域比较不同逻辑策略的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定的逻辑形式化方法，限制了神经符号推理的鲁棒性和适应性。需要一种更灵活的方法来处理不同领域的推理需求。

Method: 采用LogiKEy方法学，将多种经典和非经典逻辑形式嵌入高阶逻辑中，比较逻辑外部方法（通过公理编码规范要求）和逻辑内部方法（规范模式从逻辑内置结构中产生）的效果。

Result: 实验表明逻辑内部策略能持续提升性能并产生更高效的混合证明。逻辑有效性具有领域依赖性：一阶逻辑擅长常识推理，道义逻辑和模态逻辑在伦理领域表现优异。

Conclusion: 将逻辑作为神经符号架构中的一等参数化元素，能够实现更鲁棒、模块化和适应性强的推理系统。

Abstract: Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.

</details>


### [21] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner是一个具有显式内存控制的分层多智能体框架，通过解耦高层协调与子任务执行，并利用结构化经验记忆和强化学习重用协调经验，解决了长时程协作中的内存效率问题。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在处理复杂知识密集型任务时表现出潜力，但中心化架构中的中央智能体由于缺乏内存管理，导致上下文膨胀、错误累积和跨任务泛化能力差，影响了长时程协作的稳定性。

Method: 提出StackPlanner分层多智能体框架：1）通过主动任务级内存控制解耦高层协调与子任务执行；2）利用结构化经验记忆存储协调经验；3）通过强化学习检索和利用可重用的协调经验。

Result: 在多个深度搜索和智能体系统基准测试上的实验表明，该方法能够实现可靠的长时程多智能体协作，有效解决了内存效率问题和协调经验重用问题。

Conclusion: StackPlanner通过显式内存控制和协调经验重用机制，显著提升了多智能体系统在复杂任务中的长时程协作能力，为解决中心化架构中的内存管理问题提供了有效方案。

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [22] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: TowerMind是一个基于塔防游戏的轻量级多模态环境，用于评估大语言模型在实时策略游戏中的长期规划和决策能力，同时支持幻觉检测和高定制性。


<details>
  <summary>Details</summary>
Motivation: 现有实时策略游戏环境要么计算需求高，要么缺乏文本观察支持，限制了其用于大语言模型评估。需要一种既能保持RTS游戏评估优势，又具有低计算需求和多模态观察空间的环境。

Method: 基于塔防游戏子类型构建TowerMind环境，包含像素、文本和结构化游戏状态三种观察表示，支持模型幻觉评估和高定制性。设计了五个基准关卡来评估不同多模态输入设置下的LLM表现。

Result: 实验显示LLM与人类专家在能力和幻觉维度上存在明显性能差距。LLM存在规划验证不足、决策缺乏多终局性和行动使用效率低等关键限制。同时评估了Ape-X DQN和PPO两种经典强化学习算法。

Conclusion: TowerMind通过轻量级多模态设计补充了现有RTS游戏环境，为AI智能体领域引入了新的基准测试平台。源代码已在GitHub上公开。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


### [23] [Open-Vocabulary 3D Instruction Ambiguity Detection](https://arxiv.org/abs/2601.05991)
*Jiayu Ding,Haoran Tang,Ge Li*

Main category: cs.AI

TL;DR: 该研究首次定义了开放词汇3D指令歧义检测任务，构建了包含700多个3D场景和约22k指令的大规模基准Ambi3D，并提出两阶段框架AmbiVer来解决现有3D大语言模型在判断指令歧义性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，语言歧义可能导致严重后果，但现有具身AI研究大多忽视这一问题，假设指令清晰并专注于执行而非确认。为解决这一关键安全缺口，需要系统性地检测3D场景中的指令歧义性。

Method: 提出AmbiVer两阶段框架：第一阶段从多个视角收集明确的视觉证据，第二阶段使用这些证据指导视觉语言模型判断指令歧义性。同时构建了包含700多个3D场景和约22k指令的大规模基准Ambi3D。

Result: 分析发现最先进的3D大语言模型难以可靠判断指令是否歧义。AmbiVer框架在实验中表现出有效性，为解决该任务提供了可行方案。代码和数据集已公开。

Conclusion: 该研究首次定义了开放词汇3D指令歧义检测任务，构建了相关基准，并提出了有效的解决方案，为构建更安全、更可信的具身AI系统铺平了道路。

Abstract: In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models](https://arxiv.org/abs/2601.05339)
*Badhan Chandra Das,Md Tasnim Jawad,Joaquin Molto,M. Hadi Amini,Yanzhao Wu*

Main category: cs.CR

TL;DR: 本文提出了MJAD-MLLMs框架，系统分析多模态大语言模型的多轮越狱攻击，并提出基于多LLM的防御机制FragGuard，通过实验验证攻击和防御的有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然能高精度执行多模态任务，但存在严重安全漏洞，特别是容易受到精心设计的越狱攻击，这些攻击可以操纵模型行为并绕过安全约束，对生成式人工智能研究构成严重威胁。

Method: 1. 提出多轮越狱攻击方法，利用多轮提示利用MLLMs的漏洞；2. 提出FragGuard防御机制，采用片段优化和多LLM协同防御；3. 在多个开源和闭源SOTA MLLMs及基准数据集上进行广泛实验评估。

Result: 通过实验验证了所提多轮越狱攻击的有效性，以及FragGuard防御机制在缓解MLLMs越狱攻击方面的性能，并与现有技术进行了比较分析。

Conclusion: MJAD-MLLMs框架为MLLMs的安全分析提供了系统方法，提出的攻击和防御技术有助于理解和增强多模态大语言模型的安全性，为生成式人工智能的安全研究做出贡献。

Abstract: In recent years, the security vulnerabilities of Multi-modal Large Language Models (MLLMs) have become a serious concern in the Generative Artificial Intelligence (GenAI) research. These highly intelligent models, capable of performing multi-modal tasks with high accuracy, are also severely susceptible to carefully launched security attacks, such as jailbreaking attacks, which can manipulate model behavior and bypass safety constraints. This paper introduces MJAD-MLLMs, a holistic framework that systematically analyzes the proposed Multi-turn Jailbreaking Attacks and multi-LLM-based defense techniques for MLLMs. In this paper, we make three original contributions. First, we introduce a novel multi-turn jailbreaking attack to exploit the vulnerabilities of the MLLMs under multi-turn prompting. Second, we propose a novel fragment-optimized and multi-LLM defense mechanism, called FragGuard, to effectively mitigate jailbreaking attacks in the MLLMs. Third, we evaluate the efficacy of the proposed attacks and defenses through extensive experiments on several state-of-the-art (SOTA) open-source and closed-source MLLMs and benchmark datasets, and compare their performance with the existing techniques.

</details>


### [25] [Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models](https://arxiv.org/abs/2601.05445)
*Songze Li,Ruishi He,Xiaojun Jia,Jun Wang,Zhihui Fu*

Main category: cs.CR

TL;DR: Mastermind是一个多轮越狱攻击框架，采用动态自改进的闭环方法，显著提升了对大型语言模型的攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱攻击存在关键局限性：难以在长交互中保持连贯进展，依赖僵化预定义模式，无法适应LLM的动态对话状态

Method: 采用规划-执行-反思的闭环架构，分层规划将高层攻击目标与低层战术执行解耦，通过知识库自主发现和优化攻击模式，动态重组和适应攻击向量

Result: 在GPT-5和Claude 3.7 Sonnet等先进模型上的实验表明，Mastermind显著优于现有基线，攻击成功率和危害性评分大幅提高，对多种高级防御机制表现出显著韧性

Conclusion: Mastermind框架通过动态自改进方法有效解决了现有多轮越狱攻击的局限性，为理解和防御LLM安全漏洞提供了新视角

Abstract: Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind significantly outperforms existing baselines, achieving substantially higher attack success rates and harmfulness ratings. Moreover, our framework exhibits notable resilience against multiple advanced defense mechanisms.

</details>


### [26] [Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning](https://arxiv.org/abs/2601.05466)
*Zhaoqi Wang,Zijian Zhang,Daqing He,Pengtao Kou,Xin Li,Jiamou Liu,Jincheng An,Yong Liu*

Main category: cs.CR

TL;DR: iMIST是一种新型自适应越狱攻击方法，通过伪装恶意查询为正常工具调用来绕过内容过滤器，同时采用交互式渐进优化算法在多轮对话中动态提升响应危害性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种应用中表现出色，但它们仍然容易受到越狱攻击，导致生成违反人类价值观和安全准则的有害响应。现有的防御机制在面对复杂的对抗策略时显得不足。

Method: iMIST方法结合了两个关键策略：1）将恶意查询伪装成正常的工具调用来绕过内容过滤器；2）采用交互式渐进优化算法，通过实时危害性评估指导的多轮对话动态提升响应危害性。

Result: 实验表明，iMIST在广泛使用的模型上实现了更高的攻击成功率，同时保持较低的拒绝率，揭示了当前LLM安全机制的关键漏洞。

Conclusion: iMIST的成功攻击揭示了当前大语言模型安全机制存在严重漏洞，强调了开发更强大防御策略的紧迫性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\underline{i}nteractive \underline{M}ulti-step \underline{P}rogre\underline{s}sive \underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.

</details>


### [27] [Blockchain Verifiable Proof of Quantum Supremacy as a Trigger for Quantum-Secure Signatures](https://arxiv.org/abs/2601.05534)
*Nicholas Papadopoulos*

Main category: cs.CR

TL;DR: 该论文提出了一种基于以太坊智能合约的解决方案，通过生成经典计算机难以破解的数学难题来检测量子计算优势，并在检测到量子优势时自动触发量子安全备用协议，以保护区块链资产免受量子计算威胁。


<details>
  <summary>Details</summary>
Motivation: 量子计算的进步威胁到经典密码学方案的安全性，一旦实现量子计算优势，现有区块链安全标准将变得脆弱，导致区块链资产（货币、数据等）面临欺诈和盗窃风险。需要一种机制来检测量子计算优势并确保向量子安全标准的平稳过渡。

Method: 提出并实现了一个可部署在以太坊区块链上的智能合约，该合约能够通过概率性生成大型难以分解的数字来创建经典计算机难以解决的数学难题。该合约具有双重功能：(1) 通过验证这些难题的解决方案来建立无信任、无偏见的量子计算优势证明机制；(2) 在检测到量子计算优势时触发量子安全备用协议来保护用户资金。

Result: 该机制展示了识别密码学漏洞的能力，并确保向量子安全标准的平稳过渡。智能合约能够在量子计算优势实现时自动检测并触发保护措施，为后量子时代的区块链资产提供安全保障。

Conclusion: 该研究为解决量子计算对区块链安全的威胁提供了一种实用解决方案。通过智能合约机制，能够在量子计算优势实现时及时检测并触发量子安全协议，保护区块链资产免受量子攻击，为后量子时代的区块链安全奠定了基础。

Abstract: Blockchain is a decentralized, distributed ledger technology that ensures transparency, security, and immutability through cryptographic techniques. However, advancements in quantum computing threaten the security of classical cryptographic schemes, jeopardizing blockchain integrity once cryptographic quantum supremacy is achieved. This milestone, defined here as the realization of quantum computers to solve practical cryptographic problems, would render existing security standards vulnerable, exposing blockchain assets (currency, data, etc.) to fraud and theft. To address this risk, we propose and implement a smart contract deployable on the Ethereum blockchain, having the ability to run applications on its blockchain, that generates classically intractable puzzles by probabilistically generating large, hard-to-factor numbers without requiring secret information. This contract then serves two purposes: to establish a mechanism (1) for a trustless, unbiased proof of cryptographic quantum supremacy by verifying solutions to these puzzles, and (2) to protect user funds on Ethereum by triggering quantum-secure fallback protocols upon detecting cryptographic quantum supremacy, since it is desirable to wait as long as possible to fall back to a quantum-secure scheme because of its inherent additional cost and complexity. These mechanisms demonstrate the ability to identify cryptographic vulnerabilities and ensure a smooth transition to quantum-secure standards, safeguarding blockchain assets in a post-quantum era.

</details>


### [28] [HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors](https://arxiv.org/abs/2601.05587)
*Jingxiao Yang,Ping He,Tianyu Du,Sun Bing,Xuhong Zhang*

Main category: cs.CR

TL;DR: HogVul是一个基于粒子群优化的黑盒对抗代码生成框架，通过协调词汇和语法扰动来攻击基于语言模型的漏洞检测器，相比现有方法平均攻击成功率提升26.05%。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的漏洞检测方法容易受到对抗攻击，而现有的黑盒攻击主要依赖孤立的扰动策略，限制了在对抗代码空间中高效探索最优扰动的能力。

Method: 提出HogVul框架，采用粒子群优化驱动的统一双通道优化策略，整合词汇和语法两种扰动，系统协调两级扰动来扩展对抗样本的搜索空间。

Result: 在四个基准数据集上的实验表明，HogVul相比最先进的基线方法平均攻击成功率提升了26.05%。

Conclusion: 混合优化策略在暴露模型漏洞方面具有显著潜力，HogVul框架有效提升了对抗攻击的效果。

Abstract: Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.

</details>


### [29] [Continual Pretraining on Encrypted Synthetic Data for Privacy-Preserving LLMs](https://arxiv.org/abs/2601.05635)
*Honghao Liu,Xuhui Jiang,Chengjin Xu,Cehao Yang,Yiran Cheng,Lionel Ni,Jian Guo*

Main category: cs.CR

TL;DR: 该论文提出了一种基于实体的隐私保护持续预训练框架，通过加密合成数据保护个人身份信息，使大语言模型能在保护隐私的同时学习新知识。


<details>
  <summary>Details</summary>
Motivation: 在小型领域特定语料库上预训练大语言模型时，保护敏感数据中的隐私是一个重大挑战。需要一种方法既能保护个人身份信息，又能让模型通过持续预训练学习新知识。

Method: 提出了基于实体的框架：1) 构建加权实体图指导数据合成；2) 对PII实体应用确定性加密；3) 通过解密密钥授权访问敏感数据，使LLMs能在加密数据上进行持续预训练。

Result: 在有限规模数据集上的实验表明：1) 预训练模型优于基础模型且确保PII安全；2) 与未加密合成数据训练的模型相比存在适度性能差距；3) 增加实体数量和基于图的合成能提升性能；4) 加密模型在长检索上下文中保持指令跟随能力。

Conclusion: 这项工作是对加密数据预训练设计空间的初步探索，讨论了确定性加密的安全影响和局限性，为隐私保护LLMs的发展提供了基础框架。代码已开源。

Abstract: Preserving privacy in sensitive data while pretraining large language models on small, domain-specific corpora presents a significant challenge. In this work, we take an exploratory step toward privacy-preserving continual pretraining by proposing an entity-based framework that synthesizes encrypted training data to protect personally identifiable information (PII). Our approach constructs a weighted entity graph to guide data synthesis and applies deterministic encryption to PII entities, enabling LLMs to encode new knowledge through continual pretraining while granting authorized access to sensitive data through decryption keys. Our results on limited-scale datasets demonstrate that our pretrained models outperform base models and ensure PII security, while exhibiting a modest performance gap compared to models trained on unencrypted synthetic data. We further show that increasing the number of entities and leveraging graph-based synthesis improves model performance, and that encrypted models retain instruction-following capabilities with long retrieved contexts. We discuss the security implications and limitations of deterministic encryption, positioning this work as an initial investigation into the design space of encrypted data pretraining for privacy-preserving LLMs. Our code is available at https://github.com/DataArcTech/SoE.

</details>


### [30] [The Echo Chamber Multi-Turn LLM Jailbreak](https://arxiv.org/abs/2601.05742)
*Ahmad Alobaid,Martí Jordà Roca,Carlos Castillo,Joan Vendrell*

Main category: cs.CR

TL;DR: 论文提出了一种名为"回声室"的新型多轮越狱攻击方法，通过逐步升级策略绕过聊天机器人的安全防护机制


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，低成本开发强大聊天机器人成为可能，但随之而来的安全挑战需要解决，特别是防止越狱攻击导致的财务损失和声誉损害

Method: 提出"回声室"多轮攻击方法，采用逐步升级策略，通过精心设计的交互链绕过聊天机器人的安全防护机制

Result: 通过广泛评估，该方法在多个最先进模型上表现出色，与其他多轮攻击方法相比具有优势

Conclusion: 多轮越狱攻击是聊天机器人安全的重要威胁，需要开发更强大的防护机制来应对这类逐步升级的攻击策略

Abstract: The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.

</details>
