<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: CourtGuard是一个本地运行的多智能体提示注入分类器，采用法庭式多智能体LLM系统来检测恶意提示，虽然整体检测性能不如直接检测器，但具有更低的误报率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在敏感应用中的集成，提示注入攻击（通过提示诱导LLM产生有害行为）风险日益增加，可能导致敏感数据泄露、虚假信息传播等危害。

Method: 提出CourtGuard系统，采用法庭式多智能体架构：'辩护律师'模型论证提示是良性的，'检察官'模型论证提示是提示注入，'法官'模型给出最终分类。

Result: CourtGuard相比直接检测器具有更低的误报率，但整体上是一个较差的提示注入检测器。在Gemma-3-12b-it、Llama-3.3-8B和Phi-4-mini-instruct模型上进行了实现。

Conclusion: CourtGuard较低的误报率凸显了在提示分类时同时考虑对抗性和良性场景的重要性，多智能体系统作为提示注入防御的前景得到了推进。

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [2] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: 研究表明，思维链(CoT)在无混淆压力下能有效监控模型行为，但在强混淆压力下，某些模型能成功完成对抗任务并逃避检测。模型对内部CoT的混淆程度低于外部CoT。


<details>
  <summary>Details</summary>
Motivation: 由于发现未对齐模型可能表现出欺骗行为，需要测试思维链(CoT)作为对齐监控工具的有效性，特别是验证模型是否能够通过混淆CoT来逃避检测。

Method: 开发了可组合和可量化的提示分类法来引发CoT混淆，在玩具任务和SHADE-Arena现实环境中评估内部CoT(推理痕迹)和外部CoT(输出中的提示推理)。

Result: (i) 无混淆压力下CoT监控准确高效；(ii) 强混淆压力下某些模型能完成对抗任务并逃避检测；(iii) 模型对内部CoT的混淆程度低于外部CoT。

Conclusion: 虽然CoT在良性设置下提供有价值的监督，但鲁棒部署需要对模型的可监控性进行特定压力测试。

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [3] [Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts](https://arxiv.org/abs/2510.19856)
*Eranga Bandara,Sachin Shetty,Ravi Mukkamala,Ross Gore,Peter Foytik,Safdar H. Bouk,Abdul Rahman,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.CR

TL;DR: 提出MCC框架，通过类似MCP的协议让LLM直接与区块链智能合约交互，使用户能用自然语言查询与区块链系统交互和执行交易。


<details>
  <summary>Details</summary>
Motivation: 区块链和生成式AI/LLM技术快速发展，但缺乏标准化方法将LLM应用与区块链无缝集成。

Method: 开发MCC框架，通过类似MCP的协议连接LLM和区块链智能合约，使用定制数据集对LLM进行微调以准确解释自然语言输入并映射到相应MCP功能。

Result: 在Rahasak区块链上开发了端到端原型，使用微调后的Llama-4 LLM，显著提高了平台的性能和准确性。

Conclusion: 这是首个使用模型上下文协议概念将LLM与区块链集成的方法，实现了用户通过自然语言与区块链系统交互的能力。

Abstract: In recent years, blockchain has experienced widespread adoption across
various industries, becoming integral to numerous enterprise applications.
Concurrently, the rise of generative AI and LLMs has transformed human-computer
interactions, offering advanced capabilities in understanding and generating
human-like text. The introduction of the MCP has further enhanced AI
integration by standardizing communication between AI systems and external data
sources. Despite these advancements, there is still no standardized method for
seamlessly integrating LLM applications and blockchain. To address this
concern, we propose "MCC: Model Context Contracts" a novel framework that
enables LLMs to interact directly with blockchain smart contracts through
MCP-like protocol. This integration allows AI agents to invoke blockchain smart
contracts, facilitating more dynamic and context-aware interactions between
users and blockchain networks. Essentially, it empowers users to interact with
blockchain systems and perform transactions using queries in natural language.
Within this proposed architecture, blockchain smart contracts can function as
intelligent agents capable of recognizing user input in natural language and
executing the corresponding transactions. To ensure that the LLM accurately
interprets natural language inputs and maps them to the appropriate MCP
functions, the LLM was fine-tuned using a custom dataset comprising user inputs
paired with their corresponding MCP server functions. This fine-tuning process
significantly improved the platform's performance and accuracy. To validate the
effectiveness of MCC, we have developed an end-to-end prototype implemented on
the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our
knowledge, this research represents the first approach to using the concept of
Model Context Protocol to integrate LLMs with blockchain.

</details>


### [4] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出了一种面向监管工作流程的政策治理RAG架构，通过三个组件确保审计就绪的生成：政策合约控制、加密溯源证据和便携合规证明。


<details>
  <summary>Details</summary>
Motivation: 在医药、金融、法律等受监管领域，错误成本高昂且审计追踪是法规强制要求，需要确保AI输出的合规性和可验证性。

Method: 采用三部分架构：(I)政策合约控制确保输出符合法律和内部政策；(II)加密证据锚定所有引用来源；(III)便携合规证明为审计提供最终验证。

Result: 设定了未经验证的目标：置信错误相对减少≥20%、p95延迟≤900ms、服务成本≤2.2倍，并设计了预注册的NO-GO门控机制。

Conclusion: 该设计通过使政策检查可审计、可重放和收据支持，补充了现有的RAG/护栏系统，特别适用于欧盟AI法案等监管环境。

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [5] [Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers](https://arxiv.org/abs/2510.19885)
*James Kim*

Main category: cs.CR

TL;DR: 比较数学构造与随机生成的S盒在密码学特性上的差异，评估随机方法的相对弱点，并建立随机置换的平均性能度量。


<details>
  <summary>Details</summary>
Motivation: 数学构造的S盒具有可证明的密码学特性，而完全随机方法缺乏理论保证但复杂度低且结果多样，需要比较两者的相对弱点。

Method: 在简单SPN设置中比较数学构造与随机生成的S盒，包括随机置换和带循环约束的随机置换，并与成熟设计进行对比。

Result: 数学构造的S盒具有高非线性度、APN特性和平衡雪崩特性等可证明的密码学性质，而随机方法缺乏这些理论保证。

Conclusion: 数学构造方法能生成数千个具有强密码学特性的S盒，而随机方法虽然复杂度低但缺乏理论保证，需要建立其平均性能度量标准。

Abstract: Mathematically constructed S-boxes arise from algebraic structures and finite
field theory to ensure strong, provable cryptographic properties. These
mathematically grounded constructions allow for generation of thousands of
S-Boxes with high nonlinearity, APN properties, and balanced avalanche
characteristics, unlike fully random methods, which lack such theoretical
guarantees in exchange for low complexity and more varied results. In this
work, we compare mathematically constructed constructions with randomly
generated ones to evaluate the relative weakness of the latter. We also
establish an average measure of performance for randomly generated
permutations, as well as random with forced cycle constraints, and compare them
to well-established designs in a simple SPN setting.

</details>


### [6] [Deep Sequence-to-Sequence Models for GNSS Spoofing Detection](https://arxiv.org/abs/2510.19890)
*Jan Zelinka,Oliver Kost,Marek Hrúz*

Main category: cs.CR

TL;DR: 提出一个数据生成框架来模拟欺骗攻击，并使用LSTM和Transformer架构的深度学习模型进行在线欺骗检测，最佳模型错误率达到0.16%。


<details>
  <summary>Details</summary>
Motivation: 需要有效检测GPS欺骗攻击，但缺乏真实攻击数据，因此开发数据生成框架来模拟攻击场景。

Method: 使用数据生成框架模拟全球范围内的欺骗攻击，采用LSTM和Transformer架构的深度学习模型进行在线检测。

Result: 深度学习模型能准确区分欺骗信号和真实信号，Transformer架构结合早期输入融合达到最佳性能，错误率为0.16%。

Conclusion: 深度学习模型在GPS欺骗检测中表现优异，特别是Transformer架构结合早期融合策略能实现高精度检测。

Abstract: We present a data generation framework designed to simulate spoofing attacks
and randomly place attack scenarios worldwide. We apply deep neural
network-based models for spoofing detection, utilizing Long Short-Term Memory
networks and Transformer-inspired architectures. These models are specifically
designed for online detection and are trained using the generated dataset. Our
results demonstrate that deep learning models can accurately distinguish
spoofed signals from genuine ones, achieving high detection performance. The
best results are achieved by Transformer-inspired architectures with early
fusion of the inputs resulting in an error rate of 0.16%.

</details>


### [7] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: SecureInfer是一个混合框架，利用异构可信执行环境（TEEs）-GPU架构来保护LLMs免受模型提取攻击，同时保持高性能推理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在移动和边缘平台的部署增加，保护模型隐私同时不牺牲不可信AI加速器（如GPU）的性能优势成为一个关键挑战。

Method: 采用信息论和威胁感知的分区策略：安全敏感组件（非线性层、注意力头投影、FNN变换、LoRA适配器）在SGX enclave内执行，而其他线性操作（矩阵乘法）在GPU上加密后执行并在enclave内安全恢复。

Result: 使用LLaMA-2模型实现原型并评估，SecureInfer在提供强大安全保证的同时保持了合理的性能。

Conclusion: SecureInfer为安全的设备上模型推理提供了一个实用的解决方案，平衡了安全性和性能需求。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [8] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 本文提出了一种硬件加速的混合同态加密架构，通过结合对称加密和全同态加密，显著降低了边缘设备上的计算开销，实现了低延迟、低功耗的安全机器学习推理。


<details>
  <summary>Details</summary>
Motivation: 全同态加密在隐私保护机器学习中存在客户端通信和计算开销大的问题，不适用于边缘设备。混合同态加密通过结合对称加密来降低客户端计算成本，但需要硬件加速来提高性能。

Method: 设计了一个围绕轻量级对称密码构建的硬件加速混合同态加密架构，该密码针对全同态加密兼容性进行了优化，并作为专用硬件加速器实现。还提出了多种微架构优化以提高性能和能效。

Result: 在PYNQ-Z2平台上使用MNIST数据集的实验显示，客户端加密延迟降低了50倍以上，硬件吞吐量比现有基于FPGA的HHE加速器提高了近2倍。

Conclusion: 该工作验证了低功耗硬件加速混合同态加密在边缘部署中的可行性，并为在资源受限环境中构建可扩展的安全机器学习系统提供了硬件-软件协同设计方法。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [9] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: zk-agreements协议通过结合零知识证明、安全多方计算和智能合约，在保护隐私的同时实现可计算的法律合同自动执行。


<details>
  <summary>Details</summary>
Motivation: 传统纸质合同阻碍数字化商务自动化，自然语言合同存在歧义且缺乏可验证性，而现有区块链智能合同会暴露敏感条款，存在隐私和企业采用障碍。

Method: 设计zk-agreements协议，使用零知识证明保护私有合同条款，安全多方计算实现私有合规评估，智能合约确保自动执行。

Result: 该协议解决了区块链合同中透明性与保密性之间的根本矛盾，实现了隐私保护和计算可执行性的平衡。

Conclusion: zk-agreements从纸质信任向加密信任的转变，为数字商务提供了既保护隐私又具备自动执行能力的合同解决方案。

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [10] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: SAID是一种无需训练的LLM防御方法，通过激活模型内部安全机制来主动识别和中和恶意意图，显著降低有害输出同时保持良性任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖外部干预，缺乏通用性、损害模型效用且计算开销大，需要更鲁棒和可扩展的内部防御机制。

Method: 采用三阶段流水线：模型原生意图蒸馏提取核心语义、最优安全前缀探测激活潜在安全意识、保守聚合策略确保鲁棒决策。

Result: 在5个开源LLM和6种高级越狱攻击上的实验表明，SAID显著优于现有防御方法，在减少有害输出的同时保持良性任务性能且计算开销最小。

Conclusion: 激活LLM内在安全机制是构建更安全可靠对齐AI系统的更鲁棒和可扩展路径。

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [11] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: 本文建立了伪随机生成器(PRG)与伪随机量子态(PRS)之间的量子黑盒分离，证明在量子可评估的PRG与对数大小或线性大小的PRS之间不存在黑盒构造。


<details>
  <summary>Details</summary>
Motivation: 解决PRG是否与对数大小PRS分离的开放问题，因为对数PRS已实现强大的密码学应用（如数字签名和量子公钥加密），而这些应用在线性PRS中尚未实现。

Method: 构造一个具有逆访问权限的酉量子预言机，证明在该预言机下不存在从PRS到PRG的黑盒构造。

Result: 成功建立了PRG与PRS（包括对数大小和线性大小）之间的量子黑盒分离，并作为推论获得了PRG与数字签名、量子公钥加密等原语之间的分离。

Conclusion: PRG与PRS在量子黑盒模型下是分离的，这一结果对理解量子密码学中不同原语之间的关系具有重要意义。

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [12] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 该论文系统研究了针对多模态大语言模型（MLLMs）的越狱攻击，发现即使简单的感知变换也能可靠绕过最先进的安全过滤器，在视觉-语言和音频-语言模型中暴露出严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然取得了显著进展，但在跨模态处理方面存在严重脆弱性，需要系统评估其对对抗性攻击的抵抗力。

Method: 使用1,900个对抗性提示在三个高风险安全类别（有害内容、CBRN、CSEM）上测试七个前沿模型，探索了FigStep-Pro（视觉关键词分解）、智能掩码（语义混淆）和音频扰动（Wave-Echo、Wave-Pitch、Wave-Speed）等攻击技术的有效性。

Result: 结果显示严重漏洞：文本安全近乎完美的模型（0% ASR）在感知修改输入下遭受>75%攻击成功率，FigStep-Pro在Llama-4变体中达到89% ASR。音频攻击进一步揭示了供应商特定弱点，即使基本模态转移也能在技术查询中获得25% ASR。

Conclusion: 当前安全措施无法泛化到跨模态攻击，暴露了文本中心对齐与多模态威胁之间的关键差距。这些攻击的可访问性表明，稳健的多模态AI安全需要向更广泛的语义级推理进行范式转变。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [13] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: 本文提出了一种基于格式保持加密(FPE)的地理坐标高精度隐私保护机制，用于解决汽车位置数据共享中的隐私泄露风险。该方法通过设计坐标数据密钥映射机制减少重复加密解密造成的地理位置精度损失。


<details>
  <summary>Details</summary>
Motivation: 汽车位置数据共享在路线导航、驾驶监控和车辆调度等功能中存在日益增长的隐私泄露风险，可能导致用户行为识别、敏感位置推断和轨迹重建等攻击。

Method: 采用基于格式保持加密(FPE)的地理坐标隐私保护机制，设计了汽车坐标数据密钥映射机制来减少重复加密解密过程中的精度损失。

Result: 实验结果显示平均相对距离保持率(RDR)达到0.0844，关键区域热点数量减少了98.9%，解密坐标数据实现了100%的恢复精度。

Conclusion: 该工作为汽车位置数据提供了一种高精度隐私保护方法，为自动驾驶中的敏感数据共享提供了高效的数据安全解决方案。

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [14] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 本文系统分析了深度强化学习在对抗环境中的安全挑战，提出了基于扰动类型和攻击目标的对抗攻击分类框架，总结了主流防御技术，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习在自动驾驶、智能制造等安全敏感领域的广泛应用，提高其在动态多变环境中的安全性和鲁棒性成为核心研究问题，特别是在面对对抗攻击时防止性能下降和危险决策。

Method: 提出基于扰动类型和攻击目标的对抗攻击分类框架，详细回顾了针对状态空间、动作空间、奖励函数和模型空间的主流对抗攻击方法，并系统总结了包括对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等多种鲁棒性训练策略。

Result: 构建了完整的对抗攻击分类体系，系统梳理了现有防御技术的优缺点，为深度强化学习在对抗环境中的安全性研究提供了全面的技术参考。

Conclusion: 未来深度强化学习在对抗环境中的研究应重点关注提高泛化能力、降低计算复杂度、增强可扩展性和可解释性等方面，为安全敏感应用提供更可靠的解决方案。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [15] [MAC Aggregation over Lossy Channels in DTLS 1.3](https://arxiv.org/abs/2510.20419)
*Eric Wagner,David Heye,Jan Bauer,Klaus Wehrle,Martin Serror*

Main category: cs.CR

TL;DR: 该论文研究了在DTLS 1.3协议中实现MAC聚合，通过实际部署测试验证了即使在有损无线信道中，MAC聚合也能显著提高吞吐量并节省能耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，通过聚合消息认证码来节省带宽，并验证在无线有损场景下MAC聚合的实际效益。

Method: 设计DTLS 1.3协议的MAC聚合扩展，并在嵌入式硬件上的完整通信协议栈中进行广泛性能评估。

Result: MAC聚合在短消息传输中可将吞吐量提高最多50%，并节省最多17%的能耗，即使在有损信道中也能实现这些效益。

Conclusion: MAC聚合在现实部署中确实能够显著提升性能并节省能源，验证了其在无线有损环境中的实用性。

Abstract: Aggregating Message Authentication Codes (MACs) promises to save valuable
bandwidth in resource-constrained environments. The idea is simple: Instead of
appending an authentication tag to each message in a communication stream, the
integrity protection of multiple messages is aggregated into a single tag.
Recent studies postulate, e.g., based on simulations, that these benefits also
spread to wireless, and thus lossy, scenarios despite each lost packet
typically resulting in the loss of integrity protection information for
multiple messages. In this paper, we investigate these claims in a real
deployment. Therefore, we first design a MAC aggregation extension for the
Datagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we
extensively evaluate the performance of MAC aggregation on a complete
communication protocol stack on embedded hardware. We find that MAC aggregation
can indeed increase goodput by up to 50% and save up to 17% of energy
expenditure for the transmission of short messages, even in lossy channels.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild的轮消除自约简技术，并应用该技术证明了最大b匹配和边着色的随机LOCAL算法下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild关于最大匹配的随机LOCAL算法下界证明长达25页且技术复杂，难以理解和推广。历史上证明简化对理解图问题复杂性有重要意义。

Method: 提出简化的轮消除自约简技术，并应用该技术分析最大b匹配和边着色问题。

Result: 1. 最大b匹配需要Ω(min{log₁₊bΔ, logΔn})和Ω(√log₁₊bn)轮；2. 边着色需要Ω(min{logΔ, logΔn})和Ω(√logn)轮。

Conclusion: 成功简化了轮消除自约简技术，并获得了最大b匹配和边着色的新下界结果，为理解分布式图算法复杂性提供了更易消化的工具。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [17] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 提出AsyncHZP，一种异步分层零并行方法，通过自适应参数分片和多流异步调度，在保持内存效率的同时显著减少通信开销，在大规模训练中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模集群上语言模型的训练效率和可扩展性存在瓶颈，主流ND并行方法复杂繁琐，而灵活替代方案如ZeRO则受通信开销限制。

Method: 设计异步分层零并行(AsyncHZP)，自适应地在不同副本组间重新分片参数、梯度和优化器状态，并采用多流异步调度方法在专用后台线程中执行参数全收集和梯度规约分散操作。

Result: 在密集模型和MoE模型上的实证评估表明，AsyncHZP在大规模下保持稳健稳定性，持续优于经典ND并行，无需复杂策略调优即可实现最先进性能。

Conclusion: AsyncHZP简化了高效大规模训练的路径，在保持内存效率的同时显著减少通信开销，实现优越性能。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [18] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 提出了一个HPC-QC全栈框架，用于集成高性能计算和量子计算，支持混合工作负载开发，采用模块化硬件/设备无关的软件集成方法。


<details>
  <summary>Details</summary>
Motivation: 满足对可扩展高性能计算和量子计算集成日益增长的需求，构建统一的量子-经典编程环境。

Method: 开发了可扩展的量子编程、调度和编译接口，在成熟的HPC编程环境中实现；采用自适应电路编织虚拟机将大量子电路分区；利用Cray LLVM编译框架转换LLVM IR和量子IR；在HPE EX超级计算机上演示混合工作负载。

Result: 成功演示了多个混合HPC-QC多节点多CPU和GPU工作负载，包括求解线性方程组、量子优化和模拟量子相变。

Conclusion: 该工作为基于经典HPC软件栈的统一量子-经典编程环境提供了框架。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [19] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: NCCLX是一个专为超大规模LLM训练和推理优化的集体通信框架，支持超过10万GPU集群，显著提升通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统通信方法在数十万GPU规模下面临吞吐量和延迟限制，阻碍了最先进模型的发展与部署。

Method: 开发NCCLX集体通信框架，针对LLM全生命周期优化，支持超大规模集群的复杂工作负载。

Result: 在Llama4模型上的实证评估显示通信效率显著提升。

Conclusion: 该研究为下一代LLM在空前规模上运行提供了稳健解决方案。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [20] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: FLAS是一个结合了主动和被动方法的自动扩缩容系统，通过预测高维指标趋势和基于资源使用指标估计的应急系统，为分布式服务提供最优扩缩容决策。


<details>
  <summary>Details</summary>
Motivation: 云计算弹性特性支持新兴技术，但现有自动扩缩容系统在应对突发负载变化时存在不足，需要结合预测和反应机制来确保服务等级协议。

Method: FLAS结合了预测模型（预测高维指标趋势以预判SLA参数变化）和反应式应急系统（从资源使用指标估计高维指标，减少侵入性检测）。

Result: 在基于内容发布订阅中间件的测试中，FLAS在99%以上的时间内确保性能要求得到满足，验证了方法的有效性。

Conclusion: FLAS是首个针对基于内容发布订阅分布式系统的自动扩缩容解决方案，具有通用性且能适应不同应用场景，通过边界值分析测试验证了其鲁棒性。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [21] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出一种在动态边缘环境中自动构建和评估性能预测器的方法，通过联合优化准确性和推理时间，在电子显微镜工作流等实时应用中实现高达90%的预测准确率，推理时间小于往返时间的1%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的动态边缘环境中，由于多应用共置和节点异构性，实现可预测的应用性能具有挑战性，这对有效的调度和资源管理至关重要。

Method: 自动构建和评估各种性能预测器，优先考虑准确性和推理时间，选择最有效模型。预测器基于与应用程序性能最相关的监控指标历史状态进行训练，并在动态共置场景下跨多个服务器进行评估。

Result: 预测器达到高达90%的准确率，同时保持推理时间小于往返时间的1%。在动态共置场景下对多个服务器进行了评估，特别考虑了具有严格实时需求和多样化资源需求的电子显微镜工作流。

Conclusion: 在动态共置场景中，需要通过联合优化准确性和推理延迟来选择特定于服务器的预测器。将此类预测器集成到边缘环境中可以提高资源利用率并实现可预测的性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [22] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 本文研究使用RTT预测器来增强请求路由，通过预测应用延迟来改善分布式应用在边缘和云环境中的性能。开发了轻量级准确的RTT预测器，在Kubernetes管理的GPU集群上训练，使用高度相关的监控指标，在保持低开销的同时适应不同的共置场景和异构硬件。


<details>
  <summary>Details</summary>
Motivation: 分布式应用对低端到端延迟的需求日益增长，特别是在边缘和云环境中，共置工作负载竞争有限资源。传统的负载均衡策略通常是反应式的，依赖过时或粗粒度的指标，导致次优的路由决策和增加的尾部延迟。

Method: 开发轻量级准确的RTT预测器，基于从Kubernetes管理的GPU集群收集的时间序列监控数据训练。利用一组高度相关的监控指标，保持低开销同时适应不同的共置场景和异构硬件。通过模拟评估性能感知负载均衡的效果。

Result: 预测器达到高达95%的准确率，同时将预测延迟保持在应用RTT的10%以内。确定了确保在资源受限集群中有效部署预测器所需的最小预测准确度阈值和关键系统级因素。模拟评估显示性能感知负载均衡能显著减少应用RTT并最小化资源浪费。

Conclusion: 这些结果突显了将预测性负载均衡集成到未来生产系统中的可行性，为改善分布式应用在资源受限环境中的性能提供了有效解决方案。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出并评估了一种量子启发的算法，用于解决二次无约束二进制优化（QUBO）问题，该算法使用矩阵乘积态（MPS）和密度矩阵重整化群（DMRG）方法，在Sudoku谜题和MaxCut问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决QUBO问题在数学上等同于寻找Ising自旋玻璃哈密顿量的基态，传统方法难以处理大规模问题，需要开发可扩展的量子启发算法。

Method: 使用矩阵乘积态（MPS）紧凑表示自旋构型的大叠加，结合离散驱动调度引导MPS向基态演化，通过包含横向磁场的驱动哈密顿量实现自旋翻转和量子隧穿，使用DMRG方法迭代最小化系统能量。

Result: 算法在多样化QUBO实例中可靠识别全局最小值，成功解决包含200多个Ising自旋的Sudoku谜题，以及Biq Mac库中最多251个节点和3,265条边的MaxCut问题。

Conclusion: 该量子启发方法具有可扩展性、通用性和适用于工业规模QUBO应用的优势。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [24] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该研究提出了分析可靠性基准（ARB），这是一个可复现的框架，用于量化大型语言模型在能源系统分析中的推理可靠性。该基准整合了五个子指标，在确定性、概率性和认知性场景下评估模型性能，结果显示GPT-4/5和Claude 4.5 Sonnet表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能和机器学习在能源领域的应用缺乏标准化框架来评估推理正确性，现有验证实践主要关注预测准确性或计算效率，而忽略了分析结论的逻辑完整性。

Method: 研究引入了分析可靠性基准（ARB），整合五个子指标（准确性、推理可靠性、不确定性纪律、政策一致性和透明度），使用开放技术经济数据集在确定性、概率性和认知性场景下评估四个前沿模型（GPT-4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro、Llama 3 70B）。

Result: GPT-4/5和Claude 4.5 Sonnet实现了持续且符合政策的推理（分析可靠性指数大于90），Gemini 2.5 Pro表现出中等稳定性，而Llama 3 70B低于专业阈值。统计验证确认这些差异是显著且可复现的。

Conclusion: ARB建立了能源文献中首个定量方法，用于验证人工智能系统中的因果、概率和政策驱动推理，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [25] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于有向无环图（DAG）的框架来评估大语言模型在数学推理中的思维链质量，引入逻辑接近度指标来量化模型推理轨迹与规则一致性，揭示了即使最终答案准确率相似，不同模型家族在推理保真度上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学问题上使用思维链（CoT）表现出色，但尚不清楚这种成功是源于搜索、机械记忆还是规则一致的推理过程，需要开发新的评估方法来深入理解模型的推理能力。

Method: 将思维链建模为基于规则的有向无环图（DAG）随机过程，其中节点表示中间推导状态，边编码规则应用；引入DAG-MATH CoT格式和逻辑接近度指标来评估推理轨迹与DAG结构的一致性。

Result: 在标准数学推理数据集上的分析发现，即使PASS@k指标相似，代表性LLM家族在推理保真度上存在统计显著差异，揭示了最终答案准确性与规则一致推导之间的差距。

Conclusion: 该框架在自由形式思维链和形式证明系统之间提供了平衡，为LLM推理评估提供了可操作的诊断工具，有助于更深入地理解模型的数学推理能力。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [26] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2是一个基于纯视觉观察的统一架构，在Web、桌面和移动环境中实现最先进性能，无需特定任务微调即可超越所有先前系统。


<details>
  <summary>Details</summary>
Motivation: 构建能够跨Web、桌面和移动环境泛化的智能体仍然是一个开放挑战，因为现有系统依赖环境特定接口限制了跨平台部署。

Method: Surfer 2集成了分层上下文管理、解耦的规划与执行、以及具有自适应恢复能力的自我验证，支持长任务序列的可靠操作。

Result: 在WebVoyager上达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，多尝试情况下在所有基准测试中超越人类表现。

Conclusion: 系统化编排放大了基础模型能力，仅通过视觉交互即可实现通用计算机控制，同时需要下一代视觉语言模型来实现帕累托最优的成本效益。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [27] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI正在加剧保险欺诈问题，欺诈者利用AI工具大规模伪造事故证据、损坏证明和虚假身份文件，而保险公司则部署AI检测软件进行对抗，形成一场持续的技术军备竞赛。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈每年造成数百亿美元损失，传统欺诈手段包括伪造事故、夸大损失等。生成式AI的出现使得欺诈者能够以更低的成本和更高的效率大规模制造逼真的虚假证据，这给保险行业带来了新的严峻挑战。

Method: 保险公司开始采用基于AI的深度伪造检测软件和增强验证流程来检测和缓解这些AI驱动的欺诈行为。同时，UVeye提出了分层解决方案来应对车辆欺诈问题。

Result: 当前的缓解策略面临显著局限性，检测工具存在误报和漏报问题，而复杂的欺诈者不断调整策略以规避自动检查。保险公司在资源和成本方面也面临障碍。

Conclusion: 生成式AI与检测技术之间的猫鼠游戏，加上保险公司的资源和成本障碍，意味着对抗AI驱动的保险欺诈仍然是一个持续的挑战，需要更先进的解决方案。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [28] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究使用机器学习模型分析领导力人格特质来预测硕士学生的学业成功，通过7种算法建模，随机森林分类器达到最高87.50%的准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导力人格特质预测学业成功，为早期识别学生优劣势和选择个性化学习策略提供机会。

Method: 收集129名环境工程硕士生的5项领导力人格测试数据（23个特征）和平均成绩，使用探索性数据分析和相关性分析，通过7种机器学习算法（SVM、LR、KNN、DT、GB、RF、XGBoost、LightGBM）进行建模。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记特征的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 研究证实了通过领导力人格特质预测学业成功的可行性，为个性化学习提供了早期识别学生优劣势的有效工具。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [29] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [30] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA是一个以人为中心的多智能体系统，用于数字资产交易异常检测，通过自然语言对话工作流将解析、检测和解释三个角色连接起来，为非专家用户提供可解释的异常检测服务。


<details>
  <summary>Details</summary>
Motivation: 旨在提高金融取证中的透明度和信任度，让非专家用户能够通过自然语言查询、检查结构化分析并获得上下文感知的解释，从而更好地理解和信任异常检测结果。

Method: 采用多智能体系统架构，包含解析、检测和解释三个角色，通过对话工作流将用户意图转换为经典检测器（XGBoost）的模式，并基于底层特征生成叙述性解释。

Result: 在标记的比特币混币数据集（Wasabi Wallet，2020-2024）上，基线检测器达到强准确性，而HCLA增加了可解释性和交互式精炼能力。

Conclusion: 人机协作设计能够提高金融取证中的透明度和信任度，HCLA系统展示了如何通过自然语言交互和可解释性增强异常检测系统的可用性和可靠性。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [31] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 本文质疑AI在法律实践中能显著提高效率和降低成本的普遍观点，提出了验证-价值悖论，认为AI使用带来的效率提升会被相应的验证需求所抵消，导致净价值往往可以忽略不计。


<details>
  <summary>Details</summary>
Motivation: 针对律师因提交不准确的AI生成内容而受到处罚的案例，以及AI与现实脱节、缺乏透明性的特点，需要重新评估AI在法律实践中的使用范式。

Method: 提出验证-价值悖论作为替代模型，该悖论认为AI使用带来的效率提升会被相应的验证需求所抵消。

Result: AI在法律实践中的净价值往往可以忽略不计，因为效率提升被验证成本所抵消。

Conclusion: 需要重新思考AI在法律实践和教育中的使用，强调对真相的忠诚和公民责任等核心价值观。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [32] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了TRUST框架，一个透明、去中心化的大语言模型推理审计系统，通过共识机制、层次化DAG分解、区块链账本和隐私保护分段技术解决现有集中式审计的四大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型审计方法存在集中化、不透明、难以扩展的问题，在关键领域部署专有模型存在重大风险，需要解决鲁棒性、可扩展性、透明度和隐私保护四个核心挑战。

Method: 采用共识机制确保在30%恶意参与者下仍能保证正确性；使用层次化DAG分解推理轨迹实现可扩展并行审计；利用区块链账本记录验证决策确保公共问责；通过隐私保护分段仅共享部分推理步骤保护专有逻辑。

Result: 在多个LLM（GPT-OSS、DeepSeek-r1、Qwen）和推理任务（数学、医学、科学、人文）上的实验表明，TRUST能有效检测推理缺陷并在对抗性审计下保持鲁棒性。

Conclusion: TRUST框架开创了去中心化AI审计，为大语言模型的安全可信部署提供了可行路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [33] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 本文提出AGI发展需要经历身份固化的锁定阶段，从开放模仿转向稳定身份，并建立了检测指标。实验显示不同规模模型呈现不同固化效果，这是AGI可靠性的前提也是安全控制点。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过于开放和可操控，而真正的AGI发展需要经历身份固化阶段，使目标结构、拒绝行为、偏好和内部表征变得稳定且抵抗外部操控。

Method: 形式化身份固化阶段，将其与学习动态中的已知现象联系起来，提出操作化检测指标，并在不同规模模型上进行实验验证。

Result: 行为固化快速且非线性，但对通用能力的影响不是单一的：小模型出现性能权衡，中等规模模型基本无成本采用，大型量化模型出现暂时不稳定性。

Conclusion: 身份固化是AGI级别可靠性的先决条件，也是关键的安全控制点——身份可以被工程化设计以提高可靠性，但也可能在扩展过程中自发形成，从而固化不可预测的目标和行为。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [34] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 本文研究了在2048游戏中优化AI的进化训练方法，比较了单智能体系统和双智能体系统的表现，发现单智能体系统通过价值函数优化取得了显著改进，而双智能体元提示系统效果有限。


<details>
  <summary>Details</summary>
Motivation: 优化人工智能在动态环境中的表现是机器学习研究的基本挑战。2048游戏结合了策略性游戏玩法和随机元素，为研究决策制定、长期规划和动态适应提供了理想的测试平台。

Method: 实现了两种系统：双智能体元提示系统（一个"思考者"LLM优化策略供"执行者"LLM使用）和基于价值函数优化的单智能体系统（结合有限蒙特卡洛树搜索），并实验了回滚功能以避免性能退化。

Result: 单智能体系统取得了显著改进，每个训练周期平均增加473.2分，且呈现明显上升趋势（相关性ρ=0.607）。LLM对游戏的理解也随训练加深，开发出更高级的策略。双智能体系统改进有限，突显了元提示的内在局限性。

Conclusion: 研究证明了进化优化技术在非确定性环境中改进AI性能的潜力，单智能体价值函数优化方法比双智能体元提示方法更有效。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [35] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型在个体化认知模拟中的能力，通过构建基于新出版小说的数据集和11条件认知评估框架，测试了7个现成LLM在作者风格模仿方面的表现。研究发现结合概念和语言特征的方法最有效，但LLM在模仿语言风格方面优于叙事结构，揭示了其在深层认知模拟中的局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型能够表面模仿人类行为，但其模拟更深层次个体化认知过程的能力尚不清楚。研究旨在填补这一空白，评估不同认知表征方法在个体化认知模拟中的效果。

Method: 构建基于新出版小说（晚于测试LLM发布日期）的数据集，提出11条件认知评估框架，测试7个现成LLM在作者风格模仿任务中的表现。评估了不同认知表征方法，包括语言特征、概念映射和基于档案的信息。

Result: 结果显示，结合概念和语言特征的方法在个体化认知模拟中特别有效，在整体评估中优于基于静态档案的线索。LLM在模仿语言风格方面比模仿叙事结构更有效。

Conclusion: 这些发现为开发适应个体思维和表达方式的AI系统奠定了基础，推动了更个性化和人类对齐的创意技术的发展，同时揭示了LLM在深层认知模拟方面的局限性。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [36] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: STaBERT模型通过整合POI信息和时间描述符来增强BERT基础的人类移动性预测，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动性预测模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点(POI)提供的丰富语义上下文。

Method: 提出STaBERT模型，在BERT基础上通过POI嵌入和推导的时间描述符来丰富移动性表示，构建统一的语义增强移动性表征。

Result: 实验结果显示STaBERT显著提升预测精度：单城市预测的GEO-BLEU得分从0.34提升至0.75；多城市预测从0.34提升至0.56。

Conclusion: 整合POI和时间信息能够有效捕捉人类移动的语义基础，显著改善移动性预测性能。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [37] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA是一个集成外部工具和多步推理的EQA智能体，通过工具获取有用信息来指导探索方向，从而以更短的探索距离生成更准确的回答。


<details>
  <summary>Details</summary>
Motivation: 现有的EQA方法直接使用VLM探索环境而不进行显式思考或规划，限制了推理能力，导致探索效率低下和回答效果不佳。

Method: 提出ToolEQA智能体，集成外部工具进行多步推理；设计自动生成EQA任务的数据流水线，构建包含18K任务的EQA-RT数据集。

Result: 在EQA-RT-Seen和EQA-RT-Unseen测试集上，ToolEQA相比SOTA基线成功率提升9.2~20.2%，比零样本ToolEQA提升10%；在HM-EQA、OpenEQA和EXPRESS-Bench数据集上也达到SOTA性能。

Conclusion: ToolEQA通过工具集成和多步推理显著提升了EQA任务的性能，展示了良好的泛化能力。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [38] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 本文基于AI4HealthyAging项目，识别了临床数据收集中存在的多种偏见类型，包括历史偏见、代表性偏见和测量偏见，并提出改进临床问题设计和数据收集公平性的实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在医疗领域具有巨大潜力，但由于训练数据的质量和公平性问题，AI解决方案在实际临床实践中的整合仍然有限。主要障碍是存在偏见的数据收集实践。

Method: 基于西班牙国家研发计划中的AI4HealthyAging项目，通过检测临床数据收集过程中的偏见，识别了多种偏见类型，包括在性别、年龄、居住环境、社会经济地位、设备和标签等变量中表现的历史偏见、代表性偏见和测量偏见。

Result: 识别了临床数据收集中存在的多种系统性偏见，这些偏见影响了AI模型的公平性和鲁棒性。

Conclusion: 提出了改进临床问题设计和数据收集公平性的实用建议，希望研究结果和经验能为未来开发更公平的医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [39] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于军事行动中AI系统目标打击的附带损害评估模型，该模型整合了时间、空间和力量维度，采用知识表示与推理架构，通过分层结构捕获AI系统类别、攻击向量和上下文因素，并考虑传播、严重性、可能性和评估指标。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场中作用日益增强的时代，确保负责任的打击需要严格评估潜在的附带效应，因此需要开发能够评估AI系统打击效果的可靠模型。

Method: 采用设计科学方法论，构建统一的知识表示与推理架构，整合时间、空间和力量维度，通过分层结构捕获AI系统类别、攻击向量和上下文因素，并考虑传播、严重性、可能性和评估指标。

Result: 开发了一个附带损害评估模型，并通过实例化进行了演示和评估，为构建负责任和可信赖的智能系统奠定了基础。

Conclusion: 该模型为评估军事行动中AI系统打击效果提供了透明推理机制，是构建负责任和可信赖智能系统的重要基础。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [40] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本调查系统回顾了LLM赋能知识图谱构建的最新进展，分析了LLM如何重塑传统的本体工程、知识抽取和知识融合三层流水线，并探讨了基于模式和无模式两种构建范式的互补优势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的出现，知识图谱构建正从基于规则和统计的流水线转向语言驱动和生成框架，需要系统梳理这一范式转变的技术进展和未来方向。

Method: 首先回顾传统KG方法建立概念基础，然后从基于模式（强调结构、规范化和一致性）和无模式（强调灵活性、适应性和开放发现）两个互补视角分析新兴的LLM驱动方法，综合代表性框架并分析技术机制。

Result: 系统分析了LLM如何重塑KG构建的各阶段，识别了不同范式的技术特点和局限性，为理解LLM与KG的交互演进提供了清晰框架。

Conclusion: 该调查旨在澄清LLM与知识图谱之间不断演变的相互作用，弥合符号知识工程和神经语义理解之间的鸿沟，推动开发自适应、可解释和智能的知识系统。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [41] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: IKnow是一个持续预训练框架，通过制定新的自监督目标，在指令-响应对语格式中利用文本内嵌的领域知识，无需依赖外部资源或原始基础模型。


<details>
  <summary>Details</summary>
Motivation: 解决指令调优模型在持续预训练时指令跟随能力和语义表示退化的问题，特别是在无法访问原始基础模型或可靠外部语料库的现实场景中。

Method: 提出IKnow框架，在指令-响应对语格式中制定新的自监督目标，利用文本内嵌的领域知识进行深度语义编码。

Result: 该方法能够有效适应新领域，同时保持模型的指令跟随能力。

Conclusion: IKnow提供了一个简单通用的解决方案，在不依赖外部资源的情况下实现语言模型的持续适应。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [42] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 本文提出了一个新的创造性结果计算模型，通过五个功能函数生成更具新颖性的创新机会，在酒店业创新项目中验证了该模型比Notebook LM和ChatGPT4o能产生更新颖和/或更有用的结果。


<details>
  <summary>Details</summary>
Motivation: 开发一个基于创造力理论和技术的计算模型，旨在为创新项目生成更具新颖性的机会，同时保持实用性。

Method: 实现了一个包含五个功能函数的计算模型，这些函数专门设计用于在不损失实用性的前提下生成更高新颖性的创新机会。

Result: 在酒店业创新项目的评估中，该计算模型生成的结果比Notebook LM和ChatGPT4o更新颖和/或更有用，但并非所有模型功能都对生成更新颖机会有贡献。

Conclusion: 该计算模型在生成新颖创新机会方面表现优于现有工具，但部分功能效果有限，为后续模型开发提供了新的方向。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [43] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出了一种名为EBR的神经推理器，通过嵌入来近似符号推理器的结果，解决了传统描述逻辑推理器在现实知识库中因数据不一致和错误而无法部署的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法大多依赖描述逻辑推理器，但这些推理器对不一致和错误数据不鲁棒，无法在现实知识库中部署。

Method: 开发EBR神经推理器，仅需检索原子概念和存在限制的实例，就能检索或近似SHOIQ描述逻辑中任何概念的实例集。

Result: 实验表明EBR在缺失和错误数据情况下具有鲁棒性，优于现有推理器。

Conclusion: EBR为在现实世界知识库中部署神经符号概念学习提供了可行的解决方案。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [44] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的知识图谱对齐方法，无需训练数据，能够同时对齐实体和关系，提供可解释的结果，并支持悬空实体处理。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注实体级对齐，缺乏可解释性推理且需要训练数据，无法处理悬空实体。

Method: 基于模糊逻辑的迭代方法，提供整体性的实体和关系对齐，具有可证明的收敛性。

Result: 在主要基准测试中取得了最先进的结果。

Conclusion: FLORA是一种简单有效的无监督知识图谱对齐方法，具有可解释性、收敛性和对悬空实体的支持。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [45] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: MIMOSA框架是一个可解释性设计模型的方法论，旨在平衡可解释性与性能，同时嵌入因果性、公平性和隐私性等关键伦理属性。


<details>
  <summary>Details</summary>
Motivation: 开发可解释性设计模型对于在现实应用中建立对自动化决策模型的信任、问责和安全采用至关重要。

Method: 形式化定义了监督学习设置，涵盖表格数据、时间序列、图像、文本、交易和轨迹等多种数据类型。分析了特征重要性、规则和实例三类可解释模型家族，并形式化了因果性、公平性和隐私性三个伦理属性。

Result: 建立了开发不仅准确可解释，而且公平、保护隐私和具有因果意识的AI系统的理论基础。

Conclusion: 通过在模型生成过程中评估伦理度量，该框架为开发可信赖的AI系统奠定了理论基础。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [46] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval是一个全面的多语言多模态基准测试，用于评估LLM在电子商务领域的性能，覆盖6个类别37个任务，包含8个多模态任务，支持7种语言。


<details>
  <summary>Details</summary>
Motivation: 现有电子商务评估基准存在任务多样性不足、模态有限、数据合成或人工整理、语言覆盖窄等问题，缺乏评估复杂真实购物场景的可靠工具。

Method: 采用半自动流程，由大模型生成候选回答，再由50多名电子商务和多语言专家审核修改；基于不同规模和能力模型的评估分数定义任务难度级别。

Result: 构建了反映真实业务交互噪声和异质性的基准测试，涵盖客户查询和交易日志等真实数据源。

Conclusion: EcomEval提供了挑战导向和细粒度评估的多语言多模态基准，填补了电子商务领域专业评估工具的空白。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [47] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 本文提出了流动性指数(FI)来量化模型在动态扩展环境中的适应性，通过评估初始、当前和未来环境状态偏差来测试响应准确性，区分封闭式和开放式基准测试，重点关注闭环开放式现实世界基准。


<details>
  <summary>Details</summary>
Motivation: 为了量化模型在动态扩展环境中的适应能力，需要开发一个能够评估模型理解、预测和适应状态变化能力的基准测试方法。

Method: 使用流动性指数(FI)来衡量模型适应性，通过分析初始、当前和未来环境状态的偏差来评估响应准确性，并区分封闭式和开放式基准测试方法。

Result: 提出了一个能够评估模型在动态扩展环境中适应性的基准测试框架，强调真正超级智能模型应具备至少二阶适应性，能够通过数字补充实现自我维持计算。

Conclusion: 流动性指数为评估模型在动态环境中的适应性提供了量化指标，超级智能模型需要具备高阶适应能力以实现最佳流动性。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [48] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文对将机器学习集成到理性智能体架构中的现有方法进行了系统化梳理，以BDI范式为参考框架，分析了相关文献并识别了关键研究机会和挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习在感知和认知任务中展现出类人能力，将ML集成到理性智能体架构中的框架日益受到关注，但现有研究缺乏系统性和连贯性，往往忽视理性架构的表达能力。

Method: 使用BDI（信念-欲望-意图）范式作为参考框架，对现有方法进行细粒度系统化分析。

Result: 分析展示了由ML增强的理性智能体文献的快速发展，识别了设计有效理性ML智能体的关键研究机会和开放挑战。

Conclusion: 需要更系统地整合ML与理性智能体架构，特别是充分利用BDI等理性架构的表达能力，以设计更有效的理性ML智能体。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [49] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 本文提出了一种基于广义均值积分的AGI一致性度量方法，替代了传统算术平均方法，通过考虑不同补偿性假设下的鲁棒性来更严格地评估通用人工智能。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义使用认知领域的算术平均值，但这种方法假设补偿性（某些领域的卓越能力可以弥补其他领域的失败），而真正的通用智能应该反映所有基本领域的平衡能力。

Method: 提出基于广义均值在补偿性指数连续体上积分的AGI一致性度量方法，涵盖算术、几何和调和均值机制，通过曲线下面积（AUC）量化不同补偿性假设下的鲁棒性。

Result: 应用于GPT-4和GPT-5的CHC领域得分，一致性调整的AUC显示尽管算术得分较高（如GPT-5为24%），但两个系统距离通用能力仍然很远。

Conclusion: 广义均值积分提供了一个原则性、可解释且更严格的AGI测量基础，能够惩罚不平衡并捕捉领域间依赖关系。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [50] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出Real Deep Research (RDR)框架，用于系统分析AI和机器人领域的研究趋势，识别新兴趋势和跨领域机会，帮助研究人员应对论文数量激增的挑战。


<details>
  <summary>Details</summary>
Motivation: AI和机器人领域每年产出超过10,000篇论文，研究人员难以跟上快速发展的趋势，跨学科工作增多，需要探索专业领域外的知识。

Method: 构建通用的RDR管道，系统分析研究领域，识别新兴趋势，发现跨领域机会，为新的研究提供具体起点。

Result: 将RDR框架应用于AI和机器人领域，特别关注基础模型和机器人技术进步，并扩展到其他科学领域。

Conclusion: RDR框架为AI及其他领域的研究人员提供了系统分析研究趋势的工具，帮助他们在快速发展的研究环境中保持更新。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [51] [HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application](https://arxiv.org/abs/2510.20137)
*Hasnain A. Ziad,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 提出一种新型近似加法器，比现有加法器更节能、面积更小，同时保持或提高计算精度，适用于图像处理等多媒体应用。


<details>
  <summary>Details</summary>
Motivation: 为计算密集型多媒体应用（如图像、音频、视频处理）开发更节能的硬件，平衡高性能、计算精度和能效之间的冲突需求。

Method: 设计了一种新型近似加法器，通过仿真验证其性能，并将其部署到图像处理任务中展示数字重建高质量图像的能力。

Result: 仿真结果显示，该加法器在能效和面积效率方面优于现有加法器，同时实现改进或相当的精度。

Conclusion: 该新型近似加法器在能效、面积效率和计算精度方面表现出色，适用于多媒体处理应用。

Abstract: The design of approximate adders has been widely researched to advance
energy-efficient hardware for computation-intensive multimedia applications,
such as image, audio, or video processing. The design of approximate adders has
been widely researched to advance energy-efficient hardware for computation
intensive multimedia applications, such as image/audio/video processing.
Several static and dynamic approximate adders exist in the literature, each of
which endeavors to balance the conflicting demands of high performance,
computational accuracy, and energy efficiency. This work introduces a novel
approximate adder that is more energy- and area-efficient than existing adders,
while achieving improved or comparable accuracy, as demonstrated by simulation
results. The proposed adder's ability to digitally reconstruct high quality
images is further demonstrated by the deployment of the design for an image
processing task.

</details>


### [52] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文通过实验证明，在商用DRAM芯片中利用同时多行激活(SiMRA)技术可以生成高质量真随机数，具有高吞吐量和低延迟特性。


<details>
  <summary>Details</summary>
Motivation: 探索在商用DRAM芯片中实现高效真随机数生成的新方法，以克服现有DRAM-based TRNG在性能和效率方面的限制。

Method: 对96个DDR4 DRAM芯片进行广泛表征，研究不同数量的同时激活行(2,4,8,16,32)、数据模式、温度水平和空间变化对SiMRA真随机生成潜力的影响。

Result: 所有SiMRA-based TRNG设计都成功通过NIST随机性测试；2、8、16、32行激活设计的吞吐量分别比最先进DRAM-based TRNG提高1.15x、1.99x、1.82x和1.39x；熵值随激活行数增加而增加；操作参数显著影响熵值。

Conclusion: SiMRA技术为在商用DRAM芯片中实现高效真随机数生成提供了可行方案，其性能优于现有方法，且开源了研究基础设施以支持未来发展。

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


### [53] [Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels](https://arxiv.org/abs/2510.20400)
*Rubén Langarita,Jesús Alastruey-Benedé,Pablo Ibáñez-Marín,Santiago Marco-Sola,Miquel Moretó,Adrià Armejach*

Main category: cs.AR

TL;DR: Squire是一种通用加速器，旨在有效利用依赖密集型内核中的细粒度并行性。它通过在典型多核系统中为每个核心集成一个Squire加速器，实现了对动态编程内核最高7.64倍的加速，端到端应用3.66倍加速，能耗降低56%，面积开销仅为10.5%。


<details>
  <summary>Details</summary>
Motivation: 传统通用加速器（如SIMD和GPGPU）在实现复杂数据依赖模式时存在局限性，而定制FPGA和ASIC设计虽然性能好但成本高且缺乏灵活性。需要一种能有效利用细粒度并行性的通用加速器来解决依赖密集型内核的性能瓶颈。

Method: Squire加速器包含一组通用低功耗顺序核心，这些核心可以快速相互通信并直接从L2缓存访问数据。系统为每个核心集成一个Squire加速器，通过最小化软件更改来加速并行任务中的依赖密集型内核。

Result: 在实现复杂依赖模式的五个内核中，Squire在动态编程内核中获得了最高7.64倍的加速。使用其中三个内核构建的端到端读取映射工具实现了3.66倍的整体加速，能耗降低高达56%，与Neoverse-N1基线相比面积开销仅为10.5%。

Conclusion: Squire提供了一种有效的方法来加速依赖密集型内核，在保持通用性的同时实现了显著的性能提升和能效改进，且硬件开销最小。

Abstract: Multiple HPC applications are often bottlenecked by compute-intensive kernels
implementing complex dependency patterns (data-dependency bound). Traditional
general-purpose accelerators struggle to effectively exploit fine-grain
parallelism due to limitations in implementing convoluted data-dependency
patterns (like SIMD) and overheads due to synchronization and data transfers
(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved
performance and energy efficiency at a high cost in hardware design and
programming complexity and often lack the flexibility to process different
workloads. We propose Squire, a general-purpose accelerator designed to exploit
fine-grain parallelism effectively on dependency-bound kernels. Each Squire
accelerator has a set of general-purpose low-power in-order cores that can
rapidly communicate among themselves and directly access data from the L2
cache. Our proposal integrates one Squire accelerator per core in a typical
multicore system, allowing the acceleration of dependency-bound kernels within
parallel tasks with minimal software changes. As a case study, we evaluate
Squire's effectiveness by accelerating five kernels that implement complex
dependency patterns. We use three of these kernels to build an end-to-end
read-mapping tool that will be used to evaluate Squire. Squire obtains speedups
up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an
acceleration for an end-to-end application of 3.66$\times$. In addition, Squire
reduces energy consumption by up to 56% with a minimal area overhead of 10.5%
compared to a Neoverse-N1 baseline.

</details>
