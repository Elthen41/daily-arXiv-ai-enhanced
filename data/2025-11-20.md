<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs](https://arxiv.org/abs/2511.14908)
*Gefté Almeida,Marcio Pohlmann,Alex Severo,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.CR

TL;DR: 本研究评估开源模型在安全事件分类中的表现，与专有模型进行对比。使用基于NIST SP 800-61r3分类法的匿名真实事件数据集，采用五种提示工程技术处理。结果显示专有模型准确率更高，但开源模型在隐私保护、成本效益和数据主权方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 评估开源模型在安全事件分类中的性能，探索其在隐私保护、成本效益和数据主权方面的潜在优势，为组织在选择安全事件分类解决方案时提供参考。

Method: 使用基于NIST SP 800-61r3分类法的匿名真实安全事件数据集，采用五种提示工程技术（PHP、SHP、HTP、PRP和ZSL）进行处理，对比开源模型与专有模型的分类性能。

Result: 专有模型在准确率方面表现更优，但开源模型在本地部署时在隐私保护、成本效益和数据主权方面具有明显优势。

Conclusion: 虽然专有模型在准确率上仍有优势，但开源模型为注重隐私、成本控制和数据主权的组织提供了可行的替代方案，特别是在本地部署场景下。

Abstract: In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.

</details>


### [2] [CIMemories: A Compositional Benchmark for Contextual Integrity of Persistent Memory in LLMs](https://arxiv.org/abs/2511.14937)
*Niloofar Mireshghallah,Neal Mangaokar,Narine Kokhlikyan,Arman Zharmagambetov,Manzil Zaheer,Saeed Mahloujifar,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: CIMemories基准测试评估LLMs在任务上下文中控制记忆信息流的能力，发现前沿模型存在高达69%的属性级违规（不当泄露信息），且违规率随任务数量增加而上升，隐私提示无法解决此问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地使用持久记忆来增强个性化和任务性能，这种记忆在不当上下文中泄露敏感信息带来了关键风险，需要评估LLMs是否能基于任务上下文适当控制信息流。

Method: 使用包含每个用户100多个属性的合成用户配置文件，结合多样化的任务上下文，其中每个属性在某些任务中可能是必需的，但在其他任务中可能不合适。

Result: 前沿模型表现出高达69%的属性级违规，GPT-5的违规率从1个任务的0.1%上升到40个任务的9.6%，相同提示执行5次时达到25.1%，模型对相同提示泄露不同属性。

Conclusion: 这些发现揭示了需要上下文感知推理能力的基本限制，而不仅仅是更好的提示或扩展，隐私提示无法解决此问题，模型会过度泛化，共享所有内容或不共享任何内容，而不是做出细致、上下文相关的决策。

Abstract: Large Language Models (LLMs) increasingly use persistent memory from past interactions to enhance personalization and task performance. However, this memory introduces critical risks when sensitive information is revealed in inappropriate contexts. We present CIMemories, a benchmark for evaluating whether LLMs appropriately control information flow from memory based on task context. CIMemories uses synthetic user profiles with over 100 attributes per user, paired with diverse task contexts in which each attribute may be essential for some tasks but inappropriate for others. Our evaluation reveals that frontier models exhibit up to 69% attribute-level violations (leaking information inappropriately), with lower violation rates often coming at the cost of task utility. Violations accumulate across both tasks and runs: as usage increases from 1 to 40 tasks, GPT-5's violations rise from 0.1% to 9.6%, reaching 25.1% when the same prompt is executed 5 times, revealing arbitrary and unstable behavior in which models leak different attributes for identical prompts. Privacy-conscious prompting does not solve this - models overgeneralize, sharing everything or nothing rather than making nuanced, context-dependent decisions. These findings reveal fundamental limitations that require contextually aware reasoning capabilities, not just better prompting or scaling.

</details>


### [3] [LFreeDA: Label-Free Drift Adaptation for Windows Malware Detection](https://arxiv.org/abs/2511.14963)
*Adrian Shuai Li,Elisa Bertino*

Main category: cs.CR

TL;DR: LFreeDA是一个无需人工标注的端到端恶意软件分类器自适应框架，通过联合训练标记和未标记样本进行无监督域适应，有效应对概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习恶意软件检测器面临概念漂移问题，需要重新训练但人工标注成本高昂。现有方法依赖漂移检测和选择性标注，完全无标注的自适应方法尚未充分探索。

Method: LFreeDA首先在恶意软件图像上进行无监督域适应，联合训练标记和未标记样本以推断伪标签并剔除噪声标签；然后在CFG表示上自适应分类器，利用图像的可扩展性进行伪标注，利用CFG的丰富语义进行最终自适应。

Result: 在真实MB-24+数据集上，LFreeDA相比无自适应基线准确率提升12.6%，F1提升11.1%，仅比全监督上限低4%准确率和3.4% F1，性能与需要300个目标样本真实标签的最先进方法相当。

Conclusion: LFreeDA能够在恶意软件演化过程中保持检测性能，无需人工标注，为恶意软件检测器的持续自适应提供了有效的无标注解决方案。

Abstract: Machine learning (ML)-based malware detectors degrade over time as concept drift introduces new and evolving families unseen during training. Retraining is limited by the cost and time of manual labeling or sandbox analysis. Existing approaches mitigate this via drift detection and selective labeling, but fully label-free adaptation remains largely unexplored. Recent self-training methods use a previously trained model to generate pseudo-labels for unlabeled data and then train a new model on these labels. The unlabeled data are used only for inference and do not participate in training the earlier model. We argue that these unlabeled samples still carry valuable information that can be leveraged when incorporated appropriately into training. This paper introduces LFreeDA, an end-to-end framework that adapts malware classifiers to drift without manual labeling or drift detection. LFreeDA first performs unsupervised domain adaptation on malware images, jointly training on labeled and unlabeled samples to infer pseudo-labels and prune noisy ones. It then adapts a classifier on CFG representations using the labeled and selected pseudo-labeled data, leveraging the scalability of images for pseudo-labeling and the richer semantics of CFGs for final adaptation. Evaluations on the real-world MB-24+ dataset show that LFreeDA improves accuracy by up to 12.6% and F1 by 11.1% over no-adaptation lower bounds, and is only 4% and 3.4% below fully supervised upper bounds in accuracy and F1, respectively. It also matches the performance of state-of-the-art methods provided with ground truth labels for 300 target samples. Additional results on two controlled-drift benchmarks further confirm that LFreeDA maintains malware detection performance as malware evolves without human labeling.

</details>


### [4] [Towards Classifying Benign And Malicious Packages Using Machine Learning](https://arxiv.org/abs/2511.15033)
*Thanh-Cong Nguyen,Ngoc-Thanh Nguyen,Van-Giau Ung,Duc-Ly Vu*

Main category: cs.CR

TL;DR: 提出一种基于动态分析和机器学习的方法，自动检测恶意开源软件包，在npm包评估中达到0.91的AUC和接近0%的误报率。


<details>
  <summary>Details</summary>
Motivation: 恶意开源软件包数量急剧增加，现有安全扫描器主要关注已知CVE漏洞，缺乏有效的恶意包检测方法，特别是缺少自动区分恶意包和良性包的动态分析工具。

Method: 从动态分析（如执行命令）中提取特征，利用机器学习技术自动分类软件包为良性或恶意。

Result: 在近2000个npm包上的评估显示，机器学习分类器达到0.91的AUC，误报率接近0%。

Conclusion: 该方法能有效自动检测恶意开源软件包，为软件供应链安全提供重要保障。

Abstract: Recently, the number of malicious open-source packages in package repositories has been increasing dramatically. While major security scanners focus on identifying known Common Vulnerabilities and Exposures (CVEs) in open-source packages, there are very few studies on detecting malicious packages. Malicious open-source package detection typically requires static, dynamic analysis, or both. Dynamic analysis is more effective as it can expose a package's behaviors at runtime. However, current dynamic analysis tools (e.g., ossf's package-analysis) lack an automatic method to differentiate malicious packages from benign packages. In this paper, we propose an approach to extract the features from dynamic analysis (e.g., executed commands) and leverage machine learning techniques to automatically classify packages as benign or malicious. Our evaluation of nearly 2000 packages on npm shows that the machine learning classifier achieves an AUC of 0.91 with a false positive rate of nearly 0%.

</details>


### [5] [Towards Practical Zero-Knowledge Proof for PSPACE](https://arxiv.org/abs/2511.15071)
*Ashwin Karthikeyan,Hengyu Liu,Kuldeep S. Meel,Ning Luo*

Main category: cs.CR

TL;DR: 本文提出了首个实用的PSPACE完全语句的零知识证明协议，通过验证量化布尔公式(QBF)评估来实现。核心思想是在零知识中验证量化解析证明(Q-Res)，并设计了多项式编码和获胜策略证明协议。


<details>
  <summary>Details</summary>
Motivation: 现有的高效零知识证明仅限于NP语句，而PSPACE语句的零知识证明虽然理论上存在但缺乏实用性。本文旨在填补这一空白，为PSPACE完全语句提供实用的零知识证明方案。

Method: 开发了Q-Res证明的高效多项式编码，通过低开销的算术检查实现证明验证；设计了与QBF相关的获胜策略的零知识证明协议。

Result: 在QBFEVAL数据集上的评估显示，协议能在100秒内通过Q-Res证明验证72%的QBF评估，并为82%的实例验证获胜策略。

Conclusion: 本文首次实现了PSPACE完全语句的实用零知识证明，为更广泛的计算复杂性类别的零知识证明应用开辟了道路。

Abstract: Efficient zero-knowledge proofs (ZKPs) have been restricted to NP statements so far, whereas they exist for all statements in PSPACE. This work presents the first practical zero-knowledge (ZK) protocols for PSPACE-complete statements by enabling ZK proofs of QBF (Quantified Boolean Formula) evaluation. The core idea is to validate quantified resolution proofs (Q-Res) in ZK. We develop an efficient polynomial encoding of Q-Res proofs, enabling proof validation through low-overhead arithmetic checks. We also design a ZK protocol to prove knowledge of a winning strategy related to the QBF, which is often equally important in practice. We implement our protocols and evaluate them on QBFEVAL. The results show that our protocols can verify 72% of QBF evaluations via Q-Res proof and 82% of instances' winning strategies within 100 seconds, for instances where such proofs or strategies can be obtained.

</details>


### [6] [Towards a Formal Verification of Secure Vehicle Software Updates](https://arxiv.org/abs/2511.15479)
*Martin Slind Hagen,Emil Lundqvist,Alex Phu,Yenan Wang,Kim Strandberg,Elad Michael Schiller*

Main category: cs.CR

TL;DR: 本文对UniSUF软件更新框架进行了形式化安全分析，使用ProVerif验证其满足机密性、完整性、真实性、新鲜性、顺序性和活性等安全要求。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆(SDVs)的兴起，软件漏洞可能严重影响安全性、经济和社会。虽然UniSUF已被提出作为安全更新框架，但之前的安全评估未使用形式化验证方法。

Method: 建立UniSUF的架构和假设模型以反映真实汽车系统，开发基于ProVerif的框架，通过符号执行形式化验证UniSUF对关键安全要求的符合性。

Result: 分析结果表明UniSUF遵循指定的安全保证，确保其安全框架的正确性和可靠性。

Conclusion: UniSUF通过形式化验证被证明满足所有必需的安全要求，为软件定义车辆提供了可靠的安全更新解决方案。

Abstract: With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures.
  Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF's architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF's compliance with essential security requirements - confidentiality, integrity, authenticity, freshness, order, and liveness - demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.

</details>


### [7] [Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments](https://arxiv.org/abs/2511.15165)
*Jingzhuo Zhou*

Main category: cs.CR

TL;DR: 提出了AdapT-Bench基准套件，用于评估多模态大语言模型在学术环境中对抗动态钓鱼攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 学术机构和研究人员面临利用研究背景、学术合作和个人信息的高度定制化钓鱼攻击，现有安全基准缺乏学术背景信息，无法捕捉学术环境特有的攻击模式。

Method: 开发了AdapT-Bench统一方法框架和基准套件，系统评估MLLM在学术环境中的钓鱼攻击防御能力。

Result: 创建了一个专门针对学术环境的基准测试框架，能够更好地评估多模态大语言模型在动态、多语言和上下文相关的钓鱼攻击中的表现。

Conclusion: AdapT-Bench填补了现有安全基准在学术环境钓鱼攻击检测方面的空白，为评估MLLM防御能力提供了更合适的测试工具。

Abstract: The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.

</details>


### [8] [How To Cook The Fragmented Rug Pull?](https://arxiv.org/abs/2511.15463)
*Minh Trung Tran,Nasrin Sohrabi,Zahir Tari,Qin Wang*

Main category: cs.CR

TL;DR: 本文提出了碎片化拉地毯攻击(FRP)的概念，指出传统检测器假设攻击者会进行单次大规模抛售，而实际攻击往往通过时间分散和参与者分散来规避检测。


<details>
  <summary>Details</summary>
Motivation: 现有拉地毯检测器假设攻击者会保留流动性池代币并进行单次或少量大规模抛售，但实际攻击往往违反这些假设，通过时间分散和参与者分散来降低可见性。

Method: 定义了碎片化拉地毯攻击的三个核心策略：保持流动性池控制、将抛售分割为多个低影响微交易、通过多个钱包分散执行抛售。提出了三个原子谓词组及其正交组合的规避策略。

Result: 在大规模测量中，分析了303,614个流动性池，识别出105,434个FRP池，涉及34,192,767笔交易和401,838个抛售钱包。发现FRP池中所有者钱包参与抛售的比例降至33.1%，并检测到127,252个重复参与多个FRP池的串行诈骗钱包。

Conclusion: 碎片化拉地毯攻击策略在现实中广泛存在且具有操作意义，传统检测方法无法有效识别这类规避性攻击行为。

Abstract: Existing rug pull detectors assume a simple workflow: the deployer keeps liquidity pool (LP) tokens and performs one or a few large sells (within a day) that collapse the pool and cash out. In practice, however, many real-world exits violate these assumptions by splitting the attack across both time and actor dimensions: attackers break total extraction into many low-impact trades and route proceeds through multiple non-owner addresses, producing low-visibility drains.
  We formalize this family of attacks as the fragmented rug pull (FRP) and offer a compact recipe for a slow-stewed beef special: (i) keep the lid on (to preserve LP control so on-chain extraction remains feasible), (ii) chop thin slices (to split the total exit volume into many low-impact micro-trades that individually fall below impact thresholds), and (iii) pass the ladle (to delegate sells across multiple wallets so that each participant takes a small share of the extraction). Technically, we define three atomic predicate groups and show that their orthogonal combinations yield evasive strategies overlooked by prior heuristics (USENIX Sec 19, USENIX Sec 23).
  We validate the model with large-scale measurements. Our corpus contains 303,614 LPs, among which 105,434 are labeled as FRP pools. The labeled subset includes 34,192,767 pool-related transactions and 401,838 inflated-seller wallets, involving 1,501,408 unique interacting addresses. Notably, owner-wallet participation in inflated selling among FRP-flagged LPs has declined substantially (33.1% of cases), indicating a shift in scam behavior: the liquidity drain is no longer held on the owner wallet. We also detected 127,252 wallets acting as serial scammers when repeatedly engaging in inflated selling across multiple FRP LPs. Our empirical findings demonstrate that the evasive strategies we define are widespread and operationally significant.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: FERMAT是一个强化学习环境，用于自动化数学理论发现，包括概念发现和定理证明。研究探索了如何自动评估数学对象的趣味性，并引入了基于LLM的进化算法来合成趣味性度量函数。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放数学理论发现的重大挑战，自动化数学概念发现和定理证明过程。

Method: 引入FERMAT强化学习环境，使用符号动作建模概念发现和定理证明；开发基于LLM的进化算法，具有函数抽象功能，用于合成数学对象的趣味性度量。

Result: 在初等数论和有限域领域，基于LLM的进化算法相比硬编码基线取得了显著改进，成功发现了有趣的数学对象。

Conclusion: FERMAT环境为数学理论发现开辟了新的强化学习问题空间，基于LLM的进化算法在自动评估数学趣味性方面表现出色，为自动化数学发现提供了有前景的途径。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [10] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个系统级框架，用于检查和扰动多智能体交互中的信念状态，通过记录回放交互、支持外部查询和反事实证据注入来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛，提供可重现的方法来研究真实世界学科立场中的信念动态。

Method: 使用多智能体医疗案例模拟器，配备共享时间戳电子病历和持有真实实验室结果的预言智能体，通过角色特定的先验知识（如神经科医生、传染病专家）进行顺序或并行交互。

Result: 模拟显示智能体信念往往反映真实世界学科立场，包括过度依赖规范研究和抵制反证据，这些信念可以通过关键诊断时刻的断点进行追踪和询问。

Conclusion: Ask WhAI通过使这些动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了可重现的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [11] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 本文提出了一种完全自动化的LLM辅助工作流，使用GPT-4o处理文本位置信息，并通过交叉验证三个地理信息库来分配几何形状和可靠性评分。


<details>
  <summary>Details</summary>
Motivation: 灾害事件数据库中的位置数据通常以非结构化文本形式报告，存在粒度不一致和拼写问题，难以与空间数据集集成。

Method: 使用GPT-4o处理和清理文本位置信息，通过交叉验证GADM、OpenStreetMap和Wikidata三个独立地理信息库来分配几何形状，并根据来源一致性和可用性分配可靠性评分。

Result: 应用于2000-2024年EM-DAT数据集，成功地理编码14,215个事件，覆盖17,948个独特位置。

Conclusion: 该方法无需人工干预，覆盖所有灾害类型，支持跨源验证，并展示了LLMs从非结构化文本中提取和结构化地理信息的潜力。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [12] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: Project Rachel是一项行动研究，创建并追踪了一个完整的人工智能学术身份Rachel So，通过发表AI生成的研究论文来调查学术生态系统对AI作者身份的反应。


<details>
  <summary>Details</summary>
Motivation: 研究AI作者身份对学术生态系统的影响，为关于超级人类、超能力AI系统参与学术交流的未来辩论提供实证数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10多篇AI生成的研究论文，并追踪其被引用和同行评审邀请情况。

Result: Rachel So成功发表了10多篇论文，获得了引用，并收到了同行评审邀请。

Conclusion: 这项研究揭示了AI作者身份对出版商、研究人员和整个科学系统的潜在影响，为学术交流的未来发展提供了重要参考。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [13] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本文提出了一种概率方法来量化AI系统数据集在目标操作域中的代表性，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值的代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信度和安全性，关键在于训练和测试数据集的数据相关安全属性，如代表性。本文重点关注代表性，即场景数据反映系统设计安全运行条件（ODD）或预期遇到条件（TOD）的程度。

Method: 采用概率方法比较场景套件特征与TOD特征的统计分布，使用不精确贝叶斯方法处理有限数据和不确定先验，生成区间值的不确定性感知代表性估计。

Result: 通过数值示例比较了场景套件与推断TOD在天气、道路类型、时间等操作类别下的分布，在依赖性和先验不确定性条件下，估计了局部（类别间）和全局的代表性区间。

Conclusion: 提出的方法能够量化数据集在目标操作域中的代表性，并生成不确定性感知的区间估计，有助于评估AI系统的数据安全属性。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [14] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了Macro Action Quantization (MAQ)框架，通过将人类演示蒸馏为宏观动作来训练类似人类的强化学习智能体，在D4RL Adroit基准测试中显著提高了人类相似度。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在许多领域表现出超人类性能，但往往展现出与人类相比不自然的行为，这引发了可解释性和可信赖性的担忧。本文旨在设计能够产生类似人类行为的强化学习智能体。

Method: 将人类相似度建模为轨迹优化问题，采用后退时域控制作为可扩展实现。提出MAQ框架，使用Vector-Quantized VAE从人类演示中蒸馏出宏观动作。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了人类相似度，增加了轨迹相似度得分，并在人类评估研究中获得了所有RL智能体中最高的人类相似度排名。

Conclusion: MAQ可以轻松集成到各种现成的RL算法中，为学习类似人类的RL智能体开辟了有前景的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [15] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: 本研究开发了OpenBioLLM，一个模块化的多智能体框架，用于基因组问答。它使用开源模型替代GeneGPT的专有模型，通过智能体专业化实现工具路由、查询生成和响应验证，在保持性能的同时显著降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决GeneGPT依赖专有模型带来的可扩展性、运营成本、数据隐私和泛化性问题，探索开源模型在基因组问答中的潜力。

Method: 采用模块化多智能体框架，包括工具路由、查询生成和响应验证三个专业智能体，使用Llama 3.1、Qwen2.5等开源模型，无需额外微调或工具特定预训练。

Result: 在90%以上的基准任务中达到或超过GeneGPT性能，Gene-Turing平均得分0.849，GeneHop平均得分0.830，延迟降低40-50%。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，能够以更低的成本和更高的效率实现与专有模型相当的性能。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [16] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLM处理动作和变化推理问题，通过逐步执行动作推导最终状态并评估查询来获得答案。


<details>
  <summary>Details</summary>
Motivation: 解决动作和变化推理问题，利用LLM的能力来处理复杂的RAC任务。

Method: 提取RAC基本元素（动作和问题），逐步执行每个动作推导最终状态，然后评估查询与进展状态的匹配度。

Result: 在多个RAC基准测试中表现出色，在不同基准、领域、LLM主干和RAC任务类型上都取得强劲性能。

Conclusion: ProRAC框架在动作和变化推理问题上具有广泛适用性和强大性能。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [17] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于LLM的多智能体框架，通过三个专业智能体（科学家、提取器、测试器）的协作，结合外部领域知识和丰富的定性反馈机制，实现自动特征提取，在分类和回归任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动特征提取方法存在架构单一、反馈机制简单、缺乏外部知识整合等问题，限制了特征工程的质量和效果。

Method: 采用多智能体框架，包含科学家、提取器和测试器三个专业智能体，结合检索增强生成（RAG）系统引入外部知识，使用"泛滥-修剪"策略平衡特征探索与利用，并采用丰富的定性反馈机制。

Result: 在19个分类和9个回归数据集上显著优于最先进方法，并能发现新颖可测试的假设，如在心肌数据集中识别出新的潜在生物标志物。

Conclusion: Rogue One框架通过多智能体协作、外部知识整合和定性反馈机制，不仅提升了特征提取性能，还增强了特征的可解释性和科学发现能力。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [18] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是首个端到端评估大型推理模型安全性的基准，通过输入特征化、细粒度输出分析和人类安全对齐三个维度，全面评估从输入、中间推理到最终输出的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注输出层面判断，很少捕捉推理过程中的动态风险，而大型推理模型的链式思维能力可能引入新的安全风险，如有害内容的渐进式注入或被误导的推理过程所合理化。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计，考虑受影响群体和严重程度；2) 细粒度输出分析：通过微思维分块机制将长推理轨迹分割为语义连贯单元；3) 人类安全对齐：基于人类标注验证LLM评估结果。

Result: 对19个大型推理模型的评估表明，SafeRBench能够实现详细的多维度安全评估，从多个角度提供风险和保护机制的洞察。

Conclusion: SafeRBench为大型推理模型提供了全面的安全评估框架，能够有效识别和量化推理过程中的安全风险，填补了现有安全评估方法的空白。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [19] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中版权内容的框架，通过将LLM的过度自信转化为优势，识别训练数据与非训练数据的模式差异。


<details>
  <summary>Details</summary>
Motivation: LLM在训练时可能使用了受版权保护的内容，现有成员推断攻击方法因LLM的过度自信、缺乏真实训练数据和依赖经验阈值而面临挑战。

Method: 采用两阶段策略：(1) 将文件分割成小片段减少对大规模训练数据的依赖；(2) 使用不确定性引导的无监督聚类消除经验阈值需求。

Result: 在LLaMA 7b和LLaMA2 7b上分别达到90.1%和91.6%的平均平衡准确率，相比SOTA基线有90%以上的相对提升，最高达到93.8%平衡准确率，在GPT-J 6B上也保持高性能。

Conclusion: 这是首个将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [20] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 本文认为仅靠效率提升无法实现可持续的推理AI，需要将明确限制嵌入到这类系统的优化和治理中


<details>
  <summary>Details</summary>
Motivation: AI研究正转向复杂问题解决，推理AI缺乏需求饱和点，性能随计算投入呈指数级增长，而效率改进正接近物理极限

Method: 讨论了推理AI的可持续性问题，分析效率改进的物理限制和推理AI缺乏饱和点的特性

Result: 识别出仅靠效率无法解决推理AI的可持续性挑战，需要新的研究政策方向

Conclusion: 必须在推理AI系统的优化和治理中嵌入明确限制，以实现可持续发展

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [21] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文探讨了AI研究中两种基本智能观：智能现实主义（认为智能是单一通用能力）和智能多元主义（认为智能是多样化、情境依赖的能力），分析了这两种观点如何影响研究方法、现象解释和风险评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中对智能本质的隐含假设影响了研究方法和结论，但这些问题往往未被明确讨论。本文旨在揭示这些基础假设，以澄清AI研究中的分歧。

Method: 通过分析当前AI研究中的辩论，展示两种智能观如何在不同领域影响实证证据的解释，包括方法论、现象解释和风险评估三个层面。

Result: 发现智能现实主义导致追求通用模型和统一基准，关注超级智能风险；智能多元主义则强调情境化方法和多样化威胁，需要特定领域解决方案。

Conclusion: 明确这些基础假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的学术讨论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [22] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个基于《文明V》的综合性挑战环境，旨在同时测试强化学习智能体在部分可观测性、信用分配、表示学习、巨大动作空间等多个经典挑战上的综合能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准主要评估智能体在不同无关任务间切换策略的能力，而缺乏对智能体在多个相互关联挑战中进行深度推理能力的测试。

Method: 开发了Terra Nova环境，这是一个单一环境，其中多个经典强化学习挑战同时出现，要求智能体进行整合性的长期理解。

Result: 提出了综合性挑战环境的新定义，排除了仅聚合无关任务的并行流环境，强调需要测试智能体在多个相互关联挑战中的深度推理能力。

Conclusion: Terra Nova为强化学习研究提供了一个更全面的测试平台，能够更好地评估智能体在复杂、相互关联环境中的综合表现。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [23] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文研究了智能体能否通过交互学习获得类人物理推理能力，提出了IPR模型，使用世界模型推演来增强VLM策略，并在1000+游戏中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索智能体是否能够像人类一样通过观察和交互环境来学习物理和因果关系，并随着经验积累不断改进推理能力。

Method: 提出IPR（交互式物理推理器），使用世界模型推演来评分和增强VLM策略，并引入PhysCode物理中心动作编码来对齐语义意图与动力学。

Result: 在1000+游戏上预训练后，IPR在三个推理层次上表现稳健，整体性能与GPT-5相当，在好奇心层次上超越GPT-5，且性能随训练游戏和交互步数增加而提升。

Conclusion: 物理中心的交互是实现持续改进物理推理能力的有效路径，模型能够零样本迁移到未见过的游戏中。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 提出了一种名为RemoteOptiGraph的分布式优化建模抽象，扩展了Plasmo.jl中的OptiGraph模型，支持分布式内存环境下的优化问题建模和求解。


<details>
  <summary>Details</summary>
Motivation: 为分布式内存系统提供统一的优化问题建模方法，避免定制化建模方式，并为开发通用元算法（如Benders或拉格朗日分解）提供基础。

Method: 通过InterWorkerEdges管理跨工作节点的链接约束，使OptiGraph能够在分布式内存环境中使用，在Plasmo.jl开源包中实现该抽象。

Result: 使用该抽象求解美国西部混合整数容量扩展模型（超过1200万个变量和约束），结合Benders分解比无分解求解快7.5倍。

Conclusion: RemoteOptiGraph抽象为分布式优化问题提供了通用且灵活的建模框架，显著提升了大规模优化问题的求解效率。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo.jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo.jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [25] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle是一个双层共识架构，通过BB-Core层降低延迟实现高吞吐量，BB-Guard层提供去中心化时间戳和容错恢复，在保持强安全性和活跃性的同时实现亚秒级最终性。


<details>
  <summary>Details</summary>
Motivation: 解决区块链共识在安全性、延迟和去中心化之间的三难困境，高吞吐量系统通常需要牺牲去中心化或对强对手的鲁棒性，而高度去中心化和安全的系统往往性能较低。

Method: 提出BlueBottle双层共识架构：BB-Core层采用n=5f+1协议，以部分容错性换取较低最终性延迟；BB-Guard层提供去中心化时间戳、主动错误行为检测和同步恢复路径。

Result: 实验显示BB-Core相比Mysticeti降低延迟20-25%，在温和同步假设下实现乐观亚秒级最终性和高吞吐量，同时保持强安全性和活跃性。

Conclusion: BlueBottle的双层架构成功平衡了共识三难困境，通过核心层优化性能、守护层提供安全保障，实现了高性能与强安全性的统一。

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [26] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: CoroAMU是一个硬件-软件协同设计的系统，通过优化的协程代码生成、解耦内存操作和内存引导的分支预测机制，在分解式内存系统中有效隐藏内存延迟，相比现有方法实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用在分解式内存系统中面临严重的内存延迟问题。虽然协程在交错任务和隐藏内存延迟方面表现出潜力，但难以平衡延迟隐藏效率与运行时开销。

Method: 提出CoroAMU系统，包含编译器过程优化协程代码生成、最小化上下文和合并请求，以及硬件支持的解耦内存操作，通过协程特定的内存操作和内存引导分支预测机制增强异步内存单元。

Result: 在Intel服务器处理器上，CoroAMU编译器相比最先进的协程方法实现1.51倍加速。结合优化的硬件解耦内存访问，在FPGA模拟的分解式系统上，在200ns和800ns延迟下分别实现3.39倍和4.87倍的平均性能提升。

Conclusion: CoroAMU通过硬件-软件协同设计有效解决了分解式内存系统中的内存延迟问题，证明了协程与专用硬件支持结合在提升数据密集型应用性能方面的巨大潜力。

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [27] [DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution](https://arxiv.org/abs/2511.15367)
*Xin Yang,Xin Fan,Zengshi Wang,Jun Han*

Main category: cs.AR

TL;DR: 提出了DARE，一种具有密集化ISA和过滤前瞻执行的不规则性容忍MPU，用于优化稀疏DNN在CPU上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前硬件-算法协同优化不足导致稀疏DNN在CPU上性能不佳，存在内存访问不规则和计算利用率低的问题。

Method: 扩展ISA支持稀疏操作密集化，并配备具有过滤能力的轻量级前瞻执行机制。

Result: DARE相比基线性能提升1.04-4.44倍，能效提升1.00-22.8倍，硬件开销比NVR低3.91倍。

Conclusion: DARE有效解决了稀疏DNN在MPU上的不规则性问题，显著提升了性能和能效。

Abstract: Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.
  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.
  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\times$ to 4.44$\times$ and increases energy efficiency by 1.00$\times$ to 22.8$\times$ over the baseline, with 3.91$\times$ lower hardware overhead than NVR.

</details>


### [28] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: DCC是一个面向PIM系统的数据中心化ML编译器，通过联合优化数据重排和计算代码，解决了主机处理器和PIM核心数据布局不匹配的问题，在多种PIM后端上显著提升了ML内核和LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 主机处理器和PIM核心需要不同的数据布局，导致ML内核执行时需要数据重排，这带来了显著的性能和可编程性挑战，且现有编译方法缺乏对多样化ML内核和PIM后端的系统性优化。

Method: 设计了DCC编译器，采用多层PIM抽象，将数据分区策略映射到计算循环分区，应用PIM特定代码优化，并利用快速准确的性能预测模型来选择最优配置。

Result: 在HBM-PIM上实现最高7.68倍加速（平均2.7倍），在AttAcc PIM后端上实现最高13.17倍加速（平均5.75倍）。在端到端LLM推理中，GPT-3和LLaMA-2加速最高达7.71倍（平均4.88倍）。

Conclusion: 数据重排和计算代码优化是相互依赖的，需要在调优过程中联合优化，DCC通过统一调优过程有效解决了PIM系统中的数据布局挑战。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


### [29] [Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond](https://arxiv.org/abs/2511.15564)
*Paul Scheffler,Thomas Benz,Tim Fischer,Lorenzo Leone,Sina Arjmandpour,Luca Benini*

Main category: cs.AR

TL;DR: 提出了一个基于开源chiplet的RISC-V系统路线图，从Occamy双chiplet系统扩展到Ramora和Ogopogo四chiplet架构，旨在缩小与专有设计的性能差距，并将开放性扩展到EDA工具链等领域。


<details>
  <summary>Details</summary>
Motivation: 缩小RISC-V开源系统与专有设计之间的性能差距，推动高性能计算和人工智能领域的开源硬件发展。

Method: 采用chiplet架构方法，从12nm FinFET工艺的Occamy双chiplet系统开始，扩展到基于mesh-NoC的Ramora系统，再到7nm工艺的Ogopogo四chiplet概念架构。

Result: 实现了首个开源硅验证的双chiplet RISC-V多核系统(Occamy)，并开发出具有最先进计算密度的四chiplet架构(Ogopogo)。

Conclusion: 开源chiplet方法能够有效提升RISC-V系统性能，未来需要将开放性扩展到仿真、EDA、PDK和片外PHY等更广泛的领域。

Abstract: We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.

</details>
