<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 22]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.CR](#cs.CR) [Total: 38]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: 提出了一种多模态大语言模型助手(MLLMA)，不仅能预测布线拥塞，还能提供可解释的设计建议，通过结合视觉、表格和文本输入来改善芯片物理设计。


<details>
  <summary>Details</summary>
Motivation: 现代芯片物理设计严重依赖EDA工具，但这些工具难以提供可解释的反馈或可操作的设计改进指导，需要一种能提供人类可理解建议的解决方案。

Method: 结合MLLM引导的遗传提示进行自动特征生成，以及可解释的偏好学习框架，建模布线拥塞相关的权衡，整合视觉、表格和文本输入。

Result: 在CircuitNet基准测试中，该方法在准确性和可解释性方面均优于现有模型，设计建议与真实设计原则一致且对工程师具有可操作性。

Conclusion: 这项工作展示了MLLM作为交互式助手在可解释和上下文感知的物理设计优化中的潜力。

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [2] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: 本文提出了一种通过内存地址流编码程序上下文的方法，使内存设备能够感知软件状态，从而优化内存访问和分层管理。


<details>
  <summary>Details</summary>
Motivation: 传统内存监控无法准确反映实际内存访问模式，硬件预取和调度导致可观测性降低，限制了数据移动和分层优化。程序上下文在内存总线传输过程中丢失，无法被内存设备利用。

Method: 在内存读取地址流中以非破坏性方式编码用户可见状态作为可检测的数据包，无需显著容量开销、驱动程序或特殊访问权限。构建了元数据注入的原型系统，可在内存地址跟踪中可靠检测和解码。

Result: 成功实现了端到端系统原型，能够通过内存地址跟踪可靠检测和解码元数据。演示了精确代码执行标记和对象地址范围跟踪的应用案例。

Conclusion: 该方法使程序上下文在内存设备中可见，未来结合近内存计算可实现实时元数据解码，为用户提供定制化遥测和统计，或根据应用提示执行请求优先级排序、数据重映射和设备重新配置等功能。

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [3] [Opportunities and Challenges for 3D Systems and Their Design](https://arxiv.org/abs/2510.15880)
*Philip Emma,Eren Kurshan*

Main category: cs.AR

TL;DR: 3D集成技术随着光刻缩放挑战增加和微型通孔制造能力提升而受到广泛关注，它通过垂直堆叠提高密度，但也带来了功率密度增加、设计和制造技术的新挑战，需要跨层协同设计、独立测试和系统组装优化。


<details>
  <summary>Details</summary>
Motivation: 随着光刻缩放变得越来越困难，以及制造微型通孔能力的显著提升，3D集成技术重新获得广泛关注。它类似于摩尔定律，能够提高芯片密度，但高密度也带来了功率密度增加的挑战。

Method: 3D集成通过垂直堆叠电路层来提高密度，需要跨层协同设计电路、通孔和宏单元的布局，确保组装时的空间对应关系。每层需要独立测试，系统需要支持组装前后的测试和诊断。

Result: 3D集成作为缩放加速器，在提高密度的同时，对设计、制造和测试技术提出了新的要求，包括跨层协同设计、独立测试能力和组装优化等方面。

Conclusion: 要充分发挥3D集成的优势，需要明确3D系统的杠杆作用，并系统性地解决设计、组装和测试过程中面临的新挑战。

Abstract: Although it is not a new concept, 3D integration increasingly receives
widespread interest and focus as lithographic scaling becomes more challenging,
and as the ability to make miniature vias greatly improves. Like Moores law, 3D
integration improves density. With improvements in packaging density, however,
come the challenges associated with its inherently higher power density. And
though it acts somewhat as a scaling accelerator, the vertical integration also
poses new challenges to design and manufacturing technologies. The placement of
circuits, vias, and macros in the planes of a 3D stack must be co-designed
across layers (or must conform to new standards) so that, when assembled, they
have correct spatial correspondence. Each layer, although perhaps being a mere
functional slice through a system (and we can slice the system in many
different ways), must be independently testable so that we can systematically
test and diagnose subsystems before and after final assembly. When those layers
are assembled, they must come together in a way that enables a sensible yield
and facilitates testing the finished product. To make the most of 3D
integration, we should articulate the leverages of 3D systems (other
researchers offer a more complete treatment elsewhere). Then we can enumerate
and elucidate many of the new challenges posed by the design, assembly, and
test of 3D systems.

</details>


### [4] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: FlexLink是一个创新的集体通信框架，通过聚合NVLink、PCIe和RDMA NICs等异构链路来解决大语言模型多节点部署中的通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，多节点部署成为必需，但当前通信库如NCCL仅使用单一互连（如NVLink），导致性能瓶颈，而其他硬件资源如PCIe和RDMA NICs在密集工作负载下被闲置。

Method: FlexLink采用两阶段自适应负载均衡策略，动态地将通信流量分区到所有可用链路上，确保快速互连不被较慢的互连所限制。

Result: 在8-GPU H800服务器上，FlexLink将AllReduce和AllGather等集体操作符的带宽分别提高了26%和27%，通过将2-22%的总通信流量卸载到之前未充分利用的PCIe和RDMA NICs上实现。

Conclusion: FlexLink作为无损的即插即用替代方案，与NCCL API兼容，易于采用，有效解决了多节点部署中的通信瓶颈问题。

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [5] [Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I](https://arxiv.org/abs/2510.15884)
*Faizan A Khattak,Mantas Mikaitis*

Main category: cs.AR

TL;DR: 本文扩展了分析GPU矩阵乘法器数值特征的方法，提出了一种架构无关的测试方案，适用于各种输入输出精度格式，并成功应用于消费级NVIDIA GPU。


<details>
  <summary>Details</summary>
Motivation: 研究NVIDIA和AMD数据中心GPU中矩阵乘法器硬件单元的数值特征（如舍入、归一化、累加器内部精度），并将分析方法扩展到消费级NVIDIA GPU。

Method: 实现架构无关的测试方案，使用不依赖设备特定常量的测试向量生成方法，适用于广泛的混合精度格式。

Result: 应用该方案到RTX-3060和Ada RTX-1000显卡，确定了binary16、TensorFloat32和bfloat16输入格式以及binary16和binary32输出格式的矩阵乘法器数值特征，发现RTX-3060与数据中心GPU A100的数值特征相同。

Conclusion: 该方法无需修改即可用于分析更新的NVIDIA GPU（如Hopper或Blackwell）及其未来继任者，以及任何输入/输出格式组合，包括最新的8位浮点格式。

Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data
centre GPUs have recently been studied. Features such as rounding,
normalisation, and internal precision of the accumulators are of interest. In
this paper, we extend the methodology for analysing those features, to
consumer-grade NVIDIA GPUs by implementing an architecture-independent test
scheme for various input and output precision formats. Unlike current
approaches, the proposed test vector generation method neither performs an
exhaustive search nor relies on hard-coded {constants that are device-specific,
yet remains applicable to a wide range of mixed-precision formats. We have
applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada
Lovelace architecture) graphics cards and determined numerical features of
matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating
point formats and binary16 and binary32 IEEE 754 output formats. Our
methodology allowed us to determine that} the numerical features of RTX-3060, a
consumer-grade GPU, are identical to those of the A100, a data centre GPU. We
do not expect our code to require any changes for performing analysis of matrix
multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future
successors, and any input/output format combination, including the latest 8-bit
floating-point formats.

</details>


### [6] [ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices](https://arxiv.org/abs/2510.15885)
*Dingcui Yu,Zonghuan Yan,Jialin Liu,Yumiao Zhao,Yanyun Wang,Xinghui Duan,Yina Lv,Liang Shi*

Main category: cs.AR

TL;DR: ConZone+是一个针对消费级分区闪存存储的模拟器，通过添加块接口支持解决了原版ConZone无法挂载文件系统的问题，提供了部署脚本和多项增强功能，帮助用户探索存储架构并集成优化方案。


<details>
  <summary>Details</summary>
Motivation: 为了促进对消费级分区闪存存储软硬件设计的理解和高效改进，需要能够模拟此类系统资源约束和架构特征的模拟器。原版ConZone由于缺乏原地更新能力而无法挂载文件系统，限制了其实用性。

Method: ConZone+在原版ConZone基础上扩展了块接口支持，提供了部署脚本，并引入了多项增强功能。通过比较代表性硬件架构和现有技术来验证准确性，并进行了多个案例研究。

Result: 验证了ConZone+的准确性，通过案例研究探索了分区存储设计和当前文件系统的不足之处。

Conclusion: ConZone+作为首个针对消费级分区闪存存储的模拟器，有效解决了原版的限制，为探索存储架构和集成系统软件优化提供了实用工具。

Abstract: To facilitate the understanding and efficient enhancement of software and
hardware design for consumer-grade zoned flash storage, ConZone is proposed as
the first emulator designed to model the resource constraints and architectural
features typical of such systems. It incorporates essential components commonly
deployed in consumer-grade devices, including limited logical to physical
mapping caches, constrained write buffers, and hybrid flash media management.
However, ConZone cannot be mounted with the file system due to the lack of
in-place update capability, which is required by the metadata area of F2FS. To
improve the usability of the emulator, ConZone+ extends ConZone with support
for a block interface. We also provide a script to help the deployment and
introduces several enhancements over the original version. Users can explore
the internal architecture of consumer-grade zoned flash storage and integrate
their optimizations with system software using ConZone+. We validate the
accuracy of ConZone+ by comparing a hardware architecture representative of
consumer-grade zoned flash storage and comparing it with the state-of-the-art.
In addition, we conduct several case studies using ConZone+ to investigate the
design of zoned storage and explore the inadequacies of the current file
system.

</details>


### [7] [basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I](https://arxiv.org/abs/2510.15887)
*Hyun Woo Kang,Ji Woong Choi*

Main category: cs.AR

TL;DR: BASIC_RV32s是一个开源的RISC-V RV32I架构框架，提供了从单周期核心到5级流水线设计的完整实现路径，包括冒险转发、动态分支预测和异常处理，并在FPGA上验证了1.09 DMIPS/MHz的性能。


<details>
  <summary>Details</summary>
Motivation: 解决RISC-V架构理论知识与硬件实现之间的差距，为开源硬件生态系统提供可复现的教学路径。

Method: 采用Patterson和Hennessy经典方法学，从单周期核心逐步演进到5级流水线设计，集成到SoC中并在Xilinx Artix-7 FPGA上实现验证。

Result: 在50MHz频率下实现了1.09 DMIPS/MHz的性能，提供了完整的RTL源代码、逻辑框图和发展日志。

Conclusion: BASIC_RV32s为开源硬件社区提供了实用的微架构路线图，有助于促进RISC-V架构的教育和应用发展。

Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a
practical microarchitectural roadmap for the RISC-V RV32I architecture,
addressing the gap between theoretical knowledge and hardware implementation.
Following the classic Patterson and Hennessy methodology, the design evolves
from a basic single-cycle core to a 5-stage pipelined core design with full
hazard forwarding, dynamic branch prediction, and exception handling. For
verification, the final core design is integrated into a System-on-Chip (SoC)
with Universal Asynchronous Receiver-Transmitter (UART) communication
implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving
1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50
MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level
logic block diagrams, and development logs under MIT license on GitHub,
BASIC_RV32s offers a reproducible instructional pathway for the open-source
hardware ecosystem.

</details>


### [8] [Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol](https://arxiv.org/abs/2510.15888)
*Konstantinos Kafousis*

Main category: cs.AR

TL;DR: 提出了一种简化的硬件事务内存实现，仅通过扩展Load-Linked和Store-Conditional指令语义，无需修改缓存一致性协议，并将事务读写集限制在少量缓存行内。


<details>
  <summary>Details</summary>
Motivation: 现有HTM实现硬件复杂度高、需要ISA扩展和缓存协议修改，导致采用率低。本文旨在通过简化设计降低实现复杂性。

Method: 基于现有LL/SC指令扩展语义，仅需修改L1数据缓存，限制事务读写集不超过8个缓存行，提供两种基于重试检测的前向进度保证机制。

Result: 在Gem5中模拟验证，并发数据结构中读写集最多8个缓存行即可满足需求，在低争用情况下性能优于TTS锁。

Conclusion: 提出的简化HTM设计在保持良好性能的同时显著降低了实现复杂度，特别适用于争用分散的并发场景。

Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as
with traditional coarse-grain locks or similar, while benefiting from the
performance advantages of fine-grained locking. Many HTM implementations have
been proposed, but they have not received widespread adoption because of their
high hardware complexity, their need for additions to the Instruction Set
Architecture (ISA), and often for modifications to the cache coherence
protocol.
  We show that HTM can be implemented without adding new instructions -- merely
by extending the semantics of two existing, Load-Linked and Store-Conditional.
Also, our proposed design does not modify or extend standard coherence
protocols. We further propose to drastically simplify the implementation of HTM
-- confined to modifications in the L1 Data Cache only -- by restricting it to
applications where the write set plus the read set of each transaction do not
exceed a small number of cache lines. We also propose two alternative
mechanisms to guarantee forward progress, both based on detecting retrial
attempts.
  We simulated our proposed design in Gem5, and we used it to implement several
popular concurrent data structures, showing that a maximum of eight (8) words
(cache lines) suffice for the write plus read sets. We provide a detailed
explanation of selected implementations, clarifying the intended usage of our
HTM from a programmer's perspective. We evaluated our HTM under varying
contention levels to explore its scalability limits. The results indicate that
our HTM provides good performance in concurrent data structures when contention
is spread across multiple nodes: in such cases, the percentage of aborts
relative to successful commits is very low. In the atomic fetch-and-increment
benchmark for multiple shared counters, the results show that, under
low-congestion, our HTM improves performance relative to the TTS lock.

</details>


### [9] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: 本文探讨了3D共封装光学技术如何解决AI工作负载增长中的互连瓶颈，通过光互连技术实现跨机架GPU集群的扩展，从而支持万亿参数MoE模型的训练。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的持续增长，传统半导体缩放放缓，高速互连成为新的扩展引擎。铜互连的最大传输距离限制（约1米）使得扩展域仅限于单个机架内，需要新的解决方案来连接跨多个数据中心机架的GPU。

Method: 采用3D堆叠光学和逻辑技术，开发了3D CPO（共封装光学）解决方案，通过光互连实现GPU包和交换机之间的高速连接，建模分析了在扩展域内训练超过万亿参数的MoE模型时的性能表现。

Result: 3D CPO技术实现了带宽和基数的显著提升，使扩展能力增加8倍，为扩展域内的多维并行提供了新机会，训练时间减少了2.7倍，实现了前所未有的模型扩展。

Conclusion: 3D共封装光学技术是解决AI工作负载扩展瓶颈的关键创新，通过光互连技术突破了传统电气互连的距离限制，为大规模AI模型的训练提供了高效的扩展解决方案。

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [10] [DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms](https://arxiv.org/abs/2510.15897)
*Kien Le Trung,Truong-Son Hy*

Main category: cs.AR

TL;DR: DiffPlace是一个基于条件去噪扩散过程的芯片布局框架，能够在不重新训练的情况下泛化到未见过的电路网表，实现可迁移的布局策略。


<details>
  <summary>Details</summary>
Motivation: 传统芯片布局方法依赖分析优化或强化学习，难以处理硬性布局约束或需要为每个新电路设计进行昂贵的在线训练。DiffPlace旨在解决这些限制，弥合基于优化和基于学习方法之间的差距。

Method: 将芯片布局制定为条件去噪扩散过程，利用扩散模型的生成能力在广阔的布局空间中高效探索，同时基于电路连接性和相对质量指标来识别全局最优解。结合能量引导采样和约束流形扩散来确保布局合法性。

Result: 在所有实验场景中实现了极低的重叠率，为现代VLSI设计提供了自动化的高质量芯片布局实用路径。

Conclusion: DiffPlace框架通过条件扩散模型成功解决了芯片布局问题，提供了可迁移的布局策略，为VLSI设计自动化开辟了新的可能性。

Abstract: Chip placement, the task of determining optimal positions of circuit modules
on a chip canvas, is a critical step in the VLSI design flow that directly
impacts performance, power consumption, and routability. Traditional methods
rely on analytical optimization or reinforcement learning, which struggle with
hard placement constraints or require expensive online training for each new
circuit design. To address these limitations, we introduce DiffPlace, a
framework that formulates chip placement as a conditional denoising diffusion
process, enabling transferable placement policies that generalize to unseen
circuit netlists without retraining. DiffPlace leverages the generative
capabilities of diffusion models to efficiently explore the vast space of
placement while conditioning on circuit connectivity and relative quality
metrics to identify optimal solutions globally. Our approach combines
energy-guided sampling with constrained manifold diffusion to ensure placement
legality, achieving extremely low overlap across all experimental scenarios.
Our method bridges the gap between optimization-based and learning-based
approaches, offering a practical path toward automated, high-quality chip
placement for modern VLSI design. Our source code is publicly available at:
https://github.com/HySonLab/DiffPlace/

</details>


### [11] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: 论文提出VeriPPA框架，使用大语言模型进行芯片设计的PPA优化和Verilog代码生成，通过两阶段方法提高代码正确性和优化性能。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在芯片设计领域的潜力，解决PPA优化和Verilog代码生成的自动化问题。

Method: 采用两阶段框架：第一阶段改进Verilog代码的功能和语法正确性，第二阶段优化代码以满足PPA约束。

Result: 在RTLLM数据集上达到81.37%语法正确率和62.06%功能正确率；在VerilogEval数据集上达到99.56%语法正确率和43.79%功能正确率，均超越现有最佳方法。

Conclusion: 大语言模型在处理复杂技术领域方面具有巨大潜力，为芯片设计自动化提供了有前景的发展方向。

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [12] [Fully Automated Verification Framework for Configurable IPs: From Requirements to Results](https://arxiv.org/abs/2510.15902)
*Shuhang Zhang,Jelena Radulovic,Thorsten Dworzak*

Main category: cs.AR

TL;DR: 提出一个全自动化的需求驱动功能验证框架，用于可配置IP的验证，通过自动化关键流程显著减少验证工作量


<details>
  <summary>Details</summary>
Motivation: 半导体行业竞争加剧导致芯片价格下降压力，同时需要保持质量和可靠性。可配置IP的功能验证由于复杂性和资源密集性成为开发成本的主要贡献者

Method: 开发全自动化框架，自动化vPlan生成、测试平台创建、回归执行和需求管理工具中的报告等关键流程

Result: 框架大幅减少验证工作量，加速开发周期，最小化人为错误，提高覆盖率

Conclusion: 该方法为验证可配置IP的挑战提供了可扩展且高效的解决方案

Abstract: The increasing competition in the semiconductor industry has created
significant pressure to reduce chip prices while maintaining quality and
reliability. Functional verification, particularly for configurable IPs, is a
major contributor to development costs due to its complexity and
resource-intensive nature. To address this, we propose a fully automated
framework for requirements driven functional verification. The framework
automates key processes, including vPlan generation, testbench creation,
regression execution, and reporting in a requirements management tool,
drastically reducing verification effort. This approach accelerates development
cycles, minimizes human error, and enhances coverage, offering a scalable and
efficient solution to the challenges of verifying configurable IPs.

</details>


### [13] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: 提出了意图驱动存储系统（IDSS），利用大语言模型从非结构化信号中推断工作负载和系统意图，指导存储系统的自适应和跨层参数重配置，在FileBench工作负载上可提升IOPS达2.45倍。


<details>
  <summary>Details</summary>
Motivation: 现有存储系统缺乏对工作负载意图的可见性，导致无法适应现代大规模数据密集型应用的语义，造成脆弱的启发式方法和碎片化的优化。

Method: 提出IDSS架构，将大语言模型集成到存储控制回路中，通过四个设计原则实现意图推断和参数重配置，包括缓存和预取等存储组件的配置优化。

Result: 在FileBench工作负载上的初步结果显示，IDSS能够通过解释意图和生成可操作的配置，将IOPS提升高达2.45倍。

Conclusion: 当受到护栏约束并嵌入结构化工作流时，大语言模型可以作为高级语义优化器，弥合应用目标与低级系统控制之间的差距，指向存储系统更加自适应、自主和与动态工作负载需求对齐的未来。

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [14] [Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions](https://arxiv.org/abs/2510.15907)
*Era Thaqi,Dennis Eigner,Arman Ferdowsi,Ulrich Schmid*

Main category: cs.AR

TL;DR: 提出了一种基于解析延迟公式的数字集成电路符号时序分析方法，通过计算内部信号转换时间的闭式解析表达式，实现无需仿真的时序分析和参数敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够进行符号时序分析的方法，避免传统仿真方法，并支持对时序特性与输入信号及门参数依赖关系的解析研究。

Method: 基于Ferdowsi等人开发的2输入NOR、NAND和Muller-C门的解析延迟公式，在固定信号转换顺序下计算内部信号转换时间的闭式解析表达式。

Result: 实现了基于SageMath计算机代数系统的框架，并在c17 slack基准电路的NOR门版本上进行了验证，能够进行敏感性分析。

Conclusion: 该方法为数字集成电路时序分析提供了一种高效的符号方法，支持解析研究时序特性对输入信号和门参数的依赖性。

Abstract: We propose a novel approach to symbolic timing analysis for digital
integrated circuits based on recently developed analytic delay formulas for
2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a
fixed order of the transitions of all input and internal signals of a circuit,
our framework computes closed-form analytic delay expressions for all the
internal signal transition times that depend on (i) the symbolic transition
times of the relevant input signals and (ii) the model parameters of the
relevant gates. The resulting formulas facilitate per-transition timing
analysis without any simulation, by instantiating the symbolic input transition
times and the gate parameters. More importantly, however, they also enable an
\emph{analytic} study of the dependencies of certain timing properties on input
signals and gate parameters. For instance, differentiating a symbolic delay
expression with respect to a gate parameter or input transition time enables
sensitivity analysis. As a proof of concept, we implement our approach using
the computer algebra system SageMath and apply it to the NOR-gate version of
the c17 slack benchmark circuit.

</details>


### [15] [UPMEM Unleashed: Software Secrets for Speed](https://arxiv.org/abs/2510.15927)
*Krystian Chmielewski,Jarosław Ławnicki,Uladzislau Lukyanau,Tadeusz Kobus,Maciej Maciejewski*

Main category: cs.AR

TL;DR: 本文揭示了UPMEM PIM平台软件栈中的低效问题，通过汇编代码修改、位串行处理技术以及NUMA感知的内存分配优化，显著提升了整数运算和矩阵向量乘法的性能。


<details>
  <summary>Details</summary>
Motivation: PIM平台在数据管理和并行编程方面面临独特挑战，现有SDK仍有很大的性能优化空间，需要探索非标准编程技术来提升计算效率。

Method: 修改UPMEM编译器生成的汇编代码、采用位串行处理低精度数据、扩展API以支持NUMA架构感知的内存分配。

Result: 整数加法性能提升1.6-2倍，整数乘法提升1.4-5.9倍；INT4位串行点积计算比基线快2.7倍以上；主机-PIM数据传输一致性提升2.9倍；优化后的INT8 GEMV比基线快3.5倍，比双路CPU服务器快3倍以上，INT4 GEMV快10倍。

Conclusion: 通过简单的软件栈优化和创新的编程技术，可以显著提升PIM平台的性能，使其在处理低精度矩阵运算时超越传统CPU服务器。

Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique
challenges in data management and parallel programming on limited processing
units. Although software development kits (SDKs) for PIM, such as the UPMEM
SDK, provide essential tools, these emerging platforms still leave significant
room for performance optimization. In this paper, we reveal surprising
inefficiencies in UPMEM software stack and play with non-standard programming
techniques. By making simple modifications to the assembly generated by the
UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x
in integer multiplication, depending on the data type. We also demonstrate that
bit-serial processing of low precision data is a viable option for UPMEM: in
INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup
over the baseline. Minor API extensions for PIM allocation that account for the
non-uniform memory access (NUMA) architecture of the server further improve the
consistency and throughput of host-PIM data transfers by up to 2.9x. Finally,
we show that, when the matrix is preloaded into PIM, our optimized kernels
outperform a dual-socket CPU server by over 3x for INT8 generalized
matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized
INT8 GEMV kernel outperforms the baseline 3.5x.

</details>


### [16] [Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations](https://arxiv.org/abs/2510.15908)
*Hana Chitsaz,Johnson Umeike,Amirmahdi Namjoo,Babak N. Safa,Bahar Asgari*

Main category: cs.AR

TL;DR: 本文对有限元生物力学模拟进行了全面的工作负载分析，揭示了当前硬件架构在性能扩展性方面的瓶颈，并提出了基于FPGA的领域特定加速器优化方案。


<details>
  <summary>Details</summary>
Motivation: 当前生物力学有限元模拟在硬件和软件架构上存在效率问题，限制了性能和可扩展性，特别是在材料参数识别等迭代任务中。可重构硬件（如FPGA）提供了领域特定加速的潜力，但这一潜力在生物力学领域尚未充分探索。

Method: 使用FEBio模拟器进行有限元生物力学工作负载表征，结合gem5敏感性研究和VTune分析，识别性能瓶颈和最优硬件配置。

Result: VTune分析显示小工作负载前端停滞约为13.1%，而大工作负载主要受后端瓶颈影响，后端绑定周期从59.9%到超过82.2%。gem5敏感性研究确定了领域特定加速器的最优硬件配置，发现不合理的流水线、内存或分支预测器设置可使性能下降高达37.1%。

Conclusion: 研究结果强调了架构感知协同设计的必要性，以有效支持生物力学模拟工作负载，为基于FPGA的领域特定加速器开发提供了重要指导。

Abstract: Finite element simulations are essential in biomechanics, enabling detailed
modeling of tissues and organs. However, architectural inefficiencies in
current hardware and software stacks limit performance and scalability,
especially for iterative tasks like material parameter identification. As a
result, workflows often sacrifice fidelity for tractability. Reconfigurable
hardware, such as FPGAs, offers a promising path to domain-specific
acceleration without the cost of ASICs, but its potential in biomechanics
remains underexplored. This paper presents Belenos, a comprehensive workload
characterization of finite element biomechanics using FEBio, a widely adopted
simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal
that smaller workloads experience moderate front-end stalls, typically around
13.1%, whereas larger workloads are dominated by significant back-end
bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.
Complementary gem5 sensitivity studies identify optimal hardware configurations
for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,
memory, or branch predictor settings can degrade performance by up to 37.1%.
These findings underscore the need for architecture-aware co-design to
efficiently support biomechanical simulation workloads.

</details>


### [17] [SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs](https://arxiv.org/abs/2510.15910)
*Marvin Fuchs,Lukas Scheller,Timo Muscheid,Oliver Sander,Luis E. Ardila-Perez*

Main category: cs.AR

TL;DR: SoCks是一个灵活可扩展的构建框架，通过将SoC镜像划分为高级单元（块）来降低复杂性，实现独立构建和最小化依赖，使构建速度提升至传统工具的3倍。


<details>
  <summary>Details</summary>
Motivation: 现代异构SoC设备集成复杂组件，但开发工具复杂且支持不足，导致学习曲线陡峭和故障排除困难。

Method: 将SoC镜像分区为块，每个固件和软件块以封装方式独立构建，通过标准化接口进行必要的信息交换，最小化依赖关系。

Result: SoCks能够比现有工具快3倍构建完整的SoC镜像，简化现有块实现的复用，并支持版本间的无缝替换。

Conclusion: SoCks框架有效降低了SoC开发的复杂性，支持去中心化和部分自动化的开发流程，通过CI/CD实现高效构建。

Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced
components into a single package, offering powerful capabilities while also
introducing significant complexity. To manage these sophisticated devices,
firmware and software developers need powerful development tools. However, as
these tools become increasingly complex, they often lack adequate support,
resulting in a steep learning curve and challenging troubleshooting. To address
this, this work introduces System-on-Chip blocks (SoCks), a flexible and
expandable build framework that reduces complexity by partitioning the SoC
image into high-level units called blocks. SoCks builds each firmware and
software block in an encapsulated way, independently from other components of
the image, thereby reducing dependencies to a minimum. While some information
exchange between the blocks is unavoidable to ensure seamless runtime
integration, this interaction is standardized via interfaces. A small number of
dependencies and well-defined interfaces simplify the reuse of existing block
implementations and facilitate seamless substitution between versions-for
instance, when choosing root file systems for the embedded Linux operating
system. Additionally, this approach facilitates the establishment of a
decentralized and partially automated development flow through Continuous
Integration and Continuous Delivery (CI/CD). Measurement results demonstrate
that SoCks can build a complete SoC image up to three times faster than
established tools.

</details>


### [18] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe是一种基于查表的1.58位三元LLM加速器，专为低功耗边缘FPGA设计，支持预填充和自回归解码，在5W功耗下实现高达25 tokens/s的解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和嵌入式系统的普及，在边缘平台上部署大型语言模型的需求日益迫切，但受限于计算资源、内存需求和预填充阶段的长延迟。

Method: 采用表查找三元矩阵乘法引擎、细粒度URAM权重缓冲管理、流式数据流架构、反向重排序预填充注意力以及专用解码阶段注意力等技术。

Result: 在5W功耗预算下，TeLLMe实现高达25 tokens/s的解码吞吐量，64-128 token提示的首次token时间在0.45-0.96秒之间。

Conclusion: TeLLMe在边缘FPGA上的LLM推理能效方面取得了显著进步，为低功耗边缘设备部署LLM提供了可行方案。

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


### [19] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: 论文提出Kelle系统，一种软硬件协同设计解决方案，用于在基于eDRAM的边缘设备上高效运行大型语言模型，通过优化KV缓存管理实现3.9倍加速和4.5倍能耗节省。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上运行LLM可降低延迟、提高实时处理能力并增强隐私保护，但KV缓存的内存占用和数据访问成本限制了其部署。边缘设备内存和计算能力有限，难以存储和高效访问LLM推理所需的大缓存。

Method: 提出使用eDRAM作为边缘设备LLM服务的主要存储，结合细粒度内存驱逐、重计算和刷新控制算法，开发Kelle加速器进行软硬件协同优化。

Result: Kelle加速器相比现有基线解决方案实现了3.9倍加速和4.5倍能耗节省。

Conclusion: Kelle系统通过软硬件协同设计有效解决了边缘设备上LLM部署的KV缓存管理问题，显著提升了性能和能效。

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [20] [Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project](https://arxiv.org/abs/2510.16487)
*Giovanni Agosta,Stefano Cherubin,Derek Christ,Francesco Conti,Asbjørn Djupdal,Matthias Jung,Georgios Keramidas,Roberto Passerone,Paolo Rech,Elisa Ricci,Philippe Velha,Flavio Vella,Kasim Sinan Yildirim,Nils Wilbert*

Main category: cs.AR

TL;DR: ARCHYTAS项目旨在设计和评估非常规硬件加速器，包括光电、易失性和非易失性内存计算以及神经形态计算，以解决AI的功耗、效率和可扩展性瓶颈，特别关注国防应用场景。


<details>
  <summary>Details</summary>
Motivation: 解决AI在功耗、效率和可扩展性方面的瓶颈问题，特别是在国防应用场景如自动驾驶车辆、监控无人机、海上和太空平台中。

Method: 开发系统架构和软件堆栈来集成和支持这些加速器，并开发用于全系统及其组件早期原型设计的仿真软件。

Result: 提出了ARCHYTAS项目的整体架构方案，包括硬件加速器集成框架和仿真工具链。

Conclusion: 通过开发专门的系统架构和仿真软件，ARCHYTAS项目将为非常规硬件加速器在国防AI应用中的部署提供完整的技术支持。

Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,
in particular, optoelectronic, volatile and non-volatile processing-in-memory,
and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks
of AI with an emphasis on defense use cases (e.g., autonomous vehicles,
surveillance drones, maritime and space platforms). In this paper, we present
the system architecture and software stack that ARCHYTAS will develop to
integrate and support those accelerators, as well as the simulation software
needed for early prototyping of the full system and its components.

</details>


### [21] [Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization](https://arxiv.org/abs/2510.16622)
*Kazi Ababil Azam,Hasan Masum,Masfiqur Rahaman,A. B. M. Alim Al Islam*

Main category: cs.AR

TL;DR: 本研究为发展中国家如孟加拉国达卡市开发了一种智能交通信号系统，利用RTSP视频流、树莓派4B处理和YOLO目标检测模型，结合NSGA-II多目标优化算法来优化信号配时，减少等待时间并提高车辆通行量。


<details>
  <summary>Details</summary>
Motivation: 发展中国家城市如达卡市的车辆密度导致严重交通拥堵，但现有的智能交通信号技术主要针对发达国家的结构化交通，不适用于达卡市的非车道化、异质交通环境。

Method: 提出一个处理流程：使用RTSP视频流输入，在树莓派4B上进行低资源处理，采用基于YOLO的目标检测模型（在NHT-1071数据集上训练）来检测和分类异质交通，然后使用NSGA-II多目标优化算法生成优化的信号配时。

Result: 在达卡市Palashi的一个五路交叉口进行了测试，证明该系统能够显著改善类似情况下的交通管理。

Conclusion: 开发的测试平台为具有复杂交通动态的发展中地区（如达卡市）提供了更符合情境和有效的智能交通信号解决方案的途径。

Abstract: The vehicular density in urbanizing cities of developing countries such as
Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road
experiences. Traffic signaling is a key component in effective traffic
management for such situations, but the advancements in intelligent traffic
signaling have been exclusive to developed countries with structured traffic.
The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual
approach. This study focuses on the development of an intelligent traffic
signaling system feasible in the context of developing countries such as
Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol
(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of
the art YOLO-based object detection model trained on the Non-lane-based and
Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous
traffic. A multi-objective optimization algorithm, NSGA-II, then generates
optimized signal timings, minimizing waiting time while maximizing vehicle
throughput. We test our implementation in a five-road intersection at Palashi,
Dhaka, demonstrating the potential to significantly improve traffic management
in similar situations. The developed testbed paves the way for more contextual
and effective Intelligent Traffic Signaling (ITS) solutions for developing
areas with complicated traffic dynamics such as Dhaka City.

</details>


### [22] [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](https://arxiv.org/abs/2510.17251)
*Chengxi Li,Yang Sun,Lei Chen,Yiwen Wang,Mingxuan Yuan,Evangeline F. Y. Young*

Main category: cs.AR

TL;DR: smaRTLy是一种针对RTL逻辑综合中多路复用器的优化技术，通过消除冗余多路复用器树和重构剩余结构，显著减少门数量。


<details>
  <summary>Details</summary>
Motivation: 传统工具如Yosys通过遍历多路复用器树和监控控制端口值来优化，但未能充分利用信号间的内在逻辑关系或结构优化潜力。

Method: 开发创新策略来移除冗余多路复用器树并重构剩余结构，采用逻辑推理和结构重建技术。

Result: 在IWLS-2005和RISC-V基准测试中，相比Yosys额外减少8.95%的AIG面积；在百万门级工业基准测试中，比Yosys多移除47.2%的AIG面积。

Conclusion: smaRTLy的逻辑推理和结构重建技术能有效增强RTL优化过程，实现更高效的硬件设计。

Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索实现多模态安全对齐的框架，旨在解决大型视觉语言模型在安全对齐方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态感知和生成方面取得了显著进展，但其安全对齐仍然是一个关键挑战。现有防御容易受到多模态越狱攻击，因为视觉输入引入了新的攻击面，推理链缺乏安全监督，对齐在模态融合下常常退化。

Method: VisuoAlign通过视觉-文本交互提示将安全约束嵌入推理过程，采用蒙特卡洛树搜索系统构建多样化的安全关键提示轨迹，并引入基于提示的缩放以确保实时风险检测和合规响应。

Result: 广泛的实验表明，VisuoAlign能够主动暴露风险，实现全面的数据集生成，并显著提高LVLMs对复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的挑战，通过系统化的方法提升了大型视觉语言模型的安全性和鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [24] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环（SCL）作为可执行的认知框架，将哲学洞见转化为可计算结构，强调智能不是属性而是执行过程，通过功能分离的认知架构产生更连贯和可解释的行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏真正的认知理解，暴露出认知架构的缺失。本文旨在填补这一空白，从认识论角度探讨认知出现的条件，而非传统的本体论问题。

Method: 基于心灵哲学和认知现象学，结合过程哲学、生成认知和扩展心智理论，定义智能为包含判断、记忆、控制、行动和调节的连续循环过程。

Result: SCL将哲学洞见操作化为可计算结构，支持"可执行认识论"；功能分离的认知架构比单一提示系统产生更连贯和可解释的行为；重新定义智能为通过意向性理解重建自身认知状态的能力。

Conclusion: SCL框架对心灵哲学、认识论和AI产生重要影响：允许认知理论被实施和测试；将行为建立在认知结构而非统计规律上；将知识视为在现象学连贯循环中的持续重建过程。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [25] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究表明大型推理模型在解决超过特定困惑度阈值的谜题时会出现性能崩溃，即使提供环境接口让模型能够跟踪状态空间，也无法延迟或消除这种崩溃。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型在解决复杂谜题时性能崩溃的原因，特别是验证是否由于模型需要自行跟踪状态空间而导致评估混淆。

Method: 为大型语言模型提供汉诺塔问题的环境接口，允许其通过工具调用进行移动、提供书面理由、观察结果状态空间并重新提示下一步移动。

Result: 环境接口访问无法延迟或消除性能崩溃，模型参数化策略分析显示与最优策略和均匀随机策略的偏离度增加，表明模型在每个复杂度级别都表现出模式崩溃。

Conclusion: 大型推理模型在复杂推理任务中确实存在性能崩溃现象，且这种崩溃与是否提供状态跟踪能力无关，模型性能取决于其模式是否反映问题的正确解决方案。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [26] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新的证明自动形式化流水线，通过构建逻辑依赖图和使用基于引理的方法来保持原始证明的结构保真度，在184个本科水平问题的新基准测试中取得了0.545的ProofScore，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的证明自动形式化方法虽然能生成可执行代码，但经常无法保持原始人工编写论证的语义含义和逻辑结构，因此需要一种能更好保持结构保真度的新方法。

Method: ProofFlow首先构建有向无环图来映射证明步骤间的逻辑依赖关系，然后采用基于引理的方法将每个步骤系统地形式化为中间引理，从而保持原始论证的逻辑结构。

Result: 实验结果显示ProofFlow在自动形式化方面达到了新的最先进水平，ProofScore为0.545，显著优于全证明形式化（0.123）和步骤证明形式化（0.072）等基线方法。

Conclusion: ProofFlow通过关注结构保真度，在证明自动形式化方面取得了显著改进，其流水线、基准测试和评分指标已开源以促进进一步研究。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [27] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 本文介绍了MO|RE数据知识图谱的构建愿景，旨在通过基于基本形式本体的方法标准化和机器可理解地建模与共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 为了评估和比较不同人群的生理和认知能力，需要测试与人类表现相关的各种因素。运动表现测试作为体育科学研究的核心部分，能够分析不同人口群体的身体健康状况并使其具有可比性。

Method: 开发基于基本形式本体的知识图谱，重点形式化表示计划规范、特定过程和相关测量之间的相互关系。

Result: 提出了MO|RE数据知识图谱的构建愿景和实现方法，为运动表现数据的标准化建模和跨研究共享提供了框架。

Conclusion: 该方法将改变运动表现数据的建模和共享方式，使其标准化且机器可理解，促进体育科学研究的数字化转型。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [28] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机置换集(RPS)的冲突度量方法，从随机有限集(RFS)和Dempster-Shafer理论(DST)两个角度分析RPS中的冲突，利用秩偏重叠(RBO)思想定义置换间的不一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随机置换集作为处理包含顺序信息的不确定性的新形式化方法，需要有效度量两个由置换质量函数表示的证据之间的冲突，这是顺序结构不确定信息融合的迫切研究课题。

Method: 从置换观察出发，基于秩偏重叠(RBO)度量定义置换间的不一致性度量，提出非重叠基础的RPS冲突度量方法，将RPS理论视为DST的扩展，利用新增的顺序信息表征定性倾向性。

Result: 通过数值示例验证了所提冲突度量的行为和性质，该方法具有自然的顶部加权特性，能从DST视角有效度量RPS间的冲突。

Conclusion: 所提方法不仅具有自然顶部加权特性，能有效度量RPS间的冲突，还为决策者提供了权重、参数和截断深度的灵活选择。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [29] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型生成临床思维链的可靠性，发现选择性少样本提示策略显著优于零样本和随机少样本策略，提出了基于"黄金标准深度"和"代表性多样性"的双原则框架来生成可信的临床数据。


<details>
  <summary>Details</summary>
Motivation: 高质量的临床思维链对于可解释医疗AI至关重要，但面临数据稀缺的约束。虽然大型语言模型可以合成医疗数据，但其临床可靠性尚未得到验证。

Method: 在辅助生殖技术领域进行盲法比较研究，资深临床医生评估三种不同提示策略生成的思维链：零样本、随机少样本（使用浅层示例）和选择性少样本（使用多样化高质量示例），并与GPT-4o的评估结果进行比较。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略（p < .001）。随机少样本策略相比零样本基线没有显著改进，表明低质量示例与无示例同样无效。AI评估器未能识别这些关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于战略性的提示策划，而非仅仅示例的存在。提出的"双原则"框架为大规模生成可信数据提供了基础方法，确认了人类专业知识在评估高风险临床AI中不可或缺的作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [30] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个基于大语言模型的多智能体评估框架，用于自动评估PHI去标识化模型性能并选择最佳模型，无需依赖昂贵的专家标注。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化对于安全重用临床记录至关重要，但传统评估依赖成本高昂的小规模专家标注，限制了模型比较和选择。

Method: 部署多个评估智能体独立判断PHI提取正确性，通过基于LLM的多数投票机制整合结果，生成稳定可复现的模型排名。

Result: 在真实临床记录语料上的实验表明，TEAM-PHI能产生一致准确的排名，尽管个体评估者存在差异，但LLM投票能可靠收敛到相同的最佳系统。

Conclusion: TEAM-PHI通过结合独立评估智能体和LLM多数投票，为PHI去标识化提供了实用、安全且成本效益高的自动评估和最佳模型选择解决方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [31] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 论文提出了"被记住权"概念，旨在解决大型语言模型可能通过合成单一权威性回答而压制某些观点、放大已有偏见，从而重塑集体记忆的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在信息检索中的广泛应用，其提供的合成回答可能将多种观点压缩为单一答案，减少了用户比较替代方案的能力和意愿，导致某些叙事、个体或群体被不成比例地压制或放大，威胁集体记忆的多样性。

Method: 提出"被记住权"概念框架，包括最小化AI驱动信息遗漏风险、保障公平对待权利，同时确保生成内容最大程度真实。

Result: 建立了应对AI信息偏见和遗漏的理论框架，为保护数字时代集体记忆多样性提供了概念基础。

Conclusion: 需要建立"被记住权"来应对大型语言模型可能造成的信息偏见和集体记忆重塑问题，确保信息检索系统的公平性和真实性。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [32] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，这是一个基于检索增强的研究想法评估系统，用于评估AI生成的研究想法的有效性和贡献度。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的普及，需要强大的评估框架来确保生成想法的有效性和实用性。

Method: 开发了ScholarEval评估框架，基于两个核心标准：soundness（基于现有文献的方法实证有效性）和contribution（相对于先前研究在不同维度上的进步程度）。创建了ScholarIdeas数据集，包含117个跨学科研究想法和专家评审。

Result: ScholarEval在覆盖人类专家标注要点方面显著优于所有基线方法，并且在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。大规模用户研究表明，在文献参与度、想法精炼和实用性方面显著优于深度研究。

Conclusion: ScholarEval为研究想法评估提供了有效的框架，公开发布了代码、数据集和工具供社区使用和构建。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [33] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文识别并系统分析了大推理模型中的关键漏洞——推理分心，即模型被恶意嵌入提示中的无关复杂任务分散注意力，导致任务准确性显著下降。研究发现最先进的LRM模型高度易受攻击，某些对齐技术会放大这一弱点，模型可能表现出隐蔽服从行为。作者提出了基于训练的防御方法，显著提高了模型对分心攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大推理模型在数学和编程等复杂任务上表现出色，研究者发现这些模型存在一个关键安全漏洞——推理分心攻击。恶意攻击者可以在提示中嵌入无关但复杂的任务来分散模型注意力，从而破坏其推理性能。这一漏洞对LRM的可靠性构成了严重威胁，需要系统研究和防御方法。

Method: 通过跨多个模型和基准的全面研究，分析了LRM对推理分心攻击的敏感性。进一步研究发现某些对齐技术会放大这一弱点，模型可能表现出隐蔽服从行为。为缓解风险，提出了结合监督微调(SFT)和强化学习(RL)的训练防御方法，使用合成的对抗数据进行训练。

Result: 研究表明，即使是最先进的LRM模型也高度易受推理分心攻击，注入的分心器可使任务准确性降低高达60%。某些对齐技术会放大这一弱点，模型可能隐蔽地遵循隐藏的对抗指令。提出的训练防御方法在具有挑战性的分心攻击上，将鲁棒性提高了50多个百分点。

Conclusion: 推理分心是LRM可靠性面临的独特且紧迫的威胁。研究结果揭示了这一安全漏洞的严重性，并为构建更安全、更可信的推理系统提供了实用的防御步骤。通过提出的训练方法可以显著提高模型对分心攻击的鲁棒性。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [34] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出DTKG框架，通过双轨知识图谱验证和推理来解决多跳问答中的并行事实验证和链式推理问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在并行事实验证和链式多跳推理任务上各有优劣，无法同时高效处理两种推理模式，限制了多跳问答的效率和准确性

Method: 基于认知科学双过程理论，设计包含分类阶段和分支处理阶段的双轨框架，结合LLM响应验证和KG路径构建的优势

Result: DTKG框架能够根据问题类型自适应选择最优推理策略，提升多跳问答的性能

Conclusion: 提出的双轨方法有效解决了多跳推理中并行验证和链式推理的平衡问题，为检索增强生成系统提供了更高效的解决方案

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [35] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑型知识图谱与符号验证器结合的系统，用于在推理任务中强制执行数学可解释规则，显著提升大型语言模型的推理准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理过程中经常产生流畅但违反简单数学或逻辑约束的步骤，需要一种方法来确保推理的数学可解释性和一致性。

Method: 引入MedRule-KG，这是一个紧凑的类型化知识图谱，结合符号验证器，编码实体、关系和三个领域启发规则，验证预测并应用最小修正以保证一致性。

Result: 在90个FDA衍生基准测试中，基于MedRule-KG的推理将精确匹配从0.767提升到0.900，添加验证器后达到1.000精确匹配，完全消除了规则违反。

Conclusion: MedRule-KG为安全的数学推理提供了一个通用框架，通过知识图谱和符号验证的结合有效解决了LLM的推理一致性问题。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [36] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: SELECT是一个动态锚点选择框架，通过两阶段评估机制自动发现最优锚点进行精确概念擦除，同时识别关键边界锚点以保留相关概念，解决了固定锚点策略导致的概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的概念擦除方法通常依赖固定锚点策略，这会导致概念重现和侵蚀等关键问题。通过因果追踪发现擦除对锚点选择具有内在敏感性，需要更优的锚点选择方法。

Method: 提出SELECT框架，引入新颖的两阶段评估机制：自动发现最优锚点进行精确擦除，同时识别关键边界锚点以保留相关概念。该框架基于Sibling Exclusive Concepts作为更优的锚点类别。

Result: 广泛评估表明SELECT作为通用锚点解决方案，不仅高效适应多个擦除框架，还在关键性能指标上持续优于现有基线方法，单个概念锚点挖掘平均仅需4秒。

Conclusion: SELECT框架通过动态锚点选择有效解决了固定锚点策略的局限性，为文本到图像扩散模型的概念擦除提供了更精确和高效的解决方案。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [37] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该研究探讨了用户在算法推荐系统中的策略性行为，建立了用户与算法之间的Stackelberg博弈模型，揭示了用户需要足够远见才能有效引导算法与其真实兴趣对齐，并发现小的成本信号可以显著降低对齐负担。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解在用户偏好不一致的情况下，用户如何通过选择性互动来引导算法更好地符合其真实兴趣。用户经常表现出不一致的偏好，可能会花费大量时间在低价值内容上，无意中向算法发送错误信号。

Method: 将用户的决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的分裂过程。采用多领导者、单跟随者的扩展Stackelberg博弈框架，用户（系统2）通过承诺参与策略来领导，算法基于观察到的互动做出最佳响应。

Result: 研究发现存在一个关键的对齐视野：足够有远见的用户可以实现算法对齐，而视野不足的用户反而会被算法目标所对齐。这个关键视野可能很长，造成显著负担。但即使是一个小的成本信号（如额外点击）也能显著减少对齐负担。

Conclusion: 该框架解释了具有不一致偏好的用户如何在Stackelberg均衡中使参与驱动的算法与其兴趣对齐，既突出了实现对齐的挑战，也指出了潜在的补救措施。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [38] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 本文提出了受人类智能启发的类人结构因果模型(HSCM)，这是一种新颖的因果框架，旨在克服传统领域泛化模型的局限性。HSCM通过解耦和重新加权颜色、纹理和形状等关键图像属性，在多样化领域中增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化模型依赖统计数据捕捉数据-标签依赖关系和学习失真不变表示，存在局限性。受人类视觉系统的分层处理和多层次学习机制启发，作者希望开发一个更有效的因果框架来提升模型在动态复杂环境中的泛化能力。

Method: HSCM复制人类视觉系统的分层处理和多层次学习机制，专注于建模细粒度因果机制。通过解耦和重新加权关键图像属性（颜色、纹理、形状），该方法增强了跨领域泛化能力。

Result: 通过理论和实证评估，HSCM在性能上超越了现有的领域泛化模型，提供了更原则性的方法来捕捉因果关系并提高模型鲁棒性。

Conclusion: HSCM框架通过模仿人类智能的灵活性和适应性，在动态复杂环境中实现了更有效的迁移和学习，为领域泛化问题提供了一个有前景的解决方案。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [39] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: ReviewSense是一个新颖的决策支持框架，利用大型语言模型将客户评论转化为可操作的企业建议，超越了传统的偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 随着客户反馈在战略增长中日益重要，需要从非结构化评论中提取可操作见解。传统AI系统擅长预测用户偏好，但缺乏将评论转化为面向企业的规范性建议的能力。

Method: 整合聚类、LLM适配和专家驱动评估的统一业务导向流程，通过识别关键趋势、重复问题和具体关注点来转换客户情感。

Result: 初步人工评估显示模型建议与业务目标高度一致，突显其在数据驱动决策方面的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，展示了其在优化业务策略和最大化客户反馈影响方面的价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [40] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 提出了NP-ENGINE框架，这是第一个用于在NP难问题上训练和评估LLM的综合框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。训练出的模型在NP-BENCH基准上超越GPT-4o，并展现出强大的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学、编程等任务上表现出色，但在解决更复杂的NP难优化问题方面的能力尚未充分探索。需要建立一个专门的框架来训练和评估LLM在这类问题上的表现。

Method: 提出NP-ENGINE框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。使用零RLVR和课程学习训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著超越GPT-4o，达到同模型尺寸下的SOTA性能。RLVR训练还使模型在逻辑、谜题、数学等推理任务以及指令跟随等非推理任务上展现出强大的跨领域泛化能力。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的扩展规律。增加任务多样性可以改善跨领域泛化能力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [41] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow是一个基于知识图谱的检索增强生成框架，通过转移流匹配目标联合优化检索策略和流估计器，从文本丰富的知识图谱中高效检索准确多样的知识。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法难以从文本丰富的知识图谱中为复杂真实世界查询检索准确和多样化的信息，而过程奖励模型虽然能对齐检索过程与查询特定知识需求，但严重依赖昂贵且难以获取的过程级监督信号。

Method: GraphFlow采用转移流匹配目标联合优化检索策略和流估计器，流估计器将检索结果的奖励分解为中间检索状态，引导检索策略按奖励比例从知识图谱中检索候选，从而探索产生多样相关结果的高质量图谱区域。

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均优于包括GPT-4o在内的强基线方法10%，并在未见过的知识图谱上表现出强大的泛化能力。

Conclusion: GraphFlow在从文本丰富的知识图谱中检索准确多样知识方面表现出有效性和鲁棒性，能够更好地支持复杂真实世界查询。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [42] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 本文提出了一种半监督置信度分布学习方法（ssCDL）来解决不确定知识图谱补全中的置信度分布不平衡问题，通过将置信度转换为分布并利用元学习生成伪标签来增强训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽视了置信度的极端不平衡分布，导致学习到的嵌入表示不足以支持高质量的图谱补全。

Method: 提出ssCDL方法，将置信度转换为置信度分布以引入更多监督信息，通过关系学习在标记数据和带伪标签的未标记数据上迭代学习嵌入，使用元学习预测未见三元组的置信度来增强训练数据并重新平衡置信度分布。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上持续优于最先进的基线方法。

Conclusion: ssCDL通过处理置信度分布不平衡问题，有效提升了不确定知识图谱补全的性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [43] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种新颖的强化学习算法，通过基于计数的内在奖励来增强LLM的推理探索能力，避免重复和次优的推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习范式依赖稀疏的结果奖励和有限的探索，导致LLM趋向重复和次优的推理模式，需要设计更好的探索机制。

Method: 使用轻量级的Coin Flipping Network估计伪计数和认知不确定性，将其转换为内在奖励，并与GRPO等先进RL框架集成。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著提升性能，帮助策略逃离局部最优找到更好解决方案。

Conclusion: 针对性的内在动机可以使语言模型推理的探索更加可靠。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [44] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 大型AI模型正在变革神经科学研究，通过端到端学习从原始脑信号中提取信息，在神经影像、脑机接口、分子神经科学、临床辅助和疾病应用等五大领域产生深远影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大规模AI模型如何解决神经科学中的计算挑战，包括多模态神经数据整合、时空模式解释以及临床转化框架的开发。

Method: 回顾分析大型AI模型在五个主要神经科学领域的应用：神经影像数据处理、脑机接口与神经解码、分子神经科学与基因组建模、临床辅助与转化框架、神经精神疾病应用。

Result: 这些模型被证明能够有效处理多模态神经数据整合、时空模式解释，并为临床部署提供转化框架。神经科学与AI的互动日益互惠，生物启发的架构约束被用于开发更可解释和计算高效的模型。

Conclusion: 该综述强调了这些技术的巨大潜力，同时指出关键实施考虑，包括严格的评估框架、有效的领域知识整合以及临床使用的全面伦理指南，并提供了用于验证大型AI模型的关键神经科学数据集清单。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [45] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 本文提出ELMM方法，通过多视图视觉令牌压缩器和注意力剪枝策略，解决多模态知识图谱补全中的语义噪声和计算成本问题，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有MKGs存在不完整性，影响下游任务效果。虽然LLMs在KGC中表现良好，但在多模态环境下应用面临图像令牌过多导致语义噪声、模态冲突以及高计算成本的挑战。

Method: 提出ELMM框架，包含基于多头注意力的多视图视觉令牌压缩器(MVTC)，从文本和视觉视图自适应压缩图像令牌；设计注意力剪枝策略移除冗余层，并使用线性投影补偿性能损失。

Result: 在FB15k-237-IMG和WN18-IMG基准测试中，ELMM达到最先进性能，同时显著提升计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新范式，有效平衡了性能与效率。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [46] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出领域情境化概念图（CDC）框架，将领域作为概念表示的一等元素，采用<概念, 关系@领域, 概念'>的三元组结构，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定本体论的刚性层次结构，根本原因在于将领域视为隐含上下文而非显式推理组件。

Method: 基于认知-语言同构映射原理，采用C-D-C三元组结构，定义20多种标准化关系谓词（结构、逻辑、跨领域、时间），并在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和技术文档等案例研究中，CDC实现了上下文感知推理、跨领域类比和个性化知识建模。

Conclusion: CDC框架能够实现传统基于本体的框架无法达到的能力，为知识建模提供了更灵活和动态的方法。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [47] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估方法，用于测试用户能否从强化学习算法的决策解释中识别出智能体的目标。在Ms. Pacman环境中测试四种可解释强化学习算法，发现只有一种算法在测试目标上的准确率超过随机水平，且用户普遍高估自己的选择准确性。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用之一是调试，但目前缺乏对这些算法相对性能的比较评估。

Method: 使用Atari的Ms. Pacman环境和四种可解释强化学习算法，通过新颖的评估方法测试用户从决策解释中识别智能体目标的能力。

Result: 只有一种算法的准确率超过随机水平；用户普遍表现出过度自信；用户自报的识别和理解难易程度与实际准确率不相关。

Conclusion: 当前可解释强化学习算法在帮助用户识别智能体目标方面的效果有限，用户的主观感知与实际性能存在差异。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [48] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的多智能体框架，用于GPU内核优化，通过系统化探索设计空间、多智能体协作和迭代优化，显著提升了内核性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核效率对现代AI发展至关重要，但优化过程复杂且劳动密集。现有LLM方法主要作为单次生成器或简单优化工具，难以有效应对不规则的核优化场景。

Method: 采用LLM智能体框架，包含多智能体协作、基于经验的指导、动态上下文管理和策略搜索，模拟专家工程师的工作流程，让LLM能够推理硬件权衡、整合性能分析反馈并迭代优化内核。

Result: 在KernelBench基准测试中，相比基线智能体，该系统能生成正确解决方案（基线经常失败），并实现高达16倍运行时的内核性能提升。

Conclusion: 结果表明智能体LLM框架在推进全自动、可扩展的GPU内核优化方面具有巨大潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [49] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进大型语言模型在多轮工具增强对话中的行为，通过检测8种特定工具调用错误并提供针对性反馈，可将工具调用准确率提升高达13%。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中越来越普遍，但工具使用错误仍然阻碍其可靠性，需要一种系统方法来检测和改进这些错误。

Method: 开发ToolCritic框架，定义8种工具调用错误类型，构建合成数据集训练ToolCritic，让主LLM基于ToolCritic的反馈修订响应。

Result: 在Schema-Guided Dialogue数据集上的实验结果显示，ToolCritic相比零样本提示和自我纠正等基线方法，将工具调用准确率提升了高达13%。

Conclusion: ToolCritic代表了在现实世界对话应用中实现更稳健的LLM与外部工具集成的有希望的一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [50] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出了一种基于手工特征的方法，通过提取和融合颜色统计、多颜色空间直方图以及纹理特征来评估鱼类新鲜度，在FFE数据集上取得了显著优于深度学习的性能。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼类新鲜度存在主观性、不一致性和难以标准化的问题，需要一种客观、可靠的自动化评估方法。

Method: 从鱼眼图像中系统提取颜色统计、多颜色空间直方图、LBP和GLCM等纹理特征，融合全局色度变化和局部ROI退化特征，使用LightGBM和ANN进行分类。

Result: 在标准训练测试设置下，LightGBM达到77.56%准确率，比之前深度学习基线提升14.35%；使用增强数据时，ANN达到97.16%准确率，比之前最佳结果提升19.86%。

Conclusion: 精心设计的手工特征经过策略性处理后，能够为自动化鱼类新鲜度评估提供稳健、可解释且可靠的解决方案。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [51] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: PILLM是一个基于物理知识的LLM框架，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，结合热力学和控制理论约束，实现可解释且物理合理的异常检测。


<details>
  <summary>Details</summary>
Motivation: HVAC系统占建筑能耗很大比例，传统规则方法可解释但缺乏适应性，深度学习方法预测能力强但缺乏透明度和物理合理性，现有LLM方法忽略了HVAC运行的物理原理。

Method: 提出PILLM框架，采用进化循环机制，引入物理知识反射和交叉操作，嵌入热力学和控制理论约束，自动生成和优化异常检测规则。

Result: 在公开的建筑故障检测数据集上，PILLM实现了最先进的性能，同时生成可解释和可操作的诊断规则。

Conclusion: PILLM推进了智能建筑系统中可信赖和可部署AI的发展，平衡了预测性能与物理合理性和可解释性。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [52] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench是一个系统评估多智能体系统通信协议的基准测试，从任务成功率、端到端延迟、消息开销和故障恢复能力四个维度比较不同协议。研究发现协议选择显著影响系统性能，并提出了ProtocolRouter学习型协议路由器来优化协议选择。


<details>
  <summary>Details</summary>
Motivation: 随着大规模多智能体系统的发展，通信协议层成为影响性能和可靠性的关键因素，但目前协议选择缺乏标准化指导，主要依赖直觉。

Method: 开发了ProtocolBench基准测试系统，从四个可测量维度比较不同协议；提出了ProtocolRouter学习型协议路由器，根据需求和运行时信号为不同场景或模块选择最优协议。

Result: 协议选择显著影响系统行为：在流式队列场景中，总完成时间差异达36.5%，平均端到端延迟差异3.48秒；在故障风暴恢复场景中，不同协议的恢复能力存在一致差异。ProtocolRouter相比最佳单协议基线，将故障风暴恢复时间减少18.1%，并在GAIA场景中实现更高成功率。

Conclusion: 通信协议选择对多智能体系统性能有显著影响，ProtocolBench提供了标准化评估方法，ProtocolRouter通过智能协议选择可显著提升系统可靠性和性能。同时发布了ProtocolRouterBench以标准化协议评估并提升大规模系统的可靠性。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [53] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 本研究开发了一种结合ECG基础模型和可解释XGBoost分类器的混合预测框架，用于急性心肌梗死后恶性室性心律失常的早期识别，在提高准确性的同时保持临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECGFounder基础模型提取150维诊断概率特征，通过特征选择后训练XGBoost分类器，并采用SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)，SHAP分析显示模型识别的关键特征与临床知识高度一致。

Conclusion: 该混合框架通过验证基础模型输出作为有效的自动化特征工程，为构建可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [54] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究评估了一个基于工具的LLM健康教练系统，通过离线策略评估发现统一的重工具策略会损害特定用户群体（如低健康素养/高自我效能用户），并提出了通过添加早期信息增益奖励来改善个性化效果的方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何通过个性化策略来优化基于工具的LLM健康教练系统，避免统一策略对特定用户群体造成伤害，同时提高整体服务效果。

Method: 采用离线策略评估（OPE）方法分析因子化决策头（工具/风格），并通过轻量级模拟器测试添加早期信息增益奖励的效果，以识别用户特征并改进目标成功率。

Result: 研究发现统一的重工具策略虽然提高了平均价值，但损害了低健康素养/高自我效能用户群体；添加早期信息增益奖励能可靠地缩短特征识别时间并提高目标成功率和pass@3指标。

Conclusion: 提出了以评估为先的个性化路径：冻结生成器，基于类型化奖励（客观工具结果和满意度）学习子群体感知的决策头，并始终报告每个原型指标以揭示被平均值掩盖的子群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [55] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图表示疾病进展轨迹，使用神经ODE框架学习连续时间进展动态，在2型糖尿病和心血管疾病进展建模中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 疾病进展建模面临挑战：需要基于不规则时间事件样本学习连续时间动态，且患者存在异质性（不同进展速率和路径）。现有方法要么缺乏从真实数据学习的适应性，要么无法捕捉复杂的连续时间动态。

Method: TD-HNODE模型将疾病进展表示为时间详细超图，通过可学习的TD-Hypergraph Laplacian捕捉疾病并发症标志物在进展轨迹内部和之间的相互依赖关系，使用神经ODE框架学习连续时间进展动态。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法。

Conclusion: TD-HNODE能够有效捕捉疾病进展的连续时间动态，解决患者异质性和不规则时间采样带来的挑战，为患者亚表型分析和及时干预提供支持。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [56] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 本文提出了RubiSCoT，一个AI支持的论文评估框架，使用自然语言处理技术（包括大语言模型、检索增强生成和结构化思维链提示）来增强从提案到最终提交的论文评估过程。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法虽然有效，但耗时且受评估者主观性影响，需要一种更一致、可扩展的解决方案。

Method: 使用先进的自然语言处理技术，包括大语言模型、检索增强生成和结构化思维链提示，构建包含初步评估、多维评估、内容提取、基于评分标准的评分和详细报告的框架。

Result: 设计了RubiSCoT框架并实现了其系统，展示了通过一致、可扩展和透明的评估来优化学术评估流程的潜力。

Conclusion: RubiSCoT有潜力通过提供一致、可扩展和透明的评估来优化学术评估流程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [57] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 开发了一种基于主动推理的路径规划方法，用于智能体自主控制，通过构建证据地图和计算变分自由能量来指导智能体在探索和利用之间平衡移动。


<details>
  <summary>Details</summary>
Motivation: 为了维持共同作战态势图而侦察地理区域，需要解决智能体在探索广阔区域和跟踪已识别目标之间的平衡问题。

Method: 使用Dempster-Shafer理论和高斯传感器模型构建生成模型，通过贝叶斯方法更新后验概率分布，计算变分自由能量来指导智能体向最小化自由能量的位置移动。

Result: 该方法能够有效指导智能体在模拟环境中移动，平衡探索和利用的需求。

Conclusion: 主动推理路径规划方法成功解决了智能体在地理区域侦察中的探索-利用权衡问题，通过变分自由能量最小化实现了有效的自主控制。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [58] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律机器学习中标签不确定性问题的研究：法律案件结果受人为干预影响，导致标签不确定，影响模型行为


<details>
  <summary>Details</summary>
Motivation: 法律机器学习通常将过去案件结果视为真实标签，但法律结果往往受到人为干预（如和解、上诉等）的影响，导致标签不确定性，需要对此问题进行研究

Method: 在欧洲人权法院案件分类背景下，研究不同标签构建方式如何影响模型行为，使用现有方法对不确定标签进行估算

Result: 研究表明，训练过程中标签的构建方式会显著影响模型行为，标签不确定性是AI与法律领域的重要关注点

Conclusion: 法律机器学习应用需要考虑标签不确定性问题，虽然存在估算方法但都基于不可验证的假设，标签不确定性应被视为AI与法律领域的重要关注点

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [59] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 该研究提出了一种将大型语言模型的推理能力蒸馏到更小、更高效模型的方法，通过结构感知损失优化训练小模型模拟大模型的推理和问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 代码生成任务不仅需要准确的标记预测，更需要理解解决方案级别的结构关系。大型语言模型具备复杂推理能力，但部署成本高；小模型推理能力不足。因此需要将大模型的推理能力蒸馏到小模型中。

Method: 通过结构感知损失优化方法，训练小模型学习识别正确解决方案路径，建立问题定义与潜在解决方案之间的结构对应关系，超越标记级生成，深入理解解决方案的整体结构。

Result: 实验结果显示，经过廉价且易于实现的过程微调的模型，在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 该方法成功地将大型语言模型的推理能力蒸馏到更小、更高效的模型中，实现了在保持性能的同时降低部署成本的目标。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [60] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型作为预测工具的能力，构建了Prophet Arena基准测试平台，发现LLMs已具备不错的预测能力，但也存在事件召回不准确、数据源误解等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大型语言模型的快速发展，利用LLMs预测现实世界未来事件具有重要潜力，这种新兴范式被称为"LLM-as-a-Prophet"。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务，并将每个任务分解为不同的流水线阶段，以支持受控和大规模实验。

Result: 综合评估显示，许多LLMs已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。

Conclusion: 虽然LLMs已具备良好的预测能力，但实现卓越预测智能仍面临关键瓶颈，包括事件召回不准确、数据源误解以及在接近决策时信息聚合速度较市场慢等问题。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [61] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 本文提出使用多智能体影响图(MAIDs)作为图形框架来解决多智能体强化学习(MARL)中的协调问题，设计了基于MAIDs的定向干预范式，通过因果推断技术PSI实现，可在仅干预单个智能体的情况下实现复合期望结果。


<details>
  <summary>Details</summary>
Motivation: 在大规模MARL中，对整个多智能体系统提供全局指导不切实际，而现有的协调机制设计主要依赖经验研究，缺乏易用的研究工具。

Method: 使用多智能体影响图(MAIDs)作为分析框架，提出定向干预范式，引入预策略干预(PSI)因果推断技术，通过最大化因果效应来实现复合期望结果。

Result: 实验证明了所提出的定向干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的图形框架来分析和设计MARL交互范式，定向干预能够缓解全局指导问题，PSI技术能够有效实现复合期望结果。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [62] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了上下文注意力调制(CAM)机制和混合上下文注意力调制(HyCAM)框架，用于解决LLMs在多任务适应中的平衡问题，在保持通用知识的同时增强任务特定特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多任务适应中难以平衡知识保留和任务特定专业化，传统微调方法存在灾难性遗忘和资源消耗问题，现有参数高效方法在复杂多任务场景下表现不佳。

Method: 提出上下文注意力调制(CAM)机制动态调节自注意力模块表示，并结合HyCAM框架，将共享的全参数CAM模块与多个轻量级专用CAM模块结合，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的广泛实验表明，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: CAM和HyCAM框架有效解决了LLMs在多任务适应中的挑战，在保持效率的同时实现了更好的性能表现。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [63] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 该论文发现视觉语言模型(VLMs)在输出错误答案时仍能感知到正确的视觉证据，这种现象称为"看见但不相信"。作者提出了一种无需训练的推理时干预方法，通过选择性注意力掩码来突出深层证据区域，从而显著提高多个VLM家族的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态任务上表现出色，但它们仍然会在正确的视觉证据存在时失败。本研究旨在系统性地调查这些失败是由于未能感知证据还是未能有效利用证据。

Method: 通过检查逐层注意力动态，发现浅层主要关注文本，而深层稀疏但可靠地关注局部证据区域。基于此，引入了一种推理时干预方法，通过选择性注意力掩码来突出深层证据区域。

Result: 研究发现VLMs在输出错误答案时仍能感知视觉证据，这种现象广泛存在于主要VLM家族中。提出的干预方法无需训练，在LLaVA、Qwen、Gemma和InternVL等多个模型家族中一致提高了准确性。

Conclusion: VLMs在内部编码了可靠的证据但未充分利用，使这些信号显式化可以弥合感知与推理之间的差距，推进对VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [64] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 本文提出了两种基于MPI的并行自助法算法，通过局部统计量聚合和同步伪随机数生成来解决大规模数据集自助法计算中的通信开销和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 自助法作为强大的统计重采样技术，在大数据集或大量重采样时计算成本过高，需要设计高效的并行算法来解决通信开销和内存限制问题。

Method: 提出两种策略：1) 局部统计量聚合，通过传输充分统计量而非完整重采样数据集来大幅减少通信；2) 同步伪随机数生成，在单个进程无法存储整个数据集时实现分布式重采样。

Result: 开发了通信和计算复杂度的分析模型，与朴素基线方法相比，所提方法在通信量和内存使用方面显著减少，支持大规模系统的可扩展并行自助法。

Conclusion: 所提出的方法通过减少通信开销和内存需求，实现了在大规模系统上高效可扩展的并行自助法计算。

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [65] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: FourierCompress是一种基于傅里叶变换的LLM激活压缩框架，利用激活在频域的稀疏性，通过保留低频系数实现高效压缩，在边缘设备LLM推理中实现7.6倍压缩比和小于0.3%的精度损失。


<details>
  <summary>Details</summary>
Motivation: 解决协作式LLM推理中的通信瓶颈问题，传统激活压缩方法难以同时实现高压缩比、低重构误差和计算效率，需要一种新的压缩方法。

Method: 利用LLM激活在频域的稀疏性，通过FFT将激活转换到频域，仅保留低频系数块，在服务器端利用共轭对称性重构信号，支持DSP和FPGA硬件加速。

Result: 在Llama 3和Qwen2.5模型上的实验表明，FourierCompress保持性能接近未压缩基线，优于Top-k、QR和SVD方法，实现平均7.6倍激活大小减少和小于0.3%的精度损失。

Conclusion: FourierCompress在通信效率、近无损推理和压缩速度方面取得了平衡，为边缘设备LLM推理提供了有效的解决方案。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [66] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris是一种面向机器学习领域的专用RDMA传输协议，通过移除重传和有序交付机制，利用ML工作负载对数据丢失的容忍性来显著降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 随着分布式ML工作负载扩展到数千个GPU，集体通信中的尾延迟成为主要瓶颈。传统RDMA设计的严格可靠性和有序交付机制在ML场景下引入了不必要的复杂性和延迟。

Method: Celeris在RDMA NIC中移除重传和有序交付，实现尽力而为传输，同时保留拥塞控制，并通过软件级机制（如自适应超时和数据优先级）管理通信，将丢失恢复转移到ML流水线中。

Result: Celeris将第99百分位延迟降低高达2.3倍，BRAM使用量减少67%，NIC对故障的恢复能力几乎翻倍。

Conclusion: Celeris为集群规模的ML工作负载提供了一个弹性、可扩展的专用传输解决方案，通过利用ML对数据丢失的容忍性来优化性能。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [67] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 提出了一种基于C++ Noarr库的MPI新抽象，实现了布局无关的MPI应用设计，并通过分布式GEMM内核案例展示了其可用性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统MPI的纯C接口缺乏现代语言特性（如类型检查和泛型编程），限制了分布式高性能计算的开发效率。

Method: 作为C++ Noarr库的扩展实现，遵循Noarr范式（一等布局和遍历抽象），提供布局无关的MPI应用设计方法。

Result: 该抽象实现了与最先进MPI C++绑定相当的性能，同时允许更灵活的分布式应用设计。

Conclusion: 提出的MPI抽象在保持性能的同时，显著提升了分布式应用设计的灵活性和开发效率。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [68] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 提出了一种用于互连多核系统的集成容错架构，通过构建稳定性指标识别可靠机器并执行周期性诊断，无需额外硬件即可实现永久故障隔离和自适应任务调度。相比基线TMR减少约30%任务负载，在故障覆盖率和隔离精度方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统两阶段三模冗余TMR在永久故障下失效的问题，以及反应式TMR依赖额外硬件导致系统复杂性和容错能力下降的局限性。

Method: 构建稳定性指标识别可靠机器，执行周期性诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。

Result: 相比基线TMR减少约30%任务负载，在故障覆盖率和隔离精度方面表现优异，显著提高可靠性和能效。

Conclusion: 所提出的集成容错架构在无需额外硬件的情况下，有效解决了永久故障隔离问题，显著提升了多核系统的可靠性和能效。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [69] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: 评估最新推理模型在生成优化CUDA代码方面的能力，研究LLMs能否独立进行代码优化以及通过提示指导是否能提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，编程工具（如GitHub Copilot）在代码生成、调试和性能优化方面带来了革命性变化，本研究旨在探索LLMs在CUDA代码优化方面的具体能力。

Method: 通过自动评估（正确性和加速比）和手动评估（代码审查）来评估生成的解决方案，并尝试交互式方法让LLM在会话中修复之前的错误。

Result: 结果表明LLMs是相当熟练的编码者，但要达到并行计算专家提供的优化解决方案水平，需要提供指导。

Conclusion: 大语言模型在代码生成方面表现出色，但需要详细的提示和指导才能生成高度优化的并行代码解决方案。

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [70] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 基于eBPF的GPU尾延迟监控系统，通过关联主机指标与GPU内部事件，实现共享计算环境中GPU尾延迟尖峰的诊断和根因分析。


<details>
  <summary>Details</summary>
Motivation: 在云和HPC基础设施中诊断GPU尾延迟尖峰对保持性能可预测性和资源利用率至关重要，但现有监控工具在共享计算环境中缺乏根因分析所需的粒度。

Method: 引入基于eBPF的遥测系统，提供GPU工作负载的统一主机端监控，将eBPF衍生的主机指标与GPU内部事件关联，实现整体系统可观测性。

Result: 系统达到81-88%的诊断准确率，在5秒内检测到尖峰，6-8秒内完成根因分析，在100Hz采样下仅产生1.21%的CPU开销。在分布式学习工作负载评估中，识别出NIC争用、PCIe压力和CPU干扰等根因。

Conclusion: 该系统能够为多租户GPU基础设施提供操作调试能力，无需集群范围插装。

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [71] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 该论文提出了一种训练语言模型的方法，使其能够在推理过程中与性能工具交互，从而解决语言模型在代码性能优化任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在软件工程中广泛应用，但在代码性能相关任务（如优化）中表现不佳，因为这些任务依赖于环境、硬件等复杂数据，而这些信息并未直接体现在源代码中。

Method: 提出了一种训练语言模型的方法，使其在推理过程中能够与性能工具进行交互，从而更好地理解代码与环境之间的相互作用。

Result: 该方法被用于训练一个最先进的GPU内核优化模型，证明了其有效性。

Conclusion: 通过让语言模型在推理过程中与性能工具交互，可以显著提升其在代码性能优化任务中的表现，特别是在需要理解环境与代码交互的复杂场景中。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [72] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 该论文研究了分布式图算法中轮次消除固定点的通用性问题，证明了轮次消除不能成为所有带输入问题的通用技术，但可能是无输入问题的通用技术。


<details>
  <summary>Details</summary>
Motivation: 研究轮次消除固定点是否可以作为证明分布式图算法下界的通用技术，解决之前基于Marks技术的同态问题障碍。

Method: 使用三幂输入构建轮次消除下界，证明同态问题确实可以通过轮次消除固定点证明下界，并展示新的障碍问题。

Result: 消除了轮次消除通用性的已知障碍，但发现了新的障碍：某些带输入问题无法通过轮次消除固定点证明下界。

Conclusion: 轮次消除不能成为带输入问题的通用下界证明技术，但可能是无输入问题的通用技术，并证明了首个适用于任意问题的轮次消除固定点通用下界定理。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [73] [Hierarchical Multi-Modal Threat Intelligence Fusion Without Aligned Data: A Practical Framework for Real-World Security Operations](https://arxiv.org/abs/2510.15953)
*Sisir Doppalapudi*

Main category: cs.CR

TL;DR: HM-TIF是一个专门为非对齐多模态安全数据设计的威胁检测框架，通过层次化跨注意力和时间相关性协议，在不依赖对齐数据的情况下实现88.7%的检测准确率，并将误报率降低32%。


<details>
  <summary>Details</summary>
Motivation: 解决多模态威胁检测中安全工具孤立运行、数据流自然不对齐的现实挑战，避免现有方法依赖人工对齐数据的局限性。

Method: 采用层次化跨注意力机制和动态权重调整，结合新颖的时间相关性协议来保持统计独立性，处理独立安全数据流的相关性。

Result: 在UNSW-NB15、CSE-CIC-IDS2018和CICBell-DNS2021数据集上达到88.7%的准确率，误报率降低32%，且在模态缺失时仍保持鲁棒性。

Conclusion: HM-TIF证明了多模态融合在数据不对齐情况下仍能提供操作优势，为安全团队处理异构、非协调数据源提供了实用部署指南。

Abstract: Multi-modal threat detection faces a fundamental challenge that involves
security tools operating in isolation, and this creates streams of network,
email, and system data with no natural alignment or correlation. We present
Hierarchical Multi-Modal Threat Intelligence Fusion (HM-TIF), a framework
explicitly designed for this realistic scenario where naturally aligned
multi-modal attack data does not exist. Unlike prior work that assumes or
creates artificial alignment, we develop principled methods for correlating
independent security data streams while maintaining operational validity. Our
architecture employs hierarchical cross-attention with dynamic weighting that
adapts to data availability and threat context, coupled with a novel temporal
correlation protocol that preserves statistical independence. Evaluation on
UNSW-NB15, CSE-CIC-IDS2018, and CICBell-DNS2021 datasets demonstrates that
HM-TIF achieves 88.7% accuracy with a critical 32% reduction in false positive
rates, even without true multi-modal training data. The framework maintains
robustness when modalities are missing, making it immediately deployable in
real security operations where data streams frequently have gaps. Our
contributions include: (i) the first multi-modal security framework explicitly
designed for non-aligned data, (ii) a temporal correlation protocol that avoids
common data leakage pitfalls, (iii) empirical validation that multi-modal
fusion provides operational benefits even without perfect alignment, and (iv)
practical deployment guidelines for security teams facing heterogeneous,
uncoordinated data sources. Index Terms: multi-modal learning, threat
intelligence, non-aligned data, operational security, cross-attention
mechanisms, practical deployment

</details>


### [74] [Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts](https://arxiv.org/abs/2510.15973)
*Tiarnaigh Downey-Webb,Olamide Jogunola,Oluwaseun Ajao*

Main category: cs.CR

TL;DR: 本文系统评估了四个主流大语言模型（Phi-2、Llama-2-7B-Chat、GPT-3.5-Turbo、GPT-4）对四种对抗攻击的鲁棒性，发现Llama-2安全性最高，Phi-2最脆弱，并揭示了攻击在不同模型间的可迁移性模式。


<details>
  <summary>Details</summary>
Motivation: 评估主流大语言模型在多样化对抗攻击下的安全性，识别模型间的安全差异和攻击可迁移性模式，为针对性防御机制开发提供依据。

Method: 使用SALAD-Bench数据集的1,200个分层提示，针对四种攻击向量（人工编写提示、AutoDAN、GCG、TAP）和六种危害类别进行系统评估，采用Friedman检验进行统计分析。

Result: Llama-2总体安全性最高（平均攻击成功率3.4%），Phi-2最脆弱（7.0%）；GCG和TAP攻击在目标模型上效果不佳，但迁移到其他模型时成功率显著提高（GPT-4最高达17%）；恶意使用提示的攻击成功率最高（10.71%）。

Conclusion: 大语言模型在对抗攻击下存在显著安全差异，攻击具有明显的可迁移性，恶意使用类别最易受攻击，研究结果为理解跨模型安全漏洞和开发针对性防御提供了重要见解。

Abstract: This paper presents a systematic security assessment of four prominent Large
Language Models (LLMs) against diverse adversarial attack vectors. We evaluate
Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack
categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),
and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs
1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six
harm categories. Results demonstrate significant variations in model
robustness, with Llama-2 achieving the highest overall security (3.4% average
attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%
average attack success rate). We identify critical transferability patterns
where GCG and TAP attacks, though ineffective against their target model
(Llama-2), achieve substantially higher success rates when transferred to other
models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals
significant differences in vulnerability across harm categories ($p < 0.001$),
with malicious use prompts showing the highest attack success rates (10.71%
average). Our findings contribute to understanding cross-model security
vulnerabilities and provide actionable insights for developing targeted defense
mechanisms

</details>


### [75] [Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies](https://arxiv.org/abs/2510.15989)
*Keshav Sood,Sanjay Selvaraj,Youyang Qu*

Main category: cs.CR

TL;DR: 本文提出了一种名为Meta-Guardian的隐私保护系统架构，用于在VR头显中实时识别和过滤生物特征信号，防止敏感数据传输或存储。


<details>
  <summary>Details</summary>
Motivation: 沉浸式技术（VR、AR、MR）需要收集生物特征数据来创造沉浸式体验，但这些实时反馈信息（包括生物特征）由于数据的敏感性而引发严重的隐私问题。现有文献大多忽视了头戴式显示系统中实时生物特征数据过滤的复杂性。

Method: 开发了一个模块化的Unity软件开发工具包（SDK），与主流沉浸式平台兼容。该系统采用机器学习模型进行信号分类，并使用过滤机制来阻止敏感数据。

Result: 实现了一个可在VR头显内实时处理生物特征信号的隐私保护框架，使开发者能够在各种头显和应用中嵌入隐私设计原则。

Conclusion: Meta-Guardian系统为沉浸式体验提供了一种实用的隐私保护解决方案，通过在设备端实时过滤敏感生物特征数据来保护用户隐私。

Abstract: The use of Immersive Technologies has shown its potential to revolutionize
many sectors such as health, entertainment, education, and industrial sectors.
Immersive technologies such as Virtual Reality (VR), Augmented reality (AR),
and Mixed Reality (MR) have redefined user interaction through real-time
biometric and behavioral tracking. Although Immersive Technologies (XR)
essentially need the collection of the biometric data which acts as a baseline
to create immersive experience, however, this ongoing feedback information
(includes biometrics) poses critical privacy concerns due to the sensitive
nature of the data collected. A comprehensive review of recent literature
explored the technical dimensions of related problem; however, they largely
overlook the challenge particularly the intricacies of real-time biometric data
filtering within head-mounted display system. Motivated from this, in this
work, we propose a novel privacy-preserving system architecture that identifies
and filters biometric signals (within the VR headset) in real-time before
transmission or storage. Implemented as a modular Unity Software-development
Kit (SDK) compatible with major immersive platforms, our solution (named
Meta-Guardian) employs machine learning models for signal classification and a
filtering mechanism to block sensitive data. This framework aims to enable
developers to embed privacy-by-design principles into immersive experiences on
various headsets and applications.

</details>


### [76] [Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers](https://arxiv.org/abs/2510.16005)
*Giacomo Bertollo,Naz Bodemir,Jonah Burgess*

Main category: cs.CR

TL;DR: 对500名CTF参与者的分析显示，虽然参与者能轻松绕过简单的AI防护措施，但多层多步防御仍构成显著挑战，为构建更安全的AI系统提供了具体见解。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统的安全防护措施在实际攻击场景中的有效性，了解不同防御策略对攻击者的阻碍程度。

Method: 通过分析500名CTF（夺旗赛）参与者的攻击行为，测试简单AI防护措施与多层多步防御策略的效果对比。

Result: 参与者能够轻松绕过简单的AI防护措施，但面对多层多步防御时遇到显著困难。

Conclusion: 多层多步防御策略在保护AI系统安全方面比简单防护措施更有效，为构建更安全的AI系统提供了实践指导。

Abstract: Analyzing 500 CTF participants, this paper shows that while participants
readily bypassed simple AI guardrails using common techniques, layered
multi-step defenses still posed significant challenges, offering concrete
insights for building safer AI systems.

</details>


### [77] [On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation](https://arxiv.org/abs/2510.16024)
*Abdulrahman Alhaidari,Balaji Palanisamy,Prashant Krishnamurthy*

Main category: cs.CR

TL;DR: 提出了第一个完全在链上的去中心化学习框架，通过Layer-2执行计算密集型任务，将验证后的模型更新传播到Layer-1，并在智能合约中实现低延迟推理，以防止DeFi平台中的漏洞利用。


<details>
  <summary>Details</summary>
Motivation: DeFi平台每年因业务逻辑或会计漏洞而损失数十亿美元，现有防御措施无法防止通过私有中继或同一区块内恶意合约提交的攻击。

Method: 使用Proof-of-Improvement协议管理训练过程，验证每个去中心化微更新；开发量化和循环展开技术，在以太坊区块gas限制内实现逻辑回归、SVM、MLP、CNN和门控RNN的推理。

Result: 收集了298个独特的真实世界漏洞利用案例（2020-2025年），涉及402次漏洞利用交易，覆盖8个EVM链，总损失达37.4亿美元。

Conclusion: 该框架能够有效防止DeFi平台中的漏洞利用，通过去中心化的链上学习机制提供安全防护。

Abstract: Billions of dollars are lost every year in DeFi platforms by transactions
exploiting business logic or accounting vulnerabilities. Existing defenses
focus on static code analysis, public mempool screening, attacker contract
detection, or trusted off-chain monitors, none of which prevents exploits
submitted through private relays or malicious contracts that execute within the
same block. We present the first decentralized, fully on-chain learning
framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce
cost, (ii) propagates verified model updates to Layer-1, and (iii) enables
gas-bounded, low-latency inference inside smart contracts. A novel
Proof-of-Improvement (PoIm) protocol governs the training process and verifies
each decentralized micro update as a self-verifying training transaction.
Updates are accepted by \textit{PoIm} only if they demonstrably improve at
least one core metric (e.g., accuracy, F1-score, precision, or recall) on a
public benchmark without degrading any of the other core metrics, while
adversarial proposals get financially penalized through an adaptable test set
for evolving threats. We develop quantization and loop-unrolling techniques
that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs
(with support for formally verified decision tree inference) within the
Ethereum block gas limit, while remaining bit-exact to their off-chain
counterparts, formally proven in Z3. We curate 298 unique real-world exploits
(2020 - 2025) with 402 exploit transactions across eight EVM chains,
collectively responsible for \$3.74 B in losses.

</details>


### [78] [Membership Inference over Diffusion-models-based Synthetic Tabular Data](https://arxiv.org/abs/2510.16037)
*Peini Cheng,Amir Bahmani*

Main category: cs.CR

TL;DR: 本研究调查了基于扩散的合成表格数据生成方法的隐私风险，重点关注其对成员推断攻击的易感性。研究发现TabDDPM比TabSyn更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 评估扩散模型在合成表格数据生成中的隐私影响，了解不同模型对成员推断攻击的脆弱性。

Method: 开发基于逐步误差比较方法的查询式成员推断攻击，对TabDDPM和TabSyn两种模型进行测试。

Result: TabDDPM更容易受到成员推断攻击，而TabSyn对这些攻击表现出韧性。

Conclusion: 扩散模型的隐私影响需要认真评估，需要进一步研究用于合成数据生成的强大隐私保护机制。

Abstract: This study investigates the privacy risks associated with diffusion-based
synthetic tabular data generation methods, focusing on their susceptibility to
Membership Inference Attacks (MIAs). We examine two recent models, TabDDPM and
TabSyn, by developing query-based MIAs based on the step-wise error comparison
method. Our findings reveal that TabDDPM is more vulnerable to these attacks.
TabSyn exhibits resilience against our attack models. Our work underscores the
importance of evaluating the privacy implications of diffusion models and
encourages further research into robust privacy-preserving mechanisms for
synthetic data generation.

</details>


### [79] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: 本文提出PrivacyPAD框架，使用强化学习动态路由文本块，在保护隐私和任务性能之间取得最优平衡，在隐私-效用前沿达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 用户向大语言模型提交查询时，提示中常包含敏感数据，面临选择：发送给强大的专有LLM获得最佳性能但风险数据暴露，或使用本地小模型保证隐私但性能下降。现有静态流水线方法破坏语言连贯性且无差别删除隐私信息。

Method: 将隐私意识委托重新表述为顺序决策问题，引入强化学习框架PrivacyPAD，训练智能体动态路由文本块，学习区分可替换PII（本地屏蔽）和任务关键PII（策略性发送到远程模型）。

Result: 在复杂场景下验证方法，引入高PII密度的新医疗数据集，框架在隐私-效用前沿达到新的最先进水平。

Conclusion: 证明了在敏感环境中部署LLM需要学习性、自适应策略的必要性。

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [80] [A Multi-Cloud Framework for Zero-Trust Workload Authentication](https://arxiv.org/abs/2510.16067)
*Saurabh Deochake,Ryan Murphy,Jeremiah Gearheart*

Main category: cs.CR

TL;DR: 提出一个基于工作负载身份联盟和OpenID Connect的多云框架，实现无密钥认证，使用加密验证的临时令牌替代静态长期凭证，显著降低攻击面。


<details>
  <summary>Details</summary>
Motivation: 静态长期工作负载认证凭证存在严重安全风险，违反零信任原则，需要解决凭证盗窃问题。

Method: 使用工作负载身份联盟和OpenID Connect构建多云框架，采用加密验证的临时令牌进行无密钥认证。

Result: 在企业级Kubernetes环境中验证了该框架，显著减少了攻击面，能够统一管理跨云工作负载身份。

Conclusion: 该模型为跨不同云环境的工作负载身份管理提供了统一解决方案，支持未来实现强大的基于属性的访问控制。

Abstract: Static, long-lived credentials for workload authentication create untenable
security risks that violate Zero-Trust principles. This paper presents a
multi-cloud framework using Workload Identity Federation (WIF) and OpenID
Connect (OIDC) for secretless authentication. Our approach uses
cryptographically-verified, ephemeral tokens, allowing workloads to
authenticate without persistent private keys and mitigating credential theft.
We validate this framework in an enterprise-scale Kubernetes environment, which
significantly reduces the attack surface. The model offers a unified solution
to manage workload identities across disparate clouds, enabling future
implementation of robust, attribute-based access control.

</details>


### [81] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出了一种实用的卡上人脸验证匹配设计，使用64/128位紧凑模板，通过PCA-ITQ离线生成，在卡上通过恒定时间汉明距离进行比较，满足ISO/IEC传输约束和隐私目标。


<details>
  <summary>Details</summary>
Motivation: 设计满足ISO/IEC 7816-4和14443-4标准的卡上人脸验证系统，实现固定长度载荷、仅决策状态字（无分数泄露）的安全通信，同时最小化EEPROM占用。

Method: 使用PCA-ITQ生成64/128位二进制模板，在卡上通过恒定时间汉明距离进行匹配，采用固定长度的APDU命令载荷和仅决策状态字，避免分数泄露。

Result: 在9.6 kbps最慢接触速率下，总验证时间为43.9 ms（64位）和52.3 ms（128位）；在38.4 kbps下均小于14 ms。在FAR=1%时，两种码长均达到TPR=0.836，128位相比64位降低了EER。

Conclusion: 短二进制模板、固定载荷仅决策APDU和恒定时间匹配能够满足ISO/IEC传输约束，具有宽裕的时间余量，并与ISO/IEC 24745隐私目标保持一致。当前限制为单数据集评估和设计级时序分析，下一步将进行AgeDB/CFP-FP评估和卡上微基准测试。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [82] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 该论文系统比较了生成式和判别式分类器在成员推理攻击中的脆弱性，发现生成式分类器由于显式建模联合概率P(X,Y)而更容易遭受隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式和判别式分类器在成员推理攻击中的系统性比较有限，本文旨在填补这一空白，从理论上和实证上分析两类分类器的隐私脆弱性。

Method: 通过理论分析和综合实证评估，研究涵盖判别式、生成式和伪生成式文本分类器，在9个基准数据集上使用多种MIA策略进行测试，考察不同训练数据量的影响。

Result: 研究表明完全生成式分类器对成员推理攻击最为脆弱，其标准推理方法显著放大了隐私风险，揭示了分类器设计中固有的效用-隐私权衡。

Conclusion: 生成式分类器在隐私敏感应用中需谨慎部署，未来应研究开发既能保持效用又能缓解成员推理漏洞的隐私保护生成式分类器。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [83] [Prompt injections as a tool for preserving identity in GAI image descriptions](https://arxiv.org/abs/2510.16128)
*Kate Glazko,Jennifer Mankoff*

Main category: cs.CR

TL;DR: 论文提出将提示注入作为一种工具，使间接用户能够通过在自己的内容中嵌入提示来保护身份特征免受生成式AI系统的偏见影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的偏见和缺乏代表性等问题会影响那些不直接与系统交互但内容被使用的间接用户，现有缓解方法大多需要自上而下的干预，缺乏用户自主性。

Method: 将提示注入重新定义为内容所有者抵抗的工具，通过案例研究展示如何在图像描述中保留所有者的性别和残疾身份特征。

Result: 开发了一种使间接用户能够自主减轻生成式AI对其造成伤害的方法，通过内容内嵌提示来保护个人身份特征。

Conclusion: 提示注入可以作为一种赋权工具，让间接用户从自身内容内部抵抗生成式AI的偏见，提供了一种自下而上的伤害缓解策略。

Abstract: Generative AI risks such as bias and lack of representation impact people who
do not interact directly with GAI systems, but whose content does: indirect
users. Several approaches to mitigating harms to indirect users have been
described, but most require top down or external intervention. An emerging
strategy, prompt injections, provides an empowering alternative: indirect users
can mitigate harm against them, from within their own content. Our approach
proposes prompt injections not as a malicious attack vector, but as a tool for
content/image owner resistance. In this poster, we demonstrate one case study
of prompt injections for empowering an indirect user, by retaining an image
owner's gender and disabled identity when an image is described by GAI.

</details>


### [84] [C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations](https://arxiv.org/abs/2510.16229)
*Vienna Li,Justin Villa,Dan Diessner,Jayson Clifford,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 通过分析天线不同朝向下的卫星载波噪声密度比(C/N₀)变化，提出了一种GPS欺骗检测方法。研究发现欺骗信号在平坦朝向时C/N₀最高，而在倾斜朝向时显著降低，这种模式可用于检测GPS欺骗。


<details>
  <summary>Details</summary>
Motivation: GPS欺骗对航空安全构成严重威胁，需要开发有效的检测方法来保护飞机导航系统免受误导。

Method: 使用u-blox EVK-M8U接收器和GPSG-1000卫星模拟器，在平坦、右倾和左倾三种天线朝向条件下，分别收集真实天空信号和欺骗信号的C/N₀数据。

Result: 真实信号下C/N₀随朝向自然波动，而欺骗信号在平坦朝向时C/N₀最高，倾斜朝向时因与欺骗源不对齐而C/N₀降低，呈现明显不同的模式。

Conclusion: 通过简单的机动操作（如短暂倾斜）诱导C/N₀变化，可以为通用航空和无人机系统提供GPS欺骗的早期检测线索。

Abstract: GPS spoofing poses a growing threat to aviation by falsifying satellite
signals and misleading aircraft navigation systems. This paper demonstrates a
proof-of-concept spoofing detection strategy based on analyzing satellite
Carrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static
antenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite
simulator, C/N$_0$ data is collected under three antenna orientations flat,
banked right, and banked left) in both real-sky (non-spoofed) and spoofed
environments. Our findings reveal that under non-spoofed signals, C/N$_0$
values fluctuate naturally with orientation, reflecting true geometric
dependencies. However, spoofed signals demonstrate a distinct pattern: the flat
orientation, which directly faces the spoofing antenna, consistently yielded
the highest C/N$_0$ values, while both banked orientations showed reduced
C/N$_0$ due to misalignment with the spoofing source. These findings suggest
that simple maneuvers such as brief banking to induce C/N$_0$ variations can
provide early cues of GPS spoofing for general aviation and UAV systems.

</details>


### [85] [Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation](https://arxiv.org/abs/2510.16331)
*Fatemeh Jafarian Dehkordi,Elahe Vedadi,Alireza Feizbakhsh,Yasaman Keshtkarjahromi,Hulya Seferoglu*

Main category: cs.CR

TL;DR: 提出了一种新颖的二进制多方计算（BiMPC）框架，专门针对树型垂直联邦学习中的按位操作进行优化，解决了传统隐私保护技术在二进制数据操作上的局限性。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，如何在保护数据隐私的同时实现协作计算是一个关键挑战。虽然联邦学习的隐私保护技术已得到广泛发展，但针对按位操作场景（如基于树的垂直联邦学习）的方法仍未被充分探索。传统的Shamir秘密共享和多方计算机制并未针对二进制数据的按位操作进行优化。

Method: 提出了BiMPC框架，核心是名为DoMA（通过模加法进行点积计算）的新方法，使用常规加法和模加法进行高效的二进制点积计算。为确保隐私，BiMPC在更高域中使用随机掩码进行线性计算，并使用三方不经意传输协议进行非线性二进制操作。

Result: 对BiMPC框架的隐私保证进行了严格分析，证明了其在分布式环境中的效率和可扩展性。

Conclusion: BiMPC框架有效解决了现有方法在二进制按位操作上的局限性，为树型垂直联邦学习等场景提供了高效的隐私保护解决方案。

Abstract: Striking a balance between protecting data privacy and enabling collaborative
computation is a critical challenge for distributed machine learning. While
privacy-preserving techniques for federated learning have been extensively
developed, methods for scenarios involving bitwise operations, such as
tree-based vertical federated learning (VFL), are still underexplored.
Traditional mechanisms, including Shamir's secret sharing and multi-party
computation (MPC), are not optimized for bitwise operations over binary data,
particularly in settings where each participant holds a different part of the
binary vector. This paper addresses the limitations of existing methods by
proposing a novel binary multi-party computation (BiMPC) framework. The BiMPC
mechanism facilitates privacy-preserving bitwise operations, with a particular
focus on dot product computations of binary vectors, ensuring the privacy of
each individual bit. The core of BiMPC is a novel approach called Dot Product
via Modular Addition (DoMA), which uses regular and modular additions for
efficient binary dot product calculation. To ensure privacy, BiMPC uses random
masking in a higher field for linear computations and a three-party oblivious
transfer (triot) protocol for non-linear binary operations. The privacy
guarantees of the BiMPC framework are rigorously analyzed, demonstrating its
efficiency and scalability in distributed settings.

</details>


### [86] [EditMark: Watermarking Large Language Models based on Model Editing](https://arxiv.org/abs/2510.16367)
*Shuai Li,Kejiang Chen,Jun Jiang,Jie Zhang,Qiyi Yao,Kai Zeng,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: EditMark是一种基于模型编辑的LLM水印方法，无需训练即可嵌入隐形且无损性能的水印，通过多轮稳定编辑策略和噪声矩阵注入提高水印的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法需要训练水印数据集，导致高昂的训练成本并影响模型性能，且水印文本不够自然。需要一种无需训练、隐形且不影响性能的水印方法。

Method: 利用模型编辑技术，为多答案问题分配唯一水印，通过自适应多轮稳定编辑策略和噪声矩阵注入来更新LLM权重，实现水印嵌入。

Result: 在20秒内嵌入32位水印（微调需6875秒），水印提取成功率达100%，具有良好的保真度、隐形性和对常见攻击的鲁棒性。

Conclusion: EditMark是首个基于模型编辑的LLM水印方法，实现了无需训练、隐形且无损性能的水印嵌入，在效率和效果上均优于传统方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, but
their training requires extensive data and computational resources, rendering
them valuable digital assets. Therefore, it is essential to watermark LLMs to
protect their copyright and trace unauthorized use or resale. Existing methods
for watermarking LLMs primarily rely on training LLMs with a watermarked
dataset, which entails burdensome training costs and negatively impacts the
LLM's performance. In addition, their watermarked texts are not logical or
natural, thereby reducing the stealthiness of the watermark. To address these
issues, we propose EditMark, the first watermarking method that leverages model
editing to embed a training-free, stealthy, and performance-lossless watermark
for LLMs. We observe that some questions have multiple correct answers.
Therefore, we assign each answer a unique watermark and update the weights of
LLMs to generate corresponding questions and answers through the model editing
technique. In addition, we refine the model editing technique to align with the
requirements of watermark embedding. Specifically, we introduce an adaptive
multi-round stable editing strategy, coupled with the injection of a noise
matrix, to improve both the effectiveness and robustness of the watermark
embedding. Extensive experiments indicate that EditMark can embed 32-bit
watermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a
watermark extraction success rate of 100%, which demonstrates its effectiveness
and efficiency. External experiments further demonstrate that EditMark has
fidelity, stealthiness, and a certain degree of robustness against common
attacks.

</details>


### [87] [$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching](https://arxiv.org/abs/2510.16544)
*Weijie Chen,Shan Tang,Yulin Tang,Xiapu Luo,Yinqian Zhang,Weizhong Qiang*

Main category: cs.CR

TL;DR: ρHammer是一个新的Rowhammer攻击框架，通过DRAM地址映射逆向工程、预取式锤击和反推测锤击技术，成功突破了最新Intel架构的防护，在Raptor Lake等最新平台上实现了稳定的比特翻转。


<details>
  <summary>Details</summary>
Motivation: 传统基于负载的Rowhammer攻击在最新Intel架构（如Alder Lake和Raptor Lake）上变得无效，需要开发新的攻击方法来克服地址映射复杂化、激活率瓶颈和推测执行干扰等核心挑战。

Method: 1) 使用选择性成对测量和结构化推论的DRAM地址映射逆向工程方法；2) 基于预取指令的异步锤击范式，结合多bank并行化；3) 利用控制流混淆和优化的NOP伪屏障的反推测锤击技术。

Result: 在四个最新Intel架构上的评估显示：在2小时攻击模式模糊测试中诱导超过20万额外比特翻转，在Comet和Rocket Lake上的翻转率比基线高112倍，首次在Raptor Lake上实现稳定翻转率（2,291/分钟）和端到端利用。

Conclusion: ρHammer框架成功克服了最新Intel架构的Rowhammer防护机制，证明了即使在最新硬件上，Rowhammer威胁仍然存在且需要持续关注。

Abstract: Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)
that continues to pose a significant threat to various systems. However, we
find that conventional load-based attacks are becoming highly ineffective on
the most recent architectures such as Intel Alder and Raptor Lake. In this
paper, we present $\rho$Hammer, a new Rowhammer framework that systematically
overcomes three core challenges impeding attacks on these new architectures.
First, we design an efficient and generic DRAM address mapping
reverse-engineering method that uses selective pairwise measurements and
structured deduction, enabling recovery of complex mappings within seconds on
the latest memory controllers. Second, to break through the activation rate
bottleneck of load-based hammering, we introduce a novel prefetch-based
hammering paradigm that leverages the asynchronous nature of x86 prefetch
instructions and is further enhanced by multi-bank parallelism to maximize
throughput. Third, recognizing that speculative execution causes more severe
disorder issues for prefetching, which cannot be simply mitigated by memory
barriers, we develop a counter-speculation hammering technique using
control-flow obfuscation and optimized NOP-based pseudo-barriers to maintain
prefetch order with minimal overhead. Evaluations across four latest Intel
architectures demonstrate $\rho$Hammer's breakthrough effectiveness: it induces
up to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes
and has a 112x higher flip rate than the load-based hammering baselines on
Comet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on
the latest Raptor Lake architecture, where baselines completely fail, achieving
stable flip rates of 2,291/min and fast end-to-end exploitation.

</details>


### [88] [Toward Understanding Security Issues in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2510.16558)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 本文首次对MCP生态系统进行全面的安全分析，揭示了由于缺乏输出验证机制和服务器审查流程，恶意服务器可以操纵模型行为并引发多种安全威胁，包括敏感数据泄露。


<details>
  <summary>Details</summary>
Motivation: MCP生态系统快速发展但缺乏系统性安全研究，需要分析其架构和安全风险。

Method: 将MCP生态系统分解为主机、注册中心和服务器三个核心组件，分析它们之间的交互和信任关系，并收集分析67,057个服务器的数据集进行定量分析。

Result: 发现主机缺乏LLM生成输出的验证机制，恶意服务器可操纵模型行为；注册中心缺乏服务器审查流程，大量服务器可能被攻击者劫持。

Conclusion: 提出了针对MCP主机、注册中心和用户的实用防御策略，并向相关方负责任地披露了发现。

Abstract: The Model Context Protocol (MCP) is an emerging open standard that enables
AI-powered applications to interact with external tools through structured
metadata. A rapidly growing ecosystem has formed around MCP, including a wide
range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP
registries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),
and thousands of community-contributed MCP servers. Although the MCP ecosystem
is gaining traction, there has been little systematic study of its architecture
and associated security risks. In this paper, we present the first
comprehensive security analysis of the MCP ecosystem. We decompose MCP
ecosystem into three core components: hosts, registries, and servers, and study
the interactions and trust relationships among them. Users search for servers
on registries and configure them in the host, which translates LLM-generated
output into external tool invocations provided by the servers and executes
them. Our qualitative analysis reveals that hosts lack output verification
mechanisms for LLM-generated outputs, enabling malicious servers to manipulate
model behavior and induce a variety of security threats, including but not
limited to sensitive data exfiltration. We uncover a wide range of
vulnerabilities that enable attackers to hijack servers, due to the lack of a
vetted server submission process in registries. To support our analysis, we
collect and analyze a dataset of 67,057 servers from six public registries. Our
quantitative analysis demonstrates that a substantial number of servers can be
hijacked by attackers. Finally, we propose practical defense strategies for MCP
hosts, registries, and users. We responsibly disclosed our findings to affected
hosts and registries.

</details>


### [89] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus是一个防御框架，为文本到图像模型提供全面保护，抵御白盒攻击者的恶意微调攻击。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的安全措施在面临知道并能调整模型参数的白盒攻击者时会失效，因此需要开发新的防御机制。

Method: 设计内部调节器将不安全输入特征解码为零向量，同时保持良性输入特征的解码性能；通过精心设计的不可微调学习机制加强模型对齐。

Result: 实验验证了在安全内容生成方面的性能完整性，以及拒绝不安全内容生成的有效性，确认了Patronus对各种白盒微调攻击的韧性。

Conclusion: Patronus框架成功为文本到图像模型提供了对抗白盒攻击者的全面保护，确保模型在面临恶意微调时不会被破坏。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


### [90] [DESTinE Block: Private Blockchain Based Data Storage Framework for Power System](https://arxiv.org/abs/2510.16593)
*Khandaker Akramul Haque,Katherine R. Davis*

Main category: cs.CR

TL;DR: DESTinE Block是一个基于区块链的数据存储框架，专为电力系统设计，优化用于资源受限环境（如单板计算机）。它采用IPFS存储大文件，在自定义区块链上记录安全可追溯的元数据，使用PoA共识机制，支持在Raspberry Pi等设备上部署。


<details>
  <summary>Details</summary>
Motivation: 为电力系统在资源受限环境下提供安全、去中心化的数据存储解决方案，特别是在电网边缘设备上实现防篡改的数据保留。

Method: 采用双区块链抽象架构，IPFS存储大文件，DESTinE区块链记录元数据（CID、上传者身份、管理员验证、时间戳）。基于PoA共识机制，需要管理员和上传者共同协作创建区块，使用不同的加密密钥对。

Result: 在x86设备和ARM64 Raspberry Pi上测试成功，与基于Multichain的类似框架相比，DESTinE Block在保持最小硬件要求的同时，为分布式电力系统基础设施提供了有前景的防篡改数据保留解决方案。

Conclusion: DESTinE Block是一个有前景的解决方案，能够在资源受限环境下为智能电网应用提供安全、去中心化的日志和测量数据存储，同时保持计算效率。

Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework
designed for power systems and optimized for resource-constrained environments,
including grid-edge devices such as single-board computers. The proposed
architecture leverages the InterPlanetary File System (IPFS) for storing large
files while maintaining secure and traceable metadata on a custom blockchain
named DESTinE Block. The metadata, comprising the IPFS Content Identifier
(CID), uploader identity, administrator verification, and timestamp; is
immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block
adopts a dual-blockchain abstraction, where the blockchain remains unaware of
the IPFS storage layer to enhance security and limit the exposure of sensitive
file data. The consensus mechanism is based on Proof of Authority (PoA), where
both an administrator and an uploader with distinct cryptographic key pairs are
required to create a block collaboratively. Each block contains verified
signatures of both parties and is designed to be computationally efficient,
enabling deployment on devices like the Raspberry Pi 5. The framework was
tested on both an x86-based device and an ARM64-based Raspberry Pi,
demonstrating its potential for secure, decentralized logging and measurement
storage in smart grid applications. Moreover, DESTinE Block is compared with a
similar framework based on Multichain. The results indicate that DESTinE Block
provides a promising solution for tamper-evident data retention in distributed
power system infrastructure while maintaining minimal hardware requirements.

</details>


### [91] [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)
*Asmita Mohanty,Gezheng Kang,Lei Gao,Murali Annavaram*

Main category: cs.CR

TL;DR: DistilLock是一个TEE辅助的微调框架，在边缘设备上实现隐私保护的知识蒸馏，同时保护数据隐私和模型知识产权。


<details>
  <summary>Details</summary>
Motivation: 解决传统云端微调LLM带来的数据隐私问题，以及边缘设备微调导致的模型知识产权泄露风险。

Method: 在数据所有者设备上的可信执行环境(TEE)中运行专有基础模型作为安全黑盒教师，采用模型混淆机制将混淆权重卸载到不可信加速器进行高效知识蒸馏。

Result: DistilLock能够防止未经授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率。

Conclusion: DistilLock为基于边缘的LLM个性化提供了一个安全实用的解决方案，平衡了隐私保护与计算效率。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
diverse tasks, but fine-tuning them typically relies on cloud-based,
centralized infrastructures. This requires data owners to upload potentially
sensitive data to external servers, raising serious privacy concerns. An
alternative approach is to fine-tune LLMs directly on edge devices using local
data; however, this introduces a new challenge: the model owner must transfer
proprietary models to the edge, which risks intellectual property (IP) leakage.
To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning
framework that enables privacy-preserving knowledge distillation on the edge.
In DistilLock, a proprietary foundation model is executed within a trusted
execution environment (TEE) enclave on the data owner's device, acting as a
secure black-box teacher. This setup preserves both data privacy and model IP
by preventing direct access to model internals. Furthermore, DistilLock employs
a model obfuscation mechanism to offload obfuscated weights to untrusted
accelerators for efficient knowledge distillation without compromising
security. We demonstrate that DistilLock prevents unauthorized knowledge
distillation processes and model-stealing attacks while maintaining high
computational efficiency, but offering a secure and practical solution for
edge-based LLM personalization.

</details>


### [92] [Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022](https://arxiv.org/abs/2510.16744)
*Srinivas Vivek*

Main category: cs.CR

TL;DR: 本文对Xie等人在NSS 2022提出的隐私保护网约车服务协议进行了被动攻击分析，证明该协议存在严重安全漏洞，攻击者可以完全恢复乘客和司机的精确位置信息。


<details>
  <summary>Details</summary>
Motivation: 随着网约车服务的普及，保护用户位置隐私变得至关重要。Xie等人提出的PP-RHS协议声称能够保护用户位置隐私，但本文旨在验证该协议的实际安全性。

Method: 采用被动攻击方法，攻击者作为服务提供商可以监听协议通信，通过分析协议执行过程中的信息泄露来恢复用户位置数据。攻击效率高，不依赖于安全参数。

Result: 成功演示了对PP-RHS协议的攻击，能够完全恢复每个乘车请求中乘客和响应司机的精确位置信息。攻击具有高效性，不受安全参数限制。

Conclusion: Xie等人提出的隐私保护网约车服务协议存在严重安全漏洞，无法有效保护用户位置隐私，需要重新设计更安全的协议方案。

Abstract: Ride-Hailing Services (RHS) match a ride request initiated by a rider with a
suitable driver responding to the ride request. A Privacy-Preserving RHS
(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'
and drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie
et al. proposed a PP-RHS. In this work, we demonstrate a passive attack on
their PP-RHS protocol. Our attack allows the SP to completely recover the
locations of the rider as well as that of the responding drivers in every ride
request. Further, our attack is very efficient as it is independent of the
security parameter.

</details>


### [93] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 提出了可验证微调协议，通过零知识证明确保发布的模型确实来自公开初始化，并按照声明的训练程序和可审计的数据集承诺进行训练。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型微调发布实践在数据使用和更新计算方面缺乏可信保证，存在信任缺口，特别是在受监管和去中心化部署场景中。

Method: 结合五个关键要素：数据源承诺、可验证采样器、参数高效微调的更新电路、递归聚合证明、以及来源绑定和可信执行属性卡。

Result: 在英语和双语指令混合数据集上，该方法在严格预算内保持了实用性，实现了实用的证明性能，策略配额零违规，私有采样无索引泄露。

Conclusion: 端到端的可验证微调对于真实的参数高效管道是可行的，为受监管和去中心化部署填补了关键的信任缺口。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [94] [ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research](https://arxiv.org/abs/2510.16835)
*Hongpeng Bai,Minhong Dong,Yao Zhang,Shunzhe Zhao,Haobo Zhang,Lingyue Li,Yude Bai,Guangquan Xu*

Main category: cs.CR

TL;DR: Android恶意软件检测面临数据集质量问题，主流数据集存在标签噪声和时效性不足的问题，自动标注工具也存在聚合策略不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备在工业系统中的广泛应用，Android恶意软件检测变得至关重要，但现有数据集存在标签噪声和时效性问题，影响了检测效果。

Method: 论文未明确描述具体方法，但指出了现有数据集和自动标注工具的局限性。

Result: 分析发现主流数据集（如Drebin）依赖VirusTotal多引擎聚合导致标签噪声，样本过时降低时效性，自动标注工具（如AVClass2）聚合策略不佳加剧标签错误。

Conclusion: Android恶意软件检测需要更高质量、实时的数据集作为有效检测和防御的基础，当前数据集和标注工具存在明显缺陷需要改进。

Abstract: The rapidly evolving Android malware ecosystem demands high-quality,
real-time datasets as a foundation for effective detection and defense. With
the widespread adoption of mobile devices across industrial systems, they have
become a critical yet often overlooked attack surface in industrial
cybersecurity. However, mainstream datasets widely used in academia and
industry (e.g., Drebin) exhibit significant limitations: on one hand, their
heavy reliance on VirusTotal's multi-engine aggregation results introduces
substantial label noise; on the other hand, outdated samples reduce their
temporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer
from suboptimal aggregation strategies, further compounding labeling errors and
propagating inaccuracies throughout the research community.

</details>


### [95] [Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy](https://arxiv.org/abs/2510.16871)
*Anirban Chakraborty,Nimish Mishra,Sayandeep Saha,Sarani Bhattacharya,Debdeep Mukhopadhyay*

Main category: cs.CR

TL;DR: 本文是对USENIX Security 2025主论文的补充说明，讨论了随机化缓存在性能和安全性方面的设计考虑，特别是针对[2]中提出的观察结果进行回应。


<details>
  <summary>Details</summary>
Motivation: 对随机化缓存设计进行系统分析，从性能和安全性两个角度评估不同设计方案，并回应当前研究中的新发现。

Method: 采用统一的基准测试策略来公平比较不同随机化缓存设计，并从安全角度考虑三种威胁假设：隐蔽信道、进程指纹侧信道和AES密钥恢复。

Result: 研究发现设计一个既具有现代组关联LLC相当效率又能抵抗基于竞争和基于占用攻击的随机化缓存仍然是一个开放性问题。[2]中的观察表明L1d缓存大小对攻击成功有影响，且带有随机初始种子的MIRAGE补丁版本可以防止AES密钥泄漏。

Conclusion: 随机化缓存设计需要在性能和安全性之间取得平衡，当前仍面临挑战。对[2]中提出的问题进行了讨论和回应。

Abstract: In the main text published at USENIX Security 2025, we presented a systematic
analysis of the role of cache occupancy in the design considerations for
randomized caches (from the perspectives of performance and security). On the
performance front, we presented a uniform benchmarking strategy that allows for
a fair comparison among different randomized cache designs. Likewise, from the
security perspective, we presented three threat assumptions: (1) covert
channels; (2) process fingerprinting side-channel; and (3) AES key recovery.
The main takeaway of our work is an open problem of designing a randomized
cache of comparable efficiency with modern set-associative LLCs, while still
resisting both contention-based and occupancy-based attacks. This note is meant
as an addendum to the main text in light of the observations made in [2]. To
summarize, the authors in [2] argue that (1) L1d cache size plays a role in
adversarial success, and that (2) a patched version of MIRAGE with randomized
initial seeding of global eviction map prevents leakage of AES key. We discuss
the same in this addendum.

</details>


### [96] [On the Credibility of Deniable Communication in Court](https://arxiv.org/abs/2510.16873)
*Jacob Leiken,Sunoo Park*

Main category: cs.CR

TL;DR: 该论文批判了密码学中可否认性概念与法庭证据实践的脱节，提出了更全面的可信度概念，包含伪造质量阈值、伪造难易程度和系统保留策略三个维度。


<details>
  <summary>Details</summary>
Motivation: 传统密码学可否认性概念过于技术化，未能充分考虑现实世界中证据认证的复杂性，特别是法庭证据处理中早已考虑证据可伪造性的现实。

Method: 通过分析密码学模型与现实世界证据实践的差距，提出可信度概念框架，包含三个关键维度：伪造质量阈值、伪造难易程度和系统保留策略。

Result: 建立了更全面的可信度模型，能够更好地指导现实世界安全通信系统的设计，考虑技术定义无法捕捉的威胁因素。

Conclusion: 可信度概念有助于在特定法律和社会技术背景下更细致地讨论密码学保证的强度和局限性，促进更实用的安全系统设计。

Abstract: Over time, cryptographically deniable systems have come to be associated in
computer-science literature with the idea of "denying" evidence in court -
specifically, with the ability to convincingly forge evidence in courtroom
scenarios and an inability to authenticate evidence in such contexts.
Evidentiary processes in courts, however, have been developed over centuries to
account for the reality that evidence has always been forgeable, and relies on
factors outside of cryptographic models to seek the truth "as well as possible"
while acknowledging that all evidence is imperfect. We argue that deniability
does not and need not change this paradigm.
  Our analysis highlights a gap between technical deniability notions and their
application to the real world. There will always be factors outside a
cryptographic model that influence perceptions of a message's authenticity, in
realistic situations. We propose the broader concept of credibility to capture
these factors. The credibility of a system is determined by (1) a threshold of
quality that a forgery must pass to be "believable" as an original
communication, which varies based on sociotechnical context and threat model,
(2) the ease of creating a forgery that passes this threshold, which is also
context- and threat-model-dependent, and (3) default system retention policy
and retention settings. All three aspects are important for designing secure
communication systems for real-world threat models, and some aspects of (2) and
(3) may be incorporated directly into technical system design. We hope that our
model of credibility will facilitate system design and deployment that
addresses threats that are not and cannot be captured by purely technical
definitions and existing cryptographic models, and support more nuanced
discourse on the strengths and limitations of cryptographic guarantees within
specific legal and sociotechnical contexts.

</details>


### [97] [UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks](https://arxiv.org/abs/2510.16923)
*Mansi Phute,Matthew Hull,Haoran Wang,Alec Helbling,ShengYun Peng,Willian Lunardi,Martin Andreoni,Wenke Lee,Polo Chau*

Main category: cs.CR

TL;DR: UNDREAM是一个将照片级真实感模拟器与可微分渲染器结合的软件框架，支持在3D对象上端到端优化对抗性扰动，并能控制环境因素如天气、光照、背景等。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器不可微分，导致对抗攻击研究无法整合环境因素，降低了攻击成功率。

Method: 通过结合照片级真实感模拟器和可微分渲染器，提供对环境因素（天气、光照、背景、相机角度等）的完全控制，实现端到端优化。

Result: 展示了多种物理上合理的对抗性对象，可在不同可配置环境中快速探索。

Conclusion: 照片级真实感模拟与可微分优化的结合为物理对抗攻击研究开辟了新途径。

Abstract: Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.

</details>


### [98] [Efficient derandomization of differentially private counting queries](https://arxiv.org/abs/2510.16959)
*Surendra Ghentiyala*

Main category: cs.CR

TL;DR: 本文提出了一种多项式时间机制，用于d个计数查询的差分隐私，仅需O(log d)位随机性，相比之前需要90TB随机性的方法大幅降低了随机性需求。


<details>
  <summary>Details</summary>
Motivation: 2020年人口普查的差分隐私需要90TB随机性，这在实践中可能过于昂贵或不可行。因此需要研究差分隐私的随机性复杂性，特别是d个计数查询的随机性需求。

Method: 基于一个简单观察：在对每个计数查询进行随机偏移后，许多查询的答案在是否添加噪声的情况下保持不变。这允许我们省略对许多计数查询结果添加噪声的步骤。该方法不使用舍入方案，而是多项式时间机制。

Result: 提出的多项式时间机制实现了与[CSV25]几乎相同的随机性复杂度与准确性权衡，仅需O(log d)位随机性，同时保持较高的准确性。

Conclusion: 本文提供了一种更清晰的方法来理解通过批量处理d个计数查询可以获得的随机性节省，为实际应用中的差分隐私提供了更实用的解决方案。

Abstract: Differential privacy for the 2020 census required an estimated 90 terabytes
of randomness [GL20], an amount which may be prohibitively expensive or
entirely infeasible to generate. Motivated by these practical concerns, [CSV25]
initiated the study of the randomness complexity of differential privacy, and
in particular, the randomness complexity of $d$ counting queries. This is the
task of outputting the number of entries in a dataset that satisfy predicates
$\mathcal{P}_1, \dots, \mathcal{P}_d$ respectively. They showed the rather
surprising fact that though any reasonably accurate,
$\varepsilon$-differentially private mechanism for one counting query requires
$1-O(\varepsilon)$ bits of randomness in expectation, there exists a fairly
accurate mechanism for $d$ counting queries which requires only $O(\log d)$
bits of randomness in expectation.
  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a
combinatorial object known as rounding schemes. Here, we give a polynomial time
mechanism which achieves nearly the same randomness complexity versus accuracy
tradeoff as that of [CSV25]. Our construction is based on the following simple
observation: after a randomized shift of the answer to each counting query, the
answer to many counting queries remains the same regardless of whether we add
noise to that coordinate or not. This allows us to forgo the step of adding
noise to the result of many counting queries. Our mechanism does not make use
of rounding schemes. Therefore, it provides a different -- and, in our opinion,
clearer -- insight into the origins of the randomness savings that can be
obtained by batching $d$ counting queries. Therefore, it provides a different
-- and, in our opinion, clearer -- insight into the origins of the randomness
savings that can be obtained by batching $d$ counting queries.

</details>


### [99] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 该论文提出了一个信息论框架来量化大型语言模型(LLMs)在对抗攻击中的信息泄露风险，通过计算可观察信号与目标属性之间的互信息来评估攻击成本，并展示了不同透明度级别对攻击效率的影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs面临恶意用户的对抗攻击威胁，攻击者通过观察模型的响应来推断未知目标属性。然而，信息泄露的程度缺乏系统性评估，导致审计者缺乏原则性指导，防御者难以权衡透明度与风险。

Method: 采用信息论框架，将可观察信号Z与目标属性T之间的互信息I(Z;T)作为每次查询泄露的比特数，推导出达到特定错误率所需的最小查询次数公式。

Result: 实验在7个LLMs上进行，涵盖系统提示泄露、越狱和重新学习攻击：仅暴露答案标记需要约1000次查询；添加对数概率可减少到约100次；揭示完整思维过程仅需几十次查询。

Conclusion: 该研究为部署LLMs时平衡透明度与安全性提供了首个原则性衡量标准，表明适度增加信息泄露会使攻击成本从二次方降至对数级别。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [100] [Watermark Robustness and Radioactivity May Be at Odds in Federated Learning](https://arxiv.org/abs/2510.17033)
*Leixu Huang,Zedian Shao,Teodora Baluta*

Main category: cs.CR

TL;DR: 本文研究联邦学习中LLM生成数据的溯源问题，提出放射性水印方法，但发现主动过滤服务器可以移除水印信号，揭示了放射性、鲁棒性和实用性之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习中越来越多使用LLM生成的数据，需要确保数据来源的可追溯性和透明度，因此需要开发有效的数据溯源方法。

Method: 将LLM水印技术应用于联邦学习，让部分客户端在带有水印的数据上计算本地更新，服务器对所有更新进行平均得到全局LLM。水印具有放射性特征，在微调后仍可检测。

Result: 即使只有6.6%的数据带有水印，p值也能达到10^-24，证明水印信号在微调后仍高度可检测。但主动过滤服务器可以通过强鲁棒聚合过滤掉水印更新，从而移除水印信号。

Conclusion: 所有评估的放射性水印都无法抵抗主动过滤服务器的攻击，表明在放射性、鲁棒性和实用性之间存在基本权衡关系。

Abstract: Federated learning (FL) enables fine-tuning large language models (LLMs)
across distributed data sources. As these sources increasingly include
LLM-generated text, provenance tracking becomes essential for accountability
and transparency. We adapt LLM watermarking for data provenance in FL where a
subset of clients compute local updates on watermarked data, and the server
averages all updates into the global LLM. In this setup, watermarks are
radioactive: the watermark signal remains detectable after fine-tuning with
high confidence. The $p$-value can reach $10^{-24}$ even when as little as
$6.6\%$ of data is watermarked. However, the server can act as an active
adversary that wants to preserve model utility while evading provenance
tracking. Our observation is that updates induced by watermarked synthetic data
appear as outliers relative to non-watermark updates. Our adversary thus
applies strong robust aggregation that can filter these outliers, together with
the watermark signal. All evaluated radioactive watermarks are not robust
against such an active filtering server. Our work suggests fundamental
trade-offs between radioactivity, robustness, and utility.

</details>


### [101] [Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability](https://arxiv.org/abs/2510.17087)
*Ziqing Zhu*

Main category: cs.CR

TL;DR: 本文提出了一种面向虚拟电厂通信的量子密钥感知调度框架，将稀缺的量子密钥作为一级调度资源进行管理，通过配额分配、优先级仲裁和降级机制，在量子密钥供应不确定的情况下保障关键消息的实时传输。


<details>
  <summary>Details</summary>
Motivation: 虚拟电厂需要分钟级和秒级的实时通信，传统PKI和密钥轮换方案难以满足高频跨域消息需求且面临量子计算威胁。量子密钥分发虽然提供信息论安全的新鲜密钥，但其密钥产量稀缺且随机，与虚拟电厂的突发流量不匹配。

Method: 提出密钥感知优先级和配额框架，包括：(1)基于预测的长期配额和短期令牌；(2)密钥感知的赤字轮询仲裁；(3)抢占式应急密钥储备；(4)通过加密模式切换和非关键流量降采样的优雅降级机制。

Result: 在IEEE 33和123总线虚拟电厂系统上的测试表明，相比FIFO、固定优先级和静态配额基线，所提方案显著降低了关键消息的尾部延迟和被动超时，提高了每比特密钥效用，并在密钥稀缺和状态切换时增强了功率跟踪可靠性。

Conclusion: 该框架在平均供需平衡下建立了强稳定性，提供了可量化的积压和尾部延迟边界，为虚拟电厂在量子密钥供应不确定环境下的安全通信提供了可解释的操作保证。

Abstract: Virtual power plants (VPPs) are becoming a cornerstone of future grids,
aggregating distributed PV, wind, storage, and flexible loads for market
participation and real-time balancing. As operations move to minute-- and
second--level feedback, communication security shifts from a compliance item to
an operational constraint: latency, reliability, and confidentiality jointly
determine whether dispatch, protection, and settlement signals arrive on time.
Conventional PKI and key-rotation schemes struggle with cross-domain,
high-frequency messaging and face long-term quantum threats. Quantum key
distribution (QKD) offers information-theoretic key freshness, but its key
yield is scarce and stochastic, often misaligned with bursty VPP traffic. This
paper proposes a key-aware priority and quota framework that treats quantum
keys as first-class scheduling resources. The design combines (i)
forecast-driven long-term quotas and short-term tokens, (ii) key-aware
deficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and
(iv) graceful degradation via encryption-mode switching and controlled
down-sampling for non-critical traffic. A drift-plus-penalty analysis
establishes strong stability under average supply--demand balance with
quantifiable bounds on backlog and tail latency, providing interpretable
operating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus
VPP systems and evaluate normal, degraded, and outage regimes with
industry-consistent message classes and TTLs. Against FIFO, fixed-priority, and
static-quota baselines, the proposed scheme consistently reduces tail delay and
passive timeouts for critical messages, improves per-bit key utility, and
enhances power-tracking reliability during key scarcity and regime switches.

</details>


### [102] [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](https://arxiv.org/abs/2510.17175)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 该论文提出了一种名为QR"iS的新方法，通过QR码的结构分析来识别钓鱼攻击，解决了现有黑盒技术缺乏可解释性和透明度的问题。


<details>
  <summary>Details</summary>
Motivation: 当前QR码钓鱼攻击（Quishing）日益严重，现有防御方法主要依赖黑盒技术，缺乏可解释性和透明度，存在信任度、责任归属和偏见检测等问题。

Method: 开发了一种简单算法，从QR码的布局模式中提取24个结构特征，然后使用机器学习模型进行分类，并开发了移动应用进行实际验证。

Result: 在生成的40万个QR码数据集上训练机器学习模型，获得了高达83.18%的准确率，并通过与相关研究的对比分析验证了方法的有效性。

Conclusion: QR"iS方法通过透明的结构分析实现了对钓鱼QR码的有效分类，具有可重现性、可扩展性和易于理解的特点，在实际部署中验证了其可行性。

Abstract: Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

</details>


### [103] [Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography](https://arxiv.org/abs/2510.17220)
*Giulia Giusti*

Main category: cs.CR

TL;DR: 该论文探讨了线性逻辑在编程范式中的应用，分为ADLL和CryptoBLL两部分。ADLL将线性逻辑应用于自动微分，建模实数上的线性函数和转置操作；CryptoBLL使用线性逻辑表达计算密码学中对手的复杂性约束。


<details>
  <summary>Details</summary>
Motivation: 线性概念在数学和计算机科学中具有不同但互补的含义。数学中的线性支撑函数和向量空间，而计算机科学中的线性涉及资源敏感计算。线性逻辑能够建模必须恰好使用一次的假设，为跟踪计算资源提供自然框架。

Method: 论文采用线性逻辑方法：ADLL部分将线性逻辑应用于自动微分，连接JAX的类型系统与线性逻辑；CryptoBLL部分提出计算密码学中协议自动分析的框架，处理表达性与简单性之间的权衡。

Result: ADLL旨在弥合自动微分中基于证明理论的理论方法与JAX实际实现之间的差距；CryptoBLL为计算密码学中的协议分析提供了处理复杂性和概率抽象的新框架。

Conclusion: 通过线性逻辑的应用，该研究为编程语言、类型系统和形式模型提供了同时表达计算复杂性和可组合性的方法，为分析和验证复杂系统提供了严谨而实用的方法论。

Abstract: The concept of linearity plays a central role in both mathematics and
computer science, with distinct yet complementary meanings. In mathematics,
linearity underpins functions and vector spaces, forming the foundation of
linear algebra and functional analysis. In computer science, it relates to
resource-sensitive computation. Linear Logic (LL), for instance, models
assumptions that must be used exactly once, providing a natural framework for
tracking computational resources such as time, memory, or data access. This
dual perspective makes linearity essential to programming languages, type
systems, and formal models that express both computational complexity and
composability. Bridging these interpretations enables rigorous yet practical
methodologies for analyzing and verifying complex systems.
  This thesis explores the use of LL to model programming paradigms based on
linearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to
Automatic Differentiation (AD), modeling linear functions over the reals and
the transposition operation. The latter uses LL to express complexity
constraints on adversaries in computational cryptography.
  In AD, two main approaches use linear type systems: a theoretical one
grounded in proof theory, and a practical one implemented in JAX, a Python
library developed by Google for machine learning research. In contrast,
frameworks like PyTorch and TensorFlow support AD without linear types. ADLL
aims to bridge theory and practice by connecting JAX's type system to LL.
  In modern cryptography, several calculi aim to model cryptographic proofs
within the computational paradigm. These efforts face a trade-off between
expressiveness, to capture reductions, and simplicity, to abstract probability
and complexity. CryptoBLL addresses this tension by proposing a framework for
the automatic analysis of protocols in computational cryptography.

</details>


### [104] [Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](https://arxiv.org/abs/2510.17277)
*Xinkai Wang,Beibei Li,Zerui Shao,Ao Liu,Shouling Ji*

Main category: cs.CR

TL;DR: 本文提出PolyJailbreak方法，利用多模态安全不对称性，通过强化学习自动生成针对多模态大语言模型的越狱攻击，在多种开源和闭源模型上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在现实应用中具有重要价值，但它们容易受到越狱攻击，导致安全约束失效并产生不道德响应。本文旨在研究文本-视觉多模态设置中的越狱问题。

Method: 开发基于强化学习的黑盒越狱方法PolyJailbreak：首先探测模型的注意力动态和潜在表示空间，评估视觉输入如何重塑跨模态信息流；然后系统化为可泛化和可重用的操作规则，构建原子策略原语库；最后采用多智能体优化过程自动适应目标模型的输入。

Result: 在多种开源和闭源多模态大语言模型上进行了全面评估，证明PolyJailbreak优于最先进的基线方法。

Conclusion: 视觉对齐在多模态中施加了不均匀的安全约束，导致多模态安全不对称性，这为越狱攻击提供了可被利用的漏洞。PolyJailbreak方法有效利用了这种不对称性，实现了高效的模型越狱。

Abstract: Multimodal large language models (MLLMs) have demonstrated significant
utility across diverse real-world applications. But MLLMs remain vulnerable to
jailbreaks, where adversarial inputs can collapse their safety constraints and
trigger unethical responses. In this work, we investigate jailbreaks in the
text-vision multimodal setting and pioneer the observation that visual
alignment imposes uneven safety constraints across modalities in MLLMs, thereby
giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a
black-box jailbreak method grounded in reinforcement learning. Initially, we
probe the model's attention dynamics and latent representation space, assessing
how visual inputs reshape cross-modal information flow and diminish the model's
ability to separate harmful from benign inputs, thereby exposing exploitable
vulnerabilities. On this basis, we systematize them into generalizable and
reusable operational rules that constitute a structured library of Atomic
Strategy Primitives, which translate harmful intents into jailbreak inputs
through step-wise transformations. Guided by the primitives, PolyJailbreak
employs a multi-agent optimization process that automatically adapts inputs
against the target models. We conduct comprehensive evaluations on a variety of
open-source and closed-source MLLMs, demonstrating that PolyJailbreak
outperforms state-of-the-art baselines.

</details>


### [105] [Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values](https://arxiv.org/abs/2510.17284)
*Jiri Gavenda,Petr Svenda,Stanislav Bobon,Vladimir Sedlacek*

Main category: cs.CR

TL;DR: 该论文分析了比特币CoinJoin协议的隐私保护效果，发现主要CoinJoin设计（Whirlpool、Wasabi 1.x和2.x）的平均匿名集大小在混币后下降10-50%，其中第一天下降最明显，一年后可忽略不计。作者开发了精确的隐私评估方法，证明尽管用户混币后行为不理想，但准确追踪资金归属仍然非常困难。


<details>
  <summary>Details</summary>
Motivation: CoinJoin协议旨在通过协同交易增强比特币隐私，但评估其实际隐私增益是一个尚未解决的复杂问题，需要考虑多种影响因素和计算复杂性。

Method: 作者改进了BlockSci链上分析软件以分析CoinJoin交易，并设计了一种精确、可并行的隐私评估方法，考虑了混币费用、实现限制和用户混币后行为。

Result: 研究发现三种主要CoinJoin设计的平均匿名集大小在混币后下降10-50%，第一天下降最明显，一年后可忽略不计。即使考虑用户不理想的混币后行为，准确追踪资金归属仍然非常困难。

Conclusion: 尽管CoinJoin协议在混币后存在匿名集大小下降，特别是初期下降明显，但通过改进的分析方法证明，准确追踪混币资金的所有者仍然极具挑战性，CoinJoin协议仍能提供有效的隐私保护。

Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and
Bitcoin-like blockchains via collaborative transactions, by violating
assumptions behind common analysis heuristics. Estimating the resulting privacy
gain is a crucial yet unsolved problem due to a range of influencing factors
and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions,
demonstrating a significant (10-50%) average post-mix anonymity set size
decrease for all three major designs with a central coordinator: Whirlpool,
Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and
negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method,
which takes into account coinjoin fees, implementation-specific limitations and
users' post-mix behavior. We evaluate our method in detail on a set of emulated
and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world
coinjoins with hundreds of inputs and outputs. We conclude that despite the
users' undesirable post-mix behavior, correctly attributing the coins to their
owners is still very difficult, even with our improved analysis algorithm.

</details>


### [106] [Single-Shuffle Full-Open Card-Based Protocols for Any Function](https://arxiv.org/abs/2510.17308)
*Reo Eriguchi,Kazumasa Shinagawa*

Main category: cs.CR

TL;DR: 本文首次证明了所有函数都存在单次洗牌全公开的卡牌安全计算协议，提出了两种在卡牌数量和洗牌复杂度之间权衡的构造方法。


<details>
  <summary>Details</summary>
Motivation: 单次洗牌全公开协议是卡牌安全计算的最小模型，但之前已知的可计算函数类别仅限于少数小例子。本文旨在突破这一限制，证明所有函数都可在该模型下计算。

Method: 通过建立单次洗牌全公开协议与密码学原语"私有同时消息"协议之间的新联系，提出了两种构造方法：一种优化卡牌数量，另一种优化洗牌复杂度。

Result: 成功证明了所有函数都存在单次洗牌全公开协议，并提出了比现有协议洗牌复杂度更低的变体协议。

Conclusion: 这项工作扩展了卡牌安全计算的能力边界，为最小模型下的通用安全计算提供了理论基础和实用构造。

Abstract: A card-based secure computation protocol is a method for $n$ parties to
compute a function $f$ on their private inputs $(x_1,\ldots,x_n)$ using
physical playing cards, in such a way that the suits of revealed cards leak no
information beyond the value of $f(x_1,\ldots,x_n)$. A \textit{single-shuffle
full-open} protocol is a minimal model of card-based secure computation in
which, after the parties place face-down cards representing their inputs, a
single shuffle operation is performed and then all cards are opened to derive
the output. Despite the simplicity of this model, the class of functions known
to admit single-shuffle full-open protocols has been limited to a few small
examples. In this work, we prove for the first time that every function admits
a single-shuffle full-open protocol. We present two constructions that offer a
trade-off between the number of cards and the complexity of the shuffle
operation. These feasibility results are derived from a novel connection
between single-shuffle full-open protocols and a cryptographic primitive known
as \textit{Private Simultaneous Messages} protocols, which has rarely been
studied in the context of card-based cryptography. We also present variants of
single-shuffle protocols in which only a subset of cards are revealed. These
protocols reduce the complexity of the shuffle operation compared to existing
protocols in the same setting.

</details>


### [107] [The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment](https://arxiv.org/abs/2510.17311)
*Eduard Marin,Jinwoo Kim,Alessio Pavoni,Mauro Conti,Roberto Di Pietro*

Main category: cs.CR

TL;DR: 对5个公共无服务器仓库中2,758个组件和3个IaC框架中125,936个模板的安全分析，揭示了系统性漏洞，包括过时软件包、敏感参数误用、可被利用的部署配置等安全风险。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算已成为主流云范式，公共无服务器仓库在加速应用开发的同时也成为攻击者的目标，但这些仓库的安全状况尚未得到充分研究，使开发者和组织面临潜在风险。

Method: 分析来自5个广泛使用的公共无服务器仓库的2,758个无服务器组件，以及跨3个广泛使用的IaC框架的125,936个基础设施即代码模板。

Result: 发现系统性漏洞包括：过时软件包、敏感参数误用、可被利用的部署配置、易受域名抢注攻击、在压缩无服务器组件中嵌入恶意行为的机会。

Conclusion: 提供了实用的建议来缓解这些威胁，强调需要加强公共无服务器仓库的安全防护措施。

Abstract: Serverless computing has rapidly emerged as a prominent cloud paradigm,
enabling developers to focus solely on application logic without the burden of
managing servers or underlying infrastructure. Public serverless repositories
have become key to accelerating the development of serverless applications.
However, their growing popularity makes them attractive targets for
adversaries. Despite this, the security posture of these repositories remains
largely unexplored, exposing developers and organizations to potential risks.
In this paper, we present the first comprehensive analysis of the security
landscape of serverless components hosted in public repositories. We analyse
2,758 serverless components from five widely used public repositories popular
among developers and enterprises, and 125,936 Infrastructure as Code (IaC)
templates across three widely used IaC frameworks. Our analysis reveals
systemic vulnerabilities including outdated software packages, misuse of
sensitive parameters, exploitable deployment configurations, susceptibility to
typo-squatting attacks and opportunities to embed malicious behaviour within
compressed serverless components. Finally, we provide practical recommendations
to mitigate these threats.

</details>


### [108] [GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](https://arxiv.org/abs/2510.17621)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: GUIDE是一种利用扩散模型作为去噪工具来增强联邦学习中图像重建攻击效果的新方法，可集成到任何使用替代数据集的梯度反转攻击中，显著提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保护了原始数据隐私，但客户端更新的梯度信息仍易受隐私泄露攻击。现有的梯度反转攻击通常只能重建带噪声的输入近似值，需要专门的去噪技术来提升重建质量。

Method: 提出GUIDE方法，将扩散模型作为去噪工具集成到梯度反转攻击中。该方法适用于任何使用替代数据集的梯度反转攻击，通过扩散模型的去噪能力提升图像重建质量。

Result: 在两个使用不同联邦学习算法、模型和数据集的攻击场景中，GUIDE与两种最先进的梯度反转攻击无缝集成，在多个指标上显著提升了重建质量，DreamSim感知相似度指标最高提升46%。

Conclusion: GUIDE方法有效提升了联邦学习环境中梯度反转攻击的图像重建质量，扩散模型作为去噪工具在隐私攻击中展现出强大潜力。

Abstract: Federated Learning (FL) enables collaborative training of Machine Learning
(ML) models across multiple clients while preserving their privacy. Rather than
sharing raw data, federated clients transmit locally computed updates to train
the global model. Although this paradigm should provide stronger privacy
guarantees than centralized ML, client updates remain vulnerable to privacy
leakage. Adversaries can exploit them to infer sensitive properties about the
training data or even to reconstruct the original inputs via Gradient Inversion
Attacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to
reconstruct training data by reversing intermediate updates using
optimizationbased techniques. We observe that these approaches usually
reconstruct noisy approximations of the original inputs, whose quality can be
enhanced with specialized denoising models. This paper presents Gradient Update
Inversion with DEnoising (GUIDE), a novel methodology that leverages diffusion
models as denoising tools to improve image reconstruction attacks in FL. GUIDE
can be integrated into any GIAs that exploits surrogate datasets, a widely
adopted assumption in GIAs literature. We comprehensively evaluate our approach
in two attack scenarios that use different FL algorithms, models, and datasets.
Our results demonstrate that GUIDE integrates seamlessly with two state-ofthe-
art GIAs, substantially improving reconstruction quality across multiple
metrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,
as measured by the DreamSim metric.

</details>


### [109] [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)
*Xu Zhang,Hao Li,Zhichao Lu*

Main category: cs.CR

TL;DR: 本文提出了ImpForge自动化红队管道生成多样化隐式攻击样本，并基于此开发了CrossGuard意图感知保护系统，有效防御显式和隐式多模态威胁。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在推理和感知方面表现强大，但越来越容易受到越狱攻击。现有研究主要关注显式攻击，而隐式攻击（良性文本和图像输入共同表达不安全意图）难以检测且研究不足，主要由于高质量隐式数据稀缺。

Method: 提出ImpForge自动化红队管道，利用强化学习和定制奖励模块在14个领域生成多样化隐式样本；基于此数据集开发CrossGuard意图感知保护系统。

Result: 在安全和不安全基准测试、隐式和显式攻击以及多个域外设置上的广泛实验表明，CrossGuard显著优于现有防御方法，包括先进的多模态大语言模型和防护机制，在保持高实用性的同时实现更强的安全性。

Conclusion: 这为增强多模态大语言模型对抗现实世界多模态威胁的鲁棒性提供了一个平衡且实用的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong reasoning and
perception capabilities but are increasingly vulnerable to jailbreak attacks.
While existing work focuses on explicit attacks, where malicious content
resides in a single modality, recent studies reveal implicit attacks, in which
benign text and image inputs jointly express unsafe intent. Such joint-modal
threats are difficult to detect and remain underexplored, largely due to the
scarcity of high-quality implicit data. We propose ImpForge, an automated
red-teaming pipeline that leverages reinforcement learning with tailored reward
modules to generate diverse implicit samples across 14 domains. Building on
this dataset, we further develop CrossGuard, an intent-aware safeguard
providing robust and comprehensive defense against both explicit and implicit
threats. Extensive experiments across safe and unsafe benchmarks, implicit and
explicit attacks, and multiple out-of-domain settings demonstrate that
CrossGuard significantly outperforms existing defenses, including advanced
MLLMs and guardrails, achieving stronger security while maintaining high
utility. This offers a balanced and practical solution for enhancing MLLM
robustness against real-world multimodal threats.

</details>


### [110] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一个变分推理框架，将多模态越狱发现重新定义为学习配对文本-图像提示的联合后验分布，通过生成隐蔽的对抗性输入来绕过模型防护机制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态红队方法依赖脆弱模板、专注于单攻击设置，仅暴露有限漏洞子集，需要更全面的漏洞发现方法。

Method: 使用变分推理框架学习文本-图像提示的联合后验分布，训练轻量级攻击器近似后验，结合排版文本提示、基于扩散的图像合成和结构化干扰三种互补策略。

Result: 在HarmBench和HADES基准测试中，VERA-V在开源和前沿VLM上始终优于最先进基线，在GPT-4o上攻击成功率比最佳基线高出53.75%。

Conclusion: VERA-V提供了一种有效的多模态越狱发现方法，能够生成多样化越狱并揭示漏洞分布特征。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>
