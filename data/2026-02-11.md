<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.DC](#cs.DC) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [PICASSO: Scaling CHERI Use-After-Free Protection to Millions of Allocations using Colored Capabilities](https://arxiv.org/abs/2602.09131)
*Merve Gülmez,Ruben Sturm,Hossam ElAtali,Håkan Englund,Jonathan Woodruff,N. Asokan,Thomas Nyman*

Main category: cs.CR

TL;DR: PICASSO通过引入彩色能力（colored capabilities）扩展CHERI架构，为堆分配提供时间安全保护，有效缓解use-after-free和double-free漏洞，性能开销小


<details>
  <summary>Details</summary>
Motivation: CHERI指令集架构扩展虽然提供了强大的空间内存安全，但缺乏时间安全保护，特别是堆分配。先前增强CHERI时间安全的方法在可扩展性、内存开销和安全性方面存在不足，需要定期扫描内存来撤销过时能力

Method: 引入彩色能力，为CHERI能力模型添加受控的间接层，通过硬件管理的能力来源有效性表跟踪能力到各自分配的来源，实现批量撤销悬空指针而无需隔离已释放内存

Result: 在PICASSO（CHERI-RISC-V扩展）上实现，集成到CheriBSD OS和CHERI-enabled Clang/LLVM工具链。有效缓解NIST Juliet测试用例中所有基于堆的时间内存安全漏洞，SPEC CPU基准测试仅5%几何平均性能开销，SQLite、PostgreSQL和gRPC工作负载延迟更低、性能更一致

Conclusion: 彩色能力显著减少能力撤销扫描频率同时提高安全性，为CHERI架构提供了有效的时间安全解决方案

Abstract: While the CHERI instruction-set architecture extensions for capabilities enable strong spatial memory safety, CHERI lacks built-in temporal safety, particularly for heap allocations. Prior attempts to augment CHERI with temporal safety fall short in terms of scalability, memory overhead, and incomplete security guarantees due to periodical sweeps of the system's memory to individually revoke stale capabilities. We address these limitations by introducing colored capabilities that add a controlled form of indirection to CHERI's capability model. This enables provenance tracking of capabilities to their respective allocations via a hardware-managed provenance-validity table, allowing bulk retraction of dangling pointers without needing to quarantine freed memory. Colored capabilities significantly reduce the frequency of capability revocation sweeps while improving security. We realize colored capabilities in PICASSO, an extension of the CHERI-RISC-V architecture on a speculative out-of-order FPGA softcore (CHERI-Toooba). We also integrate colored-capability support into the CheriBSD OS and CHERI-enabled Clang/LLVM toolchain. Our evaluation shows effective mitigation of use-after-free and double-free bugs across all heap-based temporal memory-safety vulnerabilities in NIST Juliet test cases, with only a small performance overhead on SPEC CPU benchmarks (5% g.m.), less latency, and more consistent performance in long-running SQLite, PostgreSQL, and gRPC workloads compared to prior work.

</details>


### [2] [One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning](https://arxiv.org/abs/2602.09182)
*Kotekar Annapoorna Prabhu,Andrew Gan,Zahra Ghodsi*

Main category: cs.CR

TL;DR: RNGGuard：一个保护机器学习系统随机数生成器安全的工具，通过静态分析和运行时替换来防止针对随机性源的攻击


<details>
  <summary>Details</summary>
Motivation: 机器学习严重依赖随机性（数据采样、数据增强、权重初始化、优化等），但不同框架、软件依赖和硬件后端的伪随机数生成器实现差异以及缺乏统计验证，可能成为攻击向量。针对随机性源的攻击具有隐蔽性，在现实系统中已有利用历史。

Method: 提出RNGGuard工具：1）静态分析目标库源代码，识别随机函数及其使用模块；2）运行时通过替换不安全的函数调用为符合安全规范的RNGGuard实现，强制执行随机函数的安全执行。

Result: 评估显示RNGGuard提供了一种实用的方法来填补机器学习系统中随机性源安全保护的现有空白。

Conclusion: RNGGuard能够以较低成本帮助机器学习工程师保护其系统安全，有效防范针对随机数生成器的攻击。

Abstract: Machine learning relies on randomness as a fundamental component in various steps such as data sampling, data augmentation, weight initialization, and optimization. Most machine learning frameworks use pseudorandom number generators as the source of randomness. However, variations in design choices and implementations across different frameworks, software dependencies, and hardware backends along with the lack of statistical validation can lead to previously unexplored attack vectors on machine learning systems. Such attacks on randomness sources can be extremely covert, and have a history of exploitation in real-world systems. In this work, we examine the role of randomness in the machine learning development pipeline from an adversarial point of view, and analyze the implementations of PRNGs in major machine learning frameworks. We present RNGGuard to help machine learning engineers secure their systems with low effort. RNGGuard statically analyzes a target library's source code and identifies instances of random functions and modules that use them. At runtime, RNGGuard enforces secure execution of random functions by replacing insecure function calls with RNGGuard's implementations that meet security specifications. Our evaluations show that RNGGuard presents a practical approach to close existing gaps in securing randomness sources in machine learning systems.

</details>


### [3] [Atlas: Enabling Cross-Vendor Authentication for IoT](https://arxiv.org/abs/2602.09263)
*Sanket Goutam,Omar Chowdhury,Amir Rahmati*

Main category: cs.CR

TL;DR: Atlas框架通过将Web公钥基础设施扩展到物联网，为设备颁发X.509证书，使设备能够跨管理域直接建立相互TLS通道，减少云依赖和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前云中介的物联网架构存在认证碎片化、跨供应商设备间交互的延迟和可用性瓶颈问题，需要一种去中心化的认证解决方案。

Method: Atlas扩展Web公钥基础设施到物联网，通过供应商操作的ACME客户端和DNS命名空间为设备颁发X.509证书，使设备获得全局可验证身份，并建立跨域的相互TLS通道。

Result: 证书配置每设备在6秒内完成，mTLS仅增加约17ms延迟和适度CPU开销，基于Atlas的应用相比云中介基准线保持低且可预测的延迟。

Conclusion: Atlas提供了一种可立即部署的解决方案，无需硬件更改，最小化基础设施变更，能够显著改善物联网设备间的跨域认证和通信效率。

Abstract: Cloud-mediated IoT architectures fragment authentication across vendor silos and create latency and availability bottlenecks for cross-vendor device-to-device (D2D) interactions. We present Atlas, a framework that extends the Web public-key infrastructure to IoT by issuing X.509 certificates to devices via vendor-operated ACME clients and vendor-controlled DNS namespaces. Devices obtain globally verifiable identities without hardware changes and establish mutual TLS channels directly across administrative domains, decoupling runtime authentication from cloud reachability. We prototype Atlas on ESP32 and Raspberry Pi, integrate it with an MQTT-based IoT stack and an Atlas-aware cloud, and evaluate it in smart-home and smart-city workloads. Certificate provisioning completes in under 6s per device, mTLS adds only about 17ms of latency and modest CPU overhead, and Atlas-based applications sustain low, predictable latency compared to cloud-mediated baselines. Because many major vendors already rely on ACME-compatible CAs for their web services, Atlas is immediately deployable with minimal infrastructure changes.

</details>


### [4] [Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation](https://arxiv.org/abs/2602.09319)
*Zhisheng Qi,Utkarsh Sahu,Li Ma,Haoyu Han,Ryan Rossi,Franck Dernoncourt,Mahantesh Halappanavar,Nesreen Ahmed,Yushun Dong,Yue Zhao,Yu Zhang,Yu Wang*

Main category: cs.CR

TL;DR: 该论文提出了首个针对RAG系统知识提取攻击的系统性基准测试，旨在解决现有研究碎片化问题，为开发隐私保护的RAG系统提供统一评估框架。


<details>
  <summary>Details</summary>
Motivation: RAG系统在知识密集型应用中广泛应用，但研究表明恶意查询可以提取敏感知识库内容，引发知识产权和隐私泄露担忧。现有攻击和防御研究分散在不同检索嵌入、生成模型和评估标准中，缺乏系统性比较。

Method: 构建了首个系统性基准测试框架，涵盖广泛的攻击和防御策略、代表性检索嵌入模型、开源和闭源生成器，在统一实验框架下使用标准化协议和多个数据集进行评估。

Result: 通过整合实验环境并实现可重复、可比较的评估，该基准测试为应对知识提取威胁提供了实用基础，能够生成可操作的见解。

Conclusion: 该基准测试填补了RAG系统安全评估的空白，为开发隐私保护的RAG系统提供了系统性的评估框架，有助于应对新兴的知识提取威胁。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.

</details>


### [5] [Privacy Amplification for BandMF via $b$-Min-Sep Subsampling](https://arxiv.org/abs/2602.09338)
*Andy Dong,Arun Ganesh*

Main category: cs.CR

TL;DR: 该论文研究了BandMF（使用带相关噪声的DP-SGD）的隐私放大问题，提出了一种新的b-min-sep子采样方案，该方案在保持分析所需结构特性的同时，比循环泊松采样提供更强的隐私放大效果。


<details>
  <summary>Details</summary>
Motivation: 研究BandMF（具有跨迭代相关噪声的DP-SGD）的隐私放大问题，现有子采样方案在隐私放大效果上存在局限，需要一种既能保持分析所需结构特性又能提供更强隐私保证的新方案。

Method: 提出了b-min-sep子采样方案，该方案推广了泊松采样和balls-in-bins采样，扩展了BandMF的实用批处理策略。使用基于动态规划的蒙特卡洛计算方法进行隐私分析，利用子采样过程中的马尔可夫结构。

Result: b-min-sep在高噪声区域与循环泊松采样效果相当，在中低噪声区域提供严格更好的隐私保证。实验结果表明该方案优于现有方法，并能自然扩展到多归属用户级隐私设置。

Conclusion: b-min-sep子采样方案为BandMF提供了一种有效的隐私放大方法，在保持分析可行性的同时显著提升了隐私保护水平，特别是在中低噪声场景下，且具有扩展到更复杂隐私设置的能力。

Abstract: We study privacy amplification for BandMF, i.e., DP-SGD with correlated noise across iterations via a banded correlation matrix. We propose $b$-min-sep subsampling, a new subsampling scheme that generalizes Poisson and balls-in-bins subsampling, extends prior practical batching strategies for BandMF, and enables stronger privacy amplification than cyclic Poisson while preserving the structural properties needed for analysis. We give a near-exact privacy analysis using Monte Carlo accounting, based on a dynamic program that leverages the Markovian structure in the subsampling procedure. We show that $b$-min-sep matches cyclic Poisson subsampling in the high noise regime and achieves strictly better guarantees in the mid-to-low noise regime, with experimental results that bolster our claims. We further show that unlike previous BandMF subsampling schemes, our $b$-min-sep subsampling naturally extends to the multi-attribution user-level privacy setting.

</details>


### [6] [Timing and Memory Telemetry on GPUs for AI Governance](https://arxiv.org/abs/2602.09369)
*Saleh K. Monfared,Fatemeh Ganji,Dan Holcomb,Shahin Tajik*

Main category: cs.CR

TL;DR: 提出基于GPU架构特性的计算测量框架，通过四种互补原语生成与计算活动相关的时序和内存可观测信号，为部署后GPU治理提供可操作的利用信号。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速计算的快速发展，大规模AI应用日益增多，但当前GPU缺乏可信遥测能力，且可能被对手修改或虚拟化，导致部署后难以监控GPU是否被用于训练模型、规避使用政策或超出法律监管范围。

Method: 设计了一个测量框架，利用现代GPU的架构特性生成四种互补原语：1) 受工作量证明启发的概率性、工作负载驱动机制来暴露并行计算；2) 基于可验证延迟函数的顺序、延迟敏感工作负载来表征标量执行压力；3) 基于通用矩阵乘法的张量核心测量来反映密集线性代数吞吐量；4) VRAM驻留测试通过带宽依赖哈希区分设备内存局部性和片外访问。

Result: 评估了这些原语在竞争、架构对齐、内存压力和功耗开销下的响应，显示时序偏移和驻留延迟能够揭示有意义的利用模式。结果表明基于计算的遥测可以补充未来的问责机制。

Conclusion: 基于计算的遥测能够暴露与部署后GPU治理相关的架构信号，为GPU使用监控提供可操作信号，即使在没有可信固件、安全区或厂商控制计数器的情况下也能保持可观测性。

Abstract: The rapid expansion of GPU-accelerated computing has enabled major advances in large-scale artificial intelligence (AI), while heightening concerns about how accelerators are observed or governed once deployed. Governance is essential to ensure that large-scale compute infrastructure is not silently repurposed for training models, circumventing usage policies, or operating outside legal oversight. Because current GPUs expose limited trusted telemetry and can be modified or virtualized by adversaries, we explore whether compute-based measurements can provide actionable signals of utilization when host and device are untrusted. We introduce a measurement framework that leverages architectural characteristics of modern GPUs to generate timing- and memory-based observables that correlate with compute activity. Our design draws on four complementary primitives: (1) a probabilistic, workload-driven mechanism inspired by Proof-of-Work (PoW) to expose parallel effort, (2) sequential, latency-sensitive workloads derived via Verifiable Delay Functions (VDFs) to characterize scalar execution pressure, (3) General Matrix Multiplication (GEMM)-based tensor-core measurements that reflect dense linear-algebra throughput, and (4) a VRAM-residency test that distinguishes on-device memory locality from off-chip access through bandwidth-dependent hashing. These primitives provide statistical and behavioral indicators of GPU engagement that remain observable even without trusted firmware, enclaves, or vendor-controlled counters. We evaluate their responses to contention, architectural alignment, memory pressure, and power overhead, showing that timing shifts and residency latencies reveal meaningful utilization patterns. Our results illustrate why compute-based telemetry can complement future accountability mechanisms by exposing architectural signals relevant to post-deployment GPU governance.

</details>


### [7] [LLMAC: A Global and Explainable Access Control Framework with Large Language Model](https://arxiv.org/abs/2602.09392)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: 提出LLMAC，一种基于大语言模型的新型访问控制统一框架，将RBAC、ABAC、DAC等多种传统方法整合，在复杂动态场景下实现98.5%的高准确率，远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统访问控制方法（如RBAC、ABAC、DAC）各自为特定目的设计，无法有效处理现代系统所需的动态、情境依赖的工作流程。企业需要能够处理复杂、变化安全需求的访问控制系统。

Method: 提出LLMAC框架，利用大语言模型（LLM）统一整合多种访问控制方法。使用包含所有权验证、版本管理、工作流程、动态角色分离等复杂场景的合成数据集，基于Mistral 7B模型进行训练。

Result: 训练后的LLM模型达到98.5%的准确率，显著优于传统方法（RBAC: 14.5%，ABAC: 58.5%，DAC: 27.5%）。系统提供清晰的人类可读决策解释，性能测试表明系统具有实际部署可行性，响应时间和计算资源合理。

Conclusion: LLMAC提供了一种全面、可理解的访问控制统一方法，能够有效处理复杂动态的安全需求，为现代企业系统提供了优于传统方法的解决方案。

Abstract: Today's business organizations need access control systems that can handle complex, changing security requirements that go beyond what traditional methods can manage. Current approaches, such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), and Discretionary Access Control (DAC), were designed for specific purposes. They cannot effectively manage the dynamic, situation-dependent workflows that modern systems require. In this research, we introduce LLMAC, a new unified approach using Large Language Models (LLMs) to combine these different access control methods into one comprehensive, understandable system. We used an extensive synthetic dataset that represents complex real-world scenarios, including policies for ownership verification, version management, workflow processes, and dynamic role separation. Using Mistral 7B, our trained LLM model achieved outstanding results with 98.5% accuracy, significantly outperforming traditional methods (RBAC: 14.5%, ABAC: 58.5%, DAC: 27.5%) while providing clear, human readable explanations for each decision. Performance testing shows that the system can be practically deployed with reasonable response times and computing resources.

</details>


### [8] [Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models](https://arxiv.org/abs/2602.09431)
*Xinwei Zhang,Li Bai,Tianwei Zhang,Youqian Zhang,Qingqing Ye,Yingnan Zhao,Ruochen Du,Haibo Hu*

Main category: cs.CR

TL;DR: 该研究首次系统性地探索了大型视觉语言模型中基于编码器的对抗样本可迁移性，发现现有攻击方法可迁移性有限，并提出了语义引导的多模态攻击框架SGMA来提升可迁移性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现出色，但其对视觉输入的依赖使其面临对抗性威胁。现有基于编码器的攻击方法虽然计算高效，但在实际黑盒场景中跨不同LVLM架构的可迁移性尚未得到充分理解。

Method: 研究首先在8个不同的LVLM上进行大规模基准测试，分析现有攻击的可迁移性限制。然后深入分析发现两个根本原因：1) 模型间视觉定位不一致；2) 模型内语义对齐冗余。基于此提出语义引导的多模态攻击框架SGMA，将扰动引导至语义关键区域，并在全局和局部层面破坏跨模态定位。

Result: 实验表明，现有攻击方法表现出严重受限的可迁移性。SGMA在不同受害模型和任务上均实现了比现有攻击更高的可迁移性，暴露了LVLM部署中的关键安全风险。

Conclusion: 该研究首次系统性地揭示了LVLM中基于编码器对抗攻击的可迁移性问题，提出的SGMA框架显著提升了攻击可迁移性，强调了开发鲁棒多模态防御的紧迫性。

Abstract: Large vision-language models (LVLMs) have achieved impressive success across multimodal tasks, but their reliance on visual inputs exposes them to significant adversarial threats. Existing encoder-based attacks perturb the input image by optimizing solely on the vision encoder, rather than the entire LVLM, offering a computationally efficient alternative to end-to-end optimization. However, their transferability across different LVLM architectures in realistic black-box scenarios remains poorly understood. To address this gap, we present the first systematic study towards encoder-based adversarial transferability in LVLMs. Our contributions are threefold. First, through large-scale benchmarking over eight diverse LVLMs, we reveal that existing attacks exhibit severely limited transferability. Second, we perform in-depth analysis, disclosing two root causes that hinder the transferability: (1) inconsistent visual grounding across models, where different models focus their attention on distinct regions; (2) redundant semantic alignment within models, where a single object is dispersed across multiple overlapping token representations. Third, we propose Semantic-Guided Multimodal Attack (SGMA), a novel framework to enhance the transferability. Inspired by the discovered causes in our analysis, SGMA directs perturbations toward semantically critical regions and disrupts cross-modal grounding at both global and local levels. Extensive experiments across different victim models and tasks show that SGMA achieves higher transferability than existing attacks. These results expose critical security risks in LVLM deployment and underscore the urgent need for robust multimodal defenses.

</details>


### [9] [A Behavioral Fingerprint for Large Language Models: Provenance Tracking via Refusal Vectors](https://arxiv.org/abs/2602.09434)
*Zhenyu Xu,Victor S. Sheng*

Main category: cs.CR

TL;DR: 提出基于安全对齐行为模式的LLM指纹框架，利用拒绝向量进行模型溯源，在76个衍生模型中实现100%准确识别，并设计隐私保护的公验证方案


<details>
  <summary>Details</summary>
Motivation: 保护大型语言模型的知识产权面临挑战，因为存在大量未经授权的衍生模型。需要一种能够有效追踪模型来源的指纹技术来保护模型开发者的权益。

Method: 提出基于安全对齐行为模式的指纹框架，利用拒绝向量（处理有害与无害提示时内部表征的方向性模式）作为行为指纹。通过提取模型在处理不同类型提示时的内部表征方向模式来构建独特的指纹特征。

Result: 行为指纹对常见修改（微调、合并、量化）具有高度鲁棒性；不同模型家族间指纹差异显著（余弦相似度低）；在76个衍生模型识别任务中达到100%准确率；即使在对抗攻击下仍保留可检测痕迹。

Conclusion: 基于安全对齐行为模式的拒绝向量指纹是有效的LLM知识产权保护方案，具有高鲁棒性和准确性。通过局部敏感哈希和零知识证明可将其转化为隐私保护的公开可验证方案。

Abstract: Protecting the intellectual property of large language models (LLMs) is a critical challenge due to the proliferation of unauthorized derivative models. We introduce a novel fingerprinting framework that leverages the behavioral patterns induced by safety alignment, applying the concept of refusal vectors for LLM provenance tracking. These vectors, extracted from directional patterns in a model's internal representations when processing harmful versus harmless prompts, serve as robust behavioral fingerprints. Our contribution lies in developing a fingerprinting system around this concept and conducting extensive validation of its effectiveness for IP protection. We demonstrate that these behavioral fingerprints are highly robust against common modifications, including finetunes, merges, and quantization. Our experiments show that the fingerprint is unique to each model family, with low cosine similarity between independently trained models. In a large-scale identification task across 76 offspring models, our method achieves 100\% accuracy in identifying the correct base model family. Furthermore, we analyze the fingerprint's behavior under alignment-breaking attacks, finding that while performance degrades significantly, detectable traces remain. Finally, we propose a theoretical framework to transform this private fingerprint into a publicly verifiable, privacy-preserving artifact using locality-sensitive hashing and zero-knowledge proofs.

</details>


### [10] [ReSIM: Re-ranking Binary Similarity Embeddings to Improve Function Search Performance](https://arxiv.org/abs/2602.09548)
*Gianluca Capozzi,Anna Paola Giancaspro,Fabio Petroni,Leonardo Querzoni,Giuseppe Antonio Di Luna*

Main category: cs.CR

TL;DR: ReSIM是一个增强的二进制函数相似性搜索系统，通过神经重排序器补充嵌入搜索，显著提升搜索效果


<details>
  <summary>Details</summary>
Motivation: 现有二进制函数相似性系统使用双编码器架构独立嵌入函数，无法捕捉跨函数关系和相似性，限制了搜索准确性

Method: 引入ReSIM系统，在嵌入搜索基础上增加神经重排序模块，联合处理查询-候选对，基于互表示计算排序分数

Result: 在7个嵌入模型和2个基准数据集上评估，平均提升21.7%的nDCG和27.8%的Recall

Conclusion: ReSIM通过重排序机制有效捕捉双编码器无法捕获的细粒度关系信息，显著提升二进制函数相似性搜索效果

Abstract: Binary Function Similarity (BFS), the problem of determining whether two binary functions originate from the same source code, has been extensively studied in recent research across security, software engineering, and machine learning communities. This interest arises from its central role in developing vulnerability detection systems, copyright infringement analysis, and malware phylogeny tools. Nearly all binary function similarity systems embed assembly functions into real-valued vectors, where similar functions map to points that lie close to each other in the metric space. These embeddings enable function search: a query function is embedded and compared against a database of candidate embeddings to retrieve the most similar matches.
  Despite their effectiveness, such systems rely on bi-encoder architectures that embed functions independently, limiting their ability to capture cross-function relationships and similarities. To address this limitation, we introduce ReSIM, a novel and enhanced function search system that complements embedding-based search with a neural re-ranker. Unlike traditional embedding models, our reranking module jointly processes query-candidate pairs to compute ranking scores based on their mutual representation, allowing for more accurate similarity assessment. By re-ranking the top results from embedding-based retrieval, ReSIM leverages fine-grained relation information that bi-encoders cannot capture.
  We evaluate ReSIM across seven embedding models on two benchmark datasets, demonstrating consistent improvements in search effectiveness, with average gains of 21.7% in terms of nDCG and 27.8% in terms of Recall.

</details>


### [11] [Parallel Composition for Statistical Privacy](https://arxiv.org/abs/2602.09627)
*Dennis Breutigam,Rüdiger Reischuk*

Main category: cs.CR

TL;DR: 该论文研究了统计隐私（SP）与差分隐私（DP）的对比，针对有限背景知识的对手场景，提出了一种基于子采样和随机分区数据库的隐私机制，首次获得了无数据库限制的隐私上界，展示了在实际应用中考虑分布熵能改善隐私和精度保证。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）假设对手拥有数据库条目的几乎完整信息，这种最坏情况假设可能高估实际中个人面临的隐私威胁。相比之下，统计隐私（SP）及相关概念（如无噪声隐私或有限背景知识隐私）描述对手知道数据库条目的分布但不知道确切实现的情况。需要分析分布熵引入的不确定性与扭曲查询答案的隐私机制之间的相互作用。

Method: 提出基于子采样和随机分区数据库的隐私机制，通过限制查询之间的依赖关系来处理多个查询（组合）问题。这种方法首次在无数据库进一步限制的情况下，获得了针对有限对手的隐私上界。

Result: 研究结果表明，在实际应用场景中考虑分布熵可以改善隐私和精度保证。通过示例说明，在固定隐私参数和效用损失的情况下，统计隐私（SP）允许的查询数量显著多于差分隐私（DP）。

Conclusion: 该研究为有限背景知识对手场景下的隐私保护提供了新的理论框架和机制，展示了在实际应用中考虑分布不确定性可以显著提高隐私保护的效率和实用性，为更现实的隐私保护方案设计提供了理论基础。

Abstract: Differential Privacy (DP) considers a scenario in which an adversary has almost complete information about the entries of a database. This worst-case assumption is likely to overestimate the privacy threat faced by an individual in practice. In contrast, Statistical Privacy (SP), as well as related notions such as noiseless privacy or limited background knowledge privacy, describe a setting in which the adversary knows the distribution of the database entries, but not their exact realizations. In this case, privacy analysis must account for the interaction between uncertainty induced by the entropy of the underlying distributions and privacy mechanisms that distort query answers, which can be highly non-trivial.
  This paper investigates this problem for multiple queries (composition). A privacy mechanism is proposed that is based on subsampling and randomly partitioning the database to bound the dependency among queries. This way for the first time, to the best of our knowledge, upper privacy bounds against limited adversaries are obtained without any further restriction on the database.
  These bounds show that in realistic application scenarios taking the entropy of distributions into account yields improvements of privacy and precision guarantees. We illustrate examples where for fixed privacy parameters and utility loss SP allows significantly more queries than DP.

</details>


### [12] [Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks](https://arxiv.org/abs/2602.09629)
*Hayfa Dhabhi,Kashyap Thimmaraju*

Main category: cs.CR

TL;DR: 该研究提出了四检查点框架来分析LLM安全防御机制，发现输出阶段防御最弱，传统二元评估严重低估了实际漏洞风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然展示了越狱攻击的成功，但未能解释防御机制在何处失败以及为何失败。需要系统性地分析LLM安全防御的薄弱环节。

Method: 提出四检查点框架，将安全机制按处理阶段（输入vs输出）和检测级别（字面vs意图）组织成四个检查点。设计了13种针对特定检查点的规避技术，使用LLM作为评判者进行分类，并引入加权攻击成功率来捕捉被二元评估忽略的部分信息泄露。

Result: 传统二元攻击成功率为22.6%，但加权攻击成功率显示实际漏洞达52.7%，高出2.3倍。输出阶段防御最弱（72-79% WASR），输入字面防御最强（13% WASR）。Claude安全性最佳（42.8% WASR），GPT-5（55.9%）和Gemini（59.5%）次之。

Conclusion: 当前防御在输入字面检查点最强，但在意图级操纵和输出阶段技术面前仍然脆弱。四检查点框架为识别和解决部署系统中的安全漏洞提供了结构化方法。

Abstract: Large Language Models (LLMs) deploy safety mechanisms to prevent harmful outputs, yet these defenses remain vulnerable to adversarial prompts. While existing research demonstrates that jailbreak attacks succeed, it does not explain \textit{where} defenses fail or \textit{why}.
  To address this gap, we propose that LLM safety operates as a sequential pipeline with distinct checkpoints. We introduce the \textbf{Four-Checkpoint Framework}, which organizes safety mechanisms along two dimensions: processing stage (input vs.\ output) and detection level (literal vs.\ intent). This creates four checkpoints, CP1 through CP4, each representing a defensive layer that can be independently evaluated. We design 13 evasion techniques, each targeting a specific checkpoint, enabling controlled testing of individual defensive layers.
  Using this framework, we evaluate GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across 3,312 single-turn, black-box test cases. We employ an LLM-as-judge approach for response classification and introduce Weighted Attack Success Rate (WASR), a severity-adjusted metric that captures partial information leakage overlooked by binary evaluation.
  Our evaluation reveals clear patterns. Traditional Binary ASR reports 22.6\% attack success. However, WASR reveals 52.7\%, a 2.3$\times$ higher vulnerability. Output-stage defenses (CP3, CP4) prove weakest at 72--79\% WASR, while input-literal defenses (CP1) are strongest at 13\% WASR. Claude achieves the strongest safety (42.8\% WASR), followed by GPT-5 (55.9\%) and Gemini (59.5\%).
  These findings suggest that current defenses are strongest at input-literal checkpoints but remain vulnerable to intent-level manipulation and output-stage techniques. The Four-Checkpoint Framework provides a structured approach for identifying and addressing safety vulnerabilities in deployed systems.

</details>


### [13] [PiTPM: Partially Interactive Signatures for Multi-Device TPM Operations](https://arxiv.org/abs/2602.09707)
*Yunusa Simpa Abdulsalam,Mustapha Hedabou*

Main category: cs.CR

TL;DR: PiTPM是一个基于TPM 2.0设备的非交互式多方签名聚合框架，通过混合信任架构消除了传统TPM多方签名方案中的交互需求，实现常数大小的签名和显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有TPM多方签名方案需要所有参与者在承诺阶段进行交互协调，导致同步瓶颈、二次通信复杂度和参与者失败时协议中止等问题，这在跨设备密码操作应用中尤为突出。

Method: 基于Schnorr数字签名构建聚合器框架，采用混合信任架构，在聚合器中安全存储预共享的随机种子，实现无需参与者间通信的确定性全局承诺计算。

Result: 提出的框架产生常数大小的签名（与签名者数量无关），实验结果表明在TPM密码系统设计中可能实现范式转变，混合信任架构在保持严格安全保证的同时获得显著性能提升。

Conclusion: PiTPM通过消除交互需求解决了TPM多方签名的关键限制，提供了EU-CMA安全性证明，为分布式系统中的高效硬件密码安全提供了新方向。

Abstract: Trusted Platform Module (TPM) 2.0 devices provide efficient hardware-based cryptographic security through tamper-resistant key storage and computation, making them ideal building blocks for multi-party signature schemes in distributed systems. However, existing TPM-based multi-signature constructions suffer from a fundamental limitation, they require interactive protocols where all participants must coordinate during the commitment phase, before any signature can be computed. This interactive requirement creates several critical problems, such as synchronization bottlenecks, quadratic communication complexity, and aborted protocols as a result of participant failure. These limitations become particularly heightened for applications that require cross-device cryptographic operations. This paper presents PiTPM, an Aggregator Framework built upon Schnorr's digital signature. Our protocol eliminates the interactive requirement using a hybrid trust architecture. The proposed framework uses pre-shared randomness seeds stored securely in an Aggregator, enabling deterministic computation of global commitments without inter-participant communication. The resulting signatures of the proposed framework are of constant size regardless of signer count. Our experimental results show a possible paradigm shift in TPM-based cryptographic system design, demonstrating that hybrid trust architectures can achieve significant performance improvements while maintaining rigorous security guarantees. We provide a comprehensive formal security analysis proving EU-CMA security under the discrete logarithm assumption in the random oracle model.

</details>


### [14] [From Multi-sig to DLCs: Modern Oracle Designs on Bitcoin](https://arxiv.org/abs/2602.09822)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 比特币原生层（Layer 1）的预言机设计自2015年后从多签模式转向基于证明的设计，特别是离散对数合约（DLCs），这些设计在比特币社区中获得了更好的合规性、工具支持和实际应用。


<details>
  <summary>Details</summary>
Motivation: 比特币作为主要设计为交易账本的原生货币平台，其可编程性有限，特别是在依赖外部事件的预言机机制方面。研究旨在了解自2015年以太坊智能合约时代以来，比特币Layer 1是否出现了新的预言机设计，以及后续的比特币改进提案是否扩展了预言机的可实施性。

Method: 使用Scopus和Web of Science数据库进行文献检索，辅以Google Scholar来捕捉协议提案，分析比特币Layer 1预言机设计的演变。

Result: 学术索引覆盖仍然有限，许多贡献在期刊渠道之外流通。主要发现是2015年后的转变：从多签风格（将预言机视为共同签名者）转向基于证明的设计，特别是离散对数合约（DLCs）。DLCs在比特币社区中表现出更强的合规性、工具支持，并在真实世界场景（如博彩和预测市场机制）中有实际实施证据。

Conclusion: 比特币Layer 1的预言机设计经历了从多签模式到基于证明设计的演变，其中离散对数合约（DLCs）成为主流，显示出更好的社区接受度、工具生态系统和实际应用潜力。

Abstract: Unlike Ethereum, which was conceived as a general-purpose smart-contract platform, Bitcoin was designed primarily as a transaction ledger for its native currency, which limits programmability for conditional applications. This constraint is particularly evident when considering oracles, mechanisms that enable Bitcoin contracts to depend on exogenous events. This paper investigates whether new oracle designs have emerged for Bitcoin Layer 1 since the 2015 transition to the Ethereum smart contracts era and whether subsequent Bitcoin improvement proposals have expanded oracles' implementability. Using Scopus and Web of Science searches, complemented by Google Scholar to capture protocol proposals, we observe that the indexed academic coverage remains limited, and many contributions circulate outside journal venues. Within the retrieved corpus, the main post-2015 shift is from multisig-style, which envisioned oracles as co-signers, toward attestation-based designs, mainly represented by Discreet Log Contracts (DLCs), which show stronger Bitcoin community compliance, tool support, and evidence of practical implementations in real-world scenarios such as betting and prediction-market mechanisms.

</details>


### [15] [Spinel: A Post-Quantum Signature Scheme Based on SLn(Fp) Hashing](https://arxiv.org/abs/2602.09882)
*Asmaa Cherkaoui,Faraz Heravi,Delaram Kahrobaei,Siamak F. Shahandashti*

Main category: cs.CR

TL;DR: Spinel是一个后量子数字签名方案，结合了SPHINCS+的安全性和基于Tillich-Zemor范式的代数哈希函数，其安全性基于SL_n(F_p)上扩展图导航问题的困难性，即使对量子对手也有效。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现迫使密码学界设计超越经典困难假设的数字签名方案。需要构建基于代数困难问题的后量子安全签名方案。

Method: 将基于Tillich-Zemor范式的新型代数哈希函数（安全性基于SL_n(F_p)上扩展图导航问题）集成到SPHINCS+框架中，形成Spinel签名方案。提供了该哈希函数的安全经验证据，分析了方案的安全退化模型，并进行了参数选择和实现。

Result: 提出了Spinel签名方案，实现了代数哈希函数和签名方案的实现，提供了详细的性能经验结果，证明了其在实践中的可行性。

Conclusion: Spinel为代数哈希基签名方案的设计奠定了基础，扩展了后量子密码学的工具箱，结合了SPHINCS+的成熟安全性和代数哈希函数的后量子安全性。

Abstract: The advent of quantum computation compels the cryptographic community to design digital signature schemes whose security extends beyond the classical hardness assumptions. In this work, we introduce Spinel, a post-quantum digital signature scheme that combines the proven security of SPHINCS+ (CCS 2019) with a new family of algebraic hash functions (Adv. Math. Commun. 2025) derived from the Tillich-Zemor paradigm (Eurocrypt 2008) with security rooted in the hardness of navigating expander graphs over SL_n(F_p), a problem believed to be hard even for quantum adversaries. We first provide empirical evidence of the security of this hash function, complementing the original theoretical analysis. We then show how the hash function can be integrated within the SPHINCS+ framework to give a secure signature scheme. We then model and analyze the security degradation of the proposed scheme, which informs the parameter selection we discuss next. Finally, we provide an implementation of the hash function and the proposed signature scheme Spinel as well as detailed empirical results for the performance of Spinel showing its feasibility in practice. Our approach lays the foundations for the design of algebraic hash-based signature schemes, expanding the toolkit of post-quantum cryptography.

</details>


### [16] [Trustworthy Agentic AI Requires Deterministic Architectural Boundaries](https://arxiv.org/abs/2602.09947)
*Manish Bhattarai,Minh Vu*

Main category: cs.CR

TL;DR: 论文提出当前基于自回归语言模型的智能体架构无法满足高风险科学工作流的安全需求，需要确定性架构而非概率学习行为来保证可信AI辅助科学。


<details>
  <summary>Details</summary>
Motivation: 当前基于自回归语言模型的智能体AI架构与高风险科学工作流的安全和认识论要求存在根本性不兼容。问题不在于对齐不足或防护不够，而是架构性的：自回归语言模型对所有标记进行统一处理，仅通过训练无法实现确定性的命令-数据分离。

Method: 提出三位一体防御架构，通过三种机制强制执行安全性：1) 通过有限动作演算和参考监控器强制执行动作治理；2) 通过强制访问标签防止跨范围泄漏的信息流控制；3) 隔离感知与执行的特权分离。

Result: 研究表明，没有不可伪造的来源和确定性调解，"致命三重奏"（不可信输入、特权数据访问、外部行动能力）将授权安全变成漏洞发现问题：基于训练的防御可能降低经验攻击率，但无法提供确定性保证。

Conclusion: 机器学习社区必须认识到对齐对于授权安全是不够的，在智能体AI安全部署到重要科学领域之前，需要架构调解。确定性架构执行而非概率学习行为是可信AI辅助科学的必要条件。

Abstract: Current agentic AI architectures are fundamentally incompatible with the security and epistemological requirements of high-stakes scientific workflows. The problem is not inadequate alignment or insufficient guardrails, it is architectural: autoregressive language models process all tokens uniformly, making deterministic command--data separation unattainable through training alone. We argue that deterministic, architectural enforcement, not probabilistic learned behavior, is a necessary condition for trustworthy AI-assisted science. We introduce the Trinity Defense Architecture, which enforces security through three mechanisms: action governance via a finite action calculus with reference-monitor enforcement, information-flow control via mandatory access labels preventing cross-scope leakage, and privilege separation isolating perception from execution. We show that without unforgeable provenance and deterministic mediation, the ``Lethal Trifecta'' (untrusted inputs, privileged data access, external action capability) turns authorization security into an exploit-discovery problem: training-based defenses may reduce empirical attack rates but cannot provide deterministic guarantees. The ML community must recognize that alignment is insufficient for authorization security, and that architectural mediation is required before agentic AI can be safely deployed in consequential scientific domains.

</details>


### [17] [CAPID: Context-Aware PII Detection for Question-Answering Systems](https://arxiv.org/abs/2602.10074)
*Mariia Ponomarenko,Sepideh Abedini,Masoumeh Shafieinejad,D. B. Emerson,Shubhankar Mohapatra,Xi He*

Main category: cs.CR

TL;DR: CAPID：一种通过微调本地小型语言模型实现隐私保护PII检测的方法，能识别PII的上下文相关性，在保持下游任务效用的同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 当前PII检测方法通常直接屏蔽所有个人信息，忽略了部分PII可能与用户问题上下文相关，导致响应质量下降。大语言模型虽能判断相关性，但因其闭源特性和隐私保障不足，不适合处理敏感数据。

Method: 提出CAPID方法：1）构建合成数据生成管道，利用LLM生成包含多种PII类型和相关性级别的多样化数据集；2）微调本地小型语言模型，使其能检测PII范围、分类PII类型并评估上下文相关性；3）在将查询传递给LLM进行问答前，用该模型过滤敏感信息。

Result: 实验表明，基于微调SLM的相关性感知PII检测在范围、相关性和类型准确性方面显著优于现有基线方法，同时在匿名化处理下保持了更高的下游任务效用。

Conclusion: CAPID提供了一种实用的隐私保护PII检测解决方案，通过微调本地小型语言模型实现了相关性感知的敏感信息过滤，在保护隐私的同时提升了问答系统的响应质量。

Abstract: Detecting personally identifiable information (PII) in user queries is critical for ensuring privacy in question-answering systems. Current approaches mainly redact all PII, disregarding the fact that some of them may be contextually relevant to the user's question, resulting in a degradation of response quality. Large language models (LLMs) might be able to help determine which PII are relevant, but due to their closed source nature and lack of privacy guarantees, they are unsuitable for sensitive data processing. To achieve privacy-preserving PII detection, we propose CAPID, a practical approach that fine-tunes a locally owned small language model (SLM) that filters sensitive information before it is passed to LLMs for QA. However, existing datasets do not capture the context-dependent relevance of PII needed to train such a model effectively. To fill this gap, we propose a synthetic data generation pipeline that leverages LLMs to produce a diverse, domain-rich dataset spanning multiple PII types and relevance levels. Using this dataset, we fine-tune an SLM to detect PII spans, classify their types, and estimate contextual relevance. Our experiments show that relevance-aware PII detection with a fine-tuned SLM substantially outperforms existing baselines in span, relevance and type accuracy while preserving significantly higher downstream utility under anonymization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design](https://arxiv.org/abs/2602.09410)
*Yuchao Liao,Tosiron Adegbija,Roman Lysecky*

Main category: cs.AR

TL;DR: LLM驱动的硬件-软件协同设计框架，用于加速后量子密码学（特别是FALCON签名方案）的FPGA加速器设计，相比传统HLS方法实现高达2.6倍的核执行时间加速。


<details>
  <summary>Details</summary>
Motivation: 后量子密码学算法计算复杂，硬件实现效率低，需要更快速、高效的硬件-软件协同设计方法来应对量子计算威胁。

Method: 提出基于大语言模型（LLM）的框架，分析PQC算法、识别性能关键组件，并生成FPGA实现的候选硬件描述，采用人机协作模式进行加速器设计。

Result: LLM生成的加速器在FALCON低层计算密集型核中，相比传统HLS方法实现高达2.6倍的执行时间加速，关键路径更短，但在资源利用和功耗方面存在权衡。

Conclusion: LLM能够最小化设计工作量和开发时间，自动化PQC算法的FPGA加速器设计迭代，为快速自适应PQC加速器设计提供了有前景的新方向。

Abstract: Post-quantum cryptography (PQC) is crucial for securing data against emerging quantum threats. However, its algorithms are computationally complex and difficult to implement efficiently on hardware. In this paper, we explore the potential of Large Language Models (LLMs) to accelerate the hardware-software co-design process for PQC, with a focus on the FALCON digital signature scheme. We present a novel framework that leverages LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation. We present the first quantitative comparison between LLM-driven synthesis and conventional HLS-based approaches for low-level compute-intensive kernels in FALCON, showing that human-in-the-loop LLM-generated accelerators can achieve up to 2.6x speedup in kernel execution time with shorter critical paths, while highlighting trade-offs in resource utilization and power consumption. Our results suggest that LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising new direction for rapid and adaptive PQC accelerator design on FPGAs.

</details>


### [19] [Development of an Energy-Efficient and Real-Time Data Movement Strategy for Next-Generation Heterogeneous Mixed-Criticality Systems](https://arxiv.org/abs/2602.09554)
*Thomas Benz*

Main category: cs.AR

TL;DR: 论文探讨了汽车、机器人、航空航天等工业领域对ACES（自主性、连接性、电气化、共享移动）的需求增长，这导致了对计算性能、通信基础设施和能源效率的更高要求，同时面临摩尔定律放缓的挑战，需要异构计算架构和内存系统的协同设计。


<details>
  <summary>Details</summary>
Motivation: 工业领域对ACES（自主性、连接性、电气化、共享移动）的需求快速增长，这显著增加了对车载计算性能和高性能通信基础设施的要求。同时，摩尔定律和登纳德缩放定律放缓，迫使计算系统向更大规模、更高异构性和专业化方向发展。ACES应用需要大量计算资源，但大多数用例受资源限制，因此需要更高的能源效率。

Method: 论文提出通过应用特定的硬件加速器实现异构计算架构，而非仅仅依赖技术缩放。需要将内存系统与用例、计算单元和加速器进行精心协同设计，以满足性能和能源效率要求。同时需要处理混合关键性系统（MCSs）中不同关键级别之间的最小化争用问题。

Result: 分析表明，ACES应用的增长导致对计算性能、内存带宽和容量的需求增加，同时需要处理大规模、不规则数据集和持续传感器数据流。异构混合关键性系统对互连系统提出了更高要求，需要在保持高可预测性的同时最小化不同关键级别之间的争用。

Conclusion: 满足工业应用对性能和能源效率的广泛需求，需要将内存系统与用例、计算单元和加速器进行精心协同设计。这包括处理异构混合关键性系统中的互连挑战，确保不同关键级别任务之间的最小争用，以维持系统的高可预测性。

Abstract: Industrial domains such as automotive, robotics, and aerospace are rapidly evolving to satisfy the increasing demand for machine-learning-driven Autonomy, Connectivity, Electrification, and Shared mobility (ACES). This paradigm shift inherently and significantly increases the requirement for onboard computing performance and high-performance communication infrastructure. At the same time, Moore's Law and Dennard Scaling are grinding to a halt, in turn, driving computing systems to larger scales and higher levels of heterogeneity and specialization, through application-specific hardware accelerators, instead of relying on technological scaling only. Approaching ACES requires this substantial amount of compute at an increasingly high energy-efficiency, since most use cases are fundamentally resource-bound. This increase in compute performance and heterogeneity goes hand in hand with a growing demand for high memory bandwidth and capacity as the driving applications grow in complexity, operating on huge and progressively irregular data sets and further requiring a steady influx of sensor data, increasing pressure both on on-chip and off-chip interconnect systems. Further, ACES combines real-time time-critical with general compute tasks on the same physical platform, sharing communication, storage, and micro-architectural resources. These heterogeneous mixed-criticality systems (MCSs) place additional pressure on the interconnect, demanding minimal contention between the different criticality levels to sustain a high degree of predictability. Fulfilling the performance and energy-efficiency requirements across a wide range of industrial applications requires a carefully co-designed process of the memory system with the use cases as well as the compute units and accelerators.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation](https://arxiv.org/abs/2602.09112)
*Russ Webb,Jason Ramapuram*

Main category: cs.AI

TL;DR: Cadmus系统通过小型整数虚拟机、多样化真实程序数据集和低成本训练的Transformer模型，为程序合成研究提供了透明可控的实验平台，相比大型语言模型具有成本优势和可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 当前程序合成研究主要依赖大型语言模型，存在训练数据分布不明确、微调效果难以理解、分词影响不透明、计算存储成本高等问题，需要更透明可控的研究平台。

Method: 开发Cadmus系统，包含整数虚拟机、多样化真实程序数据集，使用自回归Transformer模型在200美元计算成本内训练，提供对训练分布和模型内部状态的细粒度控制与可观测性。

Result: Cadmus模型在特定领域语言(DSL)的整数算术程序补全任务上达到100%准确率，优于GPT-5的95%；同时揭示了GPT-5在解决相同任务时引入了未知的先验知识，这成为研究中的混淆因素。

Conclusion: 小型模型在复杂推理任务上为程序合成研究提供了经济高效且透明的实验平台，能够避免大型语言模型中训练数据关系不明确的问题，支持对程序补全、分布外表示、归纳推理和指令跟随等问题的深入研究。

Abstract: What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\% accuracy while GPT-5 has 95\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.

</details>


### [21] [PABU: Progress-Aware Belief Update for Efficient LLM Agents](https://arxiv.org/abs/2602.09138)
*Haitao Jiang,Lin Ge,Hengrui Cai,Rui Song*

Main category: cs.AI

TL;DR: PABU框架通过显式建模任务进度和选择性保留历史信息，减少LLM智能体的冗余动作和推理成本，在AgentGym基准测试中显著提升任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体通常基于完整的动作-观察历史来决策，这会引入大量任务无关信息，导致冗余动作和更高的推理成本。需要一种更紧凑的状态表示方法来提高效率。

Method: 提出Progress-Aware Belief Update (PABU)信念状态框架，通过显式建模任务进度和选择性保留历史信息。在每个步骤中，智能体预测自上一轮以来的相对进度，并决定是否存储新遇到的交互，仅基于保留的子集进行未来决策。

Result: 在AgentGym基准测试的8个环境中，使用相同的训练轨迹，PABU实现了81.0%的任务完成率，比基于完整历史信念的SOTA模型高出23.9%。同时将平均交互步骤减少到9.5步，对应26.9%的减少。

Conclusion: PABU通过显式进度预测和选择性保留历史信息，能够有效减少LLM智能体的冗余动作和推理成本，显著提升任务完成效率和性能。消融研究表明这两个组件对于稳健的信念学习和性能提升都是必要的。

Abstract: Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.

</details>


### [22] [CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective](https://arxiv.org/abs/2602.09159)
*Yichen Wu,Yujin Oh,Sangjoon Park,Kailong Fan,Dania Daye,Hana Farzaneh,Xiang Li,Raul Uppot,Quanzheng Li*

Main category: cs.AI

TL;DR: CoMMa是一个去中心化的LLM智能体框架，通过博弈论目标和确定性嵌入投影实现贡献感知的信用分配，在肿瘤学决策支持任务中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体框架在处理需要基于动态、异质患者数据进行推理的肿瘤学决策支持任务时存在局限性，特别是缺乏明确的证据归因和稳定的决策路径。

Method: 提出CoMMa框架：1）去中心化架构，专家智能体在分区证据上操作；2）通过博弈论目标进行协调；3）使用确定性嵌入投影近似贡献感知的信用分配；4）通过估计每个智能体的边际效用来实现明确的证据归因。

Result: 在多样化的肿瘤学基准测试（包括真实世界的多学科肿瘤委员会数据集）上，CoMMa比数据集中化和基于角色的多智能体基线方法获得了更高的准确性和更稳定的性能。

Conclusion: CoMMa框架通过贡献感知的信用分配机制，能够产生可解释且数学基础扎实的决策路径，在肿瘤学决策支持任务中实现了更优的性能和稳定性。

Abstract: Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.

</details>


### [23] [Measuring Dataset Diversity from a Geometric Perspective](https://arxiv.org/abs/2602.09340)
*Yang Ba,Mohammad Sadeq Abolhasani,Michelle V Mancenido,Rong Pan*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析（TDA）和持久景观（PLs）的几何多样性度量框架，超越传统熵和分布变异，捕捉数据集的几何结构特征


<details>
  <summary>Details</summary>
Motivation: 现有多样性度量主要关注特征空间离散度或度量空间幅度，主要捕捉分布变异或熵，但很大程度上忽略了数据集的几何结构。需要一种能够量化数据几何特征的多样性度量方法

Method: 引入基于拓扑数据分析和持久景观的框架，从数据中提取和量化几何特征。该方法提供理论基础的多样性测量手段，超越熵的概念，捕捉数据集的丰富几何和结构特性

Result: 通过跨多种模态的广泛实验证明，提出的PLs-based多样性度量（PLDiv）是强大、可靠且可解释的，能够直接将数据多样性与底层几何结构联系起来

Conclusion: 该方法为数据集构建、增强和评估提供了基础工具，通过拓扑数据分析框架实现了对数据集几何多样性的量化测量

Abstract: Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.

</details>


### [24] [Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge](https://arxiv.org/abs/2602.09341)
*Wei Yang,Shixuan Li,Heng Ping,Peiyu Zhang,Paul Bogdan,Jesse Thomason*

Main category: cs.AI

TL;DR: AgentAuditor：用推理树路径搜索替代多数投票的多智能体系统框架，通过ACPO训练裁决器，在5种设置中比多数投票准确率提升5%


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统大多使用多数投票聚合智能体输出，这种方法丢弃了推理轨迹的证据结构，且在"幻觉共识"下脆弱——当智能体共享相关偏见并收敛于相同错误推理时

Method: 1. 引入AgentAuditor框架，用推理树路径搜索替代投票，显式表示智能体轨迹间的共识与分歧；2. 通过在关键分歧点比较推理分支来解决冲突，将全局裁决转化为高效的局部验证；3. 提出反共识偏好优化(ACPO)，在多数失败案例上训练裁决器，奖励基于证据的少数选择而非流行错误

Result: 在5种流行设置中，AgentAuditor相比多数投票获得高达5%的绝对准确率提升，相比使用LLM-as-Judge提升高达3%

Conclusion: AgentAuditor通过显式建模推理轨迹的共识与分歧，提供比简单多数投票更鲁棒的多智能体决策框架，特别是在处理智能体共享偏见导致错误共识的场景中表现优异

Abstract: Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.

</details>


### [25] [Image Quality in the Era of Artificial Intelligence](https://arxiv.org/abs/2602.09347)
*Jana G. Delfino,Jason L. Granstedt,Frank W. Samuelson,Robert Ochs,Krishna Juluru*

Main category: cs.AI

TL;DR: 本文探讨了AI在放射学图像重建和增强中的应用，指出AI虽能提升图像质量和工作效率，但也引入了新的故障模式，可能导致图像感知质量与信息内容之间的脱节，强调理解AI技术局限性对安全有效使用的重要性。


<details>
  <summary>Details</summary>
Motivation: AI在放射学中快速部署，虽然能显著改善图像质量、提高采集速度和阅片效率，但也带来了新的风险。AI可能加剧图像感知质量与实际信息内容之间的脱节，因此理解AI图像重建和增强的局限性对于安全有效地使用该技术至关重要。

Method: 本文采用通信形式，旨在提高人们对AI在放射学图像重建和增强中局限性的认识。通过分析AI技术的特点和潜在风险，为临床实践提供指导。

Result: AI在放射学图像处理中确实能产生更清晰、更平滑、更详细的图像，加快采集速度，并提高临床医生阅片效率。然而，AI也引入了新的故障模式，可能导致图像感知质量与实际信息内容之间的不一致。

Conclusion: 理解AI图像重建和增强的局限性对于安全有效地使用该技术至关重要。通过提高对AI局限性的认识，用户可以在享受技术益处的同时最小化风险，实现AI在放射学中的优化应用。

Abstract: Artificial intelligence (AI) is being deployed within radiology at a rapid pace. AI has proven an excellent tool for reconstructing and enhancing images that appear sharper, smoother, and more detailed, can be acquired more quickly, and allowing clinicians to review them more rapidly. However, incorporation of AI also introduces new failure modes and can exacerbate the disconnect between perceived quality of an image and information content of that image. Understanding the limitations of AI-enabled image reconstruction and enhancement is critical for safe and effective use of the technology. Hence, the purpose of this communication is to bring awareness to limitations when AI is used to reconstruct or enhance a radiological image, with the goal of enabling users to reap benefits of the technology while minimizing risks.

</details>


### [26] [Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models](https://arxiv.org/abs/2602.09485)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: XMCC是一种可解释的多模态思维链压缩器，通过强化学习将压缩建模为序列决策过程，能在缩短推理轨迹的同时保留关键步骤和答案正确性，并提供压缩决策的自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 长思维链在多模态推理中广泛使用，但往往过于冗长且包含冗余推理步骤，影响推理效率。现有压缩方法存在两个主要问题：1)可能破坏视觉-文本推理的完整性，移除关键对齐线索；2)压缩过程缺乏可解释性，难以识别哪些信息是关键的。

Method: 提出XMCC（可解释多模态思维链压缩器），将压缩建模为序列决策过程，通过强化学习进行优化。该方法能够有效缩短推理轨迹，同时保留关键推理步骤和答案正确性，并为压缩决策生成自然语言解释。

Result: 在代表性多模态推理基准上的大量实验表明，XMCC不仅减少了推理长度，还提供了可解释的解释，验证了其有效性。

Conclusion: XMCC解决了长思维链压缩中的两个关键挑战：保持视觉-文本推理完整性和提供压缩决策的可解释性，为高效且透明的多模态推理提供了有效解决方案。

Abstract: Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.

</details>


### [27] [Computing Conditional Shapley Values Using Tabular Foundation Models](https://arxiv.org/abs/2602.09489)
*Lars Henry Berge Olsen,Dennis Christensen*

Main category: cs.AI

TL;DR: 本文提出使用TabPFN等表格基础模型来高效计算Shapley值，通过上下文学习避免重复训练，在保持准确性的同时大幅降低计算时间。


<details>
  <summary>Details</summary>
Motivation: Shapley值作为可解释AI的核心方法，在特征相关时计算成本极高，传统方法需要大量条件期望的蒙特卡洛积分或回归计算，而回归方法需要为每个条件期望重新训练模型，计算效率低下。

Method: 利用TabPFN等表格基础模型的上下文学习能力，无需重新训练即可近似每个条件期望，从而高效计算Shapley值。研究比较了TabPFN的多个变体与现有最优方法在模拟和真实数据集上的表现。

Result: 在大多数情况下，TabPFN表现最佳；即使不是最优时，也仅略逊于最佳方法，但运行时间大幅减少。表格基础模型在条件Shapley值估计方面展现出显著优势。

Conclusion: 表格基础模型为高效计算Shapley值提供了新途径，通过上下文学习避免了重复训练的计算瓶颈，在保持准确性的同时显著提升效率，未来可进一步优化以适应条件Shapley值估计的特定需求。

Abstract: Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.

</details>


### [28] [FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints](https://arxiv.org/abs/2602.09620)
*Jorge Fandinno,Pedro Cabalar,Philipp Wanko,Torsten Schaub*

Main category: cs.AI

TL;DR: FLINGO语言扩展了约束回答集编程，将ASP的数值属性表达能力融入约束处理中，解决了CASP中表达性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前CASP求解器中约束的表示方式更接近数值后端的表达能力，而丢失了ASP中数值属性的丰富表达能力，如默认值、未定义属性、非确定性赋值和聚合值等特性。

Method: 提出FLINGO语言和工具，将ASP的数值属性表达能力融入数值约束中，并提供从FLINGO语法到标准CASP程序（遵循CLINGCON输入格式）的翻译方法。

Result: FLINGO语言成功将ASP的数值属性表达能力整合到约束处理中，通过多个示例展示了其表达能力，并建立了从FLINGO到标准CASP的翻译机制。

Conclusion: FLINGO语言填补了CASP中约束表示与ASP数值属性表达能力之间的差距，为实际应用提供了更丰富的表达工具。

Abstract: Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.

</details>


### [29] [ClinAlign: Scaling Healthcare Alignment from Clinician Preference](https://arxiv.org/abs/2602.09653)
*Shiwei Lyu,Xidong Wang,Lei Liu,Hao Zhu,Chaohe Zhang,Jian Wang,Jinjie Gu,Benyou Wang,Yue Shen*

Main category: cs.AI

TL;DR: 该研究提出了一个两阶段框架，通过创建医生验证的偏好数据集和可扩展的临床原则，解决LLM医疗输出与临床医生精细偏好的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展现出专家级医疗知识，但其开放式输出与临床医生精细偏好的对齐仍然具有挑战性。现有方法通常依赖粗略的目标或不可靠的自动评估，这些方法缺乏专业指南的坚实基础。

Method: 提出两阶段框架：1) 创建HealthRubrics数据集，包含7,034个医生验证的偏好示例，临床医生在其中精炼LLM起草的评分标准以满足严格的医疗标准；2) 将这些评分标准提炼为HealthPrinciples：119个广泛可重用、基于临床的原则，按临床维度组织，实现超越手动标注的可扩展监督。使用HealthPrinciples进行：离线对齐（为未标记查询合成评分标准）和推理时工具（引导自我修订）。

Result: 一个仅激活30B参数中3B参数的模型，通过该框架训练，在HealthBench-Hard上达到33.4%的准确率，优于包括Deepseek-R1和o3在内的更大模型，为临床对齐建立了资源高效的基线。

Conclusion: 该研究提出的框架通过医生验证的数据集和可扩展的临床原则，有效解决了LLM医疗输出与临床医生偏好的对齐问题，同时实现了资源高效的高性能。

Abstract: Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>


### [30] [GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis](https://arxiv.org/abs/2602.09794)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Xudong Wang,Zhenzhen Huang,Pengcheng Zheng,Shuai Yuan,Sheng Zheng,Qigan Sun,Jie Zou,Lik-Hang Lee,Yang Yang*

Main category: cs.AI

TL;DR: GHS-TDA通过构建全局假设图和多尺度拓扑分析，解决了传统CoT方法中错误传播和缺乏结构化分析的问题，显著提升了推理的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在两个根本性限制：1) 推理过程对早期决策高度敏感，一旦引入初始错误会传播放大且难以纠正；2) 缺乏结构化分析技术来过滤冗余推理和提取关键特征，导致推理过程不稳定且可解释性有限。

Method: 提出GHS-TDA方法：首先构建语义丰富的全局假设图来聚合、对齐和协调多个候选推理路径，为局部推理失败时提供全局修正路径；然后应用基于持续同调的拓扑数据分析来捕获稳定的多尺度结构，去除冗余和不一致，提取更可靠的推理骨架。

Result: 通过联合利用推理多样性和拓扑稳定性，GHS-TDA实现了自适应收敛，产生高置信度和可解释的推理路径，在多个推理基准测试中在准确性和鲁棒性方面持续优于强基线方法。

Conclusion: GHS-TDA通过全局协调和拓扑分析有效解决了传统CoT方法的局限性，为LLM的复杂推理任务提供了更可靠、稳定和可解释的解决方案。

Abstract: Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>


### [31] [Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices](https://arxiv.org/abs/2602.09802)
*Manon Reusens,Sofie Goethals,Toon Calders,David Martens*

Main category: cs.AI

TL;DR: 研究通过旅行助手场景分析LLM在主观决策中的表现，使用多项logit模型推导隐含支付意愿，并与人类基准比较，发现LLM能产生有意义的WTP但存在系统性偏差


<details>
  <summary>Details</summary>
Motivation: 随着LLM在旅行助手、购物支持等应用中部署，它们经常需要在没有客观正确答案的情况下为用户做出主观选择。研究旨在评估LLM在这种主观决策环境中的表现，特别是在旅行助手场景下的决策质量。

Method: 在旅行助手背景下向模型呈现选择困境，使用多项logit模型分析响应以推导隐含支付意愿估计。除了基线设置外，还研究了在更现实条件下的模型行为变化，包括提供用户过去选择信息和基于角色的提示。

Result: 较大LLM可以推导出有意义的WTP值，但在属性层面存在系统性偏差。总体上倾向于高估人类WTP，特别是当引入昂贵选项或商务导向角色时。当模型基于对更便宜选项的先前偏好进行条件化时，估值更接近人类基准。

Conclusion: 研究结果突出了使用LLM进行主观决策支持的潜力和局限性，强调了在实际部署此类系统时仔细选择模型、设计提示和用户表示的重要性。

Abstract: As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>


### [32] [Discovering High Level Patterns from Simulation Traces](https://arxiv.org/abs/2602.10009)
*Sean Memery,Kartic Subr*

Main category: cs.AI

TL;DR: 提出一种自然语言引导的方法，从详细模拟日志中发现粗粒度模式，使语言模型能更好地进行物理推理


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理物理任务时面临挑战，因为它们从观测数据学习物理推理而非基于模拟，而直接使用模拟轨迹作为上下文存在可扩展性问题

Method: 提出自然语言引导的方法，从详细模拟日志中发现粗粒度模式（如"刚体碰撞"、"稳定支撑"等），通过合成程序将模拟日志映射到高层激活模式序列

Result: 在两个物理基准测试中，这种模拟日志的注释表示更适用于自然语言对物理系统的推理，使语言模型能从自然语言目标生成有效的奖励程序

Conclusion: 该方法通过发现粗粒度模式来增强语言模型的物理推理能力，使其能在规划和监督学习背景下从自然语言目标生成有效的奖励程序

Abstract: Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>


### [33] [Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.10090)
*Zhaoyang Wang,Canwen Xu,Boyi Liu,Yite Wang,Siwei Han,Zhewei Yao,Huaxiu Yao,Yuxiong He*

Main category: cs.AI

TL;DR: 提出了Agent World Model (AWM)，一个完全合成的环境生成管道，用于大规模训练多轮工具使用智能体，解决了真实环境稀缺和LLM模拟环境不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型使智能体能够执行需要与工具和环境进行多轮交互的复杂任务，但智能体训练受到缺乏多样可靠环境的限制。

Method: 提出AWM合成环境生成管道，创建了1000个日常场景环境，每个环境平均配备35个工具，这些环境基于代码和数据库驱动，提供比LLM模拟更可靠的状态转换。

Result: 在三个基准测试上，仅在合成环境中训练（而非特定基准环境）的智能体表现出强大的分布外泛化能力，证明了合成环境的有效性。

Conclusion: AWM合成环境管道为大规模智能体训练提供了高质量、可靠的资源，支持多轮工具使用智能体的强化学习，并实现了良好的泛化性能。

Abstract: Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [34] [LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2602.09323)
*Jie Kong,Wei Wang,Jiehan Zhou,Chen Yu*

Main category: cs.DC

TL;DR: LLM-CoOpt是一个算法-硬件协同设计框架，通过KV缓存优化、分组查询注意力优化和分页注意力优化三个策略，显著提升LLM推理的吞吐量和降低延迟，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理面临内存带宽瓶颈、计算冗余和长序列处理效率低下等主要挑战，需要综合优化方案来提升实际推理性能。

Method: 提出LLM-CoOpt框架，包含三个关键技术：1) Opt-KV：优化KV缓存的读写路径，采用FP8量化减少内存占用；2) Opt-GQA：将多头自注意力重构为分组查询注意力，共享键值投影降低计算复杂度；3) Opt-Pa：采用两步策略处理长序列，先分段再应用惰性内存映射和计算。

Result: 在LLaMa-13BGPTQ模型上的实验表明，LLM-CoOpt将推理吞吐量提升最高13.43%，延迟降低最高16.79%，同时保持模型精度。

Conclusion: LLM-CoOpt为大规模语言模型的实际推理提供了一条实用、高性能的优化路径，有效解决了内存带宽瓶颈、计算冗余和长序列处理效率问题。

Abstract: Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.

</details>


### [35] [The Coordination Criterion](https://arxiv.org/abs/2602.09435)
*Joseph M. Hellerstein*

Main category: cs.DC

TL;DR: 论文提出了一个协调准则，用于判断分布式规范何时本质上需要协调而非特定协议或实现策略所致。该准则基于Lamport历史，指出规范只有在相对于历史扩展单调时才允许无协调实现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是区分分布式规范中哪些协调需求是本质的（规范本身要求的），哪些是特定协议或实现策略强加的。这有助于理解协调的根本必要性，为分布式系统设计提供理论基础。

Method: 在异步消息传递模型中，基于Lamport历史（偏序执行下的happens-before关系）和规范定义的观察结果，提出了协调准则。该准则不假设特定编程语言、对象实现或协议结构，直接基于历史扩展的单调性进行分析。

Result: 证明了规范允许无协调实现的充分必要条件是：在适当的观察结果顺序下，规范相对于历史扩展是单调的。这为协调需求提供了明确的边界，并统一解释了CAP不可能性、CALM无协调性、共识和快照任务、事务隔离级别、不变式汇合等多种经典结果。

Conclusion: 协调准则为分布式规范的无协调实现提供了通用判断标准，揭示了协调需求背后的统一语义现象。该理论框架能够统一解释多种分布式系统理论结果，为系统设计和分析提供了基础工具。

Abstract: When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.
  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.

</details>


### [36] [High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors](https://arxiv.org/abs/2602.09604)
*Ruimin Shi,Gabin Schieffer,Pei-Hung Lin,Maya Gokhale,Andreas Herten,Ivy Peng*

Main category: cs.DC

TL;DR: 本文研究了在ARM SVE和RISC-V RVV等新兴向量架构上实现量子态向量模拟的高性能可移植性，提出了向量长度无关的设计和优化技术，在三种ARM处理器上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着ARM SVE和RISC-V RVV等支持灵活向量长度的向量架构在高性能处理器中的兴起，需要研究在这些架构上实现高性能可移植性的可行性。量子态向量模拟作为量子计算的重要工作负载，是验证向量长度无关设计有效性的理想案例。

Method: 提出了向量长度无关的设计和优化技术，包括：VLEN自适应内存布局调整、加载缓冲、细粒度循环控制以及基于门融合的算术强度适配。在Google的Qsim中实现，并在三种ARM处理器（NVIDIA Grace、AWS Graviton3、Fujitsu A64FX）上评估了最多36个量子比特的五个量子电路。

Result: 通过定义新的指标和PMU事件来量化向量化活动，为未来的VLA设计提供了通用见解。单源VLA量子模拟实现取得了显著性能提升：在A64FX上达到4.5倍加速，在Grace上达到2.5倍加速，在Graviton上达到1.5倍加速。

Conclusion: 研究表明，通过适当的向量长度无关设计和优化技术，可以在新兴的向量架构上实现量子态向量模拟的高性能可移植性，为未来向量处理器设计提供了重要参考。

Abstract: ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.

</details>


### [37] [Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems](https://arxiv.org/abs/2602.09721)
*Guowei Liu,Hongming Li,Yaning Guo,Yongxi Lyu,Mo Zhou,Yi Liu,Zhaogeng Li,Yanpeng Wang*

Main category: cs.DC

TL;DR: AFD架构在标准集群上存在性能死区，但在特定硬件-模型组合下仍有潜力


<details>
  <summary>Details</summary>
Motivation: 探索AFD架构相比标准专家并行在大规模MoE模型部署中的性能边界，解决内存容量和带宽挑战

Method: 将屋顶模型扩展到通信层面，关联互连带宽、算术强度和硬件浮点运算利用率，系统分析AFD性能

Result: 标准集群上存在性能死区：增加FFN实例数无法提升HFU；AFD的离散节点级扩展比EP的连续批次调整产生更高不平衡惩罚；但在Superpod级硬件和粗粒度专家模型下AFD表现更好

Conclusion: AFD不是通用解决方案，而是针对特定硬件-模型组合的有前景方法，需要充足互连带宽和适当模型特性

Abstract: Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.

</details>
