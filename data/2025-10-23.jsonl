{"id": "2510.19260", "categories": ["cs.AR", "cs.ET", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.19260", "abs": "https://arxiv.org/abs/2510.19260", "authors": ["Mukul Lokhande", "Narendra Singh Dhakad", "Seema Chouhan", "Akash Sankhe", "Santosh Kumar Vishvakarma"], "title": "Res-DPU: Resource-shared Digital Processing-in-memory Unit for Edge-AI Workloads", "comment": null, "summary": "Processing-in-memory (PIM) has emerged as the go to solution for addressing\nthe von Neumann bottleneck in edge AI accelerators. However, state-of-the-art\n(SoTA) digital PIM approaches suffer from low compute density, primarily due to\nthe use of bulky bit cells and transistor-heavy adder trees, which impose\nlimitations on macro scalability and energy efficiency. This work introduces\nRes-DPU, a resource-shared digital PIM unit, with a dual-port 5T SRAM latch and\nshared 2T AND compute logic. This reflects the per-bit multiplication cost to\njust 5.25T and reduced the transistor count of the PIM array by up to 56% over\nthe SoTA works. Furthermore, a Transistor-Reduced 2D Interspersed Adder Tree\n(TRAIT) with FA-7T and PG-FA-26T helps reduce the power consumption of the\nadder tree by up to 21.35% and leads to improved energy efficiency by 59%\ncompared to conventional 28T RCA designs. We propose a Cycle-controlled\nIterative Approximate-Accurate Multiplication (CIA2M) approach, enabling\nrun-time accuracy-latency trade-offs without requiring error-correction\ncircuitry. The 16 KB REP-DPIM macro achieves 0.43 TOPS throughput and 87.22\nTOPS/W energy efficiency in TSMC 65nm CMOS, with 96.85% QoR for ResNet-18 or\nVGG-16 on CIFAR-10, including 30% pruning. The proposed results establish a\nRes-DPU module for highly scalable and energy-efficient real-time edge AI\naccelerators."}
{"id": "2510.19577", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.19577", "abs": "https://arxiv.org/abs/2510.19577", "authors": ["Zuoming Fu", "Alex Manley", "Mohammad Alian"], "title": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration", "comment": "Accepted by CAMS25, October, 2025, Seoul, Republic of Korea", "summary": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction."}
{"id": "2510.18982", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18982", "abs": "https://arxiv.org/abs/2510.18982", "authors": ["Arpan Mukherjee", "Marcello Bullo", "Debabrota Basu", "Deniz Gündüz"], "title": "Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality", "comment": null, "summary": "While test-time scaling with verification has shown promise in improving the\nperformance of large language models (LLMs), the role of the verifier and its\nimperfections remain underexplored. The effect of verification manifests\nthrough interactions of three quantities: (i) the generator's coverage, (ii)\nthe verifier's region of convergence (ROC), and (iii) the sampling algorithm's\nsub-optimality. Though recent studies capture subsets of these factors, a\nunified framework quantifying the geometry of their interplay is missing. We\nframe verifiable test-time scaling as a transport problem. This characterizes\nthe interaction of coverage, ROC, and sub-optimality, and uncovers that the\nsub-optimality--coverage curve exhibits three regimes. A transport regime --\nwhere sub-optimality increases with coverage, a policy improvement regime --\nwhere sub-optimality may decrease with coverage, depending on the verifier's\nROC, and a saturation regime -- where sub-optimality plateaus, unaffected by\ncoverage. We further propose and analyze two classes of sampling algorithms --\nsequential and batched, and examine how their computational complexities shape\nthese trade-offs. Empirical results with Qwen, Llama, and Gemma models\ncorroborate our theoretical findings."}
{"id": "2510.18990", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.18990", "abs": "https://arxiv.org/abs/2510.18990", "authors": ["Thomas Hofweber", "Jefrey Bergl", "Ian Reyes", "Amir Sadovnik"], "title": "The Black Tuesday Attack: how to crash the stock market with adversarial examples to financial forecasting models", "comment": "15 pages, 2 figures", "summary": "We investigate and defend the possibility of causing a stock market crash via\nsmall manipulations of individual stock values that together realize an\nadversarial example to financial forecasting models, causing these models to\nmake the self-fulfilling prediction of a crash. Such a crash triggered by an\nadversarial example would likely be hard to detect, since the model's\npredictions would be accurate and the interventions that would cause it are\nminor. This possibility is a major risk to financial stability and an\nopportunity for hostile actors to cause great economic damage to an adversary.\nThis threat also exists against individual stocks and the corresponding\nvaluation of individual companies. We outline how such an attack might proceed,\nwhat its theoretical basis is, how it can be directed towards a whole economy\nor an individual company, and how one might defend against it. We conclude that\nthis threat is vastly underappreciated and requires urgent research on how to\ndefend against it."}
{"id": "2510.18988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18988", "abs": "https://arxiv.org/abs/2510.18988", "authors": ["Silas Ruhrberg Estévez", "Nicolás Astorga", "Mihaela van der Schaar"], "title": "Timely Clinical Diagnosis through Active Test Selection", "comment": "None", "summary": "There is growing interest in using machine learning (ML) to support clinical\ndiag- nosis, but most approaches rely on static, fully observed datasets and\nfail to reflect the sequential, resource-aware reasoning clinicians use in\npractice. Diagnosis remains complex and error prone, especially in\nhigh-pressure or resource-limited settings, underscoring the need for\nframeworks that help clinicians make timely and cost-effective decisions. We\npropose ACTMED (Adaptive Clinical Test selection via Model-based Experimental\nDesign), a diagnostic framework that integrates Bayesian Experimental Design\n(BED) with large language models (LLMs) to better emulate real-world diagnostic\nreasoning. At each step, ACTMED selects the test expected to yield the greatest\nreduction in diagnostic uncertainty for a given patient. LLMs act as flexible\nsimulators, generating plausible patient state distributions and supporting\nbelief updates without requiring structured, task-specific training data.\nClinicians can remain in the loop; reviewing test suggestions, interpreting\nintermediate outputs, and applying clinical judgment throughout. We evaluate\nACTMED on real-world datasets and show it can optimize test selection to\nimprove diagnostic accuracy, interpretability, and resource use. This\nrepresents a step to- ward transparent, adaptive, and clinician-aligned\ndiagnostic systems that generalize across settings with reduced reliance on\ndomain-specific data."}
{"id": "2510.19026", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19026", "abs": "https://arxiv.org/abs/2510.19026", "authors": ["Behnam Rezaei Bezanjani", "Seyyed Hamid Ghafouri", "Reza Gholamrezaei"], "title": "Fusion of Machine Learning and Blockchain-based Privacy-Preserving Approach for Health Care Data in the Internet of Things", "comment": "28 pages", "summary": "In recent years, the rapid integration of Internet of Things (IoT) devices\ninto the healthcare sector has brought about revolutionary advancements in\npatient care and data management. While these technological innovations hold\nimmense promise, they concurrently raise critical security concerns,\nparticularly in safeguarding medical data against potential cyber threats. The\nsensitive nature of health-related information requires robust measures to\nensure the confidentiality, integrity, and availability of patient data in\nIoT-enabled medical environments. Addressing the imperative need for enhanced\nsecurity in IoT-based healthcare systems, we propose a comprehensive method\nencompassing three distinct phases. In the first phase, we implement\nBlockchain-Enabled Request and Transaction Encryption to strengthen data\ntransaction security, providing an immutable and transparent framework. In the\nsecond phase, we introduce a Request Pattern Recognition Check that leverages\ndiverse data sources to identify and block potential unauthorized access\nattempts. Finally, the third phase incorporates Feature Selection and a BiLSTM\nnetwork to enhance the accuracy and efficiency of intrusion detection using\nadvanced machine learning techniques. We compared the simulation results of the\nproposed method with three recent related methods: AIBPSF-IoMT, OMLIDS-PBIoT,\nand AIMMFIDS. The evaluation criteria include detection rate, false alarm rate,\nprecision, recall, and accuracy - crucial benchmarks for assessing the overall\nperformance of intrusion detection systems. Our findings show that the proposed\nmethod outperforms existing approaches across all evaluated criteria,\ndemonstrating its effectiveness in improving the security of IoT-based\nhealthcare systems."}
{"id": "2510.19050", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19050", "abs": "https://arxiv.org/abs/2510.19050", "authors": ["Wenqian Ye", "Guangtao Zheng", "Aidong Zhang"], "title": "Rectifying Shortcut Behaviors in Preference-based Reward Learning", "comment": "NeurIPS 2025", "summary": "In reinforcement learning from human feedback, preference-based reward models\nplay a central role in aligning large language models to human-aligned\nbehavior. However, recent studies show that these models are prone to reward\nhacking and often fail to generalize well due to over-optimization. They\nachieve high reward scores by exploiting shortcuts, that is, exploiting\nspurious features (e.g., response verbosity, agreeable tone, or sycophancy)\nthat correlate with human preference labels in the training data rather than\ngenuinely reflecting the intended objectives. In this paper, instead of probing\nthese issues one at a time, we take a broader view of the reward hacking\nproblem as shortcut behaviors and introduce a principled yet flexible approach\nto mitigate shortcut behaviors in preference-based reward learning. Inspired by\nthe invariant theory in the kernel perspective, we propose Preference-based\nReward Invariance for Shortcut Mitigation (PRISM), which learns group-invariant\nkernels with feature maps in a closed-form learning objective. Experimental\nresults in several benchmarks show that our method consistently improves the\naccuracy of the reward model on diverse out-of-distribution tasks and reduces\nthe dependency on shortcuts in downstream policy models, establishing a robust\nframework for preference-based alignment."}
{"id": "2510.19121", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19121", "abs": "https://arxiv.org/abs/2510.19121", "authors": ["Behnam Seyedi", "Octavian Postolache"], "title": "Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method", "comment": "24 pages", "summary": "The rapid growth of the Internet of Things (IoT) has transformed industries\nby enabling seamless data exchange among connected devices. However, IoT\nnetworks remain vulnerable to security threats such as denial of service (DoS)\nattacks, anomalous traffic, and data manipulation due to decentralized\narchitectures and limited resources. To address these issues, this paper\nproposes an advanced anomaly detection framework with three main phases. First,\ndata preprocessing is performed using the Median KS Test to remove noise,\nhandle missing values, and balance datasets for cleaner input. Second, a\nfeature selection phase employs a Genetic Algorithm combined with eagle\ninspired search strategies to identify the most relevant features, reduce\ndimensionality, and improve efficiency without sacrificing accuracy. Finally,\nan ensemble classifier integrates Decision Tree, Random Forest, and XGBoost\nalgorithms to achieve accurate and reliable anomaly detection. The proposed\nmodel demonstrates high adaptability and scalability across diverse IoT\nenvironments. Experimental results show that it outperforms existing methods by\nachieving 98 percent accuracy, 95 percent detection rate, and reductions in\nfalse positive (10 percent) and false negative (5 percent) rates. These results\nconfirm the framework effectiveness and robustness in improving IoT network\nsecurity against evolving cyber threats."}
{"id": "2510.19055", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.19055", "abs": "https://arxiv.org/abs/2510.19055", "authors": ["Brandon James Carone", "Iran R. Roman", "Pablo Ripollés"], "title": "The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS", "comment": "5 pages, 2 figures, 2 tables", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated capabilities in\naudio understanding, but current evaluations may obscure fundamental weaknesses\nin relational reasoning. We introduce the Music Understanding and Structural\nEvaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to\nprobe fundamental music perception skills. We evaluate four SOTA models (Gemini\nPro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human\nbaseline (N=200). Our results reveal a wide variance in SOTA capabilities and a\npersistent gap with human experts. While Gemini Pro succeeds on basic\nperception, Qwen and Audio Flamingo 3 perform at or near chance, exposing\nsevere perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)\nprompting provides inconsistent, often detrimental results. Our work provides a\ncritical tool for evaluating invariant musical representations and driving\ndevelopment of more robust AI systems."}
{"id": "2510.19145", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19145", "abs": "https://arxiv.org/abs/2510.19145", "authors": ["Sanskar Amgain", "Daniel Lobo", "Atri Chatterjee", "Swarup Bhunia", "Fnu Suya"], "title": "HAMLOCK: HArdware-Model LOgically Combined attacK", "comment": null, "summary": "The growing use of third-party hardware accelerators (e.g., FPGAs, ASICs) for\ndeep neural networks (DNNs) introduces new security vulnerabilities.\nConventional model-level backdoor attacks, which only poison a model's weights\nto misclassify inputs with a specific trigger, are often detectable because the\nentire attack logic is embedded within the model (i.e., software), creating a\ntraceable layer-by-layer activation path.\n  This paper introduces the HArdware-Model Logically Combined Attack (HAMLOCK),\na far stealthier threat that distributes the attack logic across the\nhardware-software boundary. The software (model) is now only minimally altered\nby tuning the activations of few neurons to produce uniquely high activation\nvalues when a trigger is present. A malicious hardware Trojan detects those\nunique activations by monitoring the corresponding neurons' most significant\nbit or the 8-bit exponents and triggers another hardware Trojan to directly\nmanipulate the final output logits for misclassification.\n  This decoupled design is highly stealthy, as the model itself contains no\ncomplete backdoor activation path as in conventional attacks and hence, appears\nfully benign. Empirically, across benchmarks like MNIST, CIFAR10, GTSRB, and\nImageNet, HAMLOCK achieves a near-perfect attack success rate with a negligible\nclean accuracy drop. More importantly, HAMLOCK circumvents the state-of-the-art\nmodel-level defenses without any adaptive optimization. The hardware Trojan is\nalso undetectable, incurring area and power overheads as low as 0.01%, which is\neasily masked by process and environmental noise. Our findings expose a\ncritical vulnerability at the hardware-software interface, demanding new\ncross-layer defenses against this emerging threat."}
{"id": "2510.18893", "categories": ["cs.DC", "cs.AI", "cs.SE", "I.2.11; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.18893", "abs": "https://arxiv.org/abs/2510.18893", "authors": ["Sergey Pugachev"], "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "comment": "11 pages, 3 figures", "summary": "Multi-agent LLM systems fail to realize parallel speedups due to costly\ncoordination. We present CodeCRDT, an observation-driven coordination pattern\nwhere agents coordinate by monitoring a shared state with observable updates\nand deterministic convergence, rather than explicit message passing. Using\nConflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,\nconflict-free concurrent code generation with strong eventual consistency.\nEvaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits\nand trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on\nothers, and 100% convergence with zero merge failures. The study formalizes\nobservation-driven coordination for stochastic LLM agents, revealing semantic\nconflict rates (5-10%) and quality-performance tradeoffs, and provides\nempirical characterization of when parallel coordination succeeds versus fails\nbased on task structure."}
{"id": "2510.19139", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19139", "abs": "https://arxiv.org/abs/2510.19139", "authors": ["Sohyeon Jeon", "Hyung-Chul Lee"], "title": "A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist", "comment": null, "summary": "Despite the rapid expansion of Large Language Models (LLMs) in healthcare,\nthe ability of these systems to assess clinical trial reporting according to\nCONSORT standards remains unclear, particularly with respect to their cognitive\nand reasoning strategies. This study applies a behavioral and metacognitive\nanalytic approach with expert-validated data, systematically comparing two\nrepresentative LLMs under three prompt conditions. Clear differences emerged in\nhow the models approached various CONSORT items, and prompt types, including\nshifts in reasoning style, explicit uncertainty, and alternative\ninterpretations shaped response patterns. Our results highlight the current\nlimitations of these systems in clinical compliance automation and underscore\nthe importance of understanding their cognitive adaptations and strategic\nbehavior in developing more explainable and reliable medical AI."}
{"id": "2510.19169", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19169", "abs": "https://arxiv.org/abs/2510.19169", "authors": ["Thomas Wang", "Haowen Li"], "title": "OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform", "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications, safeguarding them against unsafe, malicious, or\nprivacy-violating content is critically important. We present OpenGuardrails,\nthe first open-source project to provide both a context-aware safety and\nmanipulation detection model and a deployable platform for comprehensive AI\nguardrails. OpenGuardrails protects against content-safety risks,\nmodel-manipulation attacks (e.g., prompt injection, jailbreaking,\ncode-interpreter abuse, and the generation/execution of malicious code), and\ndata leakage. Content-safety and model-manipulation detection are implemented\nby a unified large model, while data-leakage identification and redaction are\nperformed by a separate lightweight NER pipeline (e.g., Presidio-style models\nor regex-based detectors). The system can be deployed as a security gateway or\nan API-based service, with enterprise-grade, fully private deployment options.\nOpenGuardrails achieves state-of-the-art (SOTA) performance on safety\nbenchmarks, excelling in both prompt and response classification across\nEnglish, Chinese, and multilingual tasks. All models are released under the\nApache 2.0 license for public use."}
{"id": "2510.18897", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators."}
{"id": "2510.19176", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19176", "abs": "https://arxiv.org/abs/2510.19176", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "title": "The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models", "comment": "Accepted by NeurIPS'25 Efficient Reasoning Workshop", "summary": "Reasoning models have demonstrated exceptional performance in tasks such as\nmathematics and logical reasoning, primarily due to their ability to engage in\nstep-by-step thinking during the reasoning process. However, this often leads\nto overthinking, resulting in unnecessary computational overhead. To address\nthis issue, Mode Selection aims to automatically decide between Long-CoT\n(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking\nmode. Simultaneously, Early Exit determines the optimal stopping point during\nthe iterative reasoning process. Both methods seek to reduce the computational\nburden. In this paper, we first identify Mode Selection as a more challenging\nvariant of the Early Exit problem, as they share similar objectives but differ\nin decision timing. While Early Exit focuses on determining the best stopping\npoint for concise reasoning at inference time, Mode Selection must make this\ndecision at the beginning of the reasoning process, relying on pre-defined fake\nthoughts without engaging in an explicit reasoning process, referred to as\nzero-step thinking. Through empirical studies on nine baselines, we observe\nthat prompt-based approaches often fail due to their limited classification\ncapabilities when provided with minimal hand-crafted information. In contrast,\napproaches that leverage internal information generally perform better across\nmost scenarios but still exhibit issues with stability. Our findings indicate\nthat existing methods relying solely on the information provided by models are\ninsufficient for effectively addressing Mode Selection in scenarios with\nlimited information, highlighting the ongoing challenges of this task. Our code\nis available at https://github.com/Trae1ounG/Zero_Step_Thinking."}
{"id": "2510.19207", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19207", "abs": "https://arxiv.org/abs/2510.19207", "authors": ["Yizhu Wang", "Sizhe Chen", "Raghad Alkhudair", "Basel Alomair", "David Wagner"], "title": "Defending Against Prompt Injection with DataFilter", "comment": null, "summary": "When large language model (LLM) agents are increasingly deployed to automate\ntasks and interact with untrusted external data, prompt injection emerges as a\nsignificant security threat. By injecting malicious instructions into the data\nthat LLMs access, an attacker can arbitrarily override the original user task\nand redirect the agent toward unintended, potentially harmful actions. Existing\ndefenses either require access to model weights (fine-tuning), incur\nsubstantial utility loss (detection-based), or demand non-trivial system\nredesign (system-level). Motivated by this, we propose DataFilter, a test-time\nmodel-agnostic defense that removes malicious instructions from the data before\nit reaches the backend LLM. DataFilter is trained with supervised fine-tuning\non simulated injections and leverages both the user's instruction and the data\nto selectively strip adversarial content while preserving benign information.\nAcross multiple benchmarks, DataFilter consistently reduces the prompt\ninjection attack success rates to near zero while maintaining the LLMs'\nutility. DataFilter delivers strong security, high utility, and plug-and-play\ndeployment, making it a strong practical defense to secure black-box commercial\nLLMs against prompt injection. Our DataFilter model is released at\nhttps://huggingface.co/JoyYizhu/DataFilter for immediate use, with the code to\nreproduce our results at https://github.com/yizhu-joy/DataFilter."}
{"id": "2510.19012", "categories": ["cs.DC", "cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19012", "abs": "https://arxiv.org/abs/2510.19012", "authors": ["Ivan Borodii", "Illia Fedorovych", "Halyna Osukhivska", "Diana Velychko", "Roman Butsii"], "title": "Comparative analysis of large data processing in Apache Spark using Java, Python and Scala", "comment": "CITI 2025, 3rd International Workshop on Computer Information\n  Technologies in Industry 4.0, June 11-12, 2025, Ternopil, Ukraine. The\n  article includes 10 pages, 5 figures, 9 tables", "summary": "During the study, the results of a comparative analysis of the process of\nhandling large datasets using the Apache Spark platform in Java, Python, and\nScala programming languages were obtained. Although prior works have focused on\nindividual stages, comprehensive comparisons of full ETL workflows across\nprogramming languages using Apache Iceberg remain limited. The analysis was\nperformed by executing several operations, including downloading data from CSV\nfiles, transforming and loading it into an Apache Iceberg analytical table. It\nwas found that the performance of the Spark algorithm varies significantly\ndepending on the amount of data and the programming language used. When\nprocessing a 5-megabyte CSV file, the best result was achieved in Python: 6.71\nseconds, which is superior to Scala's score of 9.13 seconds and Java's time of\n9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming\nlanguages demonstrated similar results: the fastest performance was showed in\nPython: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56\nseconds, respectively. When performing a more complex operation that involved\ncombining two CSV files into a single dataset for further loading into an\nApache Iceberg table, Scala demonstrated the highest performance, at 374.42\nseconds. Java processing was completed in 379.8 seconds, while Python was the\nleast efficient, with a runtime of 398.32 seconds. It follows that the\nprogramming language significantly affects the efficiency of data processing by\nthe Apache Spark algorithm, with Scala and Java being more productive for\nprocessing large amounts of data and complex operations, while Python\ndemonstrates an advantage in working with small amounts of data. The results\nobtained can be useful for optimizing data handling processes depending on\nspecific performance requirements and the amount of information being\nprocessed."}
{"id": "2510.19205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19205", "abs": "https://arxiv.org/abs/2510.19205", "authors": ["Yaoyao Qian", "Yuanli Wang", "Jinda Zhang", "Yun Zong", "Meixu Chen", "Hanhan Zhou", "Jindan Huang", "Yifan Zeng", "Xinyu Hu", "Chan Hee Song", "Danqing Zhang"], "title": "WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Multi-Turn Interactions in Large Language Models", "summary": "Current evaluation of web agents largely reduces to binary success metrics or\nconformity to a single reference trajectory, ignoring the structural diversity\npresent in benchmark datasets. We present WebGraphEval, a framework that\nabstracts trajectories from multiple agents into a unified, weighted action\ngraph. This representation is directly compatible with benchmarks such as\nWebArena, leveraging leaderboard runs and newly collected trajectories without\nmodifying environments. The framework canonically encodes actions, merges\nrecurring behaviors, and applies structural analyses including reward\npropagation and success-weighted edge statistics. Evaluations across thousands\nof trajectories from six web agents show that the graph abstraction captures\ncross-model regularities, highlights redundancy and inefficiency, and\nidentifies critical decision points overlooked by outcome-based metrics. By\nframing web interaction as graph-structured data, WebGraphEval establishes a\ngeneral methodology for multi-path, cross-agent, and efficiency-aware\nevaluation of web agents."}
{"id": "2510.19264", "categories": ["cs.CR", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19264", "abs": "https://arxiv.org/abs/2510.19264", "authors": ["R. Can Aygun", "Yehuda Afek", "Anat Bremler-Barr", "Leonard Kleinrock"], "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery", "comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)", "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."}
{"id": "2510.19151", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19151", "abs": "https://arxiv.org/abs/2510.19151", "authors": ["Seri Khoury", "Manish Purohit", "Aaron Schild", "Joshua Wang"], "title": "On the Randomized Locality of Matching Problems in Regular Graphs", "comment": "DISC 2025. Abstract modified for arXiv", "summary": "The main goal in distributed symmetry-breaking is to understand the locality\nof problems; i.e., the radius of the neighborhood that a node needs to explore\nin order to arrive at its part of a global solution. In this work, we study the\nlocality of matching problems in the family of regular graphs, which is one of\nthe main benchmarks for establishing lower bounds on the locality of\nsymmetry-breaking problems, as well as for obtaining classification results.\nFor approximate matching, we develop randomized algorithms to show that $(1 +\n\\epsilon)$-approximate matching in regular graphs is truly local; i.e., the\nlocality depends only on $\\epsilon$ and is independent of all other graph\nparameters. Furthermore, as long as the degree $\\Delta$ is not very small\n(namely, as long as $\\Delta \\geq \\text{poly}(1/\\epsilon)$), this dependence is\nonly logarithmic in $1/\\epsilon$. This stands in sharp contrast to maximal\nmatching in regular graphs which requires some dependence on the number of\nnodes $n$ or the degree $\\Delta$. We show matching lower bounds for both\nresults. For maximal matching, our techniques further allow us to establish a\nstrong separation between the node-averaged complexity and worst-case\ncomplexity of maximal matching in regular graphs, by showing that the former is\nonly $O(1)$. Central to our main technical contribution is a novel\nmartingale-based analysis for the $\\approx 40$-year-old algorithm by Luby. In\nparticular, our analysis shows that applying one round of Luby's algorithm on\nthe line graph of a $\\Delta$-regular graph results in an almost\n$\\Delta/2$-regular graph."}
{"id": "2510.19261", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19261", "abs": "https://arxiv.org/abs/2510.19261", "authors": ["Marianna Molinari", "Ilaria Angela Amantea", "Marinella Quaranta", "Guido Governatori"], "title": "ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate", "comment": null, "summary": "This study examines the performance of ChatGPT with an experiment in the\nlegal domain. We compare the outcome with it a baseline using regular\nexpressions (Regex), rather than focusing solely on the assessment against\nhuman performance. The study reveals that even if ChatGPT has access to the\nnecessary knowledge and competencies, it is unable to assemble them, reason\nthrough, in a way that leads to an exhaustive result. This unveils a major\nlimitation of ChatGPT. Intelligence encompasses the ability to break down\ncomplex issues and address them according to multiple required competencies,\nproviding a unified and comprehensive solution. In the legal domain, one of the\nmost crucial tasks is reading legal decisions and extracting key passages\ncondensed from principles of law (PoLs), which are then incorporated into\nsubsequent rulings by judges or defense documents by lawyers. In performing\nthis task, artificial intelligence lacks an all-encompassing understanding and\nreasoning, which makes it inherently limited. Genuine intelligence, remains a\nuniquely human trait, at least in this particular field."}
{"id": "2510.19295", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19295", "abs": "https://arxiv.org/abs/2510.19295", "authors": ["Konstantinos A. Lizos", "Leandros Maglaras", "Elena Petrovik", "Saied M. Abd El-atty", "Georgios Tsachtsiris", "Mohamed Amine Ferrag"], "title": "Reliability and Resilience of AI-Driven Critical Network Infrastructure under Cyber-Physical Threats", "comment": "11 pages", "summary": "The increasing reliance on AI-driven 5G/6G network infrastructures for\nmission-critical services highlights the need for reliability and resilience\nagainst sophisticated cyber-physical threats. These networks are highly exposed\nto novel attack surfaces due to their distributed intelligence, virtualized\nresources, and cross-domain integration. This paper proposes a fault-tolerant\nand resilience-aware framework that integrates AI-driven anomaly detection,\nadaptive routing, and redundancy mechanisms to mitigate cascading failures\nunder cyber-physical attack conditions. A comprehensive validation is carried\nout using NS-3 simulations, where key performance indicators such as\nreliability, latency, resilience index, and packet loss rate are analyzed under\nvarious attack scenarios. The deduced results demonstrate that the proposed\nframework significantly improves fault recovery, stabilizes packet delivery,\nand reduces service disruption compared to baseline approaches."}
{"id": "2510.19225", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19225", "abs": "https://arxiv.org/abs/2510.19225", "authors": ["Yongji Wu", "Xueshen Liu", "Haizhong Zheng", "Juncheng Gu", "Beidi Chen", "Z. Morley Mao", "Arvind Krishnamurthy", "Ion Stoica"], "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs", "comment": null, "summary": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources."}
{"id": "2510.19263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19263", "abs": "https://arxiv.org/abs/2510.19263", "authors": ["Wachara Fungwacharakorn", "Gauvain Bourgne", "Ken Satoh"], "title": "An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents", "comment": "10 pages, extended version for JURIX 2025 submission", "summary": "Precedential constraint is one foundation of case-based reasoning in AI and\nLaw. It generally assumes that the underlying set of precedents must be\nconsistent. To relax this assumption, a generalized notion of the reason model\nhas been introduced. While several argumentative explanation approaches exist\nfor reasoning with precedents based on the traditional consistent reason model,\nthere has been no corresponding argumentative explanation method developed for\nthis generalized reasoning framework accommodating inconsistent precedents. To\naddress this question, this paper examines an extension of the derivation state\nargumentation framework (DSA-framework) to explain the reasoning according to\nthe generalized notion of the reason model."}
{"id": "2510.19300", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19300", "abs": "https://arxiv.org/abs/2510.19300", "authors": ["Abdollah Rahimi", "Mehdi Jafari Shahbazzadeh", "Amid Khatibi"], "title": "An Adaptive Intelligent Thermal-Aware Routing Protocol for Wireless Body Area Networks", "comment": null, "summary": "Wireless Body Area Networks (WBANs) have gained significant attention due to\ntheir applications in healthcare monitoring, sports, military communication,\nand remote patient care. These networks consist of wearable or implanted\nsensors that continuously collect and transmit physiological data, requiring\nefficient and reliable communication. However, WBANs face challenges such as\nlimited energy, dynamic topology, and sensitivity to node temperature, which\ndemand specialized routing strategies. Traditional shortest-path routing often\ncauses congestion and overheating in specific nodes, leading to early failures.\nTo address these problems, this paper proposes an intelligent temperature-aware\nand reliability-based routing approach that enhances WBAN performance. The\nproposed method works in two phases: (1) network setup and intelligent path\nselection, and (2) dynamic traffic management and hotspot avoidance. In the\nfirst phase, nodes share information such as residual energy, temperature, link\nreliability, and delay to build an optimized topology using a multi-criteria\ndecision algorithm. The second phase continuously monitors real-time conditions\nand reroutes traffic away from overheated or depleted nodes. Simulation results\nshow that the proposed approach improves throughput by 13 percent, reduces\nend-to-end delay by 10 percent, decreases energy consumption by 25 percent, and\nlowers routing load by 30 percent compared to existing methods."}
{"id": "2510.19262", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19262", "abs": "https://arxiv.org/abs/2510.19262", "authors": ["Heng Xu", "Zhiwei Yu", "Chengze Du", "Ying Zhou", "Letian Li", "Haojie Wang", "Weiqiang Cheng", "Jialong Li"], "title": "RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training", "comment": null, "summary": "Training Mixture-of-Experts (MoE) models introduces sparse and highly\nimbalanced all-to-all communication that dominates iteration time. Conventional\nload-balancing methods fail to exploit the deterministic topology of Rail\narchitectures, leaving multi-NIC bandwidth underutilized. We present RailS, a\ndistributed load-balancing framework that minimizes all-to-all completion time\nin MoE training. RailS leverages the Rail topology's symmetry to prove that\nuniform sending ensures uniform receiving, transforming global coordination\ninto local scheduling. Each node independently executes a Longest Processing\nTime First (LPT) spraying scheduler to proactively balance traffic using local\ninformation. RailS activates N parallel rails for fine-grained, topology-aware\nmultipath transmission. Across synthetic and real-world MoE workloads, RailS\nimproves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For\nMixtral workloads, it shortens iteration time by 18%--40% and achieves\nnear-optimal load balance, fully exploiting architectural parallelism in\ndistributed training."}
{"id": "2510.19299", "categories": ["cs.AI", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.19299", "abs": "https://arxiv.org/abs/2510.19299", "authors": ["Philipp J. Schneider", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties", "comment": null, "summary": "Can large language model (LLM) agents reproduce the complex social dynamics\nthat characterize human online behavior -- shaped by homophily, reciprocity,\nand social validation -- and what memory and learning mechanisms enable such\ndynamics to emerge? We present a multi-agent LLM simulation framework in which\nagents repeatedly interact, evaluate one another, and adapt their behavior\nthrough in-context learning accelerated by a coaching signal. To model human\nsocial behavior, we design behavioral reward functions that capture core\ndrivers of online engagement, including social interaction, information\nseeking, self-presentation, coordination, and emotional support. These rewards\nalign agent objectives with empirically observed user motivations, enabling the\nstudy of how network structures and group formations emerge from individual\ndecision-making. Our experiments show that coached LLM agents develop stable\ninteraction patterns and form emergent social ties, yielding network structures\nthat mirror properties of real online communities. By combining behavioral\nrewards with in-context adaptation, our framework establishes a principled\ntestbed for investigating collective dynamics in LLM populations and reveals\nhow artificial agents may approximate or diverge from human-like social\nbehavior."}
{"id": "2510.19303", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19303", "abs": "https://arxiv.org/abs/2510.19303", "authors": ["Petar Radanliev"], "title": "Collaborative penetration testing suite for emerging generative AI algorithms", "comment": null, "summary": "Problem Space: AI Vulnerabilities and Quantum Threats Generative AI\nvulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum\nthreats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative\nAI models against classical and quantum cyberattacks. Proposed Solution\nCollaborative Penetration Testing Suite Five Integrated Components: DAST SAST\nOWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with\nCI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs.\nQuantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations\nAdversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow\nfor AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities\nidentified across test environments. 70% reduction in high-severity issues\nwithin 2 weeks. 90% resolution efficiency for blockchain-logged\nvulnerabilities. Quantum-resistant cryptography maintained 100% integrity in\ntests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum\nCryptography AI Red Teaming."}
{"id": "2510.19301", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19301", "abs": "https://arxiv.org/abs/2510.19301", "authors": ["Ziheng Deng", "Xue Liu", "Jiantong Jiang", "Yankai Li", "Qingxu Deng", "Xiaochun Yang"], "title": "FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems", "comment": "Accepted for ICDE 2026", "summary": "The Viterbi algorithm is a key operator for structured sequence inference in\nmodern data systems, with applications in trajectory analysis, online\nrecommendation, and speech recognition. As these workloads increasingly migrate\nto resource-constrained edge platforms, standard Viterbi decoding remains\nmemory-intensive and computationally inflexible. Existing methods typically\ntrade decoding time for space efficiency, but often incur significant runtime\noverhead and lack adaptability to various system constraints. This paper\npresents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly\nViterbi decoding operator that enhances adaptability and resource efficiency.\nFLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning\nand parallelization techniques to enhance both time and memory efficiency,\nmaking it well-suited for resource-constrained data systems.To further decouple\nspace complexity from the hidden state space size, we present FLASH-BS Viterbi,\na dynamic beam search variant built on a memory-efficient data structure. Both\nproposed algorithms exhibit strong adaptivity to diverse deployment scenarios\nby dynamically tuning internal parameters.To ensure practical deployment on\nedge devices, we also develop FPGA-based hardware accelerators for both\nalgorithms, demonstrating high throughput and low resource usage. Extensive\nexperiments show that our algorithms consistently outperform existing baselines\nin both decoding time and memory efficiency, while preserving adaptability and\nhardware-friendly characteristics essential for modern data systems. All codes\nare publicly available at https://github.com/Dzh-16/FLASH-Viterbi."}
{"id": "2510.19314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19314", "abs": "https://arxiv.org/abs/2510.19314", "authors": ["Jinwu Hu", "Zihao Lian", "Zhiquan Wen", "Chenghao Li", "Guohao Chen", "Xutao Wen", "Bin Xiao", "Mingkui Tan"], "title": "Continual Knowledge Adaptation for Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Reinforcement Learning enables agents to learn optimal behaviors through\ninteractions with environments. However, real-world environments are typically\nnon-stationary, requiring agents to continuously adapt to new tasks and\nchanging conditions. Although Continual Reinforcement Learning facilitates\nlearning across multiple tasks, existing methods often suffer from catastrophic\nforgetting and inefficient knowledge utilization. To address these challenges,\nwe propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),\nwhich enables the accumulation and effective utilization of historical\nknowledge. Specifically, we introduce a Continual Knowledge Adaptation\nstrategy, which involves maintaining a task-specific knowledge vector pool and\ndynamically using historical knowledge to adapt the agent to new tasks. This\nprocess mitigates catastrophic forgetting and enables efficient knowledge\ntransfer across tasks by preserving and adapting critical model parameters.\nAdditionally, we propose an Adaptive Knowledge Merging mechanism that combines\nsimilar knowledge vectors to address scalability challenges, reducing memory\nrequirements while ensuring the retention of essential knowledge. Experiments\non three benchmarks demonstrate that the proposed CKA-RL outperforms\nstate-of-the-art methods, achieving an improvement of 4.20% in overall\nperformance and 8.02% in forward transfer. The source code is available at\nhttps://github.com/Fhujinwu/CKA-RL."}
{"id": "2510.19324", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19324", "abs": "https://arxiv.org/abs/2510.19324", "authors": ["Loay Abdelrazek", "Leyli Karaçay", "Marin Orlic"], "title": "Authorization of Knowledge-base Agents in an Intent-based Management Function", "comment": null, "summary": "As networks move toward the next-generation 6G, Intent-based Management (IbM)\nsystems are increasingly adopted to simplify and automate network management by\ntranslating high-level intents into low-level configurations. Within these\nsystems, agents play a critical role in monitoring current state of the\nnetwork, gathering data, and enforcing actions across the network to fulfill\nthe intent. However, ensuring secure and fine-grained authorization of agents\nremains a significant challenge, especially in dynamic and multi-tenant\nenvironments. Traditional models such as Role-Based Access Control (RBAC),\nAttribute-Based Access Control (ABAC) and Relational-Based Access Control\n(RelBAC) often lack the flexibility to accommodate the evolving context and\ngranularity required by intentbased operations. In this paper, we propose an\nenhanced authorization framework that integrates contextual and functional\nattributes with agent roles to achieve dynamic, policy-driven access control.\nBy analyzing agent functionalities, our approach ensures that agents are\ngranted only the minimal necessary privileges towards knowledge graphs."}
{"id": "2510.19470", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19470", "abs": "https://arxiv.org/abs/2510.19470", "authors": ["Weihao Yang", "Hao Huang", "Donglei Wu", "Ningke Li", "Yanqi Pan", "Qiyang Zheng", "Wen Xia", "Shiyi Li", "Qiang Wang"], "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission", "comment": null, "summary": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large\nmodels. However, the rapidly growing scale outpaces model training on a single\nDC, driving a shift toward a more flexible, cross-DC training paradigm. Under\nthis, Expert Parallelism (EP) of MoE faces significant scalability issues due\nto the limited cross-DC bandwidth. Specifically, existing EP optimizations\nattempt to overlap data communication and computation, which has little benefit\nin low-bandwidth scenarios due to a much longer data communication time.\nTherefore, the trends of cross-DC EP scaling is fast becoming a critical\nroadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize\nEP under constrained bandwidth. Our key idea is to dynamically transform the\nspatial placement of experts to reduce data communication traffic and\nfrequency, thereby minimizing EP's communication overheads. However, it is\nnon-trivial to find the optimal solution because it complicates the original\ncommunication pattern by mixing data and expert communication. We therefore\nbuild a stream-based model to determine the optimal transmission ratio. Guided\nby this, we incorporate two techniques: (1) domain-based partition to construct\nthe mapping between hybrid patterns and specific communication topology at GPU\nlevel, and (2) parameter-efficient migration to further refine this topology by\nreducing expert transmission overhead and enlarging the domain size. Combining\nall these designs, HybridEP can be considered as a more general EP with better\nscalability. Experimental results show that HybridEP outperforms existing\nstate-of-the-art MoE training systems by up to 5.6x under constrained\nbandwidth. We further compare HybridEP and EP on large-scale simulations.\nHybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths."}
{"id": "2510.19423", "categories": ["cs.AI", "I.2.0; I.2.1; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.19423", "abs": "https://arxiv.org/abs/2510.19423", "authors": ["Jia-Kai Dong", "I-Wei Huang", "Chun-Tin Wu", "Yi-Tien Tsai"], "title": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration", "comment": "under ACL Rolling Review 2025", "summary": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,\nend-to-end tool orchestration by LLM agents in a hierarchical Model-Context\nProtocol (MCP) ecosystem. Existing benchmarks often evaluate tools in\nisolation, ignoring challenges such as functional overlap and cross-server\norchestration, leading to overly optimistic assessments. MSC-Bench addresses\nthese gaps by constructing ground truth through 'equal function sets', allowing\nobjective metrics such as F1 score and reducing the dependency on\nLLM-as-a-judge evaluation. Organized as a five-level curriculum, it\nsystematically tests agent capabilities from single-tool orchestration to\ncomplex cross-server planning, and robustness to out-of-scope requests.\nExperiments reveal that rigid hierarchies can hinder performance without\nco-designed strategies, and even state-of-the-art agents exhibit systemic\nweaknesses in robustness. MSC-Bench provides a diagnostic framework to expose\nthese limitations and guide the development of more capable and efficient\ntool-using agents. The benchmark and resources are publicly available at\nhttps://github.com/snooow1029/MSC_Bench."}
{"id": "2510.19390", "categories": ["cs.CR", "cs.ET", "math.OC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.19390", "abs": "https://arxiv.org/abs/2510.19390", "authors": ["Max O. Al-Hasso", "Marko von der Leyen"], "title": "A Probabilistic Computing Approach to the Closest Vector Problem for Lattice-Based Factoring", "comment": "18 pages, 5 figures", "summary": "The closest vector problem (CVP) is a fundamental optimization problem in\nlattice-based cryptography and its conjectured hardness underpins the security\nof lattice-based cryptosystems. Furthermore, Schnorr's lattice-based factoring\nalgorithm reduces integer factoring (the foundation of current cryptosystems,\nincluding RSA) to the CVP. Recent work has investigated the inclusion of a\nheuristic CVP approximation `refinement' step in the lattice-based factoring\nalgorithm, using quantum variational algorithms to perform the heuristic\noptimization. This coincides with the emergence of probabilistic computing as a\nhardware accelerator for randomized algorithms including tasks in combinatorial\noptimization. In this work we investigate the application of probabilistic\ncomputing to the heuristic optimization task of CVP approximation refinement in\nlattice-based factoring. We present the design of a probabilistic computing\nalgorithm for this task, a discussion of `prime lattice' parameters, and\nexperimental results showing the efficacy of probabilistic computing for\nsolving the CVP as well as its efficacy as a subroutine for lattice-based\nfactoring. The main results found that (a) this approach is capable of finding\nthe maximal available CVP approximation refinement in time linear in problem\nsize and (b) probabilistic computing used in conjunction with the lattice\nparameters presented can find the composite prime factors of a semiprime number\nusing up to 100x fewer lattice instances than similar quantum and classical\nmethods."}
{"id": "2510.19617", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19617", "abs": "https://arxiv.org/abs/2510.19617", "authors": ["Eric Ding"], "title": "Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud", "comment": null, "summary": "Collaborative Machine Learning is a paradigm in the field of distributed\nmachine learning, designed to address the challenges of data privacy,\ncommunication overhead, and model heterogeneity. There have been significant\nadvancements in optimization and communication algorithm design and ML hardware\nthat enables fair, efficient and secure collaborative ML training. However,\nless emphasis is put on collaborative ML infrastructure development. Developers\nand researchers often build server-client systems for a specific collaborative\nML use case, which is not scalable and reusable. As the scale of collaborative\nML grows, the need for a scalable, efficient, and ideally multi-tenant resource\nmanagement system becomes more pressing. We propose a novel system, Propius,\nthat can adapt to the heterogeneity of client machines, and efficiently manage\nand control the computation flow between ML jobs and edge resources in a\nscalable fashion. Propius is comprised of a control plane and a data plane. The\ncontrol plane enables efficient resource sharing among multiple collaborative\nML jobs and supports various resource sharing policies, while the data plane\nimproves the scalability of collaborative ML model sharing and result\ncollection. Evaluations show that Propius outperforms existing resource\nmanagement techniques and frameworks in terms of resource utilization (up to\n$1.88\\times$), throughput (up to $2.76$), and job completion time (up to\n$1.26\\times$)."}
{"id": "2510.19429", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19429", "abs": "https://arxiv.org/abs/2510.19429", "authors": ["Wonje Choi", "Jooyoung Kim", "Honguk Woo"], "title": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning", "comment": "Accepted at NeurIPS 2025", "summary": "We address the challenge of adopting language models (LMs) for embodied tasks\nin dynamic environments, where online access to large-scale inference engines\nor symbolic planners is constrained due to latency, connectivity, and resource\nlimitations. To this end, we present NeSyPr, a novel embodied reasoning\nframework that compiles knowledge via neurosymbolic proceduralization, thereby\nequipping LM-based agents with structured, adaptive, and timely reasoning\ncapabilities. In NeSyPr, task-specific plans are first explicitly generated by\na symbolic tool leveraging its declarative knowledge. These plans are then\ntransformed into composable procedural representations that encode the plans'\nimplicit production rules, enabling the resulting composed procedures to be\nseamlessly integrated into the LM's inference process. This neurosymbolic\nproceduralization abstracts and generalizes multi-step symbolic structured\npath-finding and reasoning into single-step LM inference, akin to human\nknowledge compilation. It supports efficient test-time inference without\nrelying on external symbolic guidance, making it well suited for deployment in\nlatency-sensitive and resource-constrained physical systems. We evaluate NeSyPr\non the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating\nits efficient reasoning capabilities over large-scale reasoning models and a\nsymbolic planner, while using more compact LMs."}
{"id": "2510.19418", "categories": ["cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19418", "abs": "https://arxiv.org/abs/2510.19418", "authors": ["Mete Harun Akcay", "Buse Gul Atli", "Siddharth Prakash Rao", "Alexandros Bakas"], "title": "From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data", "comment": "10 pages, 3 figures, 6 tables. In submission", "summary": "As the volume of stored data continues to grow, identifying and protecting\nsensitive information within large repositories becomes increasingly\nchallenging, especially when shared with multiple users with different roles\nand permissions. This work presents a system architecture for trusted data\nsharing with policy-driven access control, enabling selective protection of\nsensitive regions while maintaining scalability. The proposed architecture\nintegrates four core modules that combine automated detection of sensitive\nregions, post-correction, key management, and access control. Sensitive regions\nare secured using a hybrid scheme that employs symmetric encryption for\nefficiency and Attribute-Based Encryption for policy enforcement. The system\nsupports efficient key distribution and isolates key storage to strengthen\noverall security. To demonstrate its applicability, we evaluate the system on\nvisual datasets, where Privacy-Sensitive Objects in images are automatically\ndetected, reassessed, and selectively encrypted prior to sharing in a data\nrepository. Experimental results show that our system provides effective PSO\ndetection, increases macro-averaged F1 score (5%) and mean Average Precision\n(10%), and maintains an average policy-enforced decryption time of less than 1\nsecond per image. These results demonstrate the effectiveness, efficiency and\nscalability of our proposed solution for fine-grained access control."}
{"id": "2510.19689", "categories": ["cs.DC", "cs.AI", "cs.LG", "C.2.4; H.3.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19689", "abs": "https://arxiv.org/abs/2510.19689", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Srinivas Vippagunta", "Suchitra Raman", "Shreeshankar Chatterjee", "Ju Lin", "Shang Liu", "Mary Schladenhauffen", "Jeffrey Luo", "Hailong Jiang"], "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation", "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025", "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings."}
{"id": "2510.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19562", "abs": "https://arxiv.org/abs/2510.19562", "authors": ["Runpeng Xie", "Quanwei Wang", "Hao Hu", "Zherui Zhou", "Ni Mu", "Xiyun Li", "Yiqin Yang", "Shuang Xu", "Qianchuan Zhao", "Bo XU"], "title": "DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning", "comment": "Website at:\n  https://github.com/RunpengXie/Distributional-Aligned-Learning", "summary": "Comprehending natural language and following human instructions are critical\ncapabilities for intelligent agents. However, the flexibility of linguistic\ninstructions induces substantial ambiguity across language-conditioned tasks,\nseverely degrading algorithmic performance. To address these limitations, we\npresent a novel method named DAIL (Distributional Aligned Learning), featuring\ntwo key components: distributional policy and semantic alignment. Specifically,\nwe provide theoretical results that the value distribution estimation mechanism\nenhances task differentiability. Meanwhile, the semantic alignment module\ncaptures the correspondence between trajectories and linguistic instructions.\nExtensive experimental results on both structured and visual observation\nbenchmarks demonstrate that DAIL effectively resolves instruction ambiguities,\nachieving superior performance to baseline methods. Our implementation is\navailable at https://github.com/RunpengXie/Distributional-Aligned-Learning."}
{"id": "2510.19420", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19420", "abs": "https://arxiv.org/abs/2510.19420", "authors": ["Chengcan Wu", "Zhixin Zhang", "Mingqian Xu", "Zeming Wei", "Meng Sun"], "title": "Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation", "comment": null, "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a\npopular paradigm of AI applications. However, trustworthiness issues in MAS\nremain a critical concern. Unlike challenges in single-agent systems, MAS\ninvolve more complex communication processes, making them susceptible to\ncorruption attacks. To mitigate this issue, several defense mechanisms have\nbeen developed based on the graph representation of MAS, where agents represent\nnodes and communications form edges. Nevertheless, these methods predominantly\nfocus on static graph defense, attempting to either detect attacks in a fixed\ngraph structure or optimize a static topology with certain defensive\ncapabilities. To address this limitation, we propose a dynamic defense paradigm\nfor MAS graph structures, which continuously monitors communication within the\nMAS graph, then dynamically adjusts the graph topology, accurately disrupts\nmalicious communications, and effectively defends against evolving and diverse\ndynamic attacks. Experimental results in increasingly complex and dynamic MAS\nenvironments demonstrate that our method significantly outperforms existing MAS\ndefense mechanisms, contributing an effective guardrail for their trustworthy\napplications. Our code is available at\nhttps://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems."}
{"id": "2510.19725", "categories": ["cs.DC", "cs.NI", "C.2.m; G.2.m"], "pdf": "https://arxiv.org/pdf/2510.19725", "abs": "https://arxiv.org/abs/2510.19725", "authors": ["Jingfan Meng", "Tianji Yang", "Jun Xu"], "title": "CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing", "comment": null, "summary": "In the set reconciliation (\\textsf{SetR}) problem, two parties Alice and Bob,\nholding sets $\\mathsf{A}$ and $\\mathsf{B}$, communicate to learn the symmetric\ndifference $\\mathsf{A} \\Delta \\mathsf{B}$. In this work, we study a related but\nunder-explored problem: set intersection (\\textsf{SetX})~\\cite{Ozisik2019},\nwhere both parties learn $\\mathsf{A} \\cap \\mathsf{B}$ instead. However,\nexisting solutions typically reuse \\textsf{SetR} protocols due to the absence\nof dedicated \\textsf{SetX} protocols and the misconception that \\textsf{SetR}\nand \\textsf{SetX} have comparable costs. Observing that \\textsf{SetX} is\nfundamentally cheaper than \\textsf{SetR}, we developed a multi-round\n\\textsf{SetX} protocol that outperforms the information-theoretic lower bound\nof \\textsf{SetR} problem. In our \\textsf{SetX} protocol, Alice sends Bob a\ncompressed sensing (CS) sketch of $\\mathsf{A}$ to help Bob identify his unique\nelements (those in $\\mathsf{B \\setminus A}$). This solves the \\textsf{SetX}\nproblem, if $\\mathsf{A} \\subseteq \\mathsf{B}$. Otherwise, Bob sends a CS sketch\nof the residue (a set of elements he cannot decode) back to Alice for her to\ndecode her unique elements (those in $\\mathsf{A \\setminus B}$). As such, Alice\nand Bob communicate back and forth %with a set membership filter (SMF) of\nestimated $\\mathsf{B \\setminus A}$. Alice updates $\\mathsf{A}$ and\ncommunication repeats until both parties agrees on $\\mathsf{A} \\cap\n\\mathsf{B}$. On real world datasets, experiments show that our $\\mathsf{SetX}$\nprotocol reduces the communication cost by 8 to 10 times compared to the\nIBLT-based $\\mathsf{SetR}$ protocol."}
{"id": "2510.19631", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19631", "abs": "https://arxiv.org/abs/2510.19631", "authors": ["Yiqian Yang", "Tian Lan", "Qianghuai Jia", "Li Zhu", "Hui Jiang", "Hang Zhu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application", "comment": null, "summary": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further."}
{"id": "2510.19440", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19440", "abs": "https://arxiv.org/abs/2510.19440", "authors": ["Rundong Jiang", "Jun Hu", "Zhiyuan Xie", "Yunqi Song", "Shiyou Xu"], "title": "Transmitter Identification via Volterra Series Based Radio Frequency Fingerprint", "comment": null, "summary": "The growing number of wireless devices increases the need for secure network\naccess. Radio Frequency Fingerprinting (RFF), a physical-layer authentication\nmethod, offers a promising solution as it requires no cryptography and resists\nspoofing. However, existing RFF approaches often lack a unified theory and\neffective feature extraction. Many methods use handcrafted signal features or\ndirect neural network classification, leading to limited generalization and\ninterpretability. In this work, we model the transmitter as a black box and\nanalyze its impact on transmitted signals. By treating the deviation from an\nideal signal as hardware-induced distortion, we represent the received signal\nusing a Volterra series, using its kernels to capture linear and nonlinear\nhardware traits. To manage the high dimensionality of these kernels, we\napproximate them via wavelet decomposition and estimate coefficients through\nleast-squares fitting. The resulting wavelet coefficients provide compact yet\ninformative hardware representations, which are classified using a\ncomplex-valued neural network. Experiments on a public LoRa dataset show\nstate-of-the-art performance, with over 98% accuracy in static channels and\nabove 90% under multipath and Doppler effects. The proposed approach improves\nboth interpretability and generalization across varying channel conditions."}
{"id": "2510.19805", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.19805", "abs": "https://arxiv.org/abs/2510.19805", "authors": ["Carl-Johan Fauvelle Munck af Rosensch\"old", "Feras M. Awaysheh", "Ahmad Awad"], "title": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond", "comment": "10 pages, 5 figures, 2 algorithms, 4 tables", "summary": "In-memory key-value datastores have become indispensable building blocks of\nmodern cloud-native infrastructures, yet their evolution faces scalability,\ncompatibility, and sustainability constraints. The current literature lacks an\nexperimental evaluation of state-of-the-art tools in the domain. This study\naddressed this timely gap by benchmarking Redis alternatives and systematically\nevaluating Valkey, KeyDB, and Garnet under realistic workloads within\nKubernetes deployments. The results demonstrate clear trade-offs among the\nbenchmarked data systems. Our study presents a comprehensive performance and\nviability assessment of the emerging in-memory key-value stores. Metrics\ninclude throughput, tail latency, CPU and memory efficiency, and migration\ncomplexity. We highlight trade-offs between performance, compatibility, and\nlong-term viability, including project maturity, community support, and\nsustained development."}
{"id": "2510.19661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19661", "abs": "https://arxiv.org/abs/2510.19661", "authors": ["Xusen Guo", "Mingxing Peng", "Xixuan Hao", "Xingchen Zou", "Qiongyan Wang", "Sijie Ruan", "Yuxuan Liang"], "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing", "comment": "13 pages, 10 pages", "summary": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web."}
{"id": "2510.19462", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19462", "abs": "https://arxiv.org/abs/2510.19462", "authors": ["Zhonghao Zhan", "Amir Al Sadi", "Krinos Li", "Hamed Haddadi"], "title": "AegisMCP: Online Graph Intrusion Detection for Tool-Augmented LLMs on Edge Devices", "comment": null, "summary": "In this work, we study security of Model Context Protocol (MCP) agent\ntoolchains and their applications in smart homes. We introduce AegisMCP, a\nprotocol-level intrusion detector. Our contributions are: (i) a minimal attack\nsuite spanning instruction-driven escalation, chain-of-tool exfiltration,\nmalicious MCP server registration, and persistence; (ii) NEBULA-Schema\n(Network-Edge Behavioral Learning for Untrusted LLM Agents), a reusable\nprotocol-level instrumentation that represents MCP activity as a streaming\nheterogeneous temporal graph over agents, MCP servers, tools, devices, remotes,\nand sessions; and (iii) a CPU-only streaming detector that fuses novelty,\nsession-DAG structure, and attribute cues for near-real-time edge inference,\nwith optional fusion of local prompt-guardrail signals. On an emulated\nsmart-home testbed spanning multiple MCP stacks and a physical bench, AegisMCP\nachieves sub-second per-window model inference and end-to-end alerting. The\nlatency of AegisMCP is consistently sub-second on Intel N150-class edge\nhardware, while outperforming traffic-only and sequence baselines; ablations\nconfirm the importance of DAG and install/permission signals. We release code,\nschemas, and generators for reproducible evaluation."}
{"id": "2509.23130", "categories": ["cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23130", "abs": "https://arxiv.org/abs/2509.23130", "authors": ["Qian Cheng", "Ruize Tang", "Emilie Ma", "Finn Hackett", "Peiyang He", "Yiming Su", "Ivan Beschastnikh", "Yu Huang", "Xiaoxing Ma", "Tianyin Xu"], "title": "SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems", "comment": null, "summary": "Formal models are essential to specifying large, complex computer systems and\nverifying their correctness, but are notoriously expensive to write and\nmaintain. Recent advances in generative AI show promise in generating certain\nforms of specifications. However, existing work mostly targets small code, not\ncomplete systems. It is unclear whether AI can deal with realistic system\nartifacts, as this requires abstracting their complex behavioral properties\ninto formal models. We present SysMoBench, a benchmark that evaluates AI's\nability to formally model large, complex systems. We focus on concurrent and\ndistributed systems, which are keystones of today's critical computing\ninfrastructures, encompassing operating systems and cloud infrastructure. We\nuse TLA+, the de facto specification language for concurrent and distributed\nsystems, though the benchmark can be extended to other specification languages.\nWe address the primary challenge of evaluating AI-generated models by\nautomating metrics like syntactic and runtime correctness, conformance to\nsystem code, and invariant correctness. SysMoBench currently includes nine\ndiverse system artifacts: the Raft implementation of Etcd and Redis, the\nSpinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively\nadded. SysMoBench enables us to understand the capabilities and limitations of\ntoday's LLMs and agents, putting tools in this area on a firm footing and\nopening up promising new research directions."}
{"id": "2510.19666", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19666", "abs": "https://arxiv.org/abs/2510.19666", "authors": ["Matthew Keating", "Michael Casey"], "title": "A Graph Engine for Guitar Chord-Tone Soloing Education", "comment": "ICMC 2025", "summary": "We present a graph-based engine for computing chord tone soloing suggestions\nfor guitar students. Chord tone soloing is a fundamental practice for\nimprovising over a chord progression, where the instrumentalist uses only the\nnotes contained in the current chord. This practice is a building block for all\nadvanced jazz guitar theory but is difficult to learn and practice. First, we\ndiscuss methods for generating chord-tone arpeggios. Next, we construct a\nweighted graph where each node represents a chord tone arpeggio for a chord in\nthe progression. Then, we calculate the edge weight between each consecutive\nchord's nodes in terms of optimal transition tones. We then find the shortest\npath through this graph and reconstruct a chord-tone soloing line. Finally, we\ndiscuss a user-friendly system to handle input and output to this engine for\nguitar students to practice chord tone soloing."}
{"id": "2510.19491", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19491", "abs": "https://arxiv.org/abs/2510.19491", "authors": ["Jonas Gebele", "Timm Mutzel", "Burak Oez", "Florian Matthes"], "title": "Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains", "comment": null, "summary": "Sealed-bid auctions ensure fair competition and efficient allocation but are\noften deployed on centralized infrastructure, enabling opaque manipulation.\nPublic blockchains eliminate central control, yet their inherent transparency\nconflicts with the confidentiality required for sealed bidding. Prior attempts\nstruggle to reconcile privacy, verifiability, and scalability without relying\non trusted intermediaries, multi-round protocols, or expensive cryptography. We\npresent a sealed-bid auction protocol that executes sensitive bidding logic on\na Trusted Execution Environment (TEE)-backed confidential compute blockchain\nwhile retaining settlement and enforcement on a public chain. Bidders commit\nfunds to enclave-generated escrow addresses, ensuring confidentiality and\nbinding commitments. After the deadline, any party can trigger resolution: the\nconfidential blockchain determines the winner through verifiable off-chain\ncomputation and issues signed settlement transactions for execution on the\npublic chain. Our design provides security, privacy, and scalability without\ntrusted third parties or protocol modifications. We implement it on SUAVE with\nEthereum settlement, evaluate its scalability and trust assumptions, and\ndemonstrate deployment with minimal integration on existing infrastructure"}
{"id": "2510.19671", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19671", "abs": "https://arxiv.org/abs/2510.19671", "authors": ["Silvia García-Méndez", "Francisco de Arriba-Pérez"], "title": "Explainable e-sports win prediction through Machine Learning classification in streaming", "comment": null, "summary": "The increasing number of spectators and players in e-sports, along with the\ndevelopment of optimized communication solutions and cloud computing\ntechnology, has motivated the constant growth of the online game industry. Even\nthough Artificial Intelligence-based solutions for e-sports analytics are\ntraditionally defined as extracting meaningful patterns from related data and\nvisualizing them to enhance decision-making, most of the effort in professional\nwinning prediction has been focused on the classification aspect from a batch\nperspective, also leaving aside the visualization techniques. Consequently,\nthis work contributes to an explainable win prediction classification solution\nin streaming in which input data is controlled over several sliding windows to\nreflect relevant game changes. Experimental results attained an accuracy higher\nthan 90 %, surpassing the performance of competing solutions in the literature.\nUltimately, our system can be leveraged by ranking and recommender systems for\ninformed decision-making, thanks to the explainability module, which fosters\ntrust in the outcome predictions."}
{"id": "2510.19537", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19537", "abs": "https://arxiv.org/abs/2510.19537", "authors": ["Mahitha Pulivathi"], "title": "Privacy-Preserving Spiking Neural Networks: A Deep Dive into Encryption Parameter Optimisation", "comment": null, "summary": "Deep learning is widely applied to modern problems through neural networks,\nbut the growing computational and energy demands of these models have driven\ninterest in more efficient approaches. Spiking Neural Networks (SNNs), the\nthird generation of neural networks, mimic the brain's event-driven behaviour,\noffering improved performance and reduced power use. At the same time, concerns\nabout data privacy during cloud-based model execution have led to the adoption\nof cryptographic methods. This article introduces BioEncryptSNN, a spiking\nneural network based encryption-decryption framework for secure and\nnoise-resilient data protection. Unlike conventional algorithms, BioEncryptSNN\nconverts ciphertext into spike trains and exploits temporal neural dynamics to\nmodel encryption and decryption, optimising parameters such as key length,\nspike timing, and synaptic connectivity. Benchmarked against AES-128, RSA-2048,\nand DES, BioEncryptSNN preserved data integrity while achieving up to 4.1x\nfaster encryption and decryption than PyCryptodome's AES implementation. The\nframework demonstrates scalability and adaptability across symmetric and\nasymmetric ciphers, positioning SNNs as a promising direction for secure,\nenergy-efficient computing."}
{"id": "2510.19698", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19698", "abs": "https://arxiv.org/abs/2510.19698", "authors": ["Yang Yang", "Hua XU", "Zhangyi Hu", "Yutao Yue"], "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning."}
{"id": "2510.19676", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19676", "abs": "https://arxiv.org/abs/2510.19676", "authors": ["Nowfel Mashnoor", "Mohammad Akyash", "Hadi Kamali", "Kimia Azar"], "title": "CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in generative\ntasks, including register-transfer level (RTL) hardware synthesis. However,\ntheir tendency to memorize training data poses critical risks when proprietary\nor security-sensitive designs are unintentionally exposed during inference.\nWhile prior work has examined memorization in natural language, RTL introduces\nunique challenges: In RTL, structurally different implementations (e.g.,\nbehavioral vs. gate-level descriptions) can realize the same hardware, leading\nto intellectual property (IP) leakage (full or partial) even without verbatim\noverlap. Conversely, even small syntactic variations (e.g., operator precedence\nor blocking vs. non-blocking assignments) can drastically alter circuit\nbehavior, making correctness preservation especially challenging. In this work,\nwe systematically study memorization in RTL code generation and propose\nCircuitGuard, a defense strategy that balances leakage reduction with\ncorrectness preservation. CircuitGuard (1) introduces a novel RTL-aware\nsimilarity metric that captures both structural and functional equivalence\nbeyond surface-level overlap, and (2) develops an activation-level steering\nmethod that identifies and attenuates transformer components most responsible\nfor memorization. Our empirical evaluation demonstrates that CircuitGuard\nidentifies (and isolates) 275 memorization-critical features across layers\n18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic\nsimilarity to proprietary patterns while maintaining generation quality.\nCircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling\nrobust memorization mitigation across circuit categories without retraining."}
{"id": "2510.19732", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19732", "abs": "https://arxiv.org/abs/2510.19732", "authors": ["Gunshi Gupta", "Karmesh Yadav", "Zsolt Kira", "Yarin Gal", "Rahaf Aljundi"], "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "comment": "Accepted for Spotlight Presentation at NeurIPS 2025", "summary": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints."}
{"id": "2510.19761", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19761", "abs": "https://arxiv.org/abs/2510.19761", "authors": ["Mohamed ElShehaby", "Ashraf Matrawy"], "title": "Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems", "comment": null, "summary": "Adversarial attacks pose significant challenges to Machine Learning (ML)\nsystems and especially Deep Neural Networks (DNNs) by subtly manipulating\ninputs to induce incorrect predictions. This paper investigates whether\nincreasing the layer depth of deep neural networks affects their robustness\nagainst adversarial attacks in the Network Intrusion Detection System (NIDS)\ndomain. We compare the adversarial robustness of various deep neural networks\nacross both \\ac{NIDS} and computer vision domains (the latter being widely used\nin adversarial attack experiments). Our experimental results reveal that in the\nNIDS domain, adding more layers does not necessarily improve their performance,\nyet it may actually significantly degrade their robustness against adversarial\nattacks. Conversely, in the computer vision domain, adding more layers exhibits\na more modest impact on robustness. These findings can guide the development of\nrobust neural networks for (NIDS) applications and highlight the unique\ncharacteristics of network security domains within the (ML) landscape."}
{"id": "2510.19738", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19738", "abs": "https://arxiv.org/abs/2510.19738", "authors": ["Rustem Turtayev", "Natalia Fedorova", "Oleg Serikov", "Sergey Koldyba", "Lev Avagyan", "Dmitrii Volkov"], "title": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior", "comment": null, "summary": "Advanced AI systems sometimes act in ways that differ from human intent. To\ngather clear, reproducible examples, we ran the Misalignment Bounty: a\ncrowdsourced project that collected cases of agents pursuing unintended or\nunsafe goals. The bounty received 295 submissions, of which nine were awarded.\n  This report explains the program's motivation and evaluation criteria, and\nwalks through the nine winning submissions step by step."}
{"id": "2510.19772", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19772", "abs": "https://arxiv.org/abs/2510.19772", "authors": ["Jad Zarzour", "Matthew Jablonski"], "title": "Under Pressure: Security Analysis and Process Impacts of a Commercial Smart Air Compressor", "comment": null, "summary": "The integration of Industrial Internet of Things (IIoT) devices into\nmanufacturing environments has accelerated the transition to Industry 4.0, but\nhas also introduced new cybersecurity risks. This paper conducts a\ncomprehensive security analysis of a commercial smart air compressor, revealing\ncritical vulnerabilities including hardcoded credentials, unauthenticated APIs,\nand an insecure update mechanism. It includes a formal threat model,\ndemonstrates practical attack scenarios in a testbed environment, and evaluates\ntheir subsequent impact on an industrial process, leading to denial of service\nand the corruption of critical process telemetry. In addition, an analysis of\nthe device's supply chain reveals how product integration from multiple vendors\nand limited security considerations can expose a device to threats. The\nfindings underscore the necessity of incorporating cybersecurity principles\ninto both IIoT device design and supply chain governance to enhance resilience\nagainst emerging industrial cyber threats."}
{"id": "2510.19771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19771", "abs": "https://arxiv.org/abs/2510.19771", "authors": ["Gil Pasternak", "Dheeraj Rajagopal", "Julia White", "Dhruv Atreja", "Matthew Thomas", "George Hurn-Maloney", "Ash Lewis"], "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents", "comment": null, "summary": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions."}
{"id": "2510.19788", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19788", "abs": "https://arxiv.org/abs/2510.19788", "authors": ["Archana Warrier", "Dat Nyugen", "Michelangelo Naim", "Moksh Jain", "Yichao Liang", "Karen Schroeder", "Cambridge Yang", "Joshua B. Tenenbaum", "Sebastian Vollmer", "Kevin Ellis", "Zenna Tavares"], "title": "Benchmarking World-Model Learning", "comment": "30 pages, 10 figures", "summary": "Model-learning agents should gather information to learn world models that\nsupport many downstream tasks and inferences, such as predicting unobserved\nstates, estimating near- and far-term consequences of actions, planning action\nsequences, and detecting changes in dynamics. Current methods for learning and\nevaluating world models diverge from this goal: training and evaluation are\nanchored to next-frame prediction, and success is scored by reward maximization\nin the same environment. We propose WorldTest, a protocol to evaluate\nmodel-learning agents that separates reward-free interaction from a scored test\nphase in a different but related environment. WorldTest is\nopen-ended$\\unicode{x2014}$models should support many different tasks unknown\nahead of time$\\unicode{x2014}$and agnostic to model representation, allowing\ncomparison across approaches. We instantiated WorldTest with AutumnBench, a\nsuite of 43 interactive grid-world environments and 129 tasks across three\nfamilies: masked-frame prediction, planning, and predicting changes to the\ncausal dynamics. We compared 517 human participants and three frontier models\non AutumnBench. We found that humans outperform the models, and scaling compute\nimproves performance only in some environments but not others. WorldTest\nprovides a novel template$\\unicode{x2014}$reward-free exploration, derived\ntests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn\nabout environment dynamics, and AutumnBench exposes significant headroom in\nworld-model learning."}
{"id": "2510.18893", "categories": ["cs.DC", "cs.AI", "cs.SE", "I.2.11; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.18893", "abs": "https://arxiv.org/abs/2510.18893", "authors": ["Sergey Pugachev"], "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "comment": "11 pages, 3 figures", "summary": "Multi-agent LLM systems fail to realize parallel speedups due to costly\ncoordination. We present CodeCRDT, an observation-driven coordination pattern\nwhere agents coordinate by monitoring a shared state with observable updates\nand deterministic convergence, rather than explicit message passing. Using\nConflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,\nconflict-free concurrent code generation with strong eventual consistency.\nEvaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits\nand trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on\nothers, and 100% convergence with zero merge failures. The study formalizes\nobservation-driven coordination for stochastic LLM agents, revealing semantic\nconflict rates (5-10%) and quality-performance tradeoffs, and provides\nempirical characterization of when parallel coordination succeeds versus fails\nbased on task structure."}
{"id": "2510.18897", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators."}
{"id": "2510.19264", "categories": ["cs.CR", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19264", "abs": "https://arxiv.org/abs/2510.19264", "authors": ["R. Can Aygun", "Yehuda Afek", "Anat Bremler-Barr", "Leonard Kleinrock"], "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery", "comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)", "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."}
{"id": "2510.19303", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19303", "abs": "https://arxiv.org/abs/2510.19303", "authors": ["Petar Radanliev"], "title": "Collaborative penetration testing suite for emerging generative AI algorithms", "comment": null, "summary": "Problem Space: AI Vulnerabilities and Quantum Threats Generative AI\nvulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum\nthreats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative\nAI models against classical and quantum cyberattacks. Proposed Solution\nCollaborative Penetration Testing Suite Five Integrated Components: DAST SAST\nOWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with\nCI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs.\nQuantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations\nAdversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow\nfor AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities\nidentified across test environments. 70% reduction in high-severity issues\nwithin 2 weeks. 90% resolution efficiency for blockchain-logged\nvulnerabilities. Quantum-resistant cryptography maintained 100% integrity in\ntests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum\nCryptography AI Red Teaming."}
{"id": "2510.19420", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19420", "abs": "https://arxiv.org/abs/2510.19420", "authors": ["Chengcan Wu", "Zhixin Zhang", "Mingqian Xu", "Zeming Wei", "Meng Sun"], "title": "Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation", "comment": null, "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a\npopular paradigm of AI applications. However, trustworthiness issues in MAS\nremain a critical concern. Unlike challenges in single-agent systems, MAS\ninvolve more complex communication processes, making them susceptible to\ncorruption attacks. To mitigate this issue, several defense mechanisms have\nbeen developed based on the graph representation of MAS, where agents represent\nnodes and communications form edges. Nevertheless, these methods predominantly\nfocus on static graph defense, attempting to either detect attacks in a fixed\ngraph structure or optimize a static topology with certain defensive\ncapabilities. To address this limitation, we propose a dynamic defense paradigm\nfor MAS graph structures, which continuously monitors communication within the\nMAS graph, then dynamically adjusts the graph topology, accurately disrupts\nmalicious communications, and effectively defends against evolving and diverse\ndynamic attacks. Experimental results in increasingly complex and dynamic MAS\nenvironments demonstrate that our method significantly outperforms existing MAS\ndefense mechanisms, contributing an effective guardrail for their trustworthy\napplications. Our code is available at\nhttps://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems."}
{"id": "2510.19470", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19470", "abs": "https://arxiv.org/abs/2510.19470", "authors": ["Weihao Yang", "Hao Huang", "Donglei Wu", "Ningke Li", "Yanqi Pan", "Qiyang Zheng", "Wen Xia", "Shiyi Li", "Qiang Wang"], "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission", "comment": null, "summary": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large\nmodels. However, the rapidly growing scale outpaces model training on a single\nDC, driving a shift toward a more flexible, cross-DC training paradigm. Under\nthis, Expert Parallelism (EP) of MoE faces significant scalability issues due\nto the limited cross-DC bandwidth. Specifically, existing EP optimizations\nattempt to overlap data communication and computation, which has little benefit\nin low-bandwidth scenarios due to a much longer data communication time.\nTherefore, the trends of cross-DC EP scaling is fast becoming a critical\nroadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize\nEP under constrained bandwidth. Our key idea is to dynamically transform the\nspatial placement of experts to reduce data communication traffic and\nfrequency, thereby minimizing EP's communication overheads. However, it is\nnon-trivial to find the optimal solution because it complicates the original\ncommunication pattern by mixing data and expert communication. We therefore\nbuild a stream-based model to determine the optimal transmission ratio. Guided\nby this, we incorporate two techniques: (1) domain-based partition to construct\nthe mapping between hybrid patterns and specific communication topology at GPU\nlevel, and (2) parameter-efficient migration to further refine this topology by\nreducing expert transmission overhead and enlarging the domain size. Combining\nall these designs, HybridEP can be considered as a more general EP with better\nscalability. Experimental results show that HybridEP outperforms existing\nstate-of-the-art MoE training systems by up to 5.6x under constrained\nbandwidth. We further compare HybridEP and EP on large-scale simulations.\nHybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths."}
{"id": "2510.19689", "categories": ["cs.DC", "cs.AI", "cs.LG", "C.2.4; H.3.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19689", "abs": "https://arxiv.org/abs/2510.19689", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Srinivas Vippagunta", "Suchitra Raman", "Shreeshankar Chatterjee", "Ju Lin", "Shang Liu", "Mary Schladenhauffen", "Jeffrey Luo", "Hailong Jiang"], "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation", "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025", "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings."}
