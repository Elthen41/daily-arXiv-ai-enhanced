<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 42]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 论文提出了一个分析AI系统对人类构成生存风险的一般框架，基于两个前提构建了人类生存故事的分类法，并评估了不同生存故事面临的挑战和应对策略。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成生存风险的争论日益激烈。本文旨在建立一个系统性的框架来思考AI的生存风险问题，帮助理解不同生存情景及其面临的挑战。

Method: 基于两个前提构建分析框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。通过这两个前提构建了四种人类生存故事分类，每种故事中都有一个前提失败。

Result: 提出了四种生存故事分类：1）科学障碍阻止AI变得极其强大；2）人类禁止AI研究；3）极其强大的AI因目标设定而不毁灭人类；4）人类能可靠检测并禁用有毁灭目标的AI系统。分析了每种故事面临的独特挑战。

Conclusion: 不同的生存故事需要不同的应对策略来应对AI威胁。作者利用这个分类法对P(doom)（AI毁灭人类的概率）进行了粗略估计，为理解AI生存风险提供了系统性的分析框架。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [2] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现：人们愿意花费个人资源惩罚使用大语言模型（LLM）完成任务的同行，惩罚程度随实际使用量单调增加，且存在"可信度差距"——声称"未使用"比实际未使用受罚更重。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速普及，人们对其引发的社会反应产生担忧。先前研究记录了人们对AI用户的负面态度，但尚不清楚这种不认可是否会转化为实际代价高昂的行为。

Method: 采用两阶段在线实验（第二阶段491名参与者），参与者可以花费自己的资源来减少之前完成真实努力任务的同行的收益，这些同行在任务中可能使用或不使用LLM支持。

Result: 参与者平均摧毁了完全依赖LLM同行36%的收益，惩罚程度随实际LLM使用量单调增加。关于LLM使用的披露存在可信度差距：自称未使用的惩罚比实际未使用更严厉，而高使用水平下，实际依赖比自我报告依赖受罚更重。

Conclusion: 这是首个行为证据，表明LLM的效率提升伴随着社会制裁的代价，揭示了AI使用在社会互动中的复杂动态。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [3] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出了一种非交互式端到端推理框架AAI，通过注意力重加权来激活逻辑推理模式，无需外部资源即可提升大语言模型的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有逻辑推理方法依赖复杂的交互式框架或外部符号求解器，存在额外开销和可扩展性限制。需要一种非交互式、端到端的框架，让推理能力在模型内部自然涌现。

Method: 提出注意力感知干预（AAI）方法：1）在few-shot提示中引入结构信息激活特定注意力头；2）识别与逻辑推理操作符对齐的注意力头模式；3）在推理时对这些选定头的注意力分数进行重加权，引导模型利用先验知识。

Result: AAI在多个基准测试和模型架构上显著提升了逻辑推理性能，同时仅带来可忽略的计算开销。代码已开源。

Conclusion: AAI提供了一种高效的非交互式端到端推理框架，通过注意力调制引导模型推理，在保持可分析性的同时提升了泛化能力，无需依赖外部资源。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [4] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek是一种新颖的顺序测试时缩放方法，通过动态编码KV缓存来稳定推理准确性，无需推理长度微调，并能超越模型最大上下文长度继续推理。


<details>
  <summary>Details</summary>
Motivation: 当前顺序测试时缩放方法存在局限性：随着推理长度增加会导致准确性下降和模型不稳定，需要推理长度微调，且受限于模型最大上下文长度。

Method: 提出Min-Seek方法，只保留一个额外诱导思想的KV对在KV缓存中，使用自定义KV缓存存储不带位置嵌入的键，并在每个新生成思想前动态连续编码它们。

Result: 方法在广泛的诱导思想范围内显著提高模型准确性，稳定顺序缩放的准确性，无需推理长度微调，并能超越模型最大上下文长度继续推理。

Conclusion: Min-Seek是一种高效、稳定的顺序测试时缩放方法，在多种推理任务上提高准确性，具有线性计算复杂度，并能突破模型上下文长度限制。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [5] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 该论文将人机互补性重新定义为可靠认知过程的证据，而非单纯的预测准确性指标，为人机交互提供了更坚实的理论基础。


<details>
  <summary>Details</summary>
Motivation: 人机互补性概念在理论上面临挑战：缺乏精确的理论锚定、仅作为预测准确性的后验指标、忽视人机交互的其他期望特性、抽象化性能增益的成本效益。这些挑战使得互补性在实证环境中难以实现。

Method: 借鉴认识论框架，特别是计算可靠主义，将互补性重新置于可证明AI的话语体系中。将历史互补性实例视为特定人机交互作为可靠认知过程的证据，结合其他可靠性指标来评估人机团队的可靠性。

Result: 提出了一个理论框架，将互补性的角色从相对预测准确性度量转变为帮助校准决策以适应日益影响日常生活的AI支持过程的可靠性。

Conclusion: 互补性的价值和作用在于帮助受影响各方（患者、管理者、监管者等）进行实践推理，通过校准决策来适应AI支持过程的可靠性，而非仅仅提供预测准确性的相对度量。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [6] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出了一种基于信息流编排的多智能体范式，通过智能体间通信动态协调任务，无需预定义工作流，在GAIA基准上超越基于工作流的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要大量人工工作来编码可能任务状态，且无法穷尽复杂现实任务的状态空间。这种基于规则的设计存在根本性限制。

Method: 提出信息流编排的多智能体范式，通过专门的编排器持续监控任务进度，使用自然语言通过A2A工具包动态协调其他智能体，不依赖预定义工作流。

Result: 在GAIA基准测试中，pass@1设置下达到63.64%准确率，比基于工作流的OWL基线（55.15%）高出8.49个百分点，且token消耗相当。案例级分析显示该方法能更灵活监控任务并更稳健处理边缘情况。

Conclusion: 信息流编排的多智能体范式通过动态协调机制克服了基于规则工作流的局限性，实现了更灵活的任务监控和更稳健的边缘情况处理，在多智能体系统设计中具有重要价值。

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [7] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 论文提出连续记忆架构(CMA)作为检索增强生成(RAG)的替代方案，解决RAG将记忆视为静态查找表的问题，通过持久存储、选择性保留、关联路由、时间链和整合为高阶抽象来实现跨交互的内部状态维护和更新。


<details>
  <summary>Details</summary>
Motivation: RAG虽然已成为为LLM智能体提供上下文知识的默认策略，但存在三个主要问题：1）信息无限期持久化；2）检索是只读的；3）缺乏时间连续性。这些限制使得RAG无法积累、修改或消歧记忆，不适合长期交互的智能体。

Method: 提出连续记忆架构(CMA)这一系统类别，要求具备五个核心功能：1）跨交互的持久存储；2）选择性保留机制；3）关联路由能力；4）时间链连接；5）整合为高阶抽象。论文不披露具体实现细节，而是定义CMA的架构要求。

Result: 在知识更新、时间关联、关联回忆、上下文消歧等实证测试中，CMA展现出相对于RAG的一致行为优势，特别是在那些暴露RAG结构缺陷的任务上。CMA被证明是实现长期智能体的必要架构原语。

Conclusion: CMA为解决RAG在记忆处理上的局限性提供了必要的架构基础，但同时也面临延迟、漂移和可解释性等开放挑战。该架构为构建能够积累、修改和消歧记忆的长期智能体奠定了基础。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [8] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 本文提出了一个基于根本原因认知的幻觉管理操作框架，通过模型、数据和上下文三因素分类，结合多层次检测与缓解策略，构建了用于高风险领域的可信生成AI系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和大型推理模型在金融、法律等高风险领域具有变革潜力，但其产生事实错误或无依据内容的幻觉倾向带来了关键可靠性风险，需要系统化的管理方法。

Method: 提出了一个基于持续改进循环的幻觉管理操作框架，将幻觉来源分为模型、数据和上下文相关因素，整合了多层次检测方法（不确定性估计、推理一致性等）和分层缓解策略（知识基础、置信度校准等）。

Result: 通过分层架构和金融数据提取案例研究展示了框架应用，其中模型、上下文和数据层形成了闭环反馈循环，实现了渐进式可靠性增强。

Conclusion: 该方法为受监管环境中构建可信生成AI系统提供了系统化、可扩展的方法论，能够有效管理幻觉风险并提升模型可靠性。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [9] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM是针对中国劳动法领域专门优化的法律大语言模型，在LabourLawBench基准测试中全面超越通用模型和现有法律专用模型。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和情境敏感性的专业法律子领域时存在局限，特别是在劳动法这种专业细分领域表现不足。

Method: 开发了LabourLawLLM（专门针对中国劳动法的法律大语言模型）和LabourLawBench（涵盖法律条文引用、知识问答、案例分类、赔偿计算、命名实体识别和案例分析等任务的综合基准），采用客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估相结合的评估框架。

Result: 实验表明，LabourLawLLM在所有任务类别上持续优于通用模型和现有的法律专用大语言模型。

Conclusion: 该研究不仅针对劳动法领域，其方法论为构建其他法律子领域的专业大语言模型提供了可扩展的方法，提高了法律AI应用的准确性、可靠性和社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [10] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL是一个无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统问题，在BIRD数据集上达到68.5%执行准确率，比之前方法节省10倍以上资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键限制：1) 仅依赖正确示例进行上下文学习，忽略了历史错误修复对中的丰富信号；2) 测试时缩放方法通常任意分解问题，产生几乎相同的SQL候选，降低了集成增益。这些方法还存在明显的准确率-效率权衡问题。

Method: 提出Memo-SQL框架，包含两个核心思想：1) 结构化分解：采用实体级、分层和原子顺序三种清晰策略来鼓励多样化推理；2) 经验感知自校正：构建动态记忆库，包含成功查询和历史错误修复对，在推理时使用检索增强提示将相关示例引入上下文，无需微调或外部API。

Result: 在BIRD数据集上，Memo-SQL达到68.5%的执行准确率，在开放、零微调方法中创造了新的最先进水平，同时比之前的TTS方法使用超过10倍更少的资源。

Conclusion: Memo-SQL通过结构化分解和经验感知自校正有效解决了现有NL2SQL系统的局限性，在保持高准确率的同时显著提高了效率，为无需训练的NL2SQL系统提供了新的解决方案。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [11] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA是一个端到端计算框架，结合LLM、RAG和定制知识库，从隐私政策中提取个人数据流并构建数据流图进行分析。


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用冗长复杂的法律语言，且不同行业和组织间的实践不一致，导致用户难以完全理解。需要自动化、大规模的分析方法来帮助理解隐私政策中的个人数据处理实践。

Method: 开发LADFA框架，结合大型语言模型（LLM）、检索增强生成（RAG）和基于现有研究的定制知识库。框架包括预处理器、基于LLM的处理器和数据流后处理器，能够处理非结构化文本、提取个人数据流、构建数据流图并进行洞察发现。

Result: 通过对汽车行业十个选定隐私政策的案例研究，验证了所提方法的有效性和准确性。LADFA被证明是灵活且可定制的，适用于隐私政策分析之外的一系列基于文本的分析任务。

Conclusion: LADFA框架成功结合了LLM、RAG和定制知识库，能够有效提取和分析隐私政策中的个人数据流，为大规模隐私政策分析提供了可行的解决方案，并具有扩展到其他文本分析任务的潜力。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [12] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 该论文提出了一个基于荣格心理类型的LLM人格建模框架，包含主导-辅助协调、强化-补偿和反思三种机制，实现人格的连贯表达、情境适应和长期演化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在HCI应用中需要更自然的人格表达，现有方法难以同时实现细腻和可适应的人格特征。人格对交互参与度、决策和真实感至关重要。

Method: 采用荣格心理类型理论，设计三个核心机制：1) 主导-辅助协调机制确保核心人格连贯表达；2) 强化-补偿机制实现情境临时适应；3) 反思机制驱动长期人格演化。

Result: 使用迈尔斯-布里格斯类型指标问卷评估人格对齐，在多样化挑战场景中测试。结果表明演化的人格感知LLM能支持连贯、情境敏感的交互。

Conclusion: 该框架使LLM能保持细腻人格特质的同时动态适应交互需求，逐步更新底层结构，为HCI中的自然智能体设计提供了支持。

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [13] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep是一个基于保真度的深度学习框架，用于解决弹性-塑性固体大变形问题，通过同时使用低保真度和高保真度数据来平衡数据数量与精度之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在大变形弹性-塑性固体计算中存在固有局限性，而现有深度学习技术需要大量高精度数据，但在大变形问题中难以获取。数据构建过程中存在数量与精度的两难困境，导致深度学习模型性能不佳。

Method: 提出FilDeep框架，针对拉伸弯曲这一代表性大变形应用，同时使用低保真度（数量多但精度低）和高保真度（精度高但数量少）数据进行训练。设计了注意力机制支持的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: 大量实验表明，FilDeep能够持续实现最先进的性能，并且可以高效部署到制造应用中。这是首个使用多保真度数据解决大变形问题的深度学习框架。

Conclusion: FilDeep成功解决了大变形问题中数据数量与精度的两难困境，通过多保真度数据融合和注意力机制设计，为制造应用中的弹性-塑性固体大变形计算提供了有效的深度学习解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [14] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台对超过100万亿token的真实LLM使用数据进行分析，揭示了开源模型采用率高、创意角色扮演和编码辅助任务受欢迎、智能体推理兴起等趋势，并发现了早期用户的"玻璃鞋"留存效应。


<details>
  <summary>Details</summary>
Motivation: 随着o1等推理模型的发布，LLM领域从单次模式生成转向多步推理推断，但我们对这些模型在实际使用中的真实情况了解滞后。本研究旨在通过大规模真实世界使用数据来填补这一知识空白。

Method: 利用OpenRouter平台（一个支持多种LLM的AI推理提供商），分析了超过100万亿token的真实世界LLM交互数据，涵盖不同任务、地域和时间维度。

Result: 研究发现：1) 开源权重模型采用率显著；2) 创意角色扮演和编码辅助类别异常受欢迎；3) 智能体推理兴起；4) 早期用户的留存率远高于后期用户，称为"玻璃鞋"效应。

Conclusion: LLM的实际使用情况复杂多样，超出传统认知。这些发现对模型构建者、AI开发者和基础设施提供商具有重要启示，数据驱动的使用理解可以指导更好的LLM系统设计和部署。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [15] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT：一种基于矩阵的结构化思维链框架，通过矩阵规划、引用标注和反馈重规划机制，提升LLM在符号推理任务中的鲁棒性和可解释性，无需依赖外部求解器。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：思维链提示在依赖符号表达式和严格演绎规则的逻辑推理任务上表现不足；神经符号方法依赖外部求解器，对格式敏感且容易处理失败；纯LLM驱动方法缺乏结构化表示和过程级纠错机制。需要一种能增强LLM逻辑推理能力的新框架。

Method: 提出MatrixCoT框架：1）对自然语言表达式进行归一化和类型标注；2）添加显式引用字段；3）引入基于矩阵的规划方法以保持步骤间的全局关系；4）矩阵规划作为可验证的工件，使执行更稳定；5）添加反馈驱动的重规划机制，在语义等价约束下识别遗漏和缺陷，重写和压缩依赖矩阵。

Result: 在五个逻辑推理基准和五个LLM上的实验表明，MatrixCoT在不依赖外部求解器的情况下，显著提升了处理复杂符号推理任务的鲁棒性和可解释性，同时保持了有竞争力的性能。

Conclusion: MatrixCoT通过结构化思维链和矩阵规划，有效解决了现有方法在逻辑推理任务中的局限性，为LLM的符号推理能力提供了更稳定、可解释的解决方案。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [16] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段的分子生成框架，通过片段级检索增强和强化学习优化，在多个物理化学性质约束下精确生成分子。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在精确的多目标控制和数值推理方面存在困难，需要外部结构和反馈来生成满足多个精确数值约束的分子。

Method: 采用两阶段框架：第一阶段为原型生成，使用多智能体推理器进行检索锚定的片段级编辑；第二阶段为RL精细优化，使用GRPO训练的片段级优化器进行单跳或多跳细化，最小化属性误差。

Result: 在两组性质约束（QED、LogP、分子量和HOMO、LUMO）下的生成实验中，MolGen在有效性和精确满足多属性目标方面表现优于强LLM和基于图的算法。

Conclusion: MolGen通过片段级推理和可控细化，能够更好地生成满足精确数值约束的分子，为多属性约束下的分子生成提供了有效解决方案。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [17] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计模型，但不如专用机器学习模型，且过多上下文信息反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理和预测方面表现出色，但其从结构化行为数据中推断时间规律的能力尚未得到充分探索。本研究旨在探究LLMs是否能预测重复用户行为（如重复购买）之间的时间间隔，以及不同层次的上下文信息如何影响其预测行为。

Method: 使用简单但具有代表性的重复购买场景，在零样本设置下对最先进的LLMs进行基准测试，并与统计模型和机器学习模型进行比较。研究考察了不同层次上下文信息对LLM预测准确性的影响。

Result: 1. LLMs虽然优于轻量级统计基线模型，但始终不如专用机器学习模型，显示出其捕捉定量时间结构的能力有限。2. 适度的上下文信息可以提高LLM准确性，但添加更多用户级细节反而会降低性能，挑战了"更多上下文导致更好推理"的假设。

Conclusion: 研究揭示了当前LLMs在结构化时间推理方面的基本局限性，并为设计未来上下文感知的混合模型提供了指导，这些模型需要整合统计精确性和语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [18] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 本文提出了一种漂移感知数据流系统，通过机器学习自适应控制来应对金融市场的概念漂移和分布非平稳性，将数据增强、课程学习和数据工作流管理统一在可微分框架下，提升模型鲁棒性和风险调整收益。


<details>
  <summary>Details</summary>
Motivation: 量化金融中，训练与真实世界性能之间的差距主要由概念漂移和分布非平稳性引起，基于静态历史数据训练的模型容易过拟合，在动态市场中泛化能力差。"历史不足够"的理念强调了需要能够随市场演化的自适应数据生成方法，而非仅依赖过去观测。

Method: 提出漂移感知数据流系统，将基于机器学习的自适应控制集成到数据管理过程中。系统包含参数化数据操作模块（单股变换、多股混合和筛选操作）和自适应规划调度器，采用基于梯度的双层优化来控制整个系统。该设计将数据增强、课程学习和数据工作流管理统一在单一可微分框架下，支持溯源感知重放和持续数据质量监控。

Result: 在预测和强化学习交易任务上的大量实验表明，该框架增强了模型鲁棒性并提高了风险调整收益。系统为金融数据提供了可推广的自适应数据管理和学习引导的工作流自动化方法。

Conclusion: 该系统为解决量化金融中概念漂移和分布非平稳性问题提供了有效的解决方案，通过将自适应数据生成与机器学习控制相结合，实现了更可靠的数据驱动系统构建，为金融数据管理提供了通用框架。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [19] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 论文提出DecisionLLM框架，将大型语言模型应用于离线决策任务，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决LLM无法理解连续数值的问题，在迷宫导航和竞价场景中显著优于传统决策Transformer。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长序列决策任务中存在挑战，而大型语言模型在复杂推理和规划任务中表现出色。研究者希望探索具有相同Transformer架构但规模更大的LLM能否在长视野序列决策问题中带来性能突破，特别是解决LLM无法原生理解连续数值的问题。

Method: 提出DecisionLLM框架，将轨迹数据视为独立模态，学习将轨迹数据与自然语言任务描述对齐，使模型能够在统一框架内自回归预测未来决策。建立了该范式的缩放定律，表明性能取决于三个因素：模型规模、数据量和数据质量。

Result: 在离线实验基准和竞价场景中，DecisionLLM表现出色。DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer提升69.4分，在AuctionNet上提升0.085分。扩展了AIGB范式，为在线竞价探索指明了方向。

Conclusion: LLM能够有效应用于长序列决策任务，通过将轨迹作为独立模态与语言对齐的方法解决了LLM理解连续数值的挑战。该研究为基于LLM的决策系统开辟了新方向，特别是在计算广告实时竞价等动态环境优化中具有应用前景。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [20] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器化平台，旨在标准化医学影像AI模型的访问，解决实现多样性、文档不一致和可重复性问题，通过容器化包装模型并提供统一接口。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究面临实现多样性、文档不一致和可重复性差的问题，限制了AI在临床和研究中的应用。需要一种标准化平台来简化模型访问和使用。

Method: 开发MHub.ai开源容器化平台，将同行评审的AI模型打包成标准化容器，支持DICOM等格式直接处理，提供统一应用接口和结构化元数据，附带公开参考数据验证模型运行。

Result: 平台包含最先进的分割、预测和特征提取模型，支持多模态。通过肺分割模型的比较评估展示了临床实用性，并公开了分割结果、评估指标和交互式仪表板。

Conclusion: MHub.ai通过简化模型使用，支持相同执行命令的并行基准测试和标准化输出，降低了临床转化的障碍，增强了医学影像AI的透明度和可重复性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [21] [CtD: Composition through Decomposition in Emergent Communication](https://arxiv.org/abs/2601.10169)
*Boaz Carmeli,Ron Meir,Yonatan Belinkov*

Main category: cs.AI

TL;DR: 该研究展示了人工神经智能体如何通过"分解-组合"方法获得组合泛化能力，使其能够描述未见过的图像。


<details>
  <summary>Details</summary>
Motivation: 组合性是人类的认知机制，能够系统地将已知概念组合成新的方式。研究旨在探索人工神经智能体如何获得和利用这种组合泛化能力来描述未见过的图像。

Method: 提出"通过分解实现组合"的方法，包含两个顺序训练步骤：1) "分解"步骤：智能体在多目标协调游戏中学习使用代码本将图像分解为基本概念；2) "组合"步骤：智能体使用同一代码本将基本概念组合成复杂短语来描述新图像。

Result: 研究发现智能体能够获得组合泛化能力，并且在某些情况下，"组合"步骤中的泛化是零样本实现的，无需额外训练。

Conclusion: 人工神经智能体可以通过"分解-组合"方法获得类似人类的组合泛化能力，这种能力使其能够描述未见过的图像，甚至实现零样本泛化。

Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed "Composition through Decomposition", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.

</details>


### [22] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 该研究提出了一个系统评估高频时间序列降采样信息损失的工作流程，结合形状失真度量和分类性能分析，用于针肌电图信号分析，旨在平衡计算负载与诊断信息保留。


<details>
  <summary>Details</summary>
Motivation: 针肌电图信号的高采样率和异质性给基于特征的机器学习模型带来计算挑战，特别是近实时分析需求。降采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚不明确。

Method: 开发了一个结合形状失真度量、特征机器学习模型分类结果和特征空间分析的系统工作流程，量化不同降采样算法和因素对波形完整性和预测性能的影响。使用三类神经肌肉疾病分类任务进行实验评估。

Result: 工作流程能够识别在显著减少计算负载的同时保留诊断信息的降采样配置。形状感知降采样算法优于标准抽取，能更好地保留峰值结构和整体信号形态。

Conclusion: 研究为选择支持近实时针肌电图分析的降采样配置提供了实用指导，并提出了一个可推广的工作流程，可用于平衡其他高频时间序列应用中的数据缩减与模型性能。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [23] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG是一个新型检索增强生成框架，专门处理企业文档中混合的文本和表格数据，通过双架构分别处理叙述性内容和表格结构，相比传统线性化方法在混合查询上提升了18.4%的性能。


<details>
  <summary>Details</summary>
Motivation: 企业数据集通常包含复杂的叙述性文本和结构化表格的混合内容。当前的RAG系统采用线性化方法将多维表格转换为简单文本字符串，但这种方法在数学上已被证明不足以捕捉电子表格的几何结构，需要更尊重数据拓扑的处理方式。

Method: Topo-RAG采用双架构设计：1）传统密集检索器处理流畅的叙述性内容；2）Cell-Aware Late Interaction机制处理表格结构，保留其空间关系。这种架构尊重数据的拓扑结构，不假设"一切都是文本"。

Result: 在SEC-25（模拟真实世界复杂性的合成企业语料库）上评估，Topo-RAG在混合查询上的nDCG@10指标相比标准线性化方法提升了18.4%。

Conclusion: Topo-RAG框架挑战了"一切都是文本"的假设，通过尊重数据拓扑的双架构设计，不仅提升了搜索性能，更重要的是理解了信息的形状和结构。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [24] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出了一种针对多步推理任务的目标路由方法，仅在关键步骤（可能引发级联错误的步骤）使用大模型，而让较小模型处理常规步骤，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给单一模型，将所有推理步骤视为同等重要。多步推理任务（如数学问题解决）容易发生级联失败，单个错误步骤可能导致整个解决方案崩溃。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级别的不确定性和预算约束做出路由决策。开发了多种路由策略，从简单的基于阈值的策略到更复杂的考虑长期准确性-成本权衡的策略。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前路由方法，成本效率提高5倍；更高级的策略使用80%更少的大模型token就能匹配强大昂贵模型的性能。在AIME等更难基准上，TRIM实现了高达6倍的成本效率提升。

Conclusion: 所有方法在数学推理任务中都能有效泛化，表明步骤级别难度代表了推理的基本特征。目标步骤级干预能够从根本上改变推理效率，将昂贵的调用限制在那些需要更强模型防止级联错误的关键步骤上。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [25] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，不依赖推理或代数计算，专注于评估LLMs能否直接编码空间关系和识别几何属性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估基于推理的几何能力（使用代数方法求解），但缺乏对LLMs内在几何理解能力的评估。研究者希望了解LLMs是否能够直接编码空间关系和识别几何属性，而不依赖推理过程。

Method: 创建了包含2,500个简单几何问题的NoReGeo基准，涵盖25个类别，每个问题都精心设计为仅通过原生几何理解即可解决（假设已知对象位置）。评估了包括GPT-4在内的最先进模型，并进行了消融实验。

Result: 即使是最先进的模型，在二元分类任务中的最高准确率也只有65%。消融实验表明，仅通过微调无法获得这种几何理解能力，有效的几何理解训练需要从一开始就采用专门的方法。

Conclusion: 当前LLMs在原生理解几何概念方面存在显著差距，这为未来研究具有真正几何认知能力的模型奠定了基础。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [26] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出证据增强策略优化方法，通过密集过程监督解决长上下文推理中稀疏奖励问题，显著提升证据检索质量


<details>
  <summary>Details</summary>
Motivation: 强化学习应用于长上下文推理时面临结果奖励稀疏的问题，无法有效惩罚无根据的"幸运猜测"，导致证据检索过程缺乏监督

Method: 1) 建立证据增强推理范式，验证精确证据提取是长上下文推理的关键瓶颈；2) 提出EAPO算法，使用奖励模型计算组相对证据奖励，提供密集过程监督；3) 引入自适应奖励-策略协同进化机制，迭代优化奖励模型

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能

Conclusion: EAPO通过密集过程监督和自适应奖励-策略协同进化，有效解决了长上下文推理中的稀疏奖励问题，提升了证据检索质量和整体推理性能

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [27] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个基于临床推理的HRV解释框架，通过八步可追溯推理步骤和Z-score优先级层次结构，有效减少LLM在HRV分析中的生理幻觉问题，在情绪分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在心率变异性解释中存在生理幻觉问题，包括呼吸性窦性心律失常污染、非线性指标短数据不稳定性以及忽视个体化基线而偏好群体标准。这些问题限制了LLM在临床HRV分析中的应用。

Method: 提出C-GRASP框架，包含八步可追溯推理步骤，核心是Z-score优先级层次结构，强调个体化基线变化优于群体统计标准。系统通过自动化RSA感知防护机制减少频域指标污染，采用检索增强生成技术。

Result: 在DREAMER数据集的414个试验中，C-GRASP与高规模推理模型（如MedGemma3-thinking）结合，在4类情绪分类中达到37.3%的准确率，临床推理一致性得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持系统，为生物医学工程中更安全的AI集成铺平了道路，有效防止了原生LLM中常见的"群体偏见"。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [28] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: 本文提出LatentRefusal方法，通过分析LLM隐藏层激活来预测查询的可回答性，为文本到SQL系统提供安全拒绝机制


<details>
  <summary>Details</summary>
Motivation: 在基于LLM的文本到SQL系统中，不可回答和未明确指定的用户查询可能生成错误的SQL程序，导致误导性结果或违反安全约束，这是安全部署的主要障碍。现有的拒绝策略要么依赖输出级指令跟随（易受模型幻觉影响），要么依赖输出不确定性估计（增加复杂性和开销）。

Method: 将安全拒绝形式化为可回答性门控问题，提出LatentRefusal方法，通过分析大语言模型中间隐藏层激活来预测查询可回答性。引入Tri-Residual Gated Encoder轻量级探测架构，抑制模式噪声并放大指示不可回答性的问题-模式不匹配的稀疏局部线索。

Result: 在四个基准测试中，LatentRefusal将平均F1分数提高到88.5%，同时在两个骨干模型上都保持这一性能，仅增加约2毫秒的探测开销。广泛的实证评估、消融研究和可解释性分析证明了该方法的有效性。

Conclusion: LatentRefusal为文本到SQL系统提供了一个可附加且高效的安全层，能够有效处理不可回答和模糊查询，提高系统安全性和可靠性。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [29] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个错误感知的自动问题生成评估框架，通过显式错误诊断和两阶段评估流程，解决了现有方法忽视关键缺陷和过度评估的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成评估方法（包括基于LLM的评估器）主要采用黑盒和整体评估范式，缺乏显式错误建模，导致忽视事实幻觉和答案不匹配等关键缺陷，并过度评估问题质量。

Method: ErrEval将评估重新构建为两阶段过程：1）错误诊断阶段，使用轻量级即插即用的错误识别器检测和分类结构、语言和内容方面的常见错误；2）知情评分阶段，将这些诊断信号作为显式证据指导LLM评估器进行更细粒度和基于事实的判断。

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示纳入显式诊断能提高与人类判断的一致性。进一步分析证实ErrEval有效缓解了对低质量问题的过度评估。

Conclusion: ErrEval通过显式错误诊断和两阶段评估框架，显著提升了自动问题生成评估的准确性和可靠性，解决了现有评估方法的局限性。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [30] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种基于患者-医生范式的高效测试时对齐框架，通过细粒度token级奖励获取和流引导偏好优化，在保持生成多样性的同时实现精确对齐


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，性能受限且难以保持基础模型的生成多样性

Method: 采用患者-医生范式，从患者LLM的行为变化中提取细粒度token级偏好信号，通过token级流引导偏好优化（TFPO）训练较小的医生模型来引导大型冻结患者模型

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越DPO等完全微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐框架，在保持生成多样性的同时实现了优于传统方法的对齐性能

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [31] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差增强框架，专门为工业场景设计，通过非侵入式方式升级遗留GBDT模型，针对预测失败的"硬区域"进行修复，在金融风控系统中成功部署。


<details>
  <summary>Details</summary>
Motivation: 在工业表格应用中，GBDT模型占据主导地位，但在高并发生产环境中升级遗留模型面临昂贵的重新训练成本和系统性风险。需要一种安全、低成本的进化范式。

Method: 框架包含三个阶段：1) 通过残差找到硬区域；2) 使用大语言模型生成符号代码结构创建可解释专家，并用贝叶斯优化微调参数；3) 通过轻量级聚合器动态集成专家与遗留模型输出。

Result: 在六个公共数据集和一个私有数据集上显著优于最先进的基线方法，在真实在线数据上表现出优异的性能提升，成功部署在Qfin Holdings的核心金融风险控制系统中。

Conclusion: NSR-Boost有效捕捉传统模型遗漏的长尾风险，为工业界提供了一个安全、低成本的进化范式，特别适用于高并发生产环境中的模型升级。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [32] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出ChartComplete数据集，涵盖30种图表类型，弥补现有图表理解基准数据集仅覆盖少量图表类型的不足


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和计算机视觉技术的发展，图表理解领域快速发展，多模态大语言模型在图表理解方面表现出色。现有基准数据集仅覆盖少量图表类型，无法全面评估模型性能，需要更全面的数据集来填补这一空白。

Method: 基于可视化社区的图表分类学，构建ChartComplete数据集，涵盖30种不同的图表类型。数据集包含分类的图表图像，但不包含学习信号，以原始形式提供给研究社区。

Result: 创建了ChartComplete数据集，该数据集基于图表分类学，覆盖30种图表类型，为图表理解研究提供了更全面的基准资源。

Conclusion: ChartComplete数据集填补了现有图表理解基准数据集仅覆盖少量图表类型的空白，为研究社区提供了更全面的图表理解评估资源，有助于推动图表理解技术的发展。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [33] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出领域知识图谱融合任务(DKGF)，通过从通用知识图谱中整合相关事实来丰富领域知识图谱，并开发ExeFuse模型解决领域相关性和知识粒度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱相比通用知识图谱通常覆盖不足，需要从通用知识图谱中整合相关事实来丰富领域知识，但面临领域相关性高模糊性和知识粒度不对齐两大挑战。

Method: 提出ExeFuse模型，采用事实即程序范式，将通用知识图谱中的每个事实视为潜在语义程序，将抽象关系映射为粒度感知操作符，并通过在目标领域知识图谱上执行程序来验证领域相关性。

Result: 构建了两个基准数据集DKGF(W-I)和DKGF(Y-I)，包含21个评估配置。大量实验验证了任务的重要性和模型的有效性，为DKGF提供了首个标准化测试平台。

Conclusion: 提出了领域知识图谱融合任务和ExeFuse模型，通过统一的概率框架同时解决相关性和粒度问题，为领域知识图谱的丰富提供了有效解决方案。

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [34] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 本文通过多层级诊断框架分析不同LLM在钓鱼检测任务上的泛化失败原因，发现架构与数据多样性的协同作用、架构依赖的泛化能力差异，以及某些架构（如Mistral）天生更具泛化性。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大型语言模型在专业任务上取得了最先进的性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键开放问题。需要理解模型泛化失败的根本原因。

Method: 采用多层级诊断框架进行跨架构研究：对Llama 3.1 8B、Gemma 2 9B和Mistral模型在钓鱼检测任务上进行微调，使用SHAP分析和机制可解释性技术来揭示泛化失败的根源。

Result: 发现三个关键结果：1) 泛化由架构与数据多样性的强大协同作用驱动，Gemma 2 9B在风格多样的"通才"数据集上达到>91% F1；2) 泛化高度依赖架构，Llama 3.1 8B在窄领域表现良好但无法整合多样数据；3) 某些架构天生更具泛化性，Mistral在多种训练范式下表现一致且稳健。

Conclusion: 通过识别导致这些失败的缺陷启发式方法，本文提供了诊断和理解泛化失败的具体方法论，强调可靠的AI需要深入验证架构、数据和训练策略之间的相互作用。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [35] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿AI模型进行综合安全评估，发现安全性能存在显著异质性，不同模型在基准安全、对抗对齐、多语言泛化和合规性方面存在明显权衡，强调需要标准化安全评估来准确评估现实风险。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型和多模态大语言模型在推理、感知和生成能力方面取得显著进步，但这些进步是否带来相应的安全改进尚不清楚，部分原因是现有评估实践局限于单一模态或威胁模型，需要综合评估前沿模型的安全性能。

Method: 对7个前沿模型（GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro、Seedream 4.5）进行综合安全评估，使用统一协议在语言、视觉语言和图像生成三种设置下进行评估，包括基准评估、对抗评估、多语言评估和合规评估。

Result: 安全性能呈现显著异质性：GPT-5.2在所有评估中表现一致强大且平衡；其他模型在基准安全、对抗对齐、多语言泛化和合规性方面存在明显权衡；语言和视觉语言模态在对抗评估下表现出显著脆弱性；文本到图像模型在受监管视觉风险类别中相对更强对齐，但在对抗性或语义模糊提示下仍然脆弱。

Conclusion: 前沿模型的安全性本质上是多维的，受模态、语言和评估方案的影响，强调需要标准化安全评估来准确评估现实风险，指导负责任的模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [36] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种利用LLM解码过程中潜在安全信号进行早期不安全内容检测的方法，显著提升安全性同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过安全对齐，但现有防御机制（解码约束和后处理检测器）难以应对复杂的越狱攻击，要么检测不鲁棒，要么过度降低模型实用性。研究发现即使成功越狱，模型在生成过程中仍会表现出潜在的安全相关信号。

Method: 通过显式提取和利用解码过程中的潜在安全信号，在生成早期检测不安全内容。该方法基于观察：即使被越狱，模型内部仍存在安全信号，但这些信号被流畅生成的驱动力所压制。

Result: 实验表明该方法在多种越狱攻击下显著增强安全性，同时在良性输入上保持低拒绝率，并保持响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [37] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张研究基于大语言模型（LLM）的智能体集体行为至关重要，需要采用交互主义范式来系统考察先验知识、嵌入价值观与社会背景如何共同塑造多智能体生成式AI系统中的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解基于大语言模型的智能体集体行为是一个关键研究领域，对社会具有重要的风险与收益影响。LLM的独特性质——即通过大规模预训练获得的知识初始化、隐含的社会先验，以及通过上下文学习进行适应的能力——需要新的理论框架来研究这些系统如何在社会环境中运作。

Method: 提出交互主义范式，包含替代性的理论基础、方法论和分析工具，用于系统研究先验知识、嵌入价值观与社会背景在多智能体生成式AI系统中的相互作用。重点关注理论、方法和跨学科对话四个关键方向。

Result: 论文提出了一个研究LLM智能体集体行为的新框架，强调需要超越传统方法，采用专门针对LLM独特特性的交互主义视角，以更好地理解和预测这些系统的社会影响。

Conclusion: 研究基于大语言模型的智能体集体行为需要新的理论范式，重点关注先验知识、价值观与社会背景的交互作用，这对于负责任地开发和部署LLM集体系统至关重要。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [38] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专业智能体处理复杂基因组学查询，在GeneTuring基准测试中比当前最先进的GeneGPT系统平均提升12%性能。


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但从复杂的分布式数据库中提取数据仍然具有挑战性。大型语言模型在基因组问答方面有潜力，但受限于对领域特定数据库的访问限制。现有的GeneGPT系统虽然通过专用API调用增强了LLM，但存在API依赖性强和适应性有限的问题。

Method: 提出了GenomAgent多智能体框架，通过协调多个专业智能体来处理复杂的基因组学查询。该框架采用灵活的架构设计，能够有效整合和利用领域专业知识。

Result: 在GeneTuring基准测试的9个任务中，GenomAgent平均性能比GeneGPT高出12%。该框架不仅限于基因组学领域，还可扩展到需要专业知识提取的各种科学领域。

Conclusion: GenomAgent提供了一个灵活高效的多智能体框架，能够显著提升基因组问答系统的性能，并具有扩展到其他科学领域的潜力。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [39] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 本文提出了一种符号化算法，用于LTLf（有限迹线性时序逻辑）多属性综合问题，当无法同时满足所有属性时，通过单次不动点计算确定最大可实现属性集，相比枚举方法获得显著加速。


<details>
  <summary>Details</summary>
Motivation: 在多属性LTLf综合问题中，同时满足所有属性往往不可行。传统方法需要枚举属性子集，这在属性数量较多时计算开销巨大。需要一种更高效的方法来直接计算最大可实现属性集。

Method: 提出完全符号化算法：1) 引入布尔目标变量表示属性组合；2) 利用单调性紧凑表示指数级数量的目标组合；3) 通过单次不动点计算状态与可实现目标集的关系；4) 综合实现最大可实现属性集的策略。

Result: 该方法显著优于基于枚举的基准方法，加速比可达两个数量级。符号化表示有效处理了指数级的目标组合，单次不动点计算避免了多次重复计算。

Conclusion: 提出的符号化算法为LTLf多属性综合问题提供了高效解决方案，通过紧凑表示和单次计算大幅提升性能，适用于实际应用中属性数量较多的情况。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [40] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色但存在"猜测"而非"推理"的问题，研究发现三个意外事实：简单谜题失败、推理步骤中的"顿悟"动态、多重不动点存在。基于此提出三种扩展猜测策略，开发Augmented HRM将数独极端任务准确率从54.5%提升至96.9%。


<details>
  <summary>Details</summary>
Motivation: 尽管分层推理模型在各种推理任务中表现出色，但其推理机制尚不明确。研究旨在理解HRM的优势和潜在失败模式，揭示其实际是"猜测"而非"推理"的本质，并基于此改进模型性能。

Method: 对HRM的推理模式进行机制研究，发现三个关键事实：简单谜题失败（违反不动点属性）、推理步骤中的"顿悟"动态、多重不动点存在。基于"猜测"视角提出三种扩展策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测次数）、模型引导（利用训练随机性增加猜测次数）。

Result: 通过结合所有方法开发Augmented HRM，在Sudoku-Extreme任务上准确率从54.5%大幅提升至96.9%。研究揭示了推理模型实际是通过"猜测"而非系统推理来工作。

Conclusion: HRM本质上是通过"猜测"而非系统推理来解决问题，这解释了其优势和失败模式。提出的三种扩展策略有效提升了模型性能，为理解推理模型的工作机制提供了新视角。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [41] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出了一种基于结构感知和多样性约束的上下文气泡构建框架，用于解决传统RAG中信息碎片化、内容重复和查询上下文不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法通过排名和选择top-k段落来构建LLM上下文，但这种方法会导致信息图碎片化、内容过度检索和重复，同时无法充分捕捉查询的二级和三级层面信息。

Method: 提出了一个结构感知和多样性约束的上下文气泡构建框架，通过组织多粒度跨度（如章节和行），利用任务条件结构先验指导检索，从高相关性锚点跨度开始，通过平衡查询相关性、边际覆盖和冗余惩罚的约束选择来构建上下文气泡。

Result: 在企业文档上的实验表明，上下文气泡方法显著减少了冗余上下文，更好地覆盖了次要层面信息，在有限上下文窗口内提高了答案质量和引用忠实度。消融研究显示结构先验和多样性约束选择都是必要的。

Conclusion: 该框架通过结构感知和多样性约束的方法，有效解决了传统RAG中的信息碎片化和内容重复问题，提供了更紧凑、信息更丰富的上下文集合，同时具有可审计性和确定性调优能力。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [42] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 生成式AI对建筑概念设计任务的影响研究：AI对新手设计师有性能提升，但会降低一般创造性自我效能感，认知负荷无显著差异，效果取决于用户专业水平和提示策略。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在建筑概念设计任务中对设计性能、创造性自我效能感和认知负荷的影响，探索AI工具在不同专业水平用户中的效果差异。

Method: 36名学生参与者完成两阶段建筑设计任务：先独立设计，再使用外部工具（生成式AI辅助组和对照组使用现有建筑项目在线资源库）。设计成果由专家评估，自我效能感和认知负荷通过自我报告测量，采用双重差分分析。

Result: 生成式AI在所有参与者中没有整体性能优势，但对新手设计师显著提升设计性能；使用AI的学生一般创造性自我效能感下降；认知负荷无显著差异；迭代想法生成和视觉反馈提示与认知负荷更大降低相关。

Conclusion: 生成式AI的效果取决于用户的先前专业知识和通过提示的交互策略，AI对新手设计师有益但可能削弱创造性自我效能感，提示策略对认知负荷有调节作用。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [43] [Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute](https://arxiv.org/abs/2601.09756)
*David Brundage*

Main category: cs.CR

TL;DR: 评估大型语言模型生成的合成叙事是否能改善兽医电子健康记录的去标识化安全性，比较了合成数据增强和固定预算替换两种训练策略。


<details>
  <summary>Details</summary>
Motivation: 兽医电子健康记录包含隐私敏感标识符，限制了二次使用。虽然PetEVAL提供了兽医去标识化的基准，但该领域资源仍然匮乏。本研究旨在评估LLM生成的合成叙事是否能改善去标识化安全性。

Method: 使用PetEVAL衍生的语料库（3,750个保留集/1,249个训练集），采用隐私保护的"仅模板"机制生成10,382个合成笔记。使用三种transformer架构（PetBERT、VetBERT、Bio_ClinicalBERT）在不同混合比例下训练。评估以文档级泄漏率（至少有一个遗漏标识符的文档比例）为主要安全指标。

Result: 在固定样本替换下，用合成笔记替换真实笔记会单调增加泄漏率，表明合成数据不能安全替代真实监督。在计算匹配训练中，适度的合成混合与纯真实数据性能相当，但高合成比例会降低效用。相反，epoch扩展增强改善了性能：PetBERT的span-overlap F1从0.831提高到0.850±0.014，泄漏率从6.32%降至4.02%±0.19%。但这些增益主要反映了训练暴露的增加而非合成数据的内在质量。

Conclusion: 合成数据增强对于扩展训练暴露是有效的，但对于安全关键的兽医去标识化任务来说是补充性的，不能替代真实数据。合成数据存在系统性质量不匹配问题，限制了其作为真实数据替代品的安全性。

Abstract: Veterinary electronic health records (vEHRs) contain privacy-sensitive identifiers that limit secondary use. While PetEVAL provides a benchmark for veterinary de-identification, the domain remains low-resource. This study evaluates whether large language model (LLM)-generated synthetic narratives improve de-identification safety under distinct training regimes, emphasizing (i) synthetic augmentation and (ii) fixed-budget substitution. We conducted a controlled simulation using a PetEVAL-derived corpus (3,750 holdout/1,249 train). We generated 10,382 synthetic notes using a privacy-preserving "template-only" regime where identifiers were removed prior to LLM prompting. Three transformer backbones (PetBERT, VetBERT, Bio_ClinicalBERT) were trained under varying mixtures. Evaluation prioritized document-level leakage rate (the fraction of documents with at least one missed identifier) as the primary safety outcome. Results show that under fixed-sample substitution, replacing real notes with synthetic ones monotonically increased leakage, indicating synthetic data cannot safely replace real supervision. Under compute-matched training, moderate synthetic mixing matched real-only performance, but high synthetic dominance degraded utility. Conversely, epoch-scaled augmentation improved performance: PetBERT span-overlap F1 increased from 0.831 to 0.850 +/- 0.014, and leakage decreased from 6.32% to 4.02% +/- 0.19%. However, these gains largely reflect increased training exposure rather than intrinsic synthetic data quality. Corpus diagnostics revealed systematic synthetic-real mismatches in note length and label distribution that align with persistent leakage. We conclude that synthetic augmentation is effective for expanding exposure but is complementary, not substitutive, for safety-critical veterinary de-identification.

</details>


### [44] [A Risk-Stratified Benchmark Dataset for Bad Randomness (SWC-120) Vulnerabilities in Ethereum Smart Contracts](https://arxiv.org/abs/2601.09836)
*Hadis Rezaei,Rahim Taheri,Francesco Palmieri*

Main category: cs.CR

TL;DR: 本文提出了一个包含1752个以太坊智能合约的基准数据集，专门针对Bad Randomness漏洞（SWC-120），通过五阶段方法验证漏洞并首次提供函数级验证和风险分层。


<details>
  <summary>Details</summary>
Motivation: 当前以太坊智能合约中依赖区块属性（如block.timestamp或blockhash）生成随机数的应用存在Bad Randomness漏洞，现有检测工具只能识别简单模式且缺乏大规模准确标注的数据集，阻碍了工具改进。

Method: 开发了五阶段方法：1) 关键词过滤；2) 使用58个正则表达式进行模式匹配；3) 风险分类；4) 函数级验证；5) 上下文分析。将合约分为四个风险等级：高风险（无保护）、中风险（仅矿工可攻击）、低风险（仅所有者可攻击）、安全（使用Chainlink VRF或commit-reveal方案）。

Result: 创建了比现有数据集RNVulDet大51倍的基准数据集，包含1752个已验证的智能合约。函数级验证发现49%最初分类为受保护的合约实际上可被攻击，因为修饰符应用于与包含漏洞不同的函数。评估Slither和Mythril显示两者均未检测到样本中的任何漏洞合约，表明现有工具在处理复杂随机数模式方面存在显著局限性。

Conclusion: 本文提供了首个大规模、经过函数级验证和风险分层的Bad Randomness漏洞数据集，揭示了现有检测工具的严重不足，为智能合约安全研究提供了重要资源，数据集和验证脚本已公开可用。

Abstract: Many Ethereum smart contracts rely on block attributes such as block.timestamp or blockhash to generate random numbers for applications like lotteries and games. However, these values are predictable and miner-manipulable, creating the Bad Randomness vulnerability (SWC-120) that has led to real-world exploits. Current detection tools identify only simple patterns and fail to verify whether protective modifiers actually guard vulnerable code. A major obstacle to improving these tools is the lack of large, accurately labeled datasets. This paper presents a benchmark dataset of 1,752 Ethereum smart contracts with validated Bad Randomness vulnerabilities. We developed a five-phase methodology comprising keyword filtering, pattern matching with 58 regular expressions, risk classification, function-level validation, and context analysis. The function-level validation revealed that 49% of contracts initially classified as protected were actually exploitable because modifiers were applied to different functions than those containing vulnerabilities. We classify contracts into four risk levels based on exploitability: HIGH_RISK (no protection), MEDIUM_RISK (miner-exploitable only), LOW_RISK (owner-exploitable only), and SAFE (using Chainlink VRF or commit-reveal). Our dataset is 51 times larger than RNVulDet and the first to provide function-level validation and risk stratification. Evaluation of Slither and Mythril revealed significant detection gaps, as both tools identified none of the vulnerable contracts in our sample, indicating limitations in handling complex randomness patterns. The dataset and validation scripts are publicly available to support future research in smart contract security.

</details>


### [45] [SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations](https://arxiv.org/abs/2601.10004)
*Mohoshin Ara Tahera,Karamveer Singh Sidhu,Shuvalaxmi Dass,Sajal Saha*

Main category: cs.CR

TL;DR: 这篇SoK论文系统分析了LLM在医疗领域应用中的隐私安全威胁，涵盖数据预处理、微调和推理三个阶段，提出了威胁模型和防御建议。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗领域的应用日益广泛，但医疗数据的敏感性和医疗工作流程的高风险性带来了显著的隐私和安全挑战，特别是在异构部署环境中。

Method: 采用系统化知识（SoK）方法，分析LLM在医疗应用中的三个核心阶段：数据预处理、微调和推理；构建详细的威胁模型，包括攻击者、能力和攻击面；系统化现有隐私保护技术的缓解效果。

Result: 现有防御技术显示出一定潜力，但在保护敏感临床数据方面仍存在持续局限性，特别是在不同操作层级之间。

Conclusion: 提出了针对不同阶段的建议和未来研究方向，旨在加强LLM在受监管环境中的隐私保障，为理解LLM、威胁和隐私在医疗领域的交叉提供了基础。

Abstract: Large Language Models (LLMs) are increasingly adopted in healthcare to support clinical decision-making, summarize electronic health records (EHRs), and enhance patient care. However, this integration introduces significant privacy and security challenges, driven by the sensitivity of clinical data and the high-stakes nature of medical workflows. These risks become even more pronounced across heterogeneous deployment environments, ranging from small on-premise hospital systems to regional health networks, each with unique resource limitations and regulatory demands. This Systematization of Knowledge (SoK) examines the evolving threat landscape across the three core LLM phases: Data preprocessing, Fine-tuning, and Inference within realistic healthcare settings. We present a detailed threat model that characterizes adversaries, capabilities, and attack surfaces at each phase, and we systematize how existing privacy-preserving techniques (PPTs) attempt to mitigate these vulnerabilities. While existing defenses show promise, our analysis identifies persistent limitations in securing sensitive clinical data across diverse operational tiers. We conclude with phase-aware recommendations and future research directions aimed at strengthening privacy guarantees for LLMs in regulated environments. This work provides a foundation for understanding the intersection of LLMs, threats, and privacy in healthcare, offering a roadmap toward more robust and clinically trustworthy AI systems.

</details>


### [46] [Privacy Enhanced PEFT: Tensor Train Decomposition Improves Privacy Utility Tradeoffs under DP-SGD](https://arxiv.org/abs/2601.10045)
*Pradip Kunwar,Minh Vu,Maanak Gupta,Manish Bhattarai*

Main category: cs.CR

TL;DR: TTLoRA-DP：一种基于张量分解的隐私保护参数高效微调方法，通过结构约束提升隐私-效用平衡


<details>
  <summary>Details</summary>
Motivation: 大语言模型在敏感数据上微调存在隐私风险，成员推理攻击可揭示训练数据。差分隐私提供形式化保护，但应用于传统参数高效微调方法（如LoRA）会导致显著的效用损失。

Method: 提出TTLoRA-DP框架，将张量分解低秩适配与差分隐私结合。扩展ghost clipping算法到张量核心，通过缓存收缩状态实现高效DP-SGD，无需计算完整梯度。

Result: 在GPT-2微调实验中，TTLoRA-DP相比LoRA-DP提供更强的隐私保护，同时保持相当或更好的下游效用。即使无DP训练，TTLoRA也表现出更低的成员泄漏，参数数量平均减少7.6倍。

Conclusion: TTLoRA为参数高效语言模型适配中的隐私-效用平衡提供了实用路径，通过结构约束的参数空间缩小在保持表达力的同时增强隐私保护。

Abstract: Fine-tuning large language models on sensitive data poses significant privacy risks, as membership inference attacks can reveal whether individual records were used during training. While Differential Privacy (DP) provides formal protection, applying DP to conventional Parameter-Efficient Fine-Tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) often incurs substantial utility loss. In this work, we show that a more structurally constrained PEFT architecture, Tensor Train Low-Rank Adaptation (TTLoRA), can improve the privacy-utility tradeoff by shrinking the effective parameter space while preserving expressivity. To this end, we develop TTLoRA-DP, a differentially private training framework for TTLoRA. Specifically, we extend the ghost clipping algorithm to Tensor Train cores via cached contraction states, enabling efficient Differentially Private Stochastic Gradient Descent (DP-SGD) with exact per-example gradient norm computation without materializing full per-example gradients. Experiments on GPT-2 fine-tuning over the Enron and Penn Treebank datasets show that TTLoRA-DP consistently strengthens privacy protection relative to LoRA-DP while maintaining comparable or better downstream utility. Moreover, TTLoRA exhibits lower membership leakage even without DP training, using substantially smaller adapters and requiring on average 7.6X fewer parameters than LoRA. Overall, our results demonstrate that TTLoRA offers a practical path to improving the privacy-utility tradeoff in parameter-efficient language model adaptation.

</details>


### [47] [Fuzzychain-edge: A novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing](https://arxiv.org/abs/2601.10105)
*Khushbakht Farooq,Muhammad Ibrahim,Irsa Manzoor,Mukhtaj Khan,Wei Song*

Main category: cs.CR

TL;DR: 提出Fuzzychain-edge框架，结合模糊逻辑、区块链和零知识证明，为边缘计算中的物联网系统提供自适应访问控制，解决数据隐私和安全漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 物联网与边缘计算融合在医疗等领域带来实时数据共享和远程监控，但传统访问控制机制和集中式安全系统无法解决数据隐私泄露和安全漏洞问题，特别是在处理敏感信息的环境中。

Method: 提出Fuzzychain-edge框架，整合零知识证明（保护敏感数据验证）、模糊逻辑（实现自适应上下文感知决策）和区块链智能合约（确保透明度和可追溯性），通过动态评估数据敏感性、信任等级和用户角色来实现访问控制。

Result: 预期成果包括增强数据隐私保护、提高访问控制准确性、减少未授权访问风险、提供透明审计追踪，从而提升用户对物联网系统的信任度。

Conclusion: 该研究为物联网环境提供了隐私保护、安全且可追溯的解决方案，为医疗等关键领域的去中心化技术创新奠定了基础。

Abstract: The rapid integration of IoT with edge computing has revolutionized various domains, particularly healthcare, by enabling real-time data sharing, remote monitoring, and decision-making. However, it introduces critical challenges, including data privacy breaches, security vulnerabilities, especially in environments dealing with sensitive information. Traditional access control mechanisms and centralized security systems do not address these issues, leaving IoT environments exposed to unauthorized access and data misuse. This research proposes Fuzzychain-edge, a novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing framework designed to overcome these limitations by incorporating Zero-Knowledge Proofs (ZKPs), fuzzy logic, and smart contracts. ZKPs secure sensitive data during access control processes by enabling verification without revealing confidential details, thereby ensuring user privacy. Fuzzy logic facilitates adaptive, context-aware decision-making for access control by dynamically evaluating parameters such as data sensitivity, trust levels, and user roles. Blockchain technology, with its decentralized and immutable architecture, ensures transparency, traceability, and accountability using smart contracts that automate access control processes. The proposed framework addresses key challenges by enhancing security, reducing the likelihood of unauthorized access, and providing a transparent audit trail of data transactions. Expected outcomes include improved data privacy, accuracy in access control, and increased user trust in IoT systems. This research contributes significantly to advancing privacy-preserving, secure, and traceable solutions in IoT environments, laying the groundwork for future innovations in decentralized technologies and their applications in critical domains such as healthcare and beyond.

</details>


### [48] [Advanced Encryption Technique for Multimedia Data Using Sudoku-Based Algorithms for Enhanced Security](https://arxiv.org/abs/2601.10119)
*Mithil Bavishi,Anuj Bohra,Kushal Vadodaria,Abhinav Bohra,Neha Katre,Ramchandra Mangrulkar,Vinaya Sawant*

Main category: cs.CR

TL;DR: 本文扩展了基于数独的加密方法，将其应用于图像、音频和视频等多种媒体，并通过时间戳增强密钥安全性，形成支持多种加密模式的通用系统。


<details>
  <summary>Details</summary>
Motivation: 现有加密方法需要扩展到多媒体数据保护，同时增强密钥生成的安全性以防止数据泄露。通过结合时间戳和数独加密原理，提高对暴力破解和差分攻击的抵抗能力。

Method: 采用基于数独的块置换密码系统，支持置换和替代（如XOR）两种加密模式。密钥生成依赖于消息传输的时间戳，系统可处理图像、音频和视频等多种媒体格式。

Result: 加密后的媒体具有高度安全性：图像NPCR值接近100%，音频SNR值超过60dB，表明加密效果显著。系统对暴力破解和差分攻击具有强抵抗力，加密音频与源文件差异巨大，增加解密难度。

Conclusion: 提出的基于数独的时间戳依赖加密系统是通用、安全的多媒体加密解决方案，在多种媒体格式上表现出优异的加密性能和安全特性。

Abstract: Encryption and Decryption is the process of sending a message in a ciphered way that appears meaningless and could be deciphered using a key for security purposes to avoid data breaches. This paper expands on the previous work on Sudoku-based encryption methods, applying it to other forms of media including images, audio and video. It also enhances the security of key generation and usage by making it dependent on the timestamp of when the message was transmitted. It is a versatile system that works on multimodal data and functions as a block-based transposition cipher. Instead of shuffling, it can also employ substitution methods like XOR, making it a substitution cipher. The resulting media are highly encrypted and resilient to brute-force and differential attacks. For images, NPCR values approach 100% and for audio, SNR values exceed 60dB. This makes the encrypted audio significantly different from the source, making decryption more difficult.

</details>


### [49] [ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack](https://arxiv.org/abs/2601.10173)
*Hao Li,Yankai Yang,G. Edward Suh,Ning Zhang,Chaowei Xiao*

Main category: cs.CR

TL;DR: ReasAlign是一种针对间接提示注入攻击的模型级防御方案，通过结构化推理步骤分析用户查询、检测冲突指令，并保持用户任务连续性，在保持实用性的同时显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的智能体系统虽然能自动化复杂工作流程，但极易受到间接提示注入攻击的攻击，恶意指令嵌入外部数据可劫持智能体行为，现有防御方案在安全性和实用性之间存在权衡问题。

Method: ReasAlign采用结构化推理步骤分析用户查询、检测冲突指令、保持任务连续性；引入测试时缩放机制，通过偏好优化的评判模型对推理步骤评分并选择最佳轨迹。

Result: 在CyberSecEval2基准测试中，ReasAlign达到94.6%的实用性和仅3.6%的攻击成功率，远超Meta SecAlign的56.4%实用性和74.4%攻击成功率，实现了安全性与实用性的最佳平衡。

Conclusion: ReasAlign通过结构化推理和测试时缩放机制，在保持实用性的同时显著提升对间接提示注入攻击的防御能力，为现实世界智能体系统提供了强大实用的安全解决方案。

Abstract: Large Language Models (LLMs) have enabled the development of powerful agentic systems capable of automating complex workflows across various fields. However, these systems are highly vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external data can hijack agent behavior. In this work, we present ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea of ReasAlign is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user's intended tasks to defend against indirect injection attacks. To further ensure reasoning logic and accuracy, we introduce a test-time scaling mechanism with a preference-optimized judge model that scores reasoning steps and selects the best trajectory. Comprehensive evaluations across various benchmarks show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming Meta SecAlign, the strongest prior guardrail. On the representative open-ended CyberSecEval2 benchmark, which includes multiple prompt-injected tasks, ReasAlign achieves 94.6% utility and only 3.6% ASR, far surpassing the state-of-the-art defensive model of Meta SecAlign (56.4% utility and 74.4% ASR). These results demonstrate that ReasAlign achieves the best trade-off between security and utility, establishing a robust and practical defense against prompt injection attacks in real-world agentic systems. Our code and experimental results could be found at https://github.com/leolee99/ReasAlign.

</details>


### [50] [XuanJia: A Comprehensive Virtualization-Based Code Obfuscator for Binary Protection](https://arxiv.org/abs/2601.10261)
*Xianyu Zou,Xiaoli Gong,Jin Zhang,Shiyang Li,Pen-Chung Yew*

Main category: cs.CR

TL;DR: XuanJia是一个虚拟机二进制混淆框架，提供端到端的代码和异常处理语义保护，通过ABI兼容的EH影子机制消除异常处理元数据泄露，同时保持操作系统运行时兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有基于虚拟化的二进制混淆方法为了保护ABI兼容性，会暴露异常处理元数据，这些数据泄露了堆栈布局、控制流边界和对象生命周期等结构信息，容易被逆向工程利用。

Method: 提出ABI兼容的EH影子机制：用ABI兼容的影子展开信息替换原生EH元数据以满足操作系统驱动的展开需求，并将异常处理安全重定向到受保护的虚拟机中，在那里使用混淆代码解密、反转和重放真正的EH语义。

Result: XuanJia实现了385个x86指令编码和155个VM处理程序模板，在正确性、抗逆向性和性能方面表现良好：保持语义等价性，有效干扰IDA Pro等自动化逆向工具，空间开销可忽略，运行时开销适中。

Conclusion: XuanJia在不牺牲正确性或实用性的前提下，实现了对异常处理逻辑的强有力保护，证明了端到端保护异常处理语义的可行性。

Abstract: Virtualization-based binary obfuscation is widely adopted to protect software intellectual property, yet existing approaches leave exception-handling (EH) metadata unprotected to preserve ABI compatibility. This exposed metadata leaks rich structural information, such as stack layouts, control-flow boundaries, and object lifetimes, which can be exploited to facilitate reverse engineering. In this paper, we present XuanJia, a comprehensive VM-based binary obfuscation framework that provides end-to-end protection for both executable code and exception-handling semantics. At the core of XuanJia is ABI-Compliant EH Shadowing, a novel exception-aware protection mechanism that preserves compatibility with unmodified operating system runtimes while eliminating static EH metadata leakage. XuanJia replaces native EH metadata with ABI-compliant shadow unwind information to satisfy OS-driven unwinding, and securely redirects exception handling into a protected virtual machine where the genuine EH semantics are decrypted, reversed, and replayed using obfuscated code. We implement XuanJia from scratch, supporting 385 x86 instruction encodings and 155 VM handler templates, and design it as an extensible research testbed. We evaluate XuanJia across correctness, resilience, and performance dimensions. Our results show that XuanJia preserves semantic equivalence under extensive dynamic and symbolic testing, effectively disrupts automated reverse-engineering tools such as IDA Pro, and incurs negligible space overhead and modest runtime overhead. These results demonstrate that XuanJia achieves strong protection of exception-handling logic without sacrificing correctness or practicality.

</details>


### [51] [Hybrid Encryption with Certified Deletion in Preprocessing Model](https://arxiv.org/abs/2601.10542)
*Kunal Dey,Reihaneh Safavi-Naini*

Main category: cs.CR

TL;DR: 该论文提出了两种在预处理模型中的混合加密与认证删除方案，结合信息论密钥封装机制与提供认证删除的数据封装机制，分别实现信息论认证删除和永久认证删除，支持任意长度消息加密。


<details>
  <summary>Details</summary>
Motivation: 现有认证删除方案要么依赖一次性密码本加密，要么使用可能被未来经典或量子计算突破的计算硬度假设。需要开发更安全的认证删除方案，既能提供信息论安全性，又能支持长消息加密。

Method: 提出预处理模型中的混合加密与认证删除(pHE-CD)框架，结合信息论密钥封装机制(iKEM)和提供认证删除的数据封装机制(DEM-CD)。第一种构造实现信息论认证删除，第二种构造实现永久认证删除，后者在删除前提供计算安全性，删除后提供信息论安全性。

Result: 提出的pHE-CD方案提供IND-qe-CPA安全性，支持任意长度消息加密。第二种构造使用量子安全DEM-CD（结合量子编码和AES）实现量子安全，且密钥长度显著短于消息长度。

Conclusion: 该工作为认证删除提供了新的混合加密框架，结合了信息论和计算安全性的优势。未来工作包括使用量子密钥封装机制(qKEM)作为iKEM来实例化该框架。

Abstract: Certified deletion allows Alice to outsource data to Bob and, at a later time, obtain a verifiable guarantee that the file has been irreversibly deleted at her request. The functionality, while impossible using classical information alone, can be achieved using quantum information. Existing approaches, rely on one-time pad (OTP) encryption, or use computational hardness assumptions that may be vulnerable to future advances in classical or quantum computing. In this work, we introduce and formalize hybrid encryption with certified deletion in the preprocessing model (pHE-CD) and propose two constructions. The constructions combine an information-theoretic key encapsulation mechanism (iKEM) with a data encapsulation mechanism that provides certified deletion (DEM-CD) and, respectively, provide {\em information-theoretic certified deletion}, where both confidentiality and deletion properties are provided against a computationally unbounded adversary; and {\em everlasting certified deletion}, where confidentiality is computational before deletion, and upon successful verification of the deletion certificate, the message becomes information-theoretically hidden from an adversary that is computationally unbounded. Our pHE-CD schemes provide IND-$q_e$-CPA notion of security and support encryption of arbitrarily long messages. In the second construction, using a computationally secure DEM-CD that is quantum-safe (i.e. constructed using quantum coding and AES), we obtain quantum-safe security with keys that are significantly shorter than the message. Instantiating the proposed framework using quantum enabled kem (qKEM) as the iKEM, is a future work.

</details>


### [52] [Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay](https://arxiv.org/abs/2601.10589)
*Hao Wang,Yanting Wang,Hao Li,Rui Li,Lei Sha*

Main category: cs.CR

TL;DR: SSP框架让单个大语言模型在强化学习循环中同时扮演攻击者和防御者角色，通过自我对抗训练实现自主进化的安全对齐，显著优于基于静态对抗数据集的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐方法过度依赖静态外部红队测试，使用固定的防御提示或预收集的对抗数据集，导致防御僵化、过拟合已知模式，无法泛化到新颖复杂的威胁。

Method: 提出Safety Self-Play (SSP)系统，让单个LLM在统一的强化学习循环中同时扮演攻击者（生成越狱攻击）和防御者（拒绝有害请求）角色。引入反思经验回放机制，使用UCB采样策略专注于低奖励的失败案例，帮助模型从过去的困难错误中学习，平衡探索与利用。

Result: 大量实验表明，SSP方法能够自主进化出强大的防御能力，显著优于基于静态对抗数据集训练的基线方法，为主动安全对齐建立了新的基准。

Conclusion: 通过让模型成为自己的红队测试者，SSP框架实现了自主进化的安全对齐，解决了当前静态防御方法的局限性，为LLM安全提供了更灵活、更强大的防御机制。

Abstract: Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to novel, sophisticated threats. To address this critical limitation, we propose empowering the model to be its own red teamer, capable of achieving autonomous and evolving adversarial attacks. Specifically, we introduce Safety Self- Play (SSP), a system that utilizes a single LLM to act concurrently as both the Attacker (generating jailbreaks) and the Defender (refusing harmful requests) within a unified Reinforcement Learning (RL) loop, dynamically evolving attack strategies to uncover vulnerabilities while simultaneously strengthening defense mechanisms. To ensure the Defender effectively addresses critical safety issues during the self-play, we introduce an advanced Reflective Experience Replay Mechanism, which uses an experience pool accumulated throughout the process. The mechanism employs a Upper Confidence Bound (UCB) sampling strategy to focus on failure cases with low rewards, helping the model learn from past hard mistakes while balancing exploration and exploitation. Extensive experiments demonstrate that our SSP approach autonomously evolves robust defense capabilities, significantly outperforming baselines trained on static adversarial datasets and establishing a new benchmark for proactive safety alignment.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications](https://arxiv.org/abs/2601.10463)
*Xinyu Shi,Simei Yang,Francky Catthoor*

Main category: cs.AR

TL;DR: 论文提出了一种跨层方法对XR工作负载进行架构分类，识别出容量受限和开销敏感等关键工作负载原型，为下一代XR SoC设计提供指导，强调需要从通用资源扩展转向阶段感知调度和弹性资源分配。


<details>
  <summary>Details</summary>
Motivation: 扩展现实(XR)平台需要在严格功耗和面积约束下提供确定性超低延迟性能，但XR工作负载多样性快速增加，具有异构算子类型和复杂数据流结构，这对以CNN为中心的传统加速器架构构成挑战，且缺乏对完整XR管道的系统性架构理解。

Method: 采用跨层方法整合基于模型的高层设计空间探索(DSE)与在商用GPU和CPU硬件上的经验性性能分析，通过分析12个不同XR内核的代表性工作负载集，将其复杂架构特征提炼为少量跨层工作负载原型。

Result: 识别出容量受限和开销敏感等关键工作负载原型，提取了重要的架构洞察，为下一代XR SoC提供了可操作的设计指南，发现XR架构设计需要从通用资源扩展转向阶段感知调度和弹性资源分配。

Conclusion: XR架构设计必须从通用资源扩展转向阶段感知调度和弹性资源分配，以实现未来XR系统更高的能效和性能，跨层工作负载原型分析为下一代XR SoC设计提供了系统性的指导框架。

Abstract: Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 研究异构分布式线性可分计算问题，针对任意异构数据分配，提出通用计算方案和逆界，刻画任务函数可计算维度与通信成本之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同构设置（每个工作者持有相同数量的数据集），而实际分布式系统中数据分配往往是任意异构的。本文旨在研究更一般的任意异构数据分配场景，探索任务函数可计算维度与通信成本之间的基本权衡关系。

Method: 针对任意异构数据分配，提出通用计算方案和通用逆界，通过刻画数据分配的结构来表征可计算维度与通信成本的权衡。首先在整数通信成本约束下分析，然后将方案和逆界扩展到分数通信成本情况。

Result: 提出的通用计算方案和逆界在某些参数范围内重合，表明达到了最优权衡。通过扩展分析到分数通信成本情况，提供了更完整的理论框架。

Conclusion: 本文建立了异构分布式线性可分计算的理论基础，为任意异构数据分配场景提供了通用的计算方案和理论界限，揭示了可计算维度与通信成本之间的基本权衡关系。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>
