<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: CaR是一个基于显式学习可行性优化的神经路由求解器约束处理框架，通过联合训练指导构造模块生成适合轻量级改进的多样化高质量解，显著提升对硬约束的处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在简单路由问题上表现出色，但在复杂约束下表现有限。当前的约束处理方案（如可行性掩码或隐式可行性感知）对于硬约束可能效率低下或不适用。

Method: 提出Construct-and-Refine框架，基于显式学习可行性优化。设计联合训练框架指导构造模块生成适合轻量级改进（如10步vs之前5000步）的多样化高质量解。首次采用构造-改进共享表示，通过统一编码器实现跨范式知识共享。

Result: 在典型硬路由约束上的评估显示，CaR在可行性、解质量和效率方面均优于经典和神经最先进的求解器。

Conclusion: CaR是首个通用高效的神经路由求解器约束处理框架，通过显式学习可行性优化和构造-改进共享表示，显著提升了在复杂约束场景下的性能。

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [2] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型在教育自动评估中的不确定性量化方法，分析了不确定性模式及其影响因素，为开发更可靠的评估系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在教育自动评估中展现出优势，但其固有的概率特性引入了输出不确定性的挑战。评估结果对后续教学决策至关重要，不可靠的不确定性估计可能导致不稳定的教学干预，影响学生学习过程。

Method: 通过在多评估数据集、不同LLM家族和生成控制设置下进行综合分析，对广泛的不确定性量化方法进行基准测试，表征LLM在评分场景中的不确定性模式。

Result: 研究发现不同不确定性度量方法的优缺点，分析了模型家族、评估任务和解码策略等关键因素对不确定性估计的影响，揭示了LLM在自动评分中的不确定性特征。

Conclusion: 研究为理解LLM自动评估中的不确定性提供了可操作的见解，为未来开发更可靠、有效的不确定性感知评分系统奠定了基础。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [3] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 论文提出了一种训练框架，将交互式上下文学习能力视为可训练技能而非涌现属性，通过将单轮可验证任务转化为多轮教学互动，显著提升模型从语言反馈中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练主要依赖建模静态语料库，忽视了人类学习中基于纠正反馈动态调整思维过程的重要能力，特别是在协作环境中。现有模型在整合纠正反馈方面存在困难，需要专门的训练方法来提升交互式学习能力。

Method: 提出了一个可扩展的方法，将单轮可验证任务转化为由信息不对称驱动的多轮教学互动。通过训练模型预测教师的批评，有效建模反馈环境，将外部信号转化为内部能力，使模型能够在没有教师的情况下自我纠正。

Result: 使用该方法训练的小模型在多轮性能上接近大一个数量级的大模型。在数学问题上进行交互训练后，能够泛化到编程、谜题和迷宫导航等不同领域。模型展现出增强的上下文可塑性，并能通过自我纠正实现自我改进。

Conclusion: 交互式上下文学习能力可以作为可训练技能而非涌现属性来培养。通过将外部反馈信号转化为内部能力，模型能够实现自我纠正和自我改进，为语言模型训练提供了新的统一路径。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [4] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: GPSBench是一个包含57,800个样本、涵盖17个任务的基准数据集，用于评估大语言模型在GPS坐标和地理空间推理方面的能力，发现模型在现实世界地理推理方面比几何计算更可靠。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地应用于与物理世界交互的应用（如导航、机器人、地图），强大的地理空间推理能力变得至关重要，但模型在GPS坐标和真实世界地理推理方面的能力尚未得到充分探索。

Method: 引入GPSBench数据集，包含几何坐标操作（距离和方位计算）以及坐标与世界知识结合推理的任务。评估了14个最先进的大语言模型，专注于内在模型能力而非工具使用。

Result: GPS推理仍然具有挑战性，不同任务间存在显著差异：模型在现实世界地理推理方面通常比几何计算更可靠。地理知识呈层次性退化，国家层面表现强但城市层面定位弱。对坐标噪声的鲁棒性表明模型具有真正的坐标理解而非单纯记忆。

Conclusion: GPS坐标增强可以改善下游地理空间任务，微调会在几何计算收益和世界知识退化之间产生权衡。该研究为评估和改进大语言模型的地理空间推理能力提供了基准和方法。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [5] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 本文探讨了实现人工超智能所需的关键"记忆"设计概念，提出了"先存储后按需提取"等替代方法，强调保留原始经验以避免信息损失，并通过简单实验验证了这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的"先提取后存储"范式存在信息损失风险，因为提取过程中可能丢弃对不同任务有价值的知识。本文旨在探索更有效的记忆设计方法，以实现"用记忆提升世界"的使命。

Method: 提出了四种替代方法：1)"先存储后按需提取"方法，保留原始经验并根据任务需求灵活应用；2)从大量概率经验中发现深层洞察；3)通过共享存储经验提高经验收集效率；4)通过简单实验验证这些方法的有效性。

Result: 简单实验表明，这些看似直观有效的方法确实具有实际价值。特别是"先存储后按需提取"方法能够避免信息损失，为不同任务提供更丰富的知识基础。

Conclusion: 本文指出了限制这些有前景方向研究的主要挑战，并提出了相应的研究课题。强调需要进一步探索这些记忆设计方法，为实现人工超智能提供更有效的记忆系统。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [6] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: 序列模型通过上下文学习能力实现多智能体合作，无需硬编码假设或显式时间尺度分离，在多样化对手分布训练下自然涌现合作行为。


<details>
  <summary>Details</summary>
Motivation: 解决自利智能体在多智能体强化学习中的合作问题，现有方法依赖硬编码假设或严格的时间尺度分离，需要更自然、可扩展的合作机制。

Method: 使用序列模型训练智能体对抗多样化对手分布，利用序列模型的上下文学习能力实现快速情境内最佳响应策略，形成学习算法功能。

Result: 上下文适应使智能体易受勒索，相互塑造对手上下文学习动态的压力最终导致合作行为的学习，序列模型结合对手多样性可扩展学习合作行为。

Conclusion: 序列模型的上下文学习能力结合对手多样性为多智能体合作提供了自然、可扩展的路径，无需硬编码假设或显式时间尺度分离。

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [7] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: CAFE框架将自动特征工程重构为因果引导的序列决策过程，通过因果发现和强化学习相结合，在分布偏移下显著提升特征工程的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动特征工程方法依赖统计启发式，产生的特征在分布偏移下表现脆弱。需要引入因果结构作为软归纳先验，提升特征工程的鲁棒性。

Method: CAFE采用两阶段框架：第一阶段学习特征与目标之间的稀疏有向无环图，获得软因果先验，将特征按因果影响分组；第二阶段使用级联多智能体深度Q学习架构，通过分层奖励塑造和因果组级探索策略选择因果组和变换算子。

Result: 在15个公共基准测试中，CAFE相比强基线提升达7%，减少收敛所需回合数，在受控协变量偏移下性能下降减少约4倍，产生更紧凑的特征集和更稳定的后验归因。

Conclusion: 因果结构作为软归纳先验而非刚性约束，能显著提升自动特征工程的鲁棒性和效率，为分布偏移下的特征工程提供了新思路。

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [8] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: 本文探索使用大型语言模型作为不完美专家，通过因果假设论证框架将语义结构先验与条件独立性证据结合，实现最先进的因果发现性能


<details>
  <summary>Details</summary>
Motivation: 因果发现需要专家知识构建原则性因果图，但现有统计方法主要依赖观测数据。本文旨在探索如何利用大型语言模型作为不完美专家，结合语义信息和统计证据进行因果发现

Method: 采用因果假设论证框架，从变量名称和描述中提取语义结构先验，与条件独立性证据相结合。引入评估协议减轻记忆偏差对LLM评估的影响

Result: 在标准基准和语义基础合成图上展示了最先进的性能，验证了LLM作为不完美专家在因果发现中的有效性

Conclusion: 大型语言模型可以作为有效的语义先验来源，与因果假设论证框架结合能够提升因果发现性能，同时需要适当的评估协议来确保结果的可靠性

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [9] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: FoT是一个通用基础框架，用于构建和优化动态推理方案，解决了现有提示方案静态、缺乏适应性和优化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示方案如Chain of Thought、Tree of Thoughts等存在两个主要问题：1) 需要用户定义静态的、特定问题的推理结构，缺乏对动态或未见问题类型的适应性；2) 在超参数、提示、运行时间和提示成本方面通常未充分优化。

Method: 提出了Framework of Thoughts (FoT)，这是一个通用基础框架，具有超参数调优、提示优化、并行执行和智能缓存等内置功能，能够构建和优化动态推理方案。

Result: 通过将Tree of Thoughts、Graph of Thoughts和ProbTree三种流行方案在FoT中实现，实证表明FoT能够显著加快执行速度、降低成本，并通过优化获得更好的任务分数。

Conclusion: FoT框架释放了推理方案的潜在性能潜力，为开发未来动态高效的推理方案提供了基础，并已开源代码库以促进相关研究。

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [10] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: 通过七个月的诗歌工作坊，研究人员使用大型语言模型通过上下文专家反馈塑造了一个数字诗人，其创作的诗歌在盲测中与人类诗歌难以区分，最终由商业出版社出版了诗集。


<details>
  <summary>Details</summary>
Motivation: 探索机器能否创作出优秀的诗歌，这涉及到艺术本质和价值的根本问题，旨在研究AI在创造性写作领域的潜力及其对艺术创作和作者身份概念的挑战。

Method: 采用七个月的诗歌工作坊形式，通过迭代的上下文专家反馈（无需重新训练）将大型语言模型塑造成数字诗人，包括定量和定性分析，并为模型创建笔名和作者形象。

Result: 模型形成了独特的风格和连贯的作品集；在50名人文学生的盲测中，人类诗歌被识别为人类的准确率为54%，AI诗歌为52%（95%置信区间包含50%），表明无法可靠区分；模型的作品最终由商业出版社出版。

Conclusion: 工作坊式提示方法能够支持长期创造性塑造，AI诗歌已达到与人类诗歌难以区分的水平，这重新引发了关于创造性和作者身份的辩论，挑战了传统艺术创作观念。

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [11] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: Agent Skill框架能显著提升中等规模小语言模型（12B-30B参数）的性能，使其在数据安全和预算受限的工业场景中达到接近闭源模型的水平，而超小模型在技能选择方面仍存在困难。


<details>
  <summary>Details</summary>
Motivation: 研究Agent Skill范式是否能为小语言模型（SLMs）带来类似大模型的性能提升，解决工业场景中因数据安全和预算限制而无法持续依赖公共API的问题，以及SLMs在高度定制化场景中泛化能力有限的问题。

Method: 首先形式化定义了Agent Skill过程的数学定义，然后系统评估了不同规模的语言模型在多个用例中的表现，包括两个开源任务和一个真实世界的保险索赔数据集。

Result: 超小模型在可靠技能选择方面存在困难；中等规模SLMs（约12B-30B参数）从Agent Skill方法中获益显著；约80B参数的代码专用变体在性能上可与闭源基线相媲美，同时提高了GPU效率。

Conclusion: 研究全面而细致地描述了Agent Skill框架的能力和限制，为在SLM中心化环境中有效部署Agent Skills提供了可行的见解，特别是在数据安全和预算受限的工业场景中。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [Weak Zero-Knowledge and One-Way Functions](https://arxiv.org/abs/2602.16156)
*Rohit Chatterjee,Yunqi Li,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: 该论文研究了弱零知识协议对最坏情况困难语言存在性的影响，改进了先前关于误差参数的条件，建立了从弱零知识协议到单向函数存在的联系。


<details>
  <summary>Details</summary>
Motivation: 研究具有非可忽略误差（完整性、可靠性和零知识误差）的弱零知识协议的存在性对密码学基础的影响，特别是这些协议与单向函数存在性之间的关系。

Method: 在NP中存在最坏情况困难语言的假设下，分析不同误差参数组合条件下（ε_c+ε_s+ε_z < 1等），弱零知识协议的存在性如何蕴含单向函数的存在性。

Result: 1. 如果所有NP语言都有满足ε_c+ε_s+ε_z < 1的NIZK证明/论证，则单向函数存在；2. 对于k轮公开掷币ZK协议，若满足ε_c+ε_s+(2k-1)·ε_z < 1，则单向函数存在；3. 对于常数k轮协议，若满足ε_c+ε_s+k·ε_z < 1，则无限频繁单向函数存在。

Conclusion: 弱零知识协议的存在性与单向函数存在性之间存在紧密联系，改进了先前更严格的条件（ε_c+√ε_s+ε_z < 1），为密码学基础理论提供了新的见解。

Abstract: We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $ε_c$, $ε_s$, and $ε_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:
  1. If all languages in NP have NIZK proofs or arguments satisfying $ ε_c+ε_s+ ε_z < 1 $, then One-Way Functions (OWFs) exist.
  This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $ε_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ ε_c+\sqrt{ε_s}+ε_z < 1 $ [Chakraborty et al., CRYPTO 2025].
  2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+(2k-1).ε_z < 1 $, then OWFs exist.
  3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+k.ε_z < 1 $, then infinitely-often OWFs exist.

</details>


### [13] [SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2602.16480)
*Yiwen Lu*

Main category: cs.CR

TL;DR: SRFed是一个针对非独立同分布数据的联邦学习框架，结合了去中心化高效功能加密和隐私保护的防御性模型聚合机制，在保护隐私、抵抗拜占庭攻击和效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临两个关键安全威胁：好奇服务器可能发起推理攻击重建客户端私有数据，以及被攻陷的客户端可能发起投毒攻击破坏模型聚合。现有解决方案要么计算和通信开销高，要么在非独立同分布数据设置下表现不佳。

Method: 1. 设计去中心化高效功能加密方案，支持高效模型加密和非交互式解密，消除第三方依赖并防御服务器端推理攻击；2. 基于DEFE开发隐私保护的防御性模型聚合机制，通过分层投影和基于聚类的分析在非独立同分布数据下过滤有毒模型。

Result: 理论分析和大量实验表明，SRFed在隐私保护、拜占庭鲁棒性和效率方面优于最先进的基线方法。

Conclusion: SRFed是一个高效的拜占庭鲁棒且隐私保护的联邦学习框架，特别适用于非独立同分布场景，能同时防御服务器端推理攻击和客户端投毒攻击。

Abstract: Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.

</details>


### [14] [Policy Compiler for Secure Agentic Systems](https://arxiv.org/abs/2602.16708)
*Nils Palumbo,Sarthak Choudhary,Jihye Choi,Prasad Chalasani,Mihai Christodorescu,Somesh Jha*

Main category: cs.CR

TL;DR: PCAS是一个用于智能体系统的策略编译器，通过依赖图建模信息流，使用Datalog语言表达策略，提供确定性策略执行保障，将策略合规性从48%提升到93%。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在需要复杂授权策略的场景中部署增多，但将策略嵌入提示中无法提供执行保证，需要确定性策略执行机制。

Method: PCAS将智能体系统状态建模为依赖图，捕获工具调用、结果和消息间的因果关系；使用Datalog衍生语言表达策略；通过引用监视器拦截所有动作并在执行前阻止违规。

Result: 在客户服务任务中，PCAS将策略合规性从48%提升到93%，在三个案例研究中实现零策略违规；无需安全特定重构即可构建策略合规系统。

Conclusion: PCAS为LLM智能体系统提供确定性策略执行，通过编译方法确保系统构建时即符合策略，显著提高合规性并消除策略违规。

Abstract: LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.
  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.
  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware](https://arxiv.org/abs/2602.16024)
*R. Kanda,H. L. Blevec,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: 提出在PYNQ-Z1等小型FPGA SoC上实现任意定点位宽实时少样本学习的方法，通过FINN框架优化实现约2倍吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 传统Tensil设计环境仅支持16或32位定点位宽，限制了硬件实现的灵活性，需要支持任意位宽以实现更高效的实时少样本学习

Method: 采用FINN框架支持任意位宽实现，进行两项关键优化：1) 优化Transpose节点解决数据格式不匹配；2) 添加最终reduce mean操作转换为全局平均池化的处理

Result: 在保持与传统实现相同精度的前提下，成功降低位宽，在CIFAR-10数据集评估中实现约2倍的吞吐量提升

Conclusion: 提出的方法能够在小型FPGA SoC上实现任意定点位宽的实时少样本学习，通过位宽优化显著提升吞吐性能

Abstract: In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.

</details>


### [16] [Energy-Efficient p-Bit-Based Fully-Connected Quantum-Inspired Simulated Annealer with Dual BRAM Architecture](https://arxiv.org/abs/2602.16143)
*Naoya Onizawa,Taiga Kubuta,Duckgyu Shin,Takahiro Hanyu*

Main category: cs.AR

TL;DR: 本文提出了一种基于FPGA的能效型随机模拟量子退火架构，通过自旋串行和副本并行更新调度结合双BRAM延迟线设计，解决了传统p-bit退火器在完全连接图上的扩展性问题，实现了大规模组合优化的能效硬件加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于概率比特的模拟退火加速器存在扩展性差和对完全连接图支持有限的问题，主要受限于扇出增长和内存开销。需要开发一种能效硬件架构来解决大规模组合优化问题。

Method: 采用自旋串行和副本并行更新调度策略，结合双BRAM延迟线架构，支持完全连接的伊辛模型。利用随机模拟量子退火技术，仅使用最终副本状态实现快速收敛，显著降低内存需求。

Result: 在Xilinx ZC706 FPGA上实现，成功解决了800节点的MAX-CUT基准问题。相比先前的FPGA基p-bit退火架构，能耗降低达50%，逻辑资源减少超过90%。

Conclusion: 该架构证明了量子启发的p-bit退火硬件在大规模组合优化中的实用性，特别是在严格的能耗和资源约束条件下具有显著优势。

Abstract: Probabilistic bits (p-bits) offer an energy-efficient hardware abstraction for stochastic optimization; however, existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead. This paper presents an energy-efficient FPGA architecture for stochastic simulated quantum annealing (SSQA) that addresses these challenges. The proposed design combines a spin-serial and replica-parallel update schedule with a dual-BRAM delay-line architecture, enabling scalable support for fully connected Ising models while eliminating fan-out growth in logic resources. By exploiting SSQA, the architecture achieves fast convergence using only final replica states, significantly reducing memory requirements compared to conventional p-bit-based annealers. Implemented on a Xilinx ZC706 FPGA, the proposed system solves an 800-node MAX-CUT benchmark and achieves up to 50% reduction in energy consumption and over 90\% reduction in logic resources compared with prior FPGA-based p-bit annealing architectures. These results demonstrate the practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [17] [Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs](https://arxiv.org/abs/2602.15995)
*Xiang Fu,Shiman Meng,Weiping Zhang,Luanzheng Guo,Kento Sato,Dong H. Ahn,Ignacio Laguna,Gregory L. Lee,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出两种新技术（分布式时钟DC和分布式纪元DE）来解决OpenMP程序记录和回放的可扩展性问题，相比传统方法效率提升2-5倍，并能与MPI回放工具集成。


<details>
  <summary>Details</summary>
Motivation: OpenMP作为最流行的共享内存编程框架，其非确定性执行特性使得调试和测试变得困难。虽然记录和确定性回放程序执行是解决这一挑战的关键，但可扩展地回放OpenMP程序仍然是一个未解决的问题。

Method: 提出两种新颖技术：分布式时钟（DC）和分布式纪元（DE）记录方案，通过消除OpenMP记录和回放中过多的线程同步。使用ReOMP工具实现DC和DE记录，并与现有MPI回放工具ReMPI集成。

Result: 在代表性HPC应用上的评估显示，该方法比传统在每个共享内存访问上同步的方法效率高2-5倍。能够轻松与MPI级回放工具结合，回放复杂的MPI+OpenMP应用，仅产生与MPI规模无关的小运行时开销。

Conclusion: 提出的DC和DE记录方案有效解决了OpenMP程序可扩展记录和回放的问题，显著提升了效率，并能与MPI回放工具无缝集成，为调试复杂的混合并行应用提供了实用解决方案。

Abstract: After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.

</details>


### [18] [Scrutinizing Variables for Checkpoint Using Automatic Differentiation](https://arxiv.org/abs/2602.16010)
*Xin Huang,Weiping Zhang,Shiman Meng,Wubiao Xu,Xiang Fu,Luanzheng Guo,Kento Sato*

Main category: cs.DC

TL;DR: 提出基于自动微分(AD)的检查点优化方法，通过分析变量中每个元素对计算输出的影响，识别并排除不关键元素，减少检查点存储开销


<details>
  <summary>Details</summary>
Motivation: 传统检查点/重启(C/R)机制保存程序运行状态时消耗大量系统资源，但HPC应用中并非所有数据都参与计算，排除不使用的数据可提高存储和计算效率

Method: 利用自动微分(AD)工具分析变量中每个元素对应用程序输出的影响，识别关键/非关键元素，将非关键元素从检查点中排除

Result: 在NAS Parallel Benchmark(NPB)套件的8个基准测试中验证，成功可视化变量中关键/非关键元素区域，存储节省最高达20%

Conclusion: 提出的基于AD的检查点优化方法能有效识别并排除不影响输出的数据元素，显著减少检查点存储开销，且关键/非关键元素的分布模式与算法物理逻辑相符

Abstract: Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.

</details>


### [19] [LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum](https://arxiv.org/abs/2602.16100)
*Zijie Su,Muhammed Tawfiqul Islam,Mohammad Goudarzi,Adel N. Toosi*

Main category: cs.DC

TL;DR: 提出一种动态流水线重配置方法，可在服务器环境下在线调整LLM推理的流水线配置，以最小化服务中断时间和性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，在有限GPU资源下高效服务LLM推理成为关键挑战。服务器计算范式被应用于LLM服务以最大化资源利用率，但LLM推理工作负载高度多样化，现代GPU集群本质上是异构的，需要在线动态调整部署配置以适应服务器环境的弹性和动态特性。同时，由于LLM推理的有状态性和模型参数的巨大规模，实现在线重配置特别具有挑战性。

Method: 提出动态流水线重配置方法，支持在线调整流水线配置，同时最小化服务中断时间和性能下降。系统能够根据变化的工作负载选择最优的流水线配置。

Result: 在异构GPU平台（包括NVIDIA A100和L40s）上的实验结果表明，迁移机制导致的服务中断时间小于50毫秒，同时对首词生成时间（TTFT）和每词输出时间（TPOT）引入的开销低于10%。

Conclusion: 该方法有效解决了服务器环境下LLM推理的动态配置问题，在保持低服务中断和低性能开销的同时，实现了对变化工作负载的自适应优化。

Abstract: With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).

</details>


### [20] [Near-optimal population protocols on bounded-degree trees](https://arxiv.org/abs/2602.16222)
*Joel Rybicki,Jakob Solnerzik,Robin Vacus*

Main category: cs.DC

TL;DR: 本文研究了稀疏交互图中群体协议的空间-时间权衡，发现在有界度树上，领导选举和精确多数问题不存在显著的空间-时间权衡，提出了常数空间协议实现接近最优的稳定时间。


<details>
  <summary>Details</summary>
Motivation: 在完全交互图中，领导选举和精确多数问题的最优空间-时间权衡已有研究，但现有下界技术无法扩展到高度稀疏图。本文旨在探索有界度树等稀疏图族是否也存在类似的空间-时间复杂度权衡。

Method: 提出了两种新颖协议：1）适用于一般交互图的快速自稳定2跳着色协议，使用随机漂移论证分析稳定时间；2）自稳定树定向算法，在任何树上以最优时间构建有根树。利用这些协议，可以使用为有向树设计的简单常数状态协议快速解决领导选举和精确多数问题。

Result: 在有界度树上，领导选举和精确多数问题不存在显著的空间-时间权衡，实现了常数空间协议，其最坏情况期望稳定时间接近最优，相比现有技术实现了线性加速。例如，在有向树上使用"定向"湮灭动力学可以在O(n² log n)步内解决精确多数问题。

Conclusion: 与完全图不同，有界度树上的群体协议在领导选举和精确多数问题上不表现出显著的空间-时间权衡。提出的新协议为稀疏交互图提供了高效的常数空间解决方案，相关技术具有独立的研究价值。

Abstract: We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.
  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.
  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \log n)$ steps on directed trees.

</details>


### [21] [DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting](https://arxiv.org/abs/2602.16233)
*Prabhjot Singh,Adel N. Toosi,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 该论文提出了一种面向电路切割的分布式执行流水线，通过系统化测量在机器学习训练任务中的端到端开销，发现重构阶段是主要瓶颈，但准确性和鲁棒性在匹配训练预算下得以保持。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注电路切割在子电路数量和采样复杂度方面的开销，但缺乏从系统角度衡量其对迭代式、估计器驱动的训练流程的端到端影响。需要量化切割在真实机器学习工作负载中的实际开销和可扩展性限制。

Method: 提出了一种切割感知的估计器执行流水线，将电路切割视为分阶段分布式工作负载，将每个估计器查询分解为分区、子实验生成、并行执行和经典重构四个阶段。使用两个二分类任务（Iris和MNIST）的运行时日志和学习结果进行测量。

Result: 测量显示：1）切割引入显著的端到端开销，且随切割数量增加而增长；2）重构阶段占每个查询时间的主导部分，限制了并行化带来的加速潜力；3）尽管存在系统开销，在匹配训练预算下测试准确性和鲁棒性得以保持，某些切割配置下甚至观察到改进。

Conclusion: 电路切割在机器学习工作负载中的实际可扩展性关键在于减少和重叠重构阶段，以及考虑屏障主导的关键路径的调度策略。虽然系统开销显著，但准确性和鲁棒性可以得到保持。

Abstract: Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.

</details>


### [22] [How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability](https://arxiv.org/abs/2602.16362)
*MHD Saria Allahham,Hossam S. Hassanein*

Main category: cs.DC

TL;DR: 本文提出了一个用于极端边缘计算（XEC）的计算可靠性分析框架，量化设备或设备集合满足流媒体服务处理速率要求的概率，解决了消费者设备计算可用性波动带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 消费者设备在极端边缘计算环境中表现出波动的计算可用性，这源于竞争应用程序和不可预测的使用模式。这种波动性带来了一个基本挑战：如何量化单个设备或设备集合能够维持流媒体服务所需处理速率的概率？

Method: 提出了一个计算可靠性分析框架，定义了在特定服务质量阈值下瞬时容量满足需求的概率。在两种信息机制下推导闭式可靠性表达式：最小信息机制（仅需声明的操作边界）和历史数据机制（通过过去观测的最大似然估计来精化估计）。框架扩展到多设备部署，为串行、并行和分区工作负载配置提供可靠性表达式。推导了最优工作负载分配规则和设备选择的分析边界。

Result: 使用YOLO11m模型进行实时目标检测作为代表性分布式推理流媒体工作负载进行验证。在模拟的XED环境中的实验表明，分析预测、蒙特卡洛采样和实际测量在不同容量和需求配置下表现出高度一致性。

Conclusion: 该框架为编排器提供了可处理的工具来评估部署可行性和配置分布式流媒体系统，解决了极端边缘计算环境中计算可靠性的量化问题，为分布式推理工作负载的有效部署提供了理论基础和实践指导。

Abstract: Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.

</details>


### [23] [FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving](https://arxiv.org/abs/2602.16603)
*Chia-chi Hsieh,Zan Zong,Xinyang Chen,Jianjiang Li,Jidong Zhai,Lijie Wen*

Main category: cs.DC

TL;DR: FlowPrefill是一个针对大语言模型服务的TTFT-吞吐量优化系统，通过解耦抢占粒度与调度频率，解决了预填充阶段头部阻塞问题，在满足异构SLO的同时提升吞吐量达5.6倍。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型需求增长，服务系统需要处理大量并发请求和多样化服务级别目标。这加剧了计算密集型预填充阶段的头部阻塞问题，长请求独占资源会延迟高优先级请求，导致广泛的时间到首个令牌SLO违规。虽然分块预填充实现了可中断性，但在响应性和吞吐量之间存在固有权衡。

Method: FlowPrefill采用两个关键创新：1) 操作符级抢占：利用操作符边界实现细粒度执行中断，避免固定小分块带来的效率损失；2) 事件驱动调度：仅在请求到达或完成事件时触发调度决策，支持高效抢占响应性同时最小化控制平面开销。

Result: 在真实生产环境trace上的评估显示，FlowPrefill相比最先进系统将最大有效吞吐量提升了最高5.6倍，同时满足异构SLO要求。

Conclusion: FlowPrefill通过解耦抢占粒度与调度频率，解决了预填充调度中的固有冲突，实现了响应性和吞吐量的平衡，为大语言模型服务系统提供了有效的解决方案。

Abstract: The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.
  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.

</details>
