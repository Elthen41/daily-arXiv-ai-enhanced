<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring](https://arxiv.org/abs/2511.01898)
*Hanie Vatani,Reza Ebrahimi Atani*

Main category: cs.CR

TL;DR: FedSelect-ME是一个分层多边缘联邦学习框架，通过分布式边缘服务器、基于分数的客户端选择以及安全聚合技术，解决了传统联邦学习的可扩展性、通信成本和隐私风险问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在集中式架构带来的可扩展性限制、高通信成本和隐私风险，特别是在医疗保健等隐私敏感应用中这些问题尤为突出。

Method: 采用分层多边缘架构，多个边缘服务器分布工作负载；实施基于分数的客户端选择，综合考虑效用、能效和数据敏感性；结合同态加密和差分隐私的安全聚合保护模型更新。

Result: 在eICU医疗数据集上的评估显示，相比FedAvg、FedProx和FedSelect，FedSelect-ME实现了更高的预测准确性、更好的区域间公平性和更低的通信开销。

Conclusion: 该框架有效解决了传统联邦学习的瓶颈，为大规模隐私敏感的医疗保健应用提供了安全、可扩展且高效的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data but suffers from limited scalability, high communication costs, and
privacy risks due to its centralized architecture. This paper proposes
FedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability,
security, and energy efficiency. Multiple edge servers distribute workloads and
perform score-based client selection, prioritizing participants based on
utility, energy efficiency, and data sensitivity. Secure Aggregation with
Homomorphic Encryption and Differential Privacy protects model updates from
exposure and manipulation. Evaluated on the eICU healthcare dataset,
FedSelect-ME achieves higher prediction accuracy, improved fairness across
regions, and reduced communication overhead compared to FedAvg, FedProx, and
FedSelect. The results demonstrate that the proposed framework effectively
addresses the bottlenecks of conventional FL, offering a secure, scalable, and
efficient solution for large-scale, privacy-sensitive healthcare applications.

</details>


### [2] [Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing](https://arxiv.org/abs/2511.01952)
*Jinhua Yin,Peiru Yang,Chen Yang,Huili Wang,Zhiyang Hu,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CR

TL;DR: 提出了首个针对大型视觉语言模型的黑盒成员推理攻击框架KCMP，通过先验知识校准的记忆探测机制来评估模型对训练数据的记忆程度，在纯黑盒设置下有效识别训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型成员推理攻击方法通常需要白盒或灰盒假设，而主流模型在推理时只暴露生成输出，限制了这些方法的适用性。需要开发在纯黑盒设置下有效的攻击方法。

Method: 基于先验知识校准的记忆探测机制，通过评估模型对疑似图像数据中嵌入的私有语义信息的记忆程度来进行成员推理，这些信息不太可能仅从一般世界知识中推断出来。

Result: 在四个大型视觉语言模型和三个数据集上的实验表明，该方法在纯黑盒设置下能有效识别训练数据，性能甚至可与灰盒和白盒方法相媲美，且对潜在对抗操作具有鲁棒性。

Conclusion: KCMP是首个针对大型视觉语言模型的黑盒成员推理攻击框架，通过语义记忆探测在保护隐私的同时有效识别训练数据成员，为模型隐私安全评估提供了新工具。

Abstract: Large vision-language models (LVLMs) derive their capabilities from extensive
training on vast corpora of visual and textual data. Empowered by large-scale
parameters, these models often exhibit strong memorization of their training
data, rendering them susceptible to membership inference attacks (MIAs).
Existing MIA methods for LVLMs typically operate under white- or gray-box
assumptions, by extracting likelihood-based features for the suspected data
samples based on the target LVLMs. However, mainstream LVLMs generally only
expose generated outputs while concealing internal computational features
during inference, limiting the applicability of these methods. In this work, we
propose the first black-box MIA framework for LVLMs, based on a prior
knowledge-calibrated memory probing mechanism. The core idea is to assess the
model memorization of the private semantic information embedded within the
suspected image data, which is unlikely to be inferred from general world
knowledge alone. We conducted extensive experiments across four LVLMs and three
datasets. Empirical results demonstrate that our method effectively identifies
training data of LVLMs in a purely black-box setting and even achieves
performance comparable to gray-box and white-box methods. Further analysis
reveals the robustness of our method against potential adversarial
manipulations, and the effectiveness of the methodology designs. Our code and
data are available at https://github.com/spmede/KCMP.

</details>


### [3] [Watermarking Discrete Diffusion Language Models](https://arxiv.org/abs/2511.02083)
*Avi Bagchi,Akhil Bhimaraju,Moulik Choraria,Daniel Alabi,Lav R. Varshney*

Main category: cs.CR

TL;DR: 本文提出了首个针对离散扩散语言模型的水印方法，通过在每个扩散步骤应用保持分布的Gumbel-max技巧，并使用序列索引作为随机种子来实现可靠检测。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术主要研究自回归大语言模型和图像扩散模型，但缺乏对日益流行的离散扩散语言模型的水印解决方案，这些模型因高推理吞吐量而受到关注。

Method: 在每个扩散步骤应用分布保持的Gumbel-max技巧，并使用序列索引作为随机种子来生成水印，确保可靠检测。

Result: 实验证明该方法在最新的扩散语言模型上可可靠检测，理论分析表明它是无失真的，且误检概率随令牌序列长度呈指数衰减。

Conclusion: 提出的水印方案成功填补了离散扩散模型水印技术的空白，为AI生成内容的追踪提供了有效解决方案。

Abstract: Watermarking has emerged as a promising technique to track AI-generated
content and differentiate it from authentic human creations. While prior work
extensively studies watermarking for autoregressive large language models
(LLMs) and image diffusion models, none address discrete diffusion language
models, which are becoming popular due to their high inference throughput. In
this paper, we introduce the first watermarking method for discrete diffusion
models by applying the distribution-preserving Gumbel-max trick at every
diffusion step and seeding the randomness with the sequence index to enable
reliable detection. We experimentally demonstrate that our scheme is reliably
detectable on state-of-the-art diffusion language models and analytically prove
that it is distortion-free with an exponentially decaying probability of false
detection in the token sequence length.

</details>


### [4] [FLAME: Flexible and Lightweight Biometric Authentication Scheme in Malicious Environments](https://arxiv.org/abs/2511.02176)
*Fuyi Wang,Fangyuan Sun,Mingyuan Fan,Jianying Zhou,Jin Ma,Chao Chen,Jiangang Shu,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 提出了一种名为FLAME的灵活轻量级生物认证方案，专为恶意环境设计，通过混合轻量级秘密共享原语和双方计算，在保持安全性的同时显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护生物认证方案大多基于半诚实对手模型，但在现实场景中对手可能恶意偏离协议，需要更强的安全保证。

Method: 采用轻量级秘密共享原语与双方计算混合的方法，设计了一系列包含完整性检查的支持协议，并实现了跨度量兼容的服务器端认证。

Result: 与最先进方案相比，通信量减少97.61-110.13倍，LAN环境下速度提升2.72-2.82倍，WAN环境下提升6.58-8.51倍。

Conclusion: FLAME方案在恶意环境下实现了高效、安全的隐私保护生物认证，具有显著的实际应用价值。

Abstract: Privacy-preserving biometric authentication (PPBA) enables client
authentication without revealing sensitive biometric data, addressing privacy
and security concerns. Many studies have proposed efficient cryptographic
solutions to this problem based on secure multi-party computation, typically
assuming a semi-honest adversary model, where all parties follow the protocol
but may try to learn additional information. However, this assumption often
falls short in real-world scenarios, where adversaries may behave maliciously
and actively deviate from the protocol.
  In this paper, we propose, implement, and evaluate $\sysname$, a
\underline{F}lexible and \underline{L}ightweight biometric
\underline{A}uthentication scheme designed for a \underline{M}alicious
\underline{E}nvironment. By hybridizing lightweight secret-sharing-family
primitives within two-party computation, $\sysname$ carefully designs a line of
supporting protocols that incorporate integrity checks with rationally extra
overhead. Additionally, $\sysname$ enables server-side authentication with
various similarity metrics through a cross-metric-compatible design, enhancing
flexibility and robustness without requiring any changes to the server-side
process. A rigorous theoretical analysis validates the correctness, security,
and efficiency of $\sysname$. Extensive experiments highlight $\sysname$'s
superior efficiency, with a communication reduction by {$97.61\times \sim
110.13\times$} and a speedup of {$ 2.72\times \sim 2.82\times$ (resp. $
6.58\times \sim 8.51\times$)} in a LAN (resp. WAN) environment, when compared
to the state-of-the-art work.

</details>


### [5] [Enhancing NTRUEncrypt Security Using Markov Chain Monte Carlo Methods: Theory and Practice](https://arxiv.org/abs/2511.02365)
*Gautier-Edouard Filardo,Thibaut Heckmann*

Main category: cs.CR

TL;DR: 提出了一种使用马尔可夫链蒙特卡洛方法增强NTRUEncrypt量子抵抗性的新框架，建立了采样效率的形式界限，并提供了到格问题的安全归约。


<details>
  <summary>Details</summary>
Motivation: 在量子计算时代，增强NTRUEncrypt的量子抵抗性，同时保持其实际部署的可行性。

Method: 使用马尔可夫链蒙特卡洛方法探索私钥漏洞，建立可证明的混合时间界限，并将MCMC参数与格硬度假设联系起来。

Result: 数值实验验证了该方法，展示了改进的安全保证和计算效率。

Conclusion: 这些发现推进了NTRUEncrypt在后量子时代的理论理解和实际采用。

Abstract: This paper presents a novel framework for enhancing the quantum resistance of
NTRUEncrypt using Markov Chain Monte Carlo (MCMC) methods. We establish formal
bounds on sampling efficiency and provide security reductions to lattice
problems, bridging theoretical guarantees with practical implementations. Key
contributions include: a new methodology for exploring private key
vulnerabilities while maintaining quantum resistance, provable mixing time
bounds for high-dimensional lattices, and concrete metrics linking MCMC
parameters to lattice hardness assumptions. Numerical experiments validate our
approach, demonstrating improved security guarantees and computational
efficiency. These findings advance the theoretical understanding and practical
adoption of NTRU- Encrypt in the post-quantum era.

</details>


### [6] [On The Dangers of Poisoned LLMs In Security Automation](https://arxiv.org/abs/2511.02600)
*Patrick Karlsen,Even Eilertsen*

Main category: cs.CR

TL;DR: 本文研究LLM投毒风险，展示有限数据集微调可能引入显著偏见，导致基于LLM的警报调查系统被绕过，并提出缓解措施。


<details>
  <summary>Details</summary>
Motivation: 研究LLM训练过程中恶意或偏见数据引入的风险，特别是在安全应用中可能导致的严重后果。

Method: 使用微调的Llama3.1 8B和Qwen3 4B模型，演示针对性投毒攻击如何使模型持续忽略特定用户的真实阳性警报。

Result: 发现看似改进的LLM在有限数据集微调后可能引入显著偏见，导致基于LLM的警报调查系统被完全绕过。

Conclusion: 提出缓解措施和最佳实践，以提高安全应用中LLM的可信度、鲁棒性并降低风险。

Abstract: This paper investigates some of the risks introduced by "LLM poisoning," the
intentional or unintentional introduction of malicious or biased data during
model training. We demonstrate how a seemingly improved LLM, fine-tuned on a
limited dataset, can introduce significant bias, to the extent that a simple
LLM-based alert investigator is completely bypassed when the prompt utilizes
the introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we
demonstrate how a targeted poisoning attack can bias the model to consistently
dismiss true positive alerts originating from a specific user. Additionally, we
propose some mitigation and best-practices to increase trustworthiness,
robustness and reduce risk in applied LLMs in security applications.

</details>


### [7] [Verifying LLM Inference to Prevent Model Weight Exfiltration](https://arxiv.org/abs/2511.02620)
*Roy Rinberg,Adam Karvonen,Alex Hoover,Daniel Reuter,Keri Warr*

Main category: cs.CR

TL;DR: 本文提出了一种防御模型权重泄露的验证框架，通过检测推理过程中的异常行为来防止攻击者通过隐写术从推理服务器中窃取模型权重。


<details>
  <summary>Details</summary>
Motivation: 随着大型AI模型成为重要资产，从推理服务器窃取模型权重的风险增加。攻击者可能通过隐藏权重在普通模型输出中进行隐写式泄露。

Method: 将模型泄露形式化为安全游戏，提出可证明缓解隐写泄露的验证框架，并指定信任假设。通过表征大语言模型推理中的有效非确定性来源，引入两个实用估计器。

Result: 在多个3B到30B参数的开放权重模型上评估检测框架。在MOE-Qwen-30B上，检测器将可泄露信息减少到<0.5%，误报率为0.01%，相当于使攻击者速度降低200倍以上。

Conclusion: 这项工作为防御模型权重泄露奠定了基础，证明可以通过最小的额外成本实现强大的保护。

Abstract: As large AI models become increasingly valuable assets, the risk of model
weight exfiltration from inference servers grows accordingly. An attacker
controlling an inference server may exfiltrate model weights by hiding them
within ordinary model outputs, a strategy known as steganography. This work
investigates how to verify model responses to defend against such attacks and,
more broadly, to detect anomalous or buggy behavior during inference. We
formalize model exfiltration as a security game, propose a verification
framework that can provably mitigate steganographic exfiltration, and specify
the trust assumptions associated with our scheme. To enable verification, we
characterize valid sources of non-determinism in large language model inference
and introduce two practical estimators for them. We evaluate our detection
framework on several open-weight models ranging from 3B to 30B parameters. On
MOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with
false-positive rate of 0.01%, corresponding to a >200x slowdown for
adversaries. Overall, this work further establishes a foundation for defending
against model weight exfiltration and demonstrates that strong protection can
be achieved with minimal additional cost to inference providers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本文对已部署和正在使用的工作负载调度器解决方案进行了分析，提出了基于架构和设计的层次化分类法，特别关注影响吞吐量和可扩展性的关键设计因素以及架构改进。


<details>
  <summary>Details</summary>
Motivation: 现有的分类法存在不足，需要关注影响工作负载调度器吞吐量和可扩展性的关键设计因素，以及架构的渐进式改进。

Method: 分析已部署和正在使用的工作负载调度器解决方案，基于架构和设计构建层次化分类法，特别关注Google Borg等先进系统。

Result: 提出了一个基于架构和设计的工作负载调度器分类法，识别了影响吞吐量和可扩展性的关键设计因素。

Conclusion: 该综述为理解工作负载调度器的架构演进和性能优化提供了系统性的分类框架，特别强调了Google Borg等先进系统的设计经验。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [9] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: 本概念设计报告介绍了德国达姆施塔特FAIR研究设施的计算基础设施计划，涵盖从2028年'首次科学+'阶段到模块化启动版本的计算需求、政策、模型和架构。


<details>
  <summary>Details</summary>
Motivation: 为FAIR研究设施创建统一的计算基础设施，满足多样化研究线路的需求，并应对未来的数据挑战。

Method: 提出联邦化和中央协调的基础设施架构，包括开放数据、软件和服务政策，确保足够的可扩展性和灵活性。

Result: 制定了涵盖计算需求、存储基础设施政策、计算模型和架构的综合计划。

Conclusion: 目标是建立一个能够服务多样化研究线路、具有足够可扩展性和灵活性的联邦化中央协调基础设施。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [10] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 云计算已成为AI/ML创新的主导力量，但其成本模型不适合科学计算需求，可能导致科学工作负载在不适合的环境中运行。


<details>
  <summary>Details</summary>
Motivation: 分析云计算成本模型从科学计算到AI/ML主导的演变，探讨如何继续支持科学发现。

Method: 通过历史回顾和现状分析，讨论云计算成本模型的过去、现在和未来可能性。

Result: 发现AI/ML需求主导的云计算创新使成本模型不再适合科学计算需求，资源竞争可能阻碍科学用户访问。

Conclusion: 需要重新思考云计算成本模型，以确保科学发现工作负载得到持续支持。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [11] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: EdgeReasoning是一个针对边缘GPU部署推理型大语言模型的综合研究，系统量化了不同架构和模型尺寸下的延迟-准确率权衡，评估了减少推理token长度的技术，并分析了测试时扩展方法，为边缘部署提供最优配置指导。


<details>
  <summary>Details</summary>
Motivation: 边缘智能在自主系统中需求增长，但将大语言模型部署到边缘GPU面临严格延迟约束和有限计算资源的挑战，开发者需要在推理与非推理架构、模型尺寸、token预算和测试时扩展策略之间平衡，但目前缺乏这些变量最优组合的指导。

Method: 系统量化不同LLM架构和模型尺寸的延迟-准确率权衡；评估基于提示和模型调优的减少推理token长度技术；分析不同并行度的测试时扩展方法；绘制可实现的准确率-延迟配置的帕累托边界。

Result: 通过全面分析，EdgeReasoning绘制了可实现的准确率-延迟配置的帕累托边界，为推理型LLM的边缘部署提供了系统化指导。

Conclusion: 该研究为在边缘GPU上部署推理型大语言模型提供了系统化的配置指导，帮助开发者在严格延迟约束下优化模型性能。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [12] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 本文开发了多核处理器结构分析和效率指标评估模型，包括可靠性、容错性、生存性和灵活性评估，使用逻辑概率方法分析多核处理器的性能状态和效率提升趋势。


<details>
  <summary>Details</summary>
Motivation: 研究多核处理器（特别是具有可变结构和多功能核心的处理器）的结构分析和效率指标评估，以提升其可靠性和性能。

Method: 使用逻辑概率方法开发了多种模型：评估多功能核心可靠性和容错性的模型、基于多功能核心的多核处理器最短路径、灵活性和性能条件的逻辑概率模型、以及考虑所有可能性能状态的多核处理器可靠性、容错性和寿命估计模型。

Result: 提供了双核和四核处理器的结构分析结果，并展示了多核处理器效率指标提升的趋势。

Conclusion: 通过逻辑概率方法成功开发了多核处理器结构分析和效率评估的综合模型，为多核处理器性能优化提供了理论支持。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [13] [Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions](https://arxiv.org/abs/2511.01888)
*Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Roadrunner是一个边车shim，通过近乎零拷贝和无序列化的数据传输机制，显著提升基于WebAssembly的无服务器函数间通信性能，减少延迟和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算中函数间数据传输通常涉及序列化/反序列化操作，导致多次数据拷贝和用户/内核空间切换，增加了延迟和资源消耗。

Method: 通过映射函数内存并沿专用虚拟数据管道移动数据，绕过序列化和反序列化过程，实现近乎零拷贝的数据传输。

Result: 实验结果显示，Roadrunner将函数间通信延迟降低了44%至89%，减少了97%的数据传输序列化开销，吞吐量比现有最佳方案提高了69倍。

Conclusion: Roadrunner通过创新的数据传输机制，为基于WebAssembly的无服务器函数实现了接近原生性能的通信效率。

Abstract: Serverless computing provides infrastructure management and elastic
auto-scaling, therefore reducing operational overhead. By design serverless
functions are stateless, which means they typically leverage external remote
services to store and exchange data. Transferring data over a network typically
involves serialization and deserialization. These operations usually require
multiple data copies and transitions between user and kernel space, resulting
in overhead from context switching and memory allocation, contributing
significantly to increased latency and resource consumption. To address these
issues, we present Roadrunner, a sidecar shim that enables near-zero copy and
serialization-free data transfer between WebAssembly-based serverless
functions. Roadrunner reduces the multiple copies between user space and kernel
space by mapping the function memory and moving the data along a dedicated
virtual data hose, bypassing the costly processes of serialization and
deserialization. This approach reduces data movement overhead and context
switching, achieving near-native latency performance for WebAssembly-based
serverless functions. Our experimental results demonstrate that Roadrunner
significantly improves the inter-function communication latency from 44% up to
89%, reducing the serialization overhead in 97% of data transfer, and
increasing throughput by 69 times compared to state-of-the-art
WebAssembly-based serverless functions.

</details>


### [14] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mLRI通过记忆化技术优化ADMM-FFT算法，用缓存替代重复的FFT计算，引入变量卸载技术节省CPU内存，实现跨GPU扩展，在2Kx2Kx2K规模问题上平均提升52.8%性能。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT算法在层析成像重建中精度高但计算时间长、内存消耗大，需要优化其性能瓶颈。

Method: 利用记忆化技术缓存重复的FFT操作，引入变量卸载技术节省内存，实现跨GPU扩展。

Result: 在2Kx2Kx2K规模问题上，mLRI相比原始ADMM-FFT平均提升52.8%性能（最高65.4%），并成功扩展到最大输入问题规模。

Conclusion: mLRI通过记忆化和变量卸载技术有效解决了ADMM-FFT的计算效率和内存瓶颈问题，实现了大规模层析成像重建的性能提升。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [15] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 该论文分析了五个主要PoS区块链的地理空间去中心化问题，发现少数地区主导共识投票权，并提出GPoS解决方案，将地理空间多样性与权益投票相结合，实验显示地理空间去中心化平均提升45%，且对BFT协议性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 地理空间去中心化对区块链的监管弹性、鲁棒性和公平性至关重要，但现有PoS区块链存在少数地理区域主导共识投票权的问题。

Method: 提出地理空间感知权益证明（GPoS），将地理空间多样性与基于权益的投票权相结合，并在HotStuff和CometBFT等BFT协议中进行实验评估。

Result: 实验评估显示，通过特征向量中心性的基尼系数衡量，地理空间去中心化平均提升了45%，同时对共识性能的影响极小。

Conclusion: GPoS能够有效改善区块链的地理空间去中心化，且在实践中仅带来最小的性能开销。

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [16] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 论文提出超越传统BSP模型，通过细粒度编程模式消除分布式GPU执行中的三大性能税（批量同步、内核间数据局部性、内核启动开销），在关键内核上实现10-20%的端到端延迟加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，多GPU分布式执行中的传统BSP模型存在显著性能低效问题，需要新的执行范式来解决瓶颈。

Method: 利用Iris for Triton等库访问内核内通信原语，设计新颖的细粒度编程模式，创建直接的瓦片级生产者-消费者管道，用细粒度数据流同步替代全局屏障。

Result: 在关键内核（从All-Gather + 通用矩阵乘法到复杂Flash Decode算法）上，相比基于BSP的方法实现了10-20%的端到端延迟加速。

Conclusion: 建立了一个更可编程和高效的分布式LLM工作负载范式，系统性地消除了三大性能税。

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [17] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 提出了一种基于算子级别的自动扩缩容框架，通过细粒度资源管理优化大型生成模型的推理服务，相比传统模型级方法显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案采用静态资源配置或模型级自动扩缩容，将模型视为整体进行粗粒度资源管理，导致性能下降或资源利用率低下，无法适应动态推理流量的变化。

Method: 通过详细特征分析和系统分析发现生成模型中算子的异构性，提出算子级自动扩缩容框架，基于单个算子配置文件进行扩缩容、批处理和放置优化。

Result: 在生产规模跟踪评估中，该方法在保持SLO的前提下减少40% GPU和35%能耗，或在固定资源下实现1.6倍吞吐量提升和5%能耗降低。

Conclusion: 算子而非模型是扩展大型生成工作负载的更有效单元，细粒度资源管理能显著提升服务效率。

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [18] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出了两种新颖的调度算法来优化格点量子色动力学中的相关函数计算，通过重新排序张量收缩操作来提高时间局部性，减少峰值内存使用和数据传输，从而加速计算。


<details>
  <summary>Details</summary>
Motivation: 格点量子色动力学中的相关函数计算涉及大量二进制批量张量收缩，每个张量可能占用数百MB内存。在GPU加速器上执行这些收缩时，面临调度挑战，需要优化张量重用并减少数据流量。

Method: 提出了两种快速调度算法，利用应用特定特征（如二进制收缩和收缩树内的局部性）来重新排序收缩操作，通过输入/中间张量重用增加时间局部性，优化最小化峰值内存的目标。

Result: 调度器在峰值内存方面实现了高达2.1倍的改进，这反映在驱逐次数减少高达4.2倍，数据流量减少高达1.8倍，导致相关函数计算时间加快高达1.9倍。

Conclusion: 将调度器集成到LQCD分析软件套件Redstar中，改善了求解时间，证明了所提方法在优化格点量子色动力学计算性能方面的有效性。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [19] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: 提出了联邦注意力（FedAttn）框架，将联邦学习范式集成到自注意力机制中，实现隐私保护、通信效率和计算效率的分布式LLM推理。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在边缘协作场景中面临的隐私漏洞、通信开销和计算瓶颈等挑战。

Method: 通过本地自注意力计算和周期性的KV矩阵交换聚合，在多个Transformer块间协作生成LLM响应，同时保护私有提示不被暴露。

Result: 理论分析了错误传播动态和响应质量与通信/计算效率之间的权衡关系，实验验证了理论分析并揭示了通过稀疏注意力和自适应KV聚合的优化机会。

Conclusion: FedAttn框架为现实世界边缘部署提供了可扩展性和效率的潜力，建立了将联邦优化技术系统性地移植到协作LLM推理的原则基础。

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [20] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文比较了四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）在科学计算应用中的性能表现，发现不同框架在N体模拟和结构化网格模拟中表现出显著性能差异，OCCA在小规模验证问题上表现最佳但缺乏优化的归约算法，OpenMP在结构化网格模拟中性能较差。


<details>
  <summary>Details</summary>
Motivation: 随着异构计算架构的兴起，科学计算在百亿亿次计算时代需要跨不同硬件平台高效执行的供应商无关性能可移植性框架。Kokkos等库对于高性能计算应用在不同硬件平台上以最小代码更改实现高效执行变得至关重要。

Method: 使用分布式内存方法和硬件加速，通过四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）对两个代表性科学计算应用（N体模拟和结构化网格模拟）进行实验，在Polaris超级计算机的单个节点上使用四个NVIDIA A100 GPU进行测试。

Result: 实验结果显示框架间存在显著性能差异：OCCA在小规模验证问题上执行时间更快（可能由于JIT编译），但其缺乏优化的归约算法可能限制大规模模拟的可扩展性；OpenMP在结构化网格模拟中表现不佳，很可能由于节点间数据同步和通信效率低下。

Conclusion: 这些发现强调了需要进一步优化以最大化每个框架的能力。未来工作将专注于增强归约算法、数据通信、内存管理，以及进行可扩展性研究和全面的统计分析来评估和比较框架性能。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [21] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: EPaxos*是一个更简单且正确的Egalitarian Paxos变体，通过简化的故障恢复算法解决了原协议的复杂性和错误问题，并将协议推广到最优进程数范围。


<details>
  <summary>Details</summary>
Motivation: 经典的状态机复制协议（如Paxos）依赖领导者进程来排序命令，这导致领导者成为单点故障并增加延迟。Egalitarian Paxos引入了无领导者的方法，但协议复杂、规范模糊且存在严重错误。

Method: 提出了EPaxos*，关键技术创新是更简单的故障恢复算法，并进行了严格正确性证明。协议将Egalitarian Paxos推广到满足n ≥ max{2e+f-1, 2f+1}的整个故障阈值f和e范围。

Result: EPaxos*解决了Egalitarian Paxos的复杂性和错误问题，提供了更简单正确的协议实现，同时保持了原协议的优势特性。

Conclusion: EPaxos*是Egalitarian Paxos的改进版本，通过简化的故障恢复算法和更优的进程数配置，提供了更可靠和实用的无领导者状态机复制协议。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 该论文提出了人类-AI共体现智能的新范式，将人类用户、智能AI和可穿戴硬件集成到一个系统中，用于现实世界的实验和智能制造。通过APEX系统在柔性电子制造中的演示，实现了超越通用多模态大语言模型的上下文感知推理、实时纠错和专业知识转移。


<details>
  <summary>Details</summary>
Motivation: 科学实验和制造依赖复杂多步骤程序，需要持续的人类专业知识进行精确执行和决策。传统模型局限于虚拟领域，而现实世界的实验和制造仍依赖人类监督和专业知识，这种机器智能与物理执行之间的差距限制了科学和制造工作流程的可重复性、可扩展性和可访问性。

Method: 引入人类-AI共体现智能，将人类用户、智能AI和可穿戴硬件集成到一个系统中。人类提供精确执行和控制，智能AI贡献记忆、上下文推理、自适应规划和实时反馈。通过APEX系统，将智能推理与物理执行通过混合现实相结合，观察和解释人类动作，与标准操作程序对齐，提供3D视觉指导并分析每一步。

Result: 在洁净室柔性电子制造中实施的APEX系统实现了超越通用多模态大语言模型的上下文感知推理精度，能够实时纠正错误，并将专业知识转移给初学者。

Conclusion: 这些结果建立了一类新的智能-物理-人类智能，将智能推理从计算扩展到物理领域，将科学研究和制造转变为自主、可追溯、可解释和可扩展的过程。

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [23] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 本文展示了如何使用基础模型通过文本指令自动搜索奖励函数来设计强化学习智能体，在Gran Turismo 7赛车游戏中实现了与冠军级智能体GT Sophy竞争的性能，并生成了新颖行为。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，通过定义奖励函数来传达期望的智能体行为是一个困难的过程，特别是在复杂环境如自动驾驶赛车中。本文旨在解决这一奖励函数设计的挑战。

Method: 结合了基于LLM的奖励生成、基于VLM偏好的评估和人类反馈的系统，通过文本指令自动搜索奖励函数空间。

Result: 系统能够产生与冠军级RL赛车智能体GT Sophy竞争的赛车智能体，并能生成新颖的行为。

Conclusion: 该方法为实际应用中的自动化奖励设计铺平了道路，展示了基础模型在复杂环境中自动设计奖励函数的有效性。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [24] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: 提出了Deep Value Benchmark (DVB)评估框架，用于测试大型语言模型是否学习到人类深层价值观而非仅表面偏好。通过控制深层价值观和浅层特征的混淆，测量模型的深层价值泛化率(DVGR)。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是否学习到人类深层价值观至关重要，因为仅捕捉表面偏好的系统可能产生不协调的行为。需要评估模型是否能稳健地泛化人类意图。

Method: 使用新颖的实验设计，在训练阶段让LLMs接触深层价值观和浅层特征相关的人类偏好数据，测试阶段打破这些相关性，测量模型基于深层价值观而非浅层特征进行泛化的概率(DVGR)。

Result: 在9个不同模型中，平均DVGR仅为0.30，所有模型的深层价值泛化率都低于随机水平。较大模型的DVGR略低于较小模型。

Conclusion: 当前LLMs在泛化深层人类价值观方面表现不佳，DVB为AI对齐提供了可解释的核心特征衡量标准。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [25] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 该研究提出了InsurAgent，一个基于大语言模型的智能体，用于模拟洪水保险购买决策行为，通过检索增强生成和推理模块解决LLM在概率估计方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 美国高风险人群的洪水保险参与率极低，需要理解保险决策的行为机制，而大语言模型为模拟人类决策提供了有前景的工具。

Method: 构建基准数据集评估LLM能力，提出InsurAgent智能体，包含感知、检索、推理、行动和记忆五个模块，其中检索模块使用RAG技术基于调查数据，推理模块利用LLM常识进行推断。

Result: LLM对因素有定性理解但概率估计不足，InsurAgent通过RAG准确估计边际和双变量概率，推理模块捕捉传统模型难以处理的情境信息，记忆模块支持时间决策演化模拟。

Conclusion: InsurAgent为行为建模和政策分析提供了有价值的工具，能够准确模拟保险购买决策行为。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [26] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC是一种自适应奖励预测方法，能够根据未来思考token数量预测预期未来奖励，通过训练轻量级适配器在推理模型上实现更长的推理和更大的模型带来的改进预测。


<details>
  <summary>Details</summary>
Motivation: 为了解决推理过程中计算资源浪费和效率低下的问题，需要一种能够动态控制推理长度、优化模型选择的方法，以提高计算效率和推理准确性。

Method: 在推理模型上训练轻量级适配器，实现基于上下文的自适应奖励预测，根据未来思考token数量预测预期奖励，支持动态推理长度控制和计算时间预估。

Result: Re-FORC实现了：1）提前终止无望推理链，减少26%计算同时保持准确性；2）优化模型和思考长度选择，在相同计算下提高4%准确性，在相同准确性下减少55%计算；3）自适应测试时扩展，在高计算和低计算场景下分别提高11%和7%准确性。

Conclusion: Re-FORC提供了一种有效的动态推理控制方法，能够在保证准确性的同时显著优化计算效率，支持基于token成本阈值的灵活推理长度控制。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [27] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 提出了ATHENA框架，通过结合符号效用建模和语义适应来个性化建模人类决策过程，在旅行方式和疫苗选择任务中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 个体决策模型与群体最优预测存在差距，因为个体决策过程受到数值属性（成本、时间）和语言因素（个人偏好和约束）的共同影响。

Method: ATHENA框架包含两个阶段：1）通过LLM增强的符号发现获得群体级符号效用函数；2）基于最优效用进行个体级语义适应，创建个性化语义模板来建模个性化选择。

Result: 在真实世界的旅行模式和疫苗选择任务中，ATHENA始终优于基于效用、机器学习和其他基于LLM的模型，F1分数比最强前沿模型至少提升6.5%。消融研究证实两个阶段都至关重要且互补。

Conclusion: 通过有机整合符号效用建模和语义适应，ATHENA为建模以人为本的决策提供了新方案。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [28] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 提出了一个名为\method的框架，用于提升大语言模型在复杂表格数据推理方面的性能，该框架包含查询分解器、表格清理器和基于程序思维的推理器，并在新数据集CalTab151上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂表格数据推理中因复杂查询、噪声数据和有限数值能力而表现不佳的问题。

Method: 框架包含三个组件：(1)查询分解器分解复杂问题，(2)表格清理器清理和过滤噪声表格，(3)基于程序思维的推理器生成可执行代码从清理后的表格中推导最终答案。

Result: 在TAT-QA、TableBench和\method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到了最先进的性能水平。

Conclusion: 该框架有效提升了大语言模型在复杂表格数值推理方面的性能，并能与主流大语言模型无缝集成，为复杂表格数值推理提供了稳健的解决方案。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [29] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出了一个分析多模态大语言模型模态跟随行为的新框架，将模态跟随分解为相对推理不确定性和固有模态偏好两个因素，揭示了模态跟随概率随相对不确定性单调下降的普遍规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅用粗粒度的数据集级统计来衡量多模态大语言模型的模态跟随行为，忽略了模型在单模态推理中的置信度影响。需要更精细的框架来理解模型如何解决模态间冲突信息。

Method: 构建可控数据集系统变化视觉和文本输入的推理难度，使用熵作为细粒度不确定性度量，通过层间预测分析揭示内部机制。

Result: 发现模态跟随概率随相对不确定性单调下降的普遍规律，在平衡点处模型对两种模态的跟随概率相当，揭示了模型在模糊区域跨层振荡的内部机制。

Conclusion: 相对不确定性和固有模态偏好是控制模态跟随的两个基本原则，为理解多模态大语言模型如何解决冲突信息提供了定量框架和机制性见解。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [30] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文分析了多智能体推理中的懒惰行为问题，提出了因果影响测量方法和可验证奖励机制来改善协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的多智能体推理系统中存在懒惰行为问题，即一个智能体主导而另一个贡献很少，导致协作失效，限制了多智能体框架在复杂推理任务中的潜力。

Method: 首先进行理论分析解释懒惰行为的成因；然后引入稳定高效的因果影响测量方法；最后提出可验证奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重启推理过程。

Result: 大量实验表明，该框架有效缓解了懒惰智能体行为，释放了多智能体框架在复杂推理任务中的全部潜力。

Conclusion: 通过理论分析和提出的方法，成功解决了多智能体推理中的协作问题，为复杂推理任务提供了更有效的多智能体解决方案。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [31] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 提出ProQ-BERT框架，使用Transformer模型基于多模态电子健康记录预测慢性肾病进展，在91,816患者队列中表现优于现有方法，ROC-AUC达0.995。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病影响全球近10%人口，准确预测疾病进展对及时干预和资源优化至关重要。

Method: 基于Transformer的框架，整合人口统计、临床和实验室数据，采用基于量化的标记化处理连续实验室值，使用注意力机制提高可解释性，通过掩码语言建模预训练并针对二元分类任务微调。

Result: 在91,816患者队列评估中，模型持续优于CEHR-BERT，短期预测ROC-AUC达0.995，PR-AUC达0.989。

Conclusion: 结果证明了Transformer架构和时间设计选择在临床预后建模中的有效性，为个性化慢性肾病护理提供了有前景的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [32] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 本研究提出了一种基于模糊软集理论的专家系统，用于通过可测量的临床和生理参数评估乳腺癌风险。该系统整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算来估计乳腺癌风险。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性死亡的主要原因之一，早期诊断对有效治疗和提高生存率至关重要。但由于疾病复杂性和患者风险因素的变异性，及时检测仍然是一个挑战。

Method: 开发基于模糊软集理论的专家系统，使用BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算进行风险评估。数据集来自UCI机器学习库。

Result: 该系统能够通过常规血液分析获得参数，提供非侵入性和可访问的初步评估方法，支持医疗专业人员识别高风险患者。

Conclusion: 该专家系统旨在帮助医疗专业人员识别高风险患者，并确定是否需要进一步的诊断程序（如活检），为乳腺癌早期检测提供了一种可行的辅助工具。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [33] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 本文提出了一个基于二元分类视角的生成模型精确率-召回率(PR)曲线估计新框架，进行了统计分析和风险上界推导，扩展了现有PR指标，并通过实验验证了不同设置下曲线的行为差异。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在图像和文本领域的成功，其评估方法受到广泛关注。虽然现有方法多依赖标量指标，但PR曲线为生成模型评估提供了更丰富的分析维度，然而其估计面临诸多挑战。

Method: 基于二元分类视角构建PR曲线估计框架，进行统计理论分析，推导最小最大风险上界，并扩展现有仅关注曲线极值的PR指标。

Result: 提出了一个完整的PR曲线估计框架，获得了PR估计风险的最小最大上界，成功扩展了文献中的多个重要PR指标，并通过实验验证了不同设置下曲线行为的差异。

Conclusion: 该框架为生成模型的PR曲线估计提供了理论基础和实用工具，能够更全面地评估生成模型性能，超越了传统标量指标的局限性。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [34] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种可验证多选择重构（VMR）方法，将开放域任务转换为可验证的多选择格式，从而在缺乏标准答案的情况下应用强化学习与可验证奖励（RLVR）范式来提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR在数学和编程等有标准答案的领域取得了显著进展，但在缺乏标准答案的开放域任务（如创意写作和指令遵循）中，现有研究通常将其视为非推理场景，忽略了推理能力的潜在价值。本文旨在探索如何将RLVR范式迁移到开放域任务中。

Method: 提出了可验证多选择重构（VMR）训练策略，将开放域数据重新构建为可验证的多选择格式，从而在缺乏明确标准答案的情况下实现有效训练。

Result: 在多个基准测试上的实验结果表明，该方法能有效提升大语言模型在开放域任务上的性能。在八个开放域基准测试中，基于VMR的训练相比基线平均提升了5.99分。

Conclusion: VMR方法成功地将RLVR范式扩展到开放域任务，证明了即使在缺乏标准答案的情况下，通过适当的数据重构也能有效提升模型的推理能力，从而改善开放域任务的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [35] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: 本文提出了KLPEG框架，通过构建知识图谱来系统建模游戏元素、任务依赖和因果关系，利用LLM解析更新日志并基于知识图谱进行多跳推理，生成针对更新的测试用例，显著提升了游戏测试的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏的快速迭代和频繁更新对测试效率和特异性提出了重大挑战。现有的基于大语言模型的自动化游戏测试方法缺乏结构化知识积累机制，难以针对增量游戏更新进行精确高效的测试。

Method: 提出KLPEG框架：1）构建和维护知识图谱来系统建模游戏元素、任务依赖和因果关系；2）利用LLM解析自然语言更新日志；3）通过知识图谱上的多跳推理识别影响范围；4）生成针对更新的测试用例。

Result: 在Overcooked和Minecraft两个代表性游戏环境中的实验表明，KLPEG能够更准确地定位受更新影响的功能，并以更少的步骤完成测试，显著提高了游戏测试的效果和效率。

Conclusion: KLPEG框架通过知识图谱和LLM的结合，解决了游戏增量更新测试中的知识积累和重用问题，为自动化游戏测试提供了一种有效的解决方案。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [36] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA基准测试评估LLMs在多领域真实定量推理任务中的表现，涵盖金融、物理、健康、统计等领域。五个先进模型在500个任务中准确率仅为45-63%，主要错误为舍入错误(35%)和计算错误(33%)。


<details>
  <summary>Details</summary>
Motivation: 现有数学数据集无法充分评估LLMs在真实世界多领域定量推理中的表现，特别是逐步推理、数值精度和领域泛化能力。

Method: 使用Omni计算引擎验证输出，构建包含500个自然语言任务的基准测试，涵盖金融、物理、健康、统计等多个领域，评估模型的逐步推理和数值精度。

Result: 五个先进模型(包括ChatGPT-5、Gemini 2.5 Flash等)在ORCA基准上的准确率为45-63%，主要错误类型为舍入错误(35%)和计算错误(33%)。模型在数学和工程领域表现较好，但在物理和自然科学领域较弱。

Conclusion: LLMs在真实世界定量推理任务中仍有显著局限性，不同模型在错误类型上具有部分互补性而非完全冗余，需要进一步改进数值精度和领域泛化能力。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [37] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 本文提出了首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违规并使用归纳逻辑编程自动在线修复规范，确保屏蔽器优雅演化并保持最优奖励和完美逻辑合规性。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法假设固定的逻辑规范和手工制作的抽象，在环境假设被违反时无法适应。需要开发能够动态调整的安全强化学习屏蔽框架。

Method: 基于GR(1)规范开发自适应屏蔽框架，运行时检测环境假设违规，使用归纳逻辑编程自动在线修复GR(1)规范，确保屏蔽器演化并保持活性目标。

Result: 在Minepump和Atari Seaquest案例研究中显示：(i)静态符号控制器在优化辅助奖励时通常严重次优；(ii)配备自适应屏蔽的RL智能体相比静态屏蔽保持接近最优奖励和完美逻辑合规性。

Conclusion: 提出的自适应屏蔽框架能够有效处理环境假设违规，确保安全强化学习智能体在动态环境中保持高性能和合规性，为实际应用提供了更可靠的解决方案。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [38] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个用于分析组合空间推理能力的大规模基准数据集和生成框架，包含超过500万个数据点，通过程序化构建确保正确性，并独立验证。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够独立变化组合性多个关键方面的基准数据集，以严格评估大型语言模型在组合空间推理方面的能力。

Method: 程序化生成数据集，允许独立变化生产力（推理深度）、可替换性（实体和语言变异性）、过度泛化（输入顺序、干扰项）和系统性（新语言元素）等组合性方面，并使用符号求解器验证数据集正确性。

Result: 在多个大型语言模型上的基准测试显示，LLMs在空间推理任务中难以进行生产性和系统性泛化，但对语言变异性更具鲁棒性。

Conclusion: DecompSR提供了一个可证明正确且严格的基准数据集，能够独立变化组合性的多个关键方面，为LLMs的组合推理能力提供了稳健和细粒度的探测工具。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [39] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 该研究提出了一个协作迷宫求解基准，评估了32个领先的AI模型在单独、同质和异质配对中的协作能力，发现存在'协作差距'：单独表现良好的模型在协作时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，我们将越来越多地依赖由独立开发的异构智能体组成的系统，这些系统的成功关键在于有效协作，但目前缺乏大规模评估智能体间协作的实证研究。

Method: 开发了一个协作迷宫求解基准框架，该框架能够：(i)隔离协作能力，(ii)调节问题复杂度，(iii)实现可扩展的自动评分，(iv)保持生态合理性。使用该框架评估了32个开源和闭源模型在不同配对配置下的表现。

Result: 发现了明显的'协作差距'：单独表现好的模型在协作时性能显著下降。小型蒸馏模型在某些配对中几乎完全失败。研究发现从较强智能体开始往往能改善结果，提出了'接力推理'方法，该方法能显著缩小协作差距。

Conclusion: 研究主张：(1)协作感知的评估方法，(2)增强协作能力的训练策略，(3)可靠激发智能体潜在技能的交互设计，这些指导原则适用于AI-AI和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [40] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文提出了span query概念，将推理服务器接口泛化，支持聊天、RAG、推理时缩放和智能体工作负载，通过交换性约束优化KV缓存和注意力局部性，在vLLM上实现10-20倍TTFT降低。


<details>
  <summary>Details</summary>
Motivation: 客户端正在超越聊天完成功能，包含各种创新的推理时缩放和深度推理技术，而推理服务器仍主要针对聊天完成优化。现有工作虽然展示了KV缓存命中率的大幅提升可能，但仅针对单一用例RAG。

Method: 引入span query作为推理服务器接口的泛化，将其定义为带有交换性约束的推理调用表达式树。描述了span query的语法和语义，展示了如何自动优化以提高KV缓存局部性，并对vLLM进行小规模修改（仅492行代码）以支持高性能span query执行。

Result: span query在两个不同的非聊天用例中实现了10-20倍的TTFT降低。注意力优化的span query在2b参数模型上的准确率远超使用8b模型的标准推理服务器。

Conclusion: span query为推理服务器提供了统一的接口，能够有效支持多种新兴的非聊天工作负载，通过交换性约束优化显著提升性能，解决了中间丢失问题。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [41] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的半自动化方法，用于将控制工程领域的自然语言描述和数学定义转化为形式化知识图谱，以增强知识可访问性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 控制工程研究产出的快速增长需要新的方法来结构化和形式化领域知识，解决知识可访问性、协作性和可验证性问题。

Method: 基于Imperative Representation of Knowledge (PyIRK)框架，利用语言模型辅助将自然语言描述和LaTeX数学定义转化为形式化知识图谱，并生成交互式语义层来增强源文档。

Result: 成功演示了如何通过LLM支持的方法生成结合人类可读性和机器可解释性的形式化知识表示，实现了知识图谱的构建。

Conclusion: 该方法为实现控制工程领域易于访问、协作和可验证的知识库愿景做出了贡献，展示了LLM在知识形式化中的潜力。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [42] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级、模型无关的评估层来诊断多模态大语言模型中的模态破坏问题，即高置信度的单模态错误会覆盖其他证据并误导融合结果。该框架将每个模态视为智能体，通过融合机制识别贡献者和破坏者，为多模态推理提供诊断支架。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型快速发展，但其推理过程仍不透明：不清楚哪个模态驱动预测、如何解决冲突或何时某个模态占主导。本文旨在分析多模态融合中的动态特性，特别是模态破坏这种诊断性失败模式。

Method: 提出轻量级、模型无关的评估层，将每个模态视为智能体，生成候选标签和简要自评估用于审计。通过简单的融合机制聚合这些输出，暴露贡献者（支持正确结果的模态）和破坏者（误导的模态）。

Result: 在多模态情感识别基准测试的案例研究中，应用该诊断层揭示了系统性的可靠性特征，提供了关于失败是源于数据集伪影还是模型限制的见解。

Conclusion: 该框架为多模态推理提供了诊断支架，支持对融合动态进行原则性审计，并为可能的干预措施提供信息。

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [43] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 本文提出了一种在复杂AI控制环境中优化攻击策略的方法，通过将攻击能力分解为五个技能组件并分别优化，在数据有限的情况下使用概率模型模拟攻击动态，显著提升了攻击强度。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署变得更加复杂和高风险，准确评估其风险变得至关重要。AI控制是一个评估框架，但需要强大的攻击策略，这在计算受限的复杂环境中具有挑战性。

Method: 将攻击能力分解为五个组成技能：怀疑建模、攻击选择、计划合成、执行和隐蔽性，并分别优化每个组件。为解决数据有限的问题，开发了攻击动态的概率模型，在模拟中优化攻击超参数，然后将结果转移到SHADE-Arena环境中。

Result: 该方法显著提高了攻击强度，将安全分数从基线0.87降低到0.41，使用提出的框架实现了攻击能力的实质性改进。

Conclusion: 通过技能分解和概率建模的方法，可以在数据有限的复杂AI控制环境中有效优化攻击策略，为AI风险评估提供了更强大的工具。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [44] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: 本文提出了一种针对分解式AI GPU的NUMA感知调度策略——Swizzled Head-first Mapping，通过将注意力头与GPU NUMA域对齐来利用片内缓存重用，在AMD MI300X架构上实现了比现有注意力算法高达50%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着分解式AI GPU的兴起，多芯片设计中的非均匀内存访问(NUMA)成为大规模注意力工作负载的关键瓶颈。传统GPU内核调度策略假设均匀内存访问，无法应对不同计算区域内存延迟和带宽的显著差异。

Method: 提出了Swizzled Head-first Mapping空间感知调度策略，识别NUMA效应对多头注意力(MHA)局部性的影响，将注意力头与GPU NUMA域对齐以利用片内缓存重用。

Result: 在AMD MI300X架构上，该方法相比使用传统调度技术的最先进注意力算法实现了高达50%的性能提升，并持续保持80-97%的高L2缓存命中率。

Conclusion: NUMA感知调度对于在下一代分解式GPU上实现完全效率至关重要，为可扩展AI训练和推理提供了前进路径。

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [45] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文在IMAX CGLA加速器上实现了Whisper语音识别的核心计算内核，通过软硬件协同设计，在FPGA原型上评估并预测28nm ASIC性能，展示了比NVIDIA Jetson AGX Orin和RTX 4090更高的能效。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在自动语音识别(ASR)任务中面临严重的能耗挑战，专用集成电路(ASIC)虽然高效但缺乏算法适应性，需要找到能效与可编程性的平衡解决方案。

Method: 在通用粗粒度线性阵列(CGLA)加速器IMAX上实现Whisper核心计算内核，采用软硬件协同设计方法，通过FPGA原型进行评估，并预测28nm ASIC的性能表现。

Result: 投影的ASIC在Q8_0模型上比NVIDIA Jetson AGX Orin能效高1.90倍，比NVIDIA RTX 4090高9.83倍，展示了显著的能效优势。

Conclusion: CGLA架构是功率受限边缘设备上实现可持续ASR的有前景平台，为生成式AI应用提供了能效与可编程性的良好平衡。

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [46] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus是一个三阶段框架，通过将LLM推理聚焦于代码生成过程中的关键决策点来增强Verilog生成，显著提高了功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自一致性或仿真反馈来选择最佳候选代码，但未能充分利用LLM推理聚焦于设计中最具信息量的部分。

Method: 三阶段框架：预排名阶段生成多个代码候选并应用密度引导过滤；排名阶段使用自动生成的测试台进行仿真并基于自一致性聚类；后排名细化阶段对排名靠前的候选进行不一致性挖掘并使用推理增强的LLM提示进行细化。

Result: 在VerilogEval-Human基准测试中，VFocus显著提高了多个推理LLM的pass@1正确性。

Conclusion: VFocus通过聚焦LLM推理于关键决策点，有效增强了复杂硬件设计任务中的Verilog生成能力。

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [47] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: 在SoC FPGA上实现基于DPU的多线程面部表情识别系统，使用DenseBox进行人脸检测和CNN进行表情识别，相比之前的工作提高了准确性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决之前工作中Haar Cascade检测器在侧脸和变光照条件下准确性不足的问题，以及避免为第二个DNN推理添加新加速器带来的资源浪费。

Method: 在同一个DPU上运行DenseBox人脸检测和CNN表情识别两个DNN推理，开发多线程技术提高吞吐量和DPU利用率。

Result: 系统整体吞吐量达到25 FPS，每功耗吞吐量提高了2.4倍。

Conclusion: 使用DPU作为通用CNN加速器可以有效利用FPGA资源，同时通过多线程技术显著提升系统性能。

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [48] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: 本文提出了基于数字递归算法的posit除法单元，首次在该领域实现了radix-4数字递归技术，通过硬件优化显著降低了能耗和迭代次数。


<details>
  <summary>Details</summary>
Motivation: posit算术作为IEEE 754浮点表示的替代方案，具有更高的精度和动态范围，但其除法操作由于硬件复杂性而面临挑战。

Method: 采用基于数字递归算法的posit除法单元，结合冗余算术、在线商转换和操作数缩放等硬件优化技术来简化除法过程。

Result: 综合评估显示，与现有方法相比，实现了超过80%的能耗降低和较小的面积开销，同时显著减少了迭代次数。

Conclusion: 该适配算法有潜力显著提升基于posit的算术单元的效率。

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [49] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文在IMAX3 CGRA加速器上首次实现了stable-diffusion.cpp图像生成框架的核心计算内核，评估了其性能表现，并为未来ASIC实现提供了指导。


<details>
  <summary>Details</summary>
Motivation: 评估通用CGRA加速器IMAX3在执行高要求图像生成工作负载时的能力，为开发下一代AI专用CGLA加速器奠定基础。

Method: 在FPGA原型上实现stable-diffusion.cpp的核心计算内核，建立性能基准，并预测未来ASIC实现的潜力。

Result: 尽管采用通用架构，IMAX3在性能和能效方面表现出色，特别是在预测的ASIC形式中。

Conclusion: 这项工作为未来IMAX架构设计提供了具体指导，并为开发高效能、设备端多模态AI平台做出了贡献。

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>
