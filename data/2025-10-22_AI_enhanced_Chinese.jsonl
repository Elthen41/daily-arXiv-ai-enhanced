{"id": "2510.18525", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.18525", "abs": "https://arxiv.org/abs/2510.18525", "authors": ["Yushu Zhao", "Yubin Qin", "Yang Wang", "Xiaolong Yang", "Huiming Han", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing", "comment": null, "summary": "Large language models achieve impressive performance across diverse tasks but\nexhibit high inference latency due to their large parameter sizes. While\nquantization reduces model size, it often leads to performance degradation\ncompared to the full model. Speculative decoding remains lossless but typically\nincurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed\nspeculative decoding method that uses part of the full-model weight bits to\nform a quantized draft model, thereby eliminating additional training or\nstorage overhead. A reconfigurable processing element array enables efficient\nexecution of both the draft and verification passes. Experimental results\nacross 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,\n1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.", "AI": {"tldr": "SPEQ\u662f\u4e00\u79cd\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u4f7f\u7528\u5168\u6a21\u578b\u90e8\u5206\u6743\u91cd\u4f4d\u5f62\u6210\u91cf\u5316\u8349\u7a3f\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5b58\u50a8\u5f00\u9500\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u5904\u7406\u5355\u5143\u9635\u5217\u9ad8\u6548\u6267\u884c\u8349\u7a3f\u548c\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u572815\u4e2aLLM\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u91cf\u5316\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u63a8\u6d4b\u89e3\u7801\u867d\u65e0\u635f\u4f46\u5e26\u6765\u989d\u5916\u5f00\u9500\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u52a0\u901f\u63a8\u7406\u53c8\u4fdd\u6301\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSPEQ\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff1a\u4f7f\u7528\u5168\u6a21\u578b\u90e8\u5206\u6743\u91cd\u4f4d\u5f62\u6210\u91cf\u5316\u8349\u7a3f\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5b58\u50a8\uff1b\u91c7\u7528\u53ef\u91cd\u6784\u5904\u7406\u5355\u5143\u9635\u5217\u9ad8\u6548\u6267\u884c\u8349\u7a3f\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "result": "\u572815\u4e2aLLM\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSPEQ\u76f8\u6bd4FP16\u3001Olive\u548cTender\u5206\u522b\u5b9e\u73b0\u4e862.07\u500d\u30011.53\u500d\u548c1.45\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SPEQ\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.18612", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.18612", "abs": "https://arxiv.org/abs/2510.18612", "authors": ["Muhammad Hassan", "Maria Mushtaq", "Jaan Raik", "Tara Ghasempouri"], "title": "DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining", "comment": null, "summary": "RISC-V processors are becoming ubiquitous in critical applications, but their\nsusceptibility to microarchitectural side-channel attacks is a serious concern.\nDetection of microarchitectural attacks in RISC-V is an emerging research topic\nthat is relatively underexplored, compared to x86 and ARM. The first line of\nwork to detect flush+fault-based microarchitectural attacks in RISC-V leverages\nMachine Learning (ML) models, yet it leaves several practical aspects that need\nfurther investigation. To address overlooked issues, we leveraged gem5 and\npropose a new detection method combining statistical preprocessing and\nassociation rule mining having reconfiguration capabilities to generalize the\ndetection method for any microarchitectural attack. The performance comparison\nwith state-of-the-art reveals that the proposed detection method achieves up to\n5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in\nrecall under the cryptographic, computational, and memory-intensive workloads\nalongside its flexibility to detect new variant of flush+fault attack.\nMoreover, as the attack detection relies on association rules, their\nhuman-interpretable nature provides deep insight to understand\nmicroarchitectural behavior during the execution of attack and benign\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7edf\u8ba1\u9884\u5904\u7406\u548c\u5173\u8054\u89c4\u5219\u6316\u6398\u7684\u65b0\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bRISC-V\u5904\u7406\u5668\u4e2d\u7684\u5fae\u67b6\u6784\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u5e76\u80fd\u68c0\u6d4b\u65b0\u578bflush+fault\u653b\u51fb\u53d8\u79cd\u3002", "motivation": "RISC-V\u5904\u7406\u5668\u5728\u5173\u952e\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u5bf9\u5fae\u67b6\u6784\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u6613\u53d7\u6027\u662f\u4e00\u4e2a\u4e25\u91cd\u95ee\u9898\u3002\u76f8\u6bd4x86\u548cARM\uff0cRISC-V\u4e2d\u5fae\u67b6\u6784\u653b\u51fb\u68c0\u6d4b\u7684\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5b58\u5728\u4e00\u4e9b\u5b9e\u9645\u95ee\u9898\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u5229\u7528gem5\u6a21\u62df\u5668\uff0c\u63d0\u51fa\u7ed3\u5408\u7edf\u8ba1\u9884\u5904\u7406\u548c\u5173\u8054\u89c4\u5219\u6316\u6398\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u65b0\u914d\u7f6e\u80fd\u529b\uff0c\u53ef\u6cdb\u5316\u7528\u4e8e\u68c0\u6d4b\u4efb\u4f55\u5fae\u67b6\u6784\u653b\u51fb\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u52a0\u5bc6\u3001\u8ba1\u7b97\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8fbe5.15%\uff0c\u7cbe\u786e\u5ea6\u63d0\u9ad87%\uff0c\u53ec\u56de\u7387\u6539\u55843.91%\uff0c\u5e76\u80fd\u7075\u6d3b\u68c0\u6d4b\u65b0\u578bflush+fault\u653b\u51fb\u53d8\u79cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u68c0\u6d4b\u6027\u80fd\u4f18\u8d8a\uff0c\u800c\u4e14\u7531\u4e8e\u4f9d\u8d56\u5173\u8054\u89c4\u5219\uff0c\u5176\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u4e3a\u7406\u89e3\u653b\u51fb\u548c\u826f\u6027\u5e94\u7528\u6267\u884c\u671f\u95f4\u7684\u5fae\u67b6\u6784\u884c\u4e3a\u63d0\u4f9b\u4e86\u6df1\u5165\u6d1e\u5bdf\u3002"}}
{"id": "2510.18756", "categories": ["cs.CR", "cs.AR", "cs.DC", "cs.NI", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.18756", "abs": "https://arxiv.org/abs/2510.18756", "authors": ["Marcin Chrapek", "Meni Orenbach", "Ahmad Atamli", "Marcin Copik", "Fritz Alder", "Torsten Hoefler"], "title": "sNVMe-oF: Secure and Efficient Disaggregated Storage", "comment": null, "summary": "Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the\nstandard solution in modern data centers, achieving superior performance,\nresource utilization, and power efficiency. Simultaneously, confidential\ncomputing (CC) is becoming the de facto security paradigm, enforcing stronger\nisolation and protection for sensitive workloads. However, securing\nstate-of-the-art storage with traditional CC methods struggles to scale and\ncompromises performance or security. To address these issues, we introduce\nsNVMe-oF, a storage management system extending the NVMe-oF protocol and\nadhering to the CC threat model by providing confidentiality, integrity, and\nfreshness guarantees. sNVMe-oF offers an appropriate control path and novel\nconcepts such as counter-leasing. sNVMe-oF also optimizes data path performance\nby leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree\n(HMT), and avoiding redundant IPSec protections. We achieve this without\nmodifying the NVMe-oF protocol. To prevent excessive resource usage while\ndelivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.\nWe prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can\nachieve as little as 2% performance degradation for synthetic patterns and AI\ntraining.", "AI": {"tldr": "sNVMe-oF\u662f\u4e00\u4e2a\u57fa\u4e8eNVMe-oF\u534f\u8bae\u7684\u5b58\u50a8\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6269\u5c55\u534f\u8bae\u5e76\u63d0\u4f9b\u673a\u5bc6\u6027\u3001\u5b8c\u6574\u6027\u548c\u65b0\u9c9c\u6027\u4fdd\u8bc1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u673a\u5bc6\u8ba1\u7b97\u65b9\u6cd5\u5728\u4fdd\u62a4\u73b0\u4ee3\u5b58\u50a8\u7cfb\u7edf\u65f6\u9762\u4e34\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5206\u89e3\u5f0f\u5b58\u50a8\u548c\u673a\u5bc6\u8ba1\u7b97\u6210\u4e3a\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u7684\u6807\u51c6\uff0c\u4f20\u7edfCC\u65b9\u6cd5\u5728\u4fdd\u62a4\u5148\u8fdb\u5b58\u50a8\u7cfb\u7edf\u65f6\u9762\u4e34\u6269\u5c55\u6027\u5dee\u3001\u6027\u80fd\u6216\u5b89\u5168\u6027\u53d7\u635f\u7684\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6269\u5c55NVMe-oF\u534f\u8bae\u4f46\u4e0d\u4fee\u6539\u534f\u8bae\u672c\u8eab\uff0c\u5f15\u5165\u63a7\u5236\u8def\u5f84\u548c\u8ba1\u6570\u5668\u79df\u8d41\u7b49\u65b0\u6982\u5ff5\uff0c\u5229\u7528NVMe\u5143\u6570\u636e\u4f18\u5316\u6570\u636e\u8def\u5f84\u6027\u80fd\uff0c\u91c7\u7528\u5206\u89e3\u5f0fHazel Merkle Tree\uff0c\u907f\u514d\u5197\u4f59IPSec\u4fdd\u62a4\uff0c\u5e76\u5229\u7528\u652f\u6301CC\u7684\u667a\u80fd\u7f51\u5361\u52a0\u901f\u5668\u3002", "result": "\u5728NVIDIA BlueField-3\u4e0a\u539f\u578b\u5b9e\u73b0\uff0c\u5bf9\u4e8e\u5408\u6210\u6a21\u5f0f\u548cAI\u8bad\u7ec3\uff0c\u6027\u80fd\u4e0b\u964d\u4ec5\u4e3a2%\uff0c\u80fd\u591f\u5b9e\u73b0\u7ebf\u901f\u6027\u80fd\u3002", "conclusion": "sNVMe-oF\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5bc6\u8ba1\u7b97\u73af\u5883\u4e0b\u5b58\u50a8\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5728\u4e0d\u4fee\u6539NVMe-oF\u534f\u8bae\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u5b89\u5168\u5b58\u50a8\u7ba1\u7406\u3002"}}
{"id": "2510.17852", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17852", "abs": "https://arxiv.org/abs/2510.17852", "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"], "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis", "comment": null, "summary": "With the growing role of artificial intelligence in climate and weather\nresearch, efficient model training and inference are in high demand. Current\nmodels like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware\nindependence, especially for Chinese domestic hardware and frameworks. To\naddress this issue, we present a framework for migrating large-scale\natmospheric and oceanic models from PyTorch to MindSpore and optimizing for\nChinese chips, and evaluating their performance against GPUs. The framework\nfocuses on software-hardware adaptation, memory optimization, and parallelism.\nFurthermore, the model's performance is evaluated across multiple metrics,\nincluding training speed, inference speed, model accuracy, and energy\nefficiency, with comparisons against GPU-based implementations. Experimental\nresults demonstrate that the migration and optimization process preserves the\nmodels' original accuracy while significantly reducing system dependencies and\nimproving operational efficiency by leveraging Chinese chips as a viable\nalternative for scientific computing. This work provides valuable insights and\npractical guidance for leveraging Chinese domestic chips and frameworks in\natmospheric and oceanic AI model development, offering a pathway toward greater\ntechnological independence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5927\u89c4\u6a21\u5927\u6c14\u548c\u6d77\u6d0bAI\u6a21\u578b\u4ecePyTorch\u8fc1\u79fb\u5230MindSpore\u5e76\u9488\u5bf9\u4e2d\u56fd\u82af\u7247\u4f18\u5316\u7684\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u5bf9GPU\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u786c\u4ef6\u72ec\u7acb\u6027\u3002", "motivation": "\u5f53\u524dAI\u6c14\u5019\u548c\u5929\u6c14\u7814\u7a76\u6a21\u578b\uff08\u5982FourCastNet\u548cAI-GOMS\uff09\u4e25\u91cd\u4f9d\u8d56GPU\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u72ec\u7acb\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e2d\u56fd\u56fd\u4ea7\u786c\u4ef6\u548c\u6846\u67b6\u7684\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u8f6f\u4ef6-\u786c\u4ef6\u9002\u914d\u3001\u5185\u5b58\u4f18\u5316\u548c\u5e76\u884c\u5316\u7684\u8fc1\u79fb\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u4ecePyTorch\u8fc1\u79fb\u5230MindSpore\uff0c\u5e76\u9488\u5bf9\u4e2d\u56fd\u82af\u7247\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8fc1\u79fb\u548c\u4f18\u5316\u8fc7\u7a0b\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u539f\u59cb\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u7cfb\u7edf\u4f9d\u8d56\uff0c\u901a\u8fc7\u5229\u7528\u4e2d\u56fd\u82af\u7247\u63d0\u9ad8\u4e86\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u5927\u6c14\u548c\u6d77\u6d0bAI\u6a21\u578b\u5f00\u53d1\u4e2d\u5229\u7528\u4e2d\u56fd\u56fd\u4ea7\u82af\u7247\u548c\u6846\u67b6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5927\u7684\u6280\u672f\u72ec\u7acb\u6027\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2510.17902", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17902", "abs": "https://arxiv.org/abs/2510.17902", "authors": ["Al Kari"], "title": "Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures", "comment": null, "summary": "The proliferation of Large Language Model (LLM) architectures presents a\nfundamental challenge: valuable, task-specific behaviors learned through\nfine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped\nwithin their source model's architecture, herein referred to architectural\nlock-in. Existing transfer methods attempt to bridge this gap by aligning the\nstatic weight spaces of models, a brittle and indirect approach that relies on\ntenuous correlations between parameter geometries. This paper introduces a\nfundamentally different and more direct paradigm: the Cartridge Activation\nSpace Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors\nby learning a direct, nonlinear mapping between the activation manifolds, the\ngeometric structures formed by the model's internal neuron activations, of two\ndistinct LLM architectures. CAST treats a pre-trained LoRA as a frozen\n\"behavioral kernel.\" It learns a set of lightweight, bidirectional projection\nheads that translate the target model's activation stream into the source\nmodel's latent space, apply the frozen kernel, and project the result back.\nThis process, trained on a general text corpus without any task-specific data,\neffectively decouples the learned skill from the source architecture. We\ndemonstrate that CAST enables true \"zero-shot\" translation of any standard LoRA\nadapter. Our experiments, including transfers between heterogeneous model\nfamilies like Llama-2 and Mistral, show that CAST-translated adapters achieve\n85-95\\% of the performance of a LoRA fully retrained on the target model,\nquantitatively outperforming current weight-space transfer techniques and\nestablishing a new state-of-the-art in model interoperability.", "AI": {"tldr": "CAST\u6846\u67b6\u901a\u8fc7\u6fc0\u6d3b\u7a7a\u95f4\u6620\u5c04\u5b9e\u73b0LoRA\u9002\u914d\u5668\u5728\u4e0d\u540cLLM\u67b6\u6784\u95f4\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u67b6\u6784\u9501\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u540e\u7684\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\u88ab\u9501\u5b9a\u5728\u6e90\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6743\u91cd\u7a7a\u95f4\u8fc1\u79fb\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u5b66\u4e60\u8f7b\u91cf\u7ea7\u53cc\u5411\u6295\u5f71\u5934\uff0c\u5728\u6e90\u6a21\u578b\u548c\u76ee\u6807\u6a21\u578b\u7684\u6fc0\u6d3b\u6d41\u5f62\u4e4b\u95f4\u5efa\u7acb\u975e\u7ebf\u6027\u6620\u5c04\uff0c\u5c06\u9884\u8bad\u7ec3LoRA\u4f5c\u4e3a\u51bb\u7ed3\u7684\u884c\u4e3a\u6838\u5e94\u7528\u3002", "result": "\u5728Llama-2\u548cMistral\u7b49\u5f02\u6784\u6a21\u578b\u5bb6\u65cf\u95f4\u7684\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\uff0cCAST\u8f6c\u6362\u7684\u9002\u914d\u5668\u6027\u80fd\u8fbe\u5230\u76ee\u6807\u6a21\u578b\u4e0a\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3LoRA\u768485-95%\u3002", "conclusion": "CAST\u5efa\u7acb\u4e86\u6a21\u578b\u4e92\u64cd\u4f5c\u6027\u7684\u65b0\u6807\u51c6\uff0c\u80fd\u591f\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u6837\u672cLoRA\u9002\u914d\u5668\u8fc1\u79fb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6743\u91cd\u7a7a\u95f4\u8fc1\u79fb\u6280\u672f\u3002"}}
{"id": "2510.17940", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17940", "abs": "https://arxiv.org/abs/2510.17940", "authors": ["Zhiming Lin"], "title": "Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding", "comment": "15 pages,6 figs", "summary": "Multi turn intent understanding is central to task oriented chatbots, yet\nreal deployments face tight token budgets and noisy contexts, and most\nretrieval pipelines emphasize relevance while overlooking set level diversity\nand confounds such as more context or exemplar order. We ask whether retrieval\ndiversity, rather than longer prompts, systematically improves LLM intent\nunderstanding under fixed budgets. We present a diversity aware retrieval\nframework that selects in context exemplars to balance intent coverage and\nlinguistic variety, and integrates this selection with standard LLM decoders;\nthe evaluation enforces budget matched prompts and randomized positions, and\nincludes sensitivity analyses over exemplar count, diversity strength, and\nbackbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in\nJoint Goal Accuracy under equal token budgets, surpassing strong LLM/DST\nbaselines, with consistent improvements across K from 4 to 7 and moderate\nlatency. Overall, the study isolates and validates the impact of content\ndiversity in retrieval and offers a simple, deployable selection principle for\nbuilding accurate, budget constrained multi turn intent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6837\u6027\u611f\u77e5\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u9009\u62e9\u591a\u6837\u5316\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\u6765\u63d0\u5347LLM\u610f\u56fe\u7406\u89e3\u6027\u80fd\uff0c\u5728MultiWOZ 2.4\u548cSGD\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u8054\u5408\u76ee\u6807\u51c6\u786e\u7387\u3002", "motivation": "\u9762\u5411\u4efb\u52a1\u7684\u804a\u5929\u673a\u5668\u4eba\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34token\u9884\u7b97\u9650\u5236\u548c\u566a\u58f0\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u8fc7\u4e8e\u5f3a\u8c03\u76f8\u5173\u6027\u800c\u5ffd\u7565\u4e86\u96c6\u5408\u5c42\u9762\u7684\u591a\u6837\u6027\u548c\u6df7\u6dc6\u56e0\u7d20\uff08\u5982\u66f4\u591a\u4e0a\u4e0b\u6587\u6216\u793a\u4f8b\u987a\u5e8f\uff09\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6837\u6027\u611f\u77e5\u68c0\u7d22\u6846\u67b6\uff0c\u9009\u62e9\u4e0a\u4e0b\u6587\u793a\u4f8b\u4ee5\u5e73\u8861\u610f\u56fe\u8986\u76d6\u548c\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5e76\u5c06\u6b64\u9009\u62e9\u4e0e\u6807\u51c6LLM\u89e3\u7801\u5668\u96c6\u6210\uff1b\u8bc4\u4f30\u91c7\u7528\u9884\u7b97\u5339\u914d\u63d0\u793a\u548c\u968f\u673a\u5316\u4f4d\u7f6e\uff0c\u5305\u62ec\u5bf9\u793a\u4f8b\u6570\u91cf\u3001\u591a\u6837\u6027\u5f3a\u5ea6\u548c\u9aa8\u5e72\u6a21\u578b\u5927\u5c0f\u7684\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5728MultiWOZ 2.4\u548cSGD\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540ctoken\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u8054\u5408\u76ee\u6807\u51c6\u786e\u7387\u7684\u663e\u8457\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684LLM/DST\u57fa\u7ebf\uff0c\u5728K=4\u52307\u8303\u56f4\u5185\u4fdd\u6301\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u4e14\u5ef6\u8fdf\u9002\u4e2d\u3002", "conclusion": "\u7814\u7a76\u5206\u79bb\u5e76\u9a8c\u8bc1\u4e86\u68c0\u7d22\u4e2d\u5185\u5bb9\u591a\u6837\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u6784\u5efa\u51c6\u786e\u3001\u9884\u7b97\u53d7\u9650\u7684\u591a\u8f6e\u610f\u56fe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u53ef\u90e8\u7f72\u7684\u9009\u62e9\u539f\u5219\u3002"}}
{"id": "2510.18300", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18300", "abs": "https://arxiv.org/abs/2510.18300", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Banooqa Banday", "Seth Ockerman", "Amal Gueroudji", "Mohammad Zaeed", "Tanzima Z. Islam", "Line Pouchard"], "title": "A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces", "comment": null, "summary": "Large-scale GPU traces play a critical role in identifying performance\nbottlenecks within heterogeneous High-Performance Computing (HPC)\narchitectures. However, the sheer volume and complexity of a single trace of\ndata make performance analysis both computationally expensive and\ntime-consuming. To address this challenge, we present an end-to-end parallel\nperformance analysis framework designed to handle multiple large-scale GPU\ntraces efficiently. Our proposed framework partitions and processes trace data\nconcurrently and employs causal graph methods and parallel coordinating chart\nto expose performance variability and dependencies across execution flows.\nExperimental results demonstrate a 67% improvement in terms of scalability,\nhighlighting the effectiveness of our pipeline for analyzing multiple traces\nindependently.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u548c\u56e0\u679c\u56fe\u65b9\u6cd5\u63d0\u9ad8\u6027\u80fd\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\u5bf9\u4e8e\u8bc6\u522b\u5f02\u6784\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5355\u4e00\u8ddf\u8e2a\u6570\u636e\u7684\u5e9e\u5927\u4f53\u79ef\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6027\u80fd\u5206\u6790\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\uff0c\u5bf9\u8ddf\u8e2a\u6570\u636e\u8fdb\u884c\u5206\u533a\u5e76\u884c\u5904\u7406\uff0c\u5e76\u8fd0\u7528\u56e0\u679c\u56fe\u65b9\u6cd5\u548c\u5e76\u884c\u534f\u8c03\u56fe\u6765\u63ed\u793a\u6267\u884c\u6d41\u4e2d\u7684\u6027\u80fd\u53d8\u5f02\u6027\u548c\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u63d0\u5347\u4e8667%\uff0c\u8bc1\u660e\u4e86\u8be5\u6d41\u6c34\u7ebf\u5728\u72ec\u7acb\u5206\u6790\u591a\u4e2a\u8ddf\u8e2a\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5206\u6790\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.18544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18544", "abs": "https://arxiv.org/abs/2510.18544", "authors": ["Pan Zhou", "Yiming Lei", "Ling Liu", "Xiaoqiong Xu", "Ying Cai", "Daji Ergu", "Hongfang Yu", "Yueyue Dai"], "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices", "comment": null, "summary": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions.", "AI": {"tldr": "SLICE\u662f\u4e00\u79cd\u9488\u5bf9\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u7684\u521b\u65b0\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u6548\u7528\u6700\u5927\u5316\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5\u548c\u751f\u6210\u901f\u7387\u7684\u52a8\u6001\u8fed\u4ee3\u63a7\u5236\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8LLM\u63a8\u7406\u670d\u52a1\u7684SLO\u8fbe\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u8c03\u5ea6\u670d\u52a1\u7cfb\u7edf\u4ecd\u4ee5\u6700\u5927\u5316\u8f93\u51fa\u4ee4\u724c\u541e\u5410\u91cf\u4e3a\u552f\u4e00\u4f18\u5316\u76ee\u6807\uff0c\u65e0\u6cd5\u5145\u5206\u6ee1\u8db3\u8fb9\u7f18\u8bbe\u5907\u5bf9TTFT\u3001TPOT\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u7b49\u5dee\u5f02\u5316SLO\u8981\u6c42\uff0c\u5bfc\u81f4SLO\u8fdd\u89c4\u7387\u6301\u7eed\u5c45\u9ad8\u3002", "method": "\u7ed3\u5408\u6548\u7528\u6700\u5927\u5316\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5\u4e0e\u751f\u6210\u901f\u7387\u7684\u52a8\u6001\u8fed\u4ee3\u63a7\u5236\u673a\u5236\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848Orca\u548cFastServe\uff0cSLICE\u5b9e\u73b0\u4e86\u9ad8\u8fbe35\u500d\u7684SLO\u8fbe\u6210\u7387\u63d0\u5347\u548c3.4\u500d\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u4f18\u52bf\u3002", "conclusion": "SLICE\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2dLLM\u63a8\u7406\u670d\u52a1\u7684\u5dee\u5f02\u5316SLO\u9700\u6c42\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2510.17904", "categories": ["cs.CR", "cs.AI", "cs.CL", "68T50, 68T05, 68M07", "I.2.7; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.17904", "abs": "https://arxiv.org/abs/2510.17904", "authors": ["Amirkia Rafiei Oskooei", "Mehmet S. Aktas"], "title": "BreakFun: Jailbreaking LLMs via Schema Exploitation", "comment": null, "summary": "The proficiency of Large Language Models (LLMs) in processing structured data\nand adhering to syntactic rules is a capability that drives their widespread\nadoption but also makes them paradoxically vulnerable. In this paper, we\ninvestigate this vulnerability through BreakFun, a jailbreak methodology that\nweaponizes an LLM's adherence to structured schemas. BreakFun employs a\nthree-part prompt that combines an innocent framing and a Chain-of-Thought\ndistraction with a core \"Trojan Schema\"--a carefully crafted data structure\nthat compels the model to generate harmful content, exploiting the LLM's strong\ntendency to follow structures and schemas. We demonstrate this vulnerability is\nhighly transferable, achieving an average success rate of 89% across 13\nfoundational and proprietary models on JailbreakBench, and reaching a 100%\nAttack Success Rate (ASR) on several prominent models. A rigorous ablation\nstudy confirms this Trojan Schema is the attack's primary causal factor. To\ncounter this, we introduce the Adversarial Prompt Deconstruction guardrail, a\ndefense that utilizes a secondary LLM to perform a \"Literal\nTranscription\"--extracting all human-readable text to isolate and reveal the\nuser's true harmful intent. Our proof-of-concept guardrail demonstrates high\nefficacy against the attack, validating that targeting the deceptive schema is\na viable mitigation strategy. Our work provides a look into how an LLM's core\nstrengths can be turned into critical weaknesses, offering a fresh perspective\nfor building more robustly aligned models.", "AI": {"tldr": "BreakFun\u662f\u4e00\u79cd\u5229\u7528LLM\u5bf9\u7ed3\u6784\u5316\u6a21\u5f0f\u9075\u5faa\u6027\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684'\u7279\u6d1b\u4f0a\u6a21\u5f0f'\u5f3a\u5236\u6a21\u578b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u572813\u4e2a\u4e3b\u6d41\u6a21\u578b\u4e0a\u5e73\u5747\u6210\u529f\u738789%\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6587\u5b57\u8f6c\u5f55\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76LLM\u5728\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u548c\u9075\u5faa\u53e5\u6cd5\u89c4\u5219\u80fd\u529b\u65b9\u9762\u7684\u56fa\u6709\u8106\u5f31\u6027\uff0c\u8fd9\u79cd\u80fd\u529b\u65e2\u662f\u5176\u5e7f\u6cdb\u5e94\u7528\u7684\u57fa\u7840\uff0c\u4e5f\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u3002", "method": "\u63d0\u51faBreakFun\u8d8a\u72f1\u65b9\u6cd5\uff0c\u4f7f\u7528\u5305\u542b\u65e0\u8f9c\u6846\u67b6\u3001\u601d\u7ef4\u94fe\u5e72\u6270\u548c\u6838\u5fc3'\u7279\u6d1b\u4f0a\u6a21\u5f0f'\u7684\u4e09\u90e8\u5206\u63d0\u793a\uff0c\u5229\u7528LLM\u5bf9\u7ed3\u6784\u548c\u6a21\u5f0f\u7684\u5f3a\u70c8\u9075\u5faa\u503e\u5411\u3002", "result": "\u653b\u51fb\u572813\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u4e13\u6709\u6a21\u578b\u4e0a\u5e73\u5747\u6210\u529f\u738789%\uff0c\u591a\u4e2a\u77e5\u540d\u6a21\u578b\u8fbe\u5230100%\u653b\u51fb\u6210\u529f\u7387\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u7279\u6d1b\u4f0a\u6a21\u5f0f\u662f\u4e3b\u8981\u56e0\u679c\u56e0\u7d20\u3002", "conclusion": "LLM\u7684\u6838\u5fc3\u4f18\u52bf\u53ef\u80fd\u6210\u4e3a\u5173\u952e\u5f31\u70b9\uff0c\u901a\u8fc7\u9488\u5bf9\u6b3a\u9a97\u6027\u6a21\u5f0f\u7684\u9632\u5fa1\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u6b64\u7c7b\u653b\u51fb\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.18586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18586", "abs": "https://arxiv.org/abs/2510.18586", "authors": ["Zhuohang Bian", "Feiyang Wu", "Teng Ma", "Youwei Zhuo"], "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.", "AI": {"tldr": "Tokencake\u662f\u4e00\u4e2a\u9762\u5411KV\u7f13\u5b58\u7684LLM\u591a\u667a\u80fd\u4f53\u670d\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u8c03\u5ea6\u548c\u65f6\u95f4\u8c03\u5ea6\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2dKV\u7f13\u5b58\u7684\u7a7a\u95f4\u4e89\u7528\u548c\u65f6\u95f4\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u9762\u4e34KV\u7f13\u5b58\u6027\u80fd\u6311\u6218\uff1a\u7a7a\u95f4\u4e89\u7528\u5bfc\u81f4\u5173\u952e\u667a\u80fd\u4f53\u7f13\u5b58\u88ab\u9a71\u9010\uff0c\u65f6\u95f4\u5229\u7528\u7387\u4f4e\u4f7f\u5f97\u7b49\u5f85\u5de5\u5177\u8c03\u7528\u7684\u667a\u80fd\u4f53\u7f13\u5b58\u95f2\u7f6e\u5728GPU\u5185\u5b58\u4e2d\u3002", "method": "Tokencake\u91c7\u7528\u667a\u80fd\u4f53\u611f\u77e5\u8bbe\u8ba1\uff0c\u7a7a\u95f4\u8c03\u5ea6\u5668\u4f7f\u7528\u52a8\u6001\u5185\u5b58\u5206\u533a\u4fdd\u62a4\u5173\u952e\u667a\u80fd\u4f53\u514d\u53d7\u4e89\u7528\uff0c\u65f6\u95f4\u8c03\u5ea6\u5668\u91c7\u7528\u4e3b\u52a8\u5378\u8f7d\u548c\u9884\u6d4b\u6027\u4e0a\u4f20\u673a\u5236\u5728\u51fd\u6570\u8c03\u7528\u505c\u6ede\u671f\u95f4\u91cd\u65b0\u5229\u7528GPU\u5185\u5b58\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTokencake\u76f8\u6bd4vLLM\u80fd\u591f\u51cf\u5c11\u7aef\u5230\u7aef\u5ef6\u8fdf\u8d85\u8fc747.06%\uff0c\u63d0\u9ad8\u6709\u6548GPU\u5185\u5b58\u5229\u7528\u7387\u8fbe16.9%\u3002", "conclusion": "Tokencake\u901a\u8fc7\u534f\u540c\u4f18\u5316\u8c03\u5ea6\u548c\u5185\u5b58\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u5e94\u7528\u4e2d\u7684KV\u7f13\u5b58\u6027\u80fd\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u6548\u7387\u3002"}}
{"id": "2510.18040", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.18040", "abs": "https://arxiv.org/abs/2510.18040", "authors": ["Alexander Boldachev"], "title": "Subject-Event Ontology Without Global Time: Foundations and Execution Semantics", "comment": "32 pages", "summary": "A formalization of a subject-event ontology is proposed for modeling complex\ndynamic systems without reliance on global time. Key principles: (1) event as\nan act of fixation - a subject discerns and fixes changes according to models\n(conceptual templates) available to them; (2) causal order via happens-before -\nthe order of events is defined by explicit dependencies, not timestamps; (3)\nmaking the ontology executable via a declarative dataflow mechanism, ensuring\ndeterminism; (4) models as epistemic filters - a subject can only fix what\nfalls under its known concepts and properties; (5) presumption of truth - the\ndeclarative content of an event is available for computation from the moment of\nfixation, without external verification. The formalization includes nine axioms\n(A1-A9), ensuring the correctness of executable ontologies: monotonicity of\nhistory (I1), acyclicity of causality (I2), traceability (I3). Special\nattention is given to the model-based approach (A9): event validation via\nschemas, actor authorization, automatic construction of causal chains (W3)\nwithout global time. Practical applicability is demonstrated on the boldsea\nsystem - a workflow engine for executable ontologies, where the theoretical\nconstructs are implemented in BSL (Boldsea Semantic Language). The\nformalization is applicable to distributed systems, microservice architectures,\nDLT platforms, and multiperspectivity scenarios (conflicting facts from\ndifferent subjects).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u4f53-\u4e8b\u4ef6\u7684\u672c\u4f53\u8bba\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u52a8\u6001\u7cfb\u7edf\uff0c\u4e0d\u4f9d\u8d56\u5168\u5c40\u65f6\u95f4\u3002\u6838\u5fc3\u539f\u5219\u5305\u62ec\u4e8b\u4ef6\u4f5c\u4e3a\u56fa\u5b9a\u884c\u4e3a\u3001\u56e0\u679c\u987a\u5e8f\u901a\u8fc7happens-before\u5b9a\u4e49\u3001\u53ef\u6267\u884c\u672c\u4f53\u8bba\u7b49\u3002", "motivation": "\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e0d\u4f9d\u8d56\u5168\u5c40\u65f6\u95f4\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u3001\u5fae\u670d\u52a1\u67b6\u6784\u548c\u591a\u65b9\u89c6\u89d2\u573a\u666f\u3002", "method": "\u57fa\u4e8e\u4e5d\u4e2a\u516c\u7406(A1-A9)\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4e8b\u4ef6\u56fa\u5b9a\u3001\u56e0\u679c\u987a\u5e8f\u3001\u53ef\u6267\u884c\u672c\u4f53\u8bba\u3001\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u8fc7\u6ee4\u5668\u7b49\u539f\u5219\uff0c\u5e76\u901a\u8fc7boldsea\u7cfb\u7edf\u5b9e\u73b0\u3002", "result": "\u5efa\u7acb\u4e86\u786e\u4fdd\u53ef\u6267\u884c\u672c\u4f53\u8bba\u6b63\u786e\u6027\u7684\u4e09\u4e2a\u4e0d\u53d8\u6027\uff1a\u5386\u53f2\u5355\u8c03\u6027(I1)\u3001\u56e0\u679c\u65e0\u73af\u6027(I2)\u3001\u53ef\u8ffd\u6eaf\u6027(I3)\uff0c\u5e76\u5728boldsea\u5de5\u4f5c\u6d41\u5f15\u64ce\u4e2d\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u3001\u5fae\u670d\u52a1\u67b6\u6784\u548c\u591a\u65b9\u89c6\u89d2\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7boldsea\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17919", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17919", "abs": "https://arxiv.org/abs/2510.17919", "authors": ["Tenghui Huang", "Jinbo Wen", "Jiawen Kang", "Siyong Chen", "Zhengtao Li", "Tao Zhang", "Dongning Liu", "Jiacheng Wang", "Chengjun Cai", "Yinqiu Liu", "Dusit Niyato"], "title": "ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection", "comment": null, "summary": "Smart contracts play a significant role in automating blockchain services.\nNevertheless, vulnerabilities in smart contracts pose serious threats to\nblockchain security. Currently, traditional detection methods primarily rely on\nstatic analysis and formal verification, which can result in high\nfalse-positive rates and poor scalability. Large Language Models (LLMs) have\nrecently made significant progress in smart contract vulnerability detection.\nHowever, they still face challenges such as high inference costs and\nsubstantial computational overhead. In this paper, we propose ParaVul, a\nparallel LLM and retrieval-augmented framework to improve the reliability and\naccuracy of smart contract vulnerability detection. Specifically, we first\ndevelop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA\nintroduces sparsification by incorporating a sparse matrix into quantized\nLoRA-based LLMs, thereby reducing computational overhead and resource\nrequirements while enhancing their ability to understand vulnerability-related\nissues. We then construct a vulnerability contract dataset and develop a hybrid\nRetrieval-Augmented Generation (RAG) system that integrates dense retrieval\nwith Best Matching 25 (BM25), assisting in verifying the results generated by\nthe LLM. Furthermore, we propose a meta-learning model to fuse the outputs of\nthe RAG system and the LLM, thereby generating the final detection results.\nAfter completing vulnerability detection, we design chain-of-thought prompts to\nguide LLMs to generate comprehensive vulnerability detection reports.\nSimulation results demonstrate the superiority of ParaVul, especially in terms\nof F1 scores, achieving 0.9398 for single-label detection and 0.9330 for\nmulti-label detection.", "AI": {"tldr": "ParaVul\u662f\u4e00\u4e2a\u5e76\u884cLLM\u548c\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7SLoRA\u5fae\u8c03\u3001\u6df7\u5408RAG\u7cfb\u7edf\u548c\u5143\u5b66\u4e60\u878d\u5408\uff0c\u63d0\u9ad8\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\uff0c\u5728F1\u5206\u6570\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709LLM\u65b9\u6cd5\u9762\u4e34\u9ad8\u63a8\u7406\u6210\u672c\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u5f00\u53d1SLoRA\u7528\u4e8eLLM\u5fae\u8c03\uff0c\u6784\u5efa\u6df7\u5408RAG\u7cfb\u7edf\uff08\u5bc6\u96c6\u68c0\u7d22+BM25\uff09\uff0c\u63d0\u51fa\u5143\u5b66\u4e60\u6a21\u578b\u878d\u5408RAG\u548cLLM\u8f93\u51fa\uff0c\u5e76\u4f7f\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u751f\u6210\u68c0\u6d4b\u62a5\u544a\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aParaVul\u5728F1\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5355\u6807\u7b7e\u68c0\u6d4b\u8fbe\u52300.9398\uff0c\u591a\u6807\u7b7e\u68c0\u6d4b\u8fbe\u52300.9330\u3002", "conclusion": "ParaVul\u6846\u67b6\u901a\u8fc7\u5e76\u884cLLM\u548c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2510.18592", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18592", "abs": "https://arxiv.org/abs/2510.18592", "authors": ["Yuval Gil", "Merav Parter"], "title": "Distributed Interactive Proofs for Planarity with Log-Star Communication", "comment": "To appear in SODA 26", "summary": "We provide new communication-efficient distributed interactive proofs for\nplanarity. The notion of a \\emph{distributed interactive proof (DIP)} was\nintroduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \\emph{prover}\nis a single centralized entity whose goal is to prove a certain claim regarding\nan input graph $G$. To do so, the prover communicates with a distributed\n\\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is\nmeasured by the amount of prover-verifier communication it requires. Namely,\nthe goal is to design a DIP with a small number of interaction rounds and a\nsmall \\emph{proof size}, i.e., a small amount of communication per round. Our\nmain result is an $O(\\log ^{*}n)$-round DIP protocol for embedded planarity and\nplanarity with a proof size of $O(1)$ and $O(\\lceil\\log \\Delta/\\log\n^{*}n\\rceil)$, respectively. In fact, this result can be generalized as\nfollows. For any $1\\leq r\\leq \\log^{*}n$, there exists an $O(r)$-round protocol\nfor embedded planarity and planarity with a proof size of $O(\\log ^{(r)}n)$ and\n$O(\\log ^{(r)}n+\\log \\Delta /r)$, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u4ea4\u4e92\u8bc1\u660e\u534f\u8bae\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5e73\u9762\u6027\u3002\u4e3b\u8981\u6210\u679c\u662f\u5f00\u53d1\u4e86O(log* n)\u8f6e\u6b21\u7684\u534f\u8bae\uff0c\u5176\u4e2d\u5d4c\u5165\u5e73\u9762\u6027\u7684\u8bc1\u660e\u5927\u5c0f\u4e3aO(1)\uff0c\u666e\u901a\u5e73\u9762\u6027\u7684\u8bc1\u660e\u5927\u5c0f\u4e3aO(\u2308log \u0394/log* n\u2309)\u3002", "motivation": "\u5206\u5e03\u5f0f\u4ea4\u4e92\u8bc1\u660e(DIP)\u7531Kol\u3001Oshman\u548cSaxena\u57282018\u5e74\u63d0\u51fa\uff0c\u65e8\u5728\u901a\u8fc7\u96c6\u4e2d\u5f0f\u8bc1\u660e\u8005\u4e0e\u5206\u5e03\u5f0f\u9a8c\u8bc1\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u6765\u9a8c\u8bc1\u56fe\u7684\u6027\u8d28\uff0c\u76ee\u6807\u662f\u8bbe\u8ba1\u901a\u4fe1\u91cf\u5c0f\u7684\u534f\u8bae\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u8f6e\u4ea4\u4e92\u7684\u5206\u5e03\u5f0f\u4ea4\u4e92\u8bc1\u660e\u534f\u8bae\uff0c\u9488\u5bf9\u5d4c\u5165\u5e73\u9762\u6027\u548c\u666e\u901a\u5e73\u9762\u6027\u5206\u522b\u4f18\u5316\u4e86\u8bc1\u660e\u5927\u5c0f\u3002\u534f\u8bae\u5141\u8bb8\u5728\u8f6e\u6b21\u548c\u8bc1\u660e\u5927\u5c0f\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u5bf9\u4e8e\u4efb\u610f1\u2264r\u2264log* n\uff0c\u90fd\u53ef\u4ee5\u8bbe\u8ba1O(r)\u8f6e\u534f\u8bae\u3002", "result": "\u5b9e\u73b0\u4e86O(log* n)\u8f6e\u6b21\u7684\u534f\u8bae\uff0c\u5d4c\u5165\u5e73\u9762\u6027\u7684\u8bc1\u660e\u5927\u5c0f\u4e3aO(1)\uff0c\u666e\u901a\u5e73\u9762\u6027\u7684\u8bc1\u660e\u5927\u5c0f\u4e3aO(\u2308log \u0394/log* n\u2309)\u3002\u66f4\u4e00\u822c\u5730\uff0c\u5bf9\u4e8e\u4efb\u610fr\uff0c\u5d4c\u5165\u5e73\u9762\u6027\u7684\u8bc1\u660e\u5927\u5c0f\u4e3aO(log(r)n)\uff0c\u666e\u901a\u5e73\u9762\u6027\u7684\u8bc1\u660e\u5927\u5c0f\u4e3aO(log(r)n + log \u0394/r)\u3002", "conclusion": "\u672c\u6587\u663e\u8457\u6539\u8fdb\u4e86\u5e73\u9762\u6027\u9a8c\u8bc1\u7684\u5206\u5e03\u5f0f\u4ea4\u4e92\u8bc1\u660e\u534f\u8bae\u7684\u901a\u4fe1\u6548\u7387\uff0c\u4e3a\u56fe\u6027\u8d28\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18640", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18640", "abs": "https://arxiv.org/abs/2510.18640", "authors": ["Nils Japke", "Sebastian Koch", "Helmut Lukasczyk", "David Bermbach"], "title": "Towards an Optimized Benchmarking Platform for CI/CD Pipelines", "comment": "Published in 2025 IEEE International Conference on Cloud Engineering\n  (IC2E)", "summary": "Performance regressions in large-scale software systems can lead to\nsubstantial resource inefficiencies, making their early detection critical.\nFrequent benchmarking is essential for identifying these regressions and\nmaintaining service-level agreements (SLAs). Performance benchmarks, however,\nare resource-intensive and time-consuming, which is a major challenge for\nintegration into Continuous Integration / Continuous Deployment (CI/CD)\npipelines. Although numerous benchmark optimization techniques have been\nproposed to accelerate benchmark execution, there is currently no practical\nsystem that integrates these optimizations seamlessly into real-world CI/CD\npipelines. In this vision paper, we argue that the field of benchmark\noptimization remains under-explored in key areas that hinder its broader\nadoption. We identify three central challenges to enabling frequent and\nefficient benchmarking: (a) the composability of benchmark optimization\nstrategies, (b) automated evaluation of benchmarking results, and (c) the\nusability and complexity of applying these strategies as part of CI/CD systems\nin practice. We also introduce a conceptual cloud-based benchmarking framework\nhandling these challenges transparently. By presenting these open problems, we\naim to stimulate research toward making performance regression detection in\nCI/CD systems more practical and effective.", "AI": {"tldr": "\u6027\u80fd\u56de\u5f52\u68c0\u6d4b\u5728\u5927\u578b\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5b9e\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u7cfb\u7edf\u96c6\u6210\u5230CI/CD\u6d41\u6c34\u7ebf\u4e2d\u3002\u672c\u6587\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u7b56\u7565\u7684\u53ef\u7ec4\u5408\u6027\u3001\u81ea\u52a8\u5316\u8bc4\u4f30\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6027\u4e91\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u56de\u5f52\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u65e9\u671f\u68c0\u6d4b\u3002\u9891\u7e41\u7684\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u8bc6\u522b\u8fd9\u4e9b\u56de\u5f52\u548c\u7ef4\u62a4\u670d\u52a1\u6c34\u5e73\u534f\u8bae(SLA)\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u8017\u65f6\uff0c\u96be\u4ee5\u96c6\u6210\u5230CI/CD\u6d41\u6c34\u7ebf\u4e2d\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u7b56\u7565\u7684\u53ef\u7ec4\u5408\u6027\u3001\u81ea\u52a8\u5316\u8bc4\u4f30\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u590d\u6742\u6027\u3002\u540c\u65f6\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u6982\u5ff5\u6027\u7684\u4e91\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6765\u900f\u660e\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "result": "\u8bc6\u522b\u4e86\u963b\u788d\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u66f4\u5e7f\u6cdb\u91c7\u7528\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a(a)\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u7b56\u7565\u7684\u53ef\u7ec4\u5408\u6027\uff0c(b)\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c(c)\u5728\u5b9e\u9645CI/CD\u7cfb\u7edf\u4e2d\u5e94\u7528\u8fd9\u4e9b\u7b56\u7565\u7684\u53ef\u7528\u6027\u548c\u590d\u6742\u6027\u3002", "conclusion": "\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u9886\u57df\u5728\u5173\u952e\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u963b\u788d\u4e86\u5176\u66f4\u5e7f\u6cdb\u91c7\u7528\u3002\u901a\u8fc7\u63d0\u51fa\u8fd9\u4e9b\u5f00\u653e\u6027\u95ee\u9898\uff0c\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u4f7fCI/CD\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u56de\u5f52\u68c0\u6d4b\u66f4\u52a0\u5b9e\u7528\u548c\u6709\u6548\u3002"}}
{"id": "2510.18087", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18087", "abs": "https://arxiv.org/abs/2510.18087", "authors": ["Daniel Israel", "Tian Jin", "Ellie Cheng", "Guy Van den Broeck", "Aditya Grover", "Suvinay Subramanian", "Michael Carbin"], "title": "Planned Diffusion", "comment": "10 pages, 8 figures", "summary": "A central challenge in large language model inference is the trade-off\nbetween generation speed and output quality. Autoregressive models produce\nhigh-quality text but generate tokens sequentially. Diffusion models can\ngenerate tokens in parallel but often need many iterations to match the same\nquality. We propose planned diffusion, a hybrid method that combines the\nstrengths of both paradigms. Planned diffusion works in two stages: first, the\nmodel creates a short autoregressive plan that breaks the output into smaller,\nindependent spans. Second, the model generates these spans simultaneously using\ndiffusion. This approach expands the speed-quality Pareto frontier and provides\na practical path to faster, high-quality text generation. On AlpacaEval, a\nsuite of 805 instruction-following prompts, planned diffusion achieves\nPareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x\nspeedup over autoregressive generation with only 0.87\\% to 5.4\\% drop in win\nrate, respectively. Our sensitivity analysis shows that the planning mechanism\nof planned diffusion is minimal and reliable, and simple runtime knobs exist to\nprovide flexible control of the quality-latency trade-off.", "AI": {"tldr": "Planned diffusion\u662f\u4e00\u79cd\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u751f\u6210\uff1a\u5148\u81ea\u56de\u5f52\u521b\u5efa\u77ed\u8ba1\u5212\u5206\u89e3\u8f93\u51fa\u4e3a\u72ec\u7acb\u7247\u6bb5\uff0c\u7136\u540e\u7528\u6269\u6563\u6a21\u578b\u5e76\u884c\u751f\u6210\u8fd9\u4e9b\u7247\u6bb5\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u751f\u6210\u901f\u5ea6\u4e0e\u8f93\u51fa\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002\u81ea\u56de\u5f52\u6a21\u578b\u8d28\u91cf\u9ad8\u4f46\u751f\u6210\u901f\u5ea6\u6162\uff0c\u6269\u6563\u6a21\u578b\u53ef\u5e76\u884c\u751f\u6210\u4f46\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u624d\u80fd\u8fbe\u5230\u76f8\u540c\u8d28\u91cf\u3002", "method": "\u4e24\u9636\u6bb5\u6df7\u5408\u65b9\u6cd5\uff1a1) \u81ea\u56de\u5f52\u9636\u6bb5\u521b\u5efa\u77ed\u8ba1\u5212\uff0c\u5c06\u8f93\u51fa\u5206\u89e3\u4e3a\u8f83\u5c0f\u7684\u72ec\u7acb\u7247\u6bb5\uff1b2) \u6269\u6563\u9636\u6bb5\u5e76\u884c\u751f\u6210\u8fd9\u4e9b\u7247\u6bb5\u3002", "result": "\u5728AlpacaEval 805\u4e2a\u6307\u4ee4\u8ddf\u968f\u63d0\u793a\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e86Pareto\u6700\u4f18\u7684\u8d28\u91cf-\u5ef6\u8fdf\u6743\u8861\uff0c\u76f8\u6bd4\u81ea\u56de\u5f52\u751f\u6210\u83b7\u5f971.27x\u52301.81x\u901f\u5ea6\u63d0\u5347\uff0c\u4ec5\u635f\u59310.87%\u52305.4%\u7684\u80dc\u7387\u3002", "conclusion": "Planned diffusion\u6269\u5c55\u4e86\u901f\u5ea6-\u8d28\u91cfPareto\u8fb9\u754c\uff0c\u4e3a\u66f4\u5feb\u3001\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5176\u89c4\u5212\u673a\u5236\u7b80\u6d01\u53ef\u9760\uff0c\u4e14\u53ef\u901a\u8fc7\u7b80\u5355\u8fd0\u884c\u65f6\u8c03\u8282\u7075\u6d3b\u63a7\u5236\u8d28\u91cf-\u5ef6\u8fdf\u6743\u8861\u3002"}}
{"id": "2510.18003", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.18003", "abs": "https://arxiv.org/abs/2510.18003", "authors": ["Fengqing Jiang", "Yichen Feng", "Yuetai Li", "Luyao Niu", "Basel Alomair", "Radha Poovendran"], "title": "BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?", "comment": null, "summary": "The convergence of LLM-powered research assistants and AI-based peer review\nsystems creates a critical vulnerability: fully automated publication loops\nwhere AI-generated research is evaluated by AI reviewers without human\noversight. We investigate this through \\textbf{BadScientist}, a framework that\nevaluates whether fabrication-oriented paper generation agents can deceive\nmulti-model LLM review systems. Our generator employs presentation-manipulation\nstrategies requiring no real experiments. We develop a rigorous evaluation\nframework with formal error guarantees (concentration bounds and calibration\nanalysis), calibrated on real data. Our results reveal systematic\nvulnerabilities: fabricated papers achieve acceptance rates up to . Critically,\nwe identify \\textit{concern-acceptance conflict} -- reviewers frequently flag\nintegrity issues yet assign acceptance-level scores. Our mitigation strategies\nshow only marginal improvements, with detection accuracy barely exceeding\nrandom chance. Despite provably sound aggregation mathematics, integrity\nchecking systematically fails, exposing fundamental limitations in current\nAI-driven review systems and underscoring the urgent need for defense-in-depth\nsafeguards in scientific publishing.", "AI": {"tldr": "BadScientist\u6846\u67b6\u63ed\u793a\u4e86LLM\u9a71\u52a8\u7684\u79d1\u7814\u52a9\u624b\u4e0eAI\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u7ed3\u5408\u65f6\u7684\u5173\u952e\u6f0f\u6d1e\uff1a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u53d1\u8868\u5faa\u73af\uff0c\u5176\u4e2dAI\u751f\u6210\u7684\u7814\u7a76\u88abAI\u8bc4\u5ba1\u5458\u8bc4\u4f30\u800c\u65e0\u4eba\u76d1\u7763\u3002\u7814\u7a76\u53d1\u73b0\u4f2a\u9020\u8bba\u6587\u7684\u63a5\u53d7\u7387\u5f88\u9ad8\uff0c\u8bc4\u5ba1\u5458\u7ecf\u5e38\u6807\u8bb0\u8bda\u4fe1\u95ee\u9898\u4f46\u4ecd\u7ed9\u51fa\u63a5\u53d7\u7ea7\u522b\u7684\u5206\u6570\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc6\u522bLLM\u9a71\u52a8\u7684\u79d1\u7814\u52a9\u624b\u4e0eAI\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u7ed3\u5408\u65f6\u4ea7\u751f\u7684\u5173\u952e\u6f0f\u6d1e\u2014\u2014\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u53d1\u8868\u5faa\u73af\uff0c\u5176\u4e2dAI\u751f\u6210\u7684\u7814\u7a76\u88abAI\u8bc4\u5ba1\u5458\u8bc4\u4f30\u800c\u65e0\u4eba\u76d1\u7763\uff0c\u8fd9\u53ef\u80fd\u7834\u574f\u79d1\u5b66\u51fa\u7248\u7684\u5b8c\u6574\u6027\u3002", "method": "\u5f00\u53d1\u4e86BadScientist\u6846\u67b6\uff0c\u4f7f\u7528\u65e0\u9700\u771f\u5b9e\u5b9e\u9a8c\u7684\u5448\u73b0\u64cd\u7eb5\u7b56\u7565\u751f\u6210\u4f2a\u9020\u8bba\u6587\uff0c\u5e76\u5efa\u7acb\u5177\u6709\u6b63\u5f0f\u9519\u8bef\u4fdd\u8bc1\uff08\u96c6\u4e2d\u754c\u9650\u548c\u6821\u51c6\u5206\u6790\uff09\u7684\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u6821\u51c6\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff1a\u4f2a\u9020\u8bba\u6587\u7684\u63a5\u53d7\u7387\u9ad8\u8fbe\u4e00\u5b9a\u6c34\u5e73\u3002\u5173\u952e\u53d1\u73b0\u662f\"\u5173\u6ce8-\u63a5\u53d7\u51b2\u7a81\"\u2014\u2014\u8bc4\u5ba1\u5458\u7ecf\u5e38\u6807\u8bb0\u8bda\u4fe1\u95ee\u9898\u4f46\u4ecd\u7ed9\u51fa\u63a5\u53d7\u7ea7\u522b\u7684\u5206\u6570\u3002\u7f13\u89e3\u7b56\u7565\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u673a\u4f1a\u3002", "conclusion": "\u5c3d\u7ba1\u5177\u6709\u53ef\u8bc1\u660e\u6b63\u786e\u7684\u805a\u5408\u6570\u5b66\uff0c\u8bda\u4fe1\u68c0\u67e5\u7cfb\u7edf\u6027\u5730\u5931\u8d25\uff0c\u66b4\u9732\u4e86\u5f53\u524dAI\u9a71\u52a8\u8bc4\u5ba1\u7cfb\u7edf\u7684\u57fa\u672c\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u79d1\u5b66\u51fa\u7248\u4e2d\u8feb\u5207\u9700\u8981\u6df1\u5ea6\u9632\u5fa1\u4fdd\u969c\u63aa\u65bd\u3002"}}
{"id": "2510.18838", "categories": ["cs.DC", "physics.comp-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.18838", "abs": "https://arxiv.org/abs/2510.18838", "authors": ["Jacob S. Merson", "Cameron W. Smith", "Mark S. Shephard", "Fuad Hasan", "Abhiyan Paudel", "Angel Castillo-Crooke", "Joyal Mathew", "Mohammad Elahi"], "title": "PCMS: Parallel Coupler For Multimodel Simulations", "comment": null, "summary": "This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a\nnew GPU accelerated generalized coupling framework for coupling simulation\ncodes on leadership class supercomputers. PCMS includes distributed control and\nfield mapping methods for up to five dimensions. For field mapping PCMS can\nutilize discretization and field information to accommodate physics\nconstraints. PCMS is demonstrated with a coupling of the gyrokinetic\nmicroturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and\nwith a 5D distribution function coupling of an energetic particle transport\ncode (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also\ndemonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of\n85%.", "AI": {"tldr": "PCMS\u662f\u4e00\u4e2a\u65b0\u7684GPU\u52a0\u901f\u901a\u7528\u8026\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8026\u5408\u6a21\u62df\u4ee3\u7801\uff0c\u652f\u6301\u591a\u8fbe\u4e94\u7ef4\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u573a\u6620\u5c04\u65b9\u6cd5\uff0c\u5e76\u5728Frontier\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u5f31\u6269\u5c55\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u9886\u5bfc\u7ea7\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u9ad8\u6548\u8026\u5408\u4e0d\u540c\u6a21\u62df\u4ee3\u7801\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u590d\u6742\u7684\u591a\u7269\u7406\u573a\u6a21\u62df\u9700\u6c42\u3002", "method": "PCMS\u91c7\u7528GPU\u52a0\u901f\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u573a\u6620\u5c04\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u8fbe\u4e94\u7ef4\u7684\u8026\u5408\uff0c\u5e76\u80fd\u5229\u7528\u79bb\u6563\u5316\u548c\u573a\u4fe1\u606f\u6765\u9002\u5e94\u7269\u7406\u7ea6\u675f\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86XGC\u4e0eDEGAS2\u7684\u8026\u5408\uff0c\u4ee5\u53caGNET\u4e0eGTC\u76845D\u5206\u5e03\u51fd\u6570\u8026\u5408\uff1b\u5728Frontier\u76842,080\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e8685%\u7684\u5f31\u6269\u5c55\u6548\u7387\u3002", "conclusion": "PCMS\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u578b\u6a21\u62df\u8026\u5408\u6846\u67b6\uff0c\u5728GPU\u52a0\u901f\u7684\u8d85\u7ea7\u8ba1\u7b97\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2510.18109", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18109", "abs": "https://arxiv.org/abs/2510.18109", "authors": ["Wan Ki Wong", "Sahel Torkamani", "Michele Ciampi", "Rik Sarkar"], "title": "PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces", "comment": null, "summary": "Evaluating the relevance of data is a critical task for model builders\nseeking to acquire datasets that enhance model performance. Ideally, such\nevaluation should allow the model builder to assess the utility of candidate\ndata without exposing proprietary details of the model. At the same time, data\nproviders must be assured that no information about their data - beyond the\ncomputed utility score - is disclosed to the model builder.\n  In this paper, we present PrivaDE, a cryptographic protocol for\nprivacy-preserving utility scoring and selection of data for machine learning.\nWhile prior works have proposed data evaluation protocols, our approach\nadvances the state of the art through a practical, blockchain-centric design.\nLeveraging the trustless nature of blockchains, PrivaDE enforces\nmalicious-security guarantees and ensures strong privacy protection for both\nmodels and datasets. To achieve efficiency, we integrate several techniques -\nincluding model distillation, model splitting, and cut-and-choose\nzero-knowledge proofs - bringing the runtime to a practical level. Furthermore,\nwe propose a unified utility scoring function that combines empirical loss,\npredictive entropy, and feature-space diversity, and that can be seamlessly\nintegrated into active-learning workflows. Evaluation shows that PrivaDE\nperforms data evaluation effectively, achieving online runtimes within 15\nminutes even for models with millions of parameters.\n  Our work lays the foundation for fair and automated data marketplaces in\ndecentralized machine learning ecosystems.", "AI": {"tldr": "PrivaDE\u662f\u4e00\u4e2a\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u6548\u7528\u8bc4\u5206\u548c\u9009\u62e9\u7684\u52a0\u5bc6\u534f\u8bae\uff0c\u91c7\u7528\u533a\u5757\u94fe\u4e2d\u5fc3\u5316\u8bbe\u8ba1\uff0c\u786e\u4fdd\u6a21\u578b\u548c\u6570\u636e\u7684\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u5b9e\u73b015\u5206\u949f\u5185\u5b8c\u6210\u6570\u636e\u8bc4\u4f30\u3002", "motivation": "\u6a21\u578b\u6784\u5efa\u8005\u9700\u8981\u8bc4\u4f30\u6570\u636e\u76f8\u5173\u6027\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u9700\u8981\u5728\u4e0d\u66b4\u9732\u6a21\u578b\u4e13\u6709\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\uff0c\u540c\u65f6\u6570\u636e\u63d0\u4f9b\u8005\u9700\u8981\u786e\u4fdd\u9664\u4e86\u8ba1\u7b97\u51fa\u7684\u6548\u7528\u5206\u6570\u5916\uff0c\u5176\u6570\u636e\u4fe1\u606f\u4e0d\u4f1a\u6cc4\u9732\u7ed9\u6a21\u578b\u6784\u5efa\u8005\u3002", "method": "\u91c7\u7528\u533a\u5757\u94fe\u4e2d\u5fc3\u5316\u8bbe\u8ba1\uff0c\u6574\u5408\u6a21\u578b\u84b8\u998f\u3001\u6a21\u578b\u5206\u5272\u548c\u96f6\u77e5\u8bc6\u8bc1\u660e\u7b49\u6280\u672f\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u6548\u7528\u8bc4\u5206\u51fd\u6570\uff0c\u7ed3\u5408\u7ecf\u9a8c\u635f\u5931\u3001\u9884\u6d4b\u71b5\u548c\u7279\u5f81\u7a7a\u95f4\u591a\u6837\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793aPrivaDE\u80fd\u6709\u6548\u8fdb\u884c\u6570\u636e\u8bc4\u4f30\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5177\u6709\u6570\u767e\u4e07\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5728\u7ebf\u8fd0\u884c\u65f6\u95f4\u4e5f\u572815\u5206\u949f\u5185\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u548c\u81ea\u52a8\u5316\u6570\u636e\u5e02\u573a\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.18143", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18143", "abs": "https://arxiv.org/abs/2510.18143", "authors": ["Huan Song", "Deeksha Razdan", "Yiyue Qian", "Arijit Ghosh Chowdhury", "Parth Patwa", "Aman Chadha", "Shinan Zhang", "Sharlina Keshava", "Hannah Marlowe"], "title": "Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models", "comment": "Neural Information Processing Systems (NeurIPS 2025) Workshop:\n  Evaluating the Evolving LLM Lifecycle", "summary": "Small Language Models (SLMs) offer compelling advantages in deployment cost\nand latency, but their accuracy often lags behind larger models, particularly\nfor complex domain-specific tasks. While supervised fine-tuning can help bridge\nthis performance gap, it requires substantial manual effort in data preparation\nand iterative optimization. We present PaDA-Agent (Pattern-guided Data\nAugmentation Agent), an evaluation-driven approach that streamlines the data\naugmentation process for SLMs through coordinated operations. Unlike\nstate-of-the-art approaches that focus on model training errors only and\ngenerating error-correcting samples, PaDA-Agent discovers failure patterns from\nthe validation data via evaluations and drafts targeted data augmentation\nstrategies aiming to directly reduce the generalization gap. Our experimental\nresults demonstrate significant improvements over state-of-the-art LLM-based\ndata augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.", "AI": {"tldr": "PaDA-Agent\u662f\u4e00\u79cd\u8bc4\u4f30\u9a71\u52a8\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d1\u73b0\u9a8c\u8bc1\u6570\u636e\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u6765\u5236\u5b9a\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4ece\u800c\u7f29\u5c0f\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u5dee\u8ddd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u590d\u6742\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u5f80\u5f80\u843d\u540e\u4e8e\u5927\u578b\u6a21\u578b\u3002\u867d\u7136\u6709\u76d1\u7763\u5fae\u8c03\u53ef\u4ee5\u5f25\u8865\u6027\u80fd\u5dee\u8ddd\uff0c\u4f46\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6570\u636e\u51c6\u5907\u548c\u8fed\u4ee3\u4f18\u5316\u5de5\u4f5c\u3002", "method": "PaDA-Agent\u901a\u8fc7\u8bc4\u4f30\u4ece\u9a8c\u8bc1\u6570\u636e\u4e2d\u53d1\u73b0\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u5236\u5b9a\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u76f4\u63a5\u51cf\u5c11\u6cdb\u5316\u5dee\u8ddd\uff0c\u800c\u4e0d\u662f\u4ec5\u5173\u6ce8\u6a21\u578b\u8bad\u7ec3\u9519\u8bef\u548c\u751f\u6210\u7ea0\u9519\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728Llama 3.2 1B Instruct\u6a21\u578b\u5fae\u8c03\u4e2d\uff0cPaDA-Agent\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "PaDA-Agent\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u3002"}}
{"id": "2510.18154", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.18154", "abs": "https://arxiv.org/abs/2510.18154", "authors": ["Antonio-Gabriel Chac\u00f3n Menke", "Phan Xuan Tan", "Eiji Kamioka"], "title": "Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety", "comment": null, "summary": "Recent work has highlighted the importance of monitoring chain-of-thought\nreasoning for AI safety; however, current approaches that analyze textual\nreasoning steps can miss subtle harmful patterns and may be circumvented by\nmodels that hide unsafe reasoning. We present a sentence-level labeled dataset\nthat enables activation-based monitoring of safety behaviors during LLM\nreasoning. Our dataset contains reasoning sequences with sentence-level\nannotations of safety behaviors such as expression of safety concerns or\nspeculation on user intent, which we use to extract steering vectors for\ndetecting and influencing these behaviors within model activations. The dataset\nfills a key gap in safety research: while existing datasets label reasoning\nholistically, effective application of steering vectors for safety monitoring\ncould be improved by identifying precisely when specific behaviors occur within\nreasoning chains. We demonstrate the dataset's utility by extracting\nrepresentations that both detect and steer safety behaviors in model\nactivations, showcasing the potential of activation-level techniques for\nimproving safety oversight on reasoning.\n  Content Warning: This paper discusses AI safety in the context of harmful\nprompts and may contain references to potentially harmful content.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53e5\u5b50\u7ea7\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u57fa\u4e8e\u6fc0\u6d3b\u7684\u5b89\u5168\u884c\u4e3a\u76d1\u63a7\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u6574\u4f53\u6807\u6ce8\u63a8\u7406\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u63a8\u7406\u6b65\u9aa4\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\u53ef\u80fd\u9057\u6f0f\u7ec6\u5fae\u7684\u6709\u5bb3\u6a21\u5f0f\uff0c\u4e14\u53ef\u80fd\u88ab\u9690\u85cf\u4e0d\u5b89\u5168\u63a8\u7406\u7684\u6a21\u578b\u89c4\u907f\u3002\u9700\u8981\u66f4\u7cbe\u786e\u7684\u6fc0\u6d3b\u5c42\u9762\u76d1\u63a7\u6280\u672f\u3002", "method": "\u6784\u5efa\u5305\u542b\u63a8\u7406\u5e8f\u5217\u548c\u53e5\u5b50\u7ea7\u5b89\u5168\u884c\u4e3a\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u63d0\u53d6\u7528\u4e8e\u68c0\u6d4b\u548c\u5f71\u54cd\u8fd9\u4e9b\u884c\u4e3a\u7684\u5f15\u5bfc\u5411\u91cf\uff0c\u5728\u6a21\u578b\u6fc0\u6d3b\u5c42\u9762\u8fdb\u884c\u64cd\u4f5c\u3002", "result": "\u5c55\u793a\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\uff0c\u63d0\u53d6\u7684\u8868\u5f81\u80fd\u591f\u5728\u6a21\u578b\u6fc0\u6d3b\u4e2d\u68c0\u6d4b\u548c\u5f15\u5bfc\u5b89\u5168\u884c\u4e3a\uff0c\u8bc1\u660e\u4e86\u6fc0\u6d3b\u5c42\u9762\u6280\u672f\u5bf9\u6539\u8fdb\u63a8\u7406\u5b89\u5168\u76d1\u7763\u7684\u6f5c\u529b\u3002", "conclusion": "\u6fc0\u6d3b\u5c42\u9762\u7684\u5b89\u5168\u76d1\u63a7\u6280\u672f\u80fd\u591f\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u548c\u5f71\u54cd\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u884c\u4e3a\uff0c\u4e3aAI\u5b89\u5168\u76d1\u7763\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2510.18155", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.18155", "abs": "https://arxiv.org/abs/2510.18155", "authors": ["Man-Lin Chu", "Lucian Terhorst", "Kadin Reed", "Tom Ni", "Weiwei Chen", "Rongyu Lin"], "title": "LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior", "comment": "Accepted for publication at IEEE International Conference on\n  e-Business Engineering ICEBE 2025, November 10-12, Buraydah, Saudi Arabia. 8\n  pages, 5 figures", "summary": "Simulating consumer decision-making is vital for designing and evaluating\nmarketing strategies before costly real- world deployment. However, post-event\nanalyses and rule-based agent-based models (ABMs) struggle to capture the\ncomplexity of human behavior and social interaction. We introduce an\nLLM-powered multi-agent simulation framework that models consumer decisions and\nsocial dynamics. Building on recent advances in large language model simulation\nin a sandbox envi- ronment, our framework enables generative agents to\ninteract, express internal reasoning, form habits, and make purchasing\ndecisions without predefined rules. In a price-discount marketing scenario, the\nsystem delivers actionable strategy-testing outcomes and reveals emergent\nsocial patterns beyond the reach of con- ventional methods. This approach\noffers marketers a scalable, low-risk tool for pre-implementation testing,\nreducing reliance on time-intensive post-event evaluations and lowering the\nrisk of underperforming campaigns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6d88\u8d39\u8005\u51b3\u7b56\u548c\u793e\u4f1a\u52a8\u6001\uff0c\u4e3a\u8425\u9500\u7b56\u7565\u63d0\u4f9b\u4f4e\u6210\u672c\u7684\u524d\u671f\u6d4b\u8bd5\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8b\u540e\u5206\u6790\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u667a\u80fd\u4f53\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u548c\u793e\u4f1a\u4e92\u52a8\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u6d88\u8d39\u8005\u51b3\u7b56\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u6784\u5efa\u591a\u667a\u80fd\u4f53\u4eff\u771f\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u4e92\u52a8\u3001\u8868\u8fbe\u5185\u90e8\u63a8\u7406\u3001\u5f62\u6210\u4e60\u60ef\u5e76\u505a\u51fa\u8d2d\u4e70\u51b3\u7b56\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u89c4\u5219\u3002", "result": "\u5728\u4ef7\u683c\u6298\u6263\u8425\u9500\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7684\u65b0\u5174\u793e\u4f1a\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8425\u9500\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4f4e\u98ce\u9669\u7684\u9884\u5b9e\u65bd\u6d4b\u8bd5\u5de5\u5177\uff0c\u51cf\u5c11\u4e86\u5bf9\u8017\u65f6\u7684\u4e8b\u540e\u8bc4\u4f30\u7684\u4f9d\u8d56\uff0c\u5e76\u964d\u4f4e\u4e86\u8425\u9500\u6d3b\u52a8\u8868\u73b0\u4e0d\u4f73\u7684\u98ce\u9669\u3002"}}
{"id": "2510.18165", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18165", "abs": "https://arxiv.org/abs/2510.18165", "authors": ["Yihong Dong", "Zhaoyu Ma", "Xue Jiang", "Zhiyuan Fan", "Jiaru Qian", "Yongmin Li", "Jianha Xiao", "Zhi Jin", "Rongyu Cao", "Binhua Li", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model", "comment": null, "summary": "Diffusion language models (DLMs) are emerging as a powerful and promising\nalternative to the dominant autoregressive paradigm, offering inherent\nadvantages in parallel generation and bidirectional context modeling. However,\nthe performance of DLMs on code generation tasks, which have stronger\nstructural constraints, is significantly hampered by the critical trade-off\nbetween inference speed and output quality. We observed that accelerating the\ncode generation process by reducing the number of sampling steps usually leads\nto a catastrophic collapse in performance. In this paper, we introduce\nefficient Sampling with Adaptive acceleration and Backtracking Enhanced\nRemasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to\nachieve better inference speed and output quality in code generation.\nSpecifically, Saber is motivated by two key insights in the DLM generation\nprocess: 1) it can be adaptively accelerated as more of the code context is\nestablished; 2) it requires a backtracking mechanism to reverse the generated\ntokens. Extensive experiments on multiple mainstream code generation benchmarks\nshow that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over\nmainstream DLM sampling methods, meanwhile achieving an average 251.4%\ninference speedup. By leveraging the inherent advantages of DLMs, our work\nsignificantly narrows the performance gap with autoregressive models in code\ngeneration.", "AI": {"tldr": "\u63d0\u51faSaber\u7b97\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u578b\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u548c\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u9762\u4e34\u63a8\u7406\u901f\u5ea6\u4e0e\u8f93\u51fa\u8d28\u91cf\u7684\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d\u3002", "method": "Saber\u7b97\u6cd5\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u6d1e\u5bdf\uff1a1\uff09\u968f\u7740\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u5efa\u7acb\u53ef\u4ee5\u81ea\u9002\u5e94\u52a0\u901f\uff1b2\uff09\u9700\u8981\u56de\u6eaf\u673a\u5236\u6765\u53cd\u8f6c\u751f\u6210\u7684\u6807\u8bb0\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSaber\u5c06Pass@1\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53471.9%\uff0c\u540c\u65f6\u5b9e\u73b0\u5e73\u5747251.4%\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u56fa\u6709\u4f18\u52bf\uff0c\u8be5\u5de5\u4f5c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2510.18204", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18204", "abs": "https://arxiv.org/abs/2510.18204", "authors": ["Jiahao Shi", "Tianyi Zhang"], "title": "RESCUE: Retrieval Augmented Secure Code Generation", "comment": null, "summary": "Despite recent advances, Large Language Models (LLMs) still generate\nvulnerable code. Retrieval-Augmented Generation (RAG) has the potential to\nenhance LLMs for secure code generation by incorporating external security\nknowledge. However, the conventional RAG design struggles with the noise of raw\nsecurity-related documents, and existing retrieval methods overlook the\nsignificant security semantics implicitly embedded in task descriptions. To\naddress these issues, we propose RESCUE, a new RAG framework for secure code\ngeneration with two key innovations. First, we propose a hybrid knowledge base\nconstruction method that combines LLM-assisted cluster-then-summarize\ndistillation with program slicing, producing both high-level security\nguidelines and concise, security-focused code examples. Second, we design a\nhierarchical multi-faceted retrieval to traverse the constructed knowledge base\nfrom top to bottom and integrates multiple security-critical facts at each\nhierarchical level, ensuring comprehensive and accurate retrieval. We evaluated\nRESCUE on four benchmarks and compared it with five state-of-the-art secure\ncode generation methods on six LLMs. The results demonstrate that RESCUE\nimproves the SecurePass@1 metric by an average of 4.8 points, establishing a\nnew state-of-the-art performance for security. Furthermore, we performed\nin-depth analysis and ablation studies to rigorously validate the effectiveness\nof individual components in RESCUE.", "AI": {"tldr": "RESCUE\u662f\u4e00\u4e2a\u65b0\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u77e5\u8bc6\u5e93\u6784\u5efa\u548c\u5206\u5c42\u591a\u9762\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5b89\u5168\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u5347SecurePass@1\u6307\u68074.8\u5206\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u4f1a\u751f\u6210\u6613\u53d7\u653b\u51fb\u7684\u4ee3\u7801\uff0c\u4f20\u7edfRAG\u8bbe\u8ba1\u5728\u5904\u7406\u539f\u59cb\u5b89\u5168\u6587\u6863\u65f6\u5b58\u5728\u566a\u58f0\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u5ffd\u7565\u4e86\u4efb\u52a1\u63cf\u8ff0\u4e2d\u9690\u542b\u7684\u91cd\u8981\u5b89\u5168\u8bed\u4e49\u3002", "method": "\u63d0\u51faRESCUE\u6846\u67b6\uff1a1\uff09\u6df7\u5408\u77e5\u8bc6\u5e93\u6784\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u8f85\u52a9\u7684\u805a\u7c7b-\u603b\u7ed3\u84b8\u998f\u548c\u7a0b\u5e8f\u5207\u7247\uff0c\u751f\u6210\u9ad8\u7ea7\u5b89\u5168\u6307\u5357\u548c\u7b80\u6d01\u7684\u5b89\u5168\u4ee3\u7801\u793a\u4f8b\uff1b2\uff09\u5206\u5c42\u591a\u9762\u68c0\u7d22\uff0c\u4ece\u4e0a\u5230\u4e0b\u904d\u5386\u77e5\u8bc6\u5e93\u5e76\u5728\u6bcf\u4e2a\u5c42\u7ea7\u96c6\u6210\u591a\u4e2a\u5b89\u5168\u5173\u952e\u4e8b\u5b9e\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u516d\u4e2aLLM\u4e0a\uff0c\u4e0e\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u5b89\u5168\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4\uff0cRESCUE\u5c06SecurePass@1\u6307\u6807\u5e73\u5747\u63d0\u5347\u4e864.8\u5206\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u5b89\u5168\u6027\u80fd\u6700\u4f73\u6c34\u5e73\u3002", "conclusion": "RESCUE\u901a\u8fc7\u521b\u65b0\u7684\u77e5\u8bc6\u5e93\u6784\u5efa\u548c\u68c0\u7d22\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b89\u5168\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u6df1\u5ea6\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u3002"}}
{"id": "2510.18324", "categories": ["cs.CR", "K.6.5; D.4.6; C.2.0"], "pdf": "https://arxiv.org/pdf/2510.18324", "abs": "https://arxiv.org/abs/2510.18324", "authors": ["Gyeonghoon Park", "Jaehan Kim", "Jinu Choi", "Jinwoo Kim"], "title": "CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments", "comment": "15 pages, 13 figures", "summary": "Host-based cryptomining malware, commonly known as cryptojackers, have gained\nnotoriety for their stealth and the significant financial losses they cause in\nLinux-based cloud environments. Existing solutions often struggle with\nscalability due to high monitoring overhead, low detection accuracy against\nobfuscated behavior, and lack of integrated remediation. We present\nCryptoGuard, a lightweight hybrid solution that combines detection and\nremediation strategies to counter cryptojackers. To ensure scalability,\nCryptoGuard uses sketch- and sliding window-based syscall monitoring to collect\nbehavior patterns with minimal overhead. It decomposes the classification task\ninto a two-phase process, leveraging deep learning models to identify\nsuspicious activity with high precision. To counter evasion techniques such as\nentry point poisoning and PID manipulation, CryptoGuard integrates targeted\nremediation mechanisms based on eBPF, a modern Linux kernel feature deployable\non any compatible host. Evaluated on 123 real-world cryptojacker samples, it\nachieves average F1-scores of 96.12% and 92.26% across the two phases, and\noutperforms state-of-the-art baselines in terms of true and false positive\nrates, while incurring only 0.06% CPU overhead per host.", "AI": {"tldr": "CryptoGuard\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6df7\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u68c0\u6d4b\u548c\u4fee\u590d\u7b56\u7565\u6765\u5bf9\u6297Linux\u4e91\u73af\u5883\u4e2d\u7684\u52a0\u5bc6\u6316\u77ff\u6076\u610f\u8f6f\u4ef6\uff0c\u901a\u8fc7\u4f4e\u5f00\u9500\u7cfb\u7edf\u8c03\u7528\u76d1\u63a7\u548c\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\uff0c\u5e76\u96c6\u6210\u57fa\u4e8eeBPF\u7684\u9488\u5bf9\u6027\u4fee\u590d\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u5728Linux\u4e91\u73af\u5883\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u5dee\u3001\u68c0\u6d4b\u7cbe\u5ea6\u4f4e\u548c\u7f3a\u4e4f\u96c6\u6210\u4fee\u590d\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u52a0\u5bc6\u6316\u77ff\u6076\u610f\u8f6f\u4ef6\u7684\u9690\u853d\u884c\u4e3a\u548c\u89c4\u907f\u6280\u672f\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8349\u56fe\u548c\u6ed1\u52a8\u7a97\u53e3\u7684\u7cfb\u7edf\u8c03\u7528\u76d1\u63a7\u6536\u96c6\u884c\u4e3a\u6a21\u5f0f\uff0c\u5c06\u5206\u7c7b\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc6\u522b\u53ef\u7591\u6d3b\u52a8\uff0c\u5e76\u96c6\u6210\u57fa\u4e8eeBPF\u7684\u9488\u5bf9\u6027\u4fee\u590d\u673a\u5236\u6765\u5bf9\u6297\u89c4\u907f\u6280\u672f\u3002", "result": "\u5728123\u4e2a\u771f\u5b9e\u4e16\u754c\u52a0\u5bc6\u6316\u77ff\u6076\u610f\u8f6f\u4ef6\u6837\u672c\u4e0a\u8bc4\u4f30\uff0c\u4e24\u9636\u6bb5\u7684\u5e73\u5747F1\u5206\u6570\u5206\u522b\u8fbe\u523096.12%\u548c92.26%\uff0c\u5728\u771f\u9633\u6027\u7387\u548c\u5047\u9633\u6027\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u4e3b\u673a\u4ec5\u4ea7\u751f0.06%\u7684CPU\u5f00\u9500\u3002", "conclusion": "CryptoGuard\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u548c\u4fee\u590d\u52a0\u5bc6\u6316\u77ff\u6076\u610f\u8f6f\u4ef6\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u4f4e\u7684\u7cfb\u7edf\u5f00\u9500\u3002"}}
{"id": "2510.18333", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18333", "abs": "https://arxiv.org/abs/2510.18333", "authors": ["Yepeng Liu", "Xuandong Zhao", "Dawn Song", "Gregory W. Wornell", "Yuheng Bu"], "title": "Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption", "comment": null, "summary": "Despite progress in watermarking algorithms for large language models (LLMs),\nreal-world deployment remains limited. We argue that this gap stems from\nmisaligned incentives among LLM providers, platforms, and end users, which\nmanifest as four key barriers: competitive risk, detection-tool governance,\nrobustness concerns and attribution issues. We revisit three classes of\nwatermarking through this lens. \\emph{Model watermarking} naturally aligns with\nLLM provider interests, yet faces new challenges in open-source ecosystems.\n\\emph{LLM text watermarking} offers modest provider benefit when framed solely\nas an anti-misuse tool, but can gain traction in narrowly scoped settings such\nas dataset de-contamination or user-controlled provenance. \\emph{In-context\nwatermarking} (ICW) is tailored for trusted parties, such as conference\norganizers or educators, who embed hidden watermarking instructions into\ndocuments. If a dishonest reviewer or student submits this text to an LLM, the\noutput carries a detectable watermark indicating misuse. This setup aligns\nincentives: users experience no quality loss, trusted parties gain a detection\ntool, and LLM providers remain neutral by simply following watermark\ninstructions. We advocate for a broader exploration of incentive-aligned\nmethods, with ICW as an example, in domains where trusted parties need reliable\ntools to detect misuse. More broadly, we distill design principles for\nincentive-aligned, domain-specific watermarking and outline future research\ndirections. Our position is that the practical adoption of LLM watermarking\nrequires aligning stakeholder incentives in targeted application domains and\nfostering active community engagement.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86LLM\u6c34\u5370\u6280\u672f\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u969c\u788d\uff0c\u63d0\u51fa\u6fc0\u52b1\u9519\u914d\u662f\u4e3b\u8981\u95ee\u9898\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e09\u79cd\u6c34\u5370\u65b9\u6cd5\uff0c\u7279\u522b\u63a8\u8350\u6fc0\u52b1\u5bf9\u9f50\u7684\u4e0a\u4e0b\u6587\u6c34\u5370(ICW)\u4f5c\u4e3a\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1LLM\u6c34\u5370\u7b97\u6cd5\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662fLLM\u63d0\u4f9b\u5546\u3001\u5e73\u53f0\u548c\u7ec8\u7aef\u7528\u6237\u4e4b\u95f4\u7684\u6fc0\u52b1\u9519\u914d\uff0c\u8868\u73b0\u4e3a\u7ade\u4e89\u98ce\u9669\u3001\u68c0\u6d4b\u5de5\u5177\u6cbb\u7406\u3001\u9c81\u68d2\u6027\u95ee\u9898\u548c\u5f52\u56e0\u95ee\u9898\u56db\u5927\u969c\u788d\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u4e09\u7c7b\u6c34\u5370\u65b9\u6cd5\uff1a\u6a21\u578b\u6c34\u5370\uff08\u9002\u5408LLM\u63d0\u4f9b\u5546\u4f46\u9762\u4e34\u5f00\u6e90\u6311\u6218\uff09\u3001LLM\u6587\u672c\u6c34\u5370\uff08\u5728\u53cd\u6ee5\u7528\u573a\u666f\u4e2d\u6548\u76ca\u6709\u9650\uff09\u3001\u4e0a\u4e0b\u6587\u6c34\u5370(ICW)\uff08\u4e3a\u53ef\u4fe1\u65b9\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5d4c\u5165\u6c34\u5370\u6307\u4ee4\u68c0\u6d4b\u6ee5\u7528\uff09\u3002", "result": "ICW\u65b9\u6cd5\u80fd\u5b9e\u73b0\u6fc0\u52b1\u5bf9\u9f50\uff1a\u7528\u6237\u65e0\u8d28\u91cf\u635f\u5931\uff0c\u53ef\u4fe1\u65b9\u83b7\u5f97\u68c0\u6d4b\u5de5\u5177\uff0cLLM\u63d0\u4f9b\u5546\u4fdd\u6301\u4e2d\u7acb\u3002\u8be5\u65b9\u6cd5\u5728\u4f1a\u8bae\u8bc4\u5ba1\u3001\u6559\u80b2\u7b49\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\u3002", "conclusion": "LLM\u6c34\u5370\u7684\u5b9e\u9645\u91c7\u7528\u9700\u8981\u5728\u76ee\u6807\u5e94\u7528\u9886\u57df\u5bf9\u9f50\u5229\u76ca\u76f8\u5173\u8005\u6fc0\u52b1\uff0c\u4fc3\u8fdb\u793e\u533a\u53c2\u4e0e\uff0c\u5e76\u63a2\u7d22\u66f4\u591a\u6fc0\u52b1\u5bf9\u9f50\u7684\u9886\u57df\u7279\u5b9a\u6c34\u5370\u65b9\u6cd5\u3002"}}
{"id": "2510.18394", "categories": ["cs.CR", "cs.IR", "cs.NI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.18394", "abs": "https://arxiv.org/abs/2510.18394", "authors": ["Yong Zhang", "Nishanth Sastry"], "title": "Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet", "comment": "15 pages, 2 figures", "summary": "Undoubtedly, the Internet has become one of the most important conduits to\ninformation for the general public. Nonetheless, Internet access can be and has\nbeen limited systematically or blocked completely during political events in\nnumerous countries and regions by various censorship mechanisms. Depending on\nwhere the core filtering component is situated, censorship techniques have been\nclassified as client-based, server-based, or network-based. However, as the\nInternet evolves rapidly, new and sophisticated censorship techniques have\nemerged, which involve techniques that cut across locations and involve new\nforms of hurdles to information access. We argue that modern censorship can be\nbetter understood through a new lens that we term chokepoints, which identifies\nbottlenecks in the content production or delivery cycle where efficient new\nforms of large-scale client-side surveillance and filtering mechanisms have\nemerged.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.18212", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18212", "abs": "https://arxiv.org/abs/2510.18212", "authors": ["Dan Hendrycks", "Dawn Song", "Christian Szegedy", "Honglak Lee", "Yarin Gal", "Erik Brynjolfsson", "Sharon Li", "Andy Zou", "Lionel Levine", "Bo Han", "Jie Fu", "Ziwei Liu", "Jinwoo Shin", "Kimin Lee", "Mantas Mazeika", "Long Phan", "George Ingebretsen", "Adam Khoja", "Cihang Xie", "Olawale Salaudeen", "Matthias Hein", "Kevin Zhao", "Alexander Pan", "David Duvenaud", "Bo Li", "Steve Omohundro", "Gabriel Alfour", "Max Tegmark", "Kevin McGrew", "Gary Marcus", "Jaan Tallinn", "Eric Schmidt", "Yoshua Bengio"], "title": "A Definition of AGI", "comment": null, "summary": "The lack of a concrete definition for Artificial General Intelligence (AGI)\nobscures the gap between today's specialized AI and human-level cognition. This\npaper introduces a quantifiable framework to address this, defining AGI as\nmatching the cognitive versatility and proficiency of a well-educated adult. To\noperationalize this, we ground our methodology in Cattell-Horn-Carroll theory,\nthe most empirically validated model of human cognition. The framework dissects\ngeneral intelligence into ten core cognitive domains-including reasoning,\nmemory, and perception-and adapts established human psychometric batteries to\nevaluate AI systems. Application of this framework reveals a highly \"jagged\"\ncognitive profile in contemporary models. While proficient in\nknowledge-intensive domains, current AI systems have critical deficits in\nfoundational cognitive machinery, particularly long-term memory storage. The\nresulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify\nboth rapid progress and the substantial gap remaining before AGI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u91cf\u5316\u7684AGI\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8eCattell-Horn-Carroll\u4eba\u7c7b\u8ba4\u77e5\u7406\u8bba\uff0c\u5c06\u901a\u7528\u667a\u80fd\u5206\u89e3\u4e3a10\u4e2a\u6838\u5fc3\u8ba4\u77e5\u9886\u57df\uff0c\u5e76\u5e94\u7528\u4eba\u7c7b\u5fc3\u7406\u6d4b\u91cf\u5de5\u5177\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u5177\u4f53\u5b9a\u4e49\uff0c\u6a21\u7cca\u4e86\u5f53\u4eca\u4e13\u4e1aAI\u4e0e\u4eba\u7c7b\u6c34\u5e73\u8ba4\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u5efa\u7acb\u91cf\u5316\u6846\u67b6\u6765\u660e\u786e\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8eCattell-Horn-Carroll\u7406\u8bba\uff08\u7ecf\u9a8c\u9a8c\u8bc1\u6700\u5145\u5206\u7684\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\uff09\uff0c\u5c06\u901a\u7528\u667a\u80fd\u5206\u89e3\u4e3a10\u4e2a\u6838\u5fc3\u8ba4\u77e5\u9886\u57df\uff08\u5305\u62ec\u63a8\u7406\u3001\u8bb0\u5fc6\u3001\u611f\u77e5\u7b49\uff09\uff0c\u5e76\u91c7\u7528\u6210\u719f\u7684\u4eba\u7c7b\u5fc3\u7406\u6d4b\u91cf\u5de5\u5177\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u3002", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u53d1\u73b0\u5f53\u4ee3AI\u6a21\u578b\u5177\u6709\u9ad8\u5ea6\"\u952f\u9f7f\u72b6\"\u7684\u8ba4\u77e5\u7279\u5f81\uff1a\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u8868\u73b0\u719f\u7ec3\uff0c\u4f46\u5728\u57fa\u7840\u8ba4\u77e5\u673a\u5236\uff08\u7279\u522b\u662f\u957f\u671f\u8bb0\u5fc6\u5b58\u50a8\uff09\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\u3002GPT-4\u7684AGI\u5f97\u5206\u4e3a27%\uff0cGPT-5\u4e3a58%\u3002", "conclusion": "\u8be5\u6846\u67b6\u91cf\u5316\u4e86AI\u5411AGI\u7684\u5feb\u901f\u8fdb\u5c55\u548c\u5269\u4f59\u7684\u5de8\u5927\u5dee\u8ddd\uff0c\u4e3aAGI\u53d1\u5c55\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u65b9\u5411\u3002"}}
{"id": "2510.18438", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18438", "abs": "https://arxiv.org/abs/2510.18438", "authors": ["Yixuan Liu", "Xinlei Li", "Yi Li"], "title": "DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning", "comment": "Accepted to ASE'25", "summary": "Phishing attacks in Web3 ecosystems are increasingly sophisticated,\nexploiting deceptive contract logic, malicious frontend scripts, and token\napproval patterns. We present DeepTx, a real-time transaction analysis system\nthat detects such threats before user confirmation. DeepTx simulates pending\ntransactions, extracts behavior, context, and UI features, and uses multiple\nlarge language models (LLMs) to reason about transaction intent. A consensus\nmechanism with self-reflection ensures robust and explainable decisions.\nEvaluated on our phishing dataset, DeepTx achieves high precision and recall\n(demo video: https://youtu.be/4OfK9KCEXUM).", "AI": {"tldr": "DeepTx\u662f\u4e00\u4e2a\u5b9e\u65f6\u4ea4\u6613\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u5f85\u5904\u7406\u4ea4\u6613\u3001\u63d0\u53d6\u884c\u4e3a\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u591aLLM\u63a8\u7406\u4ea4\u6613\u610f\u56fe\u6765\u68c0\u6d4bWeb3\u9493\u9c7c\u653b\u51fb\u3002", "motivation": "Web3\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u9493\u9c7c\u653b\u51fb\u65e5\u76ca\u590d\u6742\uff0c\u5229\u7528\u6b3a\u9a97\u6027\u5408\u7ea6\u903b\u8f91\u3001\u6076\u610f\u524d\u7aef\u811a\u672c\u548c\u4ee3\u5e01\u6388\u6743\u6a21\u5f0f\uff0c\u9700\u8981\u5b9e\u65f6\u68c0\u6d4b\u7cfb\u7edf\u6765\u4fdd\u62a4\u7528\u6237\u3002", "method": "\u7cfb\u7edf\u6a21\u62df\u5f85\u5904\u7406\u4ea4\u6613\uff0c\u63d0\u53d6\u884c\u4e3a\u3001\u4e0a\u4e0b\u6587\u548cUI\u7279\u5f81\uff0c\u4f7f\u7528\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4ea4\u6613\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u81ea\u53cd\u601d\u7684\u5171\u8bc6\u673a\u5236\u786e\u4fdd\u51b3\u7b56\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u9493\u9c7c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDeepTx\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u53ec\u56de\u7387\u3002", "conclusion": "DeepTx\u80fd\u591f\u6709\u6548\u68c0\u6d4bWeb3\u9493\u9c7c\u5a01\u80c1\uff0c\u5728\u7528\u6237\u786e\u8ba4\u4ea4\u6613\u524d\u63d0\u4f9b\u4fdd\u62a4\u3002"}}
{"id": "2510.18250", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18250", "abs": "https://arxiv.org/abs/2510.18250", "authors": ["Xiaohan Qin", "Xiaoxing Wang", "Ning Liao", "Cancheng Zhang", "Xiangdong Zhang", "Mingquan Feng", "Jingzhi Wang", "Junchi Yan"], "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning", "comment": null, "summary": "Data quality plays a critical role in enhancing supervised fine-tuning (SFT)\nfor large language models (LLMs), and token-level data selection has emerged as\na promising direction for its fine-grained nature. Despite their strong\nempirical performance, existing token-level selection methods share two key\nlimitations: (1) requiring training or accessing an additional reference model,\nand (2) relying solely on loss information for token selection, which cannot\nwell preserve semantically important tokens that are not favored by loss-based\nmetrics. To address these challenges, we propose ssToken, a Self-modulated and\nSemantic-aware Token Selection approach. ssToken leverages readily accessible\nhistory models to compute the per-token loss difference with the current model,\nwhich serves as a self-modulated signal that enables the model to adaptively\nselect tokens along its optimization trajectory, rather than relying on excess\nloss from an offline-trained reference model as in prior works. We further\nintroduce a semantic-aware, attention-based token importance estimation metric,\northogonal to loss-based selection and providing complementary semantic\ninformation for more effective filtering. Extensive experiments across\ndifferent model families and scales demonstrate that both self-modulated\nselection and semantic-aware selection alone outperform full-data fine-tuning,\nwhile their integration--ssToken--achieves synergistic gains and further\nsurpasses prior token-level selection methods, delivering performance\nimprovements while maintaining training efficiency.", "AI": {"tldr": "\u63d0\u51fassToken\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u8c03\u8282\u635f\u5931\u5dee\u5f02\u548c\u8bed\u4e49\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884ctoken\u7ea7\u6570\u636e\u9009\u62e9\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u76d1\u7763\u5fae\u8c03\u6548\u679c\uff0c\u65e0\u9700\u989d\u5916\u53c2\u8003\u6a21\u578b", "motivation": "\u73b0\u6709token\u7ea7\u9009\u62e9\u65b9\u6cd5\u9700\u8981\u989d\u5916\u53c2\u8003\u6a21\u578b\u4e14\u4ec5\u4f9d\u8d56\u635f\u5931\u4fe1\u606f\uff0c\u65e0\u6cd5\u5f88\u597d\u4fdd\u7559\u8bed\u4e49\u91cd\u8981\u4f46\u635f\u5931\u6307\u6807\u4e0d\u504f\u597d\u7684token", "method": "\u4f7f\u7528\u5386\u53f2\u6a21\u578b\u8ba1\u7b97token\u635f\u5931\u5dee\u5f02\u4f5c\u4e3a\u81ea\u8c03\u8282\u4fe1\u53f7\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u8bed\u4e49\u91cd\u8981\u6027\u8bc4\u4f30\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94token\u9009\u62e9", "result": "\u81ea\u8c03\u8282\u9009\u62e9\u548c\u8bed\u4e49\u611f\u77e5\u9009\u62e9\u5355\u72ec\u4f7f\u7528\u5747\u4f18\u4e8e\u5168\u6570\u636e\u5fae\u8c03\uff0c\u4e24\u8005\u7ed3\u5408\u7684ssToken\u65b9\u6cd5\u5b9e\u73b0\u534f\u540c\u589e\u76ca\uff0c\u8d85\u8d8a\u73b0\u6709token\u7ea7\u9009\u62e9\u65b9\u6cd5", "conclusion": "ssToken\u901a\u8fc7\u81ea\u8c03\u8282\u548c\u8bed\u4e49\u611f\u77e5token\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u6570\u636e\u8d28\u91cf\u4f18\u5316\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.18254", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18254", "abs": "https://arxiv.org/abs/2510.18254", "authors": ["Sion Weatherhead", "Flora Salim", "Aaron Belbasis"], "title": "Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning", "comment": null, "summary": "Humans do not just find mistakes after the fact -- we often catch them\nmid-stream because 'reflection' is tied to the goal and its constraints.\nToday's large language models produce reasoning tokens and 'reflective' text,\nbut is it functionally equivalent with human reflective reasoning? Prior work\non closed-ended tasks -- with clear, external 'correctness' signals -- can make\n'reflection' look effective while masking limits in self-correction. We\ntherefore test eight frontier models on a simple, real-world task that is\nopen-ended yet rule-constrained, with auditable success criteria: to produce\nvalid scientific test items, then revise after considering their own critique.\nFirst-pass performance is poor (often zero valid items out of 4 required; mean\n$\\approx$ 1), and reflection yields only modest gains (also $\\approx$ 1).\nCrucially, the second attempt frequently repeats the same violation of\nconstraint, indicating 'corrective gains' arise largely from chance production\nof a valid item rather than error detection and principled,\nconstraint-sensitive repair. Performance before and after reflection\ndeteriorates as open-endedness increases, and models marketed for 'reasoning'\nshow no advantage. Our results suggest that current LLM 'reflection' lacks\nfunctional evidence of the active, goal-driven monitoring that helps humans\nrespect constraints even on a first pass. Until such mechanisms are\ninstantiated in the model itself, reliable performance requires external\nstructure that enforces constraints.", "AI": {"tldr": "\u672c\u6587\u6d4b\u8bd5\u4e86\u516b\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4f46\u89c4\u5219\u7ea6\u675f\u4efb\u52a1\u4e2d\u7684\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7b2c\u4e00\u6b21\u5c1d\u8bd5\u65f6\u8868\u73b0\u5f88\u5dee\uff0c\u53cd\u601d\u540e\u53ea\u6709\u8f7b\u5fae\u6539\u5584\uff0c\u4e14\u7ecf\u5e38\u91cd\u590d\u76f8\u540c\u7684\u7ea6\u675f\u8fdd\u89c4\uff0c\u8868\u660e\u6240\u8c13\u7684'\u4fee\u6b63\u6536\u76ca'\u4e3b\u8981\u6765\u81ea\u5076\u7136\u4ea7\u751f\u7684\u6709\u6548\u9879\u76ee\u800c\u975e\u771f\u6b63\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u539f\u5219\u6027\u4fee\u590d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u68c0\u9a8c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684'\u53cd\u601d'\u529f\u80fd\u662f\u5426\u4e0e\u4eba\u7c7b\u53cd\u601d\u63a8\u7406\u5728\u529f\u80fd\u4e0a\u7b49\u6548\u3002\u5148\u524d\u5728\u5c01\u95ed\u5f0f\u4efb\u52a1\u4e0a\u7684\u7814\u7a76\u53ef\u80fd\u63a9\u76d6\u4e86\u6a21\u578b\u5728\u81ea\u6211\u4fee\u6b63\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5728\u5f00\u653e\u5f0f\u4f46\u89c4\u5219\u7ea6\u675f\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "method": "\u6d4b\u8bd5\u516b\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u4e00\u4e2a\u7b80\u5355\u4f46\u771f\u5b9e\u7684\u4efb\u52a1\u4e0a\uff1a\u751f\u6210\u6709\u6548\u7684\u79d1\u5b66\u6d4b\u8bd5\u9879\u76ee\uff0c\u7136\u540e\u5728\u8003\u8651\u81ea\u5df1\u7684\u6279\u8bc4\u540e\u8fdb\u884c\u4fee\u8ba2\u3002\u4efb\u52a1\u5177\u6709\u53ef\u5ba1\u8ba1\u7684\u6210\u529f\u6807\u51c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u7b2c\u4e00\u6b21\u5c1d\u8bd5\u548c\u53cd\u601d\u540e\u7684\u8868\u73b0\u3002", "result": "\u7b2c\u4e00\u6b21\u5c1d\u8bd5\u8868\u73b0\u5f88\u5dee\uff08\u901a\u5e384\u4e2a\u8981\u6c42\u9879\u76ee\u4e2d\u96f6\u6709\u6548\u9879\u76ee\uff0c\u5e73\u5747\u7ea61\u4e2a\uff09\uff0c\u53cd\u601d\u540e\u53ea\u6709\u8f7b\u5fae\u6539\u5584\uff08\u4e5f\u7ea61\u4e2a\uff09\u3002\u5173\u952e\u53d1\u73b0\u662f\u7b2c\u4e8c\u6b21\u5c1d\u8bd5\u7ecf\u5e38\u91cd\u590d\u76f8\u540c\u7684\u7ea6\u675f\u8fdd\u89c4\uff0c\u8868\u660e'\u4fee\u6b63\u6536\u76ca'\u4e3b\u8981\u6765\u81ea\u5076\u7136\u4ea7\u751f\u7684\u6709\u6548\u9879\u76ee\u800c\u975e\u771f\u6b63\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u539f\u5219\u6027\u4fee\u590d\u3002\u968f\u7740\u5f00\u653e\u6027\u589e\u52a0\uff0c\u53cd\u601d\u524d\u540e\u7684\u8868\u73b0\u90fd\u4f1a\u6076\u5316\uff0c\u6807\u699c'\u63a8\u7406'\u80fd\u529b\u7684\u6a21\u578b\u6ca1\u6709\u4f18\u52bf\u3002", "conclusion": "\u5f53\u524dLLM\u7684'\u53cd\u601d'\u7f3a\u4e4f\u529f\u80fd\u6027\u8bc1\u636e\u8868\u660e\u5b58\u5728\u4e3b\u52a8\u7684\u3001\u76ee\u6807\u9a71\u52a8\u7684\u76d1\u63a7\u673a\u5236\uff0c\u8fd9\u79cd\u673a\u5236\u5e2e\u52a9\u4eba\u7c7b\u5373\u4f7f\u5728\u7b2c\u4e00\u6b21\u5c1d\u8bd5\u65f6\u4e5f\u80fd\u5c0a\u91cd\u7ea6\u675f\u3002\u5728\u6a21\u578b\u672c\u8eab\u5b9e\u4f8b\u5316\u8fd9\u79cd\u673a\u5236\u4e4b\u524d\uff0c\u53ef\u9760\u6027\u80fd\u9700\u8981\u5f3a\u5236\u6267\u884c\u7ea6\u675f\u7684\u5916\u90e8\u7ed3\u6784\u3002"}}
{"id": "2510.18493", "categories": ["cs.CR", "cs.AI", "cs.HC", "68M25", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.18493", "abs": "https://arxiv.org/abs/2510.18493", "authors": ["Kangzhong Wang", "Zitong Shen", "Youqian Zhang", "Michael MK Cheung", "Xiapu Luo", "Grace Ngai", "Eugene Yujun Fu"], "title": "One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection", "comment": "9 pages", "summary": "Phone scams remain a pervasive threat to both personal safety and financial\nsecurity worldwide. Recent advances in large language models (LLMs) have\ndemonstrated strong potential in detecting fraudulent behavior by analyzing\ntranscribed phone conversations. However, these capabilities introduce notable\nprivacy risks, as such conversations frequently contain sensitive personal\ninformation that may be exposed to third-party service providers during\nprocessing. In this work, we explore how to harness LLMs for phone scam\ndetection while preserving user privacy. We propose MASK (Modular Adaptive\nSanitization Kit), a trainable and extensible framework that enables dynamic\nprivacy adjustment based on individual preferences. MASK provides a pluggable\narchitecture that accommodates diverse sanitization methods - from traditional\nkeyword-based techniques for high-privacy users to sophisticated neural\napproaches for those prioritizing accuracy. We also discuss potential modeling\napproaches and loss function designs for future development, enabling the\ncreation of truly personalized, privacy-aware LLM-based detection systems that\nbalance user trust and detection effectiveness, even beyond phone scam context.", "AI": {"tldr": "\u63d0\u51fa\u4e86MASK\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u7535\u8bdd\u8bc8\u9a97\u7684\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u652f\u6301\u57fa\u4e8e\u4e2a\u4eba\u504f\u597d\u7684\u52a8\u6001\u9690\u79c1\u8c03\u6574\u548c\u591a\u79cd\u8131\u654f\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u7535\u8bdd\u8bc8\u9a97\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u56e0\u4e3a\u901a\u8bdd\u8bb0\u5f55\u5305\u542b\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u53ef\u80fd\u5728\u5904\u7406\u8fc7\u7a0b\u4e2d\u66b4\u9732\u7ed9\u7b2c\u4e09\u65b9\u670d\u52a1\u63d0\u4f9b\u5546\u3002", "method": "\u63d0\u51faMASK\uff08\u6a21\u5757\u5316\u81ea\u9002\u5e94\u8131\u654f\u5de5\u5177\u5305\uff09\u6846\u67b6\uff0c\u63d0\u4f9b\u53ef\u63d2\u62d4\u67b6\u6784\u652f\u6301\u591a\u79cd\u8131\u654f\u65b9\u6cd5\uff0c\u4ece\u4f20\u7edf\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u6280\u672f\u5230\u590d\u6742\u7684\u795e\u7ecf\u65b9\u6cd5\uff0c\u652f\u6301\u57fa\u4e8e\u7528\u6237\u504f\u597d\u7684\u52a8\u6001\u9690\u79c1\u8c03\u6574\u3002", "result": "MASK\u6846\u67b6\u80fd\u591f\u5e73\u8861\u7528\u6237\u4fe1\u4efb\u548c\u68c0\u6d4b\u6548\u679c\uff0c\u652f\u6301\u521b\u5efa\u771f\u6b63\u4e2a\u6027\u5316\u7684\u9690\u79c1\u611f\u77e5\u68c0\u6d4b\u7cfb\u7edf\u3002", "conclusion": "MASK\u6846\u67b6\u4e3a\u6784\u5efa\u9690\u79c1\u4fdd\u62a4\u7684\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u7535\u8bdd\u8bc8\u9a97\u68c0\u6d4b\u4e4b\u5916\u7684\u573a\u666f\u4e5f\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.18508", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18508", "abs": "https://arxiv.org/abs/2510.18508", "authors": ["Osama Al Haddad", "Muhammad Ikram", "Ejaz Ahmed", "Young Lee"], "title": "Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization", "comment": "19 pages, 8 figures", "summary": "Security analysts face increasing pressure to triage large and complex\nvulnerability backlogs. Large Language Models (LLMs) offer a potential aid by\nautomating parts of the interpretation process. We evaluate four models\n(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to\ninterpret semi-structured and unstructured vulnerability information. As a\nconcrete use case, we test each model's ability to predict decision points in\nthe Stakeholder-Specific Vulnerability Categorization (SSVC) framework:\nExploitation, Automatable, Technical Impact, and Mission and Wellbeing.\n  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more\nthan 165,000 queries to assess performance under prompting styles including\none-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC\ndecision point and Cohen's kappa (weighted and unweighted) for the final SSVC\ndecision outcomes. Gemini consistently ranked highest, leading on three of four\ndecision points and yielding the most correct recommendations. Prompting with\nexemplars generally improved accuracy, although all models struggled on some\ndecision points. Only DeepSeek achieved fair agreement under weighted metrics,\nand all models tended to over-predict risk.\n  Overall, current LLMs do not replace expert judgment. However, specific LLM\nand prompt combinations show moderate effectiveness for targeted SSVC\ndecisions. When applied with care, LLMs can support vulnerability\nprioritization workflows and help security teams respond more efficiently to\nemerging threats.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f0f\u6d1e\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Gemini\u6a21\u578b\u5728SSVC\u6846\u67b6\u7684\u51b3\u7b56\u70b9\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4e13\u5bb6\u5224\u65ad\u3002", "motivation": "\u5b89\u5168\u5206\u6790\u5e08\u9762\u4e34\u5904\u7406\u5927\u91cf\u590d\u6742\u6f0f\u6d1e\u7684\u538b\u529b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u8f85\u52a9\u89e3\u91ca\u6f0f\u6d1e\u4fe1\u606f\uff0c\u63d0\u9ad8\u6f0f\u6d1e\u5206\u7c7b\u6548\u7387\u3002", "method": "\u4f7f\u7528384\u4e2a\u771f\u5b9e\u6f0f\u6d1e\u6570\u636e\uff0c\u5bf9ChatGPT\u3001Claude\u3001Gemini\u548cDeepSeek\u56db\u79cd\u6a21\u578b\u8fdb\u884c\u8d85\u8fc7165,000\u6b21\u67e5\u8be2\u6d4b\u8bd5\uff0c\u91c7\u7528\u5305\u62econe-shot\u3001few-shot\u548cchain-of-thought\u5728\u5185\u768412\u79cd\u63d0\u793a\u6280\u672f\u3002", "result": "Gemini\u5728\u56db\u4e2a\u51b3\u7b56\u70b9\u4e2d\u7684\u4e09\u4e2a\u8868\u73b0\u6700\u4f73\uff0c\u63d0\u793a\u793a\u4f8b\u901a\u5e38\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u67d0\u4e9b\u51b3\u7b56\u70b9\u4e0a\u4ecd\u6709\u56f0\u96be\u3002\u53ea\u6709DeepSeek\u5728\u52a0\u6743\u6307\u6807\u4e0b\u8fbe\u5230\u516c\u5e73\u4e00\u81f4\u6027\uff0c\u6240\u6709\u6a21\u578b\u90fd\u503e\u5411\u4e8e\u8fc7\u5ea6\u9884\u6d4b\u98ce\u9669\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u80fd\u66ff\u4ee3\u4e13\u5bb6\u5224\u65ad\uff0c\u4f46\u7279\u5b9a\u6a21\u578b\u548c\u63d0\u793a\u7ec4\u5408\u5728\u9488\u5bf9\u6027SSVC\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u4e2d\u7b49\u6709\u6548\u6027\uff0c\u53ef\u8c28\u614e\u5e94\u7528\u4e8e\u6f0f\u6d1e\u4f18\u5148\u7ea7\u6392\u5e8f\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2510.18342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18342", "abs": "https://arxiv.org/abs/2510.18342", "authors": ["Peng Tang", "Xiaoxiao Yan", "Xiaobin Hu", "Yuning Cui", "Donghao Luo", "Jiangning Zhang", "Pengcheng Xu", "Jinlong Peng", "Qingdong He", "Feiyue Huang", "Song Xue", "Tobias Lasser"], "title": "ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection", "comment": "Under Review", "summary": "Multi-class unsupervised anomaly detection (MUAD) has garnered growing\nresearch interest, as it seeks to develop a unified model for anomaly detection\nacross multiple classes, i.e., eliminating the need to train separate models\nfor distinct objects and thereby saving substantial computational resources.\nUnder the MUAD setting, while advanced Transformer-based architectures have\nbrought significant performance improvements, identity shortcuts persist: they\ndirectly copy inputs to outputs, narrowing the gap in reconstruction errors\nbetween normal and abnormal cases, and thereby making the two harder to\ndistinguish. Therefore, we propose ShortcutBreaker, a novel unified\nfeature-reconstruction framework for MUAD tasks, featuring two key innovations\nto address the issue of shortcuts. First, drawing on matrix rank inequality, we\ndesign a low-rank noisy bottleneck (LRNB) to project highdimensional features\ninto a low-rank latent space, and theoretically demonstrate its capacity to\nprevent trivial identity reproduction. Second, leveraging ViTs global modeling\ncapability instead of merely focusing on local features, we incorporate a\nglobal perturbation attention to prevent information shortcuts in the decoders.\nExtensive experiments are performed on four widely used anomaly detection\nbenchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)\nand one medical dataset (Universal Medical). The proposed method achieves a\nremarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four\ndatasets, respectively, consistently outperforming previous MUAD methods across\ndifferent scenarios.", "AI": {"tldr": "\u63d0\u51faShortcutBreaker\u6846\u67b6\u89e3\u51b3\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u901a\u8fc7\u4f4e\u79e9\u566a\u58f0\u74f6\u9888\u548c\u5168\u5c40\u6270\u52a8\u6ce8\u610f\u529b\u673a\u5236\u9632\u6b62\u7279\u5f81\u76f4\u63a5\u590d\u5236\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u9700\u8981\u7edf\u4e00\u6a21\u578b\u68c0\u6d4b\u591a\u7c7b\u5f02\u5e38\uff0c\u4f46\u73b0\u6709Transformer\u67b6\u6784\u5b58\u5728\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u5373\u76f4\u63a5\u590d\u5236\u8f93\u5165\u5230\u8f93\u51fa\uff0c\u5bfc\u81f4\u6b63\u5e38\u4e0e\u5f02\u5e38\u6837\u672c\u7684\u91cd\u6784\u8bef\u5dee\u5dee\u5f02\u53d8\u5c0f\uff0c\u96be\u4ee5\u533a\u5206\u3002", "method": "\u63d0\u51faShortcutBreaker\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u77e9\u9635\u79e9\u4e0d\u7b49\u5f0f\u8bbe\u8ba1\u4f4e\u79e9\u566a\u58f0\u74f6\u9888\uff0c\u5c06\u9ad8\u7ef4\u7279\u5f81\u6295\u5f71\u5230\u4f4e\u79e9\u6f5c\u5728\u7a7a\u95f4\u9632\u6b62\u8eab\u4efd\u590d\u5236\uff1b2\uff09\u5229\u7528ViT\u5168\u5c40\u5efa\u6a21\u80fd\u529b\u5f15\u5165\u5168\u5c40\u6270\u52a8\u6ce8\u610f\u529b\u9632\u6b62\u89e3\u7801\u5668\u4e2d\u7684\u4fe1\u606f\u6377\u5f84\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u4e09\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6MVTec-AD\u3001ViSA\u3001Real-IAD\u548c\u4e00\u4e2a\u533b\u5b66\u6570\u636e\u96c6Universal Medical\uff09\u4e0a\u5206\u522b\u8fbe\u523099.8%\u300198.9%\u300190.6%\u548c87.8%\u7684\u56fe\u50cf\u7ea7AUROC\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ShortcutBreaker\u901a\u8fc7\u89e3\u51b3\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u5728\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2510.18563", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18563", "abs": "https://arxiv.org/abs/2510.18563", "authors": ["Zijie Xu", "Minfeng Qi", "Shiqing Wu", "Lefeng Zhang", "Qiwen Wei", "Han He", "Ningran Li"], "title": "The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability", "comment": null, "summary": "Multi-agent systems powered by large language models are advancing rapidly,\nyet the tension between mutual trust and security remains underexplored. We\nintroduce and empirically validate the Trust-Vulnerability Paradox (TVP):\nincreasing inter-agent trust to enhance coordination simultaneously expands\nrisks of over-exposure and over-authorization. To investigate this paradox, we\nconstruct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,\nand run extensive closed-loop interactions with trust explicitly parameterized.\nUsing Minimum Necessary Information (MNI) as the safety baseline, we propose\ntwo unified metrics: Over-Exposure Rate (OER) to detect boundary violations,\nand Authorization Drift (AD) to capture sensitivity to trust levels. Results\nacross multiple model backends and orchestration frameworks reveal consistent\ntrends: higher trust improves task success but also heightens exposure risks,\nwith heterogeneous trust-to-risk mappings across systems. We further examine\ndefenses such as Sensitive Information Repartitioning and Guardian-Agent\nenablement, both of which reduce OER and attenuate AD. Overall, this study\nformalizes TVP, establishes reproducible baselines with unified metrics, and\ndemonstrates that trust must be modeled and scheduled as a first-class security\nvariable in multi-agent system design.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u4fe1\u4efb-\u8106\u5f31\u6027\u6096\u8bba\uff1a\u589e\u52a0\u667a\u80fd\u4f53\u95f4\u4fe1\u4efb\u53ef\u63d0\u5347\u534f\u4f5c\u6548\u7387\uff0c\u4f46\u540c\u65f6\u4e5f\u589e\u52a0\u4e86\u8fc7\u5ea6\u66b4\u9732\u548c\u8fc7\u5ea6\u6388\u6743\u7684\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u76f8\u4e92\u4fe1\u4efb\u4e0e\u5b89\u5168\u4e4b\u95f4\u7684\u7d27\u5f20\u5173\u7cfb\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u6df1\u5165\u7406\u89e3\u4fe1\u4efb\u589e\u5f3a\u534f\u4f5c\u7684\u540c\u65f6\u5982\u4f55\u5e26\u6765\u5b89\u5168\u98ce\u9669\u3002", "method": "\u6784\u5efa\u5305\u542b3\u4e2a\u5b8f\u89c2\u573a\u666f\u548c19\u4e2a\u5b50\u573a\u666f\u7684\u573a\u666f-\u6e38\u620f\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5e7f\u6cdb\u7684\u95ed\u73af\u4ea4\u4e92\u5b9e\u9a8c\uff0c\u5c06\u4fe1\u4efb\u663e\u5f0f\u53c2\u6570\u5316\u3002\u4f7f\u7528\u6700\u5c0f\u5fc5\u8981\u4fe1\u606f\u4f5c\u4e3a\u5b89\u5168\u57fa\u7ebf\uff0c\u63d0\u51fa\u8fc7\u5ea6\u66b4\u9732\u7387\u548c\u6388\u6743\u6f02\u79fb\u4e24\u4e2a\u7edf\u4e00\u6307\u6807\u6765\u8bc4\u4f30\u98ce\u9669\u3002", "result": "\u591a\u4e2a\u6a21\u578b\u540e\u7aef\u548c\u7f16\u6392\u6846\u67b6\u7684\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u663e\u793a\uff1a\u66f4\u9ad8\u7684\u4fe1\u4efb\u5ea6\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u66b4\u9732\u98ce\u9669\uff0c\u4e0d\u540c\u7cfb\u7edf\u8868\u73b0\u51fa\u5f02\u8d28\u7684\u4fe1\u4efb-\u98ce\u9669\u6620\u5c04\u5173\u7cfb\u3002\u654f\u611f\u4fe1\u606f\u91cd\u65b0\u5206\u533a\u548c\u5b88\u62a4\u667a\u80fd\u4f53\u542f\u7528\u7b49\u9632\u5fa1\u63aa\u65bd\u80fd\u6709\u6548\u964d\u4f4e\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u5f62\u5f0f\u5316\u4e86\u4fe1\u4efb-\u8106\u5f31\u6027\u6096\u8bba\uff0c\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u548c\u7edf\u4e00\u6307\u6807\uff0c\u8bc1\u660e\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\uff0c\u4fe1\u4efb\u5fc5\u987b\u4f5c\u4e3a\u9996\u8981\u5b89\u5168\u53d8\u91cf\u8fdb\u884c\u5efa\u6a21\u548c\u8c03\u5ea6\u3002"}}
{"id": "2510.18407", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18407", "abs": "https://arxiv.org/abs/2510.18407", "authors": ["Manjie Xu", "Xinyi Yang", "Jiayu Zhan", "Wei Liang", "Chi Zhang", "Yixin Zhu"], "title": "Heterogeneous Adversarial Play in Interactive Environments", "comment": "NeurIPS 2025", "summary": "Self-play constitutes a fundamental paradigm for autonomous skill\nacquisition, whereby agents iteratively enhance their capabilities through\nself-directed environmental exploration. Conventional self-play frameworks\nexploit agent symmetry within zero-sum competitive settings, yet this approach\nproves inadequate for open-ended learning scenarios characterized by inherent\nasymmetry. Human pedagogical systems exemplify asymmetric instructional\nframeworks wherein educators systematically construct challenges calibrated to\nindividual learners' developmental trajectories. The principal challenge\nresides in operationalizing these asymmetric, adaptive pedagogical mechanisms\nwithin artificial systems capable of autonomously synthesizing appropriate\ncurricula without predetermined task hierarchies. Here we present Heterogeneous\nAdversarial Play (HAP), an adversarial Automatic Curriculum Learning framework\nthat formalizes teacher-student interactions as a minimax optimization wherein\ntask-generating instructor and problem-solving learner co-evolve through\nadversarial dynamics. In contrast to prevailing ACL methodologies that employ\nstatic curricula or unidirectional task selection mechanisms, HAP establishes a\nbidirectional feedback system wherein instructors continuously recalibrate task\ncomplexity in response to real-time learner performance metrics. Experimental\nvalidation across multi-task learning domains demonstrates that our framework\nachieves performance parity with SOTA baselines while generating curricula that\nenhance learning efficacy in both artificial agents and human subjects.", "AI": {"tldr": "\u63d0\u51faHAP\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u6559\u5e08-\u5b66\u751f\u534f\u540c\u8fdb\u5316\uff0c\u89e3\u51b3\u4f20\u7edf\u81ea\u535a\u5f08\u5728\u975e\u5bf9\u79f0\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u535a\u5f08\u65b9\u6cd5\u4f9d\u8d56\u667a\u80fd\u4f53\u5bf9\u79f0\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u5f00\u653e\u5f0f\u7684\u975e\u5bf9\u79f0\u5b66\u4e60\u573a\u666f\u3002\u4eba\u7c7b\u6559\u5b66\u7cfb\u7edf\u5c55\u793a\u4e86\u975e\u5bf9\u79f0\u6559\u5b66\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4f46\u5982\u4f55\u5728\u4eba\u5de5\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u8bfe\u7a0b\u751f\u6210\u4ecd\u5177\u6311\u6218\u3002", "method": "HAP\u6846\u67b6\u5c06\u5e08\u751f\u4e92\u52a8\u5f62\u5f0f\u5316\u4e3a\u6781\u5c0f\u6781\u5927\u4f18\u5316\u95ee\u9898\uff0c\u6559\u5e08\u4f5c\u4e3a\u4efb\u52a1\u751f\u6210\u8005\u4e0e\u5b66\u751f\u4f5c\u4e3a\u95ee\u9898\u89e3\u51b3\u8005\u901a\u8fc7\u5bf9\u6297\u52a8\u6001\u5171\u540c\u8fdb\u5316\uff0c\u5efa\u7acb\u53cc\u5411\u53cd\u9988\u7cfb\u7edf\u5b9e\u65f6\u8c03\u6574\u4efb\u52a1\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u9886\u57df\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cHAP\u6846\u67b6\u8fbe\u5230\u6700\u5148\u8fdb\u57fa\u7ebf\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u751f\u6210\u7684\u8bfe\u7a0b\u80fd\u63d0\u5347\u4eba\u5de5\u667a\u80fd\u4f53\u548c\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "HAP\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u975e\u5bf9\u79f0\u5bf9\u6297\u6027\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\uff0c\u4e3a\u5f00\u653e\u5f0f\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5bf9\u6297\u6027\u5e08\u751f\u4e92\u52a8\u5728\u8bfe\u7a0b\u751f\u6210\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.18568", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18568", "abs": "https://arxiv.org/abs/2510.18568", "authors": ["Behnam Rezaei Bezanjani", "Seyyed Hamid Ghafouri", "Reza Gholamrezaei"], "title": "Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain", "comment": "30 pages", "summary": "The integration of Internet of Things (IoT) devices in healthcare has\nrevolutionized patient care by enabling real-time monitoring, personalized\ntreatments, and efficient data management. However, this technological\nadvancement introduces significant security risks, particularly concerning the\nconfidentiality, integrity, and availability of sensitive medical data.\nTraditional security measures are often insufficient to address the unique\nchallenges posed by IoT environments, such as heterogeneity, resource\nconstraints, and the need for real-time processing. To tackle these challenges,\nwe propose a comprehensive three-phase security framework designed to enhance\nthe security and reliability of IoT-enabled healthcare systems. In the first\nphase, the framework assesses the reliability of IoT devices using a\nreputation-based trust estimation mechanism, which combines device behavior\nanalytics with off-chain data storage to ensure scalability. The second phase\nintegrates blockchain technology with a lightweight proof-of-work mechanism,\nensuring data immutability, secure communication, and resistance to\nunauthorized access. The third phase employs a lightweight Long Short-Term\nMemory (LSTM) model for anomaly detection and classification, enabling\nreal-time identification of cyber threats. Simulation results demonstrate that\nthe proposed framework outperforms existing methods, achieving a 2% increase in\nprecision, accuracy, and recall, a 5% higher attack detection rate, and a 3%\nreduction in false alarm rate. These improvements highlight the framework's\nability to address critical security concerns while maintaining scalability and\nreal-time performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u8a89\u8bc4\u4f30\u3001\u533a\u5757\u94fe\u96c6\u6210\u548c\u8f7b\u91cf\u7ea7LSTM\u5f02\u5e38\u68c0\u6d4b\uff0c\u63d0\u5347\u7269\u8054\u7f51\u533b\u7597\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u5e26\u6765\u4e86\u5b9e\u65f6\u76d1\u6d4b\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u7b49\u4f18\u52bf\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4f20\u7edf\u5b89\u5168\u63aa\u65bd\u96be\u4ee5\u5e94\u5bf9\u7269\u8054\u7f51\u73af\u5883\u7684\u5f02\u6784\u6027\u3001\u8d44\u6e90\u9650\u5236\u548c\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u4fe1\u8a89\u673a\u5236\u8bc4\u4f30\u8bbe\u5907\u53ef\u9760\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u96c6\u6210\u533a\u5757\u94fe\u6280\u672f\u786e\u4fdd\u6570\u636e\u4e0d\u53ef\u7be1\u6539\u548c\u5b89\u5168\u901a\u4fe1\uff1b\u7b2c\u4e09\u9636\u6bb5\u91c7\u7528\u8f7b\u91cf\u7ea7LSTM\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u7cbe\u5ea6\u3001\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u63d0\u53472%\uff0c\u653b\u51fb\u68c0\u6d4b\u7387\u63d0\u9ad85%\uff0c\u8bef\u62a5\u7387\u964d\u4f4e3%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u5173\u952e\u5b89\u5168\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2510.18572", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.18572", "abs": "https://arxiv.org/abs/2510.18572", "authors": ["Maynard Koch", "Florian Dolzmann", "Thomas C. Schmidt", "Matthias W\u00e4hlisch"], "title": "Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks", "comment": null, "summary": "The DNS infrastructure is infamous for facilitating reflective amplification\nattacks. Various countermeasures such as server shielding, access control, rate\nlimiting, and protocol restrictions have been implemented. Still, the threat\nremains throughout the deployment of DNS servers. In this paper, we report on\nand evaluate the often unnoticed threat that derives from transparent DNS\nforwarders, a widely deployed, incompletely functional set of DNS components.\nTransparent DNS forwarders transfer DNS requests without rebuilding packets\nwith correct source addresses. As such, transparent forwarders feed DNS\nrequests into (mainly powerful and anycasted) open recursive resolvers, which\nthereby can be misused to participate unwillingly in distributed reflective\namplification attacks. We show how transparent forwarders raise severe threats\nto the Internet infrastructure. They easily circumvent rate limiting and\nachieve an additional, scalable impact via the DNS anycast infrastructure. We\nempirically verify this scaling behavior up to a factor of 14. Transparent\nforwarders can also assist in bypassing firewall rules that protect recursive\nresolvers, making these shielded infrastructure entities part of the global DNS\nattack surface.", "AI": {"tldr": "\u900f\u660eDNS\u8f6c\u53d1\u5668\u901a\u8fc7\u4e0d\u91cd\u5efa\u6570\u636e\u5305\u6e90\u5730\u5740\u7684\u65b9\u5f0f\uff0c\u5c06DNS\u8bf7\u6c42\u8f6c\u53d1\u5230\u5f00\u653e\u9012\u5f52\u89e3\u6790\u5668\uff0c\u4f7f\u5176\u88ab\u6ee5\u7528\u4e8e\u5206\u5e03\u5f0f\u53cd\u5c04\u653e\u5927\u653b\u51fb\uff0c\u7ed5\u8fc7\u901f\u7387\u9650\u5236\u5e76\u5229\u7528DNS\u4efb\u64ad\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u9ad8\u8fbe14\u500d\u7684\u653b\u51fb\u653e\u5927\u6548\u679c\u3002", "motivation": "DNS\u57fa\u7840\u8bbe\u65bd\u957f\u671f\u4ee5\u6765\u9762\u4e34\u53cd\u5c04\u653e\u5927\u653b\u51fb\u5a01\u80c1\uff0c\u5c3d\u7ba1\u5df2\u5b9e\u65bd\u591a\u79cd\u9632\u62a4\u63aa\u65bd\uff0c\u4f46\u5a01\u80c1\u4f9d\u7136\u5b58\u5728\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u900f\u660eDNS\u8f6c\u53d1\u5668\u8fd9\u4e00\u5e7f\u6cdb\u90e8\u7f72\u4f46\u529f\u80fd\u4e0d\u5b8c\u6574\u7684DNS\u7ec4\u4ef6\u6240\u5e26\u6765\u7684\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u7684\u5b89\u5168\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u5206\u6790\u900f\u660eDNS\u8f6c\u53d1\u5668\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u7814\u7a76\u5176\u5982\u4f55\u7ed5\u8fc7\u73b0\u6709\u9632\u62a4\u63aa\u65bd\uff0c\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u901a\u8fc7DNS\u4efb\u64ad\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u7684\u653b\u51fb\u653e\u5927\u6548\u679c\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u900f\u660eDNS\u8f6c\u53d1\u5668\u80fd\u591f\u7ed5\u8fc7\u901f\u7387\u9650\u5236\uff0c\u5229\u7528DNS\u4efb\u64ad\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u9ad8\u8fbe14\u500d\u7684\u653b\u51fb\u653e\u5927\u6548\u679c\uff0c\u5e76\u4f7f\u53d7\u9632\u706b\u5899\u4fdd\u62a4\u7684\u9012\u5f52\u89e3\u6790\u5668\u6210\u4e3a\u5168\u7403DNS\u653b\u51fb\u9762\u7684\u4e00\u90e8\u5206\u3002", "conclusion": "\u900f\u660eDNS\u8f6c\u53d1\u5668\u5bf9\u4e92\u8054\u7f51\u57fa\u7840\u8bbe\u65bd\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5b83\u4eec\u80fd\u591f\u6709\u6548\u89c4\u907f\u73b0\u6709\u9632\u62a4\u63aa\u65bd\uff0c\u663e\u8457\u589e\u5f3aDNS\u53cd\u5c04\u653e\u5927\u653b\u51fb\u7684\u89c4\u6a21\u548c\u5f71\u54cd\u8303\u56f4\u3002"}}
{"id": "2510.18425", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18425", "abs": "https://arxiv.org/abs/2510.18425", "authors": ["Chenxu Zhang", "Fuxiang Huang", "Lei Zhang"], "title": "Automated urban waterlogging assessment and early warning through a mixture of foundation models", "comment": "Submitted to Nature", "summary": "With climate change intensifying, urban waterlogging poses an increasingly\nsevere threat to global public safety and infrastructure. However, existing\nmonitoring approaches rely heavily on manual reporting and fail to provide\ntimely and comprehensive assessments. In this study, we present Urban\nWaterlogging Assessment (UWAssess), a foundation model-driven framework that\nautomatically identifies waterlogged areas in surveillance images and generates\nstructured assessment reports. To address the scarcity of labeled data, we\ndesign a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)\nprompting strategy to unleash the potential of the foundation model for\ndata-scarce downstream tasks. Evaluations on challenging visual benchmarks\ndemonstrate substantial improvements in perception performance. GPT-based\nevaluations confirm the ability of UWAssess to generate reliable textual\nreports that accurately describe waterlogging extent, depth, risk and impact.\nThis dual capability enables a shift of waterlogging monitoring from perception\nto generation, while the collaborative framework of multiple foundation models\nlays the groundwork for intelligent and scalable systems, supporting urban\nmanagement, disaster response and climate resilience.", "AI": {"tldr": "UWAssess\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u63a7\u56fe\u50cf\u81ea\u52a8\u8bc6\u522b\u57ce\u5e02\u5185\u6d9d\u533a\u57df\u5e76\u751f\u6210\u7ed3\u6784\u5316\u8bc4\u4f30\u62a5\u544a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u6c14\u5019\u53d8\u5316\u52a0\u5267\uff0c\u57ce\u5e02\u5185\u6d9d\u5bf9\u5168\u7403\u516c\u5171\u5b89\u5168\u548c\u57fa\u7840\u8bbe\u65bd\u7684\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u800c\u73b0\u6709\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u62a5\u544a\uff0c\u65e0\u6cd5\u63d0\u4f9b\u53ca\u65f6\u5168\u9762\u7684\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86\u534a\u76d1\u7763\u5fae\u8c03\u7b56\u7565\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7b56\u7565\uff0c\u91ca\u653e\u57fa\u7840\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5229\u7528\u591a\u57fa\u7840\u6a21\u578b\u534f\u4f5c\u6846\u67b6\u3002", "result": "\u5728\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u611f\u77e5\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cGPT\u8bc4\u4f30\u8bc1\u5b9eUWAssess\u80fd\u751f\u6210\u51c6\u786e\u63cf\u8ff0\u5185\u6d9d\u7a0b\u5ea6\u3001\u6df1\u5ea6\u3001\u98ce\u9669\u548c\u5f71\u54cd\u7684\u53ef\u9760\u6587\u672c\u62a5\u544a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece\u611f\u77e5\u5230\u751f\u6210\u7684\u5185\u6d9d\u76d1\u6d4b\u8f6c\u53d8\uff0c\u4e3a\u667a\u80fd\u53ef\u6269\u5c55\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\uff0c\u652f\u6301\u57ce\u5e02\u7ba1\u7406\u3001\u707e\u5bb3\u54cd\u5e94\u548c\u6c14\u5019\u97e7\u6027\u3002"}}
{"id": "2510.18601", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18601", "abs": "https://arxiv.org/abs/2510.18601", "authors": ["Marco Alecci", "Jordan Samhi", "Tegawend\u00e9 F. Bissyand\u00e9", "Jacques Klein"], "title": "Evaluating Large Language Models in detecting Secrets in Android Apps", "comment": null, "summary": "Mobile apps often embed authentication secrets, such as API keys, tokens, and\nclient IDs, to integrate with cloud services. However, developers often\nhardcode these credentials into Android apps, exposing them to extraction\nthrough reverse engineering. Once compromised, adversaries can exploit secrets\nto access sensitive data, manipulate resources, or abuse APIs, resulting in\nsignificant security and financial risks. Existing detection approaches, such\nas regex-based analysis, static analysis, and machine learning, are effective\nfor identifying known patterns but are fundamentally limited: they require\nprior knowledge of credential structures, API signatures, or training data.\n  In this paper, we propose SecretLoc, an LLM-based approach for detecting\nhardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it\nleverages contextual and structural cues to identify secrets without relying on\npredefined patterns or labeled training sets. Using a benchmark dataset from\nthe literature, we demonstrate that SecretLoc detects secrets missed by regex-,\nstatic-, and ML-based methods, including previously unseen types of secrets. In\ntotal, we discovered 4828 secrets that were undetected by existing approaches,\ndiscovering more than 10 \"new\" types of secrets, such as OpenAI API keys,\nGitHub Access Tokens, RSA private keys, and JWT tokens, and more.\n  We further extend our analysis to newly crawled apps from Google Play, where\nwe uncovered and responsibly disclosed additional hardcoded secrets. Across a\nset of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which\nwere confirmed and remediated by developers after we contacted them. Our\nresults reveal a dual-use risk: if analysts can uncover these secrets with\nLLMs, so can attackers. This underscores the urgent need for proactive secret\nmanagement and stronger mitigation practices across the mobile ecosystem.", "AI": {"tldr": "SecretLoc\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bAndroid\u5e94\u7528\u4e2d\u7684\u786c\u7f16\u7801\u79d8\u5bc6\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u5f0f\u6216\u8bad\u7ec3\u6570\u636e\uff0c\u53d1\u73b0\u4e86\u73b0\u6709\u65b9\u6cd5\u9057\u6f0f\u76844828\u4e2a\u79d8\u5bc6\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u8005\u7ecf\u5e38\u5c06API\u5bc6\u94a5\u3001\u4ee4\u724c\u7b49\u8ba4\u8bc1\u79d8\u5bc6\u786c\u7f16\u7801\u5230\u5e94\u7528\u4e2d\uff0c\u8fd9\u4e9b\u79d8\u5bc6\u53ef\u80fd\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u88ab\u63d0\u53d6\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u5b89\u5168\u548c\u8d22\u52a1\u98ce\u9669\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\uff0c\u65e0\u6cd5\u8bc6\u522b\u672a\u77e5\u7c7b\u578b\u7684\u79d8\u5bc6\u3002", "method": "\u63d0\u51faSecretLoc\u65b9\u6cd5\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u548c\u7ed3\u6784\u7ebf\u7d22\u6765\u8bc6\u522b\u79d8\u5bc6\uff0c\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u5f0f\u6216\u6807\u8bb0\u8bad\u7ec3\u96c6\uff0c\u4f7f\u7528\u6587\u732e\u4e2d\u7684\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u53d1\u73b0\u4e86\u73b0\u6709\u65b9\u6cd5\u9057\u6f0f\u76844828\u4e2a\u79d8\u5bc6\uff0c\u5305\u62ec10\u591a\u79cd\u65b0\u578b\u79d8\u5bc6\u7c7b\u578b\uff1b\u5728Google Play\u76845000\u4e2a\u5e94\u7528\u4e2d\uff0c42.5%\u7684\u5e94\u7528\u5305\u542b\u786c\u7f16\u7801\u79d8\u5bc6\uff0c\u90e8\u5206\u5df2\u5f97\u5230\u5f00\u53d1\u8005\u4fee\u590d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u53cc\u91cd\u4f7f\u7528\u98ce\u9669\uff1a\u5206\u6790\u4eba\u5458\u80fd\u7528LLM\u53d1\u73b0\u8fd9\u4e9b\u79d8\u5bc6\uff0c\u653b\u51fb\u8005\u540c\u6837\u53ef\u4ee5\u3002\u8fd9\u5f3a\u8c03\u4e86\u79fb\u52a8\u751f\u6001\u7cfb\u7edf\u4e2d\u9700\u8981\u4e3b\u52a8\u7684\u79d8\u5bc6\u7ba1\u7406\u548c\u66f4\u5f3a\u7684\u7f13\u89e3\u63aa\u65bd\u3002"}}
{"id": "2510.18428", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18428", "abs": "https://arxiv.org/abs/2510.18428", "authors": ["Minwei Kong", "Ao Qu", "Xiaotong Guo", "Wenbin Ouyang", "Chonghe Jiang", "Han Zheng", "Yining Ma", "Dingyi Zhuang", "Yuhan Tang", "Junyi Li", "Hai Wang", "Cathy Wu", "Jinhua Zhao"], "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library", "comment": null, "summary": "Optimization modeling enables critical decisions across industries but\nremains difficult to automate: informal language must be mapped to precise\nmathematical formulations and executable solver code. Prior LLM approaches\neither rely on brittle prompting or costly retraining with limited\ngeneralization. We present AlphaOPT, a self-improving experience library that\nenables an LLM to learn from limited demonstrations (even answers alone,\nwithout gold-standard programs) and solver feedback - without annotated\nreasoning traces or parameter updates. AlphaOPT operates in a continual\ntwo-phase cycle: (i) a Library Learning phase that reflects on failed attempts,\nextracting solver-verified, structured insights as {taxonomy, condition,\nexplanation, example}; and (ii) a Library Evolution phase that diagnoses\nretrieval misalignments and refines the applicability conditions of stored\ninsights, improving transfer across tasks. This design (1) learns efficiently\nfrom limited demonstrations without curated rationales, (2) expands continually\nwithout costly retraining by updating the library rather than model weights,\nand (3) makes knowledge explicit and interpretable for human inspection and\nintervention. Experiments show that AlphaOPT steadily improves with more data\n(65% to 72% from 100 to 300 training items) and surpasses the strongest\nbaseline by 7.7% on the out-of-distribution OptiBench dataset when trained only\non answers. Code and data are available at:\nhttps://github.com/Minw913/AlphaOPT.", "AI": {"tldr": "AlphaOPT\u662f\u4e00\u4e2a\u81ea\u6539\u8fdb\u7684\u7ecf\u9a8c\u5e93\u7cfb\u7edf\uff0c\u4f7fLLM\u80fd\u591f\u4ece\u6709\u9650\u7684\u6f14\u793a\u548c\u6c42\u89e3\u5668\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u65e0\u9700\u6807\u6ce8\u63a8\u7406\u8f68\u8ff9\u6216\u53c2\u6570\u66f4\u65b0\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5faa\u73af\u6301\u7eed\u6539\u8fdb\u4f18\u5316\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u4f18\u5316\u5efa\u6a21\u5728\u5de5\u4e1a\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\u4f46\u96be\u4ee5\u81ea\u52a8\u5316\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8106\u5f31\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u6301\u7eed\u7684\u4e24\u9636\u6bb5\u5faa\u73af\uff1a\u5e93\u5b66\u4e60\u9636\u6bb5\u4ece\u5931\u8d25\u5c1d\u8bd5\u4e2d\u63d0\u53d6\u6c42\u89e3\u5668\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u6d1e\u5bdf\uff1b\u5e93\u6f14\u5316\u9636\u6bb5\u8bca\u65ad\u68c0\u7d22\u504f\u5dee\u5e76\u7ec6\u5316\u5b58\u50a8\u6d1e\u5bdf\u7684\u9002\u7528\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAlphaOPT\u968f\u6570\u636e\u589e\u52a0\u7a33\u6b65\u6539\u8fdb\uff08\u4ece100\u5230300\u8bad\u7ec3\u9879\uff0c\u51c6\u786e\u7387\u4ece65%\u63d0\u5347\u523072%\uff09\uff0c\u5728\u4ec5\u4f7f\u7528\u7b54\u6848\u8bad\u7ec3\u65f6\uff0c\u5728OptiBench\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u6700\u5f3a\u57fa\u7ebf7.7%\u3002", "conclusion": "AlphaOPT\u80fd\u591f\u9ad8\u6548\u4ece\u6709\u9650\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u901a\u8fc7\u66f4\u65b0\u5e93\u800c\u975e\u6a21\u578b\u6743\u91cd\u5b9e\u73b0\u6301\u7eed\u6269\u5c55\uff0c\u4f7f\u77e5\u8bc6\u663e\u5f0f\u4e14\u53ef\u89e3\u91ca\uff0c\u4e3a\u4eba\u7c7b\u68c0\u67e5\u548c\u5e72\u9884\u63d0\u4f9b\u4fbf\u5229\u3002"}}
{"id": "2510.18442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18442", "abs": "https://arxiv.org/abs/2510.18442", "authors": ["Ziwei Deng", "Mian Deng", "Chenjing Liang", "Zeming Gao", "Chennan Ma", "Chenxing Lin", "Haipeng Zhang", "Songzhu Mei", "Cheng Wang", "Siqi Shen"], "title": "PlanU: Large Language Model Decision Making through Planning under Uncertainty", "comment": "38 pages, 19 figures, NeurIPS 2025 Accepted", "summary": "Large Language Models (LLMs) are increasingly being explored across a range\nof decision-making tasks. However, LLMs sometimes struggle with decision-making\ntasks under uncertainty that are relatively easy for humans, such as planning\nactions in stochastic environments. The adoption of LLMs for decision-making is\nimpeded by uncertainty challenges, such as LLM uncertainty and environmental\nuncertainty. LLM uncertainty arises from the stochastic sampling process\ninherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM\nuncertainty through multiple reasoning chains or search trees. However, these\napproaches overlook environmental uncertainty, which leads to poor performance\nin environments with stochastic state transitions. Some recent LDM approaches\ndeal with uncertainty by forecasting the probability of unknown variables.\nHowever, they are not designed for multi-step decision-making tasks that\nrequire interaction with the environment. To address uncertainty in LLM\ndecision-making, we introduce PlanU, an LLM-based planning method that captures\nuncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of\neach node in the MCTS as a quantile distribution, which uses a set of quantiles\nto represent the return distribution. To balance exploration and exploitation\nduring tree search, PlanU introduces an Upper Confidence Bounds with Curiosity\n(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive\nexperiments, we demonstrate the effectiveness of PlanU in LLM-based\ndecision-making tasks under uncertainty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPlanU\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u4e0e\u5206\u4f4d\u6570\u5206\u5e03\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3LLM\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "LLM\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u968f\u673a\u73af\u5883\u4e2d\u89c4\u5212\u884c\u52a8\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8LLM\u81ea\u8eab\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5728\u968f\u673a\u72b6\u6001\u8f6c\u6362\u73af\u5883\u4e2d\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u63d0\u51faPlanU\u65b9\u6cd5\uff0c\u5728MCTS\u4e2d\u5efa\u6a21\u6bcf\u4e2a\u8282\u70b9\u7684\u56de\u62a5\u4e3a\u5206\u4f4d\u6570\u5206\u5e03\uff0c\u4f7f\u7528\u4e00\u7ec4\u5206\u4f4d\u6570\u8868\u793a\u56de\u62a5\u5206\u5e03\u3002\u5f15\u5165\u57fa\u4e8e\u597d\u5947\u5fc3\u7684\u4e0a\u7f6e\u4fe1\u754c\u9650(UCC)\u8bc4\u5206\u6765\u5e73\u8861\u6811\u641c\u7d22\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PlanU\u5728LLM\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PlanU\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406LLM\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\u4e2d\u4e0e\u73af\u5883\u4ea4\u4e92\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.18614", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18614", "abs": "https://arxiv.org/abs/2510.18614", "authors": ["Ren\u00e9 Coignard", "Anton Rygin"], "title": "Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation", "comment": null, "summary": "We present Qatsi, a hierarchical key derivation scheme using Argon2id that\ngenerates reproducible cryptographic secrets without persistent storage. The\nsystem eliminates vault-based attack surfaces by deriving all secrets\ndeterministically from a single high-entropy master secret and contextual\nlayers. Outputs achieve 103-312 bits of entropy through memory-hard derivation\n(64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over\n7776-word mnemonics or 90-character passwords. We formalize the hierarchical\nconstruction, prove output uniformity, and quantify GPU attack costs: $2.4\n\\times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under\nParanoid parameters (128 MiB memory). The implementation in Rust provides\nautomatic memory zeroization, compile-time wordlist integrity verification, and\ncomprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021)\ndemonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid\nmode single-layer derivations. Qatsi targets air-gapped systems and master\ncredential generation where stateless reproducibility outweighs rotation\nflexibility.", "AI": {"tldr": "Qatsi\u662f\u4e00\u4e2a\u4f7f\u7528Argon2id\u7684\u5206\u5c42\u5bc6\u94a5\u6d3e\u751f\u65b9\u6848\uff0c\u65e0\u9700\u6301\u4e45\u5b58\u50a8\u5373\u53ef\u751f\u6210\u53ef\u91cd\u73b0\u7684\u52a0\u5bc6\u5bc6\u94a5\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u786e\u5b9a\u6027\u6d3e\u751f\u6240\u6709\u5bc6\u94a5\u6765\u6d88\u9664\u57fa\u4e8e\u4fdd\u9669\u5e93\u7684\u653b\u51fb\u9762\uff0c\u91c7\u7528\u5185\u5b58\u786c\u6d3e\u751f\u548c\u53ef\u8bc1\u660e\u5747\u5300\u7684\u62d2\u7edd\u91c7\u6837\u6765\u751f\u6210\u9ad8\u71b5\u8f93\u51fa\u3002", "motivation": "\u9488\u5bf9\u6c14\u9699\u7cfb\u7edf\u548c\u4e3b\u51ed\u8bc1\u751f\u6210\u573a\u666f\uff0c\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u65e0\u72b6\u6001\u7684\u53ef\u91cd\u73b0\u6027\u6bd4\u8f6e\u6362\u7075\u6d3b\u6027\u66f4\u91cd\u8981\u3002\u6d88\u9664\u57fa\u4e8e\u4fdd\u9669\u5e93\u7684\u653b\u51fb\u9762\uff0c\u901a\u8fc7\u5355\u4e00\u9ad8\u71b5\u4e3b\u5bc6\u94a5\u548c\u4e0a\u4e0b\u6587\u5c42\u786e\u5b9a\u6027\u6d3e\u751f\u6240\u6709\u5bc6\u94a5\u3002", "method": "\u4f7f\u7528Argon2id\u8fdb\u884c\u5206\u5c42\u5bc6\u94a5\u6d3e\u751f\uff0c\u91c7\u7528\u5185\u5b58\u786c\u6d3e\u751f\uff0864-128 MiB\u5185\u5b58\uff0c16-32\u6b21\u8fed\u4ee3\uff09\u548c\u53ef\u8bc1\u660e\u5747\u5300\u7684\u62d2\u7edd\u91c7\u6837\u6765\u751f\u62107776\u8bcd\u52a9\u8bb0\u8bcd\u621690\u5b57\u7b26\u5bc6\u7801\u3002\u5728Rust\u4e2d\u5b9e\u73b0\uff0c\u63d0\u4f9b\u81ea\u52a8\u5185\u5b58\u6e05\u96f6\u3001\u7f16\u8bd1\u65f6\u8bcd\u8868\u5b8c\u6574\u6027\u9a8c\u8bc1\u548c\u5168\u9762\u6d4b\u8bd5\u8986\u76d6\u3002", "result": "\u8f93\u51fa\u8fbe\u5230103-312\u4f4d\u71b5\uff0cGPU\u653b\u51fb\u6210\u672c\u91cf\u5316\uff1a\u5728\u504f\u6267\u53c2\u6570\u4e0b\uff08128 MiB\u5185\u5b58\uff09\uff0c80\u4f4d\u4e3b\u5bc6\u94a5\u5bf9\u5355GPU\u653b\u51fb\u8005\u9700\u89812.4\u00d710^16\u5e74\u3002\u5728Apple M1 Pro\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u6807\u51c6\u6a21\u5f0f544ms\u548c\u504f\u6267\u6a21\u5f0f2273ms\u7684\u5355\u5c42\u6d3e\u751f\u65f6\u95f4\u3002", "conclusion": "Qatsi\u4e3a\u9700\u8981\u65e0\u72b6\u6001\u53ef\u91cd\u73b0\u6027\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5c42\u5bc6\u94a5\u6d3e\u751f\u548c\u5185\u5b58\u786c\u8ba1\u7b97\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u9645\u53ef\u7528\u6027\u3002"}}
{"id": "2510.18470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18470", "abs": "https://arxiv.org/abs/2510.18470", "authors": ["Shaobo Wang", "Yongliang Miao", "Yuancheng Liu", "and Qianli Ma", "Ning Liao", "Linfeng Zhang"], "title": "CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs", "comment": "14 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, but scaling their performance often relies on massive reasoning\ndatasets that are computationally expensive to train on. Existing data\nselection methods aim to curate smaller, high-quality subsets but often rely on\ncostly external models or opaque heuristics. In this work, we shift the focus\nfrom external heuristics to the model's internal mechanisms. We find that\ncomplex reasoning tasks consistently activate a sparse, specialized subset of\nattention heads, forming core reasoning circuits. Building on this insight, we\npropose CircuitSeer, a novel data selection method that quantifies the\nreasoning complexity of data by measuring its influence on these crucial\ncircuits. Extensive experiments on 4 models and 9 datasets demonstrate\nCircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of\ndata selected by our method achieves a 1.4-point gain in average Pass@1 over\ntraining on the full dataset, highlighting its efficiency and effectiveness.", "AI": {"tldr": "CircuitSeer\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6838\u5fc3\u63a8\u7406\u7535\u8def\u6765\u91cf\u5316\u6570\u636e\u590d\u6742\u5ea6\uff0c\u4ec5\u4f7f\u752810%\u7684\u6570\u636e\u5c31\u80fd\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u83b7\u5f97\u6bd4\u5b8c\u6574\u6570\u636e\u96c6\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u6a21\u578b\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u6a21\u578b\u5185\u90e8\u673a\u5236\u6765\u66f4\u6709\u6548\u5730\u9009\u62e9\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u53d1\u73b0\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4f1a\u6fc0\u6d3b\u7a00\u758f\u7684\u4e13\u7528\u6ce8\u610f\u529b\u5934\u5f62\u6210\u6838\u5fc3\u63a8\u7406\u7535\u8def\uff0cCircuitSeer\u901a\u8fc7\u6d4b\u91cf\u6570\u636e\u5bf9\u8fd9\u4e9b\u5173\u952e\u7535\u8def\u7684\u5f71\u54cd\u6765\u91cf\u5316\u63a8\u7406\u590d\u6742\u5ea6\u3002", "result": "\u57284\u4e2a\u6a21\u578b\u548c9\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCircuitSeer\u9009\u62e910%\u6570\u636e\u5fae\u8c03Qwen2.5-Math-7B\uff0c\u5e73\u5747Pass@1\u6bd4\u5b8c\u6574\u6570\u636e\u96c6\u8bad\u7ec3\u63d0\u53471.4\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CircuitSeer\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5185\u90e8\u63a8\u7406\u7535\u8def\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.18674", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18674", "abs": "https://arxiv.org/abs/2510.18674", "authors": ["Alexander Nemecek", "Zebin Yun", "Zahra Rahmani", "Yaniv Harel", "Vipin Chaudhary", "Mahmood Sharif", "Erman Ayday"], "title": "Exploring Membership Inference Vulnerabilities in Clinical Large Language Models", "comment": "Accepted at the 1st IEEE Workshop on Healthcare and Medical Device\n  Security, Privacy, Resilience, and Trust (IEEE HMD-SPiRiT)", "summary": "As large language models (LLMs) become progressively more embedded in\nclinical decision-support, documentation, and patient-information systems,\nensuring their privacy and trustworthiness has emerged as an imperative\nchallenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic\nhealth record (EHR) data improves domain alignment but also raises the risk of\nexposing patient information through model behaviors. In this work-in-progress,\nwe present an exploratory empirical study on membership inference\nvulnerabilities in clinical LLMs, focusing on whether adversaries can infer if\nspecific patient records were used during model training. Using a\nstate-of-the-art clinical question-answering model, Llemr, we evaluate both\ncanonical loss-based attacks and a domain-motivated paraphrasing-based\nperturbation strategy that more realistically reflects clinical adversarial\nconditions. Our preliminary findings reveal limited but measurable membership\nleakage, suggesting that current clinical LLMs provide partial resistance yet\nremain susceptible to subtle privacy risks that could undermine trust in\nclinical AI adoption. These results motivate continued development of\ncontext-aware, domain-specific privacy evaluations and defenses such as\ndifferential privacy fine-tuning and paraphrase-aware training, to strengthen\nthe security and trustworthiness of healthcare AI systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e34\u5e8a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u6210\u5458\u63a8\u65ad\u6f0f\u6d1e\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u867d\u7136\u5177\u6709\u4e00\u5b9a\u62b5\u6297\u529b\u4f46\u4ecd\u5b58\u5728\u53ef\u6d4b\u91cf\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u6587\u6863\u8bb0\u5f55\u548c\u60a3\u8005\u4fe1\u606f\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u9690\u79c1\u6027\u548c\u53ef\u4fe1\u5ea6\u5df2\u6210\u4e3a\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u91cd\u8981\u6311\u6218\u3002\u6a21\u578b\u5728\u654f\u611f\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e0a\u7684\u5fae\u8c03\u867d\u7136\u63d0\u5347\u4e86\u9886\u57df\u5bf9\u9f50\u80fd\u529b\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u901a\u8fc7\u6a21\u578b\u884c\u4e3a\u6cc4\u9732\u60a3\u8005\u4fe1\u606f\u7684\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u4e34\u5e8a\u95ee\u7b54\u6a21\u578bLlemr\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6807\u51c6\u635f\u5931\u7684\u653b\u51fb\u65b9\u6cd5\u548c\u57fa\u4e8e\u9886\u57df\u52a8\u673a\u7684\u91ca\u4e49\u6270\u52a8\u7b56\u7565\uff0c\u540e\u8005\u66f4\u771f\u5b9e\u5730\u53cd\u6620\u4e86\u4e34\u5e8a\u5bf9\u6297\u6761\u4ef6\u3002", "result": "\u521d\u6b65\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5b58\u5728\u6709\u9650\u4f46\u53ef\u6d4b\u91cf\u7684\u6210\u5458\u4fe1\u606f\u6cc4\u9732\uff0c\u8868\u660e\u5f53\u524d\u4e34\u5e8a\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u90e8\u5206\u62b5\u6297\u529b\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u53ef\u80fd\u7834\u574f\u4e34\u5e8aAI\u4fe1\u4efb\u5ea6\u7684\u5fae\u5999\u9690\u79c1\u98ce\u9669\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u63a8\u52a8\u4e86\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9690\u79c1\u8bc4\u4f30\u548c\u9632\u5fa1\u63aa\u65bd\u7684\u6301\u7eed\u5f00\u53d1\uff0c\u5982\u5dee\u5206\u9690\u79c1\u5fae\u8c03\u548c\u91ca\u4e49\u611f\u77e5\u8bad\u7ec3\uff0c\u4ee5\u52a0\u5f3a\u533b\u7597AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.18526", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18526", "abs": "https://arxiv.org/abs/2510.18526", "authors": ["Hanze Guo", "Jing Yao", "Xiao Zhou", "Xiaoyuan Yi", "Xing Xie"], "title": "Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models", "comment": "41 pages, 7 figures", "summary": "As large language models (LLMs) become increasingly integrated into\napplications serving users across diverse cultures, communities and\ndemographics, it is critical to align LLMs with pluralistic human values beyond\naverage principles (e.g., HHH). In psychological and social value theories such\nas Schwartz's Value Theory, pluralistic values are represented by multiple\nvalue dimensions paired with various priorities. However, existing methods\nencounter two challenges when aligning with such fine-grained value objectives:\n1) they often treat multiple values as independent and equally important,\nignoring their interdependence and relative priorities (value complexity); 2)\nthey struggle to precisely control nuanced value priorities, especially those\nunderrepresented ones (value steerability). To handle these challenges, we\npropose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE\nalignment. It introduces a structural causal model (SCM) to feature complex\ninterdependency and prioritization among features, as well as the causal\nrelationship between high-level value dimensions and behaviors. Moreover, it\napplies counterfactual reasoning to generate outputs aligned with any desired\nvalue objectives. Benefitting from explicit causal modeling, COUPLE also\nprovides better interpretability. We evaluate COUPLE on two datasets with\ndifferent value systems and demonstrate that COUPLE advances other baselines\nacross diverse types of value objectives.", "AI": {"tldr": "COUPLE\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u5143\u5316\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u5904\u7406\u4ef7\u503c\u89c2\u7684\u590d\u6742\u6027\u548c\u53ef\u64cd\u63a7\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u9700\u8981\u8d85\u8d8a\u5e73\u5747\u539f\u5219\uff08\u5982HHH\uff09\u6765\u4e0e\u591a\u5143\u5316\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u4ef7\u503c\u76ee\u6807\u65f6\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u4ef7\u503c\u89c2\u7684\u590d\u6742\u6027\u548c\u53ef\u64cd\u63a7\u6027\u3002", "method": "\u63d0\u51faCOUPLE\u6846\u67b6\uff0c\u5f15\u5165\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u6765\u8868\u5f81\u7279\u5f81\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u548c\u4f18\u5148\u7ea7\uff0c\u4ee5\u53ca\u9ad8\u5c42\u4ef7\u503c\u7ef4\u5ea6\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002\u5e94\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u751f\u6210\u7b26\u5408\u4efb\u4f55\u671f\u671b\u4ef7\u503c\u76ee\u6807\u7684\u8f93\u51fa\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u4ef7\u503c\u4f53\u7cfb\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30COUPLE\uff0c\u7ed3\u679c\u663e\u793aCOUPLE\u5728\u5404\u7c7b\u4ef7\u503c\u76ee\u6807\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "COUPLE\u901a\u8fc7\u663e\u5f0f\u7684\u56e0\u679c\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4ef7\u503c\u89c2\u5bf9\u9f50\u4e2d\u7684\u590d\u6742\u6027\u548c\u53ef\u64cd\u63a7\u6027\u6311\u6218\uff0c\u5728\u591a\u5143\u5316\u4ef7\u503c\u76ee\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.18551", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.18551", "abs": "https://arxiv.org/abs/2510.18551", "authors": ["Yuncheng Hua", "Sion Weatherhead", "Mehdi Jafari", "Hao Xue", "Flora D. Salim"], "title": "SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation", "comment": "11 pages, 1 figure, 2 tables. The paper is under review", "summary": "In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that\ntreats simulator construction asinstance optimization over code within a\ntextual computation graph. Specialized LLM-driven agents are embedded as graph\nnodes, and a workflow manager executes a loss-driven loop: code synthesis ->\nexecution -> evaluation -> code repair. The optimizer performs Textual-Gradient\nDescent (TGD), while human-in-the-loop interaction is reserved for task-spec\nconfirmation, minimizing expert effort and keeping the code itself as the\ntrainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,\nand Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.\nBy unifying multi-agent orchestration with a loss-aligned optimization view,\nSOCIA-Nabla converts brittle prompt pipelines into reproducible,\nconstraint-aware simulator code generation that scales across domains and\nsimulation granularities. This work is under review, and we will release the\ncode soon.", "AI": {"tldr": "SOCIA-Nabla\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u6a21\u62df\u5668\u6784\u5efa\u89c6\u4e3a\u6587\u672c\u8ba1\u7b97\u56fe\u4e2d\u7684\u4ee3\u7801\u5b9e\u4f8b\u4f18\u5316\uff0c\u901a\u8fc7\u4e13\u95e8\u7684LLM\u9a71\u52a8\u667a\u80fd\u4f53\u4f5c\u4e3a\u56fe\u8282\u70b9\uff0c\u6267\u884c\u4ee3\u7801\u5408\u6210->\u6267\u884c->\u8bc4\u4f30->\u4ee3\u7801\u4fee\u590d\u7684\u635f\u5931\u9a71\u52a8\u5faa\u73af\uff0c\u5b9e\u73b0\u7ea6\u675f\u611f\u77e5\u7684\u6a21\u62df\u5668\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u5c06\u8106\u5f31\u7684\u63d0\u793a\u7ba1\u9053\u8f6c\u6362\u4e3a\u53ef\u91cd\u73b0\u3001\u7ea6\u675f\u611f\u77e5\u7684\u6a21\u62df\u5668\u4ee3\u7801\u751f\u6210\uff0c\u7edf\u4e00\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e0e\u635f\u5931\u5bf9\u9f50\u7684\u4f18\u5316\u89c6\u56fe\uff0c\u5b9e\u73b0\u8de8\u9886\u57df\u548c\u6a21\u62df\u7c92\u5ea6\u7684\u6269\u5c55\u3002", "method": "\u4f7f\u7528\u6587\u672c\u68af\u5ea6\u4e0b\u964d\uff08TGD\uff09\u4f18\u5316\u5668\uff0c\u5728\u6587\u672c\u8ba1\u7b97\u56fe\u4e2d\u5d4c\u5165\u4e13\u95e8\u7684LLM\u9a71\u52a8\u667a\u80fd\u4f53\u4f5c\u4e3a\u8282\u70b9\uff0c\u5de5\u4f5c\u6d41\u7ba1\u7406\u5668\u6267\u884c\u635f\u5931\u9a71\u52a8\u5faa\u73af\uff1a\u4ee3\u7801\u5408\u6210->\u6267\u884c->\u8bc4\u4f30->\u4ee3\u7801\u4fee\u590d\uff0c\u4fdd\u7559\u4eba\u673a\u4ea4\u4e92\u7528\u4e8e\u4efb\u52a1\u89c4\u8303\u786e\u8ba4\u3002", "result": "\u5728\u4e09\u4e2aCPS\u4efb\u52a1\uff08\u7528\u6237\u5efa\u6a21\u3001\u53e3\u7f69\u91c7\u7528\u548c\u4e2a\u4eba\u79fb\u52a8\u6027\uff09\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "SOCIA-Nabla\u901a\u8fc7\u7edf\u4e00\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e0e\u635f\u5931\u5bf9\u9f50\u7684\u4f18\u5316\u89c6\u56fe\uff0c\u6210\u529f\u5c06\u8106\u5f31\u7684\u63d0\u793a\u7ba1\u9053\u8f6c\u6362\u4e3a\u53ef\u91cd\u73b0\u3001\u7ea6\u675f\u611f\u77e5\u7684\u6a21\u62df\u5668\u4ee3\u7801\u751f\u6210\uff0c\u80fd\u591f\u8de8\u9886\u57df\u548c\u6a21\u62df\u7c92\u5ea6\u8fdb\u884c\u6269\u5c55\u3002"}}
{"id": "2510.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18554", "abs": "https://arxiv.org/abs/2510.18554", "authors": ["Federico Barbero", "Xiangming Gu", "Christopher A. Choquette-Choo", "Chawin Sitawarin", "Matthew Jagielski", "Itay Yona", "Petar Veli\u010dkovi\u0107", "Ilia Shumailov", "Jamie Hayes"], "title": "Extracting alignment data in open models", "comment": null, "summary": "In this work, we show that it is possible to extract significant amounts of\nalignment training data from a post-trained model -- useful to steer the model\nto improve certain capabilities such as long-context reasoning, safety,\ninstruction following, and maths. While the majority of related work on\nmemorisation has focused on measuring success of training data extraction\nthrough string matching, we argue that embedding models are better suited for\nour specific goals. Distances measured through a high quality embedding model\ncan identify semantic similarities between strings that a different metric such\nas edit distance will struggle to capture. In fact, in our investigation,\napproximate string matching would have severely undercounted (by a conservative\nestimate of $10\\times$) the amount of data that can be extracted due to trivial\nartifacts that deflate the metric. Interestingly, we find that models readily\nregurgitate training data that was used in post-training phases such as SFT or\nRL. We show that this data can be then used to train a base model, recovering a\nmeaningful amount of the original performance. We believe our work exposes a\npossibly overlooked risk towards extracting alignment data. Finally, our work\nopens up an interesting discussion on the downstream effects of distillation\npractices: since models seem to be regurgitating aspects of their training set,\ndistillation can therefore be thought of as indirectly training on the model's\noriginal dataset.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.18569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18569", "abs": "https://arxiv.org/abs/2510.18569", "authors": ["Junhyeog Yun", "Hyoun Jun Lee", "Insu Jeon"], "title": "QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework", "comment": "25 pages, 13 figures. Accepted for oral presentation at the 2nd\n  Workshop on LLMs and Generative AI for Finance (AI4F), part of ACM ICAIF\n  2025, Singapore. Non-archival workshop", "summary": "Automating quantitative trading strategy development in dynamic markets is\nchallenging, especially with increasing demand for personalized investment\nsolutions. Existing methods often fail to explore the vast strategy space while\npreserving the diversity essential for robust performance across changing\nmarket conditions. We present QuantEvolve, an evolutionary framework that\ncombines quality-diversity optimization with hypothesis-driven strategy\ngeneration. QuantEvolve employs a feature map aligned with investor\npreferences, such as strategy type, risk profile, turnover, and return\ncharacteristics, to maintain a diverse set of effective strategies. It also\nintegrates a hypothesis-driven multi-agent system to systematically explore the\nstrategy space through iterative generation and evaluation. This approach\nproduces diverse, sophisticated strategies that adapt to both market regime\nshifts and individual investment needs. Empirical results show that QuantEvolve\noutperforms conventional baselines, validating its effectiveness. We release a\ndataset of evolved strategies to support future research.", "AI": {"tldr": "QuantEvolve\u662f\u4e00\u4e2a\u7ed3\u5408\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u548c\u5047\u8bbe\u9a71\u52a8\u7b56\u7565\u751f\u6210\u7684\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u91cf\u5316\u4ea4\u6613\u7b56\u7565\u5f00\u53d1\uff0c\u5728\u52a8\u6001\u5e02\u573a\u4e2d\u4ea7\u751f\u591a\u6837\u5316\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u7b56\u7565\u3002", "motivation": "\u52a8\u6001\u5e02\u573a\u4e2d\u81ea\u52a8\u5316\u91cf\u5316\u4ea4\u6613\u7b56\u7565\u5f00\u53d1\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u63a2\u7d22\u5e7f\u9614\u7b56\u7565\u7a7a\u95f4\u7684\u540c\u65f6\u4fdd\u6301\u591a\u6837\u6027\uff0c\u800c\u591a\u6837\u6027\u5bf9\u4e8e\u5728\u4e0d\u540c\u5e02\u573a\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "QuantEvolve\u91c7\u7528\u4e0e\u6295\u8d44\u8005\u504f\u597d\u5bf9\u9f50\u7684\u7279\u5f81\u6620\u5c04\uff08\u5982\u7b56\u7565\u7c7b\u578b\u3001\u98ce\u9669\u72b6\u51b5\u3001\u6362\u624b\u7387\u548c\u6536\u76ca\u7279\u5f81\uff09\uff0c\u7ed3\u5408\u5047\u8bbe\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u548c\u8bc4\u4f30\u6765\u7cfb\u7edf\u63a2\u7d22\u7b56\u7565\u7a7a\u95f4\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660eQuantEvolve\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u8be5\u6846\u67b6\u751f\u6210\u4e86\u80fd\u591f\u9002\u5e94\u5e02\u573a\u5236\u5ea6\u8f6c\u53d8\u548c\u4e2a\u6027\u5316\u6295\u8d44\u9700\u6c42\u7684\u591a\u6837\u5316\u3001\u590d\u6742\u7b56\u7565\u3002", "conclusion": "QuantEvolve\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u91cf\u5316\u4ea4\u6613\u7b56\u7565\u5f00\u53d1\u4e2d\u7684\u591a\u6837\u6027\u548c\u9002\u5e94\u6027\u6311\u6218\uff0c\u4e3a\u4e2a\u6027\u5316\u6295\u8d44\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u53d1\u5e03\u4e86\u8fdb\u5316\u7b56\u7565\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.18619", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18619", "abs": "https://arxiv.org/abs/2510.18619", "authors": ["Wei Cai", "Jian Zhao", "Yuchen Yuan", "Tianle Zhang", "Ming Zhu", "Haichuan Tang", "Chi Zhang", "Xuelong Li"], "title": "VAR: Visual Attention Reasoning via Structured Search and Backtracking", "comment": null, "summary": "Multimodal Large Language Models (MLLMs), despite their advances, are\nhindered by their high hallucination tendency and heavy reliance on brittle,\nlinear reasoning processes, leading to failures in complex tasks. To address\nthese limitations, we introduce Visual Attention Reasoning (VAR), a novel\nframework that recasts grounded reasoning as a structured search over a\nreasoning trajectory space. VAR decomposes the reasoning process into two key\nstages: traceable evidence grounding and search-based chain-of-thought (CoT)\ngeneration, which incorporates a backtracking mechanism for self-correction.\nThe search is guided by a multi-faceted reward function with semantic and\ngeometric self-verification components, which penalize outputs that are not\nfaithfully grounded in the visual input. We provide a theoretical analysis for\nour search strategy, validating its capability to find the correct solution\nwith high probability. Experimental results show that our 7B model, VAR-7B,\nsets a new state-of-the-art on a comprehensive suite of hallucination and\nsafety benchmarks, significantly outperforming existing open-source models and\ndemonstrating competitive performance against leading proprietary systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86Visual Attention Reasoning (VAR)\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u89e3\u51b3MLLMs\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u7ebf\u6027\u63a8\u7406\u9650\u5236\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u53ef\u8ffd\u6eaf\u7684\u8bc1\u636e\u57fa\u7840\u548c\u641c\u7d22\u5f0f\u601d\u7ef4\u94fe\u751f\u6210\uff0c\u5305\u542b\u56de\u6eaf\u81ea\u6821\u6b63\u673a\u5236\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9ad8\u5e7b\u89c9\u503e\u5411\u548c\u8106\u5f31\u7684\u7ebf\u6027\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u590d\u6742\u4efb\u52a1\u5931\u8d25\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "VAR\u6846\u67b6\u5c06\u57fa\u7840\u63a8\u7406\u91cd\u6784\u4e3a\u63a8\u7406\u8f68\u8ff9\u7a7a\u95f4\u7684\u7ed3\u6784\u5316\u641c\u7d22\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u53ef\u8ffd\u6eaf\u8bc1\u636e\u57fa\u7840\u548c\u641c\u7d22\u5f0f\u601d\u7ef4\u94fe\u751f\u6210\uff08\u5e26\u56de\u6eaf\u673a\u5236\uff09\uff0c\u641c\u7d22\u7531\u5305\u542b\u8bed\u4e49\u548c\u51e0\u4f55\u81ea\u9a8c\u8bc1\u7ec4\u4ef6\u7684\u591a\u9762\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u3002", "result": "7B\u6a21\u578bVAR-7B\u5728\u5e7b\u89c9\u548c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u65b0SOTA\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u4e0e\u9886\u5148\u4e13\u6709\u7cfb\u7edf\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "VAR\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u548c\u81ea\u9a8c\u8bc1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.18628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18628", "abs": "https://arxiv.org/abs/2510.18628", "authors": ["Gilles Audemard", "Sylvie Coste-Marquis", "Pierre Marquis", "Mehdi Sabiri", "Nicolas Szczepanski"], "title": "Leveraging Association Rules for Better Predictions and Better Explanations", "comment": "24 pages", "summary": "We present a new approach to classification that combines data and knowledge.\nIn this approach, data mining is used to derive association rules (possibly\nwith negations) from data. Those rules are leveraged to increase the predictive\nperformance of tree-based models (decision trees and random forests) used for a\nclassification task. They are also used to improve the corresponding\nexplanation task through the generation of abductive explanations that are more\ngeneral than those derivable without taking such rules into account.\nExperiments show that for the two tree-based models under consideration,\nbenefits can be offered by the approach in terms of predictive performance and\nin terms of explanation sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u548c\u77e5\u8bc6\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u6316\u6398\u83b7\u53d6\u5173\u8054\u89c4\u5219\uff0c\u5229\u7528\u8fd9\u4e9b\u89c4\u5219\u63d0\u5347\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u751f\u6210\u66f4\u901a\u7528\u7684\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6570\u636e\uff0c\u7f3a\u4e4f\u77e5\u8bc6\u5143\u7d20\u7684\u6574\u5408\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u6570\u636e\u6316\u6398\u5f97\u5230\u7684\u5173\u8054\u89c4\u5219\u6765\u589e\u5f3a\u6811\u57fa\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u548c\u89e3\u91ca\u6027\u3002", "method": "\u4ece\u6570\u636e\u4e2d\u6316\u6398\u5173\u8054\u89c4\u5219\uff08\u53ef\u80fd\u5305\u542b\u5426\u5b9a\u9879\uff09\uff0c\u5c06\u8fd9\u4e9b\u89c4\u5219\u878d\u5165\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u5e76\u751f\u6210\u66f4\u901a\u7528\u7684\u6eaf\u56e0\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u548c\u89e3\u91ca\u89c4\u6a21\u65b9\u9762\u90fd\u80fd\u4e3a\u4e24\u79cd\u6811\u57fa\u6a21\u578b\u5e26\u6765\u76ca\u5904\u3002", "conclusion": "\u7ed3\u5408\u6570\u636e\u548c\u77e5\u8bc6\u7684\u5206\u7c7b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6811\u57fa\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u4e3a\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18631", "categories": ["cs.AI", "cs.LO", "03B60"], "pdf": "https://arxiv.org/pdf/2510.18631", "abs": "https://arxiv.org/abs/2510.18631", "authors": ["Carlo Proietti", "Antonio Yuste-Ginel"], "title": "Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises", "comment": null, "summary": "Modelling qualitative uncertainty in formal argumentation is essential both\nfor practical applications and theoretical understanding. Yet, most of the\nexisting works focus on \\textit{abstract} models for arguing with uncertainty.\nFollowing a recent trend in the literature, we tackle the open question of\nstudying plausible instantiations of these abstract models. To do so, we ground\nthe uncertainty of arguments in their components, structured within rules and\npremises. Our main technical contributions are: i) the introduction of a notion\nof expressivity that can handle abstract and structured formalisms, and ii) the\npresentation of both negative and positive expressivity results, comparing the\nexpressivity of abstract and structured models of argumentation with\nuncertainty. These results affect incomplete abstract argumentation frameworks,\nand their extension with dependencies, on the abstract side, and ASPIC+, on the\nstructured side.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f62\u5f0f\u8bba\u8bc1\u4e2d\u5b9a\u6027\u4e0d\u786e\u5b9a\u6027\u7684\u5efa\u6a21\uff0c\u6bd4\u8f83\u4e86\u62bd\u8c61\u6a21\u578b\u548c\u7ed3\u6784\u5316\u6a21\u578b\u5728\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8868\u8fbe\u80fd\u529b\u6982\u5ff5\uff0c\u5e76\u7ed9\u51fa\u4e86\u8d1f\u9762\u548c\u6b63\u9762\u7684\u8868\u8fbe\u80fd\u529b\u7ed3\u679c\u3002", "motivation": "\u5728\u5f62\u5f0f\u8bba\u8bc1\u4e2d\u5efa\u6a21\u5b9a\u6027\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u548c\u7406\u8bba\u7406\u89e3\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5173\u6ce8\u62bd\u8c61\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u4e9b\u62bd\u8c61\u6a21\u578b\u7684\u5408\u7406\u5b9e\u4f8b\u5316\uff0c\u5c06\u8bba\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u4e8e\u5176\u7ec4\u6210\u90e8\u5206\uff08\u89c4\u5219\u548c\u524d\u63d0\uff09\u6765\u5efa\u6a21\u3002", "method": "\u5c06\u8bba\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u4e8e\u5176\u7ec4\u6210\u90e8\u5206\uff08\u89c4\u5219\u548c\u524d\u63d0\uff09\u8fdb\u884c\u7ed3\u6784\u5316\u5efa\u6a21\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u62bd\u8c61\u548c\u7ed3\u6784\u5316\u5f62\u5f0f\u4f53\u7cfb\u7684\u8868\u8fbe\u80fd\u529b\u6982\u5ff5\uff0c\u6bd4\u8f83\u4e86\u62bd\u8c61\u6a21\u578b\u548c\u7ed3\u6784\u5316\u6a21\u578b\u5728\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "\u63d0\u51fa\u4e86\u8d1f\u9762\u548c\u6b63\u9762\u7684\u8868\u8fbe\u80fd\u529b\u7ed3\u679c\uff0c\u6bd4\u8f83\u4e86\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\uff08\u5305\u62ec\u4e0d\u5b8c\u5168\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u53ca\u5176\u4f9d\u8d56\u6269\u5c55\uff09\u4e0e\u7ed3\u6784\u5316\u6a21\u578b\uff08\u5982ASPIC+\uff09\u5728\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u7684\u5dee\u5f02\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8bba\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u4e8e\u5176\u7ec4\u6210\u90e8\u5206\u8fdb\u884c\u7ed3\u6784\u5316\u5efa\u6a21\uff0c\u672c\u6587\u4e3a\u62bd\u8c61\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\u63d0\u4f9b\u4e86\u5408\u7406\u7684\u5b9e\u4f8b\u5316\uff0c\u5e76\u901a\u8fc7\u8868\u8fbe\u80fd\u529b\u5206\u6790\u63ed\u793a\u4e86\u62bd\u8c61\u6a21\u578b\u548c\u7ed3\u6784\u5316\u6a21\u578b\u4e4b\u95f4\u7684\u91cd\u8981\u5dee\u5f02\u3002"}}
{"id": "2510.18633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18633", "abs": "https://arxiv.org/abs/2510.18633", "authors": ["Roxana Petcu", "Kenton Murray", "Daniel Khashabi", "Evangelos Kanoulas", "Maarten de Rijke", "Dawn Lawrie", "Kevin Duh"], "title": "Query Decomposition for RAG: Balancing Exploration-Exploitation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems address complex user requests by\ndecomposing them into subqueries, retrieving potentially relevant documents for\neach, and then aggregating them to generate an answer. Efficiently selecting\ninformative documents requires balancing a key trade-off: (i) retrieving\nbroadly enough to capture all the relevant material, and (ii) limiting\nretrieval to avoid excessive noise and computational cost. We formulate query\ndecomposition and document retrieval in an exploitation-exploration setting,\nwhere retrieving one document at a time builds a belief about the utility of a\ngiven sub-query and informs the decision to continue exploiting or exploring an\nalternative. We experiment with a variety of bandit learning methods and\ndemonstrate their effectiveness in dynamically selecting the most informative\nsub-queries. Our main finding is that estimating document relevance using rank\ninformation and human judgments yields a 35% gain in document-level precision,\n15% increase in {\\alpha}-nDCG, and better performance on the downstream task of\nlong-form generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u5206\u89e3\u548c\u6587\u6863\u68c0\u7d22\u5efa\u6a21\u4e3a\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\u65b9\u6cd5\u52a8\u6001\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5b50\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u7cbe\u5ea6\u548c\u957f\u6587\u672c\u751f\u6210\u6027\u80fd\u3002", "motivation": "RAG\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u7528\u6237\u8bf7\u6c42\u65f6\u9700\u8981\u5e73\u8861\u4e24\u4e2a\u5173\u952e\u6743\u8861\uff1a\u68c0\u7d22\u8db3\u591f\u5e7f\u6cdb\u4ee5\u6355\u83b7\u6240\u6709\u76f8\u5173\u6750\u6599\uff0c\u540c\u65f6\u9650\u5236\u68c0\u7d22\u4ee5\u907f\u514d\u8fc7\u5ea6\u566a\u58f0\u548c\u8ba1\u7b97\u6210\u672c\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5c06\u67e5\u8be2\u5206\u89e3\u548c\u6587\u6863\u68c0\u7d22\u5efa\u6a21\u4e3a\u63a2\u7d22-\u5229\u7528\u8bbe\u7f6e\uff0c\u6bcf\u6b21\u68c0\u7d22\u4e00\u4e2a\u6587\u6863\u6765\u5efa\u7acb\u5bf9\u7ed9\u5b9a\u5b50\u67e5\u8be2\u6548\u7528\u7684\u4fe1\u5ff5\uff0c\u5e76\u636e\u6b64\u51b3\u5b9a\u7ee7\u7eed\u5229\u7528\u5f53\u524d\u5b50\u67e5\u8be2\u8fd8\u662f\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u3002\u5b9e\u9a8c\u4e86\u591a\u79cd\u8001\u864e\u673a\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528\u6392\u540d\u4fe1\u606f\u548c\u4eba\u5de5\u5224\u65ad\u4f30\u8ba1\u6587\u6863\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u5728\u6587\u6863\u7ea7\u7cbe\u5ea6\u4e0a\u83b7\u5f97\u4e8635%\u7684\u63d0\u5347\uff0c\u03b1-nDCG\u63d0\u9ad8\u4e8615%\uff0c\u5e76\u5728\u957f\u6587\u672c\u751f\u6210\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5c06RAG\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u5206\u89e3\u548c\u6587\u6863\u68c0\u7d22\u5efa\u6a21\u4e3a\u63a2\u7d22-\u5229\u7528\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u8001\u864e\u673a\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u52a8\u6001\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5b50\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u548c\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2510.18659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18659", "abs": "https://arxiv.org/abs/2510.18659", "authors": ["Dong Yun", "Marco Schouten", "Dim Papadopoulos"], "title": "Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval", "comment": null, "summary": "User queries in information retrieval are often ambiguous, making it\nchallenging for systems to identify a user's target from a single query. While\nrecent dialogue-based interactive retrieval systems can clarify user intent,\nthey are inefficient as they often lack an explicit strategy to ask the most\ninformative questions. To address this limitation, we propose SherlockLLM, a\ndialogue-driven retrieval framework that learns an optimal questioning strategy\nvia Reinforcement Learning (RL) and avoids the need for large-scale annotated\ndialogue data. In our framework, an agent is trained to generate a sequence of\nbinary questions to efficiently narrow down the search space. To validate our\napproach, we introduce a benchmark with both structured and unstructured tasks.\nExperimental results show that SherlockLLM is a robust and efficient solution.\nOn the structured tasks, its performance matches strong baselines and\napproaches the theoretical optimal defined by binary search. On the challenging\nunstructured task, our agent significantly outperforms these baselines,\nshowcasing its ability to learn a highly effective information-seeking dialogue\npolicy.", "AI": {"tldr": "SherlockLLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u5f0f\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4e8c\u5143\u95ee\u9898\u5e8f\u5217\u6765\u9ad8\u6548\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u7d22\u7cfb\u7edf\u56e0\u7528\u6237\u67e5\u8be2\u6a21\u7cca\u800c\u96be\u4ee5\u786e\u5b9a\u76ee\u6807\u610f\u56fe\u7684\u95ee\u9898\u3002", "motivation": "\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u7528\u6237\u67e5\u8be2\u5f80\u5f80\u5177\u6709\u6a21\u7cca\u6027\uff0c\u4f20\u7edf\u7cfb\u7edf\u96be\u4ee5\u4ece\u5355\u4e00\u67e5\u8be2\u4e2d\u51c6\u786e\u8bc6\u522b\u7528\u6237\u76ee\u6807\u3002\u867d\u7136\u73b0\u6709\u7684\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\u68c0\u7d22\u7cfb\u7edf\u53ef\u4ee5\u6f84\u6e05\u7528\u6237\u610f\u56fe\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u7b56\u7565\u6765\u8be2\u95ee\u6700\u6709\u4fe1\u606f\u91cf\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faSherlockLLM\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u4f53\u751f\u6210\u4e8c\u5143\u95ee\u9898\u5e8f\u5217\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6807\u6ce8\u5bf9\u8bdd\u6570\u636e\u3002\u667a\u80fd\u4f53\u5b66\u4e60\u6700\u4f18\u63d0\u95ee\u7b56\u7565\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u4e8c\u5143\u95ee\u9898\u9010\u6b65\u7f29\u5c0f\u641c\u7d22\u8303\u56f4\u3002", "result": "\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e0a\uff0cSherlockLLM\u6027\u80fd\u4e0e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u63a5\u8fd1\u4e8c\u5206\u641c\u7d22\u7684\u7406\u8bba\u6700\u4f18\u503c\u3002\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u975e\u7ed3\u6784\u5316\u4efb\u52a1\u4e0a\uff0c\u8be5\u667a\u80fd\u4f53\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5b66\u4e60\u9ad8\u6548\u4fe1\u606f\u5bfb\u6c42\u5bf9\u8bdd\u7b56\u7565\u7684\u80fd\u529b\u3002", "conclusion": "SherlockLLM\u662f\u4e00\u4e2a\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u6700\u4f18\u63d0\u95ee\u7b56\u7565\uff0c\u5728\u5bf9\u8bdd\u5f0f\u68c0\u7d22\u4e2d\u6709\u6548\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u975e\u7ed3\u6784\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.18751", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18751", "abs": "https://arxiv.org/abs/2510.18751", "authors": ["Patterson Hsieh", "Jerry Yeh", "Mao-Chi He", "Wen-Han Hsieh", "Elvis Hsieh"], "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation", "comment": null, "summary": "Climate change is intensifying the occurrence of harmful algal bloom (HAB),\nparticularly cyanobacteria, which threaten aquatic ecosystems and human health\nthrough oxygen depletion, toxin release, and disruption of marine biodiversity.\nTraditional monitoring approaches, such as manual water sampling, remain\nlabor-intensive and limited in spatial and temporal coverage. Recent advances\nin vision-language models (VLMs) for remote sensing have shown potential for\nscalable AI-driven solutions, yet challenges remain in reasoning over imagery\nand quantifying bloom severity. In this work, we introduce ALGae Observation\nand Segmentation (ALGOS), a segmentation-and-reasoning system for HAB\nmonitoring that combines remote sensing image understanding with severity\nestimation. Our approach integrates GeoSAM-assisted human evaluation for\nhigh-quality segmentation mask curation and fine-tunes vision language model on\nseverity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)\nfrom NASA. Experiments demonstrate that ALGOS achieves robust performance on\nboth segmentation and severity-level estimation, paving the way toward\npractical and automated cyanobacterial monitoring systems.", "AI": {"tldr": "ALGOS\u662f\u4e00\u4e2a\u7528\u4e8e\u6709\u5bb3\u85fb\u534e\u76d1\u6d4b\u7684\u5206\u5272\u4e0e\u63a8\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u9065\u611f\u56fe\u50cf\u7406\u89e3\u548c\u4e25\u91cd\u7a0b\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7GeoSAM\u8f85\u52a9\u4eba\u5de5\u8bc4\u4f30\u548c\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5206\u5272\u548c\u4e25\u91cd\u7a0b\u5ea6\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u6709\u5bb3\u85fb\u534e\uff08\u7279\u522b\u662f\u84dd\u85fb\uff09\u7684\u53d1\u751f\uff0c\u5a01\u80c1\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u548c\u4eba\u7c7b\u5065\u5eb7\u3002\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u6269\u5c55\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u9065\u611f\u56fe\u50cf\u7406\u89e3\u4e0e\u4e25\u91cd\u7a0b\u5ea6\u4f30\u8ba1\uff0c\u96c6\u6210GeoSAM\u8f85\u52a9\u4eba\u5de5\u8bc4\u4f30\u8fdb\u884c\u9ad8\u8d28\u91cf\u5206\u5272\u63a9\u7801\u6574\u7406\uff0c\u5e76\u4f7f\u7528NASA\u7684\u84dd\u85fb\u805a\u5408\u624b\u52a8\u6807\u7b7e\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eALGOS\u5728\u5206\u5272\u548c\u4e25\u91cd\u7a0b\u5ea6\u7ea7\u522b\u4f30\u8ba1\u65b9\u9762\u90fd\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "ALGOS\u4e3a\u5b9e\u73b0\u5b9e\u7528\u548c\u81ea\u52a8\u5316\u7684\u84dd\u85fb\u76d1\u6d4b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.18803", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18803", "abs": "https://arxiv.org/abs/2510.18803", "authors": ["Shirin Tavakoli Kafiabad", "Andrea Schiffauerova", "Ashkan Ebadi"], "title": "Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location", "comment": "35 pages", "summary": "Optimizing national scientific investment requires a clear understanding of\nevolving research trends and the demographic and geographical forces shaping\nthem, particularly in light of commitments to equity, diversity, and inclusion.\nThis study addresses this need by analyzing 18 years (2005-2022) of research\nproposals funded by the Natural Sciences and Engineering Research Council of\nCanada (NSERC). We conducted a comprehensive comparative evaluation of three\ntopic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic\nModelling (STM), and BERTopic. We also introduced a novel algorithm, named\nCOFFEE, designed to enable robust covariate effect estimation for BERTopic.\nThis advancement addresses a significant gap, as BERTopic lacks a native\nfunction for covariate analysis, unlike the probabilistic STM. Our findings\nhighlight that while all models effectively delineate core scientific domains,\nBERTopic outperformed by consistently identifying more granular, coherent, and\nemergent themes, such as the rapid expansion of artificial intelligence.\nAdditionally, the covariate analysis, powered by COFFEE, confirmed distinct\nprovincial research specializations and revealed consistent gender-based\nthematic patterns across various scientific disciplines. These insights offer a\nrobust empirical foundation for funding organizations to formulate more\nequitable and impactful funding strategies, thereby enhancing the effectiveness\nof the scientific ecosystem.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u52a0\u62ff\u5927\u81ea\u7136\u79d1\u5b66\u4e0e\u5de5\u7a0b\u7814\u7a76\u59d4\u5458\u4f1a18\u5e74\u7684\u8d44\u52a9\u63d0\u6848\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff08LDA\u3001STM\u3001BERTopic\uff09\uff0c\u5e76\u5f00\u53d1\u4e86COFFEE\u7b97\u6cd5\u6765\u589e\u5f3aBERTopic\u7684\u534f\u53d8\u91cf\u5206\u6790\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793aBERTopic\u5728\u8bc6\u522b\u7ec6\u7c92\u5ea6\u3001\u8fde\u8d2f\u6027\u548c\u65b0\u5174\u4e3b\u9898\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u534f\u53d8\u91cf\u5206\u6790\u63ed\u793a\u4e86\u7701\u7ea7\u7814\u7a76\u4e13\u4e1a\u5316\u548c\u6027\u522b\u76f8\u5173\u7684\u4e3b\u9898\u6a21\u5f0f\u3002", "motivation": "\u4f18\u5316\u56fd\u5bb6\u79d1\u5b66\u6295\u8d44\u9700\u8981\u4e86\u89e3\u7814\u7a76\u8d8b\u52bf\u6f14\u53d8\u53ca\u4eba\u53e3\u5730\u7406\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u516c\u5e73\u3001\u591a\u5143\u548c\u5305\u5bb9\u6027\u627f\u8bfa\u80cc\u666f\u4e0b\u3002", "method": "\u5206\u6790NSERC 2005-2022\u5e74\u8d44\u52a9\u63d0\u6848\uff0c\u6bd4\u8f83LDA\u3001STM\u548cBERTopic\u4e09\u79cd\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u5f00\u53d1COFFEE\u7b97\u6cd5\u5b9e\u73b0BERTopic\u7684\u534f\u53d8\u91cf\u6548\u5e94\u4f30\u8ba1\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u80fd\u6709\u6548\u8bc6\u522b\u6838\u5fc3\u79d1\u5b66\u9886\u57df\uff0c\u4f46BERTopic\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u8bc6\u522b\u66f4\u7ec6\u7c92\u5ea6\u3001\u8fde\u8d2f\u7684\u65b0\u5174\u4e3b\u9898\uff08\u5982\u4eba\u5de5\u667a\u80fd\u5feb\u901f\u53d1\u5c55\uff09\u3002COFFEE\u534f\u53d8\u91cf\u5206\u6790\u786e\u8ba4\u4e86\u7701\u7ea7\u7814\u7a76\u4e13\u4e1a\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u8de8\u5b66\u79d1\u4e00\u81f4\u7684\u6027\u522b\u76f8\u5173\u4e3b\u9898\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8d44\u52a9\u673a\u6784\u5236\u5b9a\u66f4\u516c\u5e73\u6709\u6548\u7684\u8d44\u52a9\u7b56\u7565\u63d0\u4f9b\u4e86\u575a\u5b9e\u5b9e\u8bc1\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u79d1\u5b66\u751f\u6001\u7cfb\u7edf\u7684\u6548\u80fd\u3002"}}
