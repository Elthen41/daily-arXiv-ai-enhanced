{"id": "2602.21411", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21411", "abs": "https://arxiv.org/abs/2602.21411", "authors": ["Marc Dufay", "Diana Ghinea", "Anton Paramonov"], "title": "General Convex Agreement with Near-Optimal Communication", "comment": "Working paper", "summary": "Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \\emph{communication}: standard approaches for CA have a communication complexity of $\u0398(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $\u03a9(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.\n  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = \u03a9(n \\cdot \u03ba)$, where $\u03ba$ is a security parameter, we achieve $O(L\\cdot n\\log n)$ communication for finite convexity spaces and $O(L\\cdot n^{1+o(1)})$ communication for Euclidean spaces $\\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(\u03c9+\\varepsilon)$ for any constant $\\varepsilon>0$, where $\u03c9$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(\u03c9+\\varepsilon+1)$ for any constant $\\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.\n  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u51f8\u4e00\u81f4\u6027\u534f\u8bae(CA)\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u9488\u5bf9\u62bd\u8c61\u51f8\u6027\u7a7a\u95f4\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u540c\u6b65\u534f\u8bae\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u3002", "motivation": "\u51f8\u4e00\u81f4\u6027\u534f\u8bae\u5728\u62dc\u5360\u5ead\u5bb9\u9519\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff08\u5982\u9c81\u68d2\u5b66\u4e60\u3001\u4f20\u611f\u5668\u878d\u5408\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u4fe1\u590d\u6742\u5ea6\u9ad8\uff08\u0398(Ln\u00b2)\uff09\uff0c\u4e0e\u62dc\u5360\u5ead\u4e00\u81f4\u6027\u534f\u8bae\u7684\u4e0b\u754c\u03a9(Ln)\u5b58\u5728\u5dee\u8ddd\u3002\u867d\u7136\u6700\u8fd1\u5de5\u4f5c\u5bf9\u8db3\u591f\u5927\u7684L\u5b9e\u73b0\u4e86\u6700\u4f18\u901a\u4fe1\u590d\u6742\u5ea6O(Ln)\uff0c\u4f46\u5c06\u5176\u63a8\u5e7f\u5230\u4e00\u822c\u51f8\u6027\u7a7a\u95f4\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u63d0\u53d6\u5668\u56fe\u83b7\u5f97\u786e\u5b9a\u6027\u59d4\u5458\u4f1a\u5206\u914d\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u62b5\u6297\u81ea\u9002\u5e94\u654c\u624b\u653b\u51fb\u3002\u9488\u5bf9\u6709\u9650\u51f8\u6027\u7a7a\u95f4\u548c\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u5206\u522b\u8bbe\u8ba1\u4e86\u534f\u8bae\uff0c\u5177\u6709\u6e10\u8fd1\u6700\u4f18\u7684\u8f6e\u590d\u6742\u5ea6O(n)\u3002", "result": "\u5f53L = \u03a9(n\u00b7\u03ba)\u65f6\uff0c\u5b9e\u73b0\u4e86O(L\u00b7n log n)\u901a\u4fe1\u590d\u6742\u5ea6\uff08\u6709\u9650\u51f8\u6027\u7a7a\u95f4\uff09\u548cO(L\u00b7n\u00b9\u207a\u1d52\u207d\u00b9\u207e)\u901a\u4fe1\u590d\u6742\u5ea6\uff08\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff09\u3002\u5f53\u8f93\u5165\u957f\u5ea6L\u6709\u5148\u9a8c\u754c\u65f6\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u5bb9\u9519\u6027t < n/(\u03c9+\u03b5)\uff1b\u5f53L\u672a\u77e5\u65f6\uff0c\u5bb9\u9519\u6027\u4e3at < n/(\u03c9+\u03b5+1)\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u51f8\u4e00\u81f4\u6027\u534f\u8bae\u901a\u4fe1\u590d\u6742\u5ea6\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u53d6\u5668\u56fe\u6280\u672f\u5b9e\u73b0\u4e86\u5bf9\u81ea\u9002\u5e94\u654c\u624b\u5177\u6709\u62b5\u6297\u529b\u7684\u786e\u5b9a\u6027\u59d4\u5458\u4f1a\u5206\u914d\uff0c\u4e3a\u62bd\u8c61\u51f8\u6027\u7a7a\u95f4\u63d0\u4f9b\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u901a\u4fe1\u6548\u7387\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21626", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21626", "abs": "https://arxiv.org/abs/2602.21626", "authors": ["Yifan Sun", "Gholamreza Haffar", "Minxian Xu", "Rajkumar Buyya", "Adel N. Toosi"], "title": "Multi-Layer Scheduling for MoE-Based LLM Reasoning", "comment": "12 pages, 10 figures", "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9MoE\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u591a\u5c42\u8c03\u5ea6\u6846\u67b6\uff0c\u5728\u8bf7\u6c42\u3001\u5f15\u64ce\u548c\u4e13\u5bb6\u4e09\u4e2a\u5c42\u9762\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\uff0c\u76f8\u6bd4\u73b0\u6709\u6846\u67b6vLLM\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u9762\u4e34\u8ba1\u7b97\u548c\u5ef6\u8fdf\u6311\u6218\uff0c\u73b0\u6709\u8c03\u5ea6\u7b56\u7565\u5982FCFS\u548cRR\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7cfb\u7edf\u8d44\u6e90\uff0c\u5b58\u5728\u961f\u5934\u963b\u585e\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\u3002MoE\u6a21\u578b\u7684\u51fa\u73b0\u5e26\u6765\u4e86\u4e13\u5bb6\u5e76\u884c\u548c\u8def\u7531\u590d\u6742\u6027\u7684\u65b0\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u8c03\u5ea6\u6846\u67b6\uff1a1) \u8bf7\u6c42\u5c42\u91c7\u7528SJF\u548c\u4f18\u5148\u7ea7\u611f\u77e5\u8001\u5316\u7b97\u6cd5\uff1b2) \u5f15\u64ce\u5c42\u8bbe\u8ba1\u8003\u8651\u524d\u7f00token\u8d1f\u8f7d\u3001KV\u7f13\u5b58\u5229\u7528\u7387\u548c\u7528\u6237\u7c98\u6027\u7684\u8d1f\u8f7d\u611f\u77e5\u5206\u53d1\u7b56\u7565\uff1b3) \u4e13\u5bb6\u5c42\u901a\u8fc7\u7f13\u89e3\u4e13\u5bb6\u70ed\u70b9\u548c\u7b56\u7565\u6027\u653e\u7f6e\u5c42\u95f4\u4e13\u5bb6\u4f9d\u8d56\u6765\u5e73\u8861\u8d1f\u8f7d\u3002", "result": "\u5728\u8d85\u8fc7100\u4e2a\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684vLLM\u63a8\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86TTFT\u5ef6\u8fdf\u964d\u4f4e17.8%\uff0cTPOT\u5ef6\u8fdf\u964d\u4f4e13.3%\u3002", "conclusion": "\u9488\u5bf9MoE\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u591a\u5c42\u8c03\u5ea6\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709\u8c03\u5ea6\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u6548\u7387\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2602.21278", "categories": ["cs.AR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.21278", "abs": "https://arxiv.org/abs/2602.21278", "authors": ["Xinxin Wang", "Lixian Yan", "Shuhan Liu", "Luke Upton", "Zhuoqi Cai", "Yiming Tan", "Shengman Li", "Koustav Jana", "Peijing Li", "Jesse Cirimelli-Low", "Thierry Tambe", "Matthew Guthaus", "H. -S. Philip Wong"], "title": "Heterogeneous Memory Design Exploration for AI Accelerators with a Gain Cell Memory Compiler", "comment": null, "summary": "As memory increasingly dominates system cost and energy, heterogeneous on-chip memory systems that combine technologies with complementary characteristics are becoming essential. Gain Cell RAM (GCRAM) offers higher density, lower power, and tunable retention, expanding the design space beyond conventional SRAM. To this end, we create an OpenGCRAM compiler supporting both SRAM and GCRAM. It generates macro-level designs and layouts for commercial CMOS processes and characterizes area, delay, and power across user-defined configurations. The tool enables systematic identification of optimal heterogeneous memory configurations for AI tasks under specified performance metrics.", "AI": {"tldr": "\u5f00\u53d1\u4e86OpenGCRAM\u7f16\u8bd1\u5668\uff0c\u652f\u6301SRAM\u548cGCRAM\u4e24\u79cd\u5185\u5b58\u6280\u672f\uff0c\u7528\u4e8e\u751f\u6210\u5f02\u6784\u5185\u5b58\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4f18\u5316AI\u4efb\u52a1\u7684\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u968f\u7740\u5185\u5b58\u6210\u672c\u5728\u7cfb\u7edf\u603b\u6210\u672c\u4e2d\u5360\u6bd4\u8d8a\u6765\u8d8a\u9ad8\uff0c\u9700\u8981\u7ed3\u5408\u4e0d\u540c\u6280\u672f\u4f18\u52bf\u7684\u5f02\u6784\u7247\u4e0a\u5185\u5b58\u7cfb\u7edf\u3002GCRAM\u76f8\u6bd4\u4f20\u7edfSRAM\u5177\u6709\u66f4\u9ad8\u5bc6\u5ea6\u3001\u66f4\u4f4e\u529f\u8017\u548c\u53ef\u8c03\u4fdd\u6301\u65f6\u95f4\u7684\u7279\u70b9\uff0c\u6269\u5c55\u4e86\u5185\u5b58\u8bbe\u8ba1\u7a7a\u95f4\u3002", "method": "\u521b\u5efa\u4e86OpenGCRAM\u7f16\u8bd1\u5668\uff0c\u652f\u6301SRAM\u548cGCRAM\u4e24\u79cd\u5185\u5b58\u6280\u672f\u3002\u8be5\u5de5\u5177\u80fd\u591f\u4e3a\u5546\u4e1aCMOS\u5de5\u827a\u751f\u6210\u5b8f\u7ea7\u8bbe\u8ba1\u548c\u5e03\u5c40\uff0c\u5e76\u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u914d\u7f6e\u6765\u8868\u5f81\u9762\u79ef\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u7279\u6027\u3002", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u5728\u7279\u5b9a\u6027\u80fd\u6307\u6807\u4e0bAI\u4efb\u52a1\u7684\u6700\u4f18\u5f02\u6784\u5185\u5b58\u914d\u7f6e\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u8005\u5728\u4e0d\u540c\u5185\u5b58\u6280\u672f\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u51b3\u7b56\u3002", "conclusion": "OpenGCRAM\u7f16\u8bd1\u5668\u4e3a\u5f02\u6784\u5185\u5b58\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u652f\u6301\uff0c\u7279\u522b\u9002\u7528\u4e8eAI\u5e94\u7528\u573a\u666f\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u5185\u5b58\u6280\u672f\u4e4b\u95f4\u627e\u5230\u6700\u4f18\u914d\u7f6e\u5e73\u8861\u6027\u80fd\u3001\u9762\u79ef\u548c\u529f\u8017\u3002"}}
{"id": "2602.21788", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21788", "abs": "https://arxiv.org/abs/2602.21788", "authors": ["Yifan Niu", "Han Xiao", "Dongyi Liu", "Wei Zhou", "Jia Li"], "title": "DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism", "comment": null, "summary": "Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.", "AI": {"tldr": "DHP\u662f\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u5e76\u884c\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u6784\u901a\u4fe1\u7ec4\u548c\u5e76\u884c\u5ea6\uff0c\u5728NPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u9ad8\u8fbe1.36\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6781\u5176\u5f02\u6784\uff0c\u800c\u73b0\u6709\u7684\u8bad\u7ec3\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u5e76\u884c\u7b56\u7565\uff0c\u5728\u6570\u636e\u5f02\u6784\u60c5\u51b5\u4e0b\u5b58\u5728\u4e25\u91cd\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3001\u5197\u4f59\u901a\u4fe1\u548c\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6df7\u5408\u5e76\u884c\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u5730\u91cd\u6784\u901a\u4fe1\u7ec4\u548c\u5e76\u884c\u5ea6\uff1b\u63a8\u5e7f\u975e2\u7684\u5e42\u6b21\u5e76\u884c\u5ea6\uff0c\u5e76\u5f00\u53d1\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u751f\u6210\u8fd1\u4f3c\u6700\u4f18\u7684\u5e76\u884c\u7b56\u7565\uff0c\u6bcf\u4e2a\u8bad\u7ec3\u6279\u6b21\u4ec5\u589e\u52a0\u6beb\u79d2\u7ea7\u5f00\u9500\u3002", "result": "DHP\u663e\u8457\u4f18\u4e8eMegatron-LM\u548cDeepSpeed\uff0c\u5728NPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u9ad8\u8fbe1.36\u500d\u7684\u8bad\u7ec3\u541e\u5410\u91cf\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u7ebf\u6027\u7684\u6269\u5c55\u6548\u7387\uff0c\u5373\u4f7f\u5728\u6781\u7aef\u6570\u636e\u53d8\u5f02\u6027\u4e0b\u4e5f\u80fd\u7ef4\u6301\u9ad8\u786c\u4ef6\u6548\u7387\u3002", "conclusion": "DHP\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.21351", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21351", "abs": "https://arxiv.org/abs/2602.21351", "authors": ["Dmitrii Pantiukhin", "Ivan Kuznetsov", "Boris Shapkin", "Antonia Anna Jost", "Thomas Jung", "Nikolay Koldunov"], "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives", "comment": "20 pages, 6 figures, 7 tables, supplementary material included", "summary": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.", "AI": {"tldr": "PANGAEA-GPT\uff1a\u4e00\u4e2a\u7528\u4e8e\u5730\u7403\u79d1\u5b66\u6570\u636e\u81ea\u4e3b\u53d1\u73b0\u548c\u5206\u6790\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u76d1\u7763-\u5de5\u4f5c\u8005\u62d3\u6251\u89e3\u51b3\u6570\u636e\u53ef\u6269\u5c55\u6027\u6311\u6218", "motivation": "\u5730\u7403\u79d1\u5b66\u6570\u636e\u5feb\u901f\u79ef\u7d2f\u5e26\u6765\u4e86\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0cPANGAEA\u7b49\u5b58\u50a8\u5e93\u4e2d\u5927\u91cf\u6570\u636e\u96c6\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9650\u5236\u4e86\u6570\u636e\u91cd\u7528\u6027", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5177\u6709\u96c6\u4e2d\u5f0f\u76d1\u7763-\u5de5\u4f5c\u8005\u62d3\u6251\u7ed3\u6784\uff0c\u5305\u542b\u6570\u636e\u7c7b\u578b\u611f\u77e5\u8def\u7531\u3001\u6c99\u76d2\u786e\u5b9a\u6027\u4ee3\u7801\u6267\u884c\u548c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u7684\u81ea\u6211\u6821\u6b63\u673a\u5236", "result": "\u901a\u8fc7\u7269\u7406\u6d77\u6d0b\u5b66\u548c\u751f\u6001\u5b66\u7684\u7528\u4f8b\u573a\u666f\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u80fd\u591f\u4ee5\u6700\u5c11\u4eba\u5de5\u5e72\u9884\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u7a0b", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u534f\u8c03\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u67e5\u8be2\u548c\u5206\u6790\u5f02\u6784\u5b58\u50a8\u5e93\u6570\u636e\u7684\u65b9\u6cd5\u8bba"}}
{"id": "2602.21897", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21897", "abs": "https://arxiv.org/abs/2602.21897", "authors": ["Aleix Bon\u00e9", "Alejandro Aguirre", "David \u00c1lvarez", "Pedro J. Martinez-Ferrer", "Vicen\u00e7 Beltran"], "title": "A task-based data-flow methodology for programming heterogeneous systems with multiple accelerator APIs", "comment": "13 pages, 8 figures", "summary": "Heterogeneous nodes that combine multi-core CPUs with diverse accelerators are rapidly becoming the norm in both high-performance computing (HPC) and AI infrastructures. Exploiting these platforms, however, requires orchestrating several low-level accelerator APIs such as CUDA, SYCL, and Triton. In some occasions they can be combined with optimized vendor math libraries: e.g., cuBLAS and oneAPI. Each API or library introduces its own abstractions, execution semantics, and synchronization mechanisms. Combining them within a single application is therefore error-prone and labor-intensive. We propose reusing a task-based data-flow methodology together with Task-Aware APIs (TA-libs) to overcome these limitations and facilitate the seamless integration of multiple accelerator programming models, while still leveraging the best-in-class kernels offered by each API.\n  Applications are expressed as a directed acyclic graph (DAG) of host tasks and device kernels managed by an OpenMP/OmpSs-2 runtime. We introduce Task-Aware SYCL (TASYCL) and leverage Task-Aware CUDA (TACUDA), which elevate individual accelerator invocations to first-class tasks. When multiple native runtimes coexist on the same multi-core CPU, they contend for threads, leading to oversubscription and performance variability. To address this, we unify their thread management under the nOS-V tasking and threading library, to which we contribute a new port of the PoCL (Portable OpenCL) runtime.\n  These results demonstrate that task-aware libraries, coupled with the nOS-V library, enable a single application to harness multiple accelerator programming models transparently and efficiently. The proposed methodology is immediately applicable to current heterogeneous nodes and is readily extensible to future systems that integrate even richer combinations of CPUs, GPUs, FPGAs, and AI accelerators.", "AI": {"tldr": "\u63d0\u51fa\u4efb\u52a1\u611f\u77e5API\u548cnOS-V\u5e93\uff0c\u5b9e\u73b0\u5f02\u6784\u8282\u70b9\u4e0a\u591a\u79cd\u52a0\u901f\u5668\u7f16\u7a0b\u6a21\u578b\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u89e3\u51b3\u591a\u8fd0\u884c\u65f6\u7ebf\u7a0b\u4e89\u7528\u95ee\u9898\u3002", "motivation": "\u5f02\u6784\u8282\u70b9\uff08CPU+\u591a\u79cd\u52a0\u901f\u5668\uff09\u6210\u4e3aHPC\u548cAI\u57fa\u7840\u8bbe\u65bd\u5e38\u6001\uff0c\u4f46\u96c6\u6210CUDA\u3001SYCL\u3001Triton\u7b49\u4e0d\u540c\u52a0\u901f\u5668API\u548c\u5e93\u65f6\u9762\u4e34\u62bd\u8c61\u5dee\u5f02\u3001\u6267\u884c\u8bed\u4e49\u4e0d\u4e00\u81f4\u3001\u540c\u6b65\u673a\u5236\u590d\u6742\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5e94\u7528\u5f00\u53d1\u9519\u8bef\u591a\u3001\u5de5\u4f5c\u91cf\u5927\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4efb\u52a1\u7684\u6570\u636e\u6d41\u65b9\u6cd5\uff0c\u7ed3\u5408\u4efb\u52a1\u611f\u77e5API\uff08TA-libs\uff09\uff0c\u5c06\u5e94\u7528\u8868\u793a\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\uff0c\u5305\u542b\u4e3b\u673a\u4efb\u52a1\u548c\u8bbe\u5907\u5185\u6838\uff0c\u7531OpenMP/OmpSs-2\u8fd0\u884c\u65f6\u7ba1\u7406\u3002\u5f15\u5165Task-Aware SYCL\uff08TASYCL\uff09\u5e76\u5229\u7528Task-Aware CUDA\uff08TACUDA\uff09\uff0c\u5c06\u5355\u4e2a\u52a0\u901f\u5668\u8c03\u7528\u63d0\u5347\u4e3a\u4e00\u7ea7\u4efb\u52a1\u3002\u4f7f\u7528nOS-V\u4efb\u52a1\u548c\u7ebf\u7a0b\u5e93\u7edf\u4e00\u7ebf\u7a0b\u7ba1\u7406\uff0c\u907f\u514d\u591a\u8fd0\u884c\u65f6\u5171\u5b58\u65f6\u7684\u7ebf\u7a0b\u4e89\u7528\u95ee\u9898\u3002", "result": "\u4efb\u52a1\u611f\u77e5\u5e93\u4e0enOS-V\u5e93\u7ed3\u5408\uff0c\u4f7f\u5355\u4e2a\u5e94\u7528\u80fd\u591f\u900f\u660e\u9ad8\u6548\u5730\u5229\u7528\u591a\u79cd\u52a0\u901f\u5668\u7f16\u7a0b\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5f53\u524d\u5f02\u6784\u8282\u70b9\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u672a\u6765\u96c6\u6210\u66f4\u4e30\u5bccCPU\u3001GPU\u3001FPGA\u548cAI\u52a0\u901f\u5668\u7684\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u611f\u77e5API\u548c\u7edf\u4e00\u7ebf\u7a0b\u7ba1\u7406\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8282\u70b9\u4e0a\u591a\u52a0\u901f\u5668\u7f16\u7a0b\u6a21\u578b\u96c6\u6210\u7684\u590d\u6742\u6027\uff0c\u4e3a\u5f53\u524d\u548c\u672a\u6765\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21534", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21534", "abs": "https://arxiv.org/abs/2602.21534", "authors": ["Xiaoxuan Wang", "Han Zhang", "Haixin Wang", "Yidan Shi", "Ruoyan Li", "Kaiqiao Han", "Chenyi Tong", "Haoran Deng", "Renliang Sun", "Alexander Taylor", "Yanqiao Zhu", "Jason Cong", "Yizhou Sun", "Wei Wang"], "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning", "comment": null, "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ARLArena\u6846\u67b6\u548cSAMPO\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3Agentic\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\u548c\u7b56\u7565\u68af\u5ea6\u5206\u89e3\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u3002", "motivation": "Agentic\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u590d\u6742\u591a\u6b65\u4ea4\u4e92\u4efb\u52a1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u6781\u4e0d\u7a33\u5b9a\uff0c\u7ecf\u5e38\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u66f4\u5927\u73af\u5883\u548c\u66f4\u957f\u4ea4\u4e92\u5468\u671f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e5f\u963b\u788d\u4e86\u5bf9\u7b97\u6cd5\u8bbe\u8ba1\u9009\u62e9\u7684\u7cfb\u7edf\u63a2\u7d22\u3002", "method": "1. \u63d0\u51faARLArena\uff1a\u4e00\u4e2a\u7a33\u5b9a\u7684\u8bad\u7ec3\u914d\u65b9\u548c\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u5728\u53ef\u63a7\u548c\u53ef\u590d\u73b0\u7684\u73af\u5883\u4e2d\u68c0\u67e5\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1b2. \u6784\u5efa\u5e72\u51c0\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u5e73\u53f0\uff1b3. \u5c06\u7b56\u7565\u68af\u5ea6\u5206\u89e3\u4e3a\u56db\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u7ef4\u5ea6\u5e76\u8bc4\u4f30\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff1b4. \u63d0\u51faSAMPO\u65b9\u6cd5\uff1a\u4e00\u79cd\u7a33\u5b9a\u7684Agentic\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u7f13\u89e3ARL\u4e2d\u7684\u4e3b\u8981\u4e0d\u7a33\u5b9a\u6765\u6e90\u3002", "result": "SAMPO\u5728\u5404\u79cdAgentic\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u5f3a\u5927\u7684\u6027\u80fd\u3002\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u4e3aARL\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7b56\u7565\u68af\u5ea6\u89c6\u89d2\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aARL\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7b56\u7565\u68af\u5ea6\u89c6\u89d2\uff0c\u5e76\u4e3a\u6784\u5efa\u7a33\u5b9a\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.22017", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22017", "abs": "https://arxiv.org/abs/2602.22017", "authors": ["Chris Egersdoerfer", "Arnav Sareen", "Jean Luca Bez", "Suren Byna", "Dongkuan", "Xu", "Dong Dai"], "title": "IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs", "comment": "Published in the Proceedings of the 2025 IEEE International Parallel and Distributed Processing Symposium (IPDPS 2025)", "summary": "As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.", "AI": {"tldr": "IOAgent\uff1a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316HPC I/O\u6027\u80fd\u8bca\u65ad\u5de5\u5177\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u9884\u5904\u7406\u3001RAG\u9886\u57df\u77e5\u8bc6\u96c6\u6210\u548c\u6811\u72b6\u5408\u5e76\u6280\u672f\uff0c\u4e3a\u79d1\u5b66\u5bb6\u63d0\u4f9b\u51c6\u786e\u7684I/O\u95ee\u9898\u8bca\u65ad\u548c\u4ea4\u4e92\u5f0f\u5206\u6790\u3002", "motivation": "HPC\u5b58\u50a8\u6808\u65e5\u76ca\u590d\u6742\uff0c\u9886\u57df\u79d1\u5b66\u5bb6\u96be\u4ee5\u6709\u6548\u5229\u7528\u5b58\u50a8\u7cfb\u7edf\u8fbe\u5230\u7406\u60f3I/O\u6027\u80fd\u3002I/O\u4e13\u5bb6\u6570\u91cf\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u589e\u957f\u7684\u9700\u6c42\uff0c\u6210\u4e3a\u79d1\u5b66\u5bb6\u63d0\u5347\u751f\u4ea7\u529b\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faIOAgent\u7cfb\u7edf\uff0c\u5305\u542b\uff1a1\uff09\u6a21\u5757\u5316\u9884\u5904\u7406\u5668\u5904\u7406Darshan\u8ddf\u8e2a\u6587\u4ef6\uff1b2\uff09\u57fa\u4e8eRAG\u7684\u9886\u57df\u77e5\u8bc6\u96c6\u6210\u5668\u6574\u5408HPC I/O\u4e13\u4e1a\u77e5\u8bc6\uff1b3\uff09\u6811\u72b6\u5408\u5e76\u5668\u51c6\u786e\u8bca\u65adI/O\u95ee\u9898\u3002\u7cfb\u7edf\u63d0\u4f9b\u8be6\u7ec6\u8bca\u65ad\u4f9d\u636e\u548c\u4ea4\u4e92\u5f0f\u754c\u9762\u3002", "result": "\u521b\u5efa\u9996\u4e2a\u5f00\u653e\u8bca\u65ad\u6d4b\u8bd5\u5957\u4ef6TraceBench\uff0c\u5305\u542b\u591a\u6837\u5316\u6807\u8bb0\u4f5c\u4e1a\u8ddf\u8e2a\u3002\u8bc4\u4f30\u663e\u793aIOAgent\u5728\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u4e0a\u5339\u914d\u6216\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684I/O\u8bca\u65ad\u5de5\u5177\uff0c\u4e14\u4e0d\u4f9d\u8d56\u7279\u5b9aLLM\uff0c\u5728\u4e13\u6709\u548c\u5f00\u6e90LLM\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "IOAgent\u80fd\u591f\u6709\u6548\u89e3\u51b3LLM\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u3001\u9886\u57df\u77e5\u8bc6\u7f3a\u4e4f\u548c\u5e7b\u89c9\u751f\u6210\u65b9\u9762\u7684\u6311\u6218\uff0c\u6709\u671b\u6210\u4e3a\u79d1\u5b66\u5bb6\u5e94\u5bf9\u590d\u6742HPC I/O\u5b50\u7cfb\u7edf\u7684\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2602.22103", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.22103", "abs": "https://arxiv.org/abs/2602.22103", "authors": ["Mao Lin", "Hyeran Jeon", "Keren Zhou"], "title": "PASTA: A Modular Program Analysis Tool Framework for Accelerators", "comment": null, "summary": "The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program AnalysiS Tool Framework for Accelerators. PASTA abstracts over low-level profiling APIs and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. Its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. We demonstrate the utility of PASTA by developing several analysis tools, including a deep learning workload characterization tool and a UVM optimization tool. Through extensive evaluation on mainstream deep learning workloads tested on NVIDIA and AMD GPUs under both single- and multi-GPU scenarios, we demonstrate PASTA's broad applicability. On NVIDIA GPUs, we further show that PASTA provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its GPU-accelerated backend. PASTA strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.", "AI": {"tldr": "PASTA\u662f\u4e00\u4e2a\u4f4e\u5f00\u9500\u3001\u6a21\u5757\u5316\u7684\u52a0\u901f\u5668\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u5e95\u5c42API\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\uff0c\u652f\u6301\u5feb\u901f\u5b9a\u5236\u5de5\u5177\u5f00\u53d1\uff0c\u5728GPU\u4e0a\u76f8\u6bd4\u4f20\u7edf\u5de5\u5177\u52a0\u901f\u9ad8\u8fbe1.3\u4e07\u500d\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u786c\u4ef6\u52a0\u901f\u5668\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u7075\u6d3b\u3001\u4f4e\u5f00\u9500\u7684\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u6765\u6ee1\u8db3\u4e0d\u540c\u52a0\u901f\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u9700\u6c42\u3002", "method": "PASTA\u901a\u8fc7\u62bd\u8c61\u5e95\u5c42\u6027\u80fd\u5206\u6790API\u548c\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u8fd0\u884c\u65f6\u4e8b\u4ef6\u6355\u83b7\u548c\u5206\u6790\u63a5\u53e3\u3002\u91c7\u7528\u53ef\u6269\u5c55\u8bbe\u8ba1\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u5feb\u901f\u539f\u578b\u5316\u5b9a\u5236\u5de5\u5177\uff0c\u5e76\u5229\u7528GPU\u52a0\u901f\u540e\u7aef\u964d\u4f4e\u5f00\u9500\u3002", "result": "\u5728NVIDIA\u548cAMD GPU\u4e0a\u5bf9\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u5355GPU\u548c\u591aGPU\u573a\u666f\u6d4b\u8bd5\uff0cPASTA\u5c55\u793a\u4e86\u5e7f\u6cdb\u9002\u7528\u6027\u3002\u5728NVIDIA GPU\u4e0a\uff0cPASTA\u63d0\u4f9b\u8be6\u7ec6\u6027\u80fd\u6d1e\u5bdf\u7684\u540c\u65f6\uff0c\u5f00\u9500\u663e\u8457\u964d\u4f4e\uff0c\u76f8\u6bd4\u4f20\u7edf\u5206\u6790\u5de5\u5177\u52a0\u901f\u9ad8\u8fbe1.3\u4e07\u500d\u3002", "conclusion": "PASTA\u5728\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5b9e\u7528\u5e73\u8861\uff0c\u975e\u5e38\u9002\u5408\u73b0\u4ee3\u57fa\u4e8e\u52a0\u901f\u5668\u7684\u8ba1\u7b97\u73af\u5883\uff0c\u4e3a\u52a0\u901f\u5668\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21858", "abs": "https://arxiv.org/abs/2602.21858", "authors": ["Dezhi Kong", "Zhengzhao Feng", "Qiliang Liang", "Hao Wang", "Haofei Sun", "Changpeng Yang", "Yang Li", "Peng Zhou", "Shuai Nie", "Hongzhen Wang", "Linfeng Zhou", "Hao Jia", "Jiaming Xu", "Runyu Shi", "Ying Huang"], "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.", "AI": {"tldr": "ProactiveMobile\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79fb\u52a8\u667a\u80fd\u4f53\u4e3b\u52a8\u667a\u80fd\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3,660\u4e2a\u5b9e\u4f8b\u300114\u4e2a\u573a\u666f\u548c63\u4e2aAPI\u51fd\u6570\uff0c\u901a\u8fc7\u5fae\u8c03\u7684Qwen2.5-VL-7B-Instruct\u6a21\u578b\u8fbe\u523019.15%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u4e3b\u8981\u5c40\u9650\u4e8e\u88ab\u52a8\u6267\u884c\u7528\u6237\u547d\u4ee4\u7684\u8303\u5f0f\uff0c\u800c\u4e3b\u52a8\u667a\u80fd\uff08\u667a\u80fd\u4f53\u81ea\u4e3b\u9884\u6d4b\u9700\u6c42\u5e76\u542f\u52a8\u884c\u52a8\uff09\u4ee3\u8868\u4e86\u4e0b\u4e00\u4ee3\u79fb\u52a8\u667a\u80fd\u4f53\u7684\u524d\u6cbf\u65b9\u5411\uff0c\u4f46\u5176\u53d1\u5c55\u53d7\u5230\u7f3a\u4e4f\u80fd\u591f\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u5e76\u63d0\u4f9b\u5ba2\u89c2\u53ef\u6267\u884c\u8bc4\u4f30\u7684\u57fa\u51c6\u6d4b\u8bd5\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faProactiveMobile\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u4e3b\u52a8\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\uff1a\u57fa\u4e8e\u8bbe\u5907\u4e0a\u4e0b\u6587\u4fe1\u53f7\u7684\u56db\u4e2a\u7ef4\u5ea6\u63a8\u65ad\u6f5c\u5728\u7528\u6237\u610f\u56fe\uff0c\u5e76\u4ece\u5305\u542b63\u4e2aAPI\u7684\u5168\u9762\u51fd\u6570\u6c60\u4e2d\u751f\u6210\u53ef\u6267\u884c\u51fd\u6570\u5e8f\u5217\u3002\u57fa\u51c6\u5305\u542b3,660\u4e2a\u5b9e\u4f8b\u300114\u4e2a\u573a\u666f\uff0c\u91c7\u7528\u591a\u7b54\u6848\u6807\u6ce8\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u590d\u6742\u6027\uff0c\u5e76\u753130\u540d\u4e13\u5bb6\u56e2\u961f\u8fdb\u884c\u6700\u7ec8\u5ba1\u6838\uff0c\u786e\u4fdd\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u903b\u8f91\u4e00\u81f4\u6027\u548c\u884c\u52a8\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u7684Qwen2.5-VL-7B-Instruct\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523019.15%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8eo1\u6a21\u578b\uff0815.71%\uff09\u548cGPT-5\u6a21\u578b\uff087.39%\uff09\u3002\u8fd9\u8868\u660e\u4e3b\u52a8\u667a\u80fd\u662f\u5f53\u524dMLLMs\u666e\u904d\u7f3a\u4e4f\u4f46\u53ef\u5b66\u4e60\u7684\u5173\u952e\u80fd\u529b\u3002", "conclusion": "\u4e3b\u52a8\u667a\u80fd\u662f\u79fb\u52a8\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u5173\u952e\u65b9\u5411\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u666e\u904d\u7f3a\u4e4f\u8fd9\u4e00\u80fd\u529b\u3002ProactiveMobile\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u4e3b\u52a8\u667a\u80fd\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u5de5\u5177\uff0c\u8bc1\u660e\u4e86\u4e3b\u52a8\u667a\u80fd\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u5f3a\u8c03\u4e86\u8be5\u57fa\u51c6\u5bf9\u4e3b\u52a8\u667a\u80fd\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
