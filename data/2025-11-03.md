<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 本文提出了CATArena评估平台，通过四款棋牌游戏的无上限评分系统来评估LLM智能体的学习能力，解决了现有基准测试中的分数饱和问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体基准测试主要评估固定场景下的端到端性能，存在分数饱和、对专家标注依赖性强等问题，无法有效评估智能体的学习能力。

Method: 提出迭代式竞争性同伴学习框架，让智能体通过重复交互和反馈优化策略；开发CATArena平台，包含四款开放式评分的棋牌游戏。

Result: 实验结果表明CATArena能够可靠、稳定、可扩展地评估智能体核心能力，特别是学习能力和策略编码能力。

Conclusion: CATArena平台为评估快速发展的智能体能力提供了持续动态的评估方法，特别关注学习能力这一核心驱动力。

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [2] [SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation](https://arxiv.org/abs/2510.26989)
*Agorakis Bompotas,Konstantinos Koutras,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Dimitra Gariza,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.AI

TL;DR: SUSTAINABLE是一个智能农业平台，整合物联网、人工智能、卫星成像和基于角色的任务编排，旨在实现高效、可追溯和可持续的农业，并以葡萄种植为试点用例。


<details>
  <summary>Details</summary>
Motivation: 全球农业部门面临日益增长的粮食需求、气候多变性和可持续实践需求，需要进行转型。

Method: 整合物联网、人工智能、卫星成像和基于角色的任务编排，特别针对地中海葡萄园进行卫星指数集成、实时环境数据和角色感知任务管理。

Result: 提出了SUSTAINABLE平台的关键特性，包括卫星指数集成、实时环境数据和角色感知任务管理。

Conclusion: SUSTAINABLE平台为农业提供了一种智能化的解决方案，能够支持高效、可追溯和可持续的农业生产。

Abstract: The global agricultural sector is undergoing a transformative shift, driven
by increasing food demands, climate variability and the need for sustainable
practices. SUSTAINABLE is a smart farming platform designed to integrate IoT,
AI, satellite imaging, and role-based task orchestration to enable efficient,
traceable, and sustainable agriculture with a pilot usecase in viticulture.
This paper explores current smart agriculture solutions, presents a comparative
evaluation, and introduces SUSTAINABLE's key features, including satellite
index integration, real-time environmental data, and role-aware task management
tailored to Mediterranean vineyards.

</details>


### [3] [Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models](https://arxiv.org/abs/2510.27009)
*Jared Junkin,Samuel Nathanson*

Main category: cs.AI

TL;DR: 研究表明，在棋盘状态数据上使用因果掩码训练的语言模型，其棋力表现优于在顺序移动数据上训练的模型，挑战了传统认为因果掩码不适用于空间数据的观点。


<details>
  <summary>Details</summary>
Motivation: 探讨在具有空间或关系结构的领域中，接受因果掩码带来的信息损失是否可行，以及是否比顺序线性化方法更优。

Method: 在国际象棋领域训练具有双向和因果自注意力机制的语言模型，分别使用空间（棋盘状态）和顺序（移动序列）数据。

Result: 在空间棋盘状态数据上训练的模型（即使使用因果掩码）始终比在顺序数据上训练的模型具有更强的棋力。

Conclusion: 将因果掩码应用于空间数据是训练单模态LLM的可行方法，在某些领域甚至优于顺序化方法。

Abstract: Language models are traditionally designed around causal masking. In domains
with spatial or relational structure, causal masking is often viewed as
inappropriate, and sequential linearizations are instead used. Yet the question
of whether it is viable to accept the information loss introduced by causal
masking on nonsequential data has received little direct study, in part because
few domains offer both spatial and sequential representations of the same
dataset. In this work, we investigate this issue in the domain of chess, which
naturally supports both representations. We train language models with
bidirectional and causal self-attention mechanisms on both spatial
(board-based) and sequential (move-based) data. Our results show that models
trained on spatial board states - \textit{even with causal masking} -
consistently achieve stronger playing strength than models trained on
sequential data. While our experiments are conducted on chess, our results are
methodological and may have broader implications: applying causal masking to
spatial data is a viable procedure for training unimodal LLMs on spatial data,
and in some domains is even preferable to sequentialization.

</details>


### [4] [e1: Learning Adaptive Control of Reasoning Effort](https://arxiv.org/abs/2510.27042)
*Michael Kleinman,Matthew Trager,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出自适应努力控制方法，通过强化学习训练模型使用用户指定的相对思维预算比例，实现动态成本-准确性权衡控制，在保持性能的同时显著减少思维链长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要用户预先指定绝对token数量，但用户难以事先知道问题难度来合理设置token预算。需要更细粒度的控制机制来平衡输出质量与延迟成本。

Method: 基于强化学习的自适应努力控制方法，训练模型使用相对于当前平均思维链长度的用户指定比例token，无需数据集和阶段特定调优。

Result: 在1.5B到32B参数规模的模型上，该方法能够实现约3倍的思维链长度减少，同时保持或改善相对于RL训练基础模型的性能。

Conclusion: 自适应努力控制方法有效解决了思维预算控制的难题，通过连续努力参数实现动态成本-准确性权衡，模型自动学习按任务难度比例分配资源。

Abstract: Increasing the thinking budget of AI models can significantly improve
accuracy, but not all questions warrant the same amount of reasoning. Users may
prefer to allocate different amounts of reasoning effort depending on how they
value output quality versus latency and cost. To leverage this tradeoff
effectively, users need fine-grained control over the amount of thinking used
for a particular query, but few approaches enable such control. Existing
methods require users to specify the absolute number of desired tokens, but
this requires knowing the difficulty of the problem beforehand to appropriately
set the token budget for a query. To address these issues, we propose Adaptive
Effort Control, a self-adaptive reinforcement learning method that trains
models to use a user-specified fraction of tokens relative to the current
average chain-of-thought length for each query. This approach eliminates
dataset- and phase-specific tuning while producing better cost-accuracy
tradeoff curves compared to standard methods. Users can dynamically adjust the
cost-accuracy trade-off through a continuous effort parameter specified at
inference time. We observe that the model automatically learns to allocate
resources proportionally to the task difficulty and, across model scales
ranging from 1.5B to 32B parameters, our approach enables approximately 3x
reduction in chain-of-thought length while maintaining or improving performance
relative to the base model used for RL training.

</details>


### [5] [Discriminative Rule Learning for Outcome-Guided Process Model Discovery](https://arxiv.org/abs/2510.27343)
*Ali Norouzifar,Wil van der Aalst*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结果感知的过程发现方法，通过区分理想和不理想的过程执行轨迹，分别学习可解释的判别规则，从而生成更聚焦和可解释的过程模型。


<details>
  <summary>Details</summary>
Motivation: 传统过程发现方法不考虑执行结果，导致模型无法捕捉理想和不理想行为之间的关键差异，不适合一致性检查和性能分析。区分这两种行为有助于理解过程结果的关键驱动因素。

Method: 通过学习控制流特征上的可解释判别规则，将具有相似理想性特征的轨迹分组，然后在每个组内分别应用过程发现技术。

Result: 该方法在多个真实事件日志上得到验证，能够有效分离和可视化关键过程模式，生成更聚焦和可解释的过程模型。

Conclusion: 结果感知的过程发现方法能够揭示理想和不理想执行的关键驱动因素，为过程改进提供更有价值的见解，并已实现为公开可用的工具。

Abstract: Event logs extracted from information systems offer a rich foundation for
understanding and improving business processes. In many real-world
applications, it is possible to distinguish between desirable and undesirable
process executions, where desirable traces reflect efficient or compliant
behavior, and undesirable ones may involve inefficiencies, rule violations,
delays, or resource waste. This distinction presents an opportunity to guide
process discovery in a more outcome-aware manner. Discovering a single process
model without considering outcomes can yield representations poorly suited for
conformance checking and performance analysis, as they fail to capture critical
behavioral differences. Moreover, prioritizing one behavior over the other may
obscure structural distinctions vital for understanding process outcomes. By
learning interpretable discriminative rules over control-flow features, we
group traces with similar desirability profiles and apply process discovery
separately within each group. This results in focused and interpretable models
that reveal the drivers of both desirable and undesirable executions. The
approach is implemented as a publicly available tool and it is evaluated on
multiple real-life event logs, demonstrating its effectiveness in isolating and
visualizing critical process patterns.

</details>


### [6] [An In-depth Study of LLM Contributions to the Bin Packing Problem](https://arxiv.org/abs/2510.27353)
*Julien Herrmann,Guillaume Pallez*

Main category: cs.AI

TL;DR: 对LLM在数学发现中贡献的重新评估，发现LLM生成的装箱问题启发式算法虽然可读但难以解释，作者提出了更简单、高效、可解释的新算法，并指出需要更严谨地评估LLM生成内容的科学价值。


<details>
  <summary>Details</summary>
Motivation: 重新评估LLM在数学发现中的贡献，特别是针对装箱问题中LLM生成的启发式算法的有效性和可解释性。

Method: 详细分析LLM生成的启发式算法行为，提出针对特定装箱问题实例的新算法类别，并进行比较验证。

Result: 提出的新算法比LLM生成的算法更简单、高效、可解释且更具泛化性，表明原问题实例本身相对简单。

Conclusion: LLM在数学发现中的贡献需要更严谨的验证和情境化评估，当前声称可能基于对问题实例研究程度的错误假设。

Abstract: Recent studies have suggested that Large Language Models (LLMs) could provide
interesting ideas contributing to mathematical discovery. This claim was
motivated by reports that LLM-based genetic algorithms produced heuristics
offering new insights into the online bin packing problem under uniform and
Weibull distributions. In this work, we reassess this claim through a detailed
analysis of the heuristics produced by LLMs, examining both their behavior and
interpretability. Despite being human-readable, these heuristics remain largely
opaque even to domain experts. Building on this analysis, we propose a new
class of algorithms tailored to these specific bin packing instances. The
derived algorithms are significantly simpler, more efficient, more
interpretable, and more generalizable, suggesting that the considered instances
are themselves relatively simple. We then discuss the limitations of the claim
regarding LLMs' contribution to this problem, which appears to rest on the
mistaken assumption that the instances had previously been studied. Our
findings instead emphasize the need for rigorous validation and
contextualization when assessing the scientific value of LLM-generated outputs.

</details>


### [7] [Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints](https://arxiv.org/abs/2510.27383)
*Yueyang Wang,Mehmet Dogar,Gustav Markkula*

Main category: cs.AI

TL;DR: 提出了一种集成视觉和运动约束的多智能体强化学习框架，用于模拟行人-驾驶员交互，在真实世界无信号人行横道数据集上验证了约束模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于规则的逻辑、博弈论模型或'黑盒'机器学习，缺乏灵活性且忽视了感知和运动约束等底层机制对行人-驾驶员交互行为的影响。

Method: 使用多智能体强化学习框架，集成行人和驾驶员智能体的视觉与运动约束，评估四种模型变体：无约束、仅运动约束、仅视觉约束、两者皆有。

Result: 同时包含视觉和运动约束的模型表现最佳。运动约束产生更平滑的运动，类似人类在穿越交互中的速度调整；视觉约束引入感知不确定性和视野限制，使智能体表现出更谨慎和可变的行为。

Conclusion: 多智能体强化学习结合人类约束是模拟真实道路用户交互的有前景建模方法，能够在小数据集情况下有效工作，并通过建模约束参数为群体级分布来考虑个体差异。

Abstract: Modelling pedestrian-driver interactions is critical for understanding human
road user behaviour and developing safe autonomous vehicle systems. Existing
approaches often rely on rule-based logic, game-theoretic models, or
'black-box' machine learning methods. However, these models typically lack
flexibility or overlook the underlying mechanisms, such as sensory and motor
constraints, which shape how pedestrians and drivers perceive and act in
interactive scenarios. In this study, we propose a multi-agent reinforcement
learning (RL) framework that integrates both visual and motor constraints of
pedestrian and driver agents. Using a real-world dataset from an unsignalised
pedestrian crossing, we evaluate four model variants, one without constraints,
two with either motor or visual constraints, and one with both, across
behavioural metrics of interaction realism. Results show that the combined
model with both visual and motor constraints performs best. Motor constraints
lead to smoother movements that resemble human speed adjustments during
crossing interactions. The addition of visual constraints introduces perceptual
uncertainty and field-of-view limitations, leading the agents to exhibit more
cautious and variable behaviour, such as less abrupt deceleration. In this
data-limited setting, our model outperforms a supervised behavioural cloning
model, demonstrating that our approach can be effective without large training
datasets. Finally, our framework accounts for individual differences by
modelling parameters controlling the human constraints as population-level
distributions, a perspective that has not been explored in previous work on
pedestrian-vehicle interaction modelling. Overall, our work demonstrates that
multi-agent RL with human constraints is a promising modelling approach for
simulating realistic road user interactions.

</details>


### [8] [DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains](https://arxiv.org/abs/2510.27419)
*Tian Liang,Wenxiang Jiao,Zhiwei He,Jiahao Xu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: DeepCompress是一个新颖框架，通过自适应长度奖励机制同时提升大型推理模型的准确性和效率，针对简单问题鼓励短推理路径，对困难问题促进长推理路径。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在认知效率问题：对简单问题"过度思考"，对复杂问题"思考不足"。现有方法使用监督微调或带令牌长度奖励的强化学习来提高效率，但往往以牺牲准确性为代价。

Method: DeepCompress采用自适应长度奖励机制，基于模型实时演化的能力动态将问题分类为"简单"或"困难"。对简单问题鼓励更短、更高效的推理，对困难问题促进更长、更具探索性的思维链。

Result: 在具有挑战性的数学基准测试中，DeepCompress始终优于基线方法，在显著提高令牌效率的同时实现了更优的准确性。

Conclusion: DeepCompress框架通过自适应调整推理链长度，能够同时提升大型推理模型的准确性和效率，证明了针对不同难度问题采用不同推理策略的有效性。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking'' simple problems and
``underthinking'' complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on
the model's evolving capability. It encourages shorter, more efficient
reasoning for ``Simple'' problems while promoting longer, more exploratory
thought chains for ``Hard'' problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.

</details>


### [9] [GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language](https://arxiv.org/abs/2510.27448)
*Yuhao Zhang,Dingxin Hu,Tinghao Yu,Hao Liu,Yiting Liu*

Main category: cs.AI

TL;DR: GeoFM是一种新的几何数据合成方法，使用形式语言在度量空间中探索条件组合，通过符号引擎确保正确性，生成的合成数据在几何问题解决任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在数学几何推理方面面临高质量几何数据稀缺的挑战，现有合成几何数据方法生成的数据缺乏多样性、噪声多，且几何图像变化有限、与真实几何图差异大。

Method: 提出GeoFM方法，使用形式语言在度量空间中探索条件组合，生成高保真几何问题，并通过符号引擎确保正确性。

Result: 实验结果显示，使用GeoFM合成数据训练的模型在MathVista几何问题解决任务上比GPT-4o高出18.7%，在GeoQA上高出16.5%；在开源模型上分别高出5.7%和2.7%。

Conclusion: GeoFM方法能有效生成高质量、多样化的几何数据，显著提升多模态大语言模型在几何推理任务上的性能。

Abstract: Multi-modal Large Language Models (MLLMs) have gained significant attention
in both academia and industry for their capabilities in handling multi-modal
tasks. However, these models face challenges in mathematical geometric
reasoning due to the scarcity of high-quality geometric data. To address this
issue, synthetic geometric data has become an essential strategy. Current
methods for generating synthetic geometric data involve rephrasing or expanding
existing problems and utilizing predefined rules and templates to create
geometric images and problems. However, these approaches often produce data
that lacks diversity or is prone to noise. Additionally, the geometric images
synthesized by existing methods tend to exhibit limited variation and deviate
significantly from authentic geometric diagrams. To overcome these limitations,
we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses
formal languages to explore combinations of conditions within metric space,
generating high-fidelity geometric problems that differ from the originals
while ensuring correctness through a symbolic engine. Experimental results show
that our synthetic data significantly outperforms existing methods. The model
trained with our data surpass the proprietary GPT-4o model by 18.7\% on
geometry problem-solving tasks in MathVista and by 16.5\% on GeoQA.
Additionally, it exceeds the performance of a leading open-source model by
5.7\% on MathVista and by 2.7\% on GeoQA.

</details>


### [10] [SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning](https://arxiv.org/abs/2510.27568)
*Ali Asgarov,Umid Suleymanov,Aadyant Khatri*

Main category: cs.AI

TL;DR: SIGMA是一个多智能体检索增强框架，通过协调专门智能体进行独立推理、定向搜索和结果合成，显著提升数学推理任务的性能，在多个基准测试中优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强模型存在单视角依赖、搜索策略不灵活、多源信息整合困难等问题，无法有效解决复杂的数学推理任务。

Method: 引入SIGMA框架，通过专门智能体独立推理并生成假设性段落优化检索，使用协调机制整合多智能体发现，实现上下文敏感且计算高效的知识集成。

Result: 在MATH500、AIME和GPQA等挑战性基准测试中，SIGMA持续优于开源和闭源系统，绝对性能提升7.4%。

Conclusion: 多智能体按需知识集成能显著提升推理准确性和效率，为复杂知识密集型问题解决提供了可扩展的方法。

Abstract: Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.

</details>


### [11] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: BEAT是首个针对MLLM驱动的具身智能体的视觉后门攻击框架，使用环境中的物体作为触发条件，能够在触发条件出现时持续执行攻击者指定的多步策略。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLM)驱动的具身智能体虽然实现了从视觉输入直接感知、推理和规划任务导向动作的能力，但也开启了新的攻击面：视觉后门攻击。当场景中出现视觉触发条件时，智能体会持续执行攻击者指定的策略。

Method: BEAT通过(1)构建跨越多样化场景、任务和触发条件放置的训练集来暴露智能体于触发条件变化；(2)引入两阶段训练方案：首先应用监督微调(SFT)，然后使用新颖的对比触发学习(CTL)，将触发条件判别建模为触发条件存在和不存在输入之间的偏好学习。

Result: 在各种具身智能体基准测试和MLLM上，BEAT实现了高达80%的攻击成功率，同时保持强大的良性任务性能，并能可靠地泛化到分布外触发条件放置。与朴素SFT相比，CTL在有限后门数据下将后门激活准确率提升高达39%。

Conclusion: 这些发现揭示了MLLM驱动的具身智能体中一个关键但未被探索的安全风险，强调了在现实世界部署前需要强大的防御措施。

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


### [12] [Validity Is What You Need](https://arxiv.org/abs/2510.27628)
*Sebastian Benthall,Andrew Clark*

Main category: cs.AI

TL;DR: 本文提出了Agentic AI的新现实主义定义，将其视为在复杂企业环境中自主工作的软件交付机制，强调其成功依赖于终端用户和主要利益相关者的验证，而非仅仅依赖大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，Agentic AI系统引起了广泛关注，但现有定义不够准确。作者旨在提供一个更现实的定义，强调Agentic AI主要是应用程序而非基础模型，其成功关键在于用户验证。

Method: 通过比较Agentic AI与软件即服务(SaaS)的相似性，提出新的现实主义定义，并分析验证工具与基础模型评估工具的区别。

Result: 研究发现，在建立良好验证机制的情况下，许多Agentic AI系统可以用更简单、快速和可解释的模型替代大型语言模型来处理核心逻辑。

Conclusion: Agentic AI的成功关键在于有效性验证，大型语言模型只是实现这一目标的可能选项之一，而非必需组件。

Abstract: While AI agents have long been discussed and studied in computer science,
today's Agentic AI systems are something new. We consider other definitions of
Agentic AI and propose a new realist definition. Agentic AI is a software
delivery mechanism, comparable to software as a service (SaaS), which puts an
application to work autonomously in a complex enterprise setting. Recent
advances in large language models (LLMs) as foundation models have driven
excitement in Agentic AI. We note, however, that Agentic AI systems are
primarily applications, not foundations, and so their success depends on
validation by end users and principal stakeholders. The tools and techniques
needed by the principal users to validate their applications are quite
different from the tools and techniques used to evaluate foundation models.
Ironically, with good validation measures in place, in many cases the
foundation models can be replaced with much simpler, faster, and more
interpretable models that handle core logic. When it comes to Agentic AI,
validity is what you need. LLMs are one option that might achieve it.

</details>


### [13] [Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training](https://arxiv.org/abs/2510.27630)
*Dayuan Fu,Yunze Wu,Xiaojie Cai,Lyumanshan Ye,Shijie Xia,Zhen Huang,Weiye Si,Tianze Xu,Jie Sun,Keyu Li,Mohan Jiang,Junfei Wang,Qishuo Hua,Pengrui Lu,Yang Xiao,Pengfei Liu*

Main category: cs.AI

TL;DR: Apollo是一个集成异步人类指导与动作级数据过滤的采样框架，用于训练LLM智能体处理长周期、领域专业化任务，相比传统方法显著提升了训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLM智能体的方法存在两个主要问题：基于行为克隆的方法需要密集的人工标注，成本过高；基于结果驱动的采样方法在领域专业化任务中容易失败，因为有效正轨迹稀少。

Method: Apollo框架结合异步人类指导和动作级数据过滤，允许人类只在智能体偏离正确轨迹时进行干预，提供先验知识和策略建议，然后通过监督控制过滤次优动作防止错误传播。

Result: 在InnovatorBench上的实验表明，使用Apollo训练的GLM-4.5模型相比未训练基线提升了50%以上，相比无人交互训练变体提升了28%。

Conclusion: Apollo证明了人类在环采样在长周期、领域专业化任务中的关键作用，其设计在处理这类任务时具有鲁棒性。

Abstract: Large Language Model (LLM) agents have recently shown strong potential in
domains such as automated coding, deep research, and graphical user interface
manipulation. However, training them to succeed on long-horizon,
domain-specialized tasks remains challenging. Current methods primarily fall
into two categories. The first relies on dense human annotations through
behavior cloning, which is prohibitively expensive for long-horizon tasks that
can take days or months. The second depends on outcome-driven sampling, which
often collapses due to the rarity of valid positive trajectories on
domain-specialized tasks. We introduce Apollo, a sampling framework that
integrates asynchronous human guidance with action-level data filtering.
Instead of requiring annotators to shadow every step, Apollo allows them to
intervene only when the agent drifts from a promising trajectory, by providing
prior knowledge, strategic advice, etc. This lightweight design makes it
possible to sustain interactions for over 30 hours and produces valuable
trajectories at a lower cost. Apollo then applies supervision control to filter
out sub-optimal actions and prevent error propagation. Together, these
components enable reliable and effective data collection in long-horizon
environments. To demonstrate the effectiveness of Apollo, we evaluate it using
InnovatorBench. Our experiments show that when applied to train the GLM-4.5
model on InnovatorBench, Apollo achieves more than a 50% improvement over the
untrained baseline and a 28% improvement over a variant trained without human
interaction. These results highlight the critical role of human-in-the-loop
sampling and the robustness of Apollo's design in handling long-horizon,
domain-specialized tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies](https://arxiv.org/abs/2510.26985)
*Mostafa Darvishi*

Main category: cs.AR

TL;DR: 本文深入分析了FPGA和ASIC中的时序收敛挑战与约束，通过比较Xilinx Kintex UltraScale+ FPGA和7nm ASIC的案例研究，展示了两种技术的时序分析和性能权衡。


<details>
  <summary>Details</summary>
Motivation: 研究FPGA和ASIC在时序收敛方面的核心挑战和约束，理解两种技术架构差异对时序行为的影响，为高性能设计提供时序分析指导。

Method: 分析核心时序原理、架构差异和设计方法学，通过XCKU040 FPGA与7nm ASIC的案例研究进行实际时序分析和性能比较。

Result: 实验结果显示ASIC实现更优的时序性能（建立时间45ps，保持时间35ps），而现代FPGA仍具有竞争力（建立时间180ps，保持时间120ps），验证了FPGA在高性能设计中的适用性。

Conclusion: ASIC在时序性能方面优于FPGA，但现代FPGA仍能满足高性能设计要求，为设计选择提供了有价值的参考依据。

Abstract: This paper presents an in-depth analysis of timing closure challenges and
constraints in Field Programmable Gate Arrays (FPGAs) and Application Specific
Integrated Circuits (ASICs). We examine core timing principles, architectural
distinctions, and design methodologies influencing timing behavior in both
technologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA
(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance
trade-offs. Experimental results show ASICs achieve superior timing of 45ps
setup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and
120ps hold times, validating their suitability for high-performance designs.

</details>


### [15] [Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review](https://arxiv.org/abs/2510.27070)
*Dong Tong*

Main category: cs.AR

TL;DR: 本文对基于描述符的对象感知内存系统进行了全面调查，该架构范式旨在弥合硬件/软件接口间的语义鸿沟，通过将描述符提升为一级架构抽象，使硬件能够动态获取和执行软件定义对象的丰富语义。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统的安全性和效率因缺乏原生架构机制来传播高级程序语义（如对象身份、边界和生命周期）而受到根本性损害。

Method: 建立了内存对象和描述符的基础概念，引入了描述符寻址模式的新分类法，提供了分析和比较不同实现的结构化框架。以CentroID模型为案例研究，展示其混合标记指针编码和描述符处理机制。

Result: 统一分析揭示了该范式如何全面解决内存保护、管理和处理相互交织的挑战。

Conclusion: 明确的对象语义跨层通信为下一代缓存层次结构、统一虚拟内存甚至128位架构提供了基础研究方向。

Abstract: The security and efficiency of modern computing systems are fundamentally
undermined by the absence of a native architectural mechanism to propagate
high-level program semantics, such as object identity, bounds, and lifetime,
across the hardware/software interface. This paper presents a comprehensive
survey of the architectural paradigm designed to bridge this semantic gap:
descriptor-based, object-aware memory systems. By elevating the descriptor to a
first-class architectural abstraction, this paradigm enables hardware to
dynamically acquire and enforce the rich semantics of software-defined objects.
This survey systematically charts the evolution and current landscape of this
approach. We establish the foundational concepts of memory objects and
descriptors and introduce a novel taxonomy of descriptor addressing modes,
providing a structured framework for analyzing and comparing diverse
implementations. Our unified analysis reveals how this paradigm holistically
addresses the intertwined challenges of memory protection, management, and
processing. As a culminating case study, we re-examine the CentroID model,
demonstrating how its hybrid tagged-pointer encoding and descriptor processing
mechanisms embody the path toward practical and efficient object-aware designs.
Finally, we outline how the explicit cross-layer communication of object
semantics provides a foundational research direction for next-generation cache
hierarchies, unified virtual memory, and even 128-bit architectures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)
*Junyi Shen,Noppanat Wadlom,Lingfeng Zhou,Dequan Wang,Xu Miao,Lei Fang,Yao Lu*

Main category: cs.DC

TL;DR: FlowMesh是一个多租户服务架构，将AI工作负载作为共享服务执行和优化，而非隔离的流水线。它通过分解工作流为细粒度算子、记录数据血缘、跨用户去重和硬件批处理请求，实现成本降低3.8倍、能耗降低2.0倍，并在动态和故障条件下保持高效。


<details>
  <summary>Details</summary>
Motivation: AI部署越来越像数据转换、微调和智能体交互的流水线，而非单一的LLM任务。为应对这一转变，需要一种能够执行和优化这些工作负载的共享服务架构。

Method: 将工作流分解为细粒度算子并记录血缘；使用全局控制平面维护集群范围内的算子池，通过单一效用函数选择批次和工作节点；数据平面采用无状态工作节点和内容寻址存储，支持弹性扩展、安全重试和跨集群可移植性。

Result: 与基线解决方案相比，FlowMesh实现高达3.8倍的成本降低和2.0倍的能耗降低，提供相似或更好的延迟特性，在动态和故障条件下保持高效。

Conclusion: FlowMesh通过多租户服务架构有效优化AI工作负载执行，显著降低成本、能耗，并保持高性能和可靠性，适用于Kubernetes和分布式GPU市场等环境。

Abstract: AI deployment increasingly resembles a pipeline of data transformation,
fine-tuning, and agent interactions rather than a monolithic LLM job; recent
examples include RLHF/RLAIF training and agentic workflows. To cope with this
shift, we propose FlowMesh, a multi-tenant service fabric that executes and
optimizes these workloads as one shared service instead of isolated pipelines.
It decomposes workflows into fine-grained operators with recorded lineage,
enabling de-duplication of work across users and batching requests on the same
hardware while preserving per-workflow provenance. A global control plane
maintains a cluster-wide pool of ready operators and uses a single utility
function to pick both the batch and the worker, balancing throughput, cost, and
data locality on heterogeneous GPUs. The data plane is an elastic fleet of
stateless workers backed by a content-addressable store, enabling rapid,
automatic scale-out, safe retry after preemption, and portability across
managed clusters such as Kubernetes and geo-distributed GPU marketplaces such
as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost
reduction and 2.0x lower energy usage, provides a similar or better latency
profile, and remains efficient under dynamic and failure-prone conditions.

</details>


### [17] [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)
*Mengshi Qi,Jiaxuan Peng,Jie Zhang,Juan Zhu,Yong Li,Huadong Ma*

Main category: cs.DC

TL;DR: 提出了一种协同张量和流水线并行调度方法，通过将前向和后向传播解耦为细粒度计算单元并交织编排，同时减少TP通信开销和PP流水线气泡，显著提升LLM和MLLM训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有混合并行方案中，张量并行(TP)带来大量集体通信开销，流水线并行(PP)存在同步效率低下的流水线气泡问题。现有工作多从孤立角度解决这些问题，缺乏协同优化。

Method: 将PP中的前向和后向传播解耦为细粒度计算单元，通过交织编排形成复合计算序列，实现TP相关气泡的近乎完全消除，并在此基础上设计PP调度以最小化PP气泡。

Result: 实验结果表明，相比现有调度方法，该方法对LLM训练吞吐量提升达12%，对MLLM提升达16%。

Conclusion: 提出的协同张量和流水线并行调度能有效同时减少两种类型的气泡，显著提升大规模模型训练效率。

Abstract: In the machine learning system, the hybrid model parallelism combining tensor
parallelism (TP) and pipeline parallelism (PP) has become the dominant solution
for distributed training of Large Language Models~(LLMs) and Multimodal LLMs
(MLLMs). However, TP introduces significant collective communication overheads,
while PP suffers from synchronization inefficiencies such as pipeline bubbles.
Existing works primarily address these challenges from isolated perspectives,
focusing either on overlapping TP communication or on flexible PP scheduling to
mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor
and pipeline parallelism schedule that simultaneously reduces both types of
bubbles. Our proposed schedule decouples the forward and backward passes in PP
into fine-grained computation units, which are then braided to form a composite
computation sequence. This compositional structure enables near-complete
elimination of TP-related bubbles. Building upon this structure, we further
design the PP schedule to minimize PP bubbles. Experimental results demonstrate
that our approach improves training throughput by up to 12% for LLMs and 16%
for MLLMs compared to existing scheduling methods. Our source code is avaiable
at https://github.com/MICLAB-BUPT/STP.

</details>


### [18] [Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing](https://arxiv.org/abs/2510.27317)
*Shuyi Chen,Panagiotis Oikonomou,Zhengchang Hua,Nikos Tziritas,Karim Djemame,Nan Zhang,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 提出一种在线策略，用于完全由能量收集供电的多接入边缘计算系统，通过动态调度计算任务和管理能耗来平衡间歇性能源供应与动态用户需求。


<details>
  <summary>Details</summary>
Motivation: 多接入边缘计算系统与可再生能源收集技术结合，在无电网供电区域运行，但间歇性能源供应与动态用户需求之间的平衡是一个重要挑战。

Method: 提出在线策略，动态调度具有依赖关系的计算任务，通过实时决策服务器频率调整和服务模块迁移来管理能耗。

Result: 使用真实世界数据集的实验表明，该算法在有效利用收集能量的同时保持低服务延迟。

Conclusion: 所提出的策略能够有效解决能量收集供电的边缘计算系统中的资源分配挑战。

Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting
applications near end-users. To promote sustainability, these systems are
increasingly integrated with renewable Energy Harvesting (EH) technologies,
enabling operation where grid electricity is unavailable. However, balancing
the intermittent nature of harvested energy with dynamic user demand presents a
significant resource allocation challenge. This work proposes an online
strategy for an MEC system powered exclusively by EH to address this trade-off.
Our strategy dynamically schedules computational tasks with dependencies and
governs energy consumption through real-time decisions on server frequency
scaling and service module migration. Experiments using real-world datasets
demonstrate our algorithm's effectiveness in efficiently utilizing harvested
energy while maintaining low service latency.

</details>


### [19] [ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method](https://arxiv.org/abs/2510.27351)
*Milena Veneva*

Main category: cs.DC

TL;DR: 本文提出了一种基于机器学习的启发式方法，用于寻找并行分区算法CUDA实现中的最优子系统大小。通过k近邻分类方法建立预测模型，并将该方法扩展到递归并行分区算法中。


<details>
  <summary>Details</summary>
Motivation: 为了在CUDA实现的并行分区算法中找到最优的子系统大小，提高计算效率，避免通过经验方法确定参数的低效性。

Method: 对不同规模的线性代数方程组进行计算实验，通过k近邻分类方法建立子系统大小的预测模型，并将该方法扩展到递归并行分区算法中，构建递归步数预测模型。

Result: 通过比较预测值与实际数据，算法表现良好，能够有效预测最优子系统大小和递归步数。

Conclusion: 基于机器学习的启发式方法能够有效预测并行分区算法的最优参数配置，为CUDA实现提供了实用的参数选择工具。

Abstract: This paper presents a machine learning (ML)-based heuristic for finding the
optimum sub-system size for the CUDA implementation of the parallel partition
algorithm. Computational experiments for different system of linear algebraic
equation (SLAE) sizes are conducted, and the optimum sub-system size for each
of them is found empirically. To estimate a model for the sub-system size, we
perform the k-nearest neighbors (kNN) classification method. Statistical
analysis of the results is done. By comparing the predicted values with the
actual data, the algorithm is deemed to be acceptably good. Next, the heuristic
is expanded to work for the recursive parallel partition algorithm as well. An
algorithm for determining the optimum sub-system size for each recursive step
is formulated. A kNN model for predicting the optimum number of recursive steps
for a particular SLAE size is built.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: CPT-Filtering是一种新颖的模型无关防护技术，利用BPE分词器的内在特性来检测和过滤使用密码和字符级编码的越狱攻击，具有低成本和高准确率的特点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到使用密码和字符级编码的越狱攻击，这些攻击能绕过安全防护机制。现有方法计算成本高，需要额外的LLM或困惑度模型。

Method: 基于BPE分词器在自然语言训练中的特性：对于分布外文本（如密码），分词器会使用更多更短的token。通过计算文本中平均每个token的字符数（CPT）来识别编码文本。

Result: 在超过10万个提示的大规模数据集上验证，测试了多种编码方案和流行分词器。实验表明简单的CPT阈值能高精度地识别编码文本，即使对于非常短的输入也有效。

Conclusion: CPT-Filtering提供了一种实用的防御层，可立即部署用于实时文本过滤和离线数据整理，具有模型无关、成本极低和近乎完美的准确性。

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


### [21] [LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks](https://arxiv.org/abs/2510.26941)
*Seif Ikbarieh,Maanak Gupta,Elmahedi Mahalal*

Main category: cs.CR

TL;DR: 提出了一种结合机器学习进行多类攻击检测与大型语言模型进行攻击行为分析和缓解建议的混合框架，并引入了新的评估指标来量化评估AI模型在物联网安全中的有效性。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展扩大了攻击面和安全漏洞，虽然人工智能在保护物联网安全方面发挥着关键作用，但现有评估缺乏标准化的客观基准来定量衡量基于AI的攻击分析和缓解效果。

Method: 结合机器学习分类器进行多类攻击检测，使用结构化角色扮演提示工程和检索增强生成技术指导ChatGPT-o3和DeepSeek-R1生成详细的情境感知响应，并引入新的评估指标和法官LLM集合进行独立评估。

Result: 随机森林表现出最佳检测性能，ChatGPT-o3在攻击分析和缓解方面优于DeepSeek-R1。

Conclusion: 提出的混合框架和量化评估方法为AI在物联网安全中的有效性提供了标准化评估基准，证明了机器学习与大型语言模型结合在物联网安全中的潜力。

Abstract: The Internet of Things has expanded rapidly, transforming communication and
operations across industries but also increasing the attack surface and
security breaches. Artificial Intelligence plays a key role in securing IoT,
enabling attack detection, attack behavior analysis, and mitigation suggestion.
Despite advancements, evaluations remain purely qualitative, and the lack of a
standardized, objective benchmark for quantitatively measuring AI-based attack
analysis and mitigation hinders consistent assessment of model effectiveness.
In this work, we propose a hybrid framework combining Machine Learning (ML) for
multi-class attack detection with Large Language Models (LLMs) for attack
behavior analysis and mitigation suggestion. After benchmarking several ML and
Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we
applied structured role-play prompt engineering with Retrieval-Augmented
Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,
context-aware responses. We introduce novel evaluation metrics for quantitative
assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,
DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon
H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the
responses. Results show that Random Forest has the best detection model, and
ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.

</details>


### [22] [Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures](https://arxiv.org/abs/2510.27190)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: 本文提出了一个包含41种商业大语言模型重复风险模式的机制中心分类法，揭示了输入被非中立解释、触发实现形响应或意外状态变化等问题，建议采用零信任架构原则来缓解跨阶段漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地集成到自动化多阶段管道中，处理阶段间未经验证的信任所产生的风险模式成为实际问题，需要系统性地识别和解决这些架构性故障模式。

Method: 开发了一个机制中心的分类法来识别41种重复风险模式，分析输入的非中立解释和意外响应行为，并提出零信任架构原则（包括来源验证、上下文密封和计划重新验证）作为防御措施。

Result: 分析表明输入经常被非中立解释，即使没有明确指令也可能触发实现形响应或意外状态变化，字符串级过滤不足以解决这些架构性故障模式。

Conclusion: 大语言模型存在架构性故障模式，需要采用零信任架构原则（如来源验证、上下文密封和计划重新验证）来缓解跨阶段漏洞，并提出了"Countermind"作为实施这些防御的概念蓝图。

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated,
multi-stage pipelines, risk patterns that arise from unvalidated trust between
processing stages become a practical concern. This paper presents a
mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs.
The analysis shows that inputs are often interpreted non-neutrally and can
trigger implementation-shaped responses or unintended state changes even
without explicit commands. We argue that these behaviors constitute
architectural failure modes and that string-level filtering alone is
insufficient. To mitigate such cross-stage vulnerabilities, we recommend
zero-trust architectural principles, including provenance enforcement, context
sealing, and plan revalidation, and we introduce "Countermind" as a conceptual
blueprint for implementing these defenses.

</details>


### [23] [Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents](https://arxiv.org/abs/2510.27275)
*Kathrin Grosse,Nico Ebert*

Main category: cs.CR

TL;DR: 对3270名英国成年人的调查显示，三分之一的AI对话助手常规用户存在可能引发攻击的行为，四分之一尝试过越狱，大多数用户不了解数据使用政策。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，用户可能无意中引入安全风险，但此类用户行为的实际发生程度尚不清楚。

Method: 2024年通过Prolific平台对3270名英国成年人进行代表性抽样调查。

Result: 三分之一的常规用户表现出可能引发攻击的行为；四分之一尝试过越狱；半数用户表示会清理数据；大多数用户不了解数据可用于训练模型且可退出。

Conclusion: 当前学术威胁模型在现实中存在，需要开发安全使用指南，供应商需加强敏感数据防护并提高数据使用政策的透明度。

Abstract: Recent improvement gains in large language models (LLMs) have lead to
everyday usage of AI-based Conversational Agents (CAs). At the same time, LLMs
are vulnerable to an array of threats, including jailbreaks and, for example,
causing remote code execution when fed specific inputs. As a result, users may
unintentionally introduce risks, for example, by uploading malicious files or
disclosing sensitive information. However, the extent to which such user
behaviors occur and thus potentially facilitate exploits remains largely
unclear. To shed light on this issue, we surveyed a representative sample of
3,270 UK adults in 2024 using Prolific. A third of these use CA services such
as ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a
third exhibited behaviors that may enable attacks, and a fourth have tried
jailbreaking (often out of understandable reasons such as curiosity, fun or
information seeking). Half state that they sanitize data and most participants
report not sharing sensitive data. However, few share very sensitive data such
as passwords. The majority are unaware that their data can be used to train
models and that they can opt-out. Our findings suggest that current academic
threat models manifest in the wild, and mitigations or guidelines for the
secure usage of CAs should be developed. In areas critical to security and
privacy, CAs must be equipped with effective AI guardrails to prevent, for
example, revealing sensitive information to curious employees. Vendors need to
increase efforts to prevent the entry of sensitive data, and to create
transparency with regard to data usage policies and settings.

</details>


### [24] [Coordinated Position Falsification Attacks and Countermeasures for Location-Based Services](https://arxiv.org/abs/2510.27346)
*Wenjie Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 本文提出了一种检测和阻止低成本位置欺骗攻击的方法，通过整合多种定位信号（包括GNSS、传感器和地面基础设施信号）来增强位置服务的完整性和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着基于位置服务（LBS）应用的普及，确保其完整性和安全性变得至关重要。现有系统容易受到低成本攻击（如Wi-Fi欺骗结合GNSS干扰），这些攻击会操纵位置数据，导致用户欺诈或服务操控。

Method: 扩展了接收机自主完整性监测（RAIM）框架，整合了机会性信息，包括来自机载传感器、地面基础设施信号以及GNSS的数据。通过融合异构信号来提高对复杂攻击的抵御能力。

Result: 实验评估显示，与基线方案相比，所提方案最多可将检测准确率提高62%，并能恢复准确的定位。

Conclusion: 通过融合多种定位信号的方法能有效检测和阻止位置欺骗攻击，显著提高位置服务的完整性和安全性。

Abstract: With the rise of location-based service (LBS) applications that rely on
terrestrial and satellite infrastructures (e.g., GNSS and crowd-sourced Wi-Fi,
Bluetooth, cellular, and IP databases) for positioning, ensuring their
integrity and security is paramount. However, we demonstrate that these
applications are susceptible to low-cost attacks (less than $50), including
Wi-Fi spoofing combined with GNSS jamming, as well as more sophisticated
coordinated location spoofing. These attacks manipulate position data to
control or undermine LBS functionality, leading to user scams or service
manipulation. Therefore, we propose a countermeasure to detect and thwart such
attacks by utilizing readily available, redundant positioning information from
off-the-shelf platforms. Our method extends the receiver autonomous integrity
monitoring (RAIM) framework by incorporating opportunistic information,
including data from onboard sensors and terrestrial infrastructure signals,
and, naturally, GNSS. We theoretically show that the fusion of heterogeneous
signals improves resilience against sophisticated adversaries on multiple
fronts. Experimental evaluations show the effectiveness of the proposed scheme
in improving detection accuracy by 62% at most compared to baseline schemes and
restoring accurate positioning.

</details>


### [25] [Sockeye: a language for analyzing hardware documentation](https://arxiv.org/abs/2510.27485)
*Ben Fiedler,Samuel Gruetter,Timothy Roscoe*

Main category: cs.CR

TL;DR: 提出一种领域特定语言来描述硬件语义、软件行为假设和安全属性，对8个SoC创建机器可读规范并形式化证明其安全性，发现文档错误和真实漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代SoC硬件复杂性高，组件语义描述不精确且易出错，难以对平台安全做出严格声明。

Method: 开发领域特定语言描述硬件语义、软件行为假设和安全属性，为8个不同SoC创建机器可读规范并进行形式化验证。

Result: 成功证明内存机密性和完整性安全属性，发现多个文档错误，并在真实服务器芯片中发现漏洞。

Conclusion: 该工具为系统集成商提供形式化描述SoC安全属性并证明或发现反例的方法。

Abstract: Systems programmers have to consolidate the ever growing hardware mess
present on modern System-on-Chips (SoCs). Correctly programming a multitude of
components, providing functionality but also security, is a difficult problem:
semantics of individual units are described in English prose, descriptions are
often underspecified, and prone to inaccuracies. Rigorous statements about
platform security are often impossible.
  We introduce a domain-specific language to describe hardware semantics,
assumptions about software behavior, and desired security properties. We then
create machine-readable specifications for a diverse set of eight SoCs from
their reference manuals, and formally prove their (in-)security. In addition to
security proofs about memory confidentiality and integrity, we discover a
handful of documentation errors. Finally, our analysis also revealed a
vulnerability on a real-world server chip. Our tooling offers system
integrators a way of formally describing security properties for entire SoCs,
and means to prove them or find counterexamples to them.

</details>


### [26] [Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models](https://arxiv.org/abs/2510.27629)
*Boyi Wei,Zora Che,Nathaniel Li,Udari Madhushani Sehwag,Jasper Götting,Samira Nedungadi,Julian Michael,Summer Yue,Dan Hendrycks,Peter Henderson,Zifan Wang,Seth Donoughe,Mantas Mazeika*

Main category: cs.CR

TL;DR: 本文提出了一个评估框架来测试生物基础模型的双重用途风险缓解措施的有效性，发现当前的数据过滤方法可能不够有效，被排除的知识可以通过微调快速恢复，且双重用途信号可能已经存在于预训练表示中。


<details>
  <summary>Details</summary>
Motivation: 开放权重的生物基础模型存在双重用途困境，既可能加速科学研究，也可能被恶意行为者用于开发生物武器。当前基于预训练数据过滤的风险缓解方法有效性尚不明确。

Method: 提出了\eval评估框架，通过三个维度评估模型对病毒的理解：序列建模、突变效应预测和毒力预测，测试数据过滤方法的鲁棒性。

Result: 结果显示当前过滤实践效果有限：被排除的知识可以通过微调快速恢复，在序列建模中表现出更广泛的泛化能力；双重用途信号可能已存在于预训练表示中，可通过简单的线性探测提取。

Conclusion: 数据过滤作为独立程序面临挑战，需要进一步研究开放权重生物基础模型的鲁棒安全和防护策略。

Abstract: Open-weight bio-foundation models present a dual-use dilemma. While holding
great promise for accelerating scientific research and drug development, they
could also enable bad actors to develop more deadly bioweapons. To mitigate the
risk posed by these models, current approaches focus on filtering biohazardous
data during pre-training. However, the effectiveness of such an approach
remains unclear, particularly against determined actors who might fine-tune
these models for malicious use. To address this gap, we propose \eval, a
framework to evaluate the robustness of procedures that are intended to reduce
the dual-use capabilities of bio-foundation models. \eval assesses models'
virus understanding through three lenses, including sequence modeling,
mutational effects prediction, and virulence prediction. Our results show that
current filtering practices may not be particularly effective: Excluded
knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits
broader generalizability in sequence modeling. Furthermore, dual-use signals
may already reside in the pretrained representations, and can be elicited via
simple linear probing. These findings highlight the challenges of data
filtering as a standalone procedure, underscoring the need for further research
into robust safety and security strategies for open-weight bio-foundation
models.

</details>
