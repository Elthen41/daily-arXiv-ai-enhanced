{"id": "2511.03825", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03825", "abs": "https://arxiv.org/abs/2511.03825", "authors": ["Ahmed Mostafa", "Raisul Arefin Nahid", "Samuel Mulder"], "title": "How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis", "comment": "Publication Notice. This paper was published in the BAR 2025 Workshop\n  (with NDSS 2025) and is for research and educational use. Copyright\n  \\c{opyright} 2025 Internet Society. All rights reserved. Personal/classroom\n  reproduction is permitted with this notice and full paper citation. All other\n  uses, including commercial, require prior written permission from the\n  Internet Society", "summary": "Tokenization is fundamental in assembly code analysis, impacting intrinsic\ncharacteristics like vocabulary size, semantic coverage, and extrinsic\nperformance in downstream tasks. Despite its significance, tokenization in the\ncontext of assembly code remains an underexplored area. This study aims to\naddress this gap by evaluating the intrinsic properties of Natural Language\nProcessing (NLP) tokenization models and parameter choices, such as vocabulary\nsize. We explore preprocessing customization options and pre-tokenization rules\ntailored to the unique characteristics of assembly code. Additionally, we\nassess their impact on downstream tasks like function signature prediction -- a\ncritical problem in binary code analysis.\n  To this end, we conduct a thorough study on various tokenization models,\nsystematically analyzing their efficiency in encoding assembly instructions and\ncapturing semantic nuances. Through intrinsic evaluations, we compare\ntokenizers based on tokenization efficiency, vocabulary compression, and\nrepresentational fidelity for assembly code. Using state-of-the-art pre-trained\nmodels such as the decoder-only Large Language Model (LLM) Llama 3.2, the\nencoder-only transformer BERT, and the encoder-decoder model BART, we evaluate\nthe effectiveness of these tokenizers across multiple performance metrics.\nPreliminary findings indicate that tokenizer choice significantly influences\ndownstream performance, with intrinsic metrics providing partial but incomplete\npredictability of extrinsic evaluation outcomes. These results reveal complex\ntrade-offs between intrinsic tokenizer properties and their utility in\npractical assembly code tasks. Ultimately, this study provides valuable\ninsights into optimizing tokenization models for low-level code analysis,\ncontributing to the robustness and scalability of Natural Language Model\n(NLM)-based binary analysis workflows.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86NLP\u5206\u8bcd\u6a21\u578b\u5728\u6c47\u7f16\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u5185\u5728\u7279\u6027\uff0c\u5305\u62ec\u8bcd\u6c47\u91cf\u5927\u5c0f\u3001\u8bed\u4e49\u8986\u76d6\u7b49\uff0c\u5e76\u7814\u7a76\u4e86\u9884\u5904\u7406\u5b9a\u5236\u548c\u9884\u5206\u8bcd\u89c4\u5219\u5bf9\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u51fd\u6570\u7b7e\u540d\u9884\u6d4b\uff09\u7684\u5f71\u54cd\u3002", "motivation": "\u6c47\u7f16\u4ee3\u7801\u5206\u8bcd\u5728\u4ee3\u7801\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8be5\u9886\u57df\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30NLP\u5206\u8bcd\u6a21\u578b\u7684\u5185\u5728\u7279\u6027\u53ca\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u591a\u79cd\u5206\u8bcd\u6a21\u578b\uff0c\u5206\u6790\u5176\u5728\u6c47\u7f16\u6307\u4ee4\u7f16\u7801\u548c\u8bed\u4e49\u6355\u6349\u65b9\u9762\u7684\u6548\u7387\u3002\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff08Llama 3.2\u3001BERT\u3001BART\uff09\u8bc4\u4f30\u5206\u8bcd\u5668\u5728\u591a\u4e2a\u6027\u80fd\u6307\u6807\u4e0a\u7684\u6709\u6548\u6027\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u8868\u660e\u5206\u8bcd\u5668\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\uff0c\u5185\u5728\u6307\u6807\u53ea\u80fd\u90e8\u5206\u9884\u6d4b\u5916\u5728\u8bc4\u4f30\u7ed3\u679c\u3002\u63ed\u793a\u4e86\u5185\u5728\u5206\u8bcd\u5668\u7279\u6027\u4e0e\u5b9e\u9645\u6c47\u7f16\u4ee3\u7801\u4efb\u52a1\u6548\u7528\u4e4b\u95f4\u7684\u590d\u6742\u6743\u8861\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4f18\u5316\u4f4e\u5c42\u4ee3\u7801\u5206\u6790\u7684\u5206\u8bcd\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u4e8c\u8fdb\u5236\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.03845", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03845", "abs": "https://arxiv.org/abs/2511.03845", "authors": ["Tianning Dong", "Luyi Ma", "Varun Vasudevan", "Jason Cho", "Sushant Kumar", "Kannan Achan"], "title": "To See or To Read: User Behavior Reasoning in Multimodal LLMs", "comment": "Accepted by the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: Efficient Reasoning", "summary": "Multimodal Large Language Models (MLLMs) are reshaping how modern agentic\nsystems reason over sequential user-behavior data. However, whether textual or\nimage representations of user behavior data are more effective for maximizing\nMLLM performance remains underexplored. We present \\texttt{BehaviorLens}, a\nsystematic benchmarking framework for assessing modality trade-offs in\nuser-behavior reasoning across six MLLMs by representing transaction data as\n(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a\nreal-world purchase-sequence dataset, we find that when data is represented as\nimages, MLLMs next-purchase prediction accuracy is improved by 87.5% compared\nwith an equivalent textual representation without any additional computational\ncost.", "AI": {"tldr": "BehaviorLens\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7528\u6237\u884c\u4e3a\u63a8\u7406\u4e2d\u7684\u6a21\u6001\u6743\u8861\uff0c\u53d1\u73b0\u56fe\u50cf\u8868\u793a\u76f8\u6bd4\u6587\u672c\u8868\u793a\u80fd\u5c06\u4e0b\u4e00\u4e2a\u8d2d\u4e70\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534787.5%\u3002", "motivation": "\u63a2\u7d22\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\u54ea\u79cd\u65b9\u5f0f\u80fd\u66f4\u6709\u6548\u5730\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7528\u6237\u884c\u4e3a\u6570\u636e\u63a8\u7406\u4e2d\u7684\u6027\u80fd\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86BehaviorLens\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u57286\u4e2aMLLMs\u4e0a\u901a\u8fc7\u4e09\u79cd\u65b9\u5f0f\u8868\u793a\u4ea4\u6613\u6570\u636e\uff1a\u6587\u672c\u6bb5\u843d\u3001\u6563\u70b9\u56fe\u548c\u6d41\u7a0b\u56fe\uff0c\u5e76\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u8d2d\u4e70\u5e8f\u5217\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5f53\u6570\u636e\u8868\u793a\u4e3a\u56fe\u50cf\u65f6\uff0cMLLMs\u7684\u4e0b\u4e00\u4e2a\u8d2d\u4e70\u9884\u6d4b\u51c6\u786e\u7387\u76f8\u6bd4\u7b49\u6548\u7684\u6587\u672c\u8868\u793a\u63d0\u9ad8\u4e8687.5%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u56fe\u50cf\u8868\u793a\u5728\u7528\u6237\u884c\u4e3a\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6587\u672c\u8868\u793a\uff0c\u4e3aMLLMs\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2511.03944", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03944", "abs": "https://arxiv.org/abs/2511.03944", "authors": ["Tong Zhang", "Vikram Sharma Mailthody", "Fei Sun", "Linsen Ma", "Chris J. Newburn", "Teresa Zhang", "Yang Liu", "Jiangpeng Li", "Hao Zhong", "Wen-Mei Hwu"], "title": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies", "comment": "13 pages, 10 figures", "summary": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a\nsimple, storage-memory-economics-based heuristic for deciding when data should\nlive in DRAM rather than on storage. Subsequent revisits to the rule largely\nretained that economics-only view, leaving host costs, feasibility limits, and\nworkload behavior out of scope. This paper revisits the rule from first\nprinciples, integrating host costs, DRAM bandwidth/capacity, and\nphysics-grounded models of SSD performance and cost, and then embedding these\nelements in a constraint- and workload-aware framework that yields actionable\nprovisioning guidance. We show that, for modern AI platforms, especially\nGPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained\nrandom access, the DRAM-to-flash caching threshold collapses from minutes to a\nfew seconds. This shift reframes NAND flash memory as an active data tier and\nexposes a broad research space across the hardware-software stack. We further\nintroduce MQSim-Next, a calibrated SSD simulator that supports validation and\nsensitivity analysis and facilitates future architectural and system research.\nFinally, we present two concrete case studies that showcase the software system\ndesign space opened by such memory hierarchy paradigm shift. Overall, we turn a\nclassical heuristic into an actionable, feasibility-aware analysis and\nprovisioning framework and set the stage for further research on AI-era memory\nhierarchy.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u7ecf\u5178\u76845\u5206\u949f\u89c4\u5219\uff0c\u901a\u8fc7\u6574\u5408\u4e3b\u673a\u6210\u672c\u3001DRAM\u5e26\u5bbd/\u5bb9\u91cf\u9650\u5236\u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u7684SSD\u6027\u80fd\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ea6\u675f\u548c\u8d1f\u8f7d\u611f\u77e5\u7684\u5206\u6790\u6846\u67b6\uff0c\u53d1\u73b0\u5728\u73b0\u4ee3AI\u5e73\u53f0\u4e2dDRAM\u5230\u95ea\u5b58\u7684\u7f13\u5b58\u9608\u503c\u4ece\u5206\u949f\u7ea7\u964d\u81f3\u79d2\u7ea7\u3002", "motivation": "\u4f20\u7edf\u76845\u5206\u949f\u89c4\u5219\u4ec5\u57fa\u4e8e\u5b58\u50a8-\u5185\u5b58\u7ecf\u6d4e\u5b66\uff0c\u5ffd\u7565\u4e86\u4e3b\u673a\u6210\u672c\u3001\u53ef\u884c\u6027\u9650\u5236\u548c\u5de5\u4f5c\u8d1f\u8f7d\u884c\u4e3a\u3002\u968f\u7740\u73b0\u4ee3AI\u5e73\u53f0\uff08\u7279\u522b\u662fGPU\u4e3b\u673a\u4e0e\u8d85\u9ad8\u6027\u80fdSSD\u914d\u5bf9\uff09\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u89c4\u5219\u3002", "method": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u51fa\u53d1\uff0c\u6574\u5408\u4e3b\u673a\u6210\u672c\u3001DRAM\u5e26\u5bbd/\u5bb9\u91cf\u9650\u5236\uff0c\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684SSD\u6027\u80fd\u548c\u6210\u672c\u6a21\u578b\uff0c\u6784\u5efa\u7ea6\u675f\u548c\u8d1f\u8f7d\u611f\u77e5\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e76\u5f00\u53d1MQSim-Next SSD\u6a21\u62df\u5668\u8fdb\u884c\u9a8c\u8bc1\u548c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5bf9\u4e8e\u73b0\u4ee3AI\u5e73\u53f0\uff0c\u7279\u522b\u662fGPU\u4e2d\u5fc3\u4e3b\u673a\u4e0e\u4e3a\u7ec6\u7c92\u5ea6\u968f\u673a\u8bbf\u95ee\u8bbe\u8ba1\u7684\u8d85\u9ad8\u6027\u80fdSSD\u914d\u5bf9\u65f6\uff0cDRAM\u5230\u95ea\u5b58\u7684\u7f13\u5b58\u9608\u503c\u4ece\u5206\u949f\u7ea7\u964d\u81f3\u79d2\u7ea7\uff0c\u5c06NAND\u95ea\u5b58\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6d3b\u8dc3\u6570\u636e\u5c42\u3002", "conclusion": "\u5c06\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\u8f6c\u53d8\u4e3a\u53ef\u64cd\u4f5c\u7684\u3001\u53ef\u884c\u6027\u611f\u77e5\u7684\u5206\u6790\u548c\u914d\u7f6e\u6846\u67b6\uff0c\u4e3aAI\u65f6\u4ee3\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u7531\u6b64\u4ea7\u751f\u7684\u8f6f\u4ef6\u7cfb\u7edf\u8bbe\u8ba1\u7a7a\u95f4\u3002"}}
{"id": "2511.03866", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.03866", "abs": "https://arxiv.org/abs/2511.03866", "authors": ["Arijit Bhattacharjee", "Ali TehraniJamsaz", "Le Chen", "Niranjan Hasabnis", "Mihai Capota", "Nesreen Ahmed", "Ali Jannesari"], "title": "OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms", "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly\naccelerated progress in code translation, enabling more accurate and efficient\ntransformation across programming languages. While originally developed for\nnatural language processing, LLMs have shown strong capabilities in modeling\nprogramming language syntax and semantics, outperforming traditional rule-based\nsystems in both accuracy and flexibility. These models have streamlined\ncross-language conversion, reduced development overhead, and accelerated legacy\ncode migration. In this paper, we introduce OMPILOT, a novel domain-specific\nencoder-decoder transformer tailored for translating C++ code into OpenMP,\nenabling effective shared-memory parallelization. OMPILOT leverages custom\npre-training objectives that incorporate the semantics of parallel constructs\nand combines both unsupervised and supervised learning strategies to improve\ncode translation robustness. Unlike previous work that focused primarily on\nloop-level transformations, OMPILOT operates at the function level to capture a\nwider semantic context. To evaluate our approach, we propose OMPBLEU, a novel\ncomposite metric specifically crafted to assess the correctness and quality of\nOpenMP parallel constructs, addressing limitations in conventional translation\nmetrics.", "AI": {"tldr": "OMPILOT\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5c06C++\u4ee3\u7801\u7ffb\u8bd1\u4e3aOpenMP\u7684\u9886\u57df\u7279\u5b9a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8f6c\u6362\u5668\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u9884\u8bad\u7ec3\u76ee\u6807\u548c\u6df7\u5408\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u51fd\u6570\u7ea7\u7684\u5e76\u884c\u5316\u8f6c\u6362\uff0c\u5e76\u63d0\u51fa\u4e86OMPBLEU\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cfOpenMP\u5e76\u884c\u6784\u9020\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5faa\u73af\u7ea7\u8f6c\u6362\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u5e7f\u6cdb\u8bed\u4e49\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\uff0c\u4e14\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30OpenMP\u5e76\u884c\u6784\u9020\u7684\u6b63\u786e\u6027\u548c\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u9886\u57df\u7279\u5b9a\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8f6c\u6362\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u5305\u542b\u5e76\u884c\u6784\u9020\u8bed\u4e49\u7684\u81ea\u5b9a\u4e49\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u4f7f\u7528\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u7684\u6df7\u5408\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u51fd\u6570\u7ea7\u522b\u8fdb\u884c\u4ee3\u7801\u7ffb\u8bd1\u3002", "result": "OMPILOT\u80fd\u591f\u6709\u6548\u5b9e\u73b0C++\u5230OpenMP\u7684\u5171\u4eab\u5185\u5b58\u5e76\u884c\u5316\u8f6c\u6362\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "OMPILOT\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u8f6c\u6362\u5668\u67b6\u6784\u548c\u4e13\u95e8\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5e76\u884c\u5316\u7ffb\u8bd1\u7684\u8d28\u91cf\u548c\u6548\u679c\uff0c\u4e3a\u9057\u7559\u4ee3\u7801\u8fc1\u79fb\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03948", "categories": ["cs.AI", "cs.HC", "I.2.6; K.3.1"], "pdf": "https://arxiv.org/pdf/2511.03948", "abs": "https://arxiv.org/abs/2511.03948", "authors": ["Kevin Hong", "Kia Karbasi", "Gregory Pottie"], "title": "Extracting Causal Relations in Deep Knowledge Tracing", "comment": "Accepted for publication in the Proceedings of the 18th International\n  Conference on Educational Data Mining, 6 pages, 1 figure", "summary": "A longstanding goal in computational educational research is to develop\nexplainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which\nleverages a Recurrent Neural Network (RNN) to predict student knowledge and\nperformance on exercises, has been proposed as a major advancement over\ntraditional KT methods. Several studies suggest that its performance gains stem\nfrom its ability to model bidirectional relationships between different\nknowledge components (KCs) within a course, enabling the inference of a\nstudent's understanding of one KC from their performance on others. In this\npaper, we challenge this prevailing explanation and demonstrate that DKT's\nstrength lies in its implicit ability to model prerequisite relationships as a\ncausal structure, rather than bidirectional relationships. By pruning exercise\nrelation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal\nsubsets of the Assistments dataset, we show that DKT's predictive capabilities\nalign strongly with these causal structures. Furthermore, we propose an\nalternative method for extracting exercise relation DAGs using DKT's learned\nrepresentations and provide empirical evidence supporting our claim. Our\nfindings suggest that DKT's effectiveness is largely driven by its capacity to\napproximate causal dependencies between KCs rather than simple relational\nmappings.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86DKT\u6a21\u578b\u6027\u80fd\u6e90\u4e8e\u53cc\u5411\u5173\u7cfb\u5efa\u6a21\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u8bc1\u660e\u5176\u771f\u6b63\u4f18\u52bf\u5728\u4e8e\u9690\u5f0f\u5efa\u6a21\u77e5\u8bc6\u7ec4\u4ef6\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u975e\u53cc\u5411\u5173\u7cfb\u3002", "motivation": "\u6f84\u6e05\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a(DKT)\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u771f\u6b63\u539f\u56e0\uff0c\u6311\u6218\u5176\u6e90\u4e8e\u53cc\u5411\u5173\u7cfb\u5efa\u6a21\u7684\u6d41\u884c\u89e3\u91ca\uff0c\u63a2\u7d22DKT\u5b9e\u9645\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u5c06\u7ec3\u4e60\u5173\u7cfb\u56fe\u4fee\u526a\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u5728Assistments\u6570\u636e\u96c6\u7684\u56e0\u679c\u5b50\u96c6\u4e0a\u8bad\u7ec3DKT\uff0c\u5e76\u57fa\u4e8eDKT\u5b66\u4e60\u8868\u793a\u63d0\u51fa\u63d0\u53d6\u7ec3\u4e60\u5173\u7cfbDAG\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDKT\u7684\u9884\u6d4b\u80fd\u529b\u4e0e\u56e0\u679c\u7ed3\u6784\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5176\u6709\u6548\u6027\u4e3b\u8981\u6e90\u4e8e\u8fd1\u4f3c\u77e5\u8bc6\u7ec4\u4ef6\u95f4\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\u3002", "conclusion": "DKT\u7684\u6210\u529f\u4e3b\u8981\u5f52\u56e0\u4e8e\u5176\u5efa\u6a21\u77e5\u8bc6\u7ec4\u4ef6\u95f4\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u800c\u975e\u7b80\u5355\u7684\u53cc\u5411\u5173\u7cfb\u6620\u5c04\uff0c\u8fd9\u4e3a\u7406\u89e3DKT\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.03941", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03941", "abs": "https://arxiv.org/abs/2511.03941", "authors": ["Fabio Diniz Rossi"], "title": "Stochastic Modeling for Energy-Efficient Edge Infrastructure", "comment": "8 pages, 4 figures, 3 tables", "summary": "Edge Computing enables low-latency processing for real-time applications but\nintroduces challenges in power management due to the distributed nature of edge\ndevices and their limited energy resources. This paper proposes a stochastic\nmodeling approach using Markov Chains to analyze power state transitions in\nEdge Computing. By deriving steady-state probabilities and evaluating energy\nconsumption, we demonstrate the benefits of AI-driven predictive power scaling\nover conventional reactive methods. Monte Carlo simulations validate the model,\nshowing strong alignment between theoretical and empirical results. Sensitivity\nanalysis highlights how varying transition probabilities affect power\nefficiency, confirming that predictive scaling minimizes unnecessary\ntransitions and improves overall system responsiveness. Our findings suggest\nthat AI-based power management strategies significantly enhance energy\nefficiency by anticipating workload demands and optimizing state transitions.\nExperimental results indicate that AI-based power management optimizes workload\ndistribution across heterogeneous edge nodes, reducing energy consumption\ndisparities between devices, improving overall efficiency, and enhancing\nadaptive power coordination in multi-node environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u968f\u673a\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u529f\u7387\u72b6\u6001\u8f6c\u6362\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u529f\u7387\u7f29\u653e\u76f8\u6bd4\u4f20\u7edf\u53cd\u5e94\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6548\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u867d\u7136\u652f\u6301\u4f4e\u5ef6\u8fdf\u5904\u7406\uff0c\u4f46\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u5206\u5e03\u5f0f\u7279\u6027\u548c\u6709\u9650\u7684\u80fd\u6e90\u8d44\u6e90\uff0c\u5728\u7535\u6e90\u7ba1\u7406\u65b9\u9762\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8fdb\u884c\u968f\u673a\u5efa\u6a21\uff0c\u63a8\u5bfc\u7a33\u6001\u6982\u7387\u5e76\u8bc4\u4f30\u80fd\u8017\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "AI\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u529f\u7387\u7f29\u653e\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u72b6\u6001\u8f6c\u6362\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u54cd\u5e94\u6027\uff0c\u5728\u5f02\u6784\u8fb9\u7f18\u8282\u70b9\u95f4\u4f18\u5316\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\uff0c\u964d\u4f4e\u4e86\u8bbe\u5907\u95f4\u7684\u80fd\u8017\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8eAI\u7684\u7535\u6e90\u7ba1\u7406\u7b56\u7565\u901a\u8fc7\u9884\u6d4b\u5de5\u4f5c\u8d1f\u8f7d\u9700\u6c42\u5e76\u4f18\u5316\u72b6\u6001\u8f6c\u6362\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\u3002"}}
{"id": "2511.03980", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03980", "abs": "https://arxiv.org/abs/2511.03980", "authors": ["Bram Bult\u00e9", "Ayla Rigouts Terryn"], "title": "LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing", "comment": "Preprint under review at Computational Linguistics. Accepted with\n  minor revisions (10/10/2025); second round", "summary": "Large Language Models (LLMs) are rapidly being adopted by users across the\nglobe, who interact with them in a diverse range of languages. At the same\ntime, there are well-documented imbalances in the training data and\noptimisation objectives of this technology, raising doubts as to whether LLMs\ncan represent the cultural diversity of their broad user base. In this study,\nwe look at LLMs and cultural values and examine how prompt language and\ncultural framing influence model responses and their alignment with human\nvalues in different countries. We probe 10 LLMs with 63 items from the Hofstede\nValues Survey Module and World Values Survey, translated into 11 languages, and\nformulated as prompts with and without different explicit cultural\nperspectives. Our study confirms that both prompt language and cultural\nperspective produce variation in LLM outputs, but with an important caveat:\nWhile targeted prompting can, to a certain extent, steer LLM responses in the\ndirection of the predominant values of the corresponding countries, it does not\novercome the models' systematic bias toward the values associated with a\nrestricted set of countries in our dataset: the Netherlands, Germany, the US,\nand Japan. All tested models, regardless of their origin, exhibit remarkably\nsimilar patterns: They produce fairly neutral responses on most topics, with\nselective progressive stances on issues such as social tolerance. Alignment\nwith cultural values of human respondents is improved more with an explicit\ncultural perspective than with a targeted prompt language. Unexpectedly,\ncombining both approaches is no more effective than cultural framing with an\nEnglish prompt. These findings reveal that LLMs occupy an uncomfortable middle\nground: They are responsive enough to changes in prompts to produce variation,\nbut too firmly anchored to specific cultural defaults to adequately represent\ncultural diversity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u6587\u5316\u4ef7\u503c\u89c2\u8868\u8fbe\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u867d\u7136\u63d0\u793a\u8bed\u8a00\u548c\u6587\u5316\u6846\u67b6\u4f1a\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\uff0c\u4f46LLMs\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u504f\u5411\u8377\u5170\u3001\u5fb7\u56fd\u3001\u7f8e\u56fd\u548c\u65e5\u672c\u7b49\u5c11\u6570\u56fd\u5bb6\u7684\u4ef7\u503c\u89c2\uff0c\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u5168\u7403\u6587\u5316\u591a\u6837\u6027\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5168\u7403\u8303\u56f4\u5185\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u7528\u6237\u4f7f\u7528\u591a\u79cd\u8bed\u8a00\u4e0e\u4e4b\u4ea4\u4e92\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u548c\u4f18\u5316\u76ee\u6807\u5b58\u5728\u4e0d\u5e73\u8861\uff0c\u5f15\u53d1\u4e86\u5bf9LLMs\u80fd\u5426\u4ee3\u8868\u5176\u5e7f\u6cdb\u7528\u6237\u7fa4\u4f53\u6587\u5316\u591a\u6837\u6027\u7684\u8d28\u7591\u3002", "method": "\u4f7f\u7528\u970d\u592b\u65af\u6cf0\u5fb7\u4ef7\u503c\u89c2\u8c03\u67e5\u6a21\u5757\u548c\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u4e2d\u768463\u4e2a\u9879\u76ee\uff0c\u7ffb\u8bd1\u621011\u79cd\u8bed\u8a00\uff0c\u4ee5\u5e26\u6709\u548c\u4e0d\u5e26\u6709\u660e\u786e\u6587\u5316\u89c6\u89d2\u7684\u63d0\u793a\u5f62\u5f0f\u6d4b\u8bd5\u4e8610\u4e2aLLM\u3002", "result": "\u63d0\u793a\u8bed\u8a00\u548c\u6587\u5316\u89c6\u89d2\u90fd\u4f1a\u5bfc\u81f4LLM\u8f93\u51fa\u53d8\u5316\uff0c\u4f46\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u504f\u5411\u7279\u5b9a\u56fd\u5bb6\u7684\u4ef7\u503c\u89c2\u3002\u660e\u786e\u7684\u6587\u89c6\u89d2\u6bd4\u9488\u5bf9\u6027\u7684\u63d0\u793a\u8bed\u8a00\u66f4\u80fd\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u53d7\u8bbf\u8005\u6587\u5316\u4ef7\u503c\u89c2\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "LLMs\u5904\u4e8e\u4e00\u4e2a\u5c34\u5c2c\u7684\u4e2d\u95f4\u5730\u5e26\uff1a\u5b83\u4eec\u5bf9\u63d0\u793a\u53d8\u5316\u8db3\u591f\u654f\u611f\u4ee5\u4ea7\u751f\u5dee\u5f02\uff0c\u4f46\u53c8\u8fc7\u4e8e\u56fa\u5b88\u7279\u5b9a\u7684\u6587\u5316\u9ed8\u8ba4\u503c\uff0c\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u6587\u5316\u591a\u6837\u6027\u3002"}}
{"id": "2511.04104", "categories": ["cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.04104", "abs": "https://arxiv.org/abs/2511.04104", "authors": ["Chao Guo", "Jiahe Xu", "Moshe Zukerman"], "title": "Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs", "comment": null, "summary": "Hardware disaggregation seeks to transform Data Center (DC) resources from\ntraditional server fleets into unified resource pools. Despite existing\nchallenges that may hinder its full realization, significant progress has been\nmade in both industry and academia. In this article, we provide an overview of\nthe motivations and recent advancements in hardware disaggregation. We further\ndiscuss the research challenges and opportunities associated with disaggregated\narchitectures, focusing on aspects that have received limited attention. We\nargue that hardware disaggregation has the potential to reshape the entire DC\necosystem, impacting application design, resource scheduling, hardware\nconfiguration, cooling, and power system optimization. Additionally, we present\na numerical study to illustrate several key aspects of these challenges.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u786c\u4ef6\u89e3\u8026\u7684\u52a8\u673a\u3001\u6700\u65b0\u8fdb\u5c55\u3001\u7814\u7a76\u6311\u6218\u4e0e\u673a\u9047\uff0c\u8ba4\u4e3a\u786c\u4ef6\u89e3\u8026\u6709\u6f5c\u529b\u91cd\u5851\u6574\u4e2a\u6570\u636e\u4e2d\u5fc3\u751f\u6001\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u503c\u7814\u7a76\u6765\u8bf4\u660e\u5173\u952e\u6311\u6218\u3002", "motivation": "\u786c\u4ef6\u89e3\u8026\u65e8\u5728\u5c06\u6570\u636e\u4e2d\u5fc3\u8d44\u6e90\u4ece\u4f20\u7edf\u670d\u52a1\u5668\u673a\u7fa4\u8f6c\u53d8\u4e3a\u7edf\u4e00\u8d44\u6e90\u6c60\uff0c\u5c3d\u7ba1\u5b58\u5728\u6311\u6218\uff0c\u4f46\u4ea7\u4e1a\u754c\u548c\u5b66\u672f\u754c\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002", "method": "\u63d0\u4f9b\u786c\u4ef6\u89e3\u8026\u7684\u7efc\u8ff0\uff0c\u8ba8\u8bba\u76f8\u5173\u7814\u7a76\u6311\u6218\u548c\u673a\u9047\uff0c\u91cd\u70b9\u5173\u6ce8\u5173\u6ce8\u5ea6\u8f83\u4f4e\u7684\u65b9\u9762\uff0c\u5e76\u8fdb\u884c\u6570\u503c\u7814\u7a76\u6765\u8bf4\u660e\u5173\u952e\u6311\u6218\u3002", "result": "\u786c\u4ef6\u89e3\u8026\u6709\u6f5c\u529b\u5f71\u54cd\u5e94\u7528\u7a0b\u5e8f\u8bbe\u8ba1\u3001\u8d44\u6e90\u8c03\u5ea6\u3001\u786c\u4ef6\u914d\u7f6e\u3001\u51b7\u5374\u548c\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u7b49\u6574\u4e2a\u6570\u636e\u4e2d\u5fc3\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "\u786c\u4ef6\u89e3\u6784\u5177\u6709\u91cd\u5851\u6570\u636e\u4e2d\u5fc3\u751f\u6001\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u76f8\u5173\u6311\u6218\u548c\u673a\u9047\u3002"}}
{"id": "2511.04268", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04268", "abs": "https://arxiv.org/abs/2511.04268", "authors": ["Iker Mart\u00edn-\u00c1lvarez", "Jos\u00e9 I. Aliaga", "Maribel Castillo", "Sergio Iserte"], "title": "Parallel Spawning Strategies for Dynamic-Aware MPI Applications", "comment": "10 pages, 1 Table, 6 Figures, 8 Equations, 2 Listings", "summary": "Dynamic resource management is an increasingly important capability of High\nPerformance Computing systems, as it enables jobs to adjust their resource\nallocation at runtime. This capability has been shown to reduce workload\nmakespan, substantially decrease job waiting times and improve overall system\nutilization. In this context, malleability refers to the ability of\napplications to adapt to new resource allocations during execution. Although\nbeneficial, malleability incurs significant reconfiguration costs, making the\nreduction of these costs an important research topic.\n  Some existing methods for MPI applications respawn the entire application,\nwhich is an expensive solution that avoids the reuse of original processes.\nOther MPI methods reuse them, but fail to fully release unneeded processes when\nshrinking, since some ranks within the same communicator remain active across\nnodes, preventing the application from returning those nodes to the system.\nThis work overcomes both limitations by proposing a novel parallel spawning\nstrategy, in which all processes cooperate in spawning before redistribution,\nthereby reducing execution time. Additionally, it removes shrinkage\nlimitations, allowing better adaptation of parallel systems to workload and\nreducing their makespan. As a result, it preserves competitive expansion times\nwith at most a $1.25\\times$ overhead, while enabling fast shrink operations\nthat reduce their cost by at least $20\\times$. This strategy has been validated\non both homogeneous and heterogeneous systems and can also be applied in\nshared-resource environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5e76\u884c\u751f\u6210\u7b56\u7565\uff0c\u901a\u8fc7\u6240\u6709\u8fdb\u7a0b\u5728\u91cd\u65b0\u5206\u914d\u524d\u534f\u4f5c\u751f\u6210\u6765\u51cf\u5c11\u6267\u884c\u65f6\u95f4\uff0c\u540c\u65f6\u6d88\u9664\u6536\u7f29\u9650\u5236\uff0c\u4f7f\u5e76\u884c\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u5e76\u51cf\u5c11\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709MPI\u5e94\u7528\u7684\u53ef\u5851\u6027\u65b9\u6cd5\u5b58\u5728\u91cd\u65b0\u914d\u7f6e\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u8981\u4e48\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u5e94\u7528\uff0c\u8981\u4e48\u5728\u6536\u7f29\u65f6\u65e0\u6cd5\u5b8c\u5168\u91ca\u653e\u4e0d\u9700\u8981\u7684\u8fdb\u7a0b\u3002", "method": "\u63d0\u51fa\u5e76\u884c\u751f\u6210\u7b56\u7565\uff0c\u6240\u6709\u8fdb\u7a0b\u5728\u91cd\u65b0\u5206\u914d\u524d\u534f\u4f5c\u751f\u6210\uff0c\u540c\u65f6\u79fb\u9664\u6536\u7f29\u9650\u5236\uff0c\u5141\u8bb8\u5e76\u884c\u7cfb\u7edf\u66f4\u597d\u5730\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u4fdd\u6301\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6269\u5c55\u65f6\u95f4\uff08\u6700\u591a1.25\u500d\u5f00\u9500\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5feb\u901f\u6536\u7f29\u64cd\u4f5c\uff0c\u6210\u672c\u81f3\u5c11\u964d\u4f4e20\u500d\u3002\u5728\u5f02\u6784\u548c\u540c\u6784\u7cfb\u7edf\u4e0a\u5747\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709MPI\u53ef\u5851\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91cd\u65b0\u914d\u7f6e\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u9002\u5e94\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.03841", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.03841", "abs": "https://arxiv.org/abs/2511.03841", "authors": ["Yedidel Louck", "Ariel Stulman", "Amit Dvir"], "title": "Security Analysis of Agentic AI Communication Protocols: A Comparative Evaluation", "comment": null, "summary": "Multi-agent systems (MAS) powered by artificial intelligence (AI) are\nincreasingly foundational to complex, distributed workflows. Yet, the security\nof their underlying communication protocols remains critically under-examined.\nThis paper presents the first empirical, comparative security analysis of the\nofficial CORAL implementation and a high-fidelity, SDK-based ACP\nimplementation, benchmarked against a literature-based evaluation of A2A. Using\na 14 point vulnerability taxonomy, we systematically assess their defenses\nacross authentication, authorization, integrity, confidentiality, and\navailability. Our results reveal a pronounced security dichotomy: CORAL\nexhibits a robust architectural design, particularly in its transport-layer\nmessage validation and session isolation, but suffers from critical\nimplementation-level vulnerabilities, including authentication and\nauthorization failures at its SSE gateway. Conversely, ACP's architectural\nflexibility, most notably its optional JWS enforcement, translates into\nhigh-impact integrity and confidentiality flaws. We contextualize these\nfindings within current industry trends, highlighting that existing protocols\nremain insufficiently secure. As a path forward, we recommend a hybrid approach\nthat combines CORAL's integrated architecture with ACP's mandatory per-message\nintegrity guarantees, laying the groundwork for resilient, next-generation\nagent communications.", "AI": {"tldr": "\u672c\u6587\u5bf9CORAL\u548cACP\u4e24\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u4fe1\u534f\u8bae\u8fdb\u884c\u4e86\u9996\u6b21\u5b9e\u8bc1\u5b89\u5168\u5206\u6790\uff0c\u53d1\u73b0CORAL\u67b6\u6784\u8bbe\u8ba1\u7a33\u5065\u4f46\u5b58\u5728\u5173\u952e\u5b9e\u73b0\u6f0f\u6d1e\uff0cACP\u67b6\u6784\u7075\u6d3b\u4f46\u5b58\u5728\u5b8c\u6574\u6027\u7f3a\u9677\uff0c\u5efa\u8bae\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u5e95\u5c42\u901a\u4fe1\u534f\u8bae\u7684\u5b89\u5168\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u5b89\u5168\u8bc4\u4f30\u3002", "method": "\u4f7f\u752814\u70b9\u6f0f\u6d1e\u5206\u7c7b\u6cd5\uff0c\u5bf9\u5b98\u65b9CORAL\u5b9e\u73b0\u548c\u57fa\u4e8eSDK\u7684ACP\u5b9e\u73b0\u8fdb\u884c\u7cfb\u7edf\u6027\u5b89\u5168\u8bc4\u4f30\uff0c\u6db5\u76d6\u8ba4\u8bc1\u3001\u6388\u6743\u3001\u5b8c\u6574\u6027\u3001\u673a\u5bc6\u6027\u548c\u53ef\u7528\u6027\u7b49\u65b9\u9762\u3002", "result": "CORAL\u5728\u4f20\u8f93\u5c42\u6d88\u606f\u9a8c\u8bc1\u548c\u4f1a\u8bdd\u9694\u79bb\u65b9\u9762\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u5728SSE\u7f51\u5173\u5b58\u5728\u8ba4\u8bc1\u548c\u6388\u6743\u5931\u8d25\u7b49\u5173\u952e\u5b9e\u73b0\u6f0f\u6d1e\uff1bACP\u7684\u67b6\u6784\u7075\u6d3b\u6027\u5bfc\u81f4\u9ad8\u5f71\u54cd\u7684\u5b8c\u6574\u6027\u548c\u673a\u5bc6\u6027\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5176\u53ef\u9009\u7684JWS\u5f3a\u5236\u6267\u884c\u673a\u5236\u3002", "conclusion": "\u73b0\u6709\u534f\u8bae\u5b89\u5168\u6027\u4e0d\u8db3\uff0c\u5efa\u8bae\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408CORAL\u7684\u96c6\u6210\u67b6\u6784\u548cACP\u7684\u5f3a\u5236\u6bcf\u6d88\u606f\u5b8c\u6574\u6027\u4fdd\u8bc1\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u4f53\u901a\u4fe1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2511.04321", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04321", "abs": "https://arxiv.org/abs/2511.04321", "authors": ["Yuanpeng Zhang", "Xing Hu", "Xi Chen", "Zhihang Yuan", "Cong Li", "Jingchen Zhu", "Zhao Wang", "Chenguang Zhang", "Xin Si", "Wei Gao", "Qiang Wu", "Runsheng Wang", "Guangyu Sun"], "title": "AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM", "comment": "18 pages, 22 figures, accepted by ISCA 2025", "summary": "SRAM Processing-in-Memory (PIM) has emerged as the most promising\nimplementation for high-performance PIM, delivering superior computing density,\nenergy efficiency, and computational precision. However, the pursuit of higher\nperformance necessitates more complex circuit designs and increased operating\nfrequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly\ndegrade chip performance and even threaten reliability. Conventional\ncircuit-level IR-drop mitigation methods, such as back-end optimizations, are\nresource-intensive and often compromise power, performance, and area (PPA). To\naddress these challenges, we propose AIM, comprehensive software and hardware\nco-design for architecture-level IR-drop mitigation in high-performance PIM.\nInitially, leveraging the bit-serial and in-situ dataflow processing properties\nof PIM, we introduce Rtog and HR, which establish a direct correlation between\nPIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,\nenabling extensive exploration of architecture-level IR-drop mitigation while\nmaintaining computational accuracy through software optimization. Subsequently,\nwe develop IR-Booster, a dynamic adjustment mechanism that integrates\nsoftware-level HR information with hardware-based IR-drop monitoring to adapt\nthe V-f pairs of the PIM macro, achieving enhanced energy efficiency and\nperformance. Finally, we propose the HR-aware task mapping method, bridging\nsoftware and hardware designs to achieve optimal improvement. Post-layout\nsimulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up\nto 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement\nand 1.152x speedup.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAIM\uff0c\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f13\u89e3\u9ad8\u6027\u80fdSRAM\u5b58\u5185\u8ba1\u7b97\u4e2d\u7684IR-drop\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u7acb\u5de5\u4f5c\u8d1f\u8f7d\u4e0eIR-drop\u7684\u5173\u8054\uff0c\u7ed3\u5408\u8f6f\u4ef6\u4f18\u5316\u548c\u786c\u4ef6\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u663e\u8457\u6539\u5584\u4e86\u80fd\u6548\u548c\u6027\u80fd\u3002", "motivation": "\u968f\u7740SRAM\u5b58\u5185\u8ba1\u7b97\u8ffd\u6c42\u66f4\u9ad8\u6027\u80fd\uff0c\u590d\u6742\u7684\u7535\u8def\u8bbe\u8ba1\u548c\u66f4\u9ad8\u7684\u5de5\u4f5c\u9891\u7387\u52a0\u5267\u4e86IR-drop\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u82af\u7247\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u800c\u4f20\u7edf\u7684\u7535\u8def\u7ea7\u7f13\u89e3\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u4f1a\u727a\u7272PPA\u6307\u6807\u3002", "method": "\u63d0\u51faAIM\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff1a\u9996\u5148\u5efa\u7acbPIM\u5de5\u4f5c\u8d1f\u8f7d\u4e0eIR-drop\u7684\u76f4\u63a5\u5173\u8054(Rtog\u548cHR)\uff0c\u7136\u540e\u901a\u8fc7\u8f6f\u4ef6\u4f18\u5316\u65b9\u6cd5(LHR\u548cWDS)\u8fdb\u884c\u67b6\u6784\u7ea7IR-drop\u7f13\u89e3\u63a2\u7d22\uff0c\u5f00\u53d1IR-Booster\u52a8\u6001\u8c03\u6574\u673a\u5236\u6574\u5408\u8f6f\u4ef6HR\u4fe1\u606f\u548c\u786c\u4ef6IR-drop\u76d1\u6d4b\uff0c\u6700\u540e\u63d0\u51faHR\u611f\u77e5\u7684\u4efb\u52a1\u6620\u5c04\u65b9\u6cd5\u3002", "result": "\u57287nm 256-TOPS PIM\u82af\u7247\u4e0a\u7684\u540e\u5e03\u5c40\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cAIM\u5b9e\u73b0\u4e86\u9ad8\u8fbe69.2%\u7684IR-drop\u7f13\u89e3\uff0c\u5e26\u6765\u4e862.29\u500d\u7684\u80fd\u6548\u63d0\u5347\u548c1.152\u500d\u7684\u52a0\u901f\u6bd4\u3002", "conclusion": "AIM\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u6027\u80fdPIM\u4e2d\u7684IR-drop\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u9ad8\u6027\u80fd\u5b58\u5185\u8ba1\u7b97\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04477", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04477", "abs": "https://arxiv.org/abs/2511.04477", "authors": ["Rongxiang Wang", "Kangyuan Shu", "Felix Xiaozhu Lin"], "title": "Enabling Dynamic Sparsity in Quantized LLM Inference", "comment": null, "summary": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u5b9e\u73b0\u52a8\u6001\u7a00\u758f\u63a8\u7406\u7684\u6280\u672f\uff0c\u901a\u8fc7\u7279\u6b8a\u7684\u91cf\u5316\u5e03\u5c40\u548c\u4e13\u7528\u5185\u6838\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u89e3\u7801\u541e\u5410\u91cf\u3002", "motivation": "\u5728\u7ec8\u7aef\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\u9650\u5236\uff0c\u800c\u52a8\u6001\u7a00\u758f\u6027\u548c\u91cf\u5316\u6280\u672f\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "method": "\u91c7\u7528\u952f\u9f7f\u5f62\u91cf\u5316\u5e03\u5c40\u7ec4\u7ec7\u6743\u91cd\u4ee5\u5339\u914d\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u8bbe\u8ba1\u4e13\u7528GEMV\u5185\u6838\u5145\u5206\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u5355\u5143\uff0c\u4ee5\u53ca\u7d27\u51d1\u7684\u8fd0\u884c\u65f6\u673a\u5236\u6536\u96c6\u7a00\u758f\u7d22\u5f15\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u786c\u4ef6\u914d\u7f6e\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad81.55\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5bc6\u96c6\u91cf\u5316\u63a8\u7406\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u7ed3\u6784\u5316\u7a00\u758f\u6027\u548c\u91cf\u5316\u6280\u672f\u53ef\u4ee5\u5728\u5546\u7528GPU\u4e0a\u6709\u6548\u5171\u5b58\uff0c\u4e3a\u7ec8\u7aef\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.04032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04032", "abs": "https://arxiv.org/abs/2511.04032", "authors": ["Divya Pathak", "Harshit Kumar", "Anuska Roy", "Felix George", "Mudit Verma", "Pratibha Moogi"], "title": "Detecting Silent Failures in Multi-Agentic AI Trajectories", "comment": null, "summary": "Multi-Agentic AI systems, powered by large language models (LLMs), are\ninherently non-deterministic and prone to silent failures such as drift,\ncycles, and missing details in outputs, which are difficult to detect. We\nintroduce the task of anomaly detection in agentic trajectories to identify\nthese failures and present a dataset curation pipeline that captures user\nbehavior, agent non-determinism, and LLM variation. Using this pipeline, we\ncurate and label two benchmark datasets comprising \\textbf{4,275 and 894}\ntrajectories from Multi-Agentic AI systems. Benchmarking anomaly detection\nmethods on these datasets, we show that supervised (XGBoost) and\nsemi-supervised (SVDD) approaches perform comparably, achieving accuracies up\nto 98% and 96%, respectively. This work provides the first systematic study of\nanomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,\nand insights to guide future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u5305\u542b4,275\u548c894\u6761\u8f68\u8ff9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u76d1\u7763\u548c\u534a\u76d1\u7763\u65b9\u6cd5\u80fd\u8fbe\u523098%\u548c96%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5177\u6709\u975e\u786e\u5b9a\u6027\u548c\u6613\u53d1\u751f\u9759\u9ed8\u6545\u969c\uff08\u5982\u6f02\u79fb\u3001\u5faa\u73af\u3001\u8f93\u51fa\u7ec6\u8282\u7f3a\u5931\uff09\u7684\u7279\u70b9\uff0c\u8fd9\u4e9b\u6545\u969c\u96be\u4ee5\u68c0\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\uff0c\u6355\u6349\u7528\u6237\u884c\u4e3a\u3001\u667a\u80fd\u4f53\u975e\u786e\u5b9a\u6027\u548cLLM\u53d8\u5316\uff0c\u5e76\u57fa\u4e8e\u6b64\u6d41\u7a0b\u6784\u5efa\u4e86\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3002\u4f7f\u7528XGBoost\uff08\u76d1\u7763\uff09\u548cSVDD\uff08\u534a\u76d1\u7763\uff09\u65b9\u6cd5\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u76d1\u7763\u65b9\u6cd5\uff08XGBoost\uff09\u548c\u534a\u76d1\u7763\u65b9\u6cd5\uff08SVDD\uff09\u8868\u73b0\u76f8\u5f53\uff0c\u5206\u522b\u8fbe\u523098%\u548c96%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.04523", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04523", "abs": "https://arxiv.org/abs/2511.04523", "authors": ["Silvia Bonomi", "Giovanni Farina", "Roy Friedman", "Eviatar B. Procaccia", "Sebastien Tixeuil"], "title": "A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems", "comment": null, "summary": "Modern distributed systems face growing security threats, as attackers\ncontinuously enhance their skills and vulnerabilities span across the entire\nsystem stack, from hardware to the application layer. In the system design\nphase, fault tolerance techniques can be employed to safeguard systems. From a\ntheoretical perspective, an attacker attempting to compromise a system can be\nabstracted by considering the presence of Byzantine processes in the system.\nAlthough this approach enhances the resilience of the distributed system, it\nintroduces certain limitations regarding the accuracy of the model in\nreflecting real-world scenarios. In this paper, we consider a self-protecting\ndistributed system based on the \\emph{Monitoring-Analyse-Plan-Execute over a\nshared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic\nMobile Byzantine Failure (MBF) that can be plugged into the Analysis component.\nOur new model captures the dynamics of evolving attacks and can be used to\ndrive the self-protection and reconfiguration strategy. We analyze\nmathematically the time that it takes until the number of Byzantine nodes\ncrosses given thresholds, or for the system to self-recover back into a safe\nstate, depending on the rates of Byzantine infection spreading \\emph{vs.} the\nrate of self-recovery. We also provide simulation results that illustrate the\nbehavior of the system under such assumptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMAPE-K\u67b6\u6784\u7684\u81ea\u4fdd\u62a4\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6027\u79fb\u52a8\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u653b\u51fb\u52a8\u6001\u548c\u9a71\u52a8\u7cfb\u7edf\u81ea\u4fdd\u62a4\u7b56\u7565\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u9762\u4e34\u65e5\u76ca\u589e\u957f\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u653b\u51fb\u8005\u6280\u80fd\u4e0d\u65ad\u63d0\u5347\uff0c\u6f0f\u6d1e\u904d\u5e03\u4ece\u786c\u4ef6\u5230\u5e94\u7528\u5c42\u7684\u6574\u4e2a\u7cfb\u7edf\u6808\u3002\u5728\u7cfb\u7edf\u8bbe\u8ba1\u9636\u6bb5\u9700\u8981\u91c7\u7528\u5bb9\u9519\u6280\u672f\u6765\u4fdd\u62a4\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8eMAPE-K\u67b6\u6784\u6784\u5efa\u81ea\u4fdd\u62a4\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u63d0\u51fa\u65b0\u7684\u6982\u7387\u6027\u79fb\u52a8\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5206\u6790\u7ec4\u4ef6\u4e2d\u3002\u901a\u8fc7\u6570\u5b66\u5206\u6790\u8ba1\u7b97\u62dc\u5360\u5ead\u8282\u70b9\u8d85\u8fc7\u9608\u503c\u6216\u7cfb\u7edf\u81ea\u6062\u590d\u5230\u5b89\u5168\u72b6\u6001\u6240\u9700\u7684\u65f6\u95f4\u3002", "result": "\u5206\u6790\u4e86\u62dc\u5360\u5ead\u611f\u67d3\u4f20\u64ad\u901f\u7387\u4e0e\u81ea\u6062\u590d\u901f\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u5bf9\u7cfb\u7edf\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u62df\u7ed3\u679c\u6765\u5c55\u793a\u7cfb\u7edf\u5728\u6b64\u7c7b\u5047\u8bbe\u4e0b\u7684\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u6027\u79fb\u52a8\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u80fd\u591f\u6355\u6349\u653b\u51fb\u6f14\u5316\u7684\u52a8\u6001\u7279\u6027\uff0c\u53ef\u7528\u4e8e\u9a71\u52a8\u7cfb\u7edf\u7684\u81ea\u4fdd\u62a4\u548c\u91cd\u914d\u7f6e\u7b56\u7565\u3002"}}
{"id": "2511.04053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04053", "abs": "https://arxiv.org/abs/2511.04053", "authors": ["Hirohane Takagi", "Gouki Minegishi", "Shota Kizawa", "Issey Sukeda", "Hitomi Yanaka"], "title": "Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models", "comment": "Accepted to IJCNLP-AACL 2025 (Main). Code available at\n  https://github.com/htkg/num_attrs", "summary": "Although behavioral studies have documented numerical reasoning errors in\nlarge language models (LLMs), the underlying representational mechanisms remain\nunclear. We hypothesize that numerical attributes occupy shared latent\nsubspaces and investigate two questions:(1) How do LLMs internally integrate\nmultiple numerical attributes of a single entity? (2)How does irrelevant\nnumerical context perturb these representations and their downstream outputs?\nTo address these questions, we combine linear probing with partial correlation\nanalysis and prompt-based vulnerability tests across models of varying sizes.\nOur results show that LLMs encode real-world numerical correlations but tend to\nsystematically amplify them. Moreover, irrelevant context induces consistent\nshifts in magnitude representations, with downstream effects that vary by model\nsize. These findings reveal a vulnerability in LLM decision-making and lay the\ngroundwork for fairer, representation-aware control under multi-attribute\nentanglement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u503c\u63a8\u7406\u4e2d\u7684\u5185\u90e8\u8868\u793a\u673a\u5236\uff0c\u53d1\u73b0LLMs\u4f1a\u7f16\u7801\u771f\u5b9e\u4e16\u754c\u7684\u6570\u503c\u76f8\u5173\u6027\u4f46\u7cfb\u7edf\u6027\u5730\u653e\u5927\u5b83\u4eec\uff0c\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u5f15\u53d1\u4e00\u81f4\u7684\u6570\u503c\u8868\u793a\u504f\u79fb\uff0c\u63ed\u793a\u4e86LLM\u51b3\u7b56\u4e2d\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u5c3d\u7ba1\u884c\u4e3a\u7814\u7a76\u5df2\u8bb0\u5f55\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u503c\u63a8\u7406\u4e2d\u7684\u9519\u8bef\uff0c\u4f46\u5176\u5e95\u5c42\u8868\u793a\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5047\u8bbe\u6570\u503c\u5c5e\u6027\u5360\u636e\u5171\u4eab\u7684\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u65e8\u5728\u63a2\u7a76LLMs\u5982\u4f55\u5185\u90e8\u6574\u5408\u5355\u4e2a\u5b9e\u4f53\u7684\u591a\u4e2a\u6570\u503c\u5c5e\u6027\uff0c\u4ee5\u53ca\u65e0\u5173\u6570\u503c\u4e0a\u4e0b\u6587\u5982\u4f55\u6270\u52a8\u8fd9\u4e9b\u8868\u793a\u53ca\u5176\u4e0b\u6e38\u8f93\u51fa\u3002", "method": "\u7ed3\u5408\u7ebf\u6027\u63a2\u6d4b\u4e0e\u504f\u76f8\u5173\u5206\u6790\uff0c\u5e76\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u57fa\u4e8e\u63d0\u793a\u7684\u8106\u5f31\u6027\u6d4b\u8bd5\uff0c\u4ee5\u7814\u7a76LLMs\u5185\u90e8\u6570\u503c\u5c5e\u6027\u7684\u8868\u793a\u673a\u5236\u3002", "result": "\u7ed3\u679c\u663e\u793aLLMs\u7f16\u7801\u771f\u5b9e\u4e16\u754c\u7684\u6570\u503c\u76f8\u5173\u6027\u4f46\u503e\u5411\u4e8e\u7cfb\u7edf\u6027\u653e\u5927\u5b83\u4eec\uff1b\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u5f15\u53d1\u4e00\u81f4\u7684\u6570\u503c\u8868\u793a\u504f\u79fb\uff0c\u4e14\u4e0b\u6e38\u6548\u5e94\u968f\u6a21\u578b\u89c4\u6a21\u800c\u53d8\u5316\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86LLM\u51b3\u7b56\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u5728\u591a\u5c5e\u6027\u7ea0\u7f20\u4e0b\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u8868\u793a\u611f\u77e5\u7684\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.04631", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04631", "abs": "https://arxiv.org/abs/2511.04631", "authors": ["Petr Kuznetsov", "Nathan Josia Schrodt"], "title": "Resolving Conflicts with Grace: Dynamically Concurrent Universality", "comment": null, "summary": "Synchronization is the major obstacle to scalability in distributed\ncomputing. Concurrent operations on the shared data engage in synchronization\nwhen they encounter a \\emph{conflict}, i.e., their effects depend on the order\nin which they are applied. Ideally, one would like to detect conflicts in a\n\\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it\nis very common that two concurrent operations conflict only in some rarely\noccurring states. In this paper, we define the notion of \\emph{dynamic\nconcurrency}: an operation employs strong synchronization primitives only if it\n\\emph{has} to arbitrate with concurrent operations, given the current system\nstate. We then present a dynamically concurrent universal construction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u52a8\u6001\u5e76\u53d1\u6982\u5ff5\uff0c\u4ec5\u5728\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u4e0b\u9700\u8981\u4e0e\u5e76\u53d1\u64cd\u4f5c\u4ef2\u88c1\u65f6\u624d\u4f7f\u7528\u5f3a\u540c\u6b65\u539f\u8bed\uff0c\u5e76\u7ed9\u51fa\u4e86\u52a8\u6001\u5e76\u53d1\u7684\u901a\u7528\u6784\u9020\u65b9\u6cd5\u3002", "motivation": "\u540c\u6b65\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7684\u4e3b\u8981\u969c\u788d\u3002\u5e76\u53d1\u64cd\u4f5c\u5728\u9047\u5230\u51b2\u7a81\u65f6\u9700\u8981\u540c\u6b65\uff0c\u4f46\u51b2\u7a81\u901a\u5e38\u53ea\u53d1\u751f\u5728\u5c11\u6570\u7f55\u89c1\u72b6\u6001\u4e0b\u3002\u7406\u60f3\u60c5\u51b5\u662f\u6839\u636e\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u52a8\u6001\u68c0\u6d4b\u51b2\u7a81\u3002", "method": "\u5b9a\u4e49\u4e86\u52a8\u6001\u5e76\u53d1\u6982\u5ff5\uff0c\u64cd\u4f5c\u4ec5\u5728\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u4e0b\u5fc5\u987b\u4e0e\u5e76\u53d1\u64cd\u4f5c\u4ef2\u88c1\u65f6\u624d\u4f7f\u7528\u5f3a\u540c\u6b65\u539f\u8bed\u3002\u63d0\u51fa\u4e86\u52a8\u6001\u5e76\u53d1\u7684\u901a\u7528\u6784\u9020\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u52a8\u6001\u5e76\u53d1\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u73b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u9700\u8981\u65f6\u624d\u8fdb\u884c\u540c\u6b65\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u5f00\u9500\u3002", "conclusion": "\u52a8\u6001\u5e76\u53d1\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u7cfb\u7edf\u72b6\u6001\u667a\u80fd\u5730\u4f7f\u7528\u540c\u6b65\u673a\u5236\uff0c\u63d0\u9ad8\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2511.03995", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03995", "abs": "https://arxiv.org/abs/2511.03995", "authors": ["Shiyin Lin"], "title": "Hybrid Fuzzing with LLM-Guided Input Mutation and Semantic Feedback", "comment": null, "summary": "Software fuzzing has become a cornerstone in automated vulnerability\ndiscovery, yet existing mutation strategies often lack semantic awareness,\nleading to redundant test cases and slow exploration of deep program states. In\nthis work, I present a hybrid fuzzing framework that integrates static and\ndynamic analysis with Large Language Model (LLM)-guided input mutation and\nsemantic feedback. Static analysis extracts control-flow and data-flow\ninformation, which is transformed into structured prompts for the LLM to\ngenerate syntactically valid and semantically diverse inputs. During execution,\nI augment traditional coverage-based feedback with semantic feedback\nsignals-derived from program state changes, exception types, and output\nsemantics-allowing the fuzzer to prioritize inputs that trigger novel program\nbehaviors beyond mere code coverage. I implement our approach atop AFL++,\ncombining program instrumentation with embedding-based semantic similarity\nmetrics to guide seed selection. Evaluation on real-world open-source targets,\nincluding libpng, tcpdump, and sqlite, demonstrates that our method achieves\nfaster time-to-first-bug, higher semantic diversity, and a competitive number\nof unique bugs compared to state-of-the-art fuzzers. This work highlights the\npotential of combining LLM reasoning with semantic-aware feedback to accelerate\nand deepen vulnerability discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9759\u6001\u5206\u6790\u3001\u52a8\u6001\u5206\u6790\u548cLLM\u5f15\u5bfc\u8f93\u5165\u53d8\u5f02\u7684\u6df7\u5408\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u53cd\u9988\u52a0\u901f\u6f0f\u6d1e\u53d1\u73b0\u3002", "motivation": "\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u7684\u53d8\u5f02\u7b56\u7565\u7f3a\u4e4f\u8bed\u4e49\u610f\u8bc6\uff0c\u5bfc\u81f4\u5197\u4f59\u6d4b\u8bd5\u7528\u4f8b\u548c\u7a0b\u5e8f\u6df1\u5c42\u72b6\u6001\u63a2\u7d22\u7f13\u6162\u3002", "method": "\u96c6\u6210\u9759\u6001\u548c\u52a8\u6001\u5206\u6790\uff0c\u5229\u7528LLM\u751f\u6210\u8bed\u4e49\u591a\u6837\u7684\u8f93\u5165\uff0c\u7ed3\u5408\u8986\u76d6\u7387\u53cd\u9988\u548c\u8bed\u4e49\u53cd\u9988\uff08\u7a0b\u5e8f\u72b6\u6001\u53d8\u5316\u3001\u5f02\u5e38\u7c7b\u578b\u3001\u8f93\u51fa\u8bed\u4e49\uff09\u6765\u6307\u5bfc\u79cd\u5b50\u9009\u62e9\u3002", "result": "\u5728libpng\u3001tcpdump\u548csqlite\u7b49\u771f\u5b9e\u5f00\u6e90\u76ee\u6807\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u9996\u6b21\u53d1\u73b0\u6f0f\u6d1e\u65f6\u95f4\u3001\u66f4\u9ad8\u7684\u8bed\u4e49\u591a\u6837\u6027\u548c\u6709\u7ade\u4e89\u529b\u7684\u72ec\u7279\u6f0f\u6d1e\u6570\u91cf\u3002", "conclusion": "\u7ed3\u5408LLM\u63a8\u7406\u548c\u8bed\u4e49\u611f\u77e5\u53cd\u9988\u6709\u6f5c\u529b\u52a0\u901f\u548c\u6df1\u5316\u6f0f\u6d1e\u53d1\u73b0\u3002"}}
{"id": "2511.04076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04076", "abs": "https://arxiv.org/abs/2511.04076", "authors": ["Hao Li", "Haotian Chen", "Ruoyuan Gong", "Juanjuan Wang", "Hao Jiang"], "title": "Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents", "comment": "Accepted by AAAI AISI 2026", "summary": "Redistricting plays a central role in shaping how votes are translated into\npolitical power. While existing computational methods primarily aim to generate\nlarge ensembles of legally valid districting plans, they often neglect the\nstrategic dynamics involved in the selection process. This oversight creates\nopportunities for partisan actors to cherry-pick maps that, while technically\ncompliant, are politically advantageous. Simply satisfying formal constraints\ndoes not ensure fairness when the selection process itself can be manipulated.\nWe propose \\textbf{Agentmandering}, a framework that reimagines redistricting\nas a turn-based negotiation between two agents representing opposing political\ninterests. Drawing inspiration from game-theoretic ideas, particularly the\n\\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction\ninto the redistricting process via large language model (LLM) agents. Agents\nalternate between selecting and freezing districts from a small set of\ncandidate maps, gradually partitioning the state through constrained and\ninterpretable choices. Evaluation on post-2020 U.S. Census data across all\nstates shows that Agentmandering significantly reduces partisan bias and\nunfairness, while achieving 2 to 3 orders of magnitude lower variance than\nstandard baselines. These results demonstrate both fairness and stability,\nespecially in swing-state scenarios. Our code is available at\nhttps://github.com/Lihaogx/AgentMandering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agentmandering\u6846\u67b6\uff0c\u5c06\u9009\u533a\u91cd\u5212\u91cd\u65b0\u6784\u60f3\u4e3a\u4e24\u4e2a\u4ee3\u8868\u5bf9\u7acb\u653f\u6cbb\u5229\u76ca\u7684\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u56de\u5408\u5236\u8c08\u5224\uff0c\u901a\u8fc7LLM\u667a\u80fd\u4f53\u5c06\u6218\u7565\u4e92\u52a8\u5d4c\u5165\u5230\u9009\u533a\u91cd\u5212\u8fc7\u7a0b\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u515a\u6d3e\u504f\u89c1\u548c\u4e0d\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u4e3b\u8981\u751f\u6210\u5927\u91cf\u5408\u6cd5\u9009\u533a\u91cd\u5212\u65b9\u6848\uff0c\u4f46\u5ffd\u89c6\u4e86\u9009\u62e9\u8fc7\u7a0b\u4e2d\u7684\u6218\u7565\u52a8\u6001\uff0c\u8fd9\u4e3a\u515a\u6d3e\u884c\u4e3a\u8005\u6311\u9009\u6280\u672f\u4e0a\u5408\u89c4\u4f46\u653f\u6cbb\u4e0a\u6709\u5229\u7684\u5730\u56fe\u521b\u9020\u4e86\u673a\u4f1a\u3002\u4ec5\u6ee1\u8db3\u5f62\u5f0f\u7ea6\u675f\u5e76\u4e0d\u80fd\u786e\u4fdd\u516c\u5e73\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6e38\u620f\u7406\u8bba\u7684Choose-and-Freeze\u534f\u8bae\uff0c\u4e24\u4e2aLLM\u667a\u80fd\u4f53\u8f6e\u6d41\u4ece\u5c11\u91cf\u5019\u9009\u5730\u56fe\u4e2d\u9009\u62e9\u548c\u51bb\u7ed3\u9009\u533a\uff0c\u901a\u8fc7\u53d7\u7ea6\u675f\u4e14\u53ef\u89e3\u91ca\u7684\u9009\u62e9\u9010\u6b65\u5212\u5206\u5dde\u3002", "result": "\u57282020\u5e74\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAgentmandering\u663e\u8457\u51cf\u5c11\u4e86\u515a\u6d3e\u504f\u89c1\u548c\u4e0d\u516c\u5e73\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6bd4\u6807\u51c6\u57fa\u7ebf\u4f4e2\u52303\u4e2a\u6570\u91cf\u7ea7\u7684\u65b9\u5dee\uff0c\u5728\u6447\u6446\u5dde\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u516c\u5e73\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "Agentmandering\u6846\u67b6\u901a\u8fc7\u5c06\u6218\u7565\u4e92\u52a8\u5d4c\u5165\u9009\u533a\u91cd\u5212\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9009\u533a\u91cd\u5212\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u4e3a\u6c11\u4e3b\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u548c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04021", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.04021", "abs": "https://arxiv.org/abs/2511.04021", "authors": ["Sergio Demian Lerner", "Ariel Autoransky"], "title": "OTS-PC: OTS-based Payment Channels for the Lightning Network", "comment": null, "summary": "We present a new type of bidirectional payment channel based on One-Time\nSignatures on state sequence numbers. This new construction is simpler than the\nPoon-Dryja construction, but provides a number of benefits such as $O(1)$\nstorage per channel, minimal information leakage, and compatibility with\nLightning Network routing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u5e8f\u5217\u53f7\u4e00\u6b21\u6027\u7b7e\u540d\u7684\u65b0\u578b\u53cc\u5411\u652f\u4ed8\u901a\u9053\uff0c\u76f8\u6bd4Poon-Dryja\u7ed3\u6784\u66f4\u7b80\u5355\uff0c\u5177\u6709O(1)\u5b58\u50a8\u3001\u6700\u5c0f\u4fe1\u606f\u6cc4\u9732\u548c\u4e0eLightning Network\u8def\u7531\u517c\u5bb9\u7b49\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u652f\u4ed8\u901a\u9053\u7ed3\u6784\u5982Poon-Dryja\u8f83\u4e3a\u590d\u6742\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u4f9b\u66f4\u7b80\u5355\u7684\u5b9e\u73b0\u3001\u66f4\u597d\u7684\u5b58\u50a8\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u4f7f\u7528\u72b6\u6001\u5e8f\u5217\u53f7\u7684\u4e00\u6b21\u6027\u7b7e\u540d\u6765\u6784\u5efa\u53cc\u5411\u652f\u4ed8\u901a\u9053\uff0c\u901a\u8fc7\u5e8f\u5217\u53f7\u7ba1\u7406\u901a\u9053\u72b6\u6001\u66f4\u65b0\u3002", "result": "\u65b0\u7ed3\u6784\u6bd4Poon-Dryja\u66f4\u7b80\u5355\uff0c\u5b9e\u73b0\u6bcf\u901a\u9053O(1)\u5b58\u50a8\uff0c\u6700\u5c0f\u5316\u4fe1\u606f\u6cc4\u9732\uff0c\u5e76\u4fdd\u6301\u4e0eLightning Network\u8def\u7531\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "\u57fa\u4e8e\u72b6\u6001\u5e8f\u5217\u53f7\u4e00\u6b21\u6027\u7b7e\u540d\u7684\u652f\u4ed8\u901a\u9053\u7ed3\u6784\u662f\u73b0\u6709\u65b9\u6848\u7684\u6709\u6548\u66ff\u4ee3\uff0c\u63d0\u4f9b\u4e86\u7b80\u5316\u5b9e\u73b0\u3001\u9ad8\u6548\u5b58\u50a8\u548c\u826f\u597d\u9690\u79c1\u4fdd\u62a4\u7684\u5e73\u8861\u3002"}}
{"id": "2511.04093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04093", "abs": "https://arxiv.org/abs/2511.04093", "authors": ["Yuanning Cui", "Zequn Sun", "Wei Hu", "Zhangjie Fu"], "title": "KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering", "comment": null, "summary": "Large language models (LLMs) excel at reasoning but struggle with\nknowledge-intensive questions due to limited context and parametric knowledge.\nHowever, existing methods that rely on finetuned LLMs or GNN retrievers are\nlimited by dataset-specific tuning and scalability on large or unseen graphs.\nWe propose the LLM-KGFR collaborative framework, where an LLM works with a\nstructured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR\nencodes relations using LLM-generated descriptions and initializes entities\nbased on their roles in the question, enabling zero-shot generalization to\nunseen KGs. To handle large graphs efficiently, it employs Asymmetric\nProgressive Propagation (APP)- a stepwise expansion that selectively limits\nhigh-degree nodes while retaining informative paths. Through node-, edge-, and\npath-level interfaces, the LLM iteratively requests candidate answers,\nsupporting facts, and reasoning paths, forming a controllable reasoning loop.\nExperiments demonstrate that LLM-KGFR achieves strong performance while\nmaintaining scalability and generalization, providing a practical solution for\nKG-augmented reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM-KGFR\u534f\u4f5c\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u68c0\u7d22\u5668\u7ed3\u5408\uff0c\u901a\u8fc7\u5173\u7cfb\u63cf\u8ff0\u7f16\u7801\u548c\u6e10\u8fdb\u5f0f\u4f20\u64ad\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u89e3\u51b3\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u9898\u65f6\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u7279\u5b9a\u8c03\u4f18\u548c\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "LLM-KGFR\u6846\u67b6\u5305\u542b\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u68c0\u7d22\u5668\uff0c\u4f7f\u7528LLM\u751f\u6210\u7684\u5173\u7cfb\u63cf\u8ff0\u7f16\u7801\u5173\u7cfb\uff0c\u57fa\u4e8e\u95ee\u9898\u4e2d\u7684\u89d2\u8272\u521d\u59cb\u5316\u5b9e\u4f53\uff0c\u91c7\u7528\u975e\u5bf9\u79f0\u6e10\u8fdb\u4f20\u64ad\u7b56\u7565\u9010\u6b65\u6269\u5c55\uff0c\u901a\u8fc7\u8282\u70b9\u3001\u8fb9\u548c\u8def\u5f84\u7ea7\u63a5\u53e3\u652f\u6301\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLLM-KGFR\u5728\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u52b2\u6027\u80fd\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LLM-KGFR\u6846\u67b6\u901a\u8fc7LLM\u4e0e\u7ed3\u6784\u5316\u68c0\u7d22\u5668\u7684\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u7684\u6311\u6218\uff0c\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u5927\u89c4\u6a21\u56fe\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.04215", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04215", "abs": "https://arxiv.org/abs/2511.04215", "authors": ["Hongwei Yao", "Yun Xia", "Shuo Shao", "Haoran Shi", "Tong Qiao", "Cong Wang"], "title": "Black-Box Guardrail Reverse-engineering Attack", "comment": null, "summary": "Large language models (LLMs) increasingly employ guardrails to enforce\nethical, legal, and application-specific constraints on their outputs. While\neffective at mitigating harmful responses, these guardrails introduce a new\nclass of vulnerabilities by exposing observable decision patterns. In this\nwork, we present the first study of black-box LLM guardrail reverse-engineering\nattacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement\nlearning-based framework that leverages genetic algorithm-driven data\naugmentation to approximate the decision-making policy of victim guardrails. By\niteratively collecting input-output pairs, prioritizing divergence cases, and\napplying targeted mutations and crossovers, our method incrementally converges\ntoward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on\nthree widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,\nand demonstrate that it achieves an rule matching rate exceeding 0.92 while\nrequiring less than $85 in API costs. These findings underscore the practical\nfeasibility of guardrail extraction and highlight significant security risks\nfor current LLM safety mechanisms. Our findings expose critical vulnerabilities\nin current guardrail designs and highlight the urgent need for more robust\ndefense mechanisms in LLM deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9ed1\u76d2LLM\u62a4\u680f\u9006\u5411\u5de5\u7a0b\u653b\u51fb\u7814\u7a76\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u9057\u4f20\u7b97\u6cd5\u9a71\u52a8\u7684\u6570\u636e\u589e\u5f3a\u6765\u8fd1\u4f3c\u53d7\u5bb3\u8005\u62a4\u680f\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u5728\u4e09\u4e2a\u5546\u4e1a\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc70.92\u7684\u89c4\u5219\u5339\u914d\u7387\uff0c\u4e14API\u6210\u672c\u4f4e\u4e8e85\u7f8e\u5143\u3002", "motivation": "\u867d\u7136LLM\u62a4\u680f\u80fd\u6709\u6548\u51cf\u8f7b\u6709\u5bb3\u54cd\u5e94\uff0c\u4f46\u5b83\u4eec\u66b4\u9732\u4e86\u53ef\u89c2\u5bdf\u7684\u51b3\u7b56\u6a21\u5f0f\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u6f0f\u6d1e\u7c7b\u522b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u62a4\u680f\u9006\u5411\u5de5\u7a0b\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u5f53\u524dLLM\u5b89\u5168\u673a\u5236\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51faGuardrail Reverse-engineering Attack (GRA)\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u9057\u4f20\u7b97\u6cd5\u9a71\u52a8\u7684\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u8fed\u4ee3\u6536\u96c6\u8f93\u5165-\u8f93\u51fa\u5bf9\u3001\u4f18\u5148\u5904\u7406\u5206\u6b67\u6848\u4f8b\u3001\u5e94\u7528\u5b9a\u5411\u7a81\u53d8\u548c\u4ea4\u53c9\uff0c\u9010\u6b65\u6536\u655b\u5230\u53d7\u5bb3\u8005\u62a4\u680f\u7684\u9ad8\u4fdd\u771f\u66ff\u4ee3\u6a21\u578b\u3002", "result": "\u5728ChatGPT\u3001DeepSeek\u548cQwen\u4e09\u4e2a\u5546\u4e1a\u7cfb\u7edf\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cGRA\u5b9e\u73b0\u4e86\u8d85\u8fc70.92\u7684\u89c4\u5219\u5339\u914d\u7387\uff0cAPI\u6210\u672c\u4f4e\u4e8e85\u7f8e\u5143\uff0c\u8bc1\u660e\u4e86\u62a4\u680f\u63d0\u53d6\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u62a4\u680f\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u5728LLM\u90e8\u7f72\u4e2d\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u673a\u5236\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2511.04261", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.04261", "abs": "https://arxiv.org/abs/2511.04261", "authors": ["Ming Liu"], "title": "A Parallel Region-Adaptive Differential Privacy Framework for Image Pixelization", "comment": null, "summary": "The widespread deployment of high-resolution visual sensing systems, coupled\nwith the rise of foundation models, has amplified privacy risks in video-based\napplications. Differentially private pixelization offers mathematically\nguaranteed protection for visual data through grid-based noise addition, but\nchallenges remain in preserving task-relevant fidelity, achieving scalability,\nand enabling efficient real-time deployment. To address this, we propose a\nnovel parallel, region-adaptive pixelization framework that combines the\ntheoretical rigor of differential privacy with practical efficiency. Our method\nadaptively adjusts grid sizes and noise scales based on regional complexity,\nleveraging GPU parallelism to achieve significant runtime acceleration compared\nto the classical baseline. A lightweight storage scheme is introduced by\nretaining only essential noisy statistics, significantly reducing space\noverhead. Formal privacy analysis is provided under the Laplace mechanism and\nparallel composition theorem. Extensive experiments on the PETS, Venice-2, and\nPPM-100 datasets demonstrate favorable privacy-utility trade-offs and\nsignificant runtime/storage reductions. A face re-identification attack\nexperiment on CelebA further confirms the method's effectiveness in preventing\nidentity inference. This validates its suitability for real-time\nprivacy-critical applications such as elderly care, smart home monitoring,\ndriver behavior analysis, and crowd behavior monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u3001\u533a\u57df\u81ea\u9002\u5e94\u7684\u50cf\u7d20\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u7684\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u9645\u6548\u7387\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u7f51\u683c\u5927\u5c0f\u548c\u566a\u58f0\u5c3a\u5ea6\uff0c\u5229\u7528GPU\u5e76\u884c\u5316\u5b9e\u73b0\u8fd0\u884c\u65f6\u52a0\u901f\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u5b58\u50a8\u65b9\u6848\u51cf\u5c11\u7a7a\u95f4\u5f00\u9500\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u4f20\u611f\u7cfb\u7edf\u7684\u5e7f\u6cdb\u90e8\u7f72\u548c\u57fa\u7840\u6a21\u578b\u7684\u5174\u8d77\u52a0\u5267\u4e86\u89c6\u9891\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u4f9b\u6570\u5b66\u4fdd\u8bc1\u7684\u9690\u79c1\u4fdd\u62a4\u53c8\u80fd\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u4fdd\u771f\u5ea6\u3001\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u5b9e\u65f6\u90e8\u7f72\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5e76\u884c\u533a\u57df\u81ea\u9002\u5e94\u50cf\u7d20\u5316\u6846\u67b6\uff0c\u57fa\u4e8e\u533a\u57df\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u7f51\u683c\u5927\u5c0f\u548c\u566a\u58f0\u5c3a\u5ea6\uff0c\u5229\u7528GPU\u5e76\u884c\u5316\u52a0\u901f\u8fd0\u884c\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u5b58\u50a8\u65b9\u6848\u4ec5\u4fdd\u7559\u5fc5\u8981\u7684\u566a\u58f0\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "\u5728PETS\u3001Venice-2\u548cPPM-100\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u826f\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u548c\u5b58\u50a8\u5f00\u9500\uff0c\u5728CelebA\u4e0a\u7684\u4eba\u8138\u91cd\u8bc6\u522b\u653b\u51fb\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u9632\u6b62\u8eab\u4efd\u63a8\u65ad\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5b9e\u65f6\u9690\u79c1\u5173\u952e\u5e94\u7528\uff0c\u5982\u8001\u5e74\u62a4\u7406\u3001\u667a\u80fd\u5bb6\u5c45\u76d1\u63a7\u3001\u9a7e\u9a76\u5458\u884c\u4e3a\u5206\u6790\u548c\u4eba\u7fa4\u884c\u4e3a\u76d1\u63a7\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u80fd\u529b\u3002"}}
{"id": "2511.04220", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04220", "abs": "https://arxiv.org/abs/2511.04220", "authors": ["Alan Seroul", "Th\u00e9o Fagnoni", "In\u00e8s Adnani", "Dana O. Mohamed", "Phillip Kingston"], "title": "Opus: A Quantitative Framework for Workflow Evaluation", "comment": null, "summary": "This paper introduces the Opus Workflow Evaluation Framework, a\nprobabilistic-normative formulation for quantifying Workflow quality and\nefficiency. It integrates notions of correctness, reliability, and cost into a\ncoherent mathematical model that enables direct comparison, scoring, and\noptimization of Workflows. The framework combines the Opus Workflow Reward, a\nprobabilistic function estimating expected performance through success\nlikelihood, resource usage, and output gain, with the Opus Workflow Normative\nPenalties, a set of measurable functions capturing structural and informational\nquality across Cohesion, Coupling, Observability, and Information Hygiene. It\nsupports automated Workflow assessment, ranking, and optimization within modern\nautomation systems such as Opus and can be integrated into Reinforcement\nLearning loops to guide Workflow discovery and refinement. In this paper, we\nintroduce the Opus Workflow Reward model that formalizes Workflow success as a\nprobabilistic expectation over costs and outcomes. We define measurable Opus\nWorkflow Normative Penalties capturing structural, semantic, and signal-related\nproperties of Workflows. Finally, we propose a unified optimization formulation\nfor identifying and ranking optimal Workflows under joint Reward-Penalty\ntrade-offs.", "AI": {"tldr": "Opus\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6846\u67b6\u662f\u4e00\u4e2a\u6982\u7387-\u89c4\u8303\u5316\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5de5\u4f5c\u6d41\u8d28\u91cf\u548c\u6548\u7387\uff0c\u7ed3\u5408\u4e86\u6b63\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u6210\u672c\uff0c\u652f\u6301\u5de5\u4f5c\u6d41\u7684\u6bd4\u8f83\u3001\u8bc4\u5206\u548c\u4f18\u5316\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\u6765\u91cf\u5316\u5de5\u4f5c\u6d41\u8d28\u91cf\uff0c\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u3001\u6392\u540d\u548c\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u73b0\u4ee3\u81ea\u52a8\u5316\u7cfb\u7edf\u4e2d\u3002", "method": "\u7ed3\u5408Opus\u5de5\u4f5c\u6d41\u5956\u52b1\uff08\u57fa\u4e8e\u6210\u529f\u6982\u7387\u3001\u8d44\u6e90\u4f7f\u7528\u548c\u8f93\u51fa\u589e\u76ca\u7684\u6982\u7387\u51fd\u6570\uff09\u548cOpus\u5de5\u4f5c\u6d41\u89c4\u8303\u60e9\u7f5a\uff08\u8861\u91cf\u7ed3\u6784\u6027\u548c\u4fe1\u606f\u8d28\u91cf\u7684\u51fd\u6570\uff09\uff0c\u5f62\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u4f18\u5316\u516c\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u5de5\u4f5c\u6d41\u81ea\u52a8\u8bc4\u4f30\u3001\u6392\u540d\u548c\u4f18\u5316\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u6307\u5bfc\u5de5\u4f5c\u6d41\u53d1\u73b0\u548c\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de5\u4f5c\u6d41\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\uff0c\u652f\u6301\u5728\u73b0\u4ee3\u81ea\u52a8\u5316\u7cfb\u7edf\u4e2d\u8fdb\u884c\u6709\u6548\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u3002"}}
{"id": "2511.04409", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.04409", "abs": "https://arxiv.org/abs/2511.04409", "authors": ["Giacomo Zonneveld", "Giulia Rafaiani", "Massimo Battaglioni", "Marco Baldi"], "title": "Data Certification Strategies for Blockchain-based Traceability Systems", "comment": null, "summary": "The use of blockchains for data certification and traceability is now well\nestablished in both the literature and practical applications. However, while\nblockchain-based certification of individual data is clear and straightforward,\nthe use of blockchain to certify large amounts of data produced on a nearly\ncontinuous basis still poses some challenges. In such a case, in fact, it is\nfirst necessary to collect the data in an off-chain buffer, and then to\norganize it, e.g., via Merkle trees, in order to keep the size and quantity of\ncertification data to be written to the blockchain small. In this paper, we\nconsider a typical system for blockchain-based traceability of a production\nprocess, and propose and comparatively analyze some strategies for certifying\nthe data of such a process on blockchain, while maintaining the possibility of\nverifying their certification in a decentralized way.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u533a\u5757\u94fe\u5728\u5927\u89c4\u6a21\u8fde\u7eed\u6570\u636e\u8ba4\u8bc1\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u6570\u636e\u8ba4\u8bc1\u7b56\u7565\uff0c\u65e8\u5728\u4fdd\u6301\u53bb\u4e2d\u5fc3\u5316\u9a8c\u8bc1\u80fd\u529b\u7684\u540c\u65f6\u4f18\u5316\u533a\u5757\u94fe\u5b58\u50a8\u6548\u7387\u3002", "motivation": "\u533a\u5757\u94fe\u5728\u5355\u4e2a\u6570\u636e\u8ba4\u8bc1\u65b9\u9762\u5df2\u6210\u719f\u5e94\u7528\uff0c\u4f46\u5728\u5904\u7406\u5927\u89c4\u6a21\u8fde\u7eed\u751f\u6210\u7684\u6570\u636e\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u7f13\u51b2\u3001\u7ec4\u7ec7\u548c\u8ba4\u8bc1\u6548\u7387\u95ee\u9898\u3002", "method": "\u8003\u8651\u5178\u578b\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u751f\u4ea7\u8fc7\u7a0b\u8ffd\u6eaf\u7cfb\u7edf\uff0c\u63d0\u51fa\u5e76\u6bd4\u8f83\u591a\u79cd\u6570\u636e\u8ba4\u8bc1\u7b56\u7565\uff0c\u5305\u62ec\u4f7f\u7528Merkle\u6811\u7b49\u65b9\u6cd5\u6765\u51cf\u5c11\u5199\u5165\u533a\u5757\u94fe\u7684\u8ba4\u8bc1\u6570\u636e\u91cf\u3002", "result": "\u5f00\u53d1\u4e86\u6709\u6548\u7684\u7b56\u7565\u6765\u8ba4\u8bc1\u751f\u4ea7\u8fc7\u7a0b\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u53bb\u4e2d\u5fc3\u5316\u9a8c\u8bc1\u7684\u53ef\u80fd\u6027\uff0c\u4f18\u5316\u4e86\u533a\u5757\u94fe\u5b58\u50a8\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u8fde\u7eed\u6570\u636e\u5728\u533a\u5757\u94fe\u4e0a\u7684\u8ba4\u8bc1\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04235", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.04235", "abs": "https://arxiv.org/abs/2511.04235", "authors": ["Zhengru Fang", "Yu Guo", "Jingjing Wang", "Yuang Zhang", "Haonan An", "Yinhai Wang", "Yuguang Fang"], "title": "Shared Spatial Memory Through Predictive Coding", "comment": "We have prepared the open-source code and video demonstration pages:\n  1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html", "summary": "Sharing and reconstructing a consistent spatial memory is a critical\nchallenge in multi-agent systems, where partial observability and limited\nbandwidth often lead to catastrophic failures in coordination. We introduce a\nmulti-agent predictive coding framework that formulate coordination as the\nminimization of mutual uncertainty among agents. Instantiated as an information\nbottleneck objective, it prompts agents to learn not only who and what to\ncommunicate but also when. At the foundation of this framework lies a\ngrid-cell-like metric as internal spatial coding for self-localization,\nemerging spontaneously from self-supervised motion prediction. Building upon\nthis internal spatial code, agents gradually develop a bandwidth-efficient\ncommunication mechanism and specialized neural populations that encode\npartners' locations: an artificial analogue of hippocampal social place cells\n(SPCs). These social representations are further enacted by a hierarchical\nreinforcement learning policy that actively explores to reduce joint\nuncertainty. On the Memory-Maze benchmark, our approach shows exceptional\nresilience to bandwidth constraints: success degrades gracefully from 73.5% to\n64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast\nbaseline collapses from 67.6% to 28.6%. Our findings establish a theoretically\nprincipled and biologically plausible basis for how complex social\nrepresentations emerge from a unified predictive drive, leading to social\ncollective intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u667a\u80fd\u4f53\u95f4\u7684\u4e92\u4e0d\u786e\u5b9a\u6027\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7a7a\u95f4\u8bb0\u5fc6\u5171\u4eab\u548c\u91cd\u5efa\u95ee\u9898\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u7f51\u683c\u7ec6\u80de\u72b6\u5ea6\u91cf\u4f5c\u4e3a\u5185\u90e8\u7a7a\u95f4\u7f16\u7801\uff0c\u81ea\u53d1\u5f62\u6210\u793e\u4f1a\u4f4d\u7f6e\u7ec6\u80de\uff0c\u5e76\u5728\u5e26\u5bbd\u53d7\u9650\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u6709\u9650\u5e26\u5bbd\u5e38\u5bfc\u81f4\u534f\u8c03\u5931\u8d25\uff0c\u9700\u8981\u89e3\u51b3\u7a7a\u95f4\u8bb0\u5fc6\u5171\u4eab\u548c\u91cd\u5efa\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u76ee\u6807\uff0c\u8ba9\u667a\u80fd\u4f53\u5b66\u4e60\u4f55\u65f6\u3001\u4e0e\u8c01\u3001\u6c9f\u901a\u4ec0\u4e48\uff1b\u4f7f\u7528\u7f51\u683c\u7ec6\u80de\u72b6\u5ea6\u91cf\u4f5c\u4e3a\u5185\u90e8\u7a7a\u95f4\u7f16\u7801\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8fd0\u52a8\u9884\u6d4b\u81ea\u53d1\u5f62\u6210\uff1b\u6784\u5efa\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e3b\u52a8\u63a2\u7d22\u4ee5\u51cf\u5c11\u8054\u5408\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728Memory-Maze\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5bf9\u5e26\u5bbd\u9650\u5236\u8868\u73b0\u51fa\u5353\u8d8a\u97e7\u6027\uff1a\u5e26\u5bbd\u4ece128\u4f4d/\u6b65\u964d\u81f34\u4f4d/\u6b65\u65f6\uff0c\u6210\u529f\u7387\u4ece73.5%\u4f18\u96c5\u964d\u81f364.4%\uff0c\u800c\u5168\u5e7f\u64ad\u57fa\u7ebf\u4ece67.6%\u5d29\u6e83\u81f328.6%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u590d\u6742\u793e\u4f1a\u8868\u5f81\u5982\u4f55\u4ece\u7edf\u4e00\u7684\u9884\u6d4b\u9a71\u52a8\u4e2d\u6d8c\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u539f\u5219\u548c\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u793e\u4f1a\u96c6\u4f53\u667a\u80fd\u3002"}}
{"id": "2511.04285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04285", "abs": "https://arxiv.org/abs/2511.04285", "authors": ["Zeng Zhiyuan", "Jiashuo Liu", "Zhangyue Yin", "Ge Zhang", "Wenhao Huang", "Xipeng Qiu"], "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization", "comment": null, "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.", "AI": {"tldr": "RLoop\u662f\u4e00\u4e2a\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7b56\u7565\u521d\u59cb\u5316\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u826f\u6027\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u8bad\u7ec3\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u6a21\u578b\u83b7\u5f97\u8bad\u7ec3\u5956\u52b1\u4f46\u5931\u53bb\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4e3b\u8981\u7531\u7b56\u7565\u8fc7\u5ea6\u4e13\u4e1a\u5316\u548c\u8bad\u7ec3\u671f\u95f4\u4ea7\u751f\u7684\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u7684\u707e\u96be\u6027\u9057\u5fd8\u9a71\u52a8\u3002", "method": "\u5f15\u5165RLoop\u6846\u67b6\uff0c\u57fa\u4e8e\u8fed\u4ee3\u7b56\u7565\u521d\u59cb\u5316\uff1a\u9996\u5148\u4f7f\u7528RL\u4ece\u7ed9\u5b9a\u7b56\u7565\u63a2\u7d22\u89e3\u7a7a\u95f4\uff0c\u7136\u540e\u8fc7\u6ee4\u6210\u529f\u8f68\u8ff9\u521b\u5efa\u4e13\u5bb6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u5fae\u8c03(RFT)\u6765\u4f18\u5316\u521d\u59cb\u7b56\u7565\uff0c\u4e3a\u4e0b\u4e00\u6b21\u8fed\u4ee3\u521b\u5efa\u66f4\u597d\u7684\u8d77\u70b9\u3002", "result": "RLoop\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u5e76\u663e\u8457\u6539\u5584\u6cdb\u5316\uff0c\u76f8\u6bd4\u539f\u59cbRL\u5c06\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53479%\uff0cpass@32\u63d0\u5347\u8d85\u8fc715%\u3002", "conclusion": "RLoop\u901a\u8fc7\u5c06\u77ac\u6001\u7b56\u7565\u53d8\u5316\u8f6c\u5316\u4e3a\u7a33\u5065\u6027\u80fd\u589e\u76ca\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04312", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04312", "abs": "https://arxiv.org/abs/2511.04312", "authors": ["Jacob Lysn\u00e6s-Larsen", "Marte Eggen", "Inga Str\u00fcmke"], "title": "Probing the Probes: Methods and Metrics for Concept Alignment", "comment": "29 pages, 17 figures", "summary": "In explainable AI, Concept Activation Vectors (CAVs) are typically obtained\nby training linear classifier probes to detect human-understandable concepts as\ndirections in the activation space of deep neural networks. It is widely\nassumed that a high probe accuracy indicates a CAV faithfully representing its\ntarget concept. However, we show that the probe's classification accuracy alone\nis an unreliable measure of concept alignment, i.e., the degree to which a CAV\ncaptures the intended concept. In fact, we argue that probes are more likely to\ncapture spurious correlations than they are to represent only the intended\nconcept. As part of our analysis, we demonstrate that deliberately misaligned\nprobes constructed to exploit spurious correlations, achieve an accuracy close\nto that of standard probes. To address this severe problem, we introduce a\nnovel concept localization method based on spatial linear attribution, and\nprovide a comprehensive comparison of it to existing feature visualization\ntechniques for detecting and mitigating concept misalignment. We further\npropose three classes of metrics for quantitatively assessing concept\nalignment: hard accuracy, segmentation scores, and augmentation robustness. Our\nanalysis shows that probes with translation invariance and spatial alignment\nconsistently increase concept alignment. These findings highlight the need for\nalignment-based evaluation metrics rather than probe accuracy, and the\nimportance of tailoring probes to both the model architecture and the nature of\nthe target concept.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5728\u53ef\u89e3\u91caAI\u4e2d\uff0c\u4ec5\u51ed\u7ebf\u6027\u5206\u7c7b\u5668\u63a2\u9488\u7684\u51c6\u786e\u7387\u6765\u8bc4\u4f30\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf(CAV)\u7684\u6982\u5ff5\u5bf9\u9f50\u5ea6\u662f\u4e0d\u53ef\u9760\u7684\uff0c\u56e0\u4e3a\u63a2\u9488\u5bb9\u6613\u6355\u6349\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u771f\u6b63\u6982\u5ff5\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u7a7a\u95f4\u7ebf\u6027\u5f52\u56e0\u7684\u65b0\u6982\u5ff5\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e09\u7c7b\u91cf\u5316\u8bc4\u4f30\u6982\u5ff5\u5bf9\u9f50\u7684\u6307\u6807\u3002", "motivation": "\u5f53\u524dCAV\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u63a2\u9488\u5206\u7c7b\u51c6\u786e\u7387\u6765\u8bc4\u4f30\u6982\u5ff5\u5bf9\u9f50\u5ea6\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u8bc4\u4f30\u65b9\u5f0f\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u63a2\u9488\u53ef\u80fd\u6355\u6349\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u771f\u6b63\u6982\u5ff5\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u53ef\u89e3\u91caAI\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7a7a\u95f4\u7ebf\u6027\u5f52\u56e0\u7684\u65b0\u6982\u5ff5\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5e76\u4e0e\u73b0\u6709\u7279\u5f81\u53ef\u89c6\u5316\u6280\u672f\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\u3002\u5f15\u5165\u4e86\u4e09\u7c7b\u91cf\u5316\u8bc4\u4f30\u6307\u6807\uff1a\u786c\u51c6\u786e\u7387\u3001\u5206\u5272\u5206\u6570\u548c\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5177\u6709\u5e73\u79fb\u4e0d\u53d8\u6027\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u63a2\u9488\u80fd\u663e\u8457\u63d0\u9ad8\u6982\u5ff5\u5bf9\u9f50\u5ea6\u3002\u6545\u610f\u6784\u5efa\u7684\u9519\u4f4d\u63a2\u9488\u5229\u7528\u865a\u5047\u76f8\u5173\u6027\u4e5f\u80fd\u8fbe\u5230\u63a5\u8fd1\u6807\u51c6\u63a2\u9488\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u5bf9\u9f50\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\u800c\u975e\u4ec5\u4f9d\u8d56\u63a2\u9488\u51c6\u786e\u7387\uff0c\u4e14\u5e94\u6839\u636e\u6a21\u578b\u67b6\u6784\u548c\u76ee\u6807\u6982\u5ff5\u7279\u6027\u5b9a\u5236\u63a2\u9488\u8bbe\u8ba1\u3002"}}
{"id": "2511.04550", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04550", "abs": "https://arxiv.org/abs/2511.04550", "authors": ["Dhruv Deepak Agarwal", "Aswani Kumar Cherukuri"], "title": "Confidential Computing for Cloud Security: Exploring Hardware based Encryption Using Trusted Execution Environments", "comment": null, "summary": "The growth of cloud computing has revolutionized data processing and storage\ncapacities to another levels of scalability and flexibility. But in the\nprocess, it has created a huge challenge of security, especially in terms of\nsafeguarding sensitive data. Classical security practices, including encryption\nat rest and during transit, fail to protect data in use and expose it to\nvarious possible breaches. In response to this problem , Confidential Computing\nhas been a tool ,seeking to secure data in processing by usage of\nhardware-based Trusted Execution Environments (TEEs). TEEs, including Intel's\nSoftware Guard Extensions (SGX) and ARM's TrustZone, offers protected contexts\nwithin the processor, where data is kept confidential ,intact and secure , even\nwith malicious software or compromised operating systems. In this research, we\nhave explored the architecture and security features of TEEs like Intel SGX and\nARM TrustZone, and their effectiveness in improving cloud data security. From a\nthorough literature survey ,we have analyzed the deployment strategies,\nperformance indicators, and practical uses of these TEEs for the same purpose.\nIn addition, we have discussed the issues regarding deployment, possible\nweaknesses, scalability issues, and integration issues. Our results focuses on\nthe central position of TEEs in strengthening and advancing cloud security\ninfrastructures, pointing towards their ability to create a secure foundation\nfor Confidential Computing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEEs\uff09\u5982Intel SGX\u548cARM TrustZone\u5728\u63d0\u5347\u4e91\u8ba1\u7b97\u6570\u636e\u5b89\u5168\u4e2d\u7684\u4f5c\u7528\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u5b89\u5168\u7279\u6027\u3001\u90e8\u7f72\u7b56\u7565\u53ca\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u4e91\u8ba1\u7b97\u7684\u53d1\u5c55\u5e26\u6765\u4e86\u6570\u636e\u5b89\u5168\u6311\u6218\uff0c\u4f20\u7edf\u52a0\u5bc6\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u62a4\u4f7f\u7528\u4e2d\u7684\u6570\u636e\u3002\u673a\u5bc6\u8ba1\u7b97\u901a\u8fc7\u786c\u4ef6TEEs\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4fdd\u62a4\u6570\u636e\u5728\u5904\u7406\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u6587\u732e\u8c03\u67e5\u5206\u6790Intel SGX\u548cARM TrustZone\u7684\u67b6\u6784\u3001\u5b89\u5168\u7279\u6027\u3001\u90e8\u7f72\u7b56\u7565\u3001\u6027\u80fd\u6307\u6807\u548c\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u3002", "result": "TEEs\u80fd\u6709\u6548\u4fdd\u62a4\u6570\u636e\u5728\u5904\u7406\u8fc7\u7a0b\u4e2d\u7684\u673a\u5bc6\u6027\u548c\u5b8c\u6574\u6027\uff0c\u5373\u4f7f\u9762\u5bf9\u6076\u610f\u8f6f\u4ef6\u6216\u53d7\u635f\u64cd\u4f5c\u7cfb\u7edf\u4e5f\u80fd\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\u3002", "conclusion": "TEEs\u5728\u52a0\u5f3a\u4e91\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u4e2d\u53d1\u6325\u6838\u5fc3\u4f5c\u7528\uff0c\u4e3a\u673a\u5bc6\u8ba1\u7b97\u63d0\u4f9b\u5b89\u5168\u57fa\u7840\uff0c\u4f46\u4ecd\u9762\u4e34\u90e8\u7f72\u590d\u6742\u6027\u3001\u6f5c\u5728\u6f0f\u6d1e\u3001\u53ef\u6269\u5c55\u6027\u548c\u96c6\u6210\u95ee\u9898\u7b49\u6311\u6218\u3002"}}
{"id": "2511.04316", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04316", "abs": "https://arxiv.org/abs/2511.04316", "authors": ["Tim Beyer", "Jonas Dornbusch", "Jakob Steimle", "Moritz Ladenburger", "Leo Schwinn", "Stephan G\u00fcnnemann"], "title": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research", "comment": null, "summary": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety.", "AI": {"tldr": "AdversariaLLM\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u8d8a\u72f1\u9c81\u68d2\u6027\u7814\u7a76\u7684\u5de5\u5177\u7bb1\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dLLM\u5b89\u5168\u7814\u7a76\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u788e\u7247\u5316\u3001\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6bd4\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u548c\u9c81\u68d2\u6027\u7814\u7a76\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u788e\u7247\u5316\u3001\u5b9e\u73b0\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u7814\u7a76\u96be\u4ee5\u590d\u73b0\u548c\u6bd4\u8f83\uff0c\u963b\u788d\u4e86\u6709\u610f\u4e49\u7684\u8fdb\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ee5\u53ef\u590d\u73b0\u6027\u3001\u6b63\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e3a\u6838\u5fc3\u7684\u5de5\u5177\u7bb1\uff0c\u5b9e\u73b0\u4e8612\u79cd\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\uff0c\u96c6\u6210\u4e867\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7Hugging Face\u63d0\u4f9b\u5bf9\u591a\u79cd\u5f00\u6e90LLM\u7684\u8bbf\u95ee\u3002", "result": "\u8be5\u6846\u67b6\u5305\u542b\u8ba1\u7b97\u8d44\u6e90\u8ddf\u8e2a\u3001\u786e\u5b9a\u6027\u7ed3\u679c\u548c\u5206\u5e03\u8bc4\u4f30\u6280\u672f\u7b49\u9ad8\u7ea7\u529f\u80fd\uff0c\u5e76\u4e0eJudgeZoo\u96c6\u6210\u8fdb\u884c\u8bc4\u5224\uff0c\u4e3aLLM\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u6bd4\u548c\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002", "conclusion": "AdversariaLLM\u65e8\u5728\u4e3aLLM\u5b89\u5168\u7814\u7a76\u5efa\u7acb\u4e00\u4e2a\u7a33\u5065\u7684\u57fa\u7840\uff0c\u4fc3\u8fdb\u900f\u660e\u3001\u53ef\u6bd4\u548c\u53ef\u590d\u73b0\u7684\u7814\u7a76\u5b9e\u8df5\u3002"}}
{"id": "2511.04328", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04328", "abs": "https://arxiv.org/abs/2511.04328", "authors": ["Jiahao Zhao", "Luxin Xu", "Minghuan Tan", "Lichao Zhang", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang"], "title": "RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation", "comment": "To appear in BIBM2025", "summary": "Numerous medical systems powered by Large Language Models (LLMs) have\nachieved remarkable progress in diverse healthcare tasks. However, research on\ntheir medication safety remains limited due to the lack of real world datasets,\nconstrained by privacy and accessibility issues. Moreover, evaluation of LLMs\nin realistic clinical consultation settings, particularly regarding medication\nsafety, is still underexplored. To address these gaps, we propose a framework\nthat simulates and evaluates clinical consultations to systematically assess\nthe medication safety capabilities of LLMs. Within this framework, we generate\ninquiry diagnosis dialogues with embedded medication risks and construct a\ndedicated medication safety database, RxRisk DB, containing 6,725\ncontraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.\nA two-stage filtering strategy ensures clinical realism and professional\nquality, resulting in the benchmark RxSafeBench with 2,443 high-quality\nconsultation scenarios. We evaluate leading open-source and proprietary LLMs\nusing structured multiple choice questions that test their ability to recommend\nsafe medications under simulated patient contexts. Results show that current\nLLMs struggle to integrate contraindication and interaction knowledge,\nespecially when risks are implied rather than explicit. Our findings highlight\nkey challenges in ensuring medication safety in LLM-based systems and provide\ninsights into improving reliability through better prompting and task-specific\ntuning. RxSafeBench offers the first comprehensive benchmark for evaluating\nmedication safety in LLMs, advancing safer and more trustworthy AI-driven\nclinical decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RxSafeBench\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u54a8\u8be2\u6765\u8bc4\u4f30LLMs\u7684\u7528\u836f\u5b89\u5168\u80fd\u529b\uff0c\u6784\u5efa\u4e86\u5305\u542b6725\u6761\u7981\u5fcc\u75c7\u300128781\u79cd\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u548c14906\u4e2a\u9002\u5e94\u75c7-\u836f\u7269\u5bf9\u7684RxRisk DB\u6570\u636e\u5e93\uff0c\u5e76\u57282443\u4e2a\u9ad8\u8d28\u91cf\u54a8\u8be2\u573a\u666f\u4e2d\u6d4b\u8bd5\u4e86\u4e3b\u6d41LLMs\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLMs\u7684\u533b\u7597\u7cfb\u7edf\u5728\u7528\u836f\u5b89\u5168\u65b9\u9762\u7684\u7814\u7a76\u6709\u9650\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u4e14\u5728\u73b0\u5b9e\u4e34\u5e8a\u54a8\u8be2\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u6a21\u62df\u4e34\u5e8a\u54a8\u8be2\u7684\u6846\u67b6\uff0c\u751f\u6210\u5305\u542b\u7528\u836f\u98ce\u9669\u7684\u8bca\u65ad\u5bf9\u8bdd\uff0c\u6784\u5efa\u4e13\u95e8\u7684\u7528\u836f\u5b89\u5168\u6570\u636e\u5e93RxRisk DB\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u6ee4\u7b56\u7565\u786e\u4fdd\u4e34\u5e8a\u771f\u5b9e\u6027\u548c\u4e13\u4e1a\u6027\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u591a\u9009\u9898\u8bc4\u4f30LLMs\u5728\u6a21\u62df\u60a3\u8005\u60c5\u5883\u4e0b\u63a8\u8350\u5b89\u5168\u836f\u7269\u7684\u80fd\u529b\u3002", "result": "\u5f53\u524dLLMs\u96be\u4ee5\u6574\u5408\u7981\u5fcc\u75c7\u548c\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u5728\u98ce\u9669\u9690\u542b\u800c\u975e\u660e\u786e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "RxSafeBench\u662f\u9996\u4e2a\u8bc4\u4f30LLMs\u7528\u836f\u5b89\u5168\u6027\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u4e3a\u6539\u8fdbAI\u9a71\u52a8\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.04341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04341", "abs": "https://arxiv.org/abs/2511.04341", "authors": ["Nick Oh", "Fernand Gobet"], "title": "Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning", "comment": "To-be presented at the Workshop on the Foundations of Reasoning in\n  Language Models at NeurIPS 2025 (non-archival)", "summary": "Test-time reasoning architectures such as those following the Generate-Verify\nparadigm -- where a model iteratively refines or verifies its own generated\noutputs -- prioritise generation and verification but exclude the monitoring\nprocesses that determine when and how reasoning should begin. This omission may\ncontribute to the prefix dominance trap, in which models commit early to\nsuboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy\nloss. We address this architectural gap by formalising Flavell's and Nelson and\nNarens' metacognitive theories into computational specifications, proposing the\nMonitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify\nparadigm by adding explicit monitoring that captures metacognitive experiences\n(from difficulty assessments to confidence judgements) before generation begins\nand refines future monitoring through verification feedback. Though we present\nno empirical validation, this work provides the first systematic computational\ntranslation of foundational metacognitive theories, offering a principled\nvocabulary for understanding reasoning system failures and suggesting specific\narchitectural interventions for future test-time reasoning designs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Monitor-Generate-Verify (MGV)\u6846\u67b6\uff0c\u901a\u8fc7\u5728Generate-Verify\u8303\u5f0f\u524d\u6dfb\u52a0\u663e\u5f0f\u76d1\u63a7\u673a\u5236\u6765\u89e3\u51b3\u6a21\u578b\u65e9\u671f\u9677\u5165\u6b21\u4f18\u63a8\u7406\u8def\u5f84\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406\u67b6\u6784\u7f3a\u4e4f\u76d1\u63a7\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6a21\u578b\u5bb9\u6613\u9677\u5165\u524d\u7f00\u4e3b\u5bfc\u9677\u9631\uff0c\u9020\u6210\u7ea620%\u7684\u51c6\u786e\u7387\u635f\u5931\u3002", "method": "\u5c06Flavell\u4ee5\u53caNelson\u548cNarens\u7684\u5143\u8ba4\u77e5\u7406\u8bba\u5f62\u5f0f\u5316\u4e3a\u8ba1\u7b97\u89c4\u8303\uff0c\u5728\u751f\u6210\u524d\u6dfb\u52a0\u663e\u5f0f\u76d1\u63a7\u6765\u6355\u83b7\u5143\u8ba4\u77e5\u4f53\u9a8c\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u53cd\u9988\u6765\u4f18\u5316\u672a\u6765\u76d1\u63a7\u3002", "result": "\u867d\u7136\u672a\u63d0\u4f9b\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4f46\u8fd9\u662f\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u5c06\u57fa\u7840\u5143\u8ba4\u77e5\u7406\u8bba\u8f6c\u5316\u4e3a\u8ba1\u7b97\u6846\u67b6\u3002", "conclusion": "MGV\u6846\u67b6\u4e3a\u7406\u89e3\u63a8\u7406\u7cfb\u7edf\u5931\u8d25\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8bcd\u6c47\uff0c\u5e76\u4e3a\u672a\u6765\u6d4b\u8bd5\u65f6\u63a8\u7406\u8bbe\u8ba1\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u67b6\u6784\u5e72\u9884\u5efa\u8bae\u3002"}}
{"id": "2511.04439", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04439", "abs": "https://arxiv.org/abs/2511.04439", "authors": ["Anisha Garg", "Ganesh Venkatesh"], "title": "The Peril of Preference: Why GRPO fails on Ordinal Rewards", "comment": null, "summary": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly\ndesirable for adapting LLMs to become experts at specific tasks. But this\nsimplicity also makes it ill-specified as we seek to enhance RL training with\nricher, non-binary feedback. When using ordinal rewards to give partial credit,\nGRPO's simplicity starts to hurt, as its group-average baseline often assigns a\npositive advantage to failed trajectories and reinforces incorrect behavior.\n  We introduce Correctness Relative Policy Optimization (CoRPO), a new\nformulation that solves this flaw. CoRPO uses an adaptive baseline that\nenforces a minimum quality threshold, ensuring failed solutions are never\npositively reinforced. Once the policy consistently meets this threshold, the\nbaseline automatically transitions to a relative preference mode, pushing the\nmodel to find optimal solutions rather than just \"acceptable\" ones. We\nempirically validate CoRPO on a code verification task, where it demonstrates\nmore stable convergence and better out-of-domain generalization.\n  This work represents a critical step in our broader research program to\nenable LLMs to learn genuinely new capabilities through reinforcement learning.\nWe achieve this by enabling LLMs to learn from rich, multi-dimensional feedback\n- progressing from binary to ordinal rewards in this work, and onward to\ndenser, per-step supervision.", "AI": {"tldr": "CoRPO\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86GRPO\u5728\u4f7f\u7528\u5e8f\u6570\u5956\u52b1\u65f6\u5bf9\u5931\u8d25\u8f68\u8ff9\u7ed9\u4e88\u6b63\u4f18\u52bf\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u57fa\u7ebf\u786e\u4fdd\u5931\u8d25\u89e3\u51b3\u65b9\u6848\u4e0d\u4f1a\u88ab\u6b63\u5411\u5f3a\u5316\u3002", "motivation": "GRPO\u7684\u7b80\u5355\u6027\u4f7f\u5176\u5728\u9002\u5e94LLMs\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u65f6\u5f88\u53d7\u6b22\u8fce\uff0c\u4f46\u8fd9\u79cd\u7b80\u5355\u6027\u4e5f\u4f7f\u5176\u5728\u5904\u7406\u66f4\u4e30\u5bcc\u7684\u975e\u4e8c\u5143\u53cd\u9988\u65f6\u5b58\u5728\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5e8f\u6570\u5956\u52b1\u7ed9\u4e88\u90e8\u5206\u4fe1\u7528\u65f6\uff0cGRPO\u7684\u7ec4\u5e73\u5747\u57fa\u7ebf\u5f80\u5f80\u5bf9\u5931\u8d25\u8f68\u8ff9\u5206\u914d\u6b63\u4f18\u52bf\uff0c\u4ece\u800c\u5f3a\u5316\u9519\u8bef\u884c\u4e3a\u3002", "method": "CoRPO\u4f7f\u7528\u81ea\u9002\u5e94\u57fa\u7ebf\u5f3a\u5236\u6267\u884c\u6700\u4f4e\u8d28\u91cf\u9608\u503c\uff0c\u786e\u4fdd\u5931\u8d25\u7684\u89e3\u51b3\u65b9\u6848\u6c38\u8fdc\u4e0d\u4f1a\u88ab\u6b63\u5411\u5f3a\u5316\u3002\u4e00\u65e6\u7b56\u7565\u6301\u7eed\u6ee1\u8db3\u6b64\u9608\u503c\uff0c\u57fa\u7ebf\u4f1a\u81ea\u52a8\u8fc7\u6e21\u5230\u76f8\u5bf9\u504f\u597d\u6a21\u5f0f\uff0c\u63a8\u52a8\u6a21\u578b\u5bfb\u627e\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u800c\u4e0d\u4ec5\u4ec5\u662f\"\u53ef\u63a5\u53d7\"\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u4ee3\u7801\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0cCoRPO\u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u6027\u548c\u66f4\u597d\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u5728\u66f4\u5e7f\u6cdb\u7814\u7a76\u8ba1\u5212\u4e2d\u7684\u5173\u952e\u4e00\u6b65\uff0c\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f7fLLMs\u80fd\u591f\u5b66\u4e60\u771f\u6b63\u7684\u65b0\u80fd\u529b\uff0c\u901a\u8fc7\u4ece\u4e30\u5bcc\u7684\u591a\u7ef4\u53cd\u9988\u4e2d\u5b66\u4e60\u2014\u2014\u4ece\u4e8c\u5143\u5956\u52b1\u8fdb\u5c55\u5230\u5e8f\u6570\u5956\u52b1\uff0c\u5e76\u8fdb\u4e00\u6b65\u53d1\u5c55\u5230\u66f4\u5bc6\u96c6\u7684\u6bcf\u6b65\u76d1\u7763\u3002"}}
{"id": "2511.04500", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04500", "abs": "https://arxiv.org/abs/2511.04500", "authors": ["Andrea Cera Palatsi", "Samuel Martin-Gutierrez", "Ana S. Cardenal", "Max Pellert"], "title": "Large language models replicate and predict human cooperation across experiments in game theory", "comment": null, "summary": "Large language models (LLMs) are increasingly used both to make decisions in\ndomains such as health, education and law, and to simulate human behavior. Yet\nhow closely LLMs mirror actual human decision-making remains poorly understood.\nThis gap is critical: misalignment could produce harmful outcomes in practical\napplications, while failure to replicate human behavior renders LLMs\nineffective for social simulations. Here, we address this gap by developing a\ndigital twin of game-theoretic experiments and introducing a systematic\nprompting and probing framework for machine-behavioral evaluation. Testing\nthree open-source models (Llama, Mistral and Qwen), we find that Llama\nreproduces human cooperation patterns with high fidelity, capturing human\ndeviations from rational choice theory, while Qwen aligns closely with Nash\nequilibrium predictions. Notably, we achieved population-level behavioral\nreplication without persona-based prompting, simplifying the simulation\nprocess. Extending beyond the original human-tested games, we generate and\npreregister testable hypotheses for novel game configurations outside the\noriginal parameter grid. Our findings demonstrate that appropriately calibrated\nLLMs can replicate aggregate human behavioral patterns and enable systematic\nexploration of unexplored experimental spaces, offering a complementary\napproach to traditional research in the social and behavioral sciences that\ngenerates new empirical predictions about human social decision-making.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u535a\u5f08\u8bba\u5b9e\u9a8c\u7684\u6570\u5b57\u5b6a\u751f\u548c\u7cfb\u7edf\u63d0\u793a\u6846\u67b6\uff0c\u6d4b\u8bd5\u53d1\u73b0Llama\u80fd\u9ad8\u4fdd\u771f\u590d\u5236\u4eba\u7c7b\u5408\u4f5c\u6a21\u5f0f\uff0c\u800cQwen\u66f4\u63a5\u8fd1\u7eb3\u4ec0\u5747\u8861\u9884\u6d4b\uff0c\u8868\u660e\u9002\u5f53\u6821\u51c6\u7684LLM\u53ef\u4ee5\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u5e76\u63a2\u7d22\u65b0\u7684\u5b9e\u9a8c\u7a7a\u95f4\u3002", "motivation": "\u7406\u89e3LLM\u4e0e\u4eba\u7c7b\u51b3\u7b56\u7684\u76f8\u4f3c\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u4e0d\u5339\u914d\u53ef\u80fd\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u5bb3\u7ed3\u679c\uff0c\u800c\u65e0\u6cd5\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u4f1a\u4f7fLLM\u5728\u793e\u4f1a\u6a21\u62df\u4e2d\u65e0\u6548\u3002", "method": "\u5f00\u53d1\u535a\u5f08\u8bba\u5b9e\u9a8c\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u5f15\u5165\u7cfb\u7edf\u63d0\u793a\u548c\u63a2\u6d4b\u6846\u67b6\u8fdb\u884c\u673a\u5668\u884c\u4e3a\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\uff08Llama\u3001Mistral\u548cQwen\uff09\u3002", "result": "Llama\u80fd\u9ad8\u4fdd\u771f\u590d\u5236\u4eba\u7c7b\u5408\u4f5c\u6a21\u5f0f\uff0c\u6355\u6349\u4eba\u7c7b\u504f\u79bb\u7406\u6027\u9009\u62e9\u7406\u8bba\u7684\u884c\u4e3a\uff1bQwen\u4e0e\u7eb3\u4ec0\u5747\u8861\u9884\u6d4b\u9ad8\u5ea6\u4e00\u81f4\uff1b\u65e0\u9700\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\u5373\u53ef\u5b9e\u73b0\u7fa4\u4f53\u7ea7\u884c\u4e3a\u590d\u5236\u3002", "conclusion": "\u9002\u5f53\u6821\u51c6\u7684LLM\u53ef\u4ee5\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u5e76\u7cfb\u7edf\u63a2\u7d22\u672a\u5f00\u53d1\u7684\u5b9e\u9a8c\u7a7a\u95f4\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u8865\u5145\u65b9\u6cd5\uff0c\u751f\u6210\u5173\u4e8e\u4eba\u7c7b\u793e\u4ea4\u51b3\u7b56\u7684\u65b0\u7ecf\u9a8c\u9884\u6d4b\u3002"}}
{"id": "2511.04584", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04584", "abs": "https://arxiv.org/abs/2511.04584", "authors": ["Daniel Gomm", "Cornelius Wolff", "Madelon Hulsebos"], "title": "Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis", "comment": "Accepted to the AI for Tabular Data workshop at EurIPS 2025", "summary": "Natural language interfaces to tabular data must handle ambiguities inherent\nto queries. Instead of treating ambiguity as a deficiency, we reframe it as a\nfeature of cooperative interaction, where the responsibility of query\nspecification is shared among the user and the system. We develop a principled\nframework distinguishing cooperative queries, i.e., queries that yield a\nresolvable interpretation, from uncooperative queries that cannot be resolved.\nApplying the framework to evaluations for tabular question answering and\nanalysis, we analyze the queries in 15 popular datasets, and observe an\nuncontrolled mixing of query types neither adequate for evaluating a system's\nexecution accuracy nor for evaluating interpretation capabilities. Our\nframework and analysis of queries shifts the perspective from fixing ambiguity\nto embracing cooperation in resolving queries. This reflection enables more\ninformed design and evaluation for natural language interfaces for tabular\ndata, for which we outline implications and directions for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u67e5\u8be2\u6b67\u4e49\u91cd\u65b0\u5b9a\u4e49\u4e3a\u534f\u4f5c\u4ea4\u4e92\u7279\u5f81\u7684\u7406\u8bba\u6846\u67b6\uff0c\u533a\u5206\u4e86\u53ef\u89e3\u6790\u7684\u5408\u4f5c\u67e5\u8be2\u4e0e\u4e0d\u53ef\u89e3\u6790\u7684\u975e\u5408\u4f5c\u67e5\u8be2\uff0c\u5e76\u5206\u6790\u4e8615\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u4e2d\u7684\u67e5\u8be2\u7c7b\u578b\u6df7\u5408\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e2d\u7684\u6b67\u4e49\u89c6\u4e3a\u7f3a\u9677\uff0c\u672c\u6587\u65e8\u5728\u5c06\u5176\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7528\u6237\u4e0e\u7cfb\u7edf\u534f\u4f5c\u4ea4\u4e92\u7684\u7279\u5f81\uff0c\u5171\u4eab\u67e5\u8be2\u89c4\u8303\u7684\u8d23\u4efb\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\u6765\u533a\u5206\u5408\u4f5c\u67e5\u8be2\u548c\u975e\u5408\u4f5c\u67e5\u8be2\uff0c\u5e76\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e15\u4e2a\u6d41\u884c\u7684\u8868\u683c\u95ee\u7b54\u548c\u5206\u6790\u6570\u636e\u96c6\u7684\u67e5\u8be2\u5206\u6790\u3002", "result": "\u5206\u6790\u53d1\u73b0\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7684\u67e5\u8be2\u7c7b\u578b\u6df7\u5408\u4e0d\u5f53\uff0c\u65e2\u4e0d\u9002\u5408\u8bc4\u4f30\u7cfb\u7edf\u6267\u884c\u51c6\u786e\u6027\uff0c\u4e5f\u4e0d\u9002\u5408\u8bc4\u4f30\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u89c6\u89d2\u4ece\u89e3\u51b3\u6b67\u4e49\u8f6c\u5411\u5728\u67e5\u8be2\u89e3\u6790\u4e2d\u62e5\u62b1\u534f\u4f5c\uff0c\u4e3a\u8868\u683c\u6570\u636e\u81ea\u7136\u8bed\u8a00\u754c\u9762\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u660e\u667a\u7684\u65b9\u5411\u3002"}}
{"id": "2511.04588", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.04588", "abs": "https://arxiv.org/abs/2511.04588", "authors": ["Soham De", "Lodewijk Gelauff", "Ashish Goel", "Smitha Milli", "Ariel Procaccia", "Alice Siu"], "title": "Question the Questions: Auditing Representation in Online Deliberative Processes", "comment": null, "summary": "A central feature of many deliberative processes, such as citizens'\nassemblies and deliberative polls, is the opportunity for participants to\nengage directly with experts. While participants are typically invited to\npropose questions for expert panels, only a limited number can be selected due\nto time constraints. This raises the challenge of how to choose a small set of\nquestions that best represent the interests of all participants. We introduce\nan auditing framework for measuring the level of representation provided by a\nslate of questions, based on the social choice concept known as justified\nrepresentation (JR). We present the first algorithms for auditing JR in the\ngeneral utility setting, with our most efficient algorithm achieving a runtime\nof $O(mn\\log n)$, where $n$ is the number of participants and $m$ is the number\nof proposed questions. We apply our auditing methods to historical\ndeliberations, comparing the representativeness of (a) the actual questions\nposed to the expert panel (chosen by a moderator), (b) participants' questions\nchosen via integer linear programming, (c) summary questions generated by large\nlanguage models (LLMs). Our results highlight both the promise and current\nlimitations of LLMs in supporting deliberative processes. By integrating our\nmethods into an online deliberation platform that has been used for over\nhundreds of deliberations across more than 50 countries, we make it easy for\npractitioners to audit and improve representation in future deliberations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5408\u7406\u4ee3\u8868\uff08JR\uff09\u6982\u5ff5\u7684\u5ba1\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u4e13\u5bb6\u95ee\u7b54\u73af\u8282\u4e2d\u95ee\u9898\u9009\u62e9\u7684\u4ee3\u8868\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u5ba1\u8ba1\u7b97\u6cd5\u3002\u901a\u8fc7\u6bd4\u8f83\u5b9e\u9645\u9009\u62e9\u7684\u95ee\u9898\u3001\u6574\u6570\u7ebf\u6027\u89c4\u5212\u9009\u62e9\u7684\u95ee\u9898\u548cLLM\u751f\u6210\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4ee3\u8868\u6027\u8868\u73b0\u3002", "motivation": "\u5728\u516c\u6c11\u5927\u4f1a\u7b49\u5ba1\u8bae\u8fc7\u7a0b\u4e2d\uff0c\u53c2\u4e0e\u8005\u5411\u4e13\u5bb6\u63d0\u95ee\u7684\u673a\u4f1a\u6709\u9650\uff0c\u5982\u4f55\u9009\u62e9\u5c11\u91cf\u95ee\u9898\u6765\u6700\u597d\u5730\u4ee3\u8868\u6240\u6709\u53c2\u4e0e\u8005\u7684\u5229\u76ca\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5408\u7406\u4ee3\u8868\uff08JR\uff09\u6982\u5ff5\u7684\u5ba1\u8ba1\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u5728\u901a\u7528\u6548\u7528\u8bbe\u7f6e\u4e0b\u5ba1\u8ba1JR\u7684\u7b97\u6cd5\uff0c\u6700\u6709\u6548\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(mn log n)\u3002\u5e94\u7528\u5ba1\u8ba1\u65b9\u6cd5\u6bd4\u8f83\u4e86\u5b9e\u9645\u9009\u62e9\u7684\u95ee\u9898\u3001\u6574\u6570\u7ebf\u6027\u89c4\u5212\u9009\u62e9\u7684\u95ee\u9898\u548cLLM\u751f\u6210\u7684\u95ee\u9898\u7684\u4ee3\u8868\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86LLM\u5728\u652f\u6301\u5ba1\u8bae\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5c06\u65b9\u6cd5\u6574\u5408\u5230\u5df2\u572850\u591a\u4e2a\u56fd\u5bb6\u4f7f\u7528\u7684\u5728\u7ebf\u5ba1\u8bae\u5e73\u53f0\u4e2d\uff0c\u4f7f\u5b9e\u8df5\u8005\u80fd\u591f\u8f7b\u677e\u5ba1\u8ba1\u548c\u6539\u8fdb\u672a\u6765\u5ba1\u8bae\u4e2d\u7684\u4ee3\u8868\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5ba1\u8bae\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ee3\u8868\u6027\u5ba1\u8ba1\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5ba1\u8bae\u8fc7\u7a0b\u7684\u6c11\u4e3b\u6027\u548c\u5305\u5bb9\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86LLM\u5728\u652f\u6301\u5ba1\u8bae\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\u524d\u666f\u548c\u9650\u5236\u3002"}}
{"id": "2511.04646", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04646", "abs": "https://arxiv.org/abs/2511.04646", "authors": ["Narjes Nourzad", "Hanqing Yang", "Shiyu Chen", "Carlee Joe-Wong"], "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration", "comment": null, "summary": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies.", "AI": {"tldr": "DR. WELL\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u89c4\u5212\u3002\u5b83\u901a\u8fc7\u4e24\u9636\u6bb5\u534f\u5546\u534f\u8bae\u5b9e\u73b0\u5408\u4f5c\uff1a\u667a\u80fd\u4f53\u9996\u5148\u63d0\u51fa\u5019\u9009\u89d2\u8272\u5e76\u63a8\u7406\uff0c\u7136\u540e\u5728\u5171\u8bc6\u548c\u73af\u5883\u7ea6\u675f\u4e0b\u627f\u8bfa\u8054\u5408\u5206\u914d\u3002\u627f\u8bfa\u540e\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u72ec\u7acb\u751f\u6210\u5e76\u6267\u884c\u5176\u89d2\u8272\u7684\u7b26\u53f7\u5316\u8ba1\u5212\uff0c\u65e0\u9700\u900f\u9732\u8be6\u7ec6\u8f68\u8ff9\u3002", "motivation": "\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u89c4\u5212\u9700\u8981\u667a\u80fd\u4f53\u5728\u90e8\u5206\u4fe1\u606f\u548c\u6709\u9650\u901a\u4fe1\u4e0b\u505a\u51fa\u8054\u5408\u51b3\u7b56\u3002\u8f68\u8ff9\u7ea7\u534f\u8c03\u7ecf\u5e38\u5931\u8d25\uff0c\u56e0\u4e3a\u65f6\u95f4\u6216\u8fd0\u52a8\u7684\u5c0f\u504f\u5dee\u4f1a\u7ea7\u8054\u6210\u51b2\u7a81\u3002\u7b26\u53f7\u5316\u89c4\u5212\u901a\u8fc7\u63d0\u9ad8\u62bd\u8c61\u7ea7\u522b\u548c\u63d0\u4f9b\u6700\u5c0f\u52a8\u4f5c\u8bcd\u6c47\u6765\u7f13\u89e3\u8fd9\u4e00\u6311\u6218\uff0c\u5b9e\u73b0\u540c\u6b65\u548c\u96c6\u4f53\u8fdb\u5c55\u3002", "method": "DR. WELL\u91c7\u7528\u4e24\u9636\u6bb5\u534f\u5546\u534f\u8bae\uff1a1\uff09\u667a\u80fd\u4f53\u63d0\u51fa\u5019\u9009\u89d2\u8272\u5e76\u8fdb\u884c\u63a8\u7406\uff1b2\uff09\u5728\u5171\u8bc6\u548c\u73af\u5883\u7ea6\u675f\u4e0b\u627f\u8bfa\u8054\u5408\u5206\u914d\u3002\u968f\u540e\u6bcf\u4e2a\u667a\u80fd\u4f53\u72ec\u7acb\u751f\u6210\u548c\u6267\u884c\u7b26\u53f7\u5316\u8ba1\u5212\uff0c\u901a\u8fc7\u5171\u4eab\u4e16\u754c\u6a21\u578b\u5c06\u8ba1\u5212\u4e0e\u6267\u884c\u7ed3\u679c\u5173\u8054\u3002", "result": "\u5728\u534f\u4f5c\u63a8\u5757\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u8de8\u56de\u5408\u9002\u5e94\uff0c\u52a8\u6001\u4e16\u754c\u6a21\u578b\u6355\u83b7\u53ef\u91cd\u7528\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6548\u7387\u3002\u901a\u8fc7\u534f\u5546\u548c\u81ea\u6211\u4f18\u5316\uff0c\u4ee5\u65f6\u95f4\u5f00\u9500\u4e3a\u4ee3\u4ef7\u5b9e\u73b0\u4e86\u4e0d\u65ad\u6f14\u5316\u7684\u66f4\u9ad8\u6548\u534f\u4f5c\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u63a8\u7406\u7b26\u53f7\u5316\u8ba1\u5212\u800c\u975e\u539f\u59cb\u8f68\u8ff9\uff0cDR. WELL\u907f\u514d\u4e86\u8106\u5f31\u7684\u6b65\u9aa4\u7ea7\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u53ef\u91cd\u7528\u3001\u53ef\u540c\u6b65\u548c\u53ef\u89e3\u91ca\u7684\u66f4\u9ad8\u7ea7\u64cd\u4f5c\u3002\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u89c4\u5212\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2511.04662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04662", "abs": "https://arxiv.org/abs/2511.04662", "authors": ["Yu Feng", "Nathaniel Weir", "Kaj Bostrom", "Sam Bayless", "Darion Cassel", "Sapana Chaudhary", "Benjamin Kiesl-Reiter", "Huzefa Rangwala"], "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks", "comment": null, "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.", "AI": {"tldr": "VeriCoT\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eceCoT\u63a8\u7406\u4e2d\u63d0\u53d6\u548c\u9a8c\u8bc1\u5f62\u5f0f\u903b\u8f91\u8bba\u8bc1\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u6b65\u9aa4\u5f62\u5f0f\u5316\u4e3a\u4e00\u9636\u903b\u8f91\u5e76\u4f7f\u7528\u81ea\u52a8\u6c42\u89e3\u5668\u9a8c\u8bc1\u903b\u8f91\u6709\u6548\u6027\u3002", "motivation": "LLMs\u867d\u7136\u80fd\u901a\u8fc7CoT\u8fdb\u884c\u591a\u6b65\u63a8\u7406\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u9a8c\u8bc1\u81ea\u8eab\u903b\u8f91\uff0c\u5373\u4f7f\u5728\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u65f6\u5e95\u5c42\u63a8\u7406\u4e5f\u53ef\u80fd\u5b58\u5728\u7f3a\u9677\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u524a\u5f31\u4e86\u53ef\u4fe1\u5ea6\u3002", "method": "\u5c06CoT\u63a8\u7406\u6b65\u9aa4\u5f62\u5f0f\u5316\u4e3a\u4e00\u9636\u903b\u8f91\uff0c\u8bc6\u522b\u57fa\u4e8e\u6e90\u4e0a\u4e0b\u6587\u3001\u5e38\u8bc6\u77e5\u8bc6\u6216\u5148\u524d\u63a8\u7406\u6b65\u9aa4\u7684\u524d\u63d0\uff0c\u4f7f\u7528\u7b26\u53f7\u8868\u793a\u548c\u81ea\u52a8\u6c42\u89e3\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728ProofWriter\u3001LegalBench\u548cBioASQ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVeriCoT\u80fd\u6709\u6548\u8bc6\u522b\u6709\u7f3a\u9677\u7684\u63a8\u7406\uff0c\u5e76\u4f5c\u4e3a\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u7684\u5f3a\u9884\u6d4b\u6307\u6807\u3002", "conclusion": "VeriCoT\u7684\u9a8c\u8bc1\u4fe1\u53f7\u53ef\u7528\u4e8e\u63a8\u7406\u65f6\u81ea\u6211\u53cd\u601d\u3001\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u5fae\u8c03\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u63a8\u7406\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\u3002"}}
