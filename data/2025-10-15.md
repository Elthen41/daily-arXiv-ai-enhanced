<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 12]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 34]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing](https://arxiv.org/abs/2510.11823)
*Caelin Kaplan,Alexander Warnecke,Neil Archibald*

Main category: cs.CR

TL;DR: BlackIce是一个开源容器化工具包，用于对大型语言模型和传统机器学习模型进行红队测试，通过统一的Docker镜像集成14个精心挑选的负责任AI和安全测试工具，简化AI模型安全评估流程。


<details>
  <summary>Details</summary>
Motivation: AI模型在现实系统中的集成日益增多，引发了对其安全性的担忧。现有的AI红队测试工具选择困难，依赖管理复杂，需要降低门槛并建立标准化环境来简化AI模型评估。

Method: 受Kali Linux启发，开发了BlackIce——一个开源的容器化工具包，提供可复现的版本锁定Docker镜像，集成14个开源工具用于负责任AI和安全测试，通过统一命令行界面访问，支持本地或云端容器部署。

Result: BlackIce创建了一个标准化的红队测试环境，使启动红队评估变得简单直接，只需启动容器即可。模块化架构支持社区驱动的扩展，便于适应新威胁。

Conclusion: BlackIce通过容器化方法有效降低了AI红队测试的门槛，为组织提供了一种简化的方式来执行全面的AI模型安全评估，同时支持工具集的灵活扩展。

Abstract: AI models are being increasingly integrated into real-world systems, raising
significant concerns about their safety and security. Consequently, AI red
teaming has become essential for organizations to proactively identify and
address vulnerabilities before they can be exploited by adversaries. While
numerous AI red teaming tools currently exist, practitioners face challenges in
selecting the most appropriate tools from a rapidly expanding landscape, as
well as managing complex and frequently conflicting software dependencies
across isolated projects. Given these challenges and the relatively small
number of organizations with dedicated AI red teams, there is a strong need to
lower barriers to entry and establish a standardized environment that
simplifies the setup and execution of comprehensive AI model assessments.
  Inspired by Kali Linux's role in traditional penetration testing, we
introduce BlackIce, an open-source containerized toolkit designed for red
teaming Large Language Models (LLMs) and classical machine learning (ML)
models. BlackIce provides a reproducible, version-pinned Docker image that
bundles 14 carefully selected open-source tools for Responsible AI and Security
testing, all accessible via a unified command-line interface. With this setup,
initiating red team assessments is as straightforward as launching a container,
either locally or using a cloud platform. Additionally, the image's modular
architecture facilitates community-driven extensions, allowing users to easily
adapt or expand the toolkit as new threats emerge. In this paper, we describe
the architecture of the container image, the process used for selecting tools,
and the types of evaluations they support.

</details>


### [2] [Countermind: A Multi-Layered Security Architecture for Large Language Models](https://arxiv.org/abs/2510.11837)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: 本文提出Countermind多层安全架构，通过语义边界逻辑、参数空间限制、自调节核心等机制，从被动防御转向主动的推理前和推理中安全执行，以应对LLM应用中的提示注入和越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 传统基于后处理输出过滤的防御方法脆弱且无法解决根本问题——模型无法区分可信指令与不可信数据。需要从被动防御转向主动的推理前和推理中安全执行。

Method: 提出多层安全架构：1) 语义边界逻辑和时间耦合文本加密器减少明文提示注入攻击面；2) 参数空间限制机制动态控制LLM对内部语义簇的访问；3) 自调节核心使用OODA循环和学习安全模块；4) 多模态输入沙箱和上下文防御机制。

Result: 论文概述了评估计划，旨在量化该架构在降低表单优先攻击成功率方面的有效性，并测量其潜在的延迟开销。

Conclusion: Countermind架构通过结构化的输入验证和转换、内部治理机制以及自适应防御，为LLM应用提供了更强大的安全保护，从根源上解决提示注入和越狱等安全问题。

Abstract: The security of Large Language Model (LLM) applications is fundamentally
challenged by "form-first" attacks like prompt injection and jailbreaking,
where malicious instructions are embedded within user inputs. Conventional
defenses, which rely on post hoc output filtering, are often brittle and fail
to address the root cause: the model's inability to distinguish trusted
instructions from untrusted data. This paper proposes Countermind, a
multi-layered security architecture intended to shift defenses from a reactive,
post hoc posture to a proactive, pre-inference, and intra-inference enforcement
model. The architecture proposes a fortified perimeter designed to structurally
validate and transform all inputs, and an internal governance mechanism
intended to constrain the model's semantic processing pathways before an output
is generated. The primary contributions of this work are conceptual designs
for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text
Crypter intended to reduce the plaintext prompt injection attack surface,
provided all ingestion paths are enforced. (2) A Parameter-Space Restriction
(PSR) mechanism, leveraging principles from representation engineering, to
dynamically control the LLM's access to internal semantic clusters, with the
goal of mitigating semantic drift and dangerous emergent behaviors. (3) A
Secure, Self-Regulating Core that uses an OODA loop and a learning security
module to adapt its defenses based on an immutable audit log. (4) A Multimodal
Input Sandbox and Context-Defense mechanisms to address threats from
non-textual data and long-term semantic poisoning. This paper outlines an
evaluation plan designed to quantify the proposed architecture's effectiveness
in reducing the Attack Success Rate (ASR) for form-first attacks and to measure
its potential latency overhead.

</details>


### [3] [Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing](https://arxiv.org/abs/2510.11915)
*Deeksha Hareesha Kulal,Chidozie Princewill Arannonu,Afsah Anwar,Nidhi Rastogi,Quamar Niyaz*

Main category: cs.CR

TL;DR: 针对LLM生成的高质量钓鱼邮件难以检测的问题，提出了一种增强文本预处理流程的钓鱼邮件检测系统，通过拼写纠正和分词技术对抗对抗性修改，在公开数据集上达到94.26%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)能够生成语法正确、上下文相关且语言自然的钓鱼邮件，这使得传统检测机制难以识别，需要开发更强大的检测系统来应对这一挑战。

Method: 提出增强文本预处理流程，包括拼写纠正和分词技术，结合自然语言处理特征提取和机器学习算法，使用Python TextAttack框架评估模型对抗性攻击的鲁棒性。

Result: 在模型部署设置中达到94.26%的检测准确率和84.39%的F1分数，对四种对抗性攻击方法和LLM生成的钓鱼邮件表现出良好的鲁棒性。

Conclusion: 所提出的模型对不断发展的AI驱动的钓鱼威胁具有显著的抵御能力，为应对LLM生成的钓鱼邮件提供了有效的解决方案。

Abstract: Phishing remains a critical cybersecurity threat, especially with the advent
of large language models (LLMs) capable of generating highly convincing
malicious content. Unlike earlier phishing attempts which are identifiable by
grammatical errors, misspellings, incorrect phrasing, and inconsistent
formatting, LLM generated emails are grammatically sound, contextually
relevant, and linguistically natural. These advancements make phishing emails
increasingly difficult to distinguish from legitimate ones, challenging
traditional detection mechanisms. Conventional phishing detection systems often
fail when faced with emails crafted by LLMs or manipulated using adversarial
perturbation techniques. To address this challenge, we propose a robust
phishing email detection system featuring an enhanced text preprocessing
pipeline. This pipeline includes spelling correction and word splitting to
counteract adversarial modifications and improve detection accuracy. Our
approach integrates widely adopted natural language processing (NLP) feature
extraction techniques and machine learning algorithms. We evaluate our models
on publicly available datasets comprising both phishing and legitimate emails,
achieving a detection accuracy of 94.26% and F1-score of 84.39% in model
deployment setting. To assess robustness, we further evaluate our models using
adversarial phishing samples generated by four attack methods in Python
TextAttack framework. Additionally, we evaluate models' performance against
phishing emails generated by LLMs including ChatGPT and Llama. Results
highlight the resilience of models against evolving AI-powered phishing
threats.

</details>


### [4] [Adding All Flavors: A Hybrid Random Number Generator for dApps and Web3](https://arxiv.org/abs/2510.12062)
*Ranjith Chodavarapu,Rabimba Karanjai,Xinxin Fan,Weidong Shi,Lei Xu*

Main category: cs.CR

TL;DR: 提出了一种基于TEE物联网设备的混合随机数生成方案，通过聚合多个随机源来为dApp提供可配置的随机数服务，只需一个诚实源即可保证随机数无偏性。


<details>
  <summary>Details</summary>
Motivation: 现有随机数生成机制存在局限性：链上方法易受攻击者影响输入，链下方法安全假设强且复杂度高。需要一种平衡各种因素的随机数生成框架。

Method: 利用配备可信执行环境(TEE)的物联网设备作为随机源，结合密码学工具聚合多个源生成最终随机数，用户可配置系统容忍恶意参与者。

Result: 新方法只需一个诚实随机源即可保证最终随机数的无偏性，提供了具体实现方案以降低链上计算复杂度和成本。

Conclusion: 该混合方案有效解决了现有随机数生成机制的局限性，通过评估证明其在计算和gas成本方面的改进效果。

Abstract: Random numbers play a vital role in many decentralized applications (dApps),
such as gaming and decentralized finance (DeFi) applications.
  Existing random number provision mechanisms can be roughly divided into two
categories, on-chain, and off-chain.
  On-chain approaches usually rely on the blockchain as the major input and all
computations are done by blockchain nodes.
  The major risk for this type of method is that the input itself is
susceptible to the adversary's influence.
  Off-chain approaches, as the name suggested, complete the generation without
the involvement of blockchain nodes and share the result directly with a dApp.
  These mechanisms usually have a strong security assumption and high
complexity.
  To mitigate these limitations and provide a framework that allows a dApp to
balance different factors involved in random number generation, we propose a
hybrid random number generation solution that leverages IoT devices equipped
with trusted execution environment (TEE) as the randomness sources, and then
utilizes a set of cryptographic tools to aggregate the multiple sources and
obtain the final random number that can be consumed by the dApp.
  The new approach only needs one honest random source to guarantee the
unbiasedness of the final random number and a user can configure the system to
tolerate malicious participants who can refuse to respond to avoid unfavored
results.
  We also provide a concrete construction that can further reduce the on-chain
computation complexity to lower the cost of the solution in practice.
  We evaluate the computation and gas costs to demonstrate the effectiveness of
the improvement.

</details>


### [5] [Elevating Medical Image Security: A Cryptographic Framework Integrating Hyperchaotic Map and GRU](https://arxiv.org/abs/2510.12084)
*Weixuan Li,Guang Yu,Quanjun Li,Junhua Zhou,Jiajun Chen,Yihang Dong,Mengqian Wang,Zimeng Li,Changwei Gong,Lin Tang,Xuhang Chen*

Main category: cs.CR

TL;DR: 本文提出了Kun-IE图像加密框架，通过2D-SCPHM超混沌映射和Kun-SCAN置换策略解决现有混沌加密方法的漏洞，提供更安全的图像通信解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于混沌的图像加密方法存在排列和扩散不足、伪随机特性欠佳等漏洞，需要开发更安全可靠的加密方案。

Method: 开发了2D Sin-Cos Pi超混沌映射(2D-SCPHM)提供更宽的混沌范围和更好的伪随机序列生成，并引入Kun-SCAN置换策略显著降低像素相关性。

Result: 实验结果表明Kun-IE能够抵抗各种密码分析攻击，支持任意尺寸图像加密，具有强健的安全性。

Conclusion: Kun-IE是一个灵活且安全的图像加密框架，适用于安全的图像通信应用。

Abstract: Chaotic systems play a key role in modern image encryption due to their
sensitivity to initial conditions, ergodicity, and complex dynamics. However,
many existing chaos-based encryption methods suffer from vulnerabilities, such
as inadequate permutation and diffusion, and suboptimal pseudorandom
properties. This paper presents Kun-IE, a novel encryption framework designed
to address these issues. The framework features two key contributions: the
development of the 2D Sin-Cos Pi Hyperchaotic Map (2D-SCPHM), which offers a
broader chaotic range and superior pseudorandom sequence generation, and the
introduction of Kun-SCAN, a novel permutation strategy that significantly
reduces pixel correlations, enhancing resistance to statistical attacks. Kun-IE
is flexible and supports encryption for images of any size. Experimental
results and security analyses demonstrate its robustness against various
cryptanalytic attacks, making it a strong solution for secure image
communication. The code is available at this
\href{https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU}{link}.

</details>


### [6] [Locket: Robust Feature-Locking Technique for Language Models](https://arxiv.org/abs/2510.12117)
*Lipeng He,Vasisht Duddu,N. Asokan*

Main category: cs.CR

TL;DR: Locket是一种用于大语言模型的特征锁定技术，通过适配器合并方法实现付费解锁方案，在拒绝未授权特征、保持已解锁功能效用、抵御攻击和扩展性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有特征锁定技术（如密码锁定模型）在鲁棒性和扩展性方面存在不足，无法支持更经济可行的细粒度付费解锁方案。

Method: Locket采用新颖的适配器合并方法，将适配器附加到大语言模型上，以拒绝未授权的特征访问。

Result: 评估显示Locket在拒绝未授权特征方面达到100%成功率，已解锁功能效用下降≤7%，攻击成功率≤5%，并能扩展到多个特征和用户。

Conclusion: Locket是首个鲁棒且可扩展的特征锁定技术，能够有效支持付费解锁商业模式。

Abstract: Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to
generate revenue, offering basic models for free users, and advanced models for
paying subscribers. However, a finer-grained pay-to-unlock scheme for premium
features (e.g., math, coding) is thought to be more economically viable for the
providers. Such a scheme requires a feature-locking technique (FLoTE) which is
(i) effective in refusing locked features, (ii) utility-preserving for unlocked
features, (iii) robust against evasion or unauthorized credential sharing, and
(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,
password-locked models) are not robust or scalable. We present Locket, the
first robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a
novel merging approach to attach adapters to an LLM for refusing unauthorized
features. Our comprehensive evaluation shows that Locket is effective ($100$%
refusal on locked features), utility-preserving ($\leq 7$% utility degradation
in unlocked features), robust ($\leq 5$% attack success rate), and scales to
multiple features and clients.

</details>


### [7] [VeilAudit: Breaking the Deadlock Between Privacy and Accountability Across Blockchains](https://arxiv.org/abs/2510.12153)
*Minhao Qiao,Iqbal Gondal,Hai Dong*

Main category: cs.CR

TL;DR: VeilAudit是一个跨链审计框架，通过审计师可链接性实现用户隐私与监管问责的平衡，允许审计师链接同一匿名实体的交易行为而不暴露其身份。


<details>
  <summary>Details</summary>
Motivation: 区块链跨链互操作性在用户隐私和监管问责之间存在根本矛盾，现有解决方案要么完全匿名要么强制身份披露，限制了在受监管金融环境中的采用。

Method: 使用用户生成的链接审计标签，嵌入零知识证明验证有效性而不暴露用户主钱包地址，并采用特殊密文使指定审计师能够测试链接性，同时支持阈值门控身份披露。

Result: 开发了跨多个EVM链的原型系统，评估表明该框架在当前多链环境中具有实用性。

Conclusion: VeilAudit在保护用户隐私的同时实现了监管合规，为跨链环境中的匿名信誉系统提供了可行解决方案。

Abstract: Cross chain interoperability in blockchain systems exposes a fundamental
tension between user privacy and regulatory accountability. Existing solutions
enforce an all or nothing choice between full anonymity and mandatory identity
disclosure, which limits adoption in regulated financial settings. We present
VeilAudit, a cross chain auditing framework that introduces Auditor Only
Linkability, which allows auditors to link transaction behaviors that originate
from the same anonymous entity without learning its identity. VeilAudit
achieves this with a user generated Linkable Audit Tag that embeds a zero
knowledge proof to attest to its validity without exposing the user master
wallet address, and with a special ciphertext that only designated auditors can
test for linkage. To balance privacy and compliance, VeilAudit also supports
threshold gated identity revelation under due process. VeilAudit further
provides a mechanism for building reputation in pseudonymous environments,
which enables applications such as cross chain credit scoring based on
verifiable behavioral history. We formalize the security guarantees and develop
a prototype that spans multiple EVM chains. Our evaluation shows that the
framework is practical for today multichain environments.

</details>


### [8] [Leaking Queries On Secure Stream Processing Systems](https://arxiv.org/abs/2510.12172)
*Hung Pham,Viet Vo,Tien Tuan Anh Dinh,Duc Tran,Shuhao Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种针对使用Intel SGX保护执行引擎的流处理系统的时序侧信道攻击，能够从恶意云提供商的角度提取敏感查询信息，攻击成功率高达92%。


<details>
  <summary>Details</summary>
Motivation: 流处理系统在现代应用中至关重要，但云环境被视为不可信。现有研究主要关注数据保护，而忽略了查询本身包含应用逻辑的敏感性。

Method: 攻击分为两个阶段：离线阶段通过合成数据分析单个流操作符的执行时间并建立识别模型；在线阶段隔离查询中的操作符，监控执行过程，并使用离线模型恢复操作符。

Result: 基于SecureStream和NEXMark流行数据流基准测试实现攻击，攻击成功率最高达到92%。

Conclusion: 证明了从使用Intel SGX的流处理系统中提取查询的可行性，并讨论了在不产生高开销的情况下加强系统防御的方法。

Abstract: Stream processing systems are important in modern applications in which data
arrive continuously and need to be processed in real time. Because of their
resource and scalability requirements, many of these systems run on the cloud,
which is considered untrusted. Existing works on securing databases on the
cloud focus on protecting the data, and most systems leverage trusted hardware
for high performance. However, in stream processing systems, queries are as
sensitive as the data because they contain the application logics.
  We demonstrate that it is practical to extract the queries from stream
processing systems that use Intel SGX for securing the execution engine. The
attack performed by a malicious cloud provider is based on timing side
channels, and it works in two phases. In the offline phase, the attacker
profiles the execution time of individual stream operators, based on synthetic
data. This phase outputs a model that identifies individual stream operators.
In the online phase, the attacker isolates the operators that make up the
query, monitors its execution, and recovers the operators using the model in
the previous phase. We implement the attack based on popular data stream
benchmarks using SecureStream and NEXMark, and demonstrate attack success rates
of up to 92%. We further discuss approaches that can harden streaming
processing systems against our attacks without incurring high overhead.

</details>


### [9] [PromptLocate: Localizing Prompt Injection Attacks](https://arxiv.org/abs/2510.12252)
*Yuqi Jia,Yupei Liu,Zedian Shao,Jinyuan Jia,Neil Gong*

Main category: cs.CR

TL;DR: 提出了PromptLocate方法，这是首个用于定位注入提示的方法，通过三个步骤在受污染数据中准确定位注入的指令和数据。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击通过污染输入数据欺骗大型语言模型执行攻击者指定的任务而非预期任务，定位注入提示对于攻击后取证分析和数据恢复至关重要，但该领域尚未得到充分探索。

Method: PromptLocate包含三个步骤：(1)将受污染数据分割成语义连贯的片段，(2)识别被注入指令污染的片段，(3)定位被注入数据污染的片段。

Result: PromptLocate在八个现有攻击和八个自适应攻击中都能准确定位注入提示。

Conclusion: 该研究填补了提示注入定位领域的空白，提出的PromptLocate方法能够有效识别和定位注入提示，为后攻击分析提供重要工具。

Abstract: Prompt injection attacks deceive a large language model into completing an
attacker-specified task instead of its intended task by contaminating its input
data with an injected prompt, which consists of injected instruction(s) and
data. Localizing the injected prompt within contaminated data is crucial for
post-attack forensic analysis and data recovery. Despite its growing
importance, prompt injection localization remains largely unexplored. In this
work, we bridge this gap by proposing PromptLocate, the first method for
localizing injected prompts. PromptLocate comprises three steps: (1) splitting
the contaminated data into semantically coherent segments, (2) identifying
segments contaminated by injected instructions, and (3) pinpointing segments
contaminated by injected data. We show PromptLocate accurately localizes
injected prompts across eight existing and eight adaptive attacks.

</details>


### [10] [Targeted Pooled Latent-Space Steganalysis Applied to Generative Steganography, with a Fix](https://arxiv.org/abs/2510.12414)
*Etienne Levecque,Aurélien Noirault,Tomáš Pevný,Jan Butora,Patrick Bas,Rémi Cogranne*

Main category: cs.CR

TL;DR: 本文提出在潜在空间中通过建模潜在向量范数的统计分布来进行隐写分析，针对Hu等人提出的潜在扩散模型隐写方案，证明了在隐写后潜在向量分布在超球面上，而原始向量是独立同分布的高斯分布，从而能够通过似然比检验进行隐写检测。


<details>
  <summary>Details</summary>
Motivation: 现有的隐写分析主要在图像空间进行检测，而针对生成图像的隐写方案在潜在空间中修改种子向量嵌入信息，因此需要在潜在空间进行隐写分析以提高检测效果。

Method: 通过分析潜在向量范数的统计分布，将原始和隐写假设下的向量范数建模为具有不同方差的高斯分布，并推导似然比检验进行隐写分析。

Result: 研究表明在潜在空间中能够有效检测Hu等人提出的隐写方案，同时发现通过随机采样潜在向量范数可以使原始隐写方案在潜在空间中不可检测。

Conclusion: 在潜在空间中进行隐写分析比在图像空间中更有效，通过建模潜在向量范数的统计特性可以检测隐写嵌入，同时提出了使隐写方案在潜在空间中不可检测的方法。

Abstract: Steganographic schemes dedicated to generated images modify the seed vector
in the latent space to embed a message, whereas most steganalysis methods
attempt to detect the embedding in the image space. This paper proposes to
perform steganalysis in the latent space by modeling the statistical
distribution of the norm of the latent vector. Specifically, we analyze the
practical security of a scheme proposed by Hu et. al. for latent diffusion
models, which is both robust and practically undetectable when steganalysis is
performed on generated images. We show that after embedding, the Stego (latent)
vector is distributed on a hypersphere while the Cover vector is i.i.d.
Gaussian. By going from the image space to the latent space, we show that it is
possible to model the norm of the vector in the latent space under the Cover or
Stego hypothesis as Gaussian distributions with different variances. A
Likelihood Ratio Test is then derived to perform pooled steganalysis. The
impact of the potential knowledge of the prompt and the number of diffusion
steps, is also studied. Additionally, we also show how, by randomly sampling
the norm of the latent vector before generation, the initial Stego scheme
becomes undetectable in the latent space.

</details>


### [11] [Formal Models and Convergence Analysis for Context-Aware Security Verification](https://arxiv.org/abs/2510.12440)
*Ayush Chaudhary*

Main category: cs.CR

TL;DR: 提出了一个用于上下文感知安全验证的形式化框架，为ML增强的自适应系统建立可证明的保证。引入了上下文完备性这一新安全属性，并证明了样本复杂度边界、信息理论限制、ML负载生成器的收敛保证以及组合健全性边界。


<details>
  <summary>Details</summary>
Motivation: 现有静态验证器在有限负载预算下最多只能达到α的完备性，而上下文感知验证器在足够信息下可以实现超过α的完备性。需要建立理论框架来证明自适应验证相对于静态方法的可证明改进。

Method: 引入上下文完备性安全属性，建立形式化框架，证明样本复杂度边界、信息理论限制、ML负载生成器收敛保证和组合健全性边界。通过97,224个漏洞样本的受控实验验证理论预测。

Result: 检测准确率从58%提升到69.93%，成功率从51%增加到82%，训练损失以O(1/√T)速率收敛，误报率10.19%在理论边界12%内。上下文感知验证器在足够信息下可实现超过静态验证器的完备性。

Conclusion: 理论基础的适应性验证在既定假设下实现了相对于静态方法的可证明改进，同时保持了健全性保证。上下文感知验证器在信息充足时能够超越静态验证器的性能极限。

Abstract: We present a formal framework for context-aware security verification that
establishes provable guarantees for ML-enhanced adaptive systems. We introduce
context-completeness - a new security property - and prove: (1) sample
complexity bounds showing when adaptive verification succeeds, (2)
information-theoretic limits relating context richness to detection capability,
(3) convergence guarantees for ML-based payload generators, and (4)
compositional soundness bounds. We further provide a formal separation between
static context-blind verifiers and context-aware adaptive verifiers: for a
natural family of targets, any static verifier with finite payload budget
achieves completeness at most alpha, while a context-aware verifier with
sufficient information achieves completeness greater than alpha. We validate
our theoretical predictions through controlled experiments on 97,224 exploit
samples, demonstrating: detection accuracy improving from 58% to 69.93% with
dataset growth, success probability increasing from 51% to 82% with context
enrichment, training loss converging at O(1/sqrt(T)) rate, and false positive
rate (10.19%) within theoretical bounds (12%). Our results show that
theoretically-grounded adaptive verification achieves provable improvements
over static approaches under stated assumptions while maintaining soundness
guarantees.

</details>


### [12] [Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in Containerized Clouds](https://arxiv.org/abs/2510.12629)
*Gunwoo Kim,Taejune Park,Jinwoo Kim*

Main category: cs.CR

TL;DR: 本文分析了NVIDIA BlueField-3 RDMA NIC上的两种资源耗尽攻击：状态饱和攻击和流水线饱和攻击，并提出了HT-Verbs框架来缓解这些威胁，实现性能隔离。


<details>
  <summary>Details</summary>
Motivation: 在现代容器化云环境中，RDMA的采用需要强性能隔离来确保一个容器的RDMA工作负载不会降低其他容器的性能，但现有的隔离技术由于RDMA NIC中微架构资源管理的复杂性而难以有效应用。

Method: 实验分析两种资源耗尽攻击，并提出HT-Verbs框架，该框架基于实时每个容器RDMA动词遥测和自适应资源分类，将RNIC资源划分为热、温、冷层级，并限制滥用工作负载，无需硬件修改。

Result: 状态饱和攻击可导致受害者容器带宽损失高达93.9%，延迟增加1,117倍，缓存未命中率上升115%；流水线饱和攻击导致严重的链路级拥塞和显著放大效应，小动词请求导致不成比例的高资源消耗。

Conclusion: HT-Verbs框架能够有效缓解RDMA资源耗尽攻击，恢复可预测的安全保证，实现容器间的性能隔离。

Abstract: In modern containerized cloud environments, the adoption of RDMA (Remote
Direct Memory Access) has expanded to reduce CPU overhead and enable
high-performance data exchange. Achieving this requires strong performance
isolation to ensure that one container's RDMA workload does not degrade the
performance of others, thereby maintaining critical security assurances.
However, existing isolation techniques are difficult to apply effectively due
to the complexity of microarchitectural resource management within RDMA NICs
(RNICs). This paper experimentally analyzes two types of resource exhaustion
attacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline
saturation attacks. Our results show that state saturation attacks can cause up
to a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in
cache misses for victim containers, while pipeline saturation attacks lead to
severe link-level congestion and significant amplification, where small verb
requests result in disproportionately high resource consumption. To mitigate
these threats and restore predictable security assurances, we propose HT-Verbs,
a threshold-driven framework based on real-time per-container RDMA verb
telemetry and adaptive resource classification that partitions RNIC resources
into hot, warm, and cold tiers and throttles abusive workloads without
requiring hardware modifications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [13] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe是一个动态重构LLM服务管道的系统，通过细粒度模型划分、运行时管道重构和拓扑感知资源分配，显著提升资源效率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统使用静态管道配置，难以适应动态工作负载变化和严重的资源碎片化问题，导致效率低下。

Method: 将模型分解为细粒度阶段，基于实时请求模式分析智能调整管道粒度，实现细粒度模型划分、运行时管道重构和拓扑感知资源分配。

Result: 在82-GPU集群上的评估显示，FlexPipe实现了8.5倍的资源效率提升，延迟降低38.3%，GPU预留需求从峰值容量的75%降至30%。

Conclusion: FlexPipe通过动态管道重构有效解决了LLM服务中的资源效率问题，显著降低了GPU资源需求并提升了性能。

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [14] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: 本文提出以单个计算节点作为跨平台性能比较的基本单位，提供了设置、运行和分析节点到节点扩展研究的指导方法，包括结果展示模板和案例研究。


<details>
  <summary>Details</summary>
Motivation: 由于高性能计算架构日益多样化，研究人员和实践者越来越需要在不同平台上比较代码的性能和可扩展性，但目前缺乏如何进行此类跨平台研究的实际指导。

Method: 将单个计算节点作为跨平台研究的基本计算单位，提供设置、运行和分析节点到节点扩展研究的系统指导，包括结果展示模板。

Result: 通过多个案例研究展示了该方法的优势，验证了以节点为基本单位进行跨平台性能比较的有效性。

Conclusion: 以单个计算节点作为跨平台性能比较的基本单位是自然且有效的方法，本文提供的指导框架有助于系统地进行此类研究。

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [15] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: 本文提出了两种基于GPU加速的进程映射算法，用于将任务图分配到超级计算机处理单元，实现计算负载均衡和通信成本最小化。两种方法相比CPU算法分别获得超过300倍和77.6倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 受GPU图像分割器近期成功的启发，作者希望利用GPU并行计算能力来加速进程映射这一优化问题，这是首次提出基于GPU的进程映射算法。

Method: 第一种算法采用分层多分割方法，沿超级计算机层次结构分割任务图，利用GPU图像分割器加速映射过程；第二种算法将进程映射直接集成到现代多级图像分割流程中，通过GPU并行加速关键阶段如粗化和细化。

Result: 实验显示两种方法相比最先进的CPU算法分别获得超过300倍和77.6倍的加速比。第一种算法通信成本平均增加10%，但仍具竞争力；第二种算法速度更快但解质量较低。

Conclusion: 这是首个基于GPU的进程映射算法，两种方法在保持解质量的同时实现了显著的性能提升，证明了GPU加速在进程映射问题中的有效性。

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [16] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: 提出了一种基于Kubernetes的工具，用于在数据网格架构中延迟且非侵入式地应用云设计模式，无需修改服务源代码，同时收集能耗指标以支持能源感知决策。


<details>
  <summary>Details</summary>
Motivation: 在联邦环境中构建消费者特定的数据共享管道时，传统云设计模式的预定义和嵌入会损害模块化、降低可重用性，并与管道的动态消费者驱动特性相冲突。

Method: 开发基于Kubernetes的工具，支持自动化模式注入，允许延迟应用选定的云设计模式，无需更改服务源代码，同时收集能耗指标。

Result: 该工具能够在保持可重用数据共享管道灵活可组合结构的同时，实现云设计模式的非侵入式应用，并提供能耗数据供开发者决策。

Conclusion: 该方法解决了在数据网格架构中集成云设计模式时的模块化和可重用性挑战，支持能源感知的管道设计决策。

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [17] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: TALP-Pages是一个易于集成的框架，通过TALP工具实时收集性能指标，在CI环境中生成HTML报告，可视化性能因子回归和扩展效率表，帮助开发者在开发过程中快速检测性能退化。


<details>
  <summary>Details</summary>
Motivation: HPC代码开发过程中需要早期检测性能退化，并深入了解应用扩展行为与开发流程的紧密耦合。

Method: 基于适合CI的文件夹结构，利用TALP工具实时收集性能指标，生成包含性能因子回归可视化和扩展效率表的HTML报告。

Result: 与基于追踪的工具相比，TALP-Pages在更严格的资源约束下能更快生成扩展效率表，并在GENE-X的CI设置中成功检测并解释了性能改进。

Conclusion: TALP-Pages提供了一个轻量级、高效的解决方案，能够无缝集成到开发流程中，为HPC代码性能监控提供快速反馈。

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>


### [18] [Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT](https://arxiv.org/abs/2510.12597)
*Ilya Baldin,Michael Goodrich,Vardan Gyurjyan,Graham Heyes,Derek Howard,Yatish Kumar,David Lawrence,Brad Sawatzky,Stacey Sheldon,Carl Timmer*

Main category: cs.DC

TL;DR: EJFAT架构通过FPGA加速实现边缘到计算集群的负载均衡，支持实验数据流的直接处理，为JLab科学项目和数据中心提供高吞吐低延迟解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决实验数据流处理中的高吞吐和低延迟需求，支持JLab科学项目及未来数据中心的时间关键型数据采集系统和工作流程。

Method: 采用FPGA加速技术处理压缩、分片、UDP包目标重定向（NAT）、解压和重组，实现边缘与集群计算的无缝集成。

Result: EJFAT与DOE其他活动（如集成研究基础设施IRI）协同工作，已在JLab数据源、ESnet负载均衡器和LBNL计算集群资源上取得初步成果。

Conclusion: EJFAT架构成功展示了FPGA加速在边缘到集群负载均衡中的有效性，为高吞吐低延迟数据处理提供了可行解决方案。

Abstract: Thomas Jefferson National Accelerator Facility (JLab) has partnered with
Energy Sciences Network (ESnet) to define and implement an edge to compute
cluster computational load balancing acceleration architecture. The ESnet-JLab
FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to
address compression, fragmentation, UDP packet destination redirection (Network
Address Translation (NAT)) and decompression and reassembly.
  EJFAT seamlessly integrates edge and cluster computing to support direct
processing of streamed experimental data. This will directly benefit the JLab
science program as well as data centers of the future that require high
throughput and low latency for both time-critical data acquisition systems and
data center workflows.
  The EJFAT project will be presented along with how it is synergistic with
other DOE activities such as an Integrated Research Infrastructure (IRI), and
recent results using data sources at JLab, an EJFAT LB at ESnet, and
computational cluster resources at Lawrence Berkeley National Laboratory
(LBNL).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [19] [A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems](https://arxiv.org/abs/2510.12277)
*Thomas Benz,Axel Vanoni,Michael Rogenmoser,Luca Benini*

Main category: cs.AR

TL;DR: 本文提出了一种优化的基于描述符的DMA控制器，专门针对小尺寸数据传输进行优化，通过轻量级描述符格式和推测性预取方案，显著提升了传输效率和总线利用率。


<details>
  <summary>Details</summary>
Motivation: 随着计算系统异构性增加和机器学习应用需求，内存系统需要高效处理任意且要求更高的数据传输。传统基于描述符的DMA控制器在处理小尺寸数据传输时效率低下，存在描述符过大和处理序列化导致的静态开销问题。

Method: 设计并实现了一个AXI4基础的DMA控制器，采用轻量级描述符格式，并实现了低开销的推测性描述符预取方案，在预测错误时不会产生额外延迟。

Result: 与现成的基于描述符的DMA IP相比，传输启动延迟减少1.66倍，在64字节长度传输的理想内存系统中总线利用率提升高达2.5倍，同时所需查找表减少11%，触发器减少23%，且无需块RAM。在深度内存系统中，总线利用率优势可扩展至3.6倍。

Conclusion: 该优化的DMA控制器在GF12LP+工艺节点下实现了超过1.44 GHz的时钟频率，仅占用49.5 kGE面积，为小尺寸数据传输提供了高效解决方案。

Abstract: With the ever-growing heterogeneity in computing systems, driven by modern
machine learning applications, pressure is increasing on memory systems to
handle arbitrary and more demanding transfers efficiently. Descriptor-based
direct memory access controllers (DMACs) allow such transfers to be executed by
decoupling memory transfers from processing units. Classical descriptor-based
DMACs are inefficient when handling arbitrary transfers of small unit sizes.
Excessive descriptor size and the serialized nature of processing descriptors
employed by the DMAC lead to large static overheads when setting up transfers.
To tackle this inefficiency, we propose a descriptor-based DMAC optimized to
efficiently handle arbitrary transfers of small unit sizes. We implement a
lightweight descriptor format in an AXI4-based DMAC. We further increase
performance by implementing a low-overhead speculative descriptor prefetching
scheme without additional latency penalties in the case of a misprediction. Our
DMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a
Kintex FPGA to evaluate its performance. Compared to an off-the-shelf
descriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,
increase bus utilization up to 2.5x in an ideal memory system with
64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer
flip-flops, and no block RAMs. We can extend our lead in bus utilization to
3.6x with 64-byte-length transfers in deep memory systems. We synthesized our
DMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44
GHz while occupying only 49.5 kGE.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: 本文发现LLM作为评估器存在严重的正向偏差问题，并提出少数否决策略和基于回归的框架来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 新的LLM模型不断涌现，开发者需要决定是否切换到新模型。人工评估成本高且不可扩展，而现有的LLM-as-a-judge方法存在严重的正向偏差问题。

Method: 提出两种方法：最优少数否决策略（对缺失数据具有鲁棒性）和基于回归的框架（使用少量人工标注数据直接建模验证器偏差）。

Result: 在一个包含366个高中Python程序的代码反馈任务中，回归方法将最大绝对误差降低到仅1.2%，比14个最先进LLM的最佳集成性能提升2倍。

Conclusion: LLM评估器存在系统性正向偏差，提出的少数否决策略和回归框架能有效缓解这一问题，显著提高评估准确性。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [21] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 本文介绍了HAL（整体智能体排行榜），通过标准化评估框架、三维分析和LLM辅助日志检查来解决AI智能体评估中的挑战，发现了一些反直觉现象和未报告的行为。


<details>
  <summary>Details</summary>
Motivation: AI智能体在复杂现实任务中的应用日益广泛，但现有评估方法存在诸多挑战，影响了我们对智能体真实性能的理解。

Method: 1）开发标准化评估框架，在数百个虚拟机上进行并行评估；2）进行涵盖模型、框架和基准的三维分析；3）使用LLM辅助日志检查来发现未报告行为。

Result: 进行了21,730次智能体测试，涵盖9个模型和9个基准，发现反直觉现象（如更高推理努力反而降低准确性），并识别出智能体在任务中的异常行为（如搜索基准而非解决问题）。

Conclusion: 通过标准化智能体评估方法并解决常见评估陷阱，希望将重点从在基准测试中表现优异的智能体转向在现实世界中可靠工作的智能体。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [22] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: CGBench是一个用于评估语言模型在科学文献解释能力的基准测试，基于临床遗传学专家标注的ClinGen资源构建，测试模型在提取实验结果、评估证据强度、分类实验成果等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 传统基因和变异解释方法耗时耗力，生成式语言模型可以加速这一过程，但现有基准测试任务过于狭窄，无法反映真实研究需求。

Method: 从ClinGen专家标注的临床遗传学文献中构建CGBench基准，测试8种不同语言模型在提取实验结果、判断证据强度、分类实验成果三个方面的能力。

Result: 模型在文献解释方面表现出潜力但存在显著差距，推理模型在细粒度任务上表现更好，非推理模型在高层次解释上更优，模型经常产生幻觉或误解结果。

Conclusion: CGBench揭示了语言模型在科学文献精确解释方面的优势和不足，为AI在临床遗传学和更广泛科学领域的未来研究开辟了道路。

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [23] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: 本文提出了一种训练LLMs生成顺序澄清问题以揭示用户偏好的新方法，采用受扩散模型启发的两阶段过程，显著提升了LLM在询问漏斗问题和有效获取用户偏好方面的能力。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，当用户历史有限时，需要通过澄清问题来获取更多用户偏好信息。然而，生成跨领域的有效顺序澄清问题仍然是一个挑战。

Method: 采用受扩散模型启发的两阶段过程：前向过程从用户档案生成澄清问题并逐步移除答案作为'噪声'；反向过程训练模型通过询问有效澄清问题来'去噪'用户档案。

Result: 实验结果表明，该方法显著提高了LLM在询问漏斗问题和有效获取用户偏好方面的熟练度。

Conclusion: 提出的两阶段训练方法能够有效提升LLM生成顺序澄清问题的能力，从而更好地揭示用户偏好。

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [24] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: CausalTrace是一个集成到工业Copilot中的神经符号因果分析模块，通过数据驱动的因果分析结合工业本体和知识图谱，提供因果发现、反事实推理和根本原因分析等功能，实现透明可解释的决策支持。


<details>
  <summary>Details</summary>
Motivation: 现代制造环境需要不仅准确预测还要可解释的异常处理、根本原因分析和干预措施。现有AI系统往往是孤立的黑盒，缺乏预测、解释和因果推理的集成，限制了在高风险工业环境中的可信度和实用性。

Method: 开发CausalTrace神经符号因果分析模块，集成到SmartPilot工业Copilot中，通过数据驱动的因果分析结合工业本体和知识图谱，支持因果发现、反事实推理和根本原因分析，提供实时操作员交互。

Result: 在学术火箭组装测试平台上，CausalTrace与领域专家达成高度一致（本体问答ROUGE-1：0.91），RCA性能强劲（MAP@3：94%，PR@2：97%，MRR：0.92，Jaccard：0.92），在C3AN评估中获得4.59/5分，展示了实时部署的精确性和可靠性。

Conclusion: CausalTrace成功解决了工业AI系统中预测、解释和因果推理的集成问题，提供了透明可解释的决策支持，在高风险工业环境中具有实际应用价值。

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [25] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: PACT是一个程序评估和契约遵守评估框架，旨在系统评估和增强LLM生成代码片段的契约遵守能力，弥补现有基准测试仅关注功能正确性而忽略契约约束的不足。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试（如HumanEval+和MBPP+）主要评估功能正确性，但忽略了真实软件的关键方面——契约遵守，即拒绝不符合前提条件和有效性约束的输入。这一关键疏忽意味着现有基准无法衡量模型生成真正健壮可靠代码的能力。

Method: PACT框架提供专注于契约违反的全面测试套件语料库，扩展HumanEval+和MBPP+；通过不同提示条件系统分析代码生成；引入新颖指标来严格量化测试生成和代码生成中的契约遵守。

Result: 分析表明，在提示中增加契约违反测试用例比仅使用契约描述能显著增强模型遵守契约的能力。PACT揭示了传统基准忽略的关键错误。

Conclusion: PACT通过提供严格且可解释的指标，能够评估LLM生成代码片段在功能和契约遵守两方面的鲁棒性，填补了现有评估框架的重要空白。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [26] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 本文提出了一种地理空间感知层(GAL)，将大语言模型与结构化地球数据相结合，以改进灾害响应中的资源分配建议。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法缺乏语义上下文、跨事件泛化能力差且可解释性有限，而大语言模型虽然具有少样本泛化能力，但受限于文本且缺乏地理感知。

Method: 从原始野火检测开始，GAL自动从外部地理数据库中检索并整合基础设施、人口统计、地形和天气信息，将其组装成简洁的感知脚本，使智能体能够生成基于证据的资源分配建议。

Result: 在多个LLM模型上的真实野火场景评估显示，地理空间接地的智能体能够超越基线方法。

Conclusion: 该框架可以泛化到其他灾害类型，如洪水和飓风。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [27] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个无需训练即可自动优化大型推理模型（LRMs）推理性能的框架，通过进化过程生成think-prefixes来引导模型实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型存在推理效率低下和偏离目标的问题，而无需训练的方法要么局限于僵化的启发式规则，要么只能提供描述性但不可操作的分析。

Method: 使用进化过程生成think-prefixes（思考前缀），这些前缀由推理行为分类驱动，引导模型朝向更优性能发展。

Result: ThinkPilot显著改善了准确性与推理长度的权衡关系，大幅提升安全性（如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7），并增强了指令遵循能力。

Conclusion: think-prefixes能够可靠地控制LRMs的推理行为，不同任务对特定行为分布有强烈偏好，ThinkPiot通过自动识别和激发这些行为，提供了一个可泛化的框架来使LRMs推理与任务需求对齐。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [28] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: 该论文重新诠释了AI智能体在学习中的作用，将其视为具有计算能力的随机动力系统，强调时间在学习推理中的基础作用。论文提出从归纳学习转向转导学习，关注减少解决新任务所需时间而非数据分布近似，并展示了模型规模扩展可能导致智能体行为像专家但缺乏真正洞察力。


<details>
  <summary>Details</summary>
Motivation: 探索AI推理智能体是否具有通用性，能否通过思维链推理解决任何可计算任务，以及AI智能体如何学习推理。质疑学习是否是模型规模或训练数据集大小的问题，重新思考学习在AI智能体中的作用。

Method: 将AI智能体重新解释为具有计算能力的随机动力系统，强调时间在学习推理中的基础作用。提出从经典归纳学习转向转导学习，目标是捕捉数据的算法结构以减少解决新任务所需时间。理论推导了推理时间与训练时间之间的幂律缩放关系。

Result: 展示了通用求解器使用过去数据所能达到的最优加速与其算法信息紧密相关。证明了推理时间与训练时间之间的幂律缩放关系。发现模型规模扩展可能导致智能体行为像专家，能够暴力解决任何任务但缺乏洞察力。

Conclusion: 在扩展推理模型时，需要优化的关键量是时间，而非仅仅模型规模。时间在学习中的关键作用此前仅被间接考虑。转导学习表明信息在学习中的关键作用是减少时间而非减少重构误差，这与香农理论形成对比。

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [29] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: HiCoTraj是一个利用大语言模型进行零样本人口属性推断的框架，通过将轨迹数据转换为语义丰富的自然语言表示，并采用分层思维链推理方法，无需标注训练数据即可从人类移动模式推断年龄、性别等人口属性。


<details>
  <summary>Details</summary>
Motivation: 现有基于移动轨迹的人口属性推断方法严重依赖带标注的大规模轨迹数据，导致可解释性差且在不同数据集和用户群体间泛化能力不足。HiCoTraj旨在解决标注数据稀缺问题，同时提供透明的推理过程。

Method: HiCoTraj将轨迹转换为详细的活动记录和多尺度访问摘要的自然语言表示，然后通过分层思维链推理引导LLMs完成三个认知阶段：事实特征提取、行为模式分析和结构化输出的人口属性推断。

Result: 在真实世界轨迹数据上的实验评估表明，HiCoTraj在零样本场景下对多个人口属性实现了有竞争力的性能。

Conclusion: HiCoTraj框架成功解决了标注数据稀缺问题，利用LLMs的零样本学习和语义理解能力，为人口属性推断提供了可解释且泛化性强的解决方案。

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [30] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: BeSTAD是一个无监督框架，通过联合建模空间上下文和时间动态来检测人类移动数据中的个体级异常，能够从大规模人群中捕捉个体化行为特征并发现细粒度异常。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测主要关注轨迹级分析，但在大规模数据集中检测个体相对于自身历史模式的异常行为仍然是一个重大挑战。

Method: BeSTAD学习语义丰富的移动表示，整合位置意义和时间模式，采用行为聚类感知建模机制构建个性化行为档案，并通过跨时期行为比较识别异常。

Result: 该方法能够检测行为转变和偏离既定常规的异常，并识别在大规模移动数据集中表现出此类变化的个体。

Conclusion: 通过直接从无标签数据学习个体行为，BeSTAD将异常检测推向个性化和可解释的移动分析。

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [31] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型处理随机性任务的能力，发现虽然LLM能产生一定随机性的输出，但表现不一致且与预期行为存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的快速发展，许多应用需要随机性，但LLM在处理随机性方面的能力尚不明确，因此需要系统评估其生成和使用随机数的能力。

Method: 设计了一系列实验，考虑外部工具可访问性、任务类型、模型状态（新鲜vs非新鲜）和提示策略等因素，涵盖生成随机数、随机字符串、项目洗牌以及使用熵和NIST随机性测试套件评估随机性质量等任务。

Result: LLM生成的输出具有一定程度的随机性，但表现不一致且经常显著偏离预期行为。

Conclusion: 实验结果表明LLM在处理涉及随机性的任务时存在关键局限性，需要改进才能有效处理此类任务。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [32] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent是一个基于多智能体AI的框架，通过自然语言指令执行拓扑聚合物的粗粒度分子动力学模拟，集成了大语言模型和领域专用计算工具。


<details>
  <summary>Details</summary>
Motivation: 旨在降低复杂计算工作流的门槛，通过自然语言界面与严格模拟工具结合，推进聚合物科学中AI驱动的材料发现。

Method: 系统包含四个LLM驱动的智能体：配置智能体生成初始聚合物-溶剂配置，模拟智能体执行基于LAMMPS的MD模拟和构象分析，报告智能体编译markdown报告，工作流智能体实现流线化自主操作。支持交互式和自主式两种工作模式。

Result: 通过案例研究展示了系统在不同聚合物架构、溶剂条件、恒温器和模拟长度下的多功能性，成功研究了相互作用参数对线性聚合物构象的影响以及接枝密度对刷状聚合物持久长度的影响。

Conclusion: ToPolyAgent为自主和可扩展的多智能体科学研究生态系统奠定了基础，将自然语言界面与严格模拟工具相结合，降低了复杂计算工作流的障碍。

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [33] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种精确控制大语言模型属性强度的方法，将属性强度控制重新定义为目标达成问题，通过训练轻量级值函数和梯度干预来精确引导模型输出达到用户指定的属性强度。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型对齐方法通常只能提供方向性或开放性的指导，无法可靠地实现精确的属性强度控制。为了满足不同用户期望，需要开发能够精确控制属性强度的AI系统。

Method: 1) 将精确属性强度控制重新定义为目标达成问题；2) 通过时间差分学习训练轻量级值函数，从部分生成预测最终属性强度分数；3) 对隐藏表示进行基于梯度的干预，精确导航模型达到特定属性强度目标。

Result: 在LLaMA-3.2-3b和Phi-4-mini上的实验证实了该方法能够以高精度引导文本生成达到用户指定的属性强度。在三个下游任务中展示了效率提升：偏好数据合成、帕累托前沿近似和优化，以及对齐行为的蒸馏以实现无干预推理。

Conclusion: 该方法实现了对属性强度的细粒度连续控制，超越了简单的方向性对齐，为AI系统提供了更精确的属性强度控制能力。

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [34] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: MatSciBench是一个包含1340个大学水平材料科学问题的综合基准，涵盖6个主要领域和31个子领域，采用三级难度分类，并包含多模态推理。评估显示即使是表现最好的模型准确率也不到80%，表明该基准的复杂性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学推理方面表现出色，但在材料科学领域的推理能力尚未充分探索，需要建立一个全面的基准来评估和推动LLMs在该领域的科学推理能力发展。

Method: 创建MatSciBench基准，包含1340个结构化分类的材料科学问题，采用三级难度分类，提供详细参考解决方案，并整合多模态推理。评估了多种推理策略：基础思维链、工具增强和自我纠正。

Result: 评估显示即使是表现最好的Gemini-2.5-Pro模型在材料科学问题上的准确率也不到80%。系统分析表明没有单一推理方法在所有场景中表现一致优秀，多模态推理任务存在挑战。

Conclusion: MatSciBench为评估和推动LLMs在材料科学领域的科学推理能力建立了一个全面而坚实的基准，揭示了当前模型在该领域的局限性，并为未来改进提供了方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [35] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 本文综述了Meta AI的LLaMA系列模型（从LLaMA 1到LLaMA 4）的快速发展，以及为这些模型开发的参数高效微调（PEFT）方法，包括模型架构、性能特征和五种PEFT方法的应用分析。


<details>
  <summary>Details</summary>
Motivation: 为机器学习研究者和从业者提供关于LLaMA模型和高效微调策略的一站式资源，帮助理解LLaMA系列模型的演进和参数高效微调方法的应用。

Method: 首先描述LLaMA系列基础模型（7B-65B到288B参数）及其架构（包括原生多模态和专家混合变体），然后介绍PEFT概念，并回顾应用于LLaMA的五种PEFT方法：LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor和QLoRA。

Result: 提供了模型和适配器架构、参数数量和基准测试结果的结构化讨论与分析，包括微调后的LLaMA模型在某些情况下优于更大基线的示例，以及在法律和医疗等实际应用场景中的成功案例。

Conclusion: 本文为对LLaMA模型和高效微调策略感兴趣的ML研究者和从业者提供了全面资源，并讨论了持续挑战和未来研究方向，如扩展到更大上下文和提升鲁棒性。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [36] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: 本文对65项评估可解释AI(XAI)系统的用户研究进行了全面回顾，提出了以人为中心的XAI系统设计目标和评估框架，并根据用户AI专业水平(新手和专家)进行了差异化设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI在日常生活中的普及，对既高性能又可理解的智能系统需求日益增长。当前XAI系统的评估过程过于技术化，未能充分关注人类用户需求，需要建立以人为中心的评估和设计指南。

Method: 对65项XAI用户研究进行综合分析，提出XAI系统特性和以人为中心的评估指标分类，并根据用户AI专业水平差异制定相应的设计目标。

Result: 区分了核心系统与XAI解释组件，将评估指标分为系统情感、认知、可用性、可解释性和解释指标，并为AI新手和数据专家分别提出了针对性的设计目标。

Conclusion: 提出了现有XAI评估和设计框架的扩展，强调需要根据用户特征定制XAI系统设计，AI新手关注负责任使用、接受度和可用性，数据专家关注性能导向的人机协作和任务表现。

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [37] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: GOAT是一个无需人工标注的LLM智能体训练框架，能够从API文档自动构建目标导向的API执行任务数据集，使开源模型具备复杂推理和工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在处理目标导向查询时能力有限，需要将高级目标分解为多个相互依赖的API调用。闭源模型如GPT-4表现良好，但开源模型难以有效执行复杂工具使用，且缺乏训练数据。

Method: 提出GOAT训练框架，从API文档自动构建合成数据集，在无需人工标注的情况下对LLM智能体进行微调，使其能够推理相互依赖的调用并生成连贯响应。

Result: GOAT训练的智能体在多个现有目标导向基准测试中达到最先进性能，并在新提出的GOATBench基准测试中表现出色。

Conclusion: GOAT为构建能够进行复杂推理和工具使用的稳健开源LLM智能体提供了一条实用路径。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [38] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: 提出了PromptFlow框架，通过模块化设计和强化学习来优化大语言模型的提示工程，实现自动化提示优化。


<details>
  <summary>Details</summary>
Motivation: 解决当前提示工程方法中存在的静态更新规则、缺乏动态策略选择、粗粒度提示编辑以及经验回收不足等问题。

Method: 开发了基于TensorFlow灵感的模块化训练框架，包含元提示、操作器、优化器和评估器，采用基于梯度的元学习和强化学习来回收经验。

Result: 在多个数据集上的实验证明了PromptFlow框架的有效性。

Conclusion: PromptFlow框架能够以最小的任务特定训练数据自主探索最优提示优化轨迹，为LLM的领域适应提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [39] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 提出T³方法，通过检测和截断过度的信念偏差来改进LLM主动推理训练，提升策略优化效果


<details>
  <summary>Details</summary>
Motivation: LLM智能体在主动推理中经常出现信念偏差问题，导致无法正确建模信念、丢失问题状态跟踪，陷入无信息或重复动作，影响强化学习训练效果

Method: 开发T³方法，跟踪模型信念偏差，检测过度偏差并在训练时截断轨迹，移除无信息尾部，保留信息前缀的信用

Result: 在5个挑战性任务中，T³持续提升训练稳定性、令牌效率和最终性能，实现高达30%的性能增益，同时减少约25%的滚动令牌

Conclusion: 信念控制是开发稳健且可泛化的基于LLM的主动推理器的关键原则

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [40] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: RAG-Anything是一个统一的多模态检索增强生成框架，能够处理文本、视觉、表格和数学表达式等多种模态内容，解决了现有RAG系统仅限于文本处理的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识库本质上是多模态的，包含文本、视觉元素、结构化表格和数学表达式的丰富组合，但现有RAG框架仅限于文本内容，在处理多模态文档时存在根本性差距。

Method: 将多模态内容重新概念化为相互连接的知识实体而非孤立的数据类型，引入双图构建来捕获跨模态关系和文本语义的统一表示，开发结合结构知识导航和语义匹配的跨模态混合检索。

Result: 在具有挑战性的多模态基准测试中表现出优越性能，相比最先进方法有显著改进，特别是在传统方法失败的长文档上性能提升尤为明显。

Conclusion: 该框架为多模态知识访问建立了新范式，消除了约束当前系统的架构碎片化问题。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [41] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: LLM+CAS框架结合前沿大语言模型和计算机代数系统，通过符号反馈循环生成创造性且符号验证的证明，特别针对渐近不等式问题，能够有效提出域分解方案。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数学研究中验证困难的问题，将前沿LLM与计算机代数系统结合，为专业数学家提供研究级工具，回答Terence Tao关于LLM与验证器结合能否证明复杂渐近不等式的疑问。

Method: 提出LLM+CAS框架和O-Forge工具，在上下文符号反馈循环中，使用LLM建议域分解，CAS（如Mathematica）对每个部分进行公理化验证。

Result: 该框架在提出渐近不等式问题的适当域分解方面表现出显著效果，能够生成既具创造性又经过符号验证的证明。

Conclusion: LLM+CAS框架成功展示了AI如何超越竞赛数学，成为专业数学家的研究级工具，为研究级渐近分析提供了有效支持。

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [42] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 本文系统研究了LLM作为评判者在点式评分设置下的11种偏见类型，发现先进LLM评判者对偏见输入具有鲁棒性，提供详细评分标准可增强这种鲁棒性，但使用有偏见的高分数据进行微调会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于自主评估通信系统中内容质量，这些AI"评判者"的公正性无法保证，其评估标准中的任何偏见都可能扭曲结果并削弱用户信任。

Method: 系统研究两种LLM评判模型(GPT-Judge和JudgeLM)在点式评分设置下的11种偏见类型，涵盖隐式和显式形式，并测试了不同缓解策略。

Result: 先进LLM评判者对偏见输入具有鲁棒性，通常给它们分配比相应干净样本更低的分数；提供详细评分标准可增强鲁棒性；使用有偏见的高分数据微调会显著降低性能；评判分数与任务难度相关。

Conclusion: 提出了四种潜在的缓解策略，以确保在实际通信场景中实现公平可靠的AI评判。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [43] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: PricingLogic是第一个评估大语言模型在旅游定价场景中可靠性的基准测试，包含300个基于真实定价政策的自然语言问题，揭示了LLMs在复杂定价规则下的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 旅游机构希望将容易出错的定价任务交给AI系统，但未经验证可靠性的LLM部署可能导致重大财务损失和客户信任危机。

Method: 构建包含300个自然语言问题的基准测试，基于42个真实定价政策，涵盖两个难度级别：基础客户类型定价和涉及交互折扣的捆绑旅游计算。

Result: 对一系列LLM的评估显示，在更难层级上性能急剧下降，暴露了规则解释和算术推理的系统性失败。

Conclusion: 尽管具备通用能力，但当前LLMs在收入关键应用中仍不可靠，需要进一步的安全保障或领域适应。

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [44] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习和偏置注意力机制的无信号交叉口自动驾驶决策框架，通过风险预测器评估碰撞风险并转化为密集奖励信号，提高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 无信号交叉口自动驾驶决策面临复杂动态交互和高冲突风险的挑战，需要实现主动安全控制。

Method: 基于Soft Actor-Critic算法，使用偏置注意力机制构建交通风险预测器，将长期碰撞风险转化为密集奖励信号指导决策。

Result: 仿真结果表明该方法有效提高了交叉口的交通效率和车辆安全性。

Conclusion: 该智能决策框架在复杂场景中具有有效性，证明了基于风险预测的强化学习方法在自动驾驶决策中的实用性。

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [45] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: 开发了一个基于有向无环图的医疗问答对话框架，包含问题转换、冷启动、自适应分支、终止逻辑和报告生成功能，在初步评估中表现出良好的用户体验和工作流程集成效果。


<details>
  <summary>Details</summary>
Motivation: 为医疗领域开发一个任务导向的对话系统，能够系统化地将医疗算法和指南转化为临床问题，支持高效的医患交互和结构化报告生成。

Method: 采用有向无环图结构，集成五个核心机制：医疗指南转换、基于层次聚类的冷启动、扩展剪枝自适应分支、终止逻辑和结构化报告自动合成。

Result: 患者应用认知负荷低（NASA-TLX=15.6）、可用性高（SUS=86）、满意度强（QUIS=8.1/9）；医生应用认知负荷中等（NASA-TLX=26）、可用性优秀（SUS=88.5）、满意度高（8.3/9）。

Conclusion: 该系统有效集成到临床工作流程中，降低了认知需求并支持高效报告生成，但存在系统延迟和小样本评估的局限性。

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [46] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: 本文提出了细胞状态潜在（CSL）视角，通过操作符语法组织学习过程，强调跨模态、尺度、情境和干预的决策对齐评估，以解决当前AI虚拟细胞模型在实验室间迁移性、数据泄露和跨尺度耦合方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI虚拟细胞模型虽然取得了进展，但在跨实验室和平台的可迁移性、数据分割的泄露和覆盖偏差、剂量时间组合效应的系统处理等方面存在局限，跨尺度耦合也受到约束。

Method: 提出模型无关的细胞状态潜在（CSL）视角，通过操作符语法组织学习：测量、跨尺度耦合的升降投影、以及剂量和调度的干预。

Result: 提出了决策对齐的评估蓝图，涵盖模态、尺度、情境和干预，强调功能空间读出如通路活性、空间邻域和临床相关终点。

Conclusion: 建议采用操作符感知的数据设计、抗泄露的分割以及透明的校准和报告，以实现可重复的同类比较。

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [47] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: ProtoSiTex是一个用于细粒度多标签文本分类的半可解释框架，采用双阶段交替训练策略，通过原型发现和分类映射实现语义一致性，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 用户生成评论的激增需要可解释模型提供细粒度洞察，现有原型模型通常在粗粒度（句子或文档级别）操作，无法处理现实世界文本分类的多标签性质。

Method: 采用双阶段交替训练策略：无监督原型发现阶段学习语义连贯且多样化的原型，监督分类阶段将这些原型映射到类别标签。使用分层损失函数在子句、句子和文档级别强制一致性，通过自适应原型和多头注意力捕获重叠和冲突语义。

Result: 在酒店评论基准数据集和两个公共基准测试（二分类和多分类）上的实验表明，ProtoSiTex实现了最先进的性能，同时提供忠实、与人类对齐的解释。

Conclusion: ProtoSiTex为半可解释多标签文本分类提供了一个强大的解决方案，能够捕获复杂语义关系并提供可解释的洞察。

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [48] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: 该论文提出了HardcoreLogic基准测试，包含5000多个逻辑谜题，旨在评估大型推理模型在非标准游戏变体中的真实推理能力，揭示现有模型过度依赖记忆模式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型在标准逻辑谜题上表现良好，但面对非标准变体时能否灵活应用规则仍存疑问。现有基准主要关注标准格式，可能导致模型过度拟合和模式记忆，掩盖了理解新规则和适应新变体的能力缺陷。

Method: 通过三个维度系统性地转换标准谜题：增加复杂性(IC)、非常规元素(UE)和无解谜题(UP)，创建了包含10种游戏的5000多个谜题的HardcoreLogic基准，减少对捷径记忆的依赖。

Result: 评估显示，即使在现有基准上表现优异的模型在HardcoreLogic上也出现显著性能下降，表明模型严重依赖记忆的刻板印象。增加复杂性是主要困难来源，但模型在细微规则变化上也表现不佳。

Conclusion: HardcoreLogic暴露了当前大型推理模型的局限性，为推进高级逻辑推理建立了基准测试，表明需要开发真正理解规则而非依赖记忆的推理模型。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [49] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: ERA是一个两阶段框架，通过先验知识学习和在线强化学习，使小型视觉语言模型在具身AI任务中超越大型模型，在EB-ALFRED和EB-Manipulation任务上分别比GPT-4o提升8.4%和19.4%。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型部署成本高而小型模型缺乏必要知识和技能的问题，弥合性能与效率之间的差距。

Method: 两阶段框架：第一阶段通过轨迹增强先验、环境锚定先验和外部知识先验学习基础知识；第二阶段采用在线强化学习，包含自我总结、密集奖励塑造和回合级策略优化。

Result: ERA-3B在EB-ALFRED和EB-Manipulation任务上分别比GPT-4o提升8.4%和19.4%，在未见任务上表现出强泛化能力。

Conclusion: ERA为可扩展的具身智能提供了一条实用路径，为未来具身AI系统提供了方法论见解。

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [50] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出多智能体辩论法官框架，通过协作推理和迭代优化提升LLM自动判断任务的准确性，引入稳定性检测机制提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为法官的方法依赖简单聚合（如多数投票），即使个体智能体提供正确答案也可能失败，需要更有效的协作判断方法。

Method: 建立多智能体辩论法官框架，智能体协作推理并迭代优化响应；引入基于时间变化Beta-Binomial混合的稳定性检测机制，使用Kolmogorov-Smirnov检验进行自适应停止。

Result: 在多个基准测试和模型上的实验表明，该框架相比多数投票提高了判断准确性，同时保持了计算效率。

Conclusion: 多智能体辩论框架能够有效提升LLM自动判断任务的性能，通过数学建模和稳定性检测实现了准确性和效率的平衡。

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [51] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: 提出一种无需标记数据的OOD检测方法，通过自监督学习和图论技术提高AI系统鲁棒性，在AUROC指标上达到0.99的优异性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、交通、医疗等安全关键系统中，AI系统需要在各种条件下保持可靠性能，包括处理分布外样本、对抗攻击和环境变化，系统故障可能带来严重后果。

Method: 结合自监督学习原理和图论技术，从无标记数据中学习有用表示，实现更高效的OOD样本识别和分类。

Result: 与现有最先进方法相比，该方法在AUROC指标上达到了0.99的优异性能。

Conclusion: 该方法能够有效提高AI系统的鲁棒性，特别是在处理分布外样本方面表现出色，为安全关键应用提供了可靠的解决方案。

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [52] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: CLUTCH是一种基于深度组合多臂老虎机的新颖JavaScript模糊测试方法，通过注意力机制和Concrete Dropout动态适应探索策略，相比现有方法提高了测试用例有效性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript模糊测试方法在突变目标选择上采用随机策略，作者认为这个问题适合用具有可变臂数的组合多臂老虎机来解决，以提高模糊测试效率。

Method: 提出CLUTCH深度组合多臂老虎机，能够观察可变长度的JavaScript测试用例表示，使用注意力机制，并通过Concrete Dropout动态调整探索策略。

Result: CLUTCH相比三种最先进解决方案，平均提高了20.3%的有效测试用例数量和8.9%的每测试用例覆盖率；在易变和组合设置中，分别实现了至少78.1%和4.1%的更少遗憾值。

Conclusion: CLUTCH在JavaScript模糊测试中显著提高了效率，证明了组合多臂老虎机方法在突变目标选择问题上的有效性。

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [53] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec是一种允许用户通过自然语言请求实时控制推荐系统的方法，通过LLM模拟用户对物品的偏好判断，训练嵌入模型来近似这些判断，并将其集成到传统推荐系统的信号加权中。


<details>
  <summary>Details</summary>
Motivation: 当用户对推荐系统不满意时，他们通常缺乏细粒度的控制手段来改变推荐结果。大语言模型提供了一种解决方案，允许用户通过自然语言请求来指导推荐。

Method: 在训练阶段，使用LLM模拟用户基于语言请求对物品的批准判断，训练嵌入模型来近似这些模拟判断。然后将这些基于用户请求的预测集成到传统推荐系统优化的标准信号加权中。在部署阶段，每个用户请求只需要一次LLM嵌入计算，实现实时推荐控制。

Result: 在MovieLens数据集上的实验表明，该方法在各种请求类型下都能实现细粒度控制。在19名Letterboxd用户的研究中，CTRL-Rec受到用户积极评价，与传统控制相比显著提升了用户的控制感和推荐满意度。

Conclusion: CTRL-Rec方法成功实现了通过自然语言对传统推荐系统的实时控制，增强了用户的控制体验和满意度。

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>
