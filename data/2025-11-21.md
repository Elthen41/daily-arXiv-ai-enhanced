<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization](https://arxiv.org/abs/2511.15714)
*Ariel Kamen,Yakov Kamen*

Main category: cs.AI

TL;DR: 本文提出了一个集成大语言模型(eLLM)框架，通过整合多个LLM来解决单个模型在文本分类中的不一致性、幻觉、类别膨胀和误分类等问题，在零样本条件下显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型在文本分类中存在不一致性、幻觉、类别膨胀和误分类等弱点，需要开发更可靠的分类解决方案以减少对人工标注的依赖。

Method: 采用集成学习框架，通过数学建模集体决策过程并建立原则性聚合标准，在IAB分层分类法下评估10个最先进的LLM，使用8,660个人工标注样本进行零样本测试。

Result: eLLM框架相比最强单模型在F1分数上提升了高达65%，实现了接近人类专家水平的性能，同时提高了鲁棒性和准确性。

Conclusion: eLLM框架为基于分类法的文本分类提供了可扩展且可靠的解决方案，可能显著减少对人工专家标注的依赖。

Abstract: This study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65\% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8{,}660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.

</details>


### [2] [Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems](https://arxiv.org/abs/2511.15715)
*Yash Raj Singh*

Main category: cs.AI

TL;DR: Graph-Memoized Reasoning是一个用于表示、存储和重用推理工作流的图结构内存框架，通过编码过去的决策图并通过结构和语义相似性检索，实现跨新推理任务的子图组合重用。


<details>
  <summary>Details</summary>
Motivation: 现代基于大语言模型的推理系统经常在不同任务中重复计算相似的推理步骤，浪费计算资源、增加推理延迟并限制可重现性，因此需要能够回忆和重用先前计算痕迹的持久推理机制。

Method: 引入Graph-Memoized Reasoning框架，将推理工作流表示为图结构内存，通过编码过去的决策图并通过结构和语义相似性检索，实现子图的组合重用。

Result: 提出了一个优化目标，最小化总推理成本并通过存储和生成工作流之间不一致性进行正则化，为智能系统中的效率-一致性权衡提供了理论基础。

Conclusion: 该框架为可解释、成本高效和自我改进的推理架构奠定了基础，朝着大规模智能系统中的持久内存迈出了一步。

Abstract: Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.
  We introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.
  We formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.
  This framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.

</details>


### [3] [MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding](https://arxiv.org/abs/2511.15716)
*Abraham Itzhak Weinberg*

Main category: cs.AI

TL;DR: MACIE是一个多智能体因果智能解释框架，结合结构因果模型、干预反事实和Shapley值，为多智能体强化学习系统提供全面的解释能力。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体强化学习系统在安全关键应用中的使用，理解智能体决策原因和集体行为形成机制变得至关重要。现有可解释AI方法在多智能体环境中存在不足，无法将集体结果归因于个体、量化涌现行为或捕捉复杂交互。

Method: MACIE框架结合结构因果模型、干预反事实和Shapley值，通过干预归因分数评估每个智能体的因果贡献，使用协同指标分离集体效应与个体贡献来量化系统级涌现智能，并通过自然语言叙述合成因果洞察提供可操作解释。

Result: 在四种MARL场景（合作、竞争和混合动机）中的评估显示：准确的结果归因（平均φ_i=5.07，标准差<0.05），在合作任务中检测到正涌现（协同指数高达0.461），计算效率高（CPU上每个数据集0.79秒）。

Conclusion: MACIE独特地结合了因果严谨性、涌现量化和多智能体支持，同时保持实时使用的实用性，代表了向可解释、可信赖和可问责的多智能体AI迈出的一步。

Abstract: As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.

</details>


### [4] [How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI](https://arxiv.org/abs/2511.15717)
*Bo Wen,Chen Wang,Erhan Bilal*

Main category: cs.AI

TL;DR: 该研究分析了不同模态（文本和图像）在ARC-AGI任务中对模型感知的影响，发现结构化文本能精确定位稀疏特征，图像能捕捉2D形状但对分辨率敏感，结合两者可提升执行效果。


<details>
  <summary>Details</summary>
Motivation: 当前系统在ARC-AGI任务中使用自然语言或DSL规则进行生成-执行-选择循环，但缺乏对编码如何影响模型感知以及如何区分指令错误和执行错误的系统分析。

Method: 使用加权集合分歧度量和两阶段推理流程，在九种文本和图像模态中分离感知与推理，比较不同表示方式的效果。

Result: 结构化文本在稀疏特征上提供精确坐标，图像能保持2D形状但对分辨率敏感，结合文本和图像可提升约8个感知点和0.20中位数相似度的执行效果。

Conclusion: 将表示与transformer归纳偏置对齐，并实现文本和图像之间的交叉验证，可在不改变底层模型的情况下获得更准确的指令和更可靠的执行。

Abstract: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.

</details>


### [5] [Chain of Summaries: Summarization Through Iterative Questioning](https://arxiv.org/abs/2511.15719)
*William Brach,Lukas Galke Poech*

Main category: cs.AI

TL;DR: 提出Chain of Summaries (CoS)方法，通过类似黑格尔辩证法的方式迭代生成信息密集的通用摘要，使网页内容更易于LLM消化，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM难以消化网页内容的问题，因为网页格式不友好且受限于上下文长度，需要生成信息密集的通用摘要。

Method: 采用类似黑格尔辩证法的Chain of Summaries方法：初始摘要作为论点，通过质疑识别局限性作为反论点，最终生成满足当前和未来信息需求的综合摘要。

Result: 在TriviaQA、TruthfulQA和SQUAD数据集上，CoS比零样本LLM基线提升66%，比BRIO和PEGASUS等专业摘要方法提升27%。生成的摘要用更少token获得更高问答性能。

Conclusion: CoS为网站维护者提供了一种使内容更易于LLM访问的可行方案，同时保留了人工监督的可能性，且不依赖特定下游LLM。

Abstract: Large Language Models (LLMs) are increasingly using external web content. However, much of this content is not easily digestible by LLMs due to LLM-unfriendly formats and limitations of context length. To address this issue, we propose a method for generating general-purpose, information-dense summaries that act as plain-text repositories of web content. Inspired by Hegel's dialectical method, our approach, denoted as Chain of Summaries (CoS), iteratively refines an initial summary (thesis) by identifying its limitations through questioning (antithesis), leading to a general-purpose summary (synthesis) that can satisfy current and anticipate future information needs. Experiments on the TriviaQA, TruthfulQA, and SQUAD datasets demonstrate that CoS outperforms zero-shot LLM baselines by up to 66% and specialized summarization methods such as BRIO and PEGASUS by up to 27%. CoS-generated summaries yield higher Q&A performance compared to the source content, while requiring substantially fewer tokens and being agnostic to the specific downstream LLM. CoS thus resembles an appealing option for website maintainers to make their content more accessible for LLMs, while retaining possibilities for human oversight.

</details>


### [6] [Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models](https://arxiv.org/abs/2511.15720)
*Islem Sahraoui*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态AI框架，结合文本和图像分析来识别建筑工地的安全隐患。通过两个案例研究评估了大型语言模型和视觉语言模型在自动危险识别中的能力。


<details>
  <summary>Details</summary>
Motivation: 在建筑工地等安全关键环境中，事故数据通常以多种格式存在（如书面报告、检查记录和现场图像），传统方法难以综合识别危险。

Method: 第一个案例研究使用GPT 4o和GPT 4o mini从28,000份OSHA事故报告中提取结构化见解；第二个案例研究使用轻量级开源视觉语言模型Molmo 7B和Qwen2 VL 2B，在ConstructionSite10k数据集上进行规则级安全违规检测。

Result: 尽管模型规模较小，但Molmo 7B和Qwen2 VL 2B在某些提示配置下表现出竞争力，证明了低资源多模态系统用于规则感知安全监控的可行性。

Conclusion: 多模态AI框架能够有效结合文本和视觉数据分析，为建筑安全监控提供可行的解决方案，特别是在资源受限的情况下。

Abstract: This thesis explores a multimodal AI framework for enhancing construction safety through the combined analysis of textual and visual data. In safety-critical environments such as construction sites, accident data often exists in multiple formats, such as written reports, inspection records, and site imagery, making it challenging to synthesize hazards using traditional approaches. To address this, this thesis proposed a multimodal AI framework that combines text and image analysis to assist in identifying safety hazards on construction sites. Two case studies were consucted to evaluate the capabilities of large language models (LLMs) and vision-language models (VLMs) for automated hazard identification.The first case study introduces a hybrid pipeline that utilizes GPT 4o and GPT 4o mini to extract structured insights from a dataset of 28,000 OSHA accident reports (2000-2025). The second case study extends this investigation using Molmo 7B and Qwen2 VL 2B, lightweight, open-source VLMs. Using the public ConstructionSite10k dataset, the performance of the two models was evaluated on rule-level safety violation detection using natural language prompts. This experiment served as a cost-aware benchmark against proprietary models and allowed testing at scale with ground-truth labels. Despite their smaller size, Molmo 7B and Quen2 VL 2B showed competitive performance in certain prompt configurations, reinforcing the feasibility of low-resource multimodal systems for rule-aware safety monitoring.

</details>


### [7] [Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods](https://arxiv.org/abs/2511.15722)
*Weichen Liu,Qiyao Xue,Haoming Wang,Xiangyu Yin,Boyuan Yang,Wei Gao*

Main category: cs.AI

TL;DR: 本文提出从认知角度对空间智能进行分类的新视角，将任务按推理复杂度划分并关联到认知功能，分析现有基准、评估方法和提升空间推理能力的途径。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常基于输入模态对空间推理进展进行分类，但作者认为空间能力不仅由输入格式决定，需要从认知角度建立更原则化的分类体系来揭示当前模型能力与人类推理之间的差距。

Method: 引入基于认知视角的分类法，按推理复杂度组织空间智能任务并关联认知功能；将现有基准映射到该分类法中；分析评估指标和方法；对比训练型和推理型提升方法。

Result: 建立了更原则化的跨任务比较框架，揭示了当前模型能力与人类推理之间的关键差距，澄清了不同提升方法的各自优势和互补机制。

Conclusion: 通过从认知角度分析任务、基准和最新进展，为研究者提供了对该领域的全面理解和未来研究的可行方向，推动了空间推理研究的系统化发展。

Abstract: Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.

</details>


### [8] [Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer](https://arxiv.org/abs/2511.15741)
*Hyo-Jeong Jang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于一致性引导的跨模态迁移的鲁棒多模态学习方法，通过将异构模态投影到共享潜在空间来减少模态差异，提高对噪声数据和不完整监督的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统面临数据噪声、标签质量低和模态异质性等不确定性挑战，特别是在人机交互场景中，这些问题尤为突出。

Method: 采用一致性引导的跨模态迁移框架，利用跨模态语义一致性进行鲁棒表示学习，将异构模态投影到共享潜在空间，并探索增强语义鲁棒性和数据效率的策略。

Result: 在多模态情感识别基准测试中，该方法显著提高了模型稳定性、判别能力和对噪声或不完整监督的鲁棒性，潜在空间分析表明即使在挑战性条件下也能捕捉可靠的跨模态结构。

Conclusion: 该论文通过整合不确定性建模、语义对齐和数据高效监督，为开发可靠的自适应脑机接口系统提供了统一的鲁棒多模态学习视角。

Abstract: Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.

</details>


### [9] [Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics](https://arxiv.org/abs/2511.15752)
*Hanzhi Yan,Qin Lu,Xianqiao Wang,Xiaoming Zhai,Tianming Liu,He Li*

Main category: cs.AI

TL;DR: 该论文提出使用检索增强生成(RAG)和多智能体系统(MAS)来增强大语言模型在生物力学教育任务中的表现，解决LLMs在专业领域知识不足和复杂推理问题上的局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用任务中表现出色，但在专业领域应用中因知识差距而效果下降，且在需要多步推理的复杂问题上性能衰退。作者希望开发教育助手来增强本科生在生物力学课程中的学习体验。

Method: 构建双模块框架：1) 使用RAG提高LLMs回答概念性判断题的特异性和逻辑一致性；2) 构建MAS系统来解决需要多步推理和代码执行的计算问题。评估了多个LLM模型在100个生物力学问题上的表现。

Result: RAG显著提升了LLMs在概念性问题上的性能和稳定性，超越了原始模型。MAS系统能够执行多步推理、推导方程、执行代码，并为需要计算的任务生成可解释的解决方案。

Conclusion: RAG和MAS的应用展示了增强LLMs在工程专业课程中性能的潜力，为开发工程教育智能辅导系统提供了有前景的方向。

Abstract: While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.

</details>


### [10] [Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response](https://arxiv.org/abs/2511.15755)
*Philip Drammeh*

Main category: cs.AI

TL;DR: 多智能体编排相比单智能体方法，在事故响应中实现了100%可操作推荐率，比单智能体的1.7%提高了80倍行动特异性和140倍解决方案正确性，且质量方差为零，使生产SLA承诺成为可能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在事故响应中具有潜力，但单智能体方法生成模糊、不可用的建议，需要多智能体编排来提升响应质量。

Method: 开发了MyAntFarm.ai可复现容器化框架，通过348次对照试验比较单智能体副驾驶与多智能体系统在相同事故场景下的表现。

Result: 多智能体编排实现100%可操作推荐率，单智能体仅为1.7%；在行动特异性上提高80倍，解决方案正确性提高140倍；多智能体系统在所有试验中质量方差为零。

Conclusion: 多智能体编排不是性能优化，而是基于LLM的事故响应的生产就绪要求，引入了决策质量新指标来评估有效性、特异性和正确性。

Abstract: Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.

</details>


### [11] [Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications](https://arxiv.org/abs/2511.15763)
*Raymond K. Sheh,Karen Geappen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.
  We survey the current state of AI risk assessment and management, with a focus on the supply chain of AI and risks relating to the behavior and outputs of the AI system. We then present a proposed taxonomy specifically for categorizing AI supply chain entities. This taxonomy helps stakeholders, especially those without extensive AI expertise, to "consider the right questions" and systematically inventory dependencies across their organization's AI systems. Our contribution bridges a gap between the current state of AI governance and the urgent need for actionable risk assessment and management of AI use in critical applications.

</details>


### [12] [Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights](https://arxiv.org/abs/2511.15778)
*Paulina Tworek,Miłosz Bargieł,Yousef Khan,Tomasz Pełech-Pilichowski,Marek Mikołajczyk,Roman Lewandowski,Jose Sousa*

Main category: cs.AI

TL;DR: 本文比较了基于规则的方法和大型语言模型在从波兰电子健康记录中提取医疗信息的性能，发现规则方法在准确性方面表现更好，而LLM在适应性和可扩展性方面更优，建议采用混合方法。


<details>
  <summary>Details</summary>
Motivation: 在非英语环境下，从非结构化临床文本中提取结构化医疗信息仍然是一个挑战，特别是在资源稀缺的情况下。本研究旨在比较不同NLP方法在医疗信息提取中的效果。

Method: 使用基于规则的低计算方法和大型语言模型从波兰儿童康复医院的电子健康记录中提取患者人口统计信息、临床发现和处方药物，并评估文本标准化缺失和翻译导致信息损失的影响。

Result: 结果显示基于规则的方法在信息检索任务中准确性更高，特别是在年龄和性别提取方面。而LLM在药物名称识别方面表现更好，具有更强的适应性和可扩展性。翻译对LLM效果有影响。

Conclusion: 研究强调了在医疗环境中部署NLP时准确性、标准化和计算成本之间的权衡，建议采用结合规则系统精确性和LLM适应性的混合方法，为现实医院提供更可靠和资源高效的临床NLP解决方案。

Abstract: Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.

</details>


### [13] [IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation](https://arxiv.org/abs/2511.15825)
*Tuan-Anh Le,Anh Mai Vu,David Yang,Akash Awasthi,Hien Van Nguyen*

Main category: cs.AI

TL;DR: IMACT-CXR是一个基于AutoGen的交互式多智能体对话导师系统，通过整合空间标注、视线分析、知识检索和图像推理来帮助学员解读胸部X光片。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时处理学员边界框标注、视线样本和自由文本观察的智能导师系统，以提升胸部X光片解读的培训效果。

Method: 使用多智能体架构，包括评估定位质量的智能体、生成苏格拉底式指导的智能体、检索PubMed证据的智能体、从REFLACX检索相似病例的智能体，以及触发NV-Reason-CXR-3B进行视觉语言推理的智能体。采用贝叶斯知识追踪维护技能掌握度估计，并集成肺叶分割模块进行解剖学感知的视线反馈。

Result: 系统展示了响应迅速的辅导流程、有界延迟、精确控制答案泄露以及向实时住院医师部署的可扩展性。初步评估显示与基线相比，定位和诊断推理能力有所提升。

Conclusion: IMACT-CXR成功实现了将多种技术整合到单一工作流中，为胸部X光片解读培训提供了有效的交互式辅导解决方案，并显示出在临床教育中的实际应用潜力。

Abstract: IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.

</details>


### [14] [Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions](https://arxiv.org/abs/2511.15830)
*Stéphane Aroca-Ouellette,Ian Berlot-Attwell,Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Tongqi Zhu,Herin Kang,Kaheer Suleman,Sam Pasupalak*

Main category: cs.AI

TL;DR: 该论文介绍了Mini Amusement Parks (MAPs)游乐场模拟器，用于评估智能体在复杂商业环境中的决策能力，发现人类表现显著优于现有LLM智能体。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以应对现实世界决策中的相互关联挑战，而现有的人机基准测试只关注部分能力，无法评估整体决策能力。

Method: 设计MAPs游乐场模拟器，统一评估环境建模、长期不确定性规划、复杂业务运营等能力，并提供人类基准和最先进LLM智能体的全面评估。

Result: 人类在简单模式下表现优于AI系统6.5倍，中等模式下优于9.8倍，揭示了AI在长期优化、样本高效学习、空间推理和世界建模方面的持续弱点。

Conclusion: MAPs通过在一个环境中统一这些挑战，为评估具有适应性决策能力的智能体提供了新的基准基础。

Abstract: Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs

</details>


### [15] [Step-Audio-R1 Technical Report](https://arxiv.org/abs/2511.15848)
*Fei Tian,Xiangyu Tony Zhang,Yuxin Zhang,Haoyang Zhang,Yuxin Li,Daijiao Liu,Yayue Deng,Donghang Wu,Jun Chen,Liang Zhao,Chengyuan Yao,Hexin Liu,Eng Siong Chng,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.AI

TL;DR: Step-Audio-R1是首个成功在音频领域解锁推理能力的音频推理模型，通过模态锚定推理蒸馏框架，在音频理解基准测试中超越Gemini 2.5 Pro，性能媲美Gemini 3 Pro。


<details>
  <summary>Details</summary>
Motivation: 解决音频语言模型中存在的困惑现象：推理越少性能越好，探索音频智能是否能真正从深思熟虑中受益。

Method: 提出模态锚定推理蒸馏（MGRD）框架，让模型学习生成与音频特征真正相关的推理链，而非产生脱节的思考过程。

Result: 模型在涵盖语音、环境声音和音乐的综合音频理解和推理基准测试中表现出强大的音频推理能力，超越了Gemini 2.5 Pro，性能与Gemini 3 Pro相当。

Conclusion: 推理是跨模态可转移的能力，当适当锚定时，扩展的深思熟虑可以从负担转变为音频智能的强大资产，为构建真正跨所有感官模态的多模态推理系统开辟了新途径。

Abstract: Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.

</details>


### [16] [Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs](https://arxiv.org/abs/2511.15895)
*Ivan Chulo,Ananya Joshi*

Main category: cs.AI

TL;DR: 通过分析激活导向前后LLMs的认知行为变化，发现心理理论能力提升主要依赖情感理解而非分析推理


<details>
  <summary>Details</summary>
Motivation: 理解激活导向如何改变语言模型内部机制以提升心理理论能力，揭示成功心理理论的内在认知机制

Method: 使用对比激活加法对Gemma-3-4B进行导向，通过45个认知行为的线性探针比较导向与基线模型的激活差异，在1000个BigToM前向信念场景中评估

Result: 信念归因任务准确率从32.5%提升至46.7%，提升主要由情感处理激活（情感感知+2.23，情感价值+2.20）驱动，同时抑制分析过程（质疑-0.78，聚合思维-1.59）

Conclusion: LLMs中成功的心理理论能力由情感理解而非分析推理介导

Abstract: Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\% to 46.7\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.

</details>


### [17] [Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs](https://arxiv.org/abs/2511.15921)
*Chelsea Zou,Yiheng Yao,Basant Khalil*

Main category: cs.AI

TL;DR: 本文开发了一个用于大型语言模型的自校正框架，通过细粒度不确定性信号（自评估置信度对齐和词元级熵峰值）实时检测和缓解多步推理中的幻觉问题，使用强化学习策略改善推理的准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多步推理中产生的幻觉问题，传统方法仅关注最终答案正确性，而忽略了中间推理步骤的可靠性和忠实性。

Method: 设计基于细粒度不确定性信号的检测机制，包括自评估置信度对齐和词元级熵峰值检测，构建复合奖励函数来惩罚不合理的高置信度和熵峰值，通过强化学习策略训练模型实现实时自校正。

Result: 实验表明该方法提高了最终答案准确性和推理校准度，消融研究验证了各个信号的有效贡献。

Conclusion: 提出的自校正框架通过实时检测和缓解幻觉，显著改善了大型语言模型在多步推理中的准确性和推理步骤的连贯性、忠实性。

Abstract: This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.

</details>


### [18] [CARE-RAG - Clinical Assessment and Reasoning in RAG](https://arxiv.org/abs/2511.15994)
*Deepthi Potluri,Aby Mammen Mathew,Jeffrey B DeWitt,Alexander L. Rasgon,Yide Hao,Junyuan Hong,Ying Ding*

Main category: cs.AI

TL;DR: 论文研究发现，即使为大型语言模型提供正确的证据，它们也未必能正确推理，特别是在临床环境中。作者使用书面暴露疗法指南作为测试平台，提出了一个评估框架来衡量准确性、一致性和推理保真度。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中，大型语言模型的输出必须符合结构化协议，但研究发现即使提供权威证据，模型推理错误仍然存在，这凸显了检索与推理之间的差距。

Method: 使用书面暴露疗法指南作为测试平台，评估模型对经过临床医生审核的问题的响应，并提出一个评估框架来衡量准确性、一致性和推理保真度。

Result: 检索增强生成可以约束输出，但安全部署需要像严格评估检索一样严格评估推理。即使提供权威段落，错误仍然存在。

Conclusion: 在临床环境中安全部署大型语言模型需要同时严格评估检索和推理能力，仅提供正确证据不足以确保正确推理。

Abstract: Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.

</details>


### [19] [SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model](https://arxiv.org/abs/2511.16018)
*Emanuel C. Silva,Emily S. M. Salum,Gabriel M. Arantes,Matheus P. Pereira,Vinicius F. Oliveira,Alessandro L. Bicho*

Main category: cs.AI

TL;DR: SpellForger是一个让玩家通过自然语言提示创建自定义法术的游戏，使用BERT模型解释玩家输入并生成平衡的法术参数，验证AI作为核心游戏机制的应用。


<details>
  <summary>Details</summary>
Motivation: 探索AI作为游戏核心共创工具的潜力，目前这方面研究不足。旨在为玩家提供个性化和创造性的独特游戏体验。

Method: 使用监督训练的BERT模型解释自然语言提示，将文本描述映射到预设法术模板并平衡参数（伤害、成本、效果）。游戏在Unity引擎开发，AI后端使用Python。

Result: 开发出功能原型，能够实时生成法术并应用于引人入胜的游戏循环中，使玩家创造力成为体验的核心。

Conclusion: 成功验证了AI作为直接游戏机制的可行性，展示了自然语言驱动的法术创建系统能够提供独特的个性化游戏体验。

Abstract: Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic.

</details>


### [20] [An Aligned Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size](https://arxiv.org/abs/2511.16045)
*Jorge A. Huertas,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: 提出了一种新的约束规划模型用于串行批处理调度问题，该模型不使用预定义的虚拟批次集合，而是通过关键对齐参数直接处理同族作业序列，实现了更紧凑的公式化表达。


<details>
  <summary>Details</summary>
Motivation: 现有约束规划模型依赖预定义的虚拟批次集合，这会导致维度灾难并增加问题复杂度，特别是在考虑最小批次大小的实际应用中。

Method: 开发了不使用虚拟批次集合的新CP模型，利用关键对齐参数直接处理同族作业序列，并通过定制搜索阶段和增强约束传播器的推理级别来改进模型。

Result: 在近5000个实例上的计算实验表明，新模型在中小规模实例（最多100个作业）上表现优越，在大规模实例（最多500个作业、10个族、10台机器）上能找到比现有方法好25%的解决方案。

Conclusion: 提出的新CP模型在串行批处理调度问题中显著优于现有方法，特别是在考虑最小批次大小的实际应用场景中。

Abstract: In serial batch (s-batch) scheduling, jobs from similar families are grouped into batches and processed sequentially to avoid repetitive setups that are required when processing consecutive jobs of different families. Despite its large success in scheduling, only three Constraint Programming (CP) models have been proposed for this problem considering minimum batch sizes, which is a common requirement in many practical settings, including the ion implantation area in semiconductor manufacturing. These existing CP models rely on a predefined virtual set of possible batches that suffers from the curse of dimensionality and adds complexity to the problem. This paper proposes a novel CP model that does not rely on this virtual set. Instead, it uses key alignment parameters that allow it to reason directly on the sequences of same-family jobs scheduled on the machines, resulting in a more compact formulation. This new model is further improved by exploiting the problem's structure with tailored search phases and strengthened inference levels of the constraint propagators. The extensive computational experiments on nearly five thousand instances compare the proposed models against existing methods in the literature, including mixed-integer programming formulations, tabu search meta-heuristics, and CP approaches. The results demonstrate the superiority of the proposed models on small-to-medium instances with up to 100 jobs, and their ability to find solutions up to 25\% better than the ones produces by existing methods on large-scale instances with up to 500 jobs, 10 families, and 10 machines.

</details>


### [21] [Artificial Intelligence and Accounting Research: A Framework and Agenda](https://arxiv.org/abs/2511.16055)
*Theophanis C. Stratopoulos,Victor Xiaoqi Wang*

Main category: cs.AI

TL;DR: 本文提出了一个框架来分类AI-会计研究，分析会计学者如何通过战略定位和合作利用专业知识，并探讨生成式AI和大型语言模型如何改变研究过程本身。


<details>
  <summary>Details</summary>
Motivation: 人工智能特别是生成式AI和大型语言模型的快速发展正在根本性地改变会计研究，为学者们创造了机遇和竞争威胁。

Method: 提出了一个二维分类框架：研究焦点（会计中心vs AI中心）和方法论方法（基于AI vs 传统方法），并将该框架应用于IJAIS特刊和主要会计期刊的AI-会计研究论文。

Result: 分析显示，虽然生成式AI民主化了某些研究能力，但通过提高对高阶贡献的期望加剧了竞争，而人类判断、创造力和理论深度在这些领域仍然有价值。

Conclusion: 这些转变要求改革博士教育，在培养AI流畅度的同时发展比较优势。

Abstract: Recent advances in artificial intelligence, particularly generative AI (GenAI) and large language models (LLMs), are fundamentally transforming accounting research, creating both opportunities and competitive threats for scholars. This paper proposes a framework that classifies AI-accounting research along two dimensions: research focus (accounting-centric versus AI-centric) and methodological approach (AI-based versus traditional methods). We apply this framework to papers from the IJAIS special issue and recent AI-accounting research published in leading accounting journals to map existing studies and identify research opportunities. Using this same framework, we analyze how accounting researchers can leverage their expertise through strategic positioning and collaboration, revealing where accounting scholars' strengths create the most value. We further examine how GenAI and LLMs transform the research process itself, comparing the capabilities of human researchers and AI agents across the entire research workflow. This analysis reveals that while GenAI democratizes certain research capabilities, it simultaneously intensifies competition by raising expectations for higher-order contributions where human judgment, creativity, and theoretical depth remain valuable. These shifts call for reforming doctoral education to cultivate comparative advantages while building AI fluency.

</details>


### [22] [A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management](https://arxiv.org/abs/2511.16075)
*Hrikshesh Kumar,Anika Garg,Anshul Gupta,Yashika Agarwal*

Main category: cs.AI

TL;DR: 提出了一种结合CNN-LSTM时间序列预测和多智能体深度强化学习的混合架构，用于云边缘工作负载的主动资源管理，通过将预测信息嵌入DRL状态空间实现前瞻性决策。


<details>
  <summary>Details</summary>
Motivation: 传统云边缘工作负载资源管理过于被动，依赖静态阈值导致要么资源过度配置造成浪费，要么资源不足影响性能，因此需要转向主动解决方案。

Method: 设计混合架构，结合CNN-LSTM模型进行时间序列预测和多智能体深度强化学习编排器，将预测结果直接嵌入DRL智能体的状态空间。

Result: 测试表明该系统明显优于传统方法，能够有效解决复杂决策问题，同时平衡成本节约、系统健康和用户体验等多个目标。

Conclusion: 通过赋予AI管理器预见未来的能力，实现了在成本节约和系统性能之间找到最佳平衡点，避免了被动应对问题，找到了平滑的前进路径。

Abstract: Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable

</details>


### [23] [SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent](https://arxiv.org/abs/2511.16108)
*Shiyi Cao,Dacheng Li,Fangzhou Zhao,Shuo Yuan,Sumanth R. Hegde,Connor Chen,Charlie Ruan,Tyler Griggs,Shu Liu,Eric Tang,Richard Liaw,Philipp Moritz,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: SkyRL-Agent是一个用于高效多轮长视野智能体训练和评估的框架，通过异步调度、轻量级工具集成和灵活后端互操作性，显著提升了训练效率。基于该框架训练的SA-SWE-32B软件工程智能体在SWE-Bench Verified上达到39.4% Pass@1，成本降低2倍以上，并能有效泛化到其他智能体任务。


<details>
  <summary>Details</summary>
Motivation: 为了解决软件工程智能体训练中的效率问题，开发一个能够支持高效多轮长视野训练、降低训练成本并提高性能的框架。

Method: 提出SkyRL-Agent框架，包含优化的异步管道调度器（1.55倍加速）和基于AST的搜索工具增强训练方法，支持与现有RL框架的无缝集成。

Result: SA-SWE-32B在SWE-Bench Verified上达到39.4% Pass@1，相比之前模型成本降低2倍以上，并能有效泛化到Terminal-Bench、BrowseComp-Plus和WebArena等其他智能体任务。

Conclusion: SkyRL-Agent框架通过高效异步调度和工具增强训练，显著提升了软件工程智能体的训练效率和性能，同时具有良好的泛化能力和扩展性。

Abstract: We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.
  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.

</details>


### [24] [Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints](https://arxiv.org/abs/2511.16139)
*Yongnan Jin,Xurui Li,Feng Cao,Liucun Gao,Juanjuan Yao*

Main category: cs.AI

TL;DR: MR-RML框架通过多维评分导向的奖励模型学习和几何投影参考约束，解决了LLMs在医疗应用中的对齐挑战，显著提升了医疗基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在医疗实践中面临的关键对齐挑战：静态评估基准与动态临床认知需求脱节、难以适应不断演变的医疗标准、传统奖励模型无法捕捉多维度医疗质量标准。

Method: 提出MR-RML框架，包含三个核心创新：1）"维度-场景-学科"医疗标准系统；2）独立多维奖励模型；3）几何投影参考约束，将医疗认知逻辑转化为数学正则化。

Result: 在权威医疗基准Healthbench上，相比基础模型Qwen-32B，在完整子集上提升45%，在困难子集上提升85%，达到开源LLMs中的SOTA水平（完整子集62.7分，困难子集44.7分）。

Conclusion: MR-RML框架有效解决了LLMs在医疗领域的对齐问题，显著提升了临床实用性，为医疗AI的发展提供了新的技术路径。

Abstract: The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.

</details>


### [25] [FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos](https://arxiv.org/abs/2511.16183)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.AI

TL;DR: 本文介绍了FOOTPASS数据集，这是首个在足球比赛中进行多模态、多智能体战术背景下的逐场动作定位基准，旨在通过结合计算机视觉输出和足球战术知识来生成可靠的逐场数据流。


<details>
  <summary>Details</summary>
Motivation: 当前足球视频理解方法在构建可靠的逐场数据方面仍不足，通常只能辅助而非完全自动化标注。同时战术建模、轨迹预测和性能分析等研究都需要基于比赛状态和逐场数据，这促使利用战术知识作为先验来支持基于计算机视觉的预测。

Method: 引入FOOTPASS数据集，支持开发利用计算机视觉任务输出（如跟踪、识别）和足球先验知识（包括长期战术规律）的球员中心动作定位方法。

Result: 创建了首个在完整足球比赛中进行多模态、多智能体战术背景下的逐场动作定位基准数据集。

Conclusion: FOOTPASS数据集能够生成可靠的逐场数据流，这些数据流是数据驱动体育分析的重要输入，有助于实现更自动化和可靠的逐场数据提取。

Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.

</details>


### [26] [From Performance to Understanding: A Vision for Explainable Automated Algorithm Design](https://arxiv.org/abs/2511.16201)
*Niki van Stein,Anna V. Kononova,Thomas Bäck*

Main category: cs.AI

TL;DR: 本文提出可解释的自动化算法设计愿景，通过结合LLM驱动的算法发现、可解释基准测试和问题类别描述符，形成知识闭环，从盲目搜索转向可解释的类别特定算法设计。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动化算法设计方法虽然能生成优化算法，但缺乏对算法为何有效、哪些组件重要以及设计选择与问题结构关系的理解，需要将自动化与系统基准测试的理解相结合。

Method: 基于三个支柱：LLM驱动的算法变体发现、可解释基准测试（将性能归因于组件和超参数）、问题类别描述符（连接算法行为与问题结构）。

Result: 形成发现、解释和泛化的知识闭环，能够从盲目搜索转向可解释的类别特定算法设计。

Conclusion: 这种集成将加速进展，同时产生关于优化策略何时以及为何成功的可重用科学见解，推动领域从性能驱动转向理解驱动的算法设计。

Abstract: Automated algorithm design is entering a new phase: Large Language Models can now generate full optimisation (meta)heuristics, explore vast design spaces and adapt through iterative feedback. Yet this rapid progress is largely performance-driven and opaque. Current LLM-based approaches rarely reveal why a generated algorithm works, which components matter or how design choices relate to underlying problem structures. This paper argues that the next breakthrough will come not from more automation, but from coupling automation with understanding from systematic benchmarking. We outline a vision for explainable automated algorithm design, built on three pillars: (i) LLM-driven discovery of algorithmic variants, (ii) explainable benchmarking that attributes performance to components and hyperparameters and (iii) problem-class descriptors that connect algorithm behaviour to landscape structure. Together, these elements form a closed knowledge loop in which discovery, explanation and generalisation reinforce each other. We argue that this integration will shift the field from blind search to interpretable, class-specific algorithm design, accelerating progress while producing reusable scientific insight into when and why optimisation strategies succeed.

</details>


### [27] [Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning](https://arxiv.org/abs/2511.16202)
*Pei Yang,Ke Zhang,Ji Wang,Xiao Chen,Yuxin Tang,Eric Yang,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: CRM是一个多智能体协作奖励模型框架，用专业评估者团队替代单一黑盒奖励模型，提高RLHF的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型难以同时优化多个可能冲突的偏好维度（如事实性、帮助性、安全性），且评分透明度有限。

Method: 将偏好评估分解为领域特定的智能体，每个产生部分信号，结合全局评估器；中央聚合器融合这些信号，平衡步进正确性、多智能体一致性和重复惩罚等因素。

Result: CRM与rewardBench基准套件一起，为更透明的奖励建模和更稳定的优化提供了实用的模块化路径。

Conclusion: 该框架在不需额外人工标注的情况下，实现了多视角奖励塑造，与标准RL流程兼容。

Abstract: We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.

</details>


### [28] [ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025](https://arxiv.org/abs/2511.16205)
*Xu Qiang,Shengyuan Bai,Leqing Chen,Zijing Liu,Yu Li*

Main category: cs.AI

TL;DR: ChemO是一个基于2025年国际化学奥林匹克竞赛的新基准测试，通过评估等效重构和结构化视觉增强解决化学问题自动评估的挑战，结合ChemLabs多智能体框架实现了超越人类金牌水平的性能。


<details>
  <summary>Details</summary>
Motivation: 数学和物理的奥林匹克级基准测试已用于评估AI推理能力，但化学因其独特的多模态符号语言一直是一个开放挑战，需要专门基准来测试AI的化学推理能力。

Method: 提出了ChemO基准测试，包含评估等效重构将视觉输出问题转换为可计算格式，结构化视觉增强分离视觉感知与化学推理能力，以及ChemLabs分层多智能体框架模拟人类专家协作。

Result: 在先进多模态模型上的实验表明，结合结构化视觉增强和多智能体系统带来显著性能提升，最佳配置达到93.6/100分，超越估计的人类金牌阈值。

Conclusion: ChemO基准和ChemLabs框架为化学问题自动解决建立了新的最先进水平，展示了多智能体协作在复杂科学推理任务中的有效性。

Abstract: Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO

</details>


### [29] [FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks](https://arxiv.org/abs/2511.16216)
*Zhen Hao Wong,Jingwen Deng,Hao Liang,Runming He,Chengyu Shen,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出自动化管道从教育文档中提取问答对和视觉问答对，结合布局感知OCR和基于LLM的语义解析，为LLM训练提供高质量监督数据。


<details>
  <summary>Details</summary>
Motivation: 现有指令调优和强化学习数据集成本高且依赖合成样本导致幻觉和多样性有限，而教材和练习材料包含大量高质量人工编写的问答内容未被充分利用。

Method: 结合布局感知OCR和基于大语言模型的语义解析，自动化提取教育文档中的问答对和视觉问答对。

Result: 实验表明该方法能生成准确、对齐且低噪声的问答对/视觉问答对。

Conclusion: 该方法能够规模化利用真实世界教育内容，为改进面向推理的LLM训练提供了实用的合成数据生成替代方案。

Abstract: The development of Large Language Models (LLMs) increasingly depends on high-quality supervised data, yet existing instruction-tuning and RL datasets remain costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity. At the same time, textbooks and exercise materials contain abundant, high-quality human-authored Question-Answer(QA) content that remains underexploited due to the difficulty of transforming raw PDFs into AI-ready supervision. Although modern OCR and vision-language models can accurately parse document structure, their outputs lack the semantic alignment required for training. We propose an automated pipeline that extracts well-formed QA and visual-QA (VQA) pairs from educational documents by combining layout-aware OCR with LLM-based semantic parsing. Experiments across diverse document types show that the method produces accurate, aligned, and low-noise QA/VQA pairs. This approach enables scalable use of real-world educational content and provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training. All code and data-processing pipelines are open-sourced at https://github.com/OpenDCAI/DataFlow.

</details>


### [30] [MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering](https://arxiv.org/abs/2511.16283)
*Zhiyuan Li,Haisheng Yu,Guangchuan Guo,Nan Zhou,Jiajun Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一个多意图科学问答基准(MuISQA)和意图感知检索框架，用于解决传统RAG系统在复杂科学问题中证据覆盖不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 复杂科学问题通常包含多个意图，需要从不同来源获取证据并进行多跳推理，而传统RAG系统通常是单一意图导向的，导致证据覆盖不完整。

Method: 提出意图感知检索框架，利用LLM假设潜在答案，将其分解为意图特定查询，并为每个底层意图检索支持段落，然后通过互惠排名融合(RRF)聚合和重新排序检索到的片段。

Result: 在MuISQA基准和其他通用RAG数据集上的实验表明，该方法在检索准确性和证据覆盖方面持续优于传统方法。

Conclusion: 该方法能够平衡不同意图的覆盖范围，同时减少冗余，有效解决了复杂科学问题中的多意图证据检索问题。

Abstract: Complex scientific questions often entail multiple intents, such as identifying gene mutations and linking them to related diseases. These tasks require evidence from diverse sources and multi-hop reasoning, while conventional retrieval-augmented generation (RAG) systems are usually single-intent oriented, leading to incomplete evidence coverage. To assess this limitation, we introduce the Multi-Intent Scientific Question Answering (MuISQA) benchmark, which is designed to evaluate RAG systems on heterogeneous evidence coverage across sub-questions. In addition, we propose an intent-aware retrieval framework that leverages large language models (LLMs) to hypothesize potential answers, decompose them into intent-specific queries, and retrieve supporting passages for each underlying intent. The retrieved fragments are then aggregated and re-ranked via Reciprocal Rank Fusion (RRF) to balance coverage across diverse intents while reducing redundancy. Experiments on both MuISQA benchmark and other general RAG datasets demonstrate that our method consistently outperforms conventional approaches, particularly in retrieval accuracy and evidence coverage.

</details>


### [31] [OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe](https://arxiv.org/abs/2511.16334)
*Kaichen Zhang,Keming Wu,Zuhao Yang,Kairui Hu,Bin Wang,Ziwei Liu,Xingxuan Li,Lidong Bing*

Main category: cs.AI

TL;DR: OpenMMReasoner是一个完全透明的两阶段多模态推理训练方法，包含监督微调（SFT）和强化学习（RL）阶段，在九个多模态推理基准上比Qwen2.5-VL-7B-Instruct基线提升11.6%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理领域缺乏透明和可复现的数据构建与训练策略，阻碍了可扩展研究的发展。

Method: 采用两阶段训练方法：SFT阶段构建87.4万样本的冷启动数据集并进行严格验证；RL阶段使用7.4万样本数据集进一步优化和稳定推理能力。

Result: 方法在九个多模态推理基准上显著超越强基线，证明了数据质量和训练设计对多模态推理性能的关键作用。

Conclusion: 该工作为未来大规模多模态推理研究建立了坚实的实证基础，并开源了所有代码、流程和数据。

Abstract: Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.

</details>


### [32] [Reducing Instability in Synthetic Data Evaluation with a Super-Metric in MalDataGen](https://arxiv.org/abs/2511.16373)
*Anna Luiza Gomes da Silva,Diego Kreutz,Angelo Diniz,Rodrigo Mansilha,Celso Nobre da Fonseca*

Main category: cs.AI

TL;DR: 本文提出了一个Super-Metric来评估Android恶意软件合成数据的质量，该指标聚合了八个度量标准，通过实验证明比传统指标更稳定和一致。


<details>
  <summary>Details</summary>
Motivation: 由于现有度量标准的不稳定性和缺乏标准化，评估Android恶意软件合成数据质量存在持续挑战。

Method: 在MalDataGen中集成Super-Metric，该指标在四个保真度维度上聚合八个度量标准，生成单一加权分数。

Result: 涉及十个生成模型和五个平衡数据集的实验表明，Super-Metric比传统指标更稳定和一致，与分类器实际性能的相关性更强。

Conclusion: Super-Metric为Android恶意软件合成数据质量评估提供了更可靠和标准化的解决方案。

Abstract: Evaluating the quality of synthetic data remains a persistent challenge in the Android malware domain due to instability and the lack of standardization among existing metrics. This work integrates into MalDataGen a Super-Metric that aggregates eight metrics across four fidelity dimensions, producing a single weighted score. Experiments involving ten generative models and five balanced datasets demonstrate that the Super-Metric is more stable and consistent than traditional metrics, exhibiting stronger correlations with the actual performance of classifiers.

</details>


### [33] [Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report](https://arxiv.org/abs/2511.16417)
*Yan Chen,Yu Zou,Jialei Zeng,Haoran You,Xiaorui Zhou,Aixi Zhong*

Main category: cs.AI

TL;DR: Pharos-ESG是一个统一框架，通过多模态解析、上下文叙述和分层标注将ESG报告转换为结构化表示，解决了ESG报告因不规则布局和弱结构化内容带来的理解挑战。


<details>
  <summary>Details</summary>
Motivation: ESG报告作为评估企业ESG表现的核心媒介，由于类似幻灯片的非规则布局导致的混乱阅读顺序以及冗长、弱结构化内容产生的隐含层次结构，给大规模理解带来了重大挑战。

Method: 该框架集成了基于布局流的阅读顺序建模模块、由目录锚点引导的层次感知分割模块，以及将视觉元素上下文转换为连贯自然语言的多模态聚合管道，并进一步用ESG、GRI和情感标签丰富其输出。

Result: 在注释基准上的广泛实验表明，Pharos-ESG始终优于专用文档解析系统和通用多模态模型。同时发布了Aurora-ESG，这是首个大规模公开ESG报告数据集。

Conclusion: Pharos-ESG框架能够有效解决ESG报告的结构化理解问题，为金融治理和决策中的ESG整合提供了更好的支持。

Abstract: Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.

</details>


### [34] [TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models](https://arxiv.org/abs/2511.16423)
*Li Zhang,Zhongxuan Han,XiaoHua Feng,Jiaming Zhang,Yuyuan Li,Linbo Jiang,Jianan Lin,Chaochao Chen*

Main category: cs.AI

TL;DR: 提出了一种名为TOFA的无训练一次性联邦视觉语言模型适应框架，通过视觉和文本双管道提取任务相关表示，无需额外训练资源，有效处理数据异构性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦视觉语言模型适应方法存在通信成本高、易受攻击的问题，而当前一次性方法在利用多模态信息、处理数据异构性和资源需求方面存在不足。

Method: 使用视觉和文本双管道：视觉管道通过分层贝叶斯模型学习个性化类别原型分布；文本管道评估并全局对齐本地文本提示；引入自适应权重校准机制平衡个性化和鲁棒性。

Result: 在9个数据集上的广泛实验表明，TOFA方法在各种联邦设置下均表现出有效性。

Conclusion: TOFA是一种高效、轻量级的无训练一次性联邦适应框架，能够充分利用预训练视觉语言模型的多模态特征，有效解决数据异构性问题。

Abstract: Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.

</details>


### [35] [From generative AI to the brain: five takeaways](https://arxiv.org/abs/2511.16432)
*Claudius Gros*

Main category: cs.AI

TL;DR: 论文探讨生成式AI的明确生成原则可能在大脑中同样适用，并分析了机器学习研究对神经信息处理系统的五个重要特征，指出神经科学可以从ML研究中学习。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI的成功原则是否适用于大脑认知过程，探索机器学习研究对神经科学的潜在启示。

Method: 通过讨论五个具体例子：世界建模的缺陷、思维过程生成、注意力机制、神经缩放定律和量化，分析ML研究对神经信息处理系统的特征描述。

Result: 识别出生成式AI的明确生成原则可能在大脑中同样有效，ML研究提供了对神经信息处理系统的深刻见解。

Conclusion: 神经科学应该系统性地研究机器学习中的生成原则和特征，这些可能对理解大脑认知功能具有重要意义。

Abstract: The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.

</details>


### [36] [PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring](https://arxiv.org/abs/2511.16445)
*Joy Lai,Alex Mihailidis*

Main category: cs.AI

TL;DR: PersonaDrift是一个合成基准测试，用于评估检测痴呆症患者日常沟通中渐进变化的机器学习方法，重点关注情感扁平化和语义漂移两种变化模式。


<details>
  <summary>Details</summary>
Motivation: 痴呆症患者的沟通方式会随时间发生渐进变化，但现有计算工具缺乏跟踪这种行为漂移的能力。需要开发能够检测日常沟通中渐进变化的方法。

Method: 基于护理人员访谈创建合成用户模型，模拟60天的交互日志。使用多种异常检测方法：无监督统计方法（CUSUM、EWMA、One-Class SVM）、基于上下文嵌入的序列模型（GRU + BERT）以及监督分类器。

Result: 情感扁平化在基线变异性低的用户中可通过简单统计模型检测，而语义漂移检测需要时序建模和个性化基线。个性化分类器在所有任务中始终优于通用分类器。

Conclusion: 个性化方法对于检测痴呆症患者沟通行为变化至关重要，不同变化模式需要不同的检测策略，情感扁平化相对容易检测而语义漂移更具挑战性。

Abstract: People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.

</details>


### [37] [Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes](https://arxiv.org/abs/2511.16548)
*Guanchen Wu,Yuzhang Xie,Huanwei Wu,Zhe He,Hui Shao,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: CLOZE是一个使用大型语言模型从临床笔记中自动提取医学实体并整合到层次化医学本体中的零样本框架，无需额外训练或标注数据，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 临床笔记作为富含详细患者观察信息的非结构化文档，是本体扩展的宝贵但未充分利用的来源，直接利用临床笔记进行本体扩展的研究仍较少。

Method: 利用预训练大型语言模型的强大语言理解和广泛生物医学知识，CLOZE有效识别疾病相关概念并捕捉复杂的层次关系，通过自动移除受保护健康信息确保患者隐私。

Result: 实验结果表明CLOZE提供了一个准确、可扩展且保护隐私的本体扩展框架。

Conclusion: CLOZE具有支持生物医学研究和临床信息学中广泛下游应用的强大潜力。

Abstract: Integrating novel medical concepts and relationships into existing ontologies can significantly enhance their coverage and utility for both biomedical research and clinical applications. Clinical notes, as unstructured documents rich with detailed patient observations, offer valuable context-specific insights and represent a promising yet underutilized source for ontology extension. Despite this potential, directly leveraging clinical notes for ontology extension remains largely unexplored. To address this gap, we propose CLOZE, a novel framework that uses large language models (LLMs) to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies. By capitalizing on the strong language understanding and extensive biomedical knowledge of pre-trained LLMs, CLOZE effectively identifies disease-related concepts and captures complex hierarchical relationships. The zero-shot framework requires no additional training or labeled data, making it a cost-efficient solution. Furthermore, CLOZE ensures patient privacy through automated removal of protected health information (PHI). Experimental results demonstrate that CLOZE provides an accurate, scalable, and privacy-preserving ontology extension framework, with strong potential to support a wide range of downstream applications in biomedical research and clinical informatics.

</details>


### [38] [Consciousness in Artificial Intelligence? A Framework for Classifying Objections and Constraints](https://arxiv.org/abs/2511.16582)
*Andres Campero,Derek Shiller,Jaan Aru,Jonathan Simon*

Main category: cs.AI

TL;DR: 本文开发了一个分类框架，用于识别数字人工智能系统意识可能性的挑战类型，区分挑战的粒度层级和力度程度，并应用于14个文献案例。


<details>
  <summary>Details</summary>
Motivation: 为数字人工智能系统意识可能性的辩论提供结构化的分析工具，澄清不同挑战的层次和力度，避免混淆计算功能主义与数字意识的挑战。

Method: 提出基于Marr层级的分类框架，将挑战按粒度分为不同层级，按力度分为三个等级：对计算功能主义的挑战、实践性挑战和严格不可能性论证。

Result: 成功应用该框架分析了14个来自科学和哲学文献的典型案例，清晰区分了不同类型的挑战。

Conclusion: 该框架为数字意识辩论提供了有价值的分析工具，能够有效澄清和分类各种挑战主张，促进更精确的讨论。

Abstract: We develop a taxonomical framework for classifying challenges to the possibility of consciousness in digital artificial intelligence systems. This framework allows us to identify the level of granularity at which a given challenge is intended (the levels we propose correspond to Marr's levels) and to disambiguate its degree of force: is it a challenge to computational functionalism that leaves the possibility of digital consciousness open (degree 1), a practical challenge to digital consciousness that suggests improbability without claiming impossibility (degree 2), or an argument claiming that digital consciousness is strictly impossible (degree 3)? We apply this framework to 14 prominent examples from the scientific and philosophical literature. Our aim is not to take a side in the debate, but to provide structure and a tool for disambiguating between challenges to computational functionalism and challenges to digital consciousness, as well as between different ways of parsing such challenges.

</details>


### [39] [You Only Forward Once: An Efficient Compositional Judging Paradigm](https://arxiv.org/abs/2511.16600)
*Tianlong Zhang,Hongwei Xue,Shilin Yan,Di Wu,Chen Xu,Yunyun Yang*

Main category: cs.AI

TL;DR: YOFO是一种基于模板的多模态大语言模型评判方法，通过在单次前向传播中验证结构化需求，实现高速、可解释的评判，解决了传统方法在生成性和效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评判方法面临基本权衡：适应输出单一分数与MLLM生成性质不匹配且限制细粒度需求理解，而自回归生成评判分析在高吞吐量场景下速度过慢。

Method: 提出YOFO方法，基于自回归模型接受结构化需求模板，在单次推理步骤中通过读取每个需求关联的最终token的logits，为每个需求生成二元是/否决策。

Result: 实验显示YOFO在标准推荐数据集上达到最先进结果，支持依赖感知分析（后续判断基于先前判断），并能从事后思维链中获益。

Conclusion: YOFO设计实现了数量级的速度提升，同时保持可解释性，有效解决了MLLM评判中的效率与精度权衡问题。

Abstract: Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis-where subsequent judgments are conditioned on previous ones-and further benefits from post-hoc CoT.

</details>


### [40] [Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization](https://arxiv.org/abs/2511.16602)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Yingji Zhang,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Haozhe Shan,Junbo Qi,Yan Bai,Dengjie Li,Jiachen Luo,Yidong Wang,Yong Dai,Zenglin Xu,Bin Shen,Qifan Wang,Jian Tang,Xiaozhu Ju*

Main category: cs.AI

TL;DR: DPPO是一个元认知训练框架，通过动态交替监督微调和强化学习来解决具身智能中的数据瓶颈和算法效率问题，在有限数据下实现自动弱点识别和针对性资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能系统中的两个主要挑战：真实世界数据稀缺昂贵的数据瓶颈，以及现有方法资源消耗大的算法效率问题。

Method: 引入DPPO（Deliberate Practice Policy Optimization）元认知训练框架，动态交替进行监督微调（能力扩展）和强化学习（技能精炼），实现自动弱点识别和针对性资源分配。

Result: 使用DPPO训练的Pelican-VL 1.0模型相比基础模型性能提升20.3%，在100B参数规模上超越开源模型10.6%。

Conclusion: DPPO提供了首个系统性框架，缓解了数据和资源瓶颈，使社区能够高效构建多功能具身智能体，相关模型和代码已开源。

Abstract: Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.

</details>


### [41] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: MedBayes-Lite是一种轻量级贝叶斯增强框架，用于基于transformer的临床语言模型，旨在产生可靠、不确定性感知的预测。该框架无需重新训练或架构修改，仅增加不到3%的参数开销，就能显著改善模型校准和可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管transformer在临床决策支持中表现出强大潜力，但它们容易过度自信，特别是在需要校准不确定性的模糊医疗案例中。临床环境中可靠的不确定性量化对于风险最小化至关重要。

Method: MedBayes-Lite集成三个组件：(i) 使用蒙特卡洛dropout进行贝叶斯嵌入校准以获取认知不确定性；(ii) 不确定性加权注意力，对token可靠性进行边缘化；(iii) 受临床风险最小化启发的置信度引导决策塑造。

Result: 在生物医学问答和临床预测基准测试（MedQA、PubMedQA、MIMIC-III）中，MedBayes-Lite持续改善校准和可信度，将过度自信降低32-48%。在模拟临床环境中，通过标记不确定预测供人工审查，可防止高达41%的诊断错误。

Conclusion: MedBayes-Lite有效实现了医疗AI系统中可靠的不确定性传播，并提高了可解释性，为临床决策支持提供了实用的不确定性量化解决方案。

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


### [42] [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](https://arxiv.org/abs/2511.16657)
*Juan C. King,Jose M. Amigo*

Main category: cs.AI

TL;DR: 本文实现了一个基于人工智能的算法交易系统，专门用于EUR-USD货币对的高频外汇交易，整合了基本面和技术面特征，并比较了这两类特征的预测能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在外汇市场高频环境中有效预测EUR-USD价格走势的AI交易系统，探索基本面和技术面分析在算法交易中的相对重要性。

Method: 整合欧元区和美国的关键宏观经济变量（如GDP、失业率）作为基本面特征，结合技术指标（指标、振荡器、斐波那契水平、价格背离）作为技术面特征，构建AI交易算法，并通过机器学习指标和回测模拟评估性能。

Result: 使用标准机器学习指标量化预测准确性，通过历史数据回测评估交易盈利能力和风险，得出算法在EUR-USD高频交易中的表现结果。

Conclusion: 通过比较分析确定基本面和技术面特征中哪一类对生成盈利交易信号具有更强和更可靠的预测能力。

Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.

</details>


### [43] [Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660)
*Priyanka Kargupta,Shuyue Stella Li,Haocheng Wang,Jinu Lee,Shan Chen,Orevaoghene Ahia,Dean Light,Thomas L. Griffiths,Max Kleiman-Weiner,Jiawei Han,Asli Celikyilmaz,Yulia Tsvetkov*

Main category: cs.AI

TL;DR: 该研究提出了一个包含28个认知元素的分类法，分析了17个模型和人类在推理过程中的行为差异，发现人类使用层次化嵌套和元认知监控，而模型依赖浅层前向链式推理。研究还开发了测试时推理指导方法，将复杂问题性能提升高达60%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能解决复杂问题但在简单变体上失败，表明它们通过不同于人类推理的机制获得正确输出。研究旨在建立认知科学与LLM研究的桥梁，开发基于原则性认知机制而非脆弱伪推理捷径的模型。

Method: 构建包含28个认知元素的分类法，分析170K个模型推理痕迹和54个人类出声思考痕迹，进行元分析1598篇LLM推理论文，开发测试时推理指导方法。

Result: 发现人类和模型在推理结构上存在系统性差异，模型拥有与成功相关的行为库但不会自发部署。测试时推理指导方法将复杂问题性能提升高达60%。

Conclusion: 通过连接认知科学和LLM研究，为开发基于原则性认知机制而非伪推理捷径的模型奠定了基础，为改进模型能力和大规模测试人类认知理论开辟了新方向。

Abstract: Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [44] [A Detailed Comparative Analysis of Blockchain Consensus Mechanisms](https://arxiv.org/abs/2511.15730)
*Kaeli Andrews,Linh Ngo,Md Amiruzzaman*

Main category: cs.CR

TL;DR: 本文对PoW和PoS两种主流区块链共识机制进行了全面比较分析，评估了能源使用、安全性、交易速度等七个关键指标，发现PoW安全性强但能耗高，PoS效率高但存在验证者中心化风险，建议混合设计结合两者优势。


<details>
  <summary>Details</summary>
Motivation: 旨在为未来区块链基础设施开发提供参考，在去中心化、性能和生态责任之间找到平衡点。

Method: 利用近期学术研究和真实区块链数据，对PoW和PoS在七个关键指标上进行系统性比较分析。

Result: PoW提供稳健的安全性但能耗高、吞吐量慢且存在矿池中心化；PoS具有更好的可扩展性和效率，环境影响显著降低，交易费用更稳定，但存在验证者中心化和长期安全成熟度问题。

Conclusion: 两种机制存在固有权衡，混合设计可能结合PoW的安全性和PoS的效率与可持续性。

Abstract: This paper presents a comprehensive comparative analysis of two dominant blockchain consensus mechanisms, Proof of Work (PoW) and Proof of Stake (PoS), evaluated across seven critical metrics: energy use, security, transaction speed, scalability, centralization risk, environmental impact, and transaction fees. Utilizing recent academic research and real-world blockchain data, the study highlights that PoW offers robust, time-tested security but suffers from high energy consumption, slower throughput, and centralization through mining pools. In contrast, PoS demonstrates improved scalability and efficiency, significantly reduced environmental impact, and more stable transaction fees, however it raises concerns over validator centralization and long-term security maturity. The findings underscore the trade-offs inherent in each mechanism and suggest hybrid designs may combine PoW's security with PoS's efficiency and sustainability. The study aims to inform future blockchain infrastructure development by striking a balance between decentralization, performance, and ecological responsibility.

</details>


### [45] [Scalable Privilege Analysis for Multi-Cloud Big Data Platforms: A Hypergraph Approach](https://arxiv.org/abs/2511.15837)
*Sai Sitharaman,Hassan Karim,Deepti Gupta,Mudit Tyagi*

Main category: cs.CR

TL;DR: 提出了一种基于超图语义和NGAC的新型特权访问管理框架，解决了传统ABAC方法在规模扩展时的立方复杂度问题，实现了次线性复杂度的特权分析。


<details>
  <summary>Details</summary>
Motivation: 多云环境的快速采用加剧了特权访问管理风险，传统基于ABAC的PAM解决方案具有O(n^3)复杂度，在企业规模下无法进行实时特权分析。

Method: 将NIST的下一代访问控制(NGAC)与超图语义集成，利用带标签超边的超图建模复杂多维特权关系，引入3维特权分析框架(攻击面、攻击窗口、攻击身份)。

Result: 在AWS系统上对200-4000用户进行实验验证，相比ABAC提升10倍性能，相比标准NGAC-DAG提升4倍性能，实现秒级特权检测。

Conclusion: 该框架能够有效检测多云基础设施中的特权升级链、过度特权用户和横向移动路径，解决了企业规模下的特权访问管理可扩展性问题。

Abstract: The rapid adoption of multi-cloud environments has amplified risks associated with privileged access mismanagement. Traditional Privileged Access Management (PAM) solutions based on Attribute-Based Access Control (ABAC) exhibit cubic O(n^3) complexity, rendering real-time privilege analysis intractable at enterprise scale. We present a novel PAM framework integrating NIST's Next Generation Access Control (NGAC) with hypergraph semantics to address this scalability crisis. Our approach leverages hypergraphs with labeled hyperedges to model complex, multi-dimensional privilege relationships, achieving sub-linear O(sqrt n) traversal complexity and O(nlogn) detection time-rigorously proven through formal complexity analysis. We introduce a 3-Dimensional Privilege Analysis framework encompassing Attack Surface, Attack Window, and Attack Identity to systematically identify privilege vulnerabilities. Experimental validation on AWS-based systems with 200-4000 users demonstrates 10x improvement over ABAC and 4x improvement over standard NGAC-DAG, enabling sub-second privilege detection at scale. Real-world use cases validate detection of privilege escalation chains, over-privileged users, and lateral movement pathways in multi-cloud infrastructures.

</details>


### [46] [Lifefin: Escaping Mempool Explosions in DAG-based BFT](https://arxiv.org/abs/2511.15936)
*Jianting Zhang,Sen Yang,Alberto Sonnino,Sebastián Loza,Aniket Kate*

Main category: cs.CR

TL;DR: Lifefin是一个通用的自稳定协议，用于解决DAG-based BFT协议中的活跃性漏洞，通过ACS机制在恶劣条件下以有限资源提交交易，几乎零开销地消除活跃性风险。


<details>
  <summary>Details</summary>
Motivation: DAG-based BFT协议存在根本的活跃性漏洞：攻击者可以通过触发内存池爆炸来阻止交易提交，从而损害协议的活跃性。

Method: Lifefin利用Agreement on Common Subset (ACS)机制，使节点能够在内存池爆炸情况下以有限资源提交交易，并与现有DAG-based BFT协议无缝集成。

Result: 将Lifefin集成到Sailfish和Mysticeti协议中，评估显示在保持可比交易吞吐量的同时，仅引入最小额外延迟即可抵抗类似攻击。

Conclusion: Lifefin有效消除了DAG-based BFT协议的活跃性漏洞，在典型情况下几乎零开销，在恶劣条件下仍能保证交易提交。

Abstract: Directed Acyclic Graph (DAG)-based Byzantine Fault-Tolerant (BFT) protocols have emerged as promising solutions for high-throughput blockchains. By decoupling data dissemination from transaction ordering and constructing a well-connected DAG in the mempool, these protocols enable zero-message ordering and implicit view changes. However, we identify a fundamental liveness vulnerability: an adversary can trigger mempool explosions to prevent transaction commitment, ultimately compromising the protocol's liveness.
  In response, this work presents Lifefin, a generic and self-stabilizing protocol designed to integrate seamlessly with existing DAG-based BFT protocols and circumvent such vulnerabilities. Lifefin leverages the Agreement on Common Subset (ACS) mechanism, allowing nodes to escape mempool explosions by committing transactions with bounded resource usage even in adverse conditions. As a result, Lifefin imposes (almost) zero overhead in typical cases while effectively eliminating liveness vulnerabilities.
  To demonstrate the effectiveness of Lifefin, we integrate it into two state-of-the-art DAG-based BFT protocols, Sailfish and Mysticeti, resulting in two enhanced variants: Sailfish-Lifefin and Mysticeti-Lifefin. We implement these variants and compare them with the original Sailfish and Mysticeti systems. Our evaluation demonstrates that Lifefin achieves comparable transaction throughput while introducing only minimal additional latency to resist similar attacks.

</details>


### [47] [Digital Agriculture Sandbox for Collaborative Research](https://arxiv.org/abs/2511.15990)
*Osama Zafar,Rosemarie Santa González,Alfonso Morales,Erman Ayday*

Main category: cs.CR

TL;DR: 本文提出了数字农业沙盒平台，通过联邦学习、差分隐私等技术解决农民因隐私顾虑不愿共享农业数据的问题，使研究人员能在不接触原始数据的情况下进行分析研究。


<details>
  <summary>Details</summary>
Motivation: 数字农业产生大量有价值数据，但农民因隐私担忧不愿分享，限制了研究人员利用这些数据改进农业实践的能力。

Method: 开发安全的在线平台，采用联邦学习、差分隐私和数据分析方法保护数据隐私，同时保持数据的研究价值。

Result: 平台使农民能够以简化的方式识别相似农户，研究人员能在不接触敏感信息的情况下从数据中学习并构建有用工具。

Conclusion: 该平台在保护农场数据隐私与利用数据解决全球粮食和农业挑战之间架起了桥梁，为农民和研究人员创造了安全的协作空间。

Abstract: Digital agriculture is transforming the way we grow food by utilizing technology to make farming more efficient, sustainable, and productive. This modern approach to agriculture generates a wealth of valuable data that could help address global food challenges, but farmers are hesitant to share it due to privacy concerns. This limits the extent to which researchers can learn from this data to inform improvements in farming. This paper presents the Digital Agriculture Sandbox, a secure online platform that solves this problem. The platform enables farmers (with limited technical resources) and researchers to collaborate on analyzing farm data without exposing private information. We employ specialized techniques such as federated learning, differential privacy, and data analysis methods to safeguard the data while maintaining its utility for research purposes. The system enables farmers to identify similar farmers in a simplified manner without needing extensive technical knowledge or access to computational resources. Similarly, it enables researchers to learn from the data and build helpful tools without the sensitive information ever leaving the farmer's system. This creates a safe space where farmers feel comfortable sharing data, allowing researchers to make important discoveries. Our platform helps bridge the gap between maintaining farm data privacy and utilizing that data to address critical food and farming challenges worldwide.

</details>


### [48] [A Quantum-Secure and Blockchain-Integrated E-Voting Framework with Identity Validation](https://arxiv.org/abs/2511.16034)
*Ashwin Poudel,Utsav Poudel,Dikshyanta Aryal,Anuj Nepal,Pranish Pathak,Subramaniyaswamy V*

Main category: cs.CR

TL;DR: 本文提出了一种后量子安全的电子投票架构，集成了Falcon格基数字签名、基于MobileNetV3和AdaFace的生物特征认证，以及许可区块链用于防篡改投票存储。


<details>
  <summary>Details</summary>
Motivation: 量子计算的快速发展对数字系统的密码学基础构成威胁，需要开发安全且可扩展的电子投票框架。

Method: 系统整合了Falcon格基数字签名、生物特征认证（使用MobileNetV3和AdaFace）和许可区块链。选民注册涉及捕获面部嵌入，使用Falcon数字签名并存储在区块链上。投票时通过反欺骗技术和余弦相似度匹配进行实时生物特征验证。

Result: 在CelebA Spoof数据集上的平均分类错误率低于3.5%，在WFAS数据集上低于8.2%。区块链锚定的gas开销最小，注册约为3.3%，投票约为0.15%。系统在并发负载下表现出可扩展性、效率和弹性。

Conclusion: 该方法为数字系统中的选民认证、数据完整性和量子弹性安全等关键挑战提供了统一解决方案。

Abstract: The rapid growth of quantum computing poses a threat to the cryptographic foundations of digital systems, requiring the development of secure and scalable electronic voting (evoting) frameworks. We introduce a post-quantum-secure evoting architecture that integrates Falcon lattice-based digital signatures, biometric authentication via MobileNetV3 and AdaFace, and a permissioned blockchain for tamper-proof vote storage. Voter registration involves capturing facial embeddings, which are digitally signed using Falcon and stored on-chain to ensure integrity and non-repudiation. During voting, real-time biometric verification is performed using anti-spoofing techniques and cosine-similarity matching. The system demonstrates low latency and robust spoof detection, monitored through Prometheus and Grafana for real-time auditing. The average classification error rates (ACER) are below 3.5% on the CelebA Spoof dataset and under 8.2% on the Wild Face Anti-Spoofing (WFAS) dataset. Blockchain anchoring incurs minimal gas overhead, approximately 3.3% for registration and 0.15% for voting, supporting system efficiency, auditability, and transparency. The experimental results confirm the system's scalability, efficiency, and resilience under concurrent loads. This approach offers a unified solution to address key challenges in voter authentication, data integrity, and quantum-resilient security for digital systems.

</details>


### [49] [Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models](https://arxiv.org/abs/2511.16110)
*Yijun Yang,Lichao Wang,Jianping Zhang,Chi Harold Liu,Lanqing Hong,Qiang Xu*

Main category: cs.CR

TL;DR: MFA攻击框架通过注意力转移攻击和轻量级传输增强算法，成功绕过GPT-4o、Gemini-Pro等领先视觉语言模型的多重安全防护，揭示共享视觉表示带来的跨模型安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLMs）现有安全防护措施在对抗攻击下的实际鲁棒性，这些防护包括对齐调优、系统提示和内容审核。

Method: 提出多面攻击（MFA）框架，核心是注意力转移攻击（ATA），将有害指令隐藏在具有竞争目标的元任务中。结合轻量级传输增强算法和简单重复策略，无需模型特定微调即可绕过输入和输出级过滤器。

Result: MFA攻击成功率达到58.5%，在最新商业模型上达到52.8%成功率，比次优攻击方法高出34%。针对一个视觉编码器优化的对抗图像能广泛传输到未见过的VLMs。

Conclusion: 当前防御机制的实际鲁棒性受到挑战，现代VLMs存在持续的安全弱点，共享视觉表示造成了跨模型安全漏洞。

Abstract: The growing misuse of Vision-Language Models (VLMs) has led providers to deploy multiple safeguards, including alignment tuning, system prompts, and content moderation. However, the real-world robustness of these defenses against adversarial attacks remains underexplored. We introduce Multi-Faceted Attack (MFA), a framework that systematically exposes general safety vulnerabilities in leading defense-equipped VLMs such as GPT-4o, Gemini-Pro, and Llama-4. The core component of MFA is the Attention-Transfer Attack (ATA), which hides harmful instructions inside a meta task with competing objectives. We provide a theoretical perspective based on reward hacking to explain why this attack succeeds. To improve cross-model transferability, we further introduce a lightweight transfer-enhancement algorithm combined with a simple repetition strategy that jointly bypasses both input-level and output-level filters without model-specific fine-tuning. Empirically, we show that adversarial images optimized for one vision encoder transfer broadly to unseen VLMs, indicating that shared visual representations create a cross-model safety vulnerability. Overall, MFA achieves a 58.5% success rate and consistently outperforms existing methods. On state-of-the-art commercial models, MFA reaches a 52.8% success rate, surpassing the second-best attack by 34%. These results challenge the perceived robustness of current defense mechanisms and highlight persistent safety weaknesses in modern VLMs. Code: https://github.com/cure-lab/MultiFacetedAttack

</details>


### [50] [ART: A Graph-based Framework for Investigating Illicit Activity in Monero via Address-Ring-Transaction Structures](https://arxiv.org/abs/2511.16192)
*Andrea Venturi,Imanol Jerico-Yoldi,Francesco Zola,Raul Orduna*

Main category: cs.CR

TL;DR: 本文提出了一种基于图的方法来分析Monero交易中的犯罪活动模式，通过构建地址-环-交易图并提取结构性和时间性特征，训练机器学习模型来检测可能的犯罪行为模式。


<details>
  <summary>Details</summary>
Motivation: 随着执法机构在加密货币取证方面的进步，犯罪分子越来越多地使用Monero等隐私保护加密货币来隐藏非法资金流动。由于Monero的强隐私保护和不可追踪特性，传统的区块链分析变得无效，因此需要新的方法来理解犯罪分子的行为模式。

Method: 采用基于图的方法，从已发现的犯罪活动相关的Monero交易中构建地址-环-交易图，提取结构性和时间性特征，并利用这些特征训练机器学习模型。

Result: 开发了能够检测类似行为模式的机器学习模型，这些模式可能揭示犯罪分子的作案手法。

Conclusion: 这是朝着开发支持隐私保护区块链生态系统调查工作的分析工具迈出的第一步，为未来的调查策略提供支持并破坏非法活动。

Abstract: As Law Enforcement Agencies advance in cryptocurrency forensics, criminal actors aiming to conceal illicit fund movements increasingly turn to "mixin" services or privacy-based cryptocurrencies. Monero stands out as a leading choice due to its strong privacy preserving and untraceability properties, making conventional blockchain analysis ineffective. Understanding the behavior and operational patterns of criminal actors within Monero is therefore challenging and it is essential to support future investigative strategies and disrupt illicit activities. In this work, we propose a case study in which we leverage a novel graph-based methodology to extract structural and temporal patterns from Monero transactions linked to already discovered criminal activities. By building Address-Ring-Transaction graphs from flagged transactions, we extract structural and temporal features and use them to train Machine Learning models capable of detecting similar behavioral patterns that could highlight criminal modus operandi. This represents a first partial step toward developing analytical tools that support investigative efforts in privacy-preserving blockchain ecosystems

</details>


### [51] [PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization](https://arxiv.org/abs/2511.16209)
*Huseein Jawad,Nicolas Brunel*

Main category: cs.CR

TL;DR: 本文提出了一种通过盾牌附加来强化系统提示的新框架，这是一种轻量级方法，在原始提示上添加保护性文本层，以防御提示提取攻击。


<details>
  <summary>Details</summary>
Motivation: 系统提示对于指导大型语言模型行为至关重要，但往往包含专有逻辑或敏感信息，容易成为提取攻击的目标。现有防御机制依赖启发式方法、计算开销大或不适用于黑盒API访问的模型。

Method: 将提示强化形式化为效用约束优化问题，利用LLM作为优化器搜索可能的SHIELD空间，最小化基于对抗攻击套件的泄漏指标，同时保持任务效用高于指定阈值。

Result: 优化的SHIELD显著减少了针对全面提取攻击集的提示泄漏，优于已建立的基线防御，且不损害模型的预期功能。

Conclusion: 这项工作为在LLM安全不断升级的背景下开发稳健、效用感知的防御提供了范例。

Abstract: System prompts are critical for guiding the behavior of Large Language Models (LLMs), yet they often contain proprietary logic or sensitive information, making them a prime target for extraction attacks. Adversarial queries can successfully elicit these hidden instructions, posing significant security and privacy risks. Existing defense mechanisms frequently rely on heuristics, incur substantial computational overhead, or are inapplicable to models accessed via black-box APIs. This paper introduces a novel framework for hardening system prompts through shield appending, a lightweight approach that adds a protective textual layer to the original prompt. Our core contribution is the formalization of prompt hardening as a utility-constrained optimization problem. We leverage an LLM-as-optimizer to search the space of possible SHIELDs, seeking to minimize a leakage metric derived from a suite of adversarial attacks, while simultaneously preserving task utility above a specified threshold, measured by semantic fidelity to baseline outputs. This black-box, optimization-driven methodology is lightweight and practical, requiring only API access to the target and optimizer LLMs. We demonstrate empirically that our optimized SHIELDs significantly reduce prompt leakage against a comprehensive set of extraction attacks, outperforming established baseline defenses without compromising the model's intended functionality. Our work presents a paradigm for developing robust, utility-aware defenses in the escalating landscape of LLM security. The code is made public on the following link: https://github.com/psm-defense/psm

</details>


### [52] [Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security](https://arxiv.org/abs/2511.16229)
*Wei Zhao,Zhe Li,Yige Li,Jun Sun*

Main category: cs.CR

TL;DR: Q-MLLM通过两级向量量化构建离散瓶颈来防御多模态大语言模型的对抗攻击，在保持模型性能的同时显著提升防御成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态理解方面表现出色，但视觉输入的对抗攻击漏洞仍然存在，这是由于视觉表示的连续性和文本安全机制向视觉内容转移不足造成的。

Method: 提出Q-MLLM架构，集成两级向量量化（像素-补丁级和语义级）来离散化视觉表示，阻断攻击路径并弥合跨模态安全对齐差距，采用两阶段训练方法确保鲁棒学习。

Result: Q-MLLM在越狱攻击和有害图像攻击方面显著优于现有方法，对越狱攻击达到100%防御成功率（除一个争议案例外），在多个效用基准测试中保持竞争力，推理开销最小。

Conclusion: 向量量化被确立为安全多模态AI系统的有效防御机制，无需昂贵的安全特定微调或检测开销。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in cross-modal understanding, but remain vulnerable to adversarial attacks through visual inputs despite robust textual safety mechanisms. These vulnerabilities arise from two core weaknesses: the continuous nature of visual representations, which allows for gradient-based attacks, and the inadequate transfer of text-based safety mechanisms to visual content. We introduce Q-MLLM, a novel architecture that integrates two-level vector quantization to create a discrete bottleneck against adversarial attacks while preserving multimodal reasoning capabilities. By discretizing visual representations at both pixel-patch and semantic levels, Q-MLLM blocks attack pathways and bridges the cross-modal safety alignment gap. Our two-stage training methodology ensures robust learning while maintaining model utility. Experiments demonstrate that Q-MLLM achieves significantly better defense success rate against both jailbreak attacks and toxic image attacks than existing approaches. Notably, Q-MLLM achieves perfect defense success rate (100\%) against jailbreak attacks except in one arguable case, while maintaining competitive performance on multiple utility benchmarks with minimal inference overhead. This work establishes vector quantization as an effective defense mechanism for secure multimodal AI systems without requiring expensive safety-specific fine-tuning or detection overhead. Code is available at https://github.com/Amadeuszhao/QMLLM.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [53] [A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference](https://arxiv.org/abs/2511.15950)
*Michael V. DeBole,Rathinakumar Appuswamy,Neil McGlohon,Brian Taba,Steven K. Esser,Filipp Akopyan,John V. Arthur,Arnon Amir,Alexander Andreopoulos,Peter J. Carlson,Andrew S. Cassidy,Pallab Datta,Myron D. Flickner,Rajamohan Gandhasri,Guillaume J. Garreau,Megumi Ito,Jennifer L. Klamo,Jeffrey A. Kusnitz,Nathaniel J. McClatchey,Jeffrey L. McKinstry,Tapan K. Nayak,Carlos Ortega Otero,Hartmut Penner,William P. Risk,Jun Sawada,Jay Sivagnaname,Daniel F. Smith,Rafael Sousa,Ignacio Terrizzano,Takanori Ueda,Trent Gray-Donald,David Cox,Dharmendra S. Modha*

Main category: cs.DC

TL;DR: 该系统集成了288个NorthPole神经推理加速器卡，提供115 peta-ops计算能力和3.7 PB/s内存带宽，在30 kW功耗下支持多模型实例部署。


<details>
  <summary>Details</summary>
Motivation: 为企业在现有数据中心环境中部署AI应用提供可扩展、高效且模块化的云推理服务解决方案。

Method: 采用垂直集成的端到端研究原型系统，结合离线训练算法、高性能运行时堆栈和容器化推理流水线。

Result: 系统可在2.8 ms用户间令牌延迟下，同时运行3个80亿参数模型实例，支持28个并发用户；或支持18个30亿参数模型实例，或单个700亿参数模型实例。

Conclusion: 该系统具有可扩展性、模块化和可重构性，适合在企业AI应用中部署智能工作流。

Abstract: A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model.

</details>


### [54] [Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows](https://arxiv.org/abs/2511.15977)
*Daniel Mas Montserrat,Ray Verma,Míriam Barrabés,Francisco M. de la Vega,Carlos D. Bustamante,Alexander G. Ioannidis*

Main category: cs.DC

TL;DR: 本文提出了多种自适应RAM高效并行化染色体级生物信息学工作流的机制，包括符号回归模型预测内存消耗、动态调度器优化任务打包、静态调度器优化处理顺序，以减少内存溢出并提高执行效率。


<details>
  <summary>Details</summary>
Motivation: 大规模基因组工作流在精准医学中处理数十到数百GB的数据集，导致高内存峰值、密集磁盘I/O和内存不足错误。静态资源分配方法难以处理不同染色体的RAM需求变化，导致资源利用率低和运行时间长。

Method: 1. 开发符号回归模型估计每染色体内存消耗，引入插值偏置保守最小化过度分配；2. 提出动态调度器使用多项式回归模型自适应预测RAM使用，将任务打包视为背包问题优化批处理；3. 提出静态调度器优化染色体处理顺序以最小化峰值内存同时保持吞吐量。

Result: 在模拟和真实基因组管道上的评估显示，所提方法提供了减少内存溢出和平衡线程负载的新机制，实现了更快的端到端执行。

Conclusion: 这些方法展示了优化大规模基因组工作流的潜力，通过自适应资源管理提高了执行效率和资源利用率。

Abstract: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.

</details>


### [55] [Can Asymmetric Tile Buffering Be Beneficial?](https://arxiv.org/abs/2511.16041)
*Chengyue Wang,Wesley Pang,Xinrui Wu,Gregory Jun,Luis Romero,Endri Taka,Diana Marculescu,Tony Nowatzki,Pranathi Vasireddy,Joseph Melber,Deming Chen,Jason Cong*

Main category: cs.DC

TL;DR: 本文提出了一种非对称瓦片缓冲(ATB)技术，通过解耦输入和输出操作数的缓冲瓦片维度，显著提升了通用矩阵乘法(GEMM)的性能，在AMD XDNA2 AI Engine上实现了4.54倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统对称瓦片缓冲方法限制了GEMM的计算效率，作者希望通过解耦输入和输出操作数的缓冲瓦片维度来提升算术强度。

Method: 开发了非对称瓦片缓冲(ATB)技术，并建立了包含ATB收益和开销的性能模型来指导瓦片因子选择。

Result: 在AMD XDNA2 AI Engine上应用ATB，混合精度BFP16-BF16 GEMM性能从4.8 TFLOPS提升到24.6 TFLOPS，创下新性能记录。

Conclusion: ATB是一种简单但强大的技术，首次证明了非对称瓦片缓冲的实用性和显著性能优势。

Abstract: General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.
  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE.

</details>


### [56] [Mitigating Shared Storage Congestion Using Control Theory](https://arxiv.org/abs/2511.16177)
*Thomas Collignon,Kouds Halitim,Raphaël Bleuse,Sophie Cerf,Bogdan Robu,Éric Rutten,Lionel Seinturier,Alexandre van Kempen*

Main category: cs.DC

TL;DR: 提出了一种基于控制理论的自适应方法，通过动态调节客户端I/O速率来解决HPC系统中的I/O拥塞问题，实验显示可减少20%的总运行时间并降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 传统I/O栈优化方法通常针对特定工作负载且需要专业知识，难以通用化；在共享HPC环境中，资源拥塞会导致性能不可预测，造成减速和超时问题。

Method: 基于控制理论设计自适应性方法，利用少量运行时系统负载指标动态调节客户端I/O速率，在集群中实现控制器并在真实测试平台上评估。

Result: 实验结果表明该方法有效缓解I/O拥塞，总运行时间最多减少20%，尾延迟降低，同时保持稳定的性能表现。

Conclusion: 基于控制理论的自适应I/O速率调节方法能够有效解决HPC系统中的拥塞问题，提高性能稳定性，减少运行时间和尾延迟。

Abstract: Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance.

</details>


### [57] [Fast LLM Post-training via Decoupled and Best-of-N Speculation](https://arxiv.org/abs/2511.16193)
*Rongxin Cheng,Kai Zhou,Xingda Wei,Siyuan Liu,Mingcong Han,Mingjing Ai,Yeju Zhou,Baoquan Zhong,Wencong Xiao,Xin Liu,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: SpecActor通过动态解耦推测和动态Best-of-N推测方法，在大型语言模型后训练中实现快速rollout，比基线方法快1.3-1.7倍。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型后训练中rollout阶段耗时长的问题，特别是在大批量配置下推测解码效率低下的挑战。

Method: 采用动态解耦推测执行方法最大化GPU计算效率，以及动态Best-of-N推测方法根据rollout进度选择和组合不同的草稿方法。

Result: SpecActor比常见的后训练基线方法快1.3-1.7倍，比简单采用推测解码的rollout快1.3-1.5倍。

Conclusion: SpecActor通过高效的推测解码技术显著加速了LLM后训练中的rollout过程，特别适用于大批量训练场景。

Abstract: Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\sys} is {1.3--1.7}\,$\times$ faster than common post-training baselines, and is {1.3--1.5}\,$\times$ faster compared to naively adopting speculative decoding for rollout.

</details>


### [58] [Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming](https://arxiv.org/abs/2511.16450)
*Ziyue Xu,Zhihong Zhang,Holger R. Roth,Chester Chen,Yan Cheng,Andrew Feng*

Main category: cs.DC

TL;DR: 本文介绍了NVIDIA FLARE如何通过消息量化和容器/文件流技术解决联邦学习中大语言模型的通信开销和本地资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式数据源上训练机器学习模型时面临通信开销和本地资源限制的挑战，特别是在包含数十亿参数的大语言模型时代，这些模型的巨大规模加剧了内存和通信约束。

Method: 通过消息量化和容器/文件流两种关键技术：量化减少消息大小，流技术实现高效内存管理，提升可扩展性和与现有工作流的集成。

Result: 这些改进显著增强了联邦学习与大语言模型的鲁棒性和效率，确保在真实联邦学习场景中实现更好的性能。

Conclusion: NVIDIA FLARE的先进通信能力通过量化和流技术有效解决了大语言模型在联邦学习中的通信和内存挑战，提升了实际部署的可行性。

Abstract: Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [CIMinus: Empowering Sparse DNN Workloads Modeling and Exploration on SRAM-based CIM Architectures](https://arxiv.org/abs/2511.16368)
*Yingjie Qi,Jianlei Yang,Rubing Yang,Cenlin Duan,Xiaolin He,Ziyan He,Weitao Pan,Weisheng Zhao*

Main category: cs.AR

TL;DR: CIMinus是一个专门用于CIM架构上稀疏DNN工作负载成本建模的框架，提供组件级能耗分析和整体延迟评估，填补了理论设计与实际实现之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前CIM系统在有效利用稀疏性方面面临挑战，缺乏针对多样化稀疏DNN工作负载的统一系统化视图和建模方法。

Method: 提出CIMinus框架，通过组件级能耗分析和整体工作负载延迟评估来建模稀疏DNN在CIM架构上的性能。

Result: 验证了CIMinus在当代CIM架构上的适用性，并通过两个用例展示了稀疏模式影响和映射策略有效性的深入洞察。

Conclusion: CIMinus框架为稀疏DNN在CIM系统中的性能建模提供了系统化方法，有助于弥合理论设计与实际实现之间的差距。

Abstract: Compute-in-memory (CIM) has emerged as a pivotal direction for accelerating workloads in the field of machine learning, such as Deep Neural Networks (DNNs). However, the effective exploitation of sparsity in CIM systems presents numerous challenges, due to the inherent limitations in their rigid array structures. Designing sparse DNN dataflows and developing efficient mapping strategies also become more complex when accounting for diverse sparsity patterns and the flexibility of a multi-macro CIM structure. Despite these complexities, there is still an absence of a unified systematic view and modeling approach for diverse sparse DNN workloads in CIM systems. In this paper, we propose CIMinus, a framework dedicated to cost modeling for sparse DNN workloads on CIM architectures. It provides an in-depth energy consumption analysis at the level of individual components and an assessment of the overall workload latency. We validate CIMinus against contemporary CIM architectures and demonstrate its applicability in two use-cases. These cases provide valuable insights into both the impact of sparsity patterns and the effectiveness of mapping strategies, bridging the gap between theoretical design and practical implementation.

</details>
