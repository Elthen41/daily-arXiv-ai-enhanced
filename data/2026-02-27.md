<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.CR](#cs.CR) [Total: 8]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Engineered Simultaneity: The Physical Impossibility of Consolidated Price Discovery Across Spacelike-Separated Exchanges](https://arxiv.org/abs/2602.22350)
*Paul Borrill*

Main category: cs.DC

TL;DR: 论文提出"工程化同时性"概念，指出美国NBBO报价系统存在参考系依赖问题，高频交易公司利用光速延迟获取信息优势，每年从其他市场参与者提取约50亿美元。


<details>
  <summary>Details</summary>
Motivation: 研究美国国家最佳买卖报价(NBBO)系统存在的根本性缺陷，揭示其在物理层面由于光速延迟导致的参考系依赖问题，以及高频交易公司如何利用这种结构性缺陷获取不公平优势。

Method: 提出"工程化同时性"概念框架，分析NBBO系统的三个特征：需要比较类空间隔事件、通过隐含同时性约定实现比较、将结果表示为客观而非约定。使用相对论框架分析交易所间的光速延迟(143-3940微秒)，对比直接数据源(约数十微秒)与统一处理器(1128微秒)的延迟差异。

Result: 证明NBBO是参考系依赖的：其价值取决于定义"当前"价格的参考系。由于交易所间距离43-1180公里，光速延迟造成不可避免的窗口期，期间不存在框架独立的价格排序。高频交易公司利用直接数据源(延迟比超过50:1)获取信息优势，这构成了Ryle意义上的范畴错误。

Conclusion: NBBO系统将"同时性"概念应用于没有框架独立意义的领域，导致的信息不对称每年从其他市场参与者提取约50亿美元。这揭示了金融市场基础设施中的根本性设计缺陷。

Abstract: We introduce the concept of engineered simultaneity: a system design that (1) requires comparing events at spacelike-separated locations, (2) implements this comparison via an implicit simultaneity convention, and (3) represents the result as objective rather than conventional. The United States National Best Bid and Offer (NBBO), mandated by SEC Regulation NMS Rule 611, is shown to be an instance of engineered simultaneity. We prove that the NBBO is frame-dependent: its value depends on the reference frame in which "current" prices are defined. Since the exchanges that generate quote data are separated by distances of 43-1,180 km, light-travel times of 143-3,940 microseconds create unavoidable windows during which no frame-independent price ordering exists. High-frequency trading firms exploit this window by accessing exchange data via direct feeds (latency ~tens of microseconds) while the consolidated Securities Information Processor operates at ~1,128 microseconds -- a ratio exceeding 50:1. We demonstrate that this constitutes a category mistake in the sense of Ryle: the NBBO applies the concept of "simultaneity" in a domain where it has no frame-independent meaning. The resulting information asymmetry extracts approximately $5 billion annually from other market participants.

</details>


### [2] [DIAL: Decentralized I/O AutoTuning via Learned Client-side Local Metrics for Parallel File System](https://arxiv.org/abs/2602.22392)
*Md Hasanur Rashid,Xinyi Li,Youbiao He,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: DIAL提出了一种去中心化的I/O自动调优方法，仅使用客户端本地可观测指标，通过机器学习实现分布式决策，提升并行文件系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行文件系统客户端I/O调优方法依赖大量全局运行时指标和精确的I/O模式建模，导致开销过大，限制了细粒度动态调优在实际系统中的能力。

Method: 采用去中心化方法，将每个I/O客户端视为独立单元，仅使用本地可观测指标进行配置调优，借助机器学习模型使多个可调单元做出独立但协同的决策。

Result: DIAL能够及时响应全局存储系统中的变化，为应用程序实现更好的全局I/O性能。

Conclusion: DIAL通过去中心化方法和本地指标学习，解决了现有自动调优方法开销过大的问题，实现了高效、高性能的并行文件系统数据访问。

Abstract: Enabling efficient, high-performance data access in parallel file systems (PFS) is critical for today's high-performance computing systems. PFS client-side I/O heavily impacts the final I/O performance delivered to individual applications and the entire system. Autotuning the key client-side I/O behaviors has been extensively studied and shows promising results. However, existing work has heavily relied on extensive number of global runtime metrics to monitor and accurate modeling of applications' I/O patterns. Such heavy overheads significantly limit the ability to enable fine-grained, dynamic tuning in practical systems. In this study, we propose DIAL (Decentralized I/O AutoTuning via Learned Client-side Local Metrics) which takes a drastically different approach. Instead of trying to extract the global I/O patterns of applications, DIAL takes a decentralized approach, treating each I/O client as an independent unit and tuning configurations using only its locally observable metrics. With the help of machine learning models, DIAL enables multiple tunable units to make independent but collective decisions, reacting to what is happening in the global storage systems in a timely manner and achieving better I/O performance globally for the application.

</details>


### [3] [AdapTBF: Decentralized Bandwidth Control via Adaptive Token Borrowing for HPC Storage](https://arxiv.org/abs/2602.22409)
*Md Hasanur Rashid,Dong Dai*

Main category: cs.DC

TL;DR: AdapTBF：一种基于自适应借还机制的分布式I/O带宽控制方法，用于解决HPC应用中存储带宽分配不均衡问题，在保证公平性的同时提高整体存储效率。


<details>
  <summary>Details</summary>
Motivation: 传统HPC系统中，应用在计算资源上运行但共享全局存储系统，导致某些应用可能消耗与其分配的计算资源不成比例的存储带宽。例如，单个计算节点上的应用可能发出大量小型随机写入操作，占用过多I/O带宽，从而阻碍分配到更多计算节点的大型作业，造成资源浪费。现有的基于令牌桶过滤器的严格比例限制方法会降低整体I/O效率，因为HPC应用通常产生短时、突发性的I/O流量。

Method: 提出AdapTBF方法，基于现代并行文件系统（如Lustre）中的令牌桶过滤器，引入去中心化的带宽控制方法，采用自适应借还机制。详细设计了算法，并在Lustre中实现了AdapTBF。

Result: 使用基于真实场景建模的合成工作负载进行评估，结果表明AdapTBF能够有效管理I/O带宽，同时保持高存储利用率，即使在极端条件下也能良好工作。

Conclusion: AdapTBF通过自适应借还机制解决了HPC系统中存储带宽分配的公平性和效率问题，在保证应用性能的同时提高了整体存储系统的利用率。

Abstract: Modern high-performance computing (HPC) applications run on compute resources but share global storage systems. This design can cause problems when applications consume a disproportionate amount of storage bandwidth relative to their allocated compute resources. For example, an application running on a single compute node can issue many small, random writes and consume excessive I/O bandwidth from a storage server. This can hinder larger jobs that write to the same storage server and are allocated many compute nodes, resulting in significant resource waste.
  A straightforward solution is to limit each application's I/O bandwidth on storage servers in proportion to its allocated compute resources. This approach has been implemented in parallel file systems using Token Bucket Filter (TBF). However, strict proportional limits often reduce overall I/O efficiency because HPC applications generate short, bursty I/O. Limiting bandwidth can waste server capacity when applications are idle or prevent applications from temporarily using higher bandwidth during bursty phases.
  We argue that I/O control should maximize per-application performance and overall storage efficiency while ensuring fairness (e.g., preventing small jobs from blocking large-scale ones). We propose AdapTBF, which builds on TBF in modern parallel file systems (e.g., Lustre) and introduces a decentralized bandwidth control approach using adaptive borrowing and lending. We detail the algorithm, implement AdapTBF in Lustre, and evaluate it using synthetic workloads modeled after real-world scenarios. Results show that AdapTBF manages I/O bandwidth effectively while maintaining high storage utilization, even under extreme conditions.

</details>


### [4] [CARAT: Client-Side Adaptive RPC and Cache Co-Tuning for Parallel File Systems](https://arxiv.org/abs/2602.22423)
*Md Hasanur Rashid,Nathan R. Tallent,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: CARAT是一个基于机器学习的框架，用于在线协同调优并行文件系统的客户端RPC和缓存参数，仅利用本地可观测指标，实现高达3倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统中并行文件系统的调优仍然具有挑战性，因为复杂的I/O路径、多样的I/O模式和动态的系统条件。现有的自动调优框架缺乏可扩展性、自适应性和在线操作能力。

Method: CARAT是一个ML引导的框架，专注于可扩展的在线调优，协同调优PFS的客户端RPC和缓存参数，仅利用本地可观测指标。每个客户端可以独立做出智能调优决策，实时响应应用I/O行为和系统状态的变化。

Result: 在动态I/O模式、真实HPC工作负载和多客户端部署的广泛评估中，CARAT相比默认或静态配置可以实现高达3倍的性能改进，验证了方法的有效性和通用性。

Conclusion: 由于其可扩展性和轻量级特性，CARAT有潜力广泛部署到现有PFS中，使各种数据密集型应用受益。

Abstract: Tuning parallel file system in High-Performance Computing (HPC) systems remains challenging due to the complex I/O paths, diverse I/O patterns, and dynamic system conditions. While existing autotuning frameworks have shown promising results in tuning PFS parameters based on applications' I/O patterns, they lack scalability, adaptivity, and the ability to operate online. In this work, focusing on scalable online tuning, we present CARAT, an ML-guided framework to co-tune client-side RPC and caching parameters of PFS, leveraging only locally observable metrics. Unlike global or pattern-dependent approaches, CARAT enables each client to make independent and intelligent tuning decisions online, responding to real-time changes in both application I/O behaviors and system states. We then prototyped CARAT using Lustre and evaluated it extensively across dynamic I/O patterns, real-world HPC workloads, and multi-client deployments. The results demonstrated that CARAT can achieve up to 3x performance improvement over the default or static configurations, validating the effectiveness and generality of our approach. Due to its scalability and lightweight, we believe CARAT has the potential to be widely deployed into existing PFS and benefit various data-intensive applications.

</details>


### [5] [GetBatch: Distributed Multi-Object Retrieval for ML Data Loading](https://arxiv.org/abs/2602.22434)
*Alex Aizman,Abhishek Gaikwad,Piotr Żelasko*

Main category: cs.DC

TL;DR: GetBatch是一种新的对象存储API，通过将批量检索提升为一级存储操作，用单一确定性、容错的流式执行替代独立的GET操作，显著提升了机器学习训练管道的数据读取性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练管道以批次方式消费数据，单个训练步骤可能需要从存储集群中分布的多个分片中抽取数千个样本。发出数千个独立的GET请求会产生每个请求的开销，这通常主导了数据传输时间，成为性能瓶颈。

Method: 引入GetBatch作为新的对象存储API，将批量检索提升为一级存储操作。该方法用单一确定性、容错的流式执行替代独立的GET操作，减少了每个请求的开销，提高了数据检索效率。

Result: GetBatch对小对象实现了高达15倍的吞吐量提升。在生产训练工作负载中，P95批次检索延迟降低了2倍，P99每对象尾部延迟降低了3.7倍，相比独立的GET请求有显著性能改进。

Conclusion: GetBatch通过将批量检索作为一级存储操作，有效解决了机器学习训练管道中大量小对象检索的性能瓶颈问题，显著提升了数据读取效率和训练性能。

Abstract: Machine learning training pipelines consume data in batches. A single training step may require thousands of samples drawn from shards distributed across a storage cluster. Issuing thousands of individual GET requests incurs per-request overhead that often dominates data transfer time. To solve this problem, we introduce GetBatch - a new object store API that elevates batch retrieval to a first-class storage operation, replacing independent GET operations with a single deterministic, fault-tolerant streaming execution. GetBatch achieves up to 15x throughput improvement for small objects and, in a production training workload, reduces P95 batch retrieval latency by 2x and P99 per-object tail latency by 3.7x compared to individual GET requests.

</details>


### [6] [veScale-FSDP: Flexible and High-Performance FSDP at Scale](https://arxiv.org/abs/2602.22437)
*Zezhou Wang,Youjie Li,Zhiqi Lin,Jiacheng Yang,Cong Xie,Guanyu Feng,Zheng Zhong,Ziyue Huang,Hongyu Zhu,Zhi Zhang,Yanghua Peng,Xin Liu*

Main category: cs.DC

TL;DR: veScale-FSDP是一个重新设计的FSDP系统，通过灵活的RaggedShard分片格式和结构感知规划算法，解决了现有FSDP系统在块结构训练和非元素级优化器方面的局限性，实现了更高的吞吐量和更低的内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有FSDP系统在支持结构感知训练方法（如块级量化训练）和非元素级优化器（如Shampoo和Muon）方面存在困难，其固定的元素或行级分片格式与块结构计算冲突。此外，现有实现在通信和内存效率方面不足，限制了向数万GPU的扩展。

Method: 提出了veScale-FSDP系统，结合灵活的RaggedShard分片格式和结构感知规划算法。该系统原生支持FSDP所需的高效数据放置，能够支持块级量化和非元素级优化器。

Result: veScale-FSDP相比现有FSDP系统实现了5~66%的吞吐量提升和16~30%的内存使用降低，同时能够高效扩展到数万GPU规模。

Conclusion: veScale-FSDP通过灵活的sharding格式和结构感知规划，解决了现有FSDP系统的局限性，为大规模模型训练提供了更好的灵活性和性能。

Abstract: Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.

</details>


### [7] [Fault-tolerant Reduce and Allreduce operations based on correction](https://arxiv.org/abs/2602.22445)
*Martin Kuettler,Hermann Haertig*

Main category: cs.DC

TL;DR: 提出一种容错的Reduce算法，通过校正通信阶段和树形通信阶段结合，然后与Broadcast结合实现Allreduce


<details>
  <summary>Details</summary>
Motivation: 现有的基于信息传播算法（如gossip或树形通信）的Broadcast实现通常后接校正算法，本文旨在将类似思想应用于Reduce操作，提供容错能力

Method: 采用校正通信阶段在前、树形通信阶段在后的两阶段方法实现Reduce算法，确保对多个进程故障的容错性，并提供算法语义证明

Result: 开发出能够容忍多个进程故障的Reduce算法，并基于此将Broadcast和Reduce结合实现了Allreduce操作

Conclusion: 成功将Broadcast的校正算法思想扩展到Reduce操作，实现了容错的Reduce算法，并在此基础上构建了完整的Allreduce实现，为分布式计算提供了可靠的集合通信原语

Abstract: Implementations of Broadcast based on some information dissemination algorithm -- e.g., gossip or tree-based communication -- followed by a correction algorithm has been proposed previously. This work describes an approach to apply a similar idea to Reduce. In it, a correction-like communication phase precedes a tree-based phase. This provides a Reduce algorithm which is tolerant to a number of failed processes. Semantics of the resulting algorithm are provided and proven.
  Based on these results, Broadcast and Reduce are combined to provide Allreduce.

</details>


### [8] [FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving](https://arxiv.org/abs/2602.22593)
*Shouwei Gao,Junqi Yin,Feiyi Wang,Wenqian Dong*

Main category: cs.DC

TL;DR: Flying Serving是一个基于vLLM的系统，支持在线DP-TP切换而无需重启引擎工作进程，通过虚拟化状态实现动态并行配置调整，显著提升LLM服务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统通常采用静态并行配置，无法适应非平稳流量、混合请求需求和长上下文推理的动态变化，导致性能受限且调整过程具有破坏性。

Method: 提出四个关键技术：1) 零拷贝模型权重管理器，按需提供TP分片视图；2) KV缓存适配器，在DP/TP布局间保持请求KV状态；3) 预先初始化的通信池，分摊集体设置开销；4) 无死锁调度器，协调执行偏差下的安全转换。

Result: 在三种流行LLM和实际服务场景下，Flying Serving在高负载下性能提升高达4.79倍，低负载下提升3.47倍，同时支持延迟和内存驱动的请求。

Conclusion: Flying Serving通过在线DP-TP切换能力，使LLM服务能够动态适应工作负载变化，显著提升吞吐量、降低延迟，并支持长上下文推理的内存需求。

Abstract: Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\times$ under high load and $3.47\times$ under low load while supporting latency- and memory-driven requests.

</details>


### [9] [A Simple Distributed Deterministic Planar Separator](https://arxiv.org/abs/2602.22916)
*Yaseen Abd-Elhaleem,Michal Dory,Oren Weimann*

Main category: cs.DC

TL;DR: 提出了一个简单确定性的分布式算法，用于在平面图中寻找大小为O(D)的平衡分隔器，运行时间为Õ(D)轮，相比之前复杂或随机的算法更简洁。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，许多优化问题的轮复杂度下界为D，因此需要大小为O(D)的分隔器。虽然已有随机算法，但确定性算法一直是一个开放性问题，直到最近才有一个复杂解法。本文旨在提供一个更简单的确定性算法。

Method: 采用简单直接的权重转移策略：每个顶点将其权重任意转移到一个它所在的面。相比之前复杂或随机的权重转移方法，这种简单方法同样有效。

Result: 提出了一个确定性分隔器算法，具有近乎最优的Õ(D)轮复杂度，比之前的确定性算法更简单，同时保持了相同的性能。

Conclusion: 这个简单的确定性分隔器算法可以直接用于去随机化平面图上许多经典问题的最先进分布式算法，如单源最短路径、最大流、有向全局最小割和可达性问题。

Abstract: A balanced separator of a graph $G$ is a set of vertices whose removal disconnects the graph into connected components that are a constant factor smaller than $G$. Lipton and Tarjan [FOCS'77] famously proved that every planar graph admits a balanced separator of size $O(\sqrt{n})$, as well as a balanced separator of size $O(D)$ that is a simple path (where $D$ is $G$'s diameter). In the centralized setting, both separators can be found in linear time. In the distributed setting, $D$ is a universal lower bound for the round complexity of solving many optimization problems, so, separators of size $O(D)$ are preferable.
  It was not until [DISC'17] that a distributed algorithm was devised by Ghaffari and Parter to compute such an $O(D)$-size separator in $\tilde O(D)$ rounds, by adapting the Lipton-Tarjan algorithm to the distributed model. Since then, this algorithm was used in several distributed algorithms for planar graphs, e.g., [GP, DISC'17], [LP, STOC'19], [AEDPW, PODC'25]. However, the algorithm is randomized, deeming the algorithms that use it to be randomized as well. Obtaining a deterministic algorithm remained an interesting open question until [PODC'25], when a (complex) deterministic separator algorithm was given by Jauregui, Montealegre and Rapaport.
  We present a much simpler deterministic separator algorithm with the same (near-optimal) $\tilde O(D)$-round complexity. While previous works devised either complicated or randomized ways of transferring weights from vertices to faces of $G$, we show that a straightforward way also works: Each vertex simply transfers its weight to one arbitrary face it lies on. That's it!
  We note that a deterministic separator algorithm directly derandomizes the state-of-the-art distributed algorithms for classical problems on planar graphs such as single-source shortest-paths, maximum-flow, directed global min-cut, and reachability.

</details>


### [10] [Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study](https://arxiv.org/abs/2602.22760)
*Philipp Wiesner,Soeren Becker,Brett Cornick,Dominik Scheinert,Alexander Acker,Odej Kao*

Main category: cs.DC

TL;DR: 利用可再生能源过剩时段进行分布式LLM训练，通过弹性调度在多个GPU集群间切换训练模式，显著降低碳排放


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练需要大量计算和能源，而可再生能源经常产生超过电网吸收能力的电力，导致弃电浪费。这些弃电时段提供了使用清洁廉价电力训练LLM的机会

Method: 开发了一个系统，在区域弃电窗口期间跨地理分布式GPU集群进行全参数LLM训练，使用Flower联邦学习框架，根据站点可用性在本地单站点训练和联邦多站点同步之间弹性切换

Result: 原型系统训练了一个5.61亿参数的Transformer模型，使用真实世界边际碳强度数据。初步结果显示，弃电感知调度在保持训练质量的同时，将运营排放降低到单站点基线的5-12%

Conclusion: 通过将LLM训练与可再生能源弃电时段对齐，可以利用清洁廉价电力进行模型预训练，显著减少碳排放，同时保持训练质量

Abstract: Training large language models (LLMs) requires substantial compute and energy. At the same time, renewable energy sources regularly produce more electricity than the grid can absorb, leading to curtailment, the deliberate reduction of clean generation that would otherwise go to waste. These periods represent an opportunity: if training is aligned with curtailment windows, LLMs can be pretrained using electricity that is both clean and cheap. This technical report presents a system that performs full-parameter LLM training across geo-distributed GPU clusters during regional curtailment windows, elastically switching between local single-site training and federated multi-site synchronization as sites become available or unavailable. Our prototype trains a 561M-parameter transformer model across three clusters using the Flower federated learning framework, with curtailment periods derived from real-world marginal carbon intensity traces. Preliminary results show that curtailment-aware scheduling preserves training quality while reducing operational emissions to 5-12% of single-site baselines.

</details>


### [11] [LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure](https://arxiv.org/abs/2602.23036)
*Jaehong Cho,Hyunmin Choi,Guseul Heo,Jongse Park*

Main category: cs.DC

TL;DR: LLMServingSim 2.0是一个统一的系统级模拟器，专门用于分析和建模异构、解耦的LLM服务基础设施中硬件与软件之间的运行时交互。


<details>
  <summary>Details</summary>
Motivation: 随着LLM服务基础设施向异构化和解耦化发展，硬件多样性增加，系统组件分布式部署，导致性能由硬件与软件的运行时交互决定。现有模拟器缺乏在统一框架下联合建模异构硬件和解耦服务技术的能力，难以理解这些复杂交互。

Method: LLMServingSim 2.0将服务决策和硬件行为嵌入到单个运行时循环中，支持批处理、路由、卸载、内存和功耗的交互感知建模。通过基于配置文件的建模方式，支持新兴加速器和内存系统的可扩展集成，同时捕捉动态服务行为和系统级效应。

Result: 验证显示，LLMServingSim 2.0能够准确复现实际部署中的关键性能、内存和功耗指标，平均误差仅为0.97%。即使对于复杂配置，模拟时间也保持在10分钟左右。

Conclusion: LLMServingSim 2.0为硬件创新和服务系统设计之间提供了实用的桥梁，支持对下一代LLM服务基础设施进行系统化探索和协同设计。

Abstract: Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or software choices in isolation, but by their runtime interaction through scheduling, data movement, and interconnect behavior. However, understanding these interactions remains challenging, as existing simulators lack the ability to jointly model heterogeneous hardware and disaggregated serving techniques within a unified, runtime-driven framework.
  This paper presents LLMServingSim 2.0, a unified system-level simulator designed to make runtime-driven hardware-software interactions in heterogeneous and disaggregated LLM serving infrastructures explicit and analyzable. LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. The simulator supports extensible integration of emerging accelerators and memory systems through profile-based modeling, while capturing dynamic serving behavior and system-level effects. We validate LLMServingSim 2.0 against real deployments, showing that it reproduces key performance, memory, and power metrics with an average error of 0.97%, while maintaining simulation times of around 10 minutes even for complex configurations. These results demonstrate that LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [FHECore: Rethinking GPU Microarchitecture for Fully Homomorphic Encryption](https://arxiv.org/abs/2602.22229)
*Lohit Daksha,Seyda Guzelhan,Kaustubh Shivdikar,Carlos Agulló Domingo,Óscar Vera Lopez,Gilbert Jonatan,Hubert Dymarkowski,Aymane El Jerari,José Cano,José L. Abellán,John Kim,David Kaeli,Ajay Joshi*

Main category: cs.AR

TL;DR: FHECore提出了一种集成在GPU流式多处理器中的专用功能单元，用于加速全同态加密计算，通过支持宽精度模乘累加操作，显著减少CKKS原语和端到端工作负载的指令数和延迟。


<details>
  <summary>Details</summary>
Motivation: 全同态加密(FHE)直接在加密数据上计算，但计算和内存开销巨大。ASIC加速器开发周期长，难以跟上FHE算法快速演进；GPU虽然可扩展且普及，但其功能单元（如Tensor Cores）专为低精度机器学习设计，不适合FHE所需的宽精度模运算。

Method: 提出FHECore，一个直接集成到GPU流式多处理器中的专用功能单元。关键洞察是：两个主要延迟贡献者——数论变换和基转换——可以表示为模线性变换，因此可以映射到支持宽精度模乘累加操作的通用硬件单元上。

Result: 模拟显示，FHECore使CKKS原语的动态指令数减少几何平均2.41倍，端到端工作负载减少1.96倍。性能分别提升1.57倍和2.12倍，包括自举延迟减少50%，而面积开销仅为2.4%。

Conclusion: FHECore通过在GPU中集成专用模运算单元，有效解决了FHE在GPU上的计算瓶颈，在保持GPU可编程性和可扩展性的同时，显著提升了FHE性能，且硬件开销很小。

Abstract: Fully Homomorphic Encryption (FHE) enables computation directly on encrypted data but incurs massive computational and memory overheads, often exceeding plaintext execution by several orders of magnitude. While custom ASIC accelerators can mitigate these costs, their long time-to-market and the rapid evolution of FHE algorithms threaten their long-term relevance. GPUs, by contrast, offer scalability, programmability, and widespread availability, making them an attractive platform for FHE. However, modern GPUs are increasingly specialized for machine learning workloads, emphasizing low-precision datatypes (e.g., INT$8$, FP$8$) that are fundamentally mismatched to the wide-precision modulo arithmetic required by FHE. Essentially, while GPUs offer ample parallelism, their functional units, like Tensor Cores, are not suited for wide-integer modulo arithmetic required by FHE schemes such as CKKS. Despite this constraint, researchers have attempted to map FHE primitives on Tensor Cores by segmenting wide integers into low-precision (INT$8$) chunks.
  To overcome these bottlenecks, we propose FHECore, a specialized functional unit integrated directly into the GPU's Streaming Multiprocessor. Our design is motivated by a key insight: the two dominant contributors to latency$-$Number Theoretic Transform and Base Conversion$-$can be formulated as modulo-linear transformations. This allows them to be mapped on a common hardware unit that natively supports wide-precision modulo-multiply-accumulate operations. Our simulations demonstrate that FHECore reduces dynamic instruction count by a geometric mean of $2.41\times$ for CKKS primitives and $1.96\times$ for end-to-end workloads. These reductions translate to performance speedups of $1.57\times$ and $2.12\times$, respectively$-$including a $50\%$ reduction in bootstrapping latency$-$all while inuring a modest $2.4\%$ area overhead.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: 本文提出GYWI系统，结合作者知识图谱和检索增强生成，为LLMs提供可控学术背景和可追溯灵感路径，以生成更优质的科学创意。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在科学创意生成中缺乏可控的学术背景和可追溯的灵感路径，导致生成结果质量受限。

Method: 1. 提出作者中心知识图谱构建方法和灵感源采样算法构建外部知识库；2. 设计混合检索机制（RAG+GraphRAG）获取深度和广度知识；3. 采用强化学习原则的Prompt优化策略自动引导LLMs优化结果。

Result: 在arXiv数据集上评估，GYWI在GPT-4o、DeepSeek-V3等多个LLMs上显著优于主流方法，在新颖性、可靠性、相关性等多个指标上表现优异。

Conclusion: GYWI系统通过结合知识图谱和检索增强生成，有效提升了LLMs生成科学创意的质量，提供了可控背景和可追溯灵感路径的解决方案。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [14] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: FIRE是一个综合性金融基准测试，用于评估LLMs的理论金融知识和实际业务场景处理能力，包含理论考试题和3000个金融场景问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估LLMs在金融领域能力的基准测试，需要同时评估理论知识和实际应用价值，以了解LLMs在金融应用中的能力边界。

Method: 1) 理论评估：从广泛认可的金融资格考试中收集多样化考题；2) 实践评估：提出系统评估矩阵，分类复杂金融领域并确保覆盖关键子域和业务活动，收集3000个金融场景问题，包括有参考答案的封闭式决策问题和按预定标准评估的开放式问题。

Result: 对包括XuanYuan 4.0（最新的金融领域模型）在内的最先进LLMs进行了全面评估，建立了强大的领域内基线，系统分析了当前LLMs在金融应用中的能力边界。

Conclusion: FIRE基准测试为评估LLMs的金融能力提供了全面框架，公开发布了基准问题和评估代码以促进未来研究，有助于理解LLMs在金融领域的实际应用潜力。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [15] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 本文提出了因果嵌入框架，将多个详细模型映射到粗粒度因果模型的子系统中，是抽象概念的推广，用于解决多分辨率边缘问题和数据集合并。


<details>
  <summary>Details</summary>
Motivation: 现有的因果抽象方法主要关注两个模型之间的关系，但实际应用中需要将多个详细模型整合到同一个粗粒度模型中。本文旨在扩展抽象概念，建立能够处理多个模型映射的因果嵌入框架。

Method: 定义因果嵌入作为抽象概念的推广，提出广义一致性概念。通过定义多分辨率边缘问题，将因果嵌入应用于统计边缘问题和因果边缘问题。

Result: 建立了因果嵌入的理论框架，展示了其在统计边缘问题和因果边缘问题中的相关性，并说明了在合并不同表示模型的数据集时的实际应用价值。

Conclusion: 因果嵌入框架为处理多个详细模型到粗粒度模型的映射提供了理论基础，在数据整合和模型合并方面具有重要应用前景。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [16] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 提出Agent Behavioral Contracts (ABC)框架，将设计契约原则应用于AI智能体，通过形式化规范（前提条件、不变量、治理策略、恢复机制）和运行时强制执行来解决智能体行为漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统软件通过API、类型系统和断言等契约来确保正确行为，而AI智能体基于提示和自然语言指令运行，缺乏形式化行为规范，这导致智能体部署中出现漂移、治理失败和项目失败的根本原因。

Method: 引入ABC框架，定义契约C=(P,I,G,R)包含前提条件、不变量、治理策略和恢复机制；提出(p,δ,k)-满足度的概率性契约合规概念；证明漂移界限定理；建立多智能体链中安全契约组合的充分条件；实现AgentAssert运行时强制执行库。

Result: 在AgentContract-Bench基准测试（200个场景，7个模型，6个供应商）的1980个会话中：契约化智能体检测到5.2-6.8个软违规（未契约基线完全错过）；实现88-100%硬约束合规；行为漂移界限D*<0.27；前沿模型100%恢复率，所有模型17-100%恢复率；每次动作开销<10毫秒。

Conclusion: ABC框架为AI智能体提供了形式化行为规范，通过运行时强制执行显著提高了合规性、检测了传统方法无法发现的违规，并有效限制了行为漂移，为可靠、可治理的智能体系统提供了理论基础和实用工具。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [17] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 研究人员开发了CogARC测试集来研究人类抽象推理策略，通过两个实验收集了260名参与者在75个视觉推理问题上的行为数据，发现参与者总体表现良好但存在策略差异。


<details>
  <summary>Details</summary>
Motivation: 研究人类在抽象推理中的认知策略，特别是如何从稀疏示例中快速学习和应用规则，为此开发了CogARC这一人类适应的抽象推理测试集。

Method: 使用CogARC（基于ARC改进的人类适应版本）对260名参与者进行测试，记录他们在75个视觉推理问题上的高时间分辨率行为数据，包括示例查看、编辑序列和多尝试提交。

Result: 参与者总体表现良好（实验1准确率约90%，实验2约80%），但问题难度和参与者表现差异大。更难的问题引发更长的思考时间和策略差异。随着任务进行，响应速度加快但准确率略有下降。即使是错误答案也常呈现高度收敛性。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为环境，揭示了人们在不确定性下如何泛化、错误泛化和调整策略，不同问题解决轨迹（直接高效vs探索重启）反映了认知策略的多样性。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [18] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文对潜在推理方法进行了全面分析，发现普遍存在捷径行为，潜在表示虽然能编码多种可能性但并未忠实实现结构化搜索，且监督强度存在权衡关系。


<details>
  <summary>Details</summary>
Motivation: 潜在推理作为一种新兴推理范式，通过在潜在空间而非文本空间进行多步推理，超越了离散语言标记的限制。尽管已有许多研究致力于提升潜在推理的性能，但其内部机制尚未得到充分研究。本文旨在深入理解潜在表示在推理过程中的角色和行为。

Method: 对具有不同监督水平的潜在推理方法进行全面分析。识别了两个关键问题：1）观察普遍的捷径行为；2）检验潜在推理是否支持潜在空间中的广度优先搜索式探索。通过分析潜在表示的行为和推理过程，揭示了监督强度与表示能力之间的权衡关系。

Result: 研究发现：1）潜在推理方法普遍存在捷径行为，能在不依赖潜在推理的情况下达到高准确率；2）虽然潜在表示能够编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐式剪枝和压缩；3）监督强度存在权衡：强监督能缓解捷径行为但限制潜在表示维持多样假设的能力，弱监督允许更丰富的潜在表示但增加了捷径行为。

Conclusion: 潜在推理方法虽然理论上具有在连续潜在空间进行多步计算的潜力，但在实践中存在捷径行为和搜索过程不忠实的问题。监督强度在缓解捷径行为与保持表示多样性之间存在根本性权衡，这为未来改进潜在推理方法提供了重要见解。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [19] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 论文提出对比世界模型(CWM)，使用对比学习训练动作可行性评分器，相比传统监督微调能更好区分物理上可行与不可行动作。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的动作评分器独立处理每个候选动作，缺乏对物理可行与不可行动作的显式区分能力，特别是难以区分语义相似但物理不可行的"困难负例"。

Method: 提出对比世界模型(CWM)，使用InfoNCE对比目标对大型语言模型进行微调，通过挖掘困难负例（语义相似但物理不可行的候选动作）来推动可行与不可行动作在评分空间中的分离。

Result: 在ScienceWorld基准测试中：1) 内在可行性评估显示，CWM在困难负例测试对上比SFT提升6.76个百分点（Precision@1）；2) 实时过滤特性研究表明，在分布外压力条件下，CWM保持更好的安全边界（-2.39 vs -3.96）。

Conclusion: 对比训练比单独使用监督微调能更准确地捕捉物理可行性，通过显式区分可行与不可行动作，特别是困难负例，提高了动作评分器的可靠性。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [20] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: 研究者开发了ConstraintBench基准测试，评估大语言模型能否直接解决完全指定的约束优化问题（无需调用求解器），发现可行性而非最优性是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLMs能否将优化问题表述为求解器代码，但缺乏评估LLMs能否直接产生正确解的能力。需要填补这一空白，评估LLMs在完全指定约束优化问题上的直接求解能力。

Method: 引入ConstraintBench基准，涵盖10个运筹学领域，所有基准解由Gurobi求解器验证。每个任务包含自然语言场景描述（实体、约束、优化目标），模型需返回结构化解，由确定性验证器检查每个约束和求解器证明的最优解。

Result: 评估6个前沿模型在200个任务上的表现：最佳模型仅达到65.0%的约束满足率，但可行解平均达到Gurobi最优目标的89-96%。没有模型能在可行性和最优性（与求解器参考值相差0.1%以内）联合指标上超过30.5%。不同领域难度差异大，可行性从生产组合领域的83.3%到机组分配领域的0.8%。

Conclusion: 可行性是LLMs解决约束优化问题的主要瓶颈，而非最优性。模型存在系统性失败模式：持续时间约束误解、实体幻觉、设施选址和车辆路径问题中的可行性与最优性脱钩。ConstraintBench和评估基础设施将公开发布。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [21] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO框架为编码智能体的优化任务提供系统评估工具，包括版本控制、奖励机制和观察记录，支持对智能体进行迭代改进的实证研究。


<details>
  <summary>Details</summary>
Motivation: 智能体优化是编码智能体的重要应用，但社区缺乏对此任务的系统性理解。智能体优化与传统软件工程有本质区别，需要同时处理确定性代码和随机LLM生成内容，并结构化捕获中间推理和执行结果。

Method: 提出VERO框架，包含：(1) 可复现的评估工具链，具有版本化智能体快照、预算控制评估和结构化执行跟踪；(2) 包含目标智能体和任务的基准测试套件，配有参考评估程序。

Result: 使用VERO进行了实证研究，比较不同优化器配置在不同任务上的表现，分析哪些修改能可靠提升目标智能体性能。

Conclusion: VERO框架支持编码智能体优化作为核心能力的研究，已公开发布以促进该领域发展。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [22] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 本文利用大语言模型对AI与生命周期评估交叉领域的研究进行了系统性综述，揭示了AI技术在LCA中的快速增长趋势，特别是LLM驱动方法的兴起，并提出了一个结合文本挖掘与传统综述的动态分析框架。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在生命周期评估中的应用近年来快速发展，但对该领域研究的全面综合分析仍然有限。本研究旨在填补这一空白，通过系统综述AI-LCA交叉领域的研究，识别当前趋势、新兴主题和未来方向。

Method: 本研究采用大语言模型（LLMs）进行文本挖掘，结合传统的文献综述技术，开发了一个动态有效的分析框架。该方法能够捕捉该领域的高层研究趋势和细微的概念模式（主题）。

Result: 分析显示，随着LCA研究的扩展，AI技术的采用急剧增长，明显转向LLM驱动的方法，机器学习应用持续增加，并且AI方法与相应LCA阶段之间存在统计学上的显著相关性。

Conclusion: 本研究证明了LLM辅助方法在支持大规模、可重复的跨领域综述方面的潜力，同时评估了在AI技术快速发展背景下计算效率高的LCA路径。这项工作有助于LCA从业者将最先进的工具和及时见解纳入环境评估，从而增强可持续性驱动决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [23] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 该论文提出从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，通过定义智能体模板来指导多个LLM的组合与协作。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然能力强大，但许多复杂问题仍超出单个LLM的能力范围。如何将多个LLM作为组件组合成一个更强大的整体系统仍存在不确定性。论文认为可以从认知科学和AI算法的现有文献中找到设计模块化语言智能体的潜在蓝图。

Method: 论文形式化了"智能体模板"的概念，该模板规定了单个LLM的角色以及它们功能应该如何组合。然后对文献中现有的各种语言智能体进行了调研，并突出了它们直接源自认知模型或AI算法的底层模板。

Result: 通过分析现有语言智能体，论文展示了认知模型和AI算法如何为模块化语言智能体设计提供有效的模板。这些模板能够帮助开发更有效、更可解释的语言智能体系统。

Conclusion: 论文呼吁关注受认知科学和AI启发的智能体模板，将其作为开发有效、可解释语言智能体的强大工具。这种基于模板的方法为解决超出单个LLM能力的复杂问题提供了系统化的设计框架。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [24] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出了一种用于无小区O-RAN的智能体AI框架，通过多个LLM智能体协作实现运营商意图的翻译和优化，在节能模式下可减少41.93%的活跃O-RU数量，使用PEFT方法可减少92%的内存使用。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究只考虑由独立智能体处理的简单意图，而需要智能体间协调的复杂意图尚未得到探索。O-RAN架构虽然支持智能体的部署和协调，但缺乏处理复杂意图的有效框架。

Method: 提出一个包含多个LLM智能体的框架：监督智能体翻译运营商意图为优化目标和最小速率要求；用户权重智能体从记忆模块检索先验经验确定用户优先级权重；O-RU管理智能体使用DRL算法确定活跃O-RU集合；监控智能体测量用户数据速率并协调其他智能体保证最小速率要求。采用PEFT方法使同一底层LLM可用于不同智能体。

Result: 仿真结果显示，在节能模式下，与三种基准方案相比，所提框架减少了41.93%的活跃O-RU数量。使用PEFT方法时，与部署单独的LLM智能体相比，内存使用减少了92%。

Conclusion: 提出的智能体AI框架能够有效处理无小区O-RAN中的复杂意图，通过多个LLM智能体的协调协作实现意图翻译和优化，在节能和内存效率方面表现出显著优势。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [25] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE框架通过主动学习如何请求专家推理，而非简单求助，显著提升LLM智能体在专业领域任务中的成功率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在通用推理方面表现出色，但在需要长尾知识的专业领域常常失败。虽然人类专家可以提供这些缺失的知识，但他们的指导通常是非结构化和不可靠的，直接集成到智能体计划中存在问题。

Method: 提出AHCE（主动人类增强挑战参与）框架，用于按需进行人机协作。核心是人类反馈模块（HFM），它采用学习策略将人类专家视为交互式推理工具。

Result: 在Minecraft中的大量实验表明该框架非常有效：在普通难度任务中任务成功率提高32%，在高难度任务中提高近70%，且只需最少的人类干预。

Conclusion: 成功增强智能体需要学习如何请求专家推理，而不仅仅是简单请求帮助。AHCE框架通过主动学习与人类专家的交互方式，显著提升了专业领域任务的表现。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [26] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard是一个基于检索增强的多智能体框架，将安全评估重新构想为证据辩论，通过基于外部政策文档的对抗性辩论实现最先进的安全性能，无需微调即可适应新治理规则。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全机制主要依赖静态的微调分类器，存在适应性僵化问题，无法在不进行昂贵重新训练的情况下强制执行新的治理规则。

Method: 引入CourtGuard框架，采用检索增强的多智能体架构，将安全评估重构为证据辩论过程，通过基于外部政策文档的对抗性辩论来评估模型安全性。

Result: 在7个安全基准测试中达到最先进性能，优于专门的策略遵循基线；实现零样本适应性（在维基百科破坏检测任务中达到90%准确率）；能够自动策划和审计9个新颖的对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了一个强大、可解释且适应性强的路径，能够满足当前和未来的监管要求。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [27] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 该论文研究了数学推理中基于示例指导的有效性不稳定性问题，发现策略使用与策略可执行性之间存在系统性差异，提出了选择性策略检索框架来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 基于示例的指导在数学推理中广泛使用，但其有效性在不同问题和模型间极不稳定，即使指导正确且与问题相关。这种不稳定性源于策略使用与策略可执行性之间的未充分探索的差距。

Method: 通过分析配对的人工编写和模型生成的解决方案，识别策略使用与可执行性的系统性差异。提出选择性策略检索框架，通过经验性、多路径、源感知信号选择性地检索和组合策略。

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源指导，提供了可靠且一致的改进，在AIME25上准确率提升高达13个百分点，在Apex上提升5个百分点。

Conclusion: 策略使用与可执行性之间的系统性差异是数学推理指导不稳定的关键原因，选择性策略检索框架通过显式建模可执行性，显著提升了推理模型的性能。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [28] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算的同步RLHF训练框架，通过动态资源适配、前缀预计算和成本感知的actor扩展策略，实现1.35倍加速和44.8%成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF框架依赖服务器基础设施，难以应对细粒度资源变化，导致同步训练中组件间或组件内的空闲时间造成开销和资源浪费。

Method: 1. 基于无服务器计算环境构建；2. 适应RLHF流程中的动态资源需求；3. 预计算共享前缀避免重复计算；4. 采用考虑响应长度变化的成本感知actor扩展策略；5. 高效分配工作负载减少函数内不平衡和空闲时间。

Result: 在物理测试平台和大规模模拟集群上的实验显示，RLHFless相比最先进的基线方法实现了最高1.35倍的加速和44.8%的成本降低。

Conclusion: RLHFless是首个可扩展的同步RLHF训练框架，通过无服务器计算有效解决了传统RLHF框架的资源效率问题，显著提升了训练速度和成本效益。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [29] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 该论文提出将心理测量学中的评分者模型整合到AI评估流程中，通过多面Rasch模型分离输出质量与评分者行为，提高人类评估的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人类评估在AI模型训练和评估中至关重要，但这些数据很少被视为存在系统误差的测量。当前AI评估中的人类评分数据存在评分者效应（严重性和中心性），导致观察到的评分失真，影响对AI输出质量的准确判断。

Method: 将心理测量学评分者模型整合到AI评估流程中，特别采用项目反应理论评分者模型，尤其是多面Rasch模型。该方法能够分离真实输出质量与评分者行为，调整评分者严重性偏差，提供评分者表现的诊断性见解。

Result: 使用OpenAI摘要数据集作为实证案例，展示调整评分者严重性后产生修正的摘要质量估计，并提供评分者表现的诊断性见解。调整后的分数比原始评分更可靠，减少了评分者偏差的影响。

Conclusion: 将心理测量学建模整合到人机协同评估中，能够更原则性和透明地使用人类数据，使开发者能够基于调整后的分数而非原始易错评分做出决策。这为AI开发和评估提供了更稳健、可解释且与构念对齐的实践路径。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [30] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest：一种利用大型推理模型自身进行KV缓存压缩的新方法，通过并行执行压缩任务来减少长时智能体任务中的内存使用


<details>
  <summary>Details</summary>
Motivation: 长时运行的智能体任务（如深度研究）需要在多个网页和文档之间进行多跳推理，导致LLM上下文被外部检索的token主导，内存使用快速增长并限制解码性能。现有的KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: 提出SideQuest方法，利用大型推理模型自身通过推理其上下文中有用性来执行KV缓存压缩。为防止压缩管理过程的token污染模型内存，将KV缓存压缩框架为与主要推理任务并行执行的辅助任务。

Result: 使用仅215个样本训练的模型进行评估，SideQuest在智能体任务中将峰值token使用减少高达65%，准确率下降最小，优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest通过让模型自身参与KV缓存压缩决策，有效解决了长时智能体任务中的内存扩展问题，为多步推理模型提供了高效的上下文管理方案。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [31] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 本文提出CC-BOS框架，利用古典中文的简洁性和模糊性自动生成对抗性提示，通过多维度果蝇优化算法在黑盒设置下实现高效越狱攻击


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，其安全风险日益受到关注。现有研究表明LLMs对越狱攻击高度敏感，且攻击效果在不同语言环境中存在差异。本文发现古典中文因其简洁性和模糊性能够部分绕过现有安全约束，暴露LLMs的显著漏洞

Method: 提出CC-BOS框架，基于多维度果蝇优化算法自动生成古典中文对抗提示。将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式和上下文），通过嗅觉搜索、视觉搜索和柯西变异迭代优化

Result: 大量实验表明，CC-BOS框架在越狱攻击中持续优于最先进的攻击方法，证明了其有效性

Conclusion: 古典中文在越狱攻击中具有独特优势，CC-BOS框架能够高效自动化地生成对抗提示，揭示了LLMs在古典中文环境下的安全漏洞，为模型安全评估提供了新视角

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [32] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于多渠道在线广告自动出价的分层框架，结合生成式规划和实时控制，相比现有基线提升13.57%的整体回报


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂多变，多渠道场景下需要有效分配预算和约束。现有方法存在局限性：基于优化的方法缺乏动态适应性，强化学习方法难以捕捉历史依赖性和观测模式

Method: 提出AHBid框架：1）基于扩散模型的高层生成规划器动态分配预算和约束；2）约束执行机制确保合规；3）轨迹细化机制利用历史数据增强适应性；4）控制基出价算法结合历史知识和实时信息

Result: 在大规模离线数据集和在线A/B测试中验证有效性，相比现有基线实现13.57%的整体回报提升

Conclusion: AHBid通过整合生成式规划和实时控制，有效解决了多渠道自动出价中的动态适应性和历史依赖性挑战，显著提升了广告投资回报

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [33] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 该论文提出了一种决策理论视角的隐写术检测方法，用于量化LLM中的隐写推理能力，通过引入广义V-信息框架和隐写间隙来检测和缓解模型中的隐写行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开始展现出隐写能力，这可能让未对齐的模型逃避监督机制。然而，目前缺乏检测和量化这种行为的原理性方法。传统的隐写术定义和检测方法需要已知非隐写信号的参考分布，这在LLM的隐写推理场景中不可行。

Method: 提出决策理论视角的隐写术观点，核心见解是隐写术在能够解码和不能解码隐藏内容的智能体之间创造了可用信息的不对称性。引入广义V-信息框架来测量输入中的可用信息量，并定义"隐写间隙"来量化隐写术，通过比较能够解码和不能解码隐藏内容的智能体对隐写信号的下游效用。

Result: 通过实证验证了该形式化框架，证明它可以用于检测、量化和缓解LLM中的隐写推理。

Conclusion: 提出的决策理论视角和隐写间隙度量提供了一种有效的方法来检测和量化LLM中的隐写行为，为解决模型对齐和监管问题提供了新工具。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [34] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了个性化LLM智能体技术，围绕四个核心组件（用户画像建模、记忆、规划、行动执行）构建分析框架，探讨了从原型到实际部署的发展路径。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长期交互场景中的应用增多，需要适应不同用户并保持行为连续性，这使得个性化成为提升智能体有效性的关键。现有研究缺乏对个性化LLM智能体技术的系统性梳理和分析框架。

Method: 采用能力导向的综述方法，将文献组织为四个相互依赖的组件：用户画像建模、记忆系统、规划机制和行动执行。通过这一分类法，综合代表性方法，分析用户信号的表示、传播和利用方式，强调组件间交互和设计权衡。

Result: 建立了理解个性化LLM智能体的结构化框架，识别了跨组件交互模式和常见设计权衡，总结了专门的评估指标和基准测试，梳理了从通用助手到专业领域的应用场景。

Conclusion: 该综述为设计和理解个性化LLM智能体提供了系统框架，规划了从原型个性化到可扩展实际助手的发展路线图，推动更用户对齐、自适应、鲁棒和可部署的智能体系统发展。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [35] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLMs显著提升生物安全相关任务中新手用户的表现，使其准确率比仅使用互联网资源的对照组提高4.16倍，甚至在某些任务上超越专家水平。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在生物学基准测试中表现良好，但尚不清楚它们是否能真正提升新手用户的实际能力，这对于理解科学加速和双重用途风险至关重要。

Method: 进行多模型、多基准的人类提升研究，比较有LLM访问权限的新手与仅使用互联网资源的新手在八个生物安全相关任务集上的表现，参与者有充足时间（最复杂任务长达13小时）。

Result: LLM访问提供了实质性提升：有LLM的新手准确率比对照组高4.16倍；在四个有专家基线的基准测试中，有LLM的新手在三个上超越了专家；独立LLM的表现常常超过LLM辅助的新手；89.6%的参与者报告获取双重用途相关信息几乎没有困难。

Conclusion: LLMs显著提升了新手在原本需要专业训练的生物任务上的表现，强调需要持续、交互式的提升评估与传统基准测试并行，同时突显了双重用途风险。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [36] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域序列推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决数据稀疏和冷启动问题，无需复杂模型架构。


<details>
  <summary>Details</summary>
Motivation: 传统模型中心范式依赖复杂定制架构，难以捕捉跨域的微妙非结构化序列依赖，导致泛化能力差且计算资源需求高。现有方法在处理跨域数据时存在领域差距导致的负迁移问题。

Method: 提出Taesar框架，采用对比解码机制自适应地将跨域上下文编码到目标域序列中，生成增强数据集，使标准模型无需复杂融合架构即可学习复杂依赖关系。

Result: 实验表明Taesar优于模型中心解决方案，能够泛化到各种序列模型，有效结合了数据中心和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生方法，解决了跨域推荐中的领域差距和负迁移问题，为推荐系统提供了一种更高效、泛化能力更强的解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [37] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 研究发现医疗AI评估中的医生分歧主要源于案例本身（81.8%），而非评分标准或医生个体差异；可减少的不确定性（如信息缺失）会增加分歧，而固有的医学模糊性则无影响。


<details>
  <summary>Details</summary>
Motivation: 分析HealthBench医疗AI评估数据集中医生分歧的来源，了解分歧方差存在于何处以及哪些可观察特征能够解释这些分歧，以改进医疗AI评估设计。

Method: 通过方差分解分析医生在医疗AI评估中的分歧，考察评分标准身份、医生身份、案例层面因素对分歧的影响，并使用多种方法（元数据标签、规范评分语言、医学专业、表面特征分类、嵌入表示等）尝试解释分歧。

Result: 评分标准身份仅解释3.6-6.9%的分歧方差，医生身份仅解释2.4%；81.8%的案例层面残差无法被现有解释变量减少；分歧与完成质量呈倒U型关系（AUC=0.689）；可减少的不确定性使分歧几率增加2.55倍，而固有的医学模糊性无影响。

Conclusion: 医疗AI评估中的一致性上限主要是结构性的，但可减少与不可减少不确定性的分离表明，在评估场景中填补信息缺口可以在非固有临床模糊性情况下降低分歧，这为改进评估设计提供了可行方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [38] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench是一个评估LLM智能体长时记忆的新基准，包含真实和合成的智能体轨迹数据，揭示了现有记忆系统的局限性，并提出AMA-Agent解决方案


<details>
  <summary>Details</summary>
Motivation: 当前智能体记忆评估主要关注人机对话场景，但实际应用中智能体记忆是连续的机器生成交互流，需要更贴近真实应用的评估标准

Method: 提出AMA-Bench基准，包含真实世界智能体轨迹和合成轨迹，支持任意长度评估；提出AMA-Agent记忆系统，采用因果图结构和工具增强检索

Result: 现有记忆系统在AMA-Bench上表现不佳，主要缺乏因果性和目标信息，受限于基于相似性的检索损失；AMA-Agent达到57.22%平均准确率，比最强基线提升11.16%

Conclusion: AMA-Bench填补了智能体记忆评估的空白，AMA-Agent通过因果图和工具增强检索有效解决了现有记忆系统的局限性，为长时记忆智能体发展提供了新方向

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [39] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 该研究开发了ClinDet-Bench基准测试，用于评估大语言模型在临床信息不完整场景下识别可确定性的能力，发现现有LLMs在这方面存在缺陷，容易做出过早判断或过度弃权。


<details>
  <summary>Details</summary>
Motivation: 临床决策经常需要在信息不完整的情况下进行，临床专家需要判断可用信息是否足以做出判断，因为过早下结论和不必要的弃权都可能危及患者安全。现有基准测试不足以评估LLMs在临床环境中的安全性。

Method: 基于临床评分系统开发了ClinDet-Bench基准测试，将信息不完整场景分解为可确定和不可确定的条件。识别可确定性需要考虑所有关于缺失信息的假设（包括不太可能的假设），并验证结论是否在所有情况下都成立。

Result: 研究发现最近的LLMs在信息不完整情况下无法识别可确定性，既会产生过早判断，也会过度弃权，尽管它们能够正确解释基础评分知识并在信息完整时表现良好。

Conclusion: 现有基准测试不足以评估LLMs在临床环境中的安全性。ClinDet-Bench提供了一个评估可确定性识别的框架，引导适当的弃权，在医学和其他高风险领域具有潜在应用价值，并且已公开可用。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [40] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能开源智能体框架，通过智能体图、深度推理模式和鲁棒工作流解决现有LLM智能体在复杂任务中的性能瓶颈和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理需要外部工具交互的复杂现实任务时能力趋于瓶颈，现有智能体框架存在工作流简单、性能不稳定、基准支持有限、依赖昂贵商业API等问题。

Method: 提出MiroFlow框架，包含：1）智能体图实现灵活编排；2）可选深度推理模式提升性能；3）鲁棒工作流执行确保稳定可复现性能。

Result: 在多个智能体基准测试（GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch、FutureX）中均达到最先进性能。

Conclusion: MiroFlow可作为深度学习社区易于访问、可复现、可比较的基准框架，推动智能体研究发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [41] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS是一个用于质谱预测的基准框架，支持构建和评估多种深度学习模型架构，提供性能影响因素分析和实用指导。


<details>
  <summary>Details</summary>
Motivation: 质谱技术在药物发现和材料科学中至关重要，但实验光谱数据缺乏阻碍了分子识别，需要建立计算模型预测方法。现有深度学习模型在质谱预测方面缺乏统一评估基准和方法异质性。

Method: 开发了FlexMS基准框架，支持动态构建多种模型架构组合，在预处理公共数据集上使用不同指标评估性能，分析数据集结构多样性、超参数、预训练效果、元数据消融和跨域迁移学习等因素。

Result: FlexMS框架提供了模型性能影响因素的系统分析，包括数据集特性、超参数设置、预训练策略等对预测效果的影响，为模型选择提供实用指导。检索基准模拟实际识别场景，基于预测光谱对潜在匹配进行评分。

Conclusion: FlexMS基准框架解决了质谱预测领域缺乏统一评估标准的问题，为研究人员提供了灵活的工具来构建和比较不同模型，推动了质谱预测技术的发展和应用。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [42] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: 通过人机协作发现Hermite求积规则新误差表示和界的研究，展示AI在数学研究中的辅助作用与局限性


<details>
  <summary>Details</summary>
Motivation: 探索人工智能是否真正能促进创造性数学研究，还是仅仅自动化常规计算并引入错误风险

Method: 通过详细案例研究，使用多个人工智能助手进行系统化的人机协作，扩展手动工作无法达到的结果

Result: 发现了Hermite求积规则的新误差表示和界，在AI辅助下制定并证明了多个定理，揭示了AI在代数操作、系统化证明探索、文献综合和LaTeX准备方面的卓越能力

Conclusion: 当以适当的怀疑态度和验证协议使用时，AI工具可以显著加速数学发现，但需要仔细的人工监督和深厚的领域专业知识

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [43] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: OmniGAIA是一个全面的多模态基准测试，用于评估跨视频、音频和图像模态的智能体在深度推理和多轮工具执行任务上的能力。同时提出了OmniAtlas，一个原生多模态基础智能体，通过工具集成推理范式提升现有开源模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型主要局限于双模态交互（如视觉-语言），缺乏统一认知能力来满足通用AI助手的需求。人类智能天然结合了跨模态感知（视觉、音频、语言）与复杂推理和工具使用，需要开发能够处理真实世界场景的下一代原生多模态AI助手。

Method: 1. 提出OmniGAIA基准：通过新颖的多模态事件图方法构建，从真实世界数据合成复杂的多跳查询，需要跨模态推理和外部工具集成。2. 提出OmniAtlas智能体：采用工具集成推理范式的原生多模态基础智能体，具有主动多模态感知能力。3. 训练方法：使用后见指导树探索策略合成轨迹，并通过OmniDPO进行细粒度错误校正。

Result: OmniAtlas有效增强了现有开源模型的工具使用能力，为真实世界场景的下一代原生多模态AI助手迈出了一步。

Conclusion: 这项工作通过OmniGAIA基准和OmniAtlas智能体，推动了多模态AI助手的发展，使其能够处理需要深度推理和多轮工具执行的复杂跨模态任务，为构建更接近人类智能的统一认知系统提供了重要进展。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [44] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 本文提出了首个通用智能体评估框架，通过统一协议和Exgentic框架对通用智能体进行系统评估，建立了开放通用智能体排行榜，证明通用智能体能在未经环境特定调优的情况下达到与领域专用智能体相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前通用智能体（能在陌生环境中执行任务而无需领域特定工程）的承诺尚未实现。现有智能体大多是专用的，虽然出现了OpenAI SDK Agent和Claude Code等更通用的实现，但缺乏对其通用性能的系统评估。当前的智能体基准测试假设领域特定集成，以排除通用智能体公平评估的方式编码任务信息。

Method: 1) 提出通用智能体评估的概念原则；2) 设计统一协议实现智能体与基准测试的集成；3) 开发Exgentic框架用于通用智能体评估；4) 在六个环境中对五个知名智能体实现进行基准测试，建立首个开放通用智能体排行榜。

Result: 实验表明，通用智能体能够泛化到多样化环境中，在没有任何环境特定调优的情况下，达到与领域专用智能体相当的性能水平。研究团队发布了评估协议、框架和排行榜，为通用智能体的系统研究奠定基础。

Conclusion: 本文首次将通用智能体评估作为首要研究目标，提出了系统评估框架和方法，证明通用智能体具有实际可行性，为未来通用智能体的研究和开发提供了重要基础。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [45] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench是一个针对扫描探针显微镜（SPM）的博士级多模态基准测试，通过自动化数据合成管道和创新的评估指标，解决了现有科学领域基准测试的数据污染、复杂度不足和人工成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在专业科学领域的能力评估存在明显缺陷：现有基准测试存在数据污染问题、复杂度不足，且人工标注成本过高。特别是在扫描探针显微镜（SPM）这样的专业领域，缺乏高质量、权威的评估基准。

Method: 1. 开发全自动数据合成管道，确保高权威性和低成本；2. 使用Anchor-Gated Sieve（AGS）技术从2023-2025年的arXiv和期刊论文中高效提取高价值图文对；3. 采用混合云-本地架构，VLMs仅返回空间坐标"llbox"进行本地高保真裁剪，实现极致的token节省；4. 引入Strict Imperfection Penalty F1（SIP-F1）评分进行客观评估。

Result: 1. 建立了SPM-Bench基准测试，能够准确评估LLMs在复杂物理场景中的推理能力；2. SIP-F1评分不仅建立了严格的能力层次，还首次量化了模型的"个性"（保守型、激进型、赌徒型或智慧型）；3. 通过将结果与模型报告的置信度和感知难度相关联，揭示了当前AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够有效评估LLMs在专业科学领域的推理能力，并揭示了模型在复杂物理场景中的真实表现边界和个性特征。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [46] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 提出诊断对齐框架，将AI生成的影像报告作为不可变推理状态，与医生验证结果系统比较，通过多级一致性评估显示二元词汇评估低估了临床有意义的一致性


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人机协同验证至关重要，但模型初始推理与专家修正之间的过渡很少被分析为结构化信号。需要一种能够量化修正动态并支持可追溯、人类对齐评估的方法。

Method: 引入诊断对齐框架，保留AI生成的影像报告作为不可变推理状态，系统比较医生验证结果。推理流程整合视觉大语言模型、基于BERT的医学实体提取和序列语言模型推理步骤，在专家评审前强制执行领域一致的细化。使用四级一致性框架评估：精确主要匹配率、语义相似性调整率、跨类别对齐和综合一致性率。

Result: 在21个皮肤病案例评估中，精确一致性达到71.4%，语义相似性下保持不变(t=0.60)，结构化跨类别和差异重叠分析产生100%综合一致性(95% CI: [83.9%, 100%])。没有案例显示完全诊断分歧。

Conclusion: 二元词汇评估显著低估了临床有意义的一致性。将专家验证建模为结构化转换，能够实现信号感知的修正动态量化，并支持基于影像的临床决策支持系统的可追溯、人类对齐评估。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [47] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: RepSPD：一种基于黎曼流形的几何深度学习模型，通过交叉注意力机制和图功能连接特征调制SPD几何属性，并引入全局双向对齐策略减少几何失真，显著提升EEG解码性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵（SPD）的EEG分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构。需要一种能够更好揭示脑功能连接物理基础的方法。

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图功能连接特征调制SPD几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减少曲率引起的几何失真。

Result: 大量实验表明，该框架显著优于现有EEG表示方法，展现出卓越的鲁棒性和泛化能力。

Conclusion: RepSPD通过几何深度学习方法有效整合了EEG的统计特征和拓扑结构信息，为脑电信号解码提供了更强大的表示学习框架。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [48] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的多智能体F1赛车策略优化方法，通过交互模块和自博弈训练生成竞争性策略，能够根据对手行为调整进站时机、轮胎选择和能量分配。


<details>
  <summary>Details</summary>
Motivation: F1比赛中，车队需要根据不断变化的比赛条件和竞争对手的行动来调整比赛策略。现有策略优化方法往往缺乏对多智能体交互的考虑，无法充分适应动态竞争环境。

Method: 基于预训练的单智能体策略，引入交互模块来考虑竞争对手的行为。结合交互模块和自博弈训练方案生成竞争性策略，并根据相对性能对智能体进行排名。

Result: 智能体能够根据对手行为自适应调整进站时机、轮胎选择和能量分配，实现稳健且一致的比赛表现。该框架仅依赖真实比赛中可用的信息，可在赛前和赛中为策略师提供决策支持。

Conclusion: 提出的强化学习方法能够有效优化多智能体F1比赛策略，通过考虑竞争对手互动实现适应性策略调整，为实际比赛策略决策提供了可行的技术框架。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [49] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 本文提出AILS-AHD方法，利用大语言模型动态生成和优化破坏启发式，在带容量约束的车辆路径问题上取得优异性能，在CVRPLib大规模基准测试中为8/10实例创造了新的最佳已知解。


<details>
  <summary>Details</summary>
Motivation: CVRP作为组合优化基础问题，其NP-hard特性在大规模实例中带来显著计算挑战。现有方法在处理大规模问题时仍有改进空间，需要更智能的启发式设计方法。

Method: 提出AILS-AHD方法，将进化搜索框架与大语言模型结合，在自适应迭代局部搜索中动态生成和优化破坏启发式，并引入基于LLM的加速机制提升计算效率。

Result: 与AILS-II和HGS等先进求解器相比，AILS-AHD在中等和大规模实例上均表现出优越性能，在CVRPLib大规模基准测试中为10个实例中的8个创造了新的最佳已知解。

Conclusion: LLM驱动的启发式设计在车辆路径优化领域具有巨大潜力，AILS-AHD方法展示了将大语言模型与进化搜索框架结合的有效性，为CVRP求解提供了创新解决方案。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [50] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个基于多智能体LLM的情感去毒系统，通过四个智能体分析、调整、监控和指导信息消费，显著降低情感刺激强度同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容让消费者暴露在过度情感刺激下，阻碍冷静决策。需要一种既能减少情感刺激又不限制原始文本访问的信息净化系统。

Method: 提出MALLET多智能体系统：1)情感分析智能体使用6情感BERT分类器量化刺激强度；2)情感调整智能体用LLM将文本重写为BALANCED(中性化文本)和COOL(中性化文本+补充文本)两种模式；3)平衡监控智能体聚合每周信息消费模式并生成个性化建议；4)个人指导智能体根据消费者敏感度推荐呈现模式。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低(最高19.3%)，情感平衡改善，语义保持良好。刺激降低与语义保持之间的相关性接近零，表明两者可独立控制。类别分析显示体育、商业、科技类刺激大幅降低(17.8-33.8%)，而世界类效果有限，因为事实本身具有高刺激性。

Conclusion: MALLET系统为支持消费者冷静接收信息提供了框架，无需限制对原始文本的访问，实现了情感刺激降低与语义保持的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [51] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 该论文提出了构建通用世界模型的三位一体一致性框架（模态、空间、时间一致性），并引入了CoW-Bench基准来评估视频生成模型和统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前虽然Sora等视频生成模型和统一多模态模型（UMM）在物理动力学建模方面取得进展，但缺乏定义通用世界模型所需基本属性的理论框架。需要建立原则性理论来指导世界模型的发展。

Method: 提出三位一体一致性框架：模态一致性作为语义接口、空间一致性作为几何基础、时间一致性作为因果引擎。通过这一框架系统回顾多模态学习演进，并引入CoW-Bench基准来统一评估视频生成模型和UMMs。

Result: 建立了通用世界模型的原则性理论框架，揭示了从松散耦合专业模块向统一架构的演进轨迹，创建了用于多帧推理和生成场景评估的CoW-Bench基准。

Conclusion: 该工作为通用世界模型的发展建立了原则性路径，明确了当前系统的局限性以及未来进展所需的架构要求，通过三位一体一致性框架和CoW-Bench基准推动了该领域的发展。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [52] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励来协调不同难度任务的学习，在时间序列问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列推理方法存在两个局限：1) 将时间序列仅视为文本或图像，无法捕捉回答特定问题所需的趋势和季节性等模式；2) 在混合简单和复杂任务训练时，简单目标主导学习过程，阻碍深度推理能力发展。

Method: 提出PATRA模型，包含：1) 模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2) 任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链。

Result: 大量实验表明，PATRA在多样化时间序列问答任务中优于强基线模型，展现出卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理设计，有效解决了现有时间序列推理方法的局限性，提升了模型对时间序列复杂动态的感知和逻辑推理深度。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [53] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个针对单细胞基础模型的自然语言评估框架，通过虚拟细胞抽象统一评估目标，包含五个自然语言任务，并引入知识增强评估来克服传统指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中，无论是通用还是专用大语言模型的评估实践都存在不足：现有基准测试分散在不同任务中，采用多项选择分类等与现实使用场景不符的格式，且依赖缺乏可解释性和生物学基础的评价指标。

Method: 提出SC-ARENA框架，包括：1）形式化虚拟细胞抽象，统一评估目标；2）定义五个自然语言任务（细胞类型注释、描述、生成、扰动预测和科学问答）；3）引入知识增强评估，整合外部本体、标记数据库和科学文献。

Result: 实验表明：1）在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均衡，特别是在需要机制或因果理解的任务上；2）知识增强评估框架确保了生物学正确性，提供可解释、有证据支持的推理，并具有高判别能力。

Conclusion: SC-ARENA为评估单细胞生物学中的大语言模型提供了一个统一且可解释的框架，指向开发与生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [54] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 论文证明基于优化的AI系统（如RLHF训练的LLM）在形式上无法实现规范性治理，因为它们缺乏真正智能体所需的两个必要条件：不可通约性和否定性响应能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统被部署在高风险领域（医疗诊断、法律研究、金融分析），人们假设它们可以被规范性治理。本文旨在证明这种假设对于基于优化的系统在形式上是无效的。

Method: 通过形式化分析，建立真正智能体所需的两项必要且充分的结构条件：1) 不可通约性 - 将某些边界视为不可协商的约束而非可交易的权重；2) 否定性响应能力 - 当这些边界受到威胁时能够暂停处理的非推理机制。证明RLHF系统在结构上与这两个条件不相容。

Result: RLHF系统在结构上无法满足规范性治理的要求。其优化操作（将所有价值统一为标量指标并始终选择最高分输出）恰恰排除了规范性治理的可能性。已知的失败模式（谄媚、幻觉、不忠实的推理）不是偶然，而是结构性的表现。

Conclusion: 基于优化的AI系统在形式上无法成为真正的规范性智能体。论文的主要积极贡献是提供了一个与基质无关的结构规范，定义了任何系统（生物、人工或制度）要成为智能体而非复杂工具所必须满足的条件。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [55] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 该论文提出了AIQI，这是第一个在一般强化学习中被证明具有渐进ε最优性的无模型智能体，通过分布动作值函数的通用归纳实现


<details>
  <summary>Details</summary>
Motivation: 现有的一般强化学习最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。研究者希望开发出第一个具有理论保证的无模型通用智能体，扩展已知通用智能体的多样性。

Method: 提出了Universal AI with Q-Induction (AIQI)，这是一种无模型智能体，通过对分布动作值函数进行通用归纳，而不是像先前工作那样对策略或环境进行归纳。

Result: 在"grain of truth"条件下，证明了AIQI具有强渐进ε最优性和渐进ε贝叶斯最优性，这是第一个被证明在一般强化学习中具有渐进最优性的无模型智能体。

Conclusion: AIQI作为第一个具有理论保证的无模型通用智能体，显著扩展了已知通用智能体的多样性，为无模型方法在一般强化学习中的理论发展奠定了基础。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [56] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 提出通过解耦正确性和可检查性条件，训练"翻译器"模型将固定求解器模型的解决方案转换为可检查形式，避免可读性税问题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要确保其输出能够被能力较低的系统轻松检查。证明者-验证者游戏可用于提高模型输出的可检查性，但会导致准确性下降（可读性税）

Method: 提出解耦方法：首先训练求解器模型以最大化正确性，然后训练翻译器模型将求解器的解决方案转换为可检查形式，同时保留求解器的答案。制定解耦的证明者-验证者游戏，其均衡对应忠实且可检查的翻译器

Result: 该方法允许在保持求解器高准确性的同时，通过翻译器实现输出的可检查性，避免了传统方法中的可读性税问题

Conclusion: 通过解耦正确性和可检查性条件，并引入翻译器模型，可以在不牺牲准确性的前提下提高大语言模型输出的可检查性，为解决可读性税问题提供了有效方案

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [57] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一个测试时修正或拒绝的剪枝框架，用于动态优化多智能体系统的信息流，无需重新训练，通过主动防火墙机制拦截智能体输出并纠正错误，显著提升数学基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息的级联影响问题。当前解决方案通常采用僵化的结构工程或昂贵的微调，限制了系统的可部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，使用检索增强的修正器基于故障驱动指标池迭代纠正错误，利用蒸馏的故障模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时采用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准测试上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能根据任务难度动态调整修正力度，并利用上下文感知指标解决广泛的错误模式。

Conclusion: AgentDropoutV2提供了一个无需重新训练的动态优化多智能体系统信息流的有效框架，通过主动错误检测和修正机制显著提升系统性能，同时保持部署灵活性和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [58] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent是一个整合大语言模型与临床诊断工具的智能体，通过图像衍生的诊断和视觉证据进行基于证据的胸部X光诊断推理，解决了现有大视觉语言模型在医疗诊断中证据不充分、可验证性差的问题。


<details>
  <summary>Details</summary>
Motivation: 胸部X光在胸部诊断中起核心作用，其解释需要多步骤、基于证据的推理。现有的大视觉语言模型（LVLMs）存在三个主要问题：1）生成的回答虽然看似合理但缺乏诊断证据的忠实基础；2）提供的视觉证据有限难以验证；3）需要昂贵的重新训练来支持新的诊断任务。这些问题限制了其在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断智能体，整合大语言模型（LLM）与临床基础诊断工具，使用图像衍生的诊断和视觉证据进行基于证据的诊断推理。同时引入了CXReasonDial多轮对话基准，包含1,946个对话，涵盖12个诊断任务。

Result: CXReasonAgent能够产生忠实基于证据的响应，相比大视觉语言模型（LVLMs）实现了更可靠和可验证的诊断推理。通过CXReasonDial基准评估证明了该方法在临床诊断任务中的有效性。

Conclusion: 在安全关键的临床环境中，整合临床基础诊断工具对于实现可靠、可验证的诊断推理至关重要。CXReasonAgent通过结合大语言模型与临床工具，为胸部X光诊断提供了更可靠的解决方案。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [59] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN是一个基于神经ODE的脑电动态预测框架，通过将时空频特征整合到谱图节点中，然后使用神经ODE建模连续潜在动态，显著提升了EEG动态预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统潜变量方法通常通过循环架构离散化时间来建模连续脑动态，这必然导致累积预测误差的复合，并且无法捕捉EEG的瞬时非线性特征。需要一种能够克服这些挑战的新方法。

Method: 提出ODEBRAIN框架：1）将时空频特征整合到谱图节点中；2）使用神经ODE建模连续潜在动态；3）确保潜表示能够捕捉任意时间点上复杂脑状态的随机变化。

Result: 大量实验验证表明，ODEBRAIN在预测EEG动态方面显著优于现有方法，具有增强的鲁棒性和泛化能力。

Conclusion: ODEBRAIN通过神经ODE建模连续潜在动态，有效克服了传统方法的局限性，为神经群体动态建模提供了更准确、鲁棒的解决方案。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [60] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该研究将KM信念更新的公理转化为包含三个模态算子的模态逻辑公理，并与AGM信念修正的模态逻辑进行比较，证明AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立KM信念更新公理与模态逻辑之间的对应关系，并通过与AGM信念修正的模态逻辑比较，澄清两种信念变化理论之间的关系。

Method: 为每个KM信念更新公理构建对应的模态逻辑公理，使用三个模态算子：单模态信念算子B、双模态条件算子>和单模态必然算子□。然后将得到的逻辑与从AGM公理转换而来的类似逻辑进行比较。

Result: 证明KM逻辑(L_KM)的每个公理都是AGM逻辑(L_AGM)的定理，表明AGM信念修正是KM信念更新的特例。对于强版本KM更新，差异可归结为处理非意外信息的单个公理。

Conclusion: AGM信念修正可以视为KM信念更新的特殊情况，两种理论的关系通过模态逻辑框架得到形式化澄清，为理解不同信念变化方法提供了理论基础。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [61] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出基于重采样的推理方法，通过对输入进行不变变换生成多个版本，聚合推理结果以提高准确性


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型在推理时仍会产生错误，主要源于偶然不确定性和认知不确定性。研究发现，当基于输入的多个不变变换进行推理时，推理错误会表现出部分独立性，这为改进推理精度提供了机会。

Method: 提出"重采样"推理方法：对训练好的AI模型，应用输入的多个变换版本进行推理，然后将这些推理输出聚合为更准确的结果。该方法利用认知不确定性导致的推理错误的部分独立性。

Result: 该方法有潜力提高推理准确性，并为平衡模型大小和性能提供策略。通过聚合多个变换版本的推理结果，可以减少认知不确定性带来的错误。

Conclusion: 基于重采样的推理方法能够利用认知不确定性导致的推理错误的部分独立性，通过聚合多个变换版本的推理结果来提高推理准确性，为模型优化提供了新思路。

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [62] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: GRAVE2、GRAVER和GRAVER2算法通过两层搜索、节点回收及其组合技术扩展GRAVE，在保持游戏强度的同时大幅减少存储节点数量。


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏博弈中表现出色，但其需要在每个节点存储额外的胜率/访问统计数据，在内存受限环境中不实用，限制了实际应用。

Method: 提出了三种新算法：GRAVE2（两层搜索）、GRAVER（节点回收）和GRAVER2（两种技术结合），通过减少存储节点数量来优化内存使用。

Result: 这些增强技术能够大幅减少存储节点数量，同时保持与原始GRAVE算法相当的游戏强度。

Conclusion: 通过两层搜索和节点回收技术，新算法在内存受限环境中实现了GRAVE算法的实用化，扩展了其在通用游戏博弈中的应用范围。

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [63] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 本文提出了一种基于细粒度任务分解的多智能体LLM交易框架，相比传统的粗粒度指令方法，显著提升了风险调整后收益，并通过投资分析输出与下游决策偏好的对齐来驱动系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体系统的自主金融交易方法通常采用分析师和经理角色的抽象指令，忽视了现实工作流程的复杂性，导致推理性能下降和决策透明度不足。

Method: 提出多智能体LLM交易框架，将投资分析明确分解为细粒度任务而非粗粒度指令；使用日本股票数据（价格、财务报表、新闻、宏观信息）在泄漏控制的回测设置下评估；进行标准投资组合优化，利用与股票指数的低相关性和各系统输出的方差。

Result: 细粒度任务分解相比传统粗粒度设计显著提高了风险调整后收益；分析中间智能体输出表明，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素；投资组合优化方法实现了优越性能。

Conclusion: 这些发现为在实际交易系统中应用LLM智能体时的智能体结构和任务配置设计提供了重要贡献，强调了细粒度任务分解和分析-决策对齐的重要性。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [64] [An Adaptive Multichain Blockchain: A Multiobjective Optimization Approach](https://arxiv.org/abs/2602.22230)
*Nimrod Talmon,Haim Zysberg*

Main category: cs.CR

TL;DR: 该论文提出了一种动态多链配置框架，将区块链配置建模为多智能体资源分配问题，通过优化器将应用和操作者分组到临时链中，并设置链级清算价格，以解决传统多链设计的静态性问题。


<details>
  <summary>Details</summary>
Motivation: 区块链在安全交易处理方面应用广泛，但其可扩展性有限，且现有的多链设计通常是静态的，无法适应需求和容量的动态变化。需要一种能够根据需求和容量变化动态调整区块链配置的解决方案。

Method: 将区块链配置建模为多智能体资源分配问题：应用和操作者声明需求、容量和价格边界；优化器在每个epoch中将它们分组到临时链中，并设置链级清算价格。目标函数是最大化应用、操作者和系统的治理加权归一化效用组合。该模型具有模块化特性，支持能力兼容性、应用类型多样性和epoch间的稳定性，并可在链下求解，结果在链上可验证。

Result: 分析了公平性和激励问题，并通过模拟实验突出了吞吐量、去中心化、操作者收益和服务稳定性之间的权衡关系。

Conclusion: 该研究提出了一种动态多链配置框架，能够根据需求和容量的变化灵活调整区块链配置，通过优化资源分配提高系统效率，同时考虑了公平性、激励兼容性和系统稳定性等多方面因素。

Abstract: Blockchains are widely used for secure transaction processing, but their scalability remains limited, and existing multichain designs are typically static even as demand and capacity shift. We cast blockchain configuration as a multiagent resource-allocation problem: applications and operators declare demand, capacity, and price bounds; an optimizer groups them into ephemeral chains each epoch and sets a chain-level clearing price. The objective maximizes a governance-weighted combination of normalized utilities for applications, operators, and the system. The model is modular -- accommodating capability compatibility, application-type diversity, and epoch-to-epoch stability -- and can be solved off-chain with outcomes verifiable on-chain. We analyze fairness and incentive issues and present simulations that highlight trade-offs among throughput, decentralization, operator yield, and service stability.

</details>


### [65] [Optimized Disaster Recovery for Distributed Storage Systems: Lightweight Metadata Architectures to Overcome Cryptographic Hashing Bottleneck](https://arxiv.org/abs/2602.22237)
*Prasanna Kumar,Nishank Soni,Gaurang Munje*

Main category: cs.CR

TL;DR: 论文分析了分布式存储架构中基于哈希的重复数据删除在灾难恢复中的局限性，提出了一种不依赖内容哈希的元数据驱动数据识别框架


<details>
  <summary>Details</summary>
Motivation: 分布式存储架构是现代云原生基础设施的基础，但灾难恢复工作流中存在一个关键操作瓶颈：依赖基于内容的加密哈希进行数据识别和同步。虽然基于哈希的重复数据删除在稳态操作中对存储效率有效，但在故障转移和故障恢复事件中，当哈希索引过时、不完整或在崩溃后必须重建时，它成为系统性缺陷。

Method: 论文精确描述了完全或部分重新哈希不可避免的操作条件，分析了加密重新哈希对恢复时间目标（RTO）合规性的下游影响，并提出了一种向确定性、元数据驱动识别的通用架构转变。该框架在数据摄取时为数据块分配全局唯一的复合标识符，独立于内容分析，从而在灾难恢复期间实现无需加密开销的即时增量计算。

Result: 论文提出了一个能够显著改善灾难恢复性能的框架，通过消除加密哈希计算的开销，实现更快的恢复时间目标（RTO）合规性。

Conclusion: 基于内容的加密哈希在灾难恢复场景中存在根本性缺陷，需要转向元数据驱动的数据识别方法，以解决分布式存储架构中的操作瓶颈并改善恢复性能。

Abstract: Distributed storage architectures are foundational to modern cloud-native infrastructure, yet a critical operational bottleneck persists within disaster recovery (DR) workflows: the dependence on content-based cryptographic hashing for data identification and synchronization. While hash-based deduplication is effective for storage efficiency in steady-state operation, it becomes a systemic liability during failover and failback events when hash indexes are stale, incomplete, or must be rebuilt following a crash. This paper precisely characterizes the operational conditions under which full or partial re-hashing becomes unavoidable. The paper also analyzes the downstream impact of cryptographic re-hashing on Recovery Time Objective (RTO) compliance, and proposes a generalized architectural shift toward deterministic, metadata-driven identification. The proposed framework assigns globally unique composite identifiers to data blocks at ingestion time-independent of content analysis enabling instantaneous delta computation during DR without any cryptographic overhead.

</details>


### [66] [Analysis of LLMs Against Prompt Injection and Jailbreak Attacks](https://arxiv.org/abs/2602.22242)
*Piyush Jaiswal,Aaditya Pratap,Shreyansh Saraswati,Harsh Kasyap,Somanath Tripathy*

Main category: cs.CR

TL;DR: 评估多个开源大语言模型对提示注入和越狱攻击的脆弱性，发现模型间存在显著行为差异，并测试了轻量级防御机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实系统中的广泛应用，提示工程成为资源有限组织采用LLM的有效工具。同时，LLM容易受到基于提示的攻击，因此分析这种风险已成为关键的安全需求。

Method: 使用大型手动策划的数据集，评估多个开源LLM（包括Phi、Mistral、DeepSeek-R1、Llama 3.2、Qwen和Gemma变体）对提示注入和越狱攻击的脆弱性。同时评估了几种轻量级推理时防御机制，这些机制作为过滤器运行，无需重新训练或GPU密集型微调。

Result: 观察到模型间存在显著的行为差异，包括拒绝响应和由内部安全机制触发的完全静默无响应。虽然防御机制能够缓解直接攻击，但始终会被长且推理密集的提示绕过。

Conclusion: 大语言模型对提示注入和越狱攻击存在显著脆弱性，现有轻量级防御机制对复杂攻击效果有限，需要更强大的安全措施来保护LLM部署。

Abstract: Large Language Models (LLMs) are widely deployed in real-world systems. Given their broader applicability, prompt engineering has become an efficient tool for resource-scarce organizations to adopt LLMs for their own purposes. At the same time, LLMs are vulnerable to prompt-based attacks. Thus, analyzing this risk has become a critical security requirement. This work evaluates prompt-injection and jailbreak vulnerability using a large, manually curated dataset across multiple open-source LLMs, including Phi, Mistral, DeepSeek-R1, Llama 3.2, Qwen, and Gemma variants. We observe significant behavioural variation across models, including refusal responses and complete silent non-responsiveness triggered by internal safety mechanisms. Furthermore, we evaluated several lightweight, inference-time defence mechanisms that operate as filters without any retraining or GPU-intensive fine-tuning. Although these defences mitigate straightforward attacks, they are consistently bypassed by long, reasoning-heavy prompts.

</details>


### [67] [A Lightweight Defense Mechanism against Next Generation of Phishing Emails using Distilled Attention-Augmented BiLSTM](https://arxiv.org/abs/2602.22250)
*Morteza Eskandarian,Mahdi Rabbani,Arun Kaniyamattam,Fatemeh Nejati,Mansur Mirani,Gunjan Piya,Igor Opushnyev,Ali A. Ghorbani,Sajjad Dadkhah*

Main category: cs.CR

TL;DR: 提出一种基于MobileBERT蒸馏到BiLSTM+多头注意力的轻量级模型，用于检测邮件网关和终端中的LLM生成的社会工程攻击内容，在保持高精度的同时实现快速推理和小模型尺寸。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型能生成复杂的社会工程内容，绕过商业通信平台的标准文本筛查系统，需要一种隐私保护且满足性能要求的检测解决方案。

Method: 使用MobileBERT作为教师模型进行微调，然后蒸馏到具有多头注意力的BiLSTM模型；构建包含人工撰写消息和LLM生成改写（使用掩码技术和个性化方法）的混合数据集；采用五种测试协议评估性能。

Result: 蒸馏模型仅450万参数，相比强Transformer基线（ModernBERT、DeBERTaV3-base等）的混合分割结果，加权F1分数差异仅1-2.5点，同时推理时间快80-95%，模型尺寸小95-99%。

Conclusion: 该系统在准确性、延迟和紧凑性方面表现优异，支持无加速硬件的实时过滤和基于策略的管理，适合高流量条件下的隐私保护部署。

Abstract: The current generation of large language models produces sophisticated social-engineering content that bypasses standard text screening systems in business communication platforms. Our proposed solution for mail gateway and endpoint deception detection operates in a privacy-protective manner while handling the performance requirements of network and mobile security systems. The MobileBERT teacher receives fine-tuning before its transformation into a BiLSTM model with multi-head attention which maintains semantic discrimination only with 4.5 million parameters. The hybrid dataset contains human-written messages together with LLM-generated paraphrases that use masking techniques and personalization methods to enhance modern attack resistance. The evaluation system uses five testing protocols which include human-only and LLM-only tests and two cross-distribution transfer tests and a production-like mixed traffic test to assess performance in native environments and across different distribution types and combined traffic scenarios. The distilled model maintains a weighted-F1 score difference of 1-2.5 points compared to the mixture split results of strong transformer baselines including ModernBERT, DeBERTaV3-base, T5-base, DeepSeek-R1 Distill Qwen-1.5B and Phi-4 mini while achieving 80-95\% faster inference times and 95-99\% smaller model sizes. The system demonstrates excellent performance in terms of accuracy and latency while maintaining a compact size which enables real-time filtering without acceleration hardware and supports policy-based management. The paper examines system performance under high traffic conditions and security measures for privacy protection and implementation methods for operational deployment.

</details>


### [68] [Differentially Private Truncation of Unbounded Data via Public Second Moments](https://arxiv.org/abs/2602.22282)
*Zilong Cao,Xuan Bi,Hai Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为PMT（公共矩引导截断）的方法，通过利用少量公共数据的二阶矩信息，解决差分隐私在无界分布数据上的局限性，显著提升DP模型的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）通常只适用于有界分布的数据，但在实际应用中数据分布往往是无界的。论文旨在解决这一限制，通过利用少量公共数据的二阶矩信息来扩展DP的应用范围。

Method: 提出PMT方法：1）利用公共数据的二阶矩矩阵对私有数据进行变换；2）应用基于数据维度和样本量等非私有量的截断机制；3）设计新的损失函数和算法，确保变换空间中的解能映射回原始域；4）应用于惩罚回归和广义线性回归。

Result: PMT方法显著改善了DP估计：1）获得条件良好的二阶矩矩阵，增强对DP噪声的抵抗能力；2）通过理论误差界、鲁棒性保证和收敛结果证明了模型改进；3）在合成和真实数据集上的实验证实了PMT能大幅提升DP模型的准确性和稳定性。

Conclusion: PMT方法通过利用少量公共数据的二阶矩信息，有效解决了差分隐私在无界分布数据上的局限性，显著提升了DP模型的性能，为隐私保护机器学习提供了更强大的工具。

Abstract: Data privacy is important in the AI era, and differential privacy (DP) is one of the golden solutions. However, DP is typically applicable only if data have a bounded underlying distribution. We address this limitation by leveraging second-moment information from a small amount of public data. We propose Public-moment-guided Truncation (PMT), which transforms private data using the public second-moment matrix and applies a principled truncation whose radius depends only on non-private quantities: data dimension and sample size. This transformation yields a well-conditioned second-moment matrix, enabling its inversion with a significantly strengthened ability to resist the DP noise. Furthermore, we demonstrate the applicability of PMT by using penalized and generalized linear regressions. Specifically, we design new loss functions and algorithms, ensuring that solutions in the transformed space can be mapped back to the original domain. We have established improvements in the models' DP estimation through theoretical error bounds, robustness guarantees, and convergence results, attributing the gains to the conditioning effect of PMT. Experiments on synthetic and real datasets confirm that PMT substantially improves the accuracy and stability of DP models.

</details>


### [69] [HubScan: Detecting Hubness Poisoning in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2602.22427)
*Idan Habler,Vineeth Sai Narajala,Stav Koren,Amy Chang,Tiffany Saade*

Main category: cs.CR

TL;DR: Hubscan是一个开源安全扫描器，用于检测RAG系统中的hubness安全漏洞，通过多检测器架构识别频繁出现在检索结果中的有害枢纽文档。


<details>
  <summary>Details</summary>
Motivation: RAG系统存在hubness安全漏洞，即某些文档会频繁出现在大量不同查询的检索结果中，这些枢纽文档可被利用来注入有害内容、操纵搜索排名、绕过内容过滤并降低系统性能。

Method: Hubscan采用多检测器架构，包括：(1)基于中位数/MAD的稳健统计hubness检测，(2)聚类分布分析评估跨聚类检索模式，(3)查询扰动下的稳定性测试，(4)领域感知和模态感知检测。支持多种向量数据库和检索技术。

Result: 在Food-101、MS-COCO和FiQA对抗性hubness基准测试中，hubscan在0.2%警报预算下达到90%召回率，0.4%时达到100%召回率，对抗性枢纽排名在99.8百分位以上。领域限定扫描可检测到100%逃避全局检测的针对性攻击。

Conclusion: Hubscan为生产环境RAG系统提供了一个实用、可扩展的hubness威胁检测框架，能有效识别安全漏洞并保护系统免受对抗性攻击。

Abstract: Retrieval-Augmented Generation (RAG) systems are essential to contemporary AI applications, allowing large language models to obtain external knowledge via vector similarity search. Nevertheless, these systems encounter a significant security flaw: hubness - items that frequently appear in the top-k retrieval results for a disproportionately high number of varied queries. These hubs can be exploited to introduce harmful content, alter search rankings, bypass content filtering, and decrease system performance.
  We introduce hubscan, an open-source security scanner that evaluates vector indices and embeddings to identify hubs in RAG systems. Hubscan presents a multi-detector architecture that integrates: (1) robust statistical hubness detection utilizing median/MAD-based z-scores, (2) cluster spread analysis to assess cross-cluster retrieval patterns, (3) stability testing under query perturbations, and (4) domain-aware and modality-aware detection for category-specific and cross-modal attacks. Our solution accommodates several vector databases (FAISS, Pinecone, Qdrant, Weaviate) and offers versatile retrieval techniques, including vector similarity, hybrid search, and lexical matching with reranking capabilities.
  We evaluate hubscan on Food-101, MS-COCO, and FiQA adversarial hubness benchmarks constructed using state-of-the-art gradient-optimized and centroid-based hub generation methods. hubscan achieves 90% recall at a 0.2% alert budget and 100% recall at 0.4%, with adversarial hubs ranking above the 99.8th percentile. Domain-scoped scanning recovers 100% of targeted attacks that evade global detection. Production validation on 1M real web documents from MS MARCO demonstrates significant score separation between clean documents and adversarial content. Our work provides a practical, extensible framework for detecting hubness threats in production RAG systems.

</details>


### [70] [Differentially Private Data-Driven Markov Chain Modeling](https://arxiv.org/abs/2602.22443)
*Alexander Benvenuti,Brandon Fallin,Calvin Hawkins,Brendan Bialy,Miriam Dennis,Warren Dixon,Matthew Hale*

Main category: cs.CR

TL;DR: 提出一种保护用户数据隐私的马尔可夫链建模方法，通过差分隐私技术处理数据库查询，在保持模型准确性的同时保护用户敏感信息。


<details>
  <summary>Details</summary>
Motivation: 马尔可夫链广泛用于建模用户行为，但构建准确模型需要大量用户数据，而共享这些模型可能泄露底层用户数据的敏感信息。需要一种既能保护用户隐私又能保持模型准确性的方法。

Method: 开发了一种保护数据库查询输出的方法，这些输出是单位单纯形中的元素。该方法被证明是差分隐私的，并通过KL散度量化准确性。将此方法扩展到随机矩阵的隐私化处理，其中每行都是数据库的单纯形值查询，包括数据驱动的马尔可夫链模型。

Result: 分析了私有和非私有马尔可夫链模型之间平稳分布和收敛速率的变化界限。模拟显示，在典型的隐私实现下，该方法在平稳分布上的误差小于2%，表明该方法能够忠实捕获所研究系统的行为。

Conclusion: 提出的差分隐私方法能够在保护用户数据隐私的同时，保持马尔可夫链模型的准确性，为隐私保护的马尔可夫链建模提供了有效解决方案。

Abstract: Markov chains model a wide range of user behaviors. However, generating accurate Markov chain models requires substantial user data, and sharing these models without privacy protections may reveal sensitive information about the underlying user data. We introduce a method for protecting user data used to formulate a Markov chain model. First, we develop a method for privatizing database queries whose outputs are elements of the unit simplex, and we prove that this method is differentially private. We quantify its accuracy by bounding the expected KL divergence between private and non-private queries. We extend this method to privatize stochastic matrices whose rows are each a simplex-valued query of a database, which includes data-driven Markov chain models. To assess their accuracy, we analytically bound the change in the stationary distribution and the change in the convergence rate between a non-private Markov chain model and its private form. Simulations show that under a typical privacy implementation, our method yields less than 2% error in the stationary distribution, indicating that our approach to private modeling faithfully captures the behavior of the systems we study.

</details>


### [71] [IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation](https://arxiv.org/abs/2602.22700)
*Yanpei Guo,Wenjie Qu,Linyu Wu,Shengfang Zhai,Lionel Z. Wang,Ming Xu,Yue Liu,Binhang Yuan,Dawn Song,Jiaheng Zhang*

Main category: cs.CR

TL;DR: IMMACULATE是一个实用的审计框架，用于检测商业大语言模型API服务中的经济动机偏差（如模型替换、量化滥用和代币超额计费），无需可信硬件或访问模型内部，通过选择性审计和可验证计算实现强检测保证。


<details>
  <summary>Details</summary>
Motivation: 商业大语言模型通常作为黑盒API服务部署，用户需要信任提供商正确执行推理并如实报告代币使用情况。当前缺乏有效机制来检测提供商可能存在的经济动机偏差，如使用廉价模型替代、滥用量化技术或虚报代币使用量以增加收入。

Method: IMMACULATE采用选择性审计策略，仅对一小部分请求进行验证，通过可验证计算技术检测偏差。框架不需要可信硬件或访问模型内部，通过密码学方法验证推理执行的正确性和代币计费的准确性，同时通过摊销密码学开销来保持实用性。

Result: 实验在密集模型和MoE模型上进行，结果显示IMMACULATE能够可靠地区分良性执行和恶意执行，同时保持低于1%的吞吐量开销。框架有效检测模型替换、量化滥用和代币超额计费等偏差。

Conclusion: IMMACULATE提供了一个实用的解决方案，用于审计商业大语言模型API服务，确保提供商不会因经济动机而偏离承诺的服务质量。该框架在保持高性能的同时提供强检测保证，有助于建立更透明可信的AI服务生态系统。

Abstract: Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model substitution, quantization abuse, and token overbilling-without trusted hardware or access to model internals. IMMACULATE selectively audits a small fraction of requests using verifiable computation, achieving strong detection guarantees while amortizing cryptographic overhead. Experiments on dense and MoE models show that IMMACULATE reliably distinguishes benign and malicious executions with under 1% throughput overhead. Our code is published at https://github.com/guo-yanpei/Immaculate.

</details>
