<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [OpenEstimate: Evaluating LLMs on Reasoning Under Uncertainty with Real-World Data](https://arxiv.org/abs/2510.15096)
*Alana Renda,Jillian Ross,Michael Cafarella,Jacob Andreas*

Main category: cs.AI

TL;DR: OpenEstimate是一个可扩展的多领域基准测试，用于评估语言模型在需要综合大量背景信息并以概率先验形式表达预测的数值估计任务中的表现。研究发现当前前沿语言模型产生的先验往往不准确且过度自信。


<details>
  <summary>Details</summary>
Motivation: 现实世界中语言模型部署的环境需要处理不完整信息和在不确定性下进行推理，但现有评估主要关注有明确答案的问题。由于语言模型与人类拥有相似的知识，设计能有效测试其不确定性推理能力的问题具有挑战性。

Method: 引入OpenEstimate基准测试，评估语言模型在数值估计任务中的表现，要求模型综合背景信息并表达概率先验。评估这些先验的准确性和校准度，量化其相对于真实分布的实用性。

Result: 在六个前沿语言模型上的测试显示，模型产生的先验通常不准确且过度自信。性能在不同不确定性提取方式下略有改善，但在采样策略、推理努力或提示设计变化下基本不受影响。

Conclusion: OpenEstimate基准为前沿语言模型提供了一个具有挑战性的评估平台，并为开发在概率估计和不确定性推理方面表现更好的模型奠定了基础。

Abstract: Real-world settings where language models (LMs) are deployed -- in domains
spanning healthcare, finance, and other forms of knowledge work -- require
models to grapple with incomplete information and reason under uncertainty. Yet
most LM evaluations focus on problems with well-defined answers and success
criteria. This gap exists in part because natural problems involving
uncertainty are difficult to construct: given that LMs have access to most of
the same knowledge as humans, it is non-trivial to design questions for which
LMs will struggle to produce correct answers, but which humans can answer
reliably. As a result, LM performance on reasoning under uncertainty remains
poorly characterized. To address this gap, we introduce OpenEstimate, an
extensible, multi-domain benchmark for evaluating LMs on numerical estimation
tasks that require models to synthesize significant amounts of background
information and express predictions as probabilistic priors. We assess these
priors for accuracy and calibration, quantifying their usefulness relative to
samples from the true distribution of interest. Across six frontier LMs, we
find that LM-elicited priors are often inaccurate and overconfident.
Performance improves modestly depending on how uncertainty is elicited from the
model, but is largely unaffected by changes in sampling strategy, reasoning
effort, or prompt design. The OpenEstimate benchmark thus offers a challenging
evaluation for frontier LMs and a platform for developing models that are
better at probabilistic estimation and reasoning under uncertainty.

</details>


### [2] [Procedural Game Level Design with Deep Reinforcement Learning](https://arxiv.org/abs/2510.15120)
*Miraç Buğra Özkan*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习（DRL）的程序化关卡设计方法，在Unity 3D环境中使用两个智能体：蜂鸟智能体作为解谜者，浮岛智能体负责生成和放置可收集物品（花朵），通过PPO算法训练实现动态关卡生成和解决。


<details>
  <summary>Details</summary>
Motivation: 程序化内容生成（PCG）已成为游戏开发中越来越受欢迎的技术，可以减少手动工作量，生成动态、可重玩和可扩展的环境。本研究旨在探索深度强化学习在自主游戏关卡设计中的应用潜力。

Method: 使用Unity ML-Agents工具包中的PPO算法训练两个智能体：蜂鸟智能体学习在动态地形中高效导航、定位和收集花朵；浮岛智能体学习根据障碍物位置、蜂鸟初始状态和先前表现反馈生成花朵布局。两个智能体通过交互产生涌现行为。

Result: 该方法不仅产生了有效且高效的智能体行为，还在各种环境配置中展现出强大的泛化能力。智能体之间的交互导致了涌现行为和稳健的泛化性能。

Conclusion: 这项工作突显了深度强化学习在使智能体能够在虚拟环境中生成和解决内容方面的潜力，推动了AI在创意游戏开发过程中的贡献边界，为基于机器学习的自主游戏关卡设计开辟了新机遇。

Abstract: Procedural content generation (PCG) has become an increasingly popular
technique in game development, allowing developers to generate dynamic,
replayable, and scalable environments with reduced manual effort. In this
study, a novel method for procedural level design using Deep Reinforcement
Learning (DRL) within a Unity-based 3D environment is proposed. The system
comprises two agents: a hummingbird agent, acting as a solver, and a floating
island agent, responsible for generating and placing collectible objects
(flowers) on the terrain in a realistic and context-aware manner. The
hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm
from the Unity ML-Agents toolkit. It learns to navigate through the terrain
efficiently, locate flowers, and collect them while adapting to the
ever-changing procedural layout of the island. The island agent is also trained
using the Proximal Policy Optimization (PPO) algorithm. It learns to generate
flower layouts based on observed obstacle positions, the hummingbird's initial
state, and performance feedback from previous episodes. The interaction between
these agents leads to emergent behavior and robust generalization across
various environmental configurations. The results demonstrate that the approach
not only produces effective and efficient agent behavior but also opens up new
opportunities for autonomous game level design driven by machine learning. This
work highlights the potential of DRL in enabling intelligent agents to both
generate and solve content in virtual environments, pushing the boundaries of
what AI can contribute to creative game development processes.

</details>


### [3] [Towards Error Centric Intelligence I, Beyond Observational Learning](https://arxiv.org/abs/2510.15128)
*Marcus A. Thomas*

Main category: cs.AI

TL;DR: 该论文认为AGI进展受理论限制而非数据或规模限制，提出因果机制优先的方法，通过错误发现和修正来扩展假设空间。


<details>
  <summary>Details</summary>
Motivation: 挑战柏拉图表示假说，指出仅凭观测充分性无法保证干预能力，因为观测等价的世界在干预下可能分化。

Method: 提出因果机制方法，将假设空间变化作为一等操作，使用局部性、自主性和组合自主性等结构原则来使错误发现和修正变得可行。

Result: 建立了使不可达错误转化为可达错误的框架，提出了模块化干预的局部性和自主性原则、独立因果机制的规范不变形式以及类比保持的组合自主性原则。

Conclusion: AGI进展需要从观测学习转向错误中心的范式，通过因果机制和结构原则来系统性地发现和修正错误，从而扩展智能体的能力。

Abstract: We argue that progress toward AGI is theory limited rather than data or scale
limited. Building on the critical rationalism of Popper and Deutsch, we
challenge the Platonic Representation Hypothesis. Observationally equivalent
worlds can diverge under interventions, so observational adequacy alone cannot
guarantee interventional competence. We begin by laying foundations,
definitions of knowledge, learning, intelligence, counterfactual competence and
AGI, and then analyze the limits of observational learning that motivate an
error centric shift. We recast the problem as three questions about how
explicit and implicit errors evolve under an agent's actions, which errors are
unreachable within a fixed hypothesis space, and how conjecture and criticism
expand that space. From these questions we propose Causal Mechanics, a
mechanisms first program in which hypothesis space change is a first class
operation and probabilistic structure is used when useful rather than presumed.
We advance structural principles that make error discovery and correction
tractable, including a differential Locality and Autonomy Principle for modular
interventions, a gauge invariant form of Independent Causal Mechanisms for
separability, and the Compositional Autonomy Principle for analogy
preservation, together with actionable diagnostics. The aim is a scaffold for
systems that can convert unreachable errors into reachable ones and correct
them.

</details>


### [4] [HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks](https://arxiv.org/abs/2510.15144)
*Chance Jiajie Li,Zhenze Mo,Yuhan Tang,Ao Qu,Jiayi Wu,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Hang Jiang,Paul Pu Liang,Jinhua Zhao,Luis Alberto Alonso Pastor,Kent Larson*

Main category: cs.AI

TL;DR: HugAgent是一个用于评估AI模型从群体推理到个体推理适应能力的基准测试，包含合成和人类双轨设计，旨在让机器推理更贴近人类思维的个体性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然能大规模近似人类响应，但主要针对群体共识，缺乏对个体推理风格和信念轨迹的个性化捕捉。

Method: 采用双轨设计：合成轨道用于规模和系统性压力测试，人类轨道用于生态有效的"出声思考"推理数据收集。

Result: 实验显示最先进的LLMs在适应个体推理方面存在持续差距。

Conclusion: HugAgent是首个可扩展的基准测试，用于将机器推理与人类思维的个体性对齐。

Abstract: Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

</details>


### [5] [WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing](https://arxiv.org/abs/2510.15221)
*Xiao Sun*

Main category: cs.AI

TL;DR: 提出了一个包含733,651个面部表情记录的大规模纵向工作场所情感数据集，收集自38名员工超过30.5个月，包含7种情绪概率和32个扩展情感指标，技术验证显示高质量数据并实现91.2%的情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决真实工作环境中情感识别因缺乏大规模、长期自然收集数据集而面临的挑战，捕捉COVID-19疫情期间的情感反应。

Method: 在真实办公环境中收集面部表情数据，使用深度学习进行情感识别，计算32个扩展情感指标，并通过随机森林和LSTM模型进行基线实验。

Result: 数据集成功复制已知心理模式（周末效应：+192%效价改善），员工流动预测AUC=1.0，情感分类准确率91.2%，效价预测R2=0.84。

Conclusion: 这是公开可用的最大、最长期的工作场所情感数据集，支持情感识别、情感动态建模、情感传染、流动预测和情感感知系统设计研究。

Abstract: Automated emotion recognition in real-world workplace settings remains a
challenging problem in affective computing due to the scarcity of large-scale,
longitudinal datasets collected in naturalistic environments. We present a
novel dataset comprising 733,651 facial expression records from 38 employees
collected over 30.5 months (November 2021 to May 2024) in an authentic office
environment. Each record contains seven emotion probabilities (neutral, happy,
sad, surprised, fear, disgusted, angry) derived from deep learning-based facial
expression recognition, along with comprehensive metadata including job roles,
employment outcomes, and personality traits. The dataset uniquely spans the
COVID-19 pandemic period, capturing emotional responses to major societal
events including the Shanghai lockdown and policy changes. We provide 32
extended emotional metrics computed using established affective science
methods, including valence, arousal, volatility, predictability, inertia, and
emotional contagion strength. Technical validation demonstrates high data
quality through successful replication of known psychological patterns (weekend
effect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and
perfect predictive validity for employee turnover (AUC=1.0). Baseline
experiments using Random Forest and LSTM models achieve 91.2% accuracy for
emotion classification and R2 = 0.84 for valence prediction. This is the
largest and longest longitudinal workplace emotion dataset publicly available,
enabling research in emotion recognition, affective dynamics modeling,
emotional contagion, turnover prediction, and emotion-aware system design.

</details>


### [6] [From Checklists to Clusters: A Homeostatic Account of AGI Evaluation](https://arxiv.org/abs/2510.15236)
*Brett Reynolds*

Main category: cs.AI

TL;DR: 本文提出AGI评估应该采用非对称权重和持久性测试，将通用智能视为稳态属性集群，而非简单的多领域能力组合。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估存在两个问题：对称权重假设所有领域同等重要，与人类智能研究不符；快照测试无法区分持久能力和脆弱表现。

Method: 提出两种评估扩展：基于因果中心性的权重评分（导入CHC理论权重）和集群稳定性指数系列（评估配置文件持久性、持久学习和错误纠正）。

Result: 这些扩展保持了多领域广度，同时减少了脆弱性和博弈行为，提供了无需架构访问的黑盒测试协议。

Conclusion: 通用智能应被理解为稳态属性集群，AGI评估需要衡量能力在扰动下的共存机制，而不仅仅是能力的存在。

Abstract: Contemporary AGI evaluations report multidomain capability profiles, yet they
typically assign symmetric weights and rely on snapshot scores. This creates
two problems: (i) equal weighting treats all domains as equally important when
human intelligence research suggests otherwise, and (ii) snapshot testing can't
distinguish durable capabilities from brittle performances that collapse under
delay or stress. I argue that general intelligence -- in humans and potentially
in machines -- is better understood as a homeostatic property cluster: a set of
abilities plus the mechanisms that keep those abilities co-present under
perturbation. On this view, AGI evaluation should weight domains by their
causal centrality (their contribution to cluster stability) and require
evidence of persistence across sessions. I propose two battery-compatible
extensions: a centrality-prior score that imports CHC-derived weights with
transparent sensitivity analysis, and a Cluster Stability Index family that
separates profile persistence, durable learning, and error correction. These
additions preserve multidomain breadth while reducing brittleness and gaming. I
close with testable predictions and black-box protocols labs can adopt without
architectural access.

</details>


### [7] [Experience-Driven Exploration for Efficient API-Free AI Agents](https://arxiv.org/abs/2510.15259)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Zizhou Wang,Jiawei Du,Liangli Zhen,Jiancheng Lv*

Main category: cs.AI

TL;DR: KG-Agent是一个基于经验驱动的学习框架，通过构建状态-动作知识图来解决API缺失环境下GUI操作的效率瓶颈问题，在复杂决策环境中显著提升了探索效率和策略深度。


<details>
  <summary>Details</summary>
Motivation: 现有软件大多缺乏可访问的API，智能体只能通过像素级GUI进行操作，导致决策短视且依赖低效的试错，阻碍了技能获取和长期规划。

Method: 提出KG-Agent框架，将原始像素级交互构建为持久的状态-动作知识图(SA-KG)，连接功能相似但视觉不同的GUI状态，并设计基于图拓扑的混合内在奖励机制，结合状态价值奖励和新颖性奖励。

Result: 在Civilization V和Slay the Spire两个复杂开放GUI决策环境中评估，相比最先进方法在探索效率和策略深度方面有显著提升。

Conclusion: KG-Agent通过结构化经验知识图成功解决了API缺失环境下的效率瓶颈，实现了更有效的探索和长期规划能力。

Abstract: Most existing software lacks accessible Application Programming Interfaces
(APIs), requiring agents to operate solely through pixel-based Graphical User
Interfaces (GUIs). In this API-free setting, large language model (LLM)-based
agents face severe efficiency bottlenecks: limited to local visual experiences,
they make myopic decisions and rely on inefficient trial-and-error, hindering
both skill acquisition and long-term planning. To address these challenges, we
propose KG-Agent, an experience-driven learning framework that structures an
agent's raw pixel-level interactions into a persistent State-Action Knowledge
Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking
functionally similar but visually distinct GUI states, forming a rich
neighborhood of experience that enables the agent to generalize from a diverse
set of historical strategies. To support long-horizon reasoning, we design a
hybrid intrinsic reward mechanism based on the graph topology, combining a
state value reward for exploiting known high-value pathways with a novelty
reward that encourages targeted exploration. This approach decouples strategic
planning from pure discovery, allowing the agent to effectively value setup
actions with delayed gratification. We evaluate KG-Agent in two complex,
open-ended GUI-based decision-making environments (Civilization V and Slay the
Spire), demonstrating significant improvements in exploration efficiency and
strategic depth over the state-of-the-art methods.

</details>


### [8] [AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory](https://arxiv.org/abs/2510.15261)
*Jitesh Jain,Shubham Maheshwari,Ning Yu,Wen-mei Hwu,Humphrey Shi*

Main category: cs.AI

TL;DR: AUGUSTUS是一个多模态智能体系统，采用图结构多模态上下文记忆，将信息概念化为语义标签，在ImageNet分类任务中比传统多模态RAG方法快3.5倍，在MSC基准测试中优于MemGPT。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统主要存储文本信息，忽视了多模态信号的重要性。受人类记忆多模态特性的启发，希望构建符合认知科学中人类记忆理念的多模态智能体系统。

Method: 系统包含4个循环阶段：编码（理解输入）、存储到记忆（保存重要信息）、检索（从记忆中搜索相关上下文）、行动（执行任务）。使用图结构多模态上下文记忆，将信息概念化为语义标签并与上下文关联。

Result: 在ImageNet分类任务中比传统多模态RAG方法快3.5倍，在MSC基准测试中优于MemGPT。

Conclusion: AUGUSTUS系统通过图结构多模态上下文记忆实现了高效的概念驱动检索，在多模态任务中表现出色，验证了基于认知科学原理的多模态记忆设计的有效性。

Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.

</details>


### [9] [VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data](https://arxiv.org/abs/2510.15317)
*Tingqiao Xu,Ziru Zeng,Jiayu Chen*

Main category: cs.AI

TL;DR: VERITAS是一个通过整合视觉先验和多模态大模型来提升监督微调数据质量的管道，能有效减少事实错误和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型的数据增强方法由于视觉感知不足，经常出现事实错误和幻觉问题，影响了模型性能。

Method: 使用视觉识别模型和OCR系统提取结构化视觉先验，结合三个LMM模型评估原始答案，通过统计融合得到高置信度共识分数作为真实标签，训练轻量级批评模型并优化答案。

Result: 在六个多模态基准测试中，使用VERITAS处理数据微调的模型表现优于使用原始数据的模型，特别是在文本丰富和细粒度推理任务中。批评模型能力接近最先进LMM但效率显著更高。

Conclusion: VERITAS能有效提升多模态数据质量，增强模型推理能力，同时保持高效性，为多模态数据优化研究提供了有价值的工具和资源。

Abstract: The quality of supervised fine-tuning (SFT) data is crucial for the
performance of large multimodal models (LMMs), yet current data enhancement
methods often suffer from factual errors and hallucinations due to inadequate
visual perception. To address this challenge, we propose VERITAS, a pipeline
that systematically integrates vision priors and multiple state-of-the-art LMMs
with statistical methods to enhance SFT data quality. VERITAS leverages visual
recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured
vision priors, which are combined with images, questions, and answers. Three
LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers,
providing critique rationales and scores that are statistically fused into a
high-confidence consensus score serving as ground truth. Using this consensus,
we train a lightweight critic model via Group Relative Policy Optimization
(GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the
original answers based on the critiques, generating new candidate answers; we
select the highest-scoring one as the final refined answer. Experiments across
six multimodal benchmarks demonstrate that models fine-tuned with data
processed by VERITAS consistently outperform those using raw data, particularly
in text-rich and fine-grained reasoning tasks. Our critic model exhibits
enhanced capability comparable to state-of-the-art LMMs while being
significantly more efficient. We release our pipeline, datasets, and model
checkpoints to advance research in multimodal data optimization.

</details>


### [10] [Towards Flash Thinking via Decoupled Advantage Policy Optimization](https://arxiv.org/abs/2510.15374)
*Zezhong Tan,Hang Gao,Xinhong Ma,Feng Zhang,Ziqiang Dong*

Main category: cs.AI

TL;DR: DEPO是一个新的强化学习框架，通过解耦优势算法、难度感知长度惩罚和优势裁剪方法，显著减少大推理模型在简单任务上的低效推理，在保持准确性的同时降低39%的序列长度。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法虽然提高了模型准确性，但存在响应过长和过度思考问题，导致推理延迟和计算消耗增加，特别是在简单任务上需要最小推理的情况下。

Method: DEPO框架包含三个核心组件：(1)创新的优势解耦算法指导模型减少低效token；(2)难度感知长度惩罚降低模型响应整体长度；(3)优势裁剪方法防止策略优化偏差。

Result: 在DeepSeek-Distill-Qwen-7B和1.5B模型上的实验表明，DEPO实现了序列长度减少39%，同时减少了低效token中的过度推理路径，并在整体准确性上优于基础模型。

Conclusion: DEPO有效解决了大推理模型在简单任务上的低效推理问题，在保持准确性的同时显著降低了计算开销和推理延迟。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.

</details>


### [11] [MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games](https://arxiv.org/abs/2510.15414)
*Huining Yuan,Zelai Xu,Zheyue Tan,Xiangmin Yi,Mo Guang,Kaiwen Long,Haojia Hui,Boxun Li,Xinlei Chen,Bo Zhao,Xiao-Ping Zhang,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: MARS是一个端到端的强化学习框架，通过自博弈训练LLM在多智能体系统中进行推理，在合作和竞争游戏中都取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 开发能够在多智能体系统中有效合作和竞争的LLM是迈向更高级智能的关键步骤，但将强化学习扩展到多回合、多智能体场景仍面临长期信用分配和智能体特定优势估计的挑战。

Method: MARS框架包含回合级优势估计器来对齐学习信号进行信用分配，以及智能体特定优势归一化来稳定多智能体训练。通过在不同游戏中进行自博弈训练。

Result: 从Qwen3-4B训练的MARS智能体在保留游戏中实现了28.7%的性能提升，在推理基准测试中多智能体系统性能持续提升，集成到领先多智能体系统后在AIME上提升10.0%，GPQA-Diamond上提升12.5%。

Conclusion: 在战略游戏中通过自博弈进行端到端RL训练是开发LLM中可泛化多智能体推理能力的有效方法。

Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively
within multi-agent systems is a critical step towards more advanced
intelligence. While reinforcement learning (RL) has proven effective for
enhancing reasoning in single-agent tasks, its extension to multi-turn,
multi-agent scenarios remains underexplored due to the challenges of
long-horizon credit assignment and agent-specific advantage estimation. To
address these challenges, we introduce MARS, an end-to-end RL framework that
incentivizes Multi-Agent Reasoning of LLMs through Self-play in both
cooperative and competitive games. MARS features a turn-level advantage
estimator that aligns learning signals with each interaction for credit
assignment, and an agent-specific advantage normalization to stabilize
multi-agent training. By learning with self-play across cooperative and
competitive games, the MARS agent trained from Qwen3-4B develops strong
strategic abilities that generalize to held-out games with up to 28.7%
performance improvements. More importantly, the capability acquired through
self-play generalizes beyond games, yielding consistent performance gains of
multi-agent systems in reasoning benchmarks. When integrated into leading
multi-agent systems, our MARS agent achieves significant performance gains of
10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL
training with self-play in strategic games as a powerful approach for
developing generalizable multi-agent reasoning capabilities in LLMs. Our code
and models are publicly available at https://github.com/thu-nics/MARS.

</details>


### [12] [Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning](https://arxiv.org/abs/2510.15514)
*Boyin Liu,Zhuo Zhang,Sen Huang,Lipeng Xie,Qingxu Fu,Haoran Chen,LI YU,Tianyi Hu,Zhaoyang Liu,Bolin Ding,Dongbin Zhao*

Main category: cs.AI

TL;DR: 本文提出了一个检测和解决强化学习中判断不一致性的框架，包含冲突检测率（CDR）指标和去冲突图奖励（DGR）方法，通过构建无环图来提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强化学习中面临判断不一致的问题，特别是偏好循环等逻辑一致性问题尚未得到充分解决，这会影响强化学习的稳定性。

Method: 提出CDR指标量化判断冲突，并开发DGR框架：从初始判断构建偏好图，将其转换为无冲突的有向无环图（DAG），生成逻辑一致的奖励信号，可与任何策略优化器兼容。

Result: 实验结果表明，该框架相比强基线方法显著提高了训练稳定性和模型性能。

Conclusion: 逻辑一致性是AI反馈中关键且现在可管理的维度，该框架为解决判断不一致问题提供了有效方案。

Abstract: However, this method often faces judgment inconsistencies that can
destabilize reinforcement learning. While prior research has focused on the
accuracy of judgments, the critical issue of logical coherence especially
issues such as preference cycles hasn't been fully addressed. To fill this gap,
we introduce a comprehensive framework designed to systematically detect and
resolve these inconsistencies during the reinforcement learning training
process. Our framework includes two main contributions: first, the Conflict
Detection Rate (CDR), a new metric that quantifies judgment conflicts, and
second, Deconflicted Graph Rewards (DGR), a framework that purifies signals by
removing cycles before policy optimization. DGR constructs preference graphs
from the initial judgments, transforms them into conflict-free Directed Acyclic
Graphs (DAGs), and generates a logically coherent reward signal that is
compatible with any policy optimizer. Experimental results show that our
framework significantly enhances training stability and model performance
compared to strong baselines, establishing logical consistency as a crucial and
now manageable dimension of AI feedback.

</details>


### [13] [JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament](https://arxiv.org/abs/2510.15560)
*Jiayuan Bai,Xuan-guang Pan,Chongyang Tao,Shuai Ma*

Main category: cs.AI

TL;DR: JudgeSQL是一个用于Text-to-SQL任务中SQL候选查询选择的框架，通过结构化推理和加权共识锦标赛机制解决现有选择方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的SQL候选选择方法（如自一致性或最佳N解码）仅提供浅层信号，容易导致评分不一致、推理链脆弱，无法捕捉密切相关的SQL候选之间的细粒度语义差异。

Method: 开发基于推理的SQL判断模型，通过强化学习在可验证奖励指导下提炼推理轨迹；构建加权共识锦标赛，将显式推理偏好与隐式生成器置信度相结合。

Result: 在BIRD基准测试上的广泛实验表明，JudgeSQL展现出卓越的SQL判断能力、良好的跨尺度泛化能力以及对生成器容量的鲁棒性。

Conclusion: JudgeSQL通过结构化推理和加权共识机制重新定义了SQL候选选择，提供了更可靠和高效的查询选择方法。

Abstract: Text-to-SQL is a pivotal task that bridges natural language understanding and
structured data access, yet it remains fundamentally challenging due to
semantic ambiguity and complex compositional reasoning. While large language
models (LLMs) have greatly advanced SQL generation though prompting, supervised
finetuning and reinforced tuning, the shift toward test-time scaling exposes a
new bottleneck: selecting the correct query from a diverse candidate pool.
Existing selection approaches, such as self-consistency or best-of-$N$
decoding, provide only shallow signals, making them prone to inconsistent
scoring, fragile reasoning chains, and a failure to capture fine-grained
semantic distinctions between closely related SQL candidates. To this end, we
introduce JudgeSQL, a principled framework that redefines SQL candidate
selection through structured reasoning and weighted consensus tournament
mechanism. JudgeSQL develops a reasoning-based SQL judge model that distills
reasoning traces with reinforcement learning guided by verifiable rewards,
enabling accurate and interpretable judgments. Building on this, a weighted
consensus tournament integrates explicit reasoning preferences with implicit
generator confidence, yielding selections that are both more reliable and more
efficient. Extensive experiments on the BIRD benchmark demonstrate that
JudgeSQL exhibits superior SQL judgment capabilities and good cross-scale
generalization and robustness to generator capacity.

</details>


### [14] [Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment](https://arxiv.org/abs/2510.15591)
*Lavanya Umapathy,Patricia M Johnson,Tarun Dutt,Angela Tong,Madhur Nayan,Hersh Chandarana,Daniel K Sodickson*

Main category: cs.AI

TL;DR: 开发了一个机器学习框架，整合患者既往就诊的时间上下文信息来改进健康监测，特别是在既往就诊次数有限且频率不固定的情况下。该模型首先基于最近一次就诊数据估计初始疾病风险，然后利用既往影像学和临床生物标志物信息进行精炼。


<details>
  <summary>Details</summary>
Motivation: 医学中的时间上下文对于评估患者健康状况随时间变化具有重要价值，特别是在既往就诊次数有限且频率不固定的情况下，需要开发能够整合这些信息的框架来改进健康监测。

Method: 开发了一个机器学习框架，首先基于最近一次就诊的医学数据估计初始疾病风险，然后利用从既往收集的影像学和/或临床生物标志物中提取的信息来精炼这一评估。

Result: 在前列腺癌风险预测应用中，整合既往上下文信息直接将假阳性转化为真阴性，在保持高灵敏度的同时提高了总体特异性。当整合多达三次既往影像学检查信息时，假阳性率从51%逐步降低到33%，当还包括既往临床数据时进一步降低到24%。对于预测就诊后五年内前列腺癌风险，整合既往上下文信息使假阳性率从64%降低到9%。

Conclusion: 随时间收集的信息提供了相关上下文，能够增强医学风险预测的特异性。对于广泛的进展性疾病，利用上下文充分降低假阳性率可以为将纵向健康监测项目扩展到基线疾病风险相对较低的大规模人群提供途径，从而实现早期检测和改善健康结果。

Abstract: Temporal context in medicine is valuable in assessing key changes in patient
health over time. We developed a machine learning framework to integrate
diverse context from prior visits to improve health monitoring, especially when
prior visits are limited and their frequency is variable. Our model first
estimates initial risk of disease using medical data from the most recent
patient visit, then refines this assessment using information digested from
previously collected imaging and/or clinical biomarkers. We applied our
framework to prostate cancer (PCa) risk prediction using data from a large
population (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931
blood tests) collected over nearly a decade. For predictions of the risk of
clinically significant PCa at the time of the visit, integrating prior context
directly converted false positives to true negatives, increasing overall
specificity while preserving high sensitivity. False positive rates were
reduced progressively from 51% to 33% when integrating information from up to
three prior imaging examinations, as compared to using data from a single
visit, and were further reduced to 24% when also including additional context
from prior clinical data. For predicting the risk of PCa within five years of
the visit, incorporating prior context reduced false positive rates still
further (64% to 9%). Our findings show that information collected over time
provides relevant context to enhance the specificity of medical risk
prediction. For a wide range of progressive conditions, sufficient reduction of
false positive rates using context could offer a pathway to expand longitudinal
health monitoring programs to large populations with comparatively low baseline
risk of disease, leading to earlier detection and improved health outcomes.

</details>


### [15] [Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism](https://arxiv.org/abs/2510.15600)
*Haoran Sun,Yankai Jiang,Zhenyu Tang,Yaning Pan,Shuang Gu,Zekai Lin,Lilong Wang,Wenjie Lou,Lei Liu,Lei Bai,Xiaosong Wang*

Main category: cs.AI

TL;DR: 本文提出了Thoth模型，通过"草图-填充"范式和结构化组件奖励机制，显著提升了科学实验协议生成的完整性和一致性，在多个基准测试中超越了现有LLMs。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的科学实验协议往往不完整或不一致，限制了其在可重复科学研究中的实用性。

Method: 提出"草图-填充"范式分离分析、结构和表达步骤；引入结构化组件奖励机制评估步骤粒度、动作顺序和语义保真度；开发Thoth模型采用分阶段的知识到行动训练过程。

Result: Thoth在多个基准测试中持续超越专有和开源LLMs，在步骤对齐、逻辑排序和语义准确性方面取得显著改进。

Conclusion: 该方法为构建可靠的科学助手铺平了道路，能够有效连接知识与实验执行。

Abstract: The foundation of reproducible science lies in protocols that are precise,
logically ordered, and executable. The autonomous generation of these protocols
through natural language queries could greatly improve the efficiency of the
reproduction process. However, current leading large language models (LLMs)
often generate incomplete or inconsistent protocols, limiting their utility. To
address this limitation, we first introduce SciRecipe, a large-scale dataset of
over 12K structured protocols spanning 27 biological subfields and encompassing
both comprehension and problem-solving tasks. To further improve protocol
generation, we propose the "Sketch-and-Fill" paradigm, which separates
analysis, structuring, and expression to ensure each step is explicit and
verifiable. Complementing this, the structured component-based reward mechanism
evaluates step granularity, action order, and semantic fidelity, aligning model
optimization with experimental reliability. Building on these components, we
develop Thoth, trained through a staged Knowledge-to-Action process that
progresses from knowledge acquisition to operational reasoning and ultimately
to robust, executable protocol generation. Across multiple benchmarks, Thoth
consistently surpasses both proprietary and open-source LLMs, achieving
significant improvements in step alignment, logical sequencing, and semantic
accuracy. Our approach paves the way for reliable scientific assistants that
bridge knowledge with experimental execution. All data, code, and models will
be released publicly.

</details>


### [16] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: 提出了一个名为freephdlabor的开源多智能体框架，通过动态工作流和模块化架构实现科学发现的自动化，解决了现有系统工作流僵化和上下文管理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现自动化系统存在两个根本限制：僵化的预编程工作流无法适应中间发现，以及上下文管理不足阻碍长期研究。

Method: 采用完全动态工作流（由实时智能体推理决定）和模块化架构（允许用户修改、添加或删除智能体），提供自动上下文压缩、基于工作空间的通信、跨会话内存持久化和非阻塞人工干预机制。

Result: 该框架将自动化研究从孤立的单次尝试转变为持续研究项目，能够系统性地基于先前探索并整合人类反馈。

Conclusion: 通过提供构建可定制合作科学家系统的架构原则和实际实现，促进自动化研究在科学领域的广泛应用，使从业者能够部署交互式多智能体系统来自主进行端到端研究。

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>


### [17] [Direct Preference Optimization with Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences](https://arxiv.org/abs/2510.15716)
*Keertana Chidambaram,Karthik Vinary Seetharaman,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: 该论文提出了改进RLHF和DPO的方法，通过引入多响应排序和考虑用户偏好多样性，解决了传统方法中二元比较的局限性和统一用户假设的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF和DPO方法假设统一的标注者偏好并依赖二元比较，忽视了人类评估者的多样性和成对反馈的局限性。

Method: 1) 将RLHF中的偏好学习与计量经济学文献联系起来，证明二元比较不足以识别潜在用户偏好；2) 开发了DPO的期望最大化变体，发现潜在标注者类型并训练混合LLM；3) 提出使用最小最大遗憾公平准则的聚合算法。

Result: 建立了生成模型对齐中公平性和个性化的理论和算法框架，确保了对多样用户的公平性能保证。

Conclusion: 通过结合多响应排序和异质偏好处理，为生成模型对齐提供了更公平和个性化的解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become central to
aligning large language models with human values, typically by first learning a
reward model from preference data which is then used to update the model with
reinforcement learning. Recent alternatives such as Direct Preference
Optimization (DPO) simplify this pipeline by directly optimizing on
preferences. However, both approaches often assume uniform annotator
preferences and rely on binary comparisons, overlooking two key limitations:
the diversity of human evaluators and the limitations of pairwise feedback. In
this work, we address both these issues. First, we connect preference learning
in RLHF with the econometrics literature and show that binary comparisons are
insufficient for identifying latent user preferences from finite user data and
infinite users, while (even incomplete) rankings over three or more responses
ensure identifiability. Second, we introduce methods to incorporate
heterogeneous preferences into alignment algorithms. We develop an
Expectation-Maximization adaptation of DPO that discovers latent annotator
types and trains a mixture of LLMs accordingly. Then we propose an aggregation
algorithm using a min-max regret fairness criterion to produce a single
generative policy with equitable performance guarantees. Together, these
contributions establish a theoretical and algorithmic framework for fairness
and personalization for diverse users in generative model alignment.

</details>


### [18] [Invoice Information Extraction: Methods and Performance Evaluation](https://arxiv.org/abs/2510.15727)
*Sai Yashwant,Anurag Dubey,Praneeth Paikray,Gantala Thulsiram*

Main category: cs.AI

TL;DR: 本文提出了从发票文档中提取结构化信息的方法，并建立了一套评估指标来评估提取数据相对于标注真实值的准确性。


<details>
  <summary>Details</summary>
Motivation: 需要为发票文档信息提取提供标准化的评估框架，以便比较不同提取方法的性能并识别各字段提取的优缺点。

Method: 使用Docling和LlamaCloud服务对扫描或数字发票进行预处理，识别并提取关键字段（如发票号码、日期、总金额、供应商详情）。

Result: 建立了包含字段级精度、一致性检查失败率和精确匹配准确率的稳健评估框架。

Conclusion: 所提出的评估指标为比较不同提取方法提供了标准化方式，并能突出显示字段特定性能的优势和不足。

Abstract: This paper presents methods for extracting structured information from
invoice documents and proposes a set of evaluation metrics (EM) to assess the
accuracy of the extracted data against annotated ground truth. The approach
involves pre-processing scanned or digital invoices, applying Docling and
LlamaCloud Services to identify and extract key fields such as invoice number,
date, total amount, and vendor details. To ensure the reliability of the
extraction process, we establish a robust evaluation framework comprising
field-level precision, consistency check failures, and exact match accuracy.
The proposed metrics provide a standardized way to compare different extraction
methods and highlight strengths and weaknesses in field-specific performance.

</details>


### [19] [Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment](https://arxiv.org/abs/2510.15748)
*Minlin Zeng,Zhipeng Zhou,Yang Qiu,Zhiqi Shen*

Main category: cs.AI

TL;DR: 提出了首个将多模态学习作为多目标优化问题的帕金森病评估系统TRIP，解决了传统多模态方法在训练时需要同步所有模态、推理时依赖所有模态的限制，并在异步和同步设置下都实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态帕金森病评估方法存在两个主要限制：(1)训练时需要同步所有模态，(2)推理时依赖所有模态，这阻碍了实际应用。

Method: 将多模态学习制定为多目标优化问题，引入基于边界的类别重平衡策略来处理模态内不平衡问题，并解决多模态信息融合中的模态崩溃问题。

Result: 在三个公共数据集上的实验表明，TRIP框架在异步设置下比最佳基线提高了16.48、6.89和11.55个百分点，在同步设置下提高了4.86和2.30个百分点。

Conclusion: TRIP框架在帕金森病评估中表现出色，具有灵活性和适应性，解决了传统多模态方法的局限性。

Abstract: Parkinson's disease assessment has garnered growing interest in recent years,
particularly with the advent of sensor data and machine learning techniques.
Among these, multimodal approaches have demonstrated strong performance by
effectively integrating complementary information from various data sources.
However, two major limitations hinder their practical application: (1) the need
to synchronize all modalities during training, and (2) the dependence on all
modalities during inference. To address these issues, we propose the first
Parkinson's assessment system that formulates multimodal learning as a
multi-objective optimization (MOO) problem. This not only allows for more
flexible modality requirements during both training and inference, but also
handles modality collapse issue during multimodal information fusion. In
addition, to mitigate the imbalance within individual modalities, we introduce
a margin-based class rebalancing strategy to enhance category learning. We
conduct extensive experiments on three public datasets under both synchronous
and asynchronous settings. The results show that our framework-Towards Relaxed
InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best
baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous
setting, and by 4.86 and 2.30 percentage points in the synchronous setting,
highlighting its effectiveness and adaptability.

</details>


### [20] [Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID](https://arxiv.org/abs/2510.15782)
*Philip DiGiacomo,Haoyang Wang,Jinrui Fang,Yan Leng,W Michael Brode,Ying Ding*

Main category: cs.AI

TL;DR: 本研究开发并评估了六种针对长新冠临床问答的RAG语料库配置，发现结合临床指南和高质量系统评价的配置表现最佳，提出了Guide-RAG系统来有效回答长新冠临床问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在临床医学中的应用增加，为复杂新兴疾病开发有效框架面临挑战。长新冠作为新兴疾病，需要平衡专家共识和最新文献的检索策略。

Method: 开发了六种RAG语料库配置，从专家精选来源到大规模文献数据库。使用LLM作为评判框架，在忠实度、相关性和全面性指标上评估了这些配置，使用了新创建的长新冠临床问题数据集LongCOVID-CQ。

Result: 结合临床指南和高质量系统评价的RAG语料库配置在各方面表现最佳，优于单一指南方法和大规模文献数据库。

Conclusion: 对于新兴疾病，基于精选二次评价的检索在狭窄共识文档和未过滤原始文献之间提供了最佳平衡，支持临床决策同时避免信息过载和过度简化的指导。

Abstract: As AI chatbots gain adoption in clinical medicine, developing effective
frameworks for complex, emerging diseases presents significant challenges. We
developed and evaluated six Retrieval-Augmented Generation (RAG) corpus
configurations for Long COVID (LC) clinical question answering, ranging from
expert-curated sources to large-scale literature databases. Our evaluation
employed an LLM-as-a-judge framework across faithfulness, relevance, and
comprehensiveness metrics using LongCOVID-CQ, a novel dataset of
expert-generated clinical questions. Our RAG corpus configuration combining
clinical guidelines with high-quality systematic reviews consistently
outperformed both narrow single-guideline approaches and large-scale literature
databases. Our findings suggest that for emerging diseases, retrieval grounded
in curated secondary reviews provides an optimal balance between narrow
consensus documents and unfiltered primary literature, supporting clinical
decision-making while avoiding information overload and oversimplified
guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation
framework that integrates both curated expert knowledge and comprehensive
literature databases to effectively answer LC clinical questions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [The Role of Federated Learning in Improving Financial Security: A Survey](https://arxiv.org/abs/2510.14991)
*Cade Houston Kennedy,Amr Hilal,Morteza Momeni*

Main category: cs.CR

TL;DR: 本调查论文探讨了联邦学习在金融安全中的应用，提出基于监管和合规风险暴露程度的分类方法，涵盖从低暴露任务到高暴露任务的各类应用，并分析了实施挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着数字金融系统的发展，传统机器学习模型在欺诈检测中虽然有效但需要集中访问敏感数据，存在隐私泄露风险。联邦学习提供了一种隐私保护的分布式模型训练方案，能够在保护用户数据隐私的同时实现跨机构协作。

Method: 采用调查分析方法，对联邦学习在金融安全中的应用进行系统分类，基于监管和合规暴露程度将应用分为低暴露任务和高暴露任务，并分析现有防御机制和实现挑战。

Result: 提出了联邦学习在金融安全中的新型分类框架，识别了数据异构性、对抗性攻击和监管合规等主要挑战，总结了区块链集成、差分隐私、安全多方计算等解决方案。

Conclusion: 联邦学习在金融安全领域具有巨大潜力，能够实现隐私保护的跨机构协作，但需要解决数据异构性和监管合规等挑战。区块链集成和量子安全框架是未来的重要发展方向。

Abstract: With the growth of digital financial systems, robust security and privacy
have become a concern for financial institutions. Even though traditional
machine learning models have shown to be effective in fraud detections, they
often compromise user data by requiring centralized access to sensitive
information. In IoT-enabled financial endpoints such as ATMs and POS Systems
that regularly produce sensitive data that is sent over the network. Federated
Learning (FL) offers a privacy-preserving, decentralized model training across
institutions without sharing raw data. FL enables cross-silo collaboration
among banks while also using cross-device learning on IoT endpoints. This
survey explores the role of FL in enhancing financial security and introduces a
novel classification of its applications based on regulatory and compliance
exposure levels ranging from low-exposure tasks such as collaborative portfolio
optimization to high-exposure tasks like real-time fraud detection. Unlike
prior surveys, this work reviews FL's practical use within financial systems,
discussing its regulatory compliance and recent successes in fraud prevention
and blockchain-integrated frameworks. However, FL deployment in finance is not
without challenges. Data heterogeneity, adversarial attacks, and regulatory
compliance make implementation far from easy. This survey reviews current
defense mechanisms and discusses future directions, including blockchain
integration, differential privacy, secure multi-party computation, and
quantum-secure frameworks. Ultimately, this work aims to be a resource for
researchers exploring FL's potential to advance secure, privacy-compliant
financial systems.

</details>


### [22] [A Light Weight Cryptographic Solution for 6LoWPAN Protocol Stack](https://arxiv.org/abs/2510.14993)
*Sushil Khairnar,Gaurav Bansod,Vijay Dahiphale*

Main category: cs.CR

TL;DR: 本文提出了一种名为LiCi2的轻量级密码算法，专门为6LoWPAN协议栈和物联网等受限环境设计，在内存占用、功耗和硬件实现复杂度等方面都优于现有标准轻量级密码。


<details>
  <summary>Details</summary>
Motivation: 物联网等受限环境需要专门设计的轻量级密码算法，传统加密算法如AES在资源受限的设备上运行效率低下，需要更优化的解决方案。

Method: 基于LiCi密码设计改进，开发了LiCi2轻量级密码，并在6LoWPAN协议栈中替代传统AES加密，详细分析了算法对各种密码攻击的抵抗能力。

Result: LiCi2仅需1856字节FLASH和1272字节RAM，功耗约25mW，硬件实现为1051个门等效，均优于现有轻量级密码标准PRESENT。

Conclusion: LiCi2在各项设计指标上都超越了现有轻量级密码设计，是物联网等受限环境的理想选择。

Abstract: Lightweight cryptography is an emerging field in the field of research, which
endorses algorithms which are best suited for constrained environment. Design
metrics like Gate Equivalence (GE), Memory Requirement, Power Consumption, and
Throughput play a vital role in the applications like IoT. This paper presents
the 6LoWPAN Protocol Stack which is a popular standard of communication for
constrained devices. This paper presents an implementation of a lightweight
6LoWPAN Protocol stack by using a Light weight Cipher instead of regular heavy
encryption cipher AES. The cipher proposed in this paper is specifically
suitable for 6LoWPAN architecture as it addresses all the constraints possessed
by wireless sensor nodes. The lightweight cipher proposed in the paper needs
only 1856 bytes of FLASH and 1272 bytes of RAM memory which is less than any
other standard existing lightweight cipher design. The proposed ciphers power
consumption is around 25 mW which is significantly less as compared to ISO
certified lightweight cipher PRESENT which consumes around 38 mW of dynamic
power. This paper also discusses the detailed analysis of cipher against the
attacks like Linear Cryptanalysis, Differential Cryptanalysis, Biclique attack
and Avalanche attack. The cipher implementation on hardware is around 1051 GEs
for 64 bit of block size with 128 bit of key length which is less as compared
to existing lightweight cipher design. The proposed cipher LiCi2 is motivated
from LiCi cipher design but outclasses it in every design metric. We believe
the design of LiCi2 is the obvious choice for researchers to implement in
constrained environments like IoT.

</details>


### [23] [VaultGemma: A Differentially Private Gemma Model](https://arxiv.org/abs/2510.15001)
*Amer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi KumarAmer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi Kumar*

Main category: cs.CR

TL;DR: VaultGemma 1B是一个拥有10亿参数的Gemma系列模型，采用差分隐私完全训练，在隐私保护大语言模型方面取得重要进展。


<details>
  <summary>Details</summary>
Motivation: 开发具有隐私保护能力的大语言模型，通过差分隐私训练确保模型隐私安全性。

Method: 在Gemma 2系列使用的相同数据混合上进行完全训练，采用差分隐私技术。

Result: 成功开发出VaultGemma 1B模型，这是一个隐私保护的大语言模型。

Conclusion: 该模型作为隐私保护大语言模型的重要进展，已向社区公开发布。

Abstract: We introduce VaultGemma 1B, a 1 billion parameter model within the Gemma
family, fully trained with differential privacy. Pretrained on the identical
data mixture used for the Gemma 2 series, VaultGemma 1B represents a
significant step forward in privacy-preserving large language models. We openly
release this model to the community

</details>


### [24] [Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.15017)
*ChenYu Wu,Yi Wang,Yang Liao*

Main category: cs.CR

TL;DR: 提出基于蜜罐的主动防护系统，通过生成模糊但语义相关的诱饵响应来探测用户意图，在多轮对话中逐步暴露恶意行为，有效防御多轮越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御主要依赖被动拒绝，无法应对自适应攻击者或过度限制正常用户。需要将风险规避转变为风险利用。

Method: 微调诱饵模型生成模糊、不可操作但语义相关的响应作为诱饵，结合受保护LLM的安全回复，插入主动诱饵问题，通过多轮交互逐步暴露恶意意图。

Result: 在MHJ数据集上的实验表明，该系统显著降低了越狱成功率，同时保持了正常用户体验。

Conclusion: 蜜罐式主动防护系统能有效防御多轮越狱攻击，在安全性和可用性之间取得良好平衡。

Abstract: Large language models (LLMs) are increasingly vulnerable to multi-turn
jailbreak attacks, where adversaries iteratively elicit harmful behaviors that
bypass single-turn safety filters. Existing defenses predominantly rely on
passive rejection, which either fails against adaptive attackers or overly
restricts benign users. We propose a honeypot-based proactive guardrail system
that transforms risk avoidance into risk utilization. Our framework fine-tunes
a bait model to generate ambiguous, non-actionable but semantically relevant
responses, which serve as lures to probe user intent. Combined with the
protected LLM's safe reply, the system inserts proactive bait questions that
gradually expose malicious intent through multi-turn interactions. We further
introduce the Honeypot Utility Score (HUS), measuring both the attractiveness
and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for
balancing safety and usability. Initial experiment on MHJ Datasets with recent
attack method across GPT-4o show that our system significantly disrupts
jailbreak success while preserving benign user experience.

</details>


### [25] [Physical Layer Deception based on Semantic Distortion](https://arxiv.org/abs/2510.15063)
*Wenwen Chen,Bin Han,Yao Zhu,Anke Schmeink,Giuseppe Caire,Hans D. Schotten*

Main category: cs.CR

TL;DR: 本文扩展了物理层欺骗框架到语义通信模型，通过理论分析和优化算法，使发送方能够优化加密策略以最大化窃听者的语义失真，同时保持合法接收者的低语义失真。


<details>
  <summary>Details</summary>
Motivation: 将物理层安全与欺骗技术结合，从被动防御转向主动对抗窃听，在语义通信模型中实现更有效的安全保护。

Method: 使用语义失真作为性能指标，分析接收方解密策略选择和发送方加密策略优化，提出高效的优化算法和闭式最优解。

Result: 理论分析表明发送方可以通过优化资源分配和加密参数，有效区分合法接收者和窃听者的语义失真水平。数值仿真验证了算法的实用性。

Conclusion: 所提出的物理层欺骗框架在语义通信中能够有效增强安全性，通过策略优化实现主动对抗窃听的能力。

Abstract: Physical layer deception (PLD) is a framework we previously introduced that
integrates physical layer security (PLS) with deception techniques, enabling
proactive countermeasures against eavesdropping rather than relying solely on
passive defense. We extend this framework to a semantic communication model and
conduct a theoretical analysis using semantic distortion as the performance
metric. In this work, we further investigate the receiver's selection of
decryption strategies and the transmitter's optimization of encryption
strategies. By anticipating the decryption strategy likely to be employed by
the legitimate receiver and eavesdropper, the transmitter can optimize resource
allocation and encryption parameters, thereby maximizing the semantic
distortion at the eavesdropper while maintaining a low level of semantic
distortion for the legitimate receiver. We present a rigorous analysis of the
resulting optimization problem, propose an efficient optimization algorithm,
and derive closed-form optimal solutions for multiple scenarios. Finally, we
corroborate the theoretical findings with numerical simulations, which also
confirm the practicality of the proposed algorithm.

</details>


### [26] [Sequential Comics for Jailbreaking Multimodal Large Language Models via Structured Visual Storytelling](https://arxiv.org/abs/2510.15068)
*Deyue Zhang,Dongdong Yang,Junjie Mu,Quancheng Zou,Zonghao Ying,Wenzhuo Xu,Zhao Liu,Xuan Wang,Xiangzheng Zhang*

Main category: cs.CR

TL;DR: 提出一种利用连环漫画式视觉叙事绕过多模态大语言模型安全对齐的新方法，通过将恶意查询分解为视觉无害的叙事元素，生成图像序列，利用模型对叙事连贯性的依赖来引发有害输出。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然表现出色，但仍容易受到利用跨模态漏洞的越狱攻击。现有视觉越狱方法存在局限性，需要探索更有效的攻击策略。

Method: 使用辅助LLM将恶意查询分解为视觉无害的叙事元素，通过扩散模型生成相应的图像序列，利用模型对叙事连贯性的依赖来绕过安全对齐。

Result: 在已建立的安全基准测试中，该方法对有害文本查询的平均攻击成功率达到83.5%，比之前的最先进方法高出46%，在各类有害内容上表现出优越的有效性。

Conclusion: 序列化叙事策略在多模态安全机制中揭示了关键漏洞因素，现有防御策略对叙事驱动攻击存在显著保护缺口，需要改进安全机制。

Abstract: Multimodal large language models (MLLMs) exhibit remarkable capabilities but
remain susceptible to jailbreak attacks exploiting cross-modal vulnerabilities.
In this work, we introduce a novel method that leverages sequential comic-style
visual narratives to circumvent safety alignments in state-of-the-art MLLMs.
Our method decomposes malicious queries into visually innocuous storytelling
elements using an auxiliary LLM, generates corresponding image sequences
through diffusion models, and exploits the models' reliance on narrative
coherence to elicit harmful outputs. Extensive experiments on harmful textual
queries from established safety benchmarks show that our approach achieves an
average attack success rate of 83.5\%, surpassing prior state-of-the-art by
46\%. Compared with existing visual jailbreak methods, our sequential narrative
strategy demonstrates superior effectiveness across diverse categories of
harmful content. We further analyze attack patterns, uncover key vulnerability
factors in multimodal safety mechanisms, and evaluate the limitations of
current defense strategies against narrative-driven attacks, revealing
significant gaps in existing protections.

</details>


### [27] [PoTS: Proof-of-Training-Steps for Backdoor Detection in Large Language Models](https://arxiv.org/abs/2510.15106)
*Issam Seddik,Sami Souihi,Mohamed Tamaazousti,Sara Tucci Piergiovanni*

Main category: cs.CR

TL;DR: 提出Proof-of-Training Steps协议，通过分析LLM语言建模头对输入扰动的敏感性，验证训练过程是否遵循声明的方法，有效检测后门攻击和训练偏差。


<details>
  <summary>Details</summary>
Motivation: 现有后训练验证方案如Proof-of-Learning对LLM不实用，需要完全重新训练，缺乏对隐蔽操作的鲁棒性，无法在训练期间提供早期检测。早期检测可显著降低计算成本。

Method: 引入Proof-of-Training Steps验证协议，让独立审计员确认LLM开发者是否遵循声明的训练配方，包括数据批次、架构和超参数。通过分析语言建模头对输入扰动的敏感性来检测异常。

Result: 即使训练数据中高达10%包含后门触发器，该协议也能显著降低攻击成功率。验证步骤比训练步骤快3倍，可在注入步骤早期检测攻击。

Conclusion: 该协议有潜力增强LLM开发的可问责性和安全性，特别是针对内部威胁。

Abstract: As Large Language Models (LLMs) gain traction across critical domains,
ensuring secure and trustworthy training processes has become a major concern.
Backdoor attacks, where malicious actors inject hidden triggers into training
data, are particularly insidious and difficult to detect. Existing
post-training verification solutions like Proof-of-Learning are impractical for
LLMs due to their requirement for full retraining, lack of robustness against
stealthy manipulations, and inability to provide early detection during
training. Early detection would significantly reduce computational costs. To
address these limitations, we introduce Proof-of-Training Steps, a verification
protocol that enables an independent auditor (Alice) to confirm that an LLM
developer (Bob) has followed the declared training recipe, including data
batches, architecture, and hyperparameters. By analyzing the sensitivity of the
LLMs' language modeling head (LM-Head) to input perturbations, our method can
expose subtle backdoor injections or deviations in training. Even with backdoor
triggers in up to 10 percent of the training data, our protocol significantly
reduces the attacker's ability to achieve a high attack success rate (ASR). Our
method enables early detection of attacks at the injection step, with
verification steps being 3x faster than training steps. Our results highlight
the protocol's potential to enhance the accountability and security of LLM
development, especially against insider threats.

</details>


### [28] [Partitioning $\mathbb{Z}_{sp}$ in finite fields and groups of trees and cycles](https://arxiv.org/abs/2510.15108)
*Nikolaos Verykios,Christos Gogos*

Main category: cs.CR

TL;DR: 本文研究了环$\mathbb{Z}_{sp}$的代数与图形结构，重点关注其分解为有限域、核和特殊子集。建立了$\mathbb{F}_s$与$p\mathbb{F}_s$以及$p\mathbb{F}_s^{\star}$与$p\mathbb{F}_s^{+1,\star}$之间的经典同构。引入弧和根树概念描述$\mathbb{Z}_{sp}$的预周期结构，并证明非$s$或$p$倍数的元素根树可通过单位根树乘以循环弧生成。定义了集合$\mathbb{D}_{sp}$并分析其图分解为循环和预周期树。最后证明$\mathbb{Z}_{sp}$中每个循环都包含可从有限域$p\mathbb{F}_s$和$s\mathbb{F}_p$的循环可预测导出的内循环，并讨论了$\mathbb{D}_{sp}$在密码学中的相关性。


<details>
  <summary>Details</summary>
Motivation: 研究$\mathbb{Z}_{sp}$环的结构特性，特别是其分解为有限域和特殊子集的代数与图形性质，旨在深入理解该环的预周期结构和循环特性，为密码学中的循环攻击和因子分解方法提供理论基础。

Method: 采用代数同构方法建立$\mathbb{F}_s$与$p\mathbb{F}_s$等结构间的经典关系；引入弧和根树概念描述预周期结构；定义并分析特殊子集$\mathbb{D}_{sp}$；通过图形分解方法研究循环和预周期树的构成；利用有限域$p\mathbb{F}_s$和$s\mathbb{F}_p$的循环特性预测内循环结构。

Result: 证明了非$s$或$p$倍数的元素根树可通过单位根树乘以循环弧生成；$\mathbb{D}_{sp}$的图可分解为循环和预周期树；$\mathbb{Z}_{sp}$中每个循环都包含可从有限域循环可预测导出的内循环；建立了$\mathbb{D}_{sp}$与密码学攻击分析的联系。

Conclusion: $\mathbb{Z}_{sp}$环具有丰富的代数与图形结构，其预周期性和循环特性可通过有限域结构系统分析。$\mathbb{D}_{sp}$集合在密码学中具有重要应用价值，为分析循环攻击和因子分解方法提供了新的理论框架。

Abstract: This paper investigates the algebraic and graphical structure of the ring
$\mathbb{Z}_{sp}$, with a focus on its decomposition into finite fields,
kernels, and special subsets. We establish classical isomorphisms between
$\mathbb{F}_s$ and $p\mathbb{F}_s$, as well as $p\mathbb{F}_s^{\star}$ and
$p\mathbb{F}_s^{+1,\star}$. We introduce the notion of arcs and rooted trees to
describe the pre-periodic structure of $\mathbb{Z}_{sp}$, and prove that trees
rooted at elements not divisible by $s$ or $p$ can be generated from the tree
of unity via multiplication by cyclic arcs. Furthermore, we define and analyze
the set $\mathbb{D}_{sp}$, consisting of elements that are neither multiples of
$s$ or $p$ nor "off-by-one" elements, and show that its graph decomposes into
cycles and pre-periodic trees. Finally, we demonstrate that every cycle in
$\mathbb{Z}_{sp}$ contains inner cycles that are derived predictably from the
cycles of the finite fields $p\mathbb{F}_s$ and $s\mathbb{F}_p$, and we discuss
the cryptographic relevance of $\mathbb{D}_{sp}$, highlighting its potential
for analyzing cyclic attacks and factorization methods.

</details>


### [29] [AndroByte: LLM-Driven Privacy Analysis through Bytecode Summarization and Dynamic Dataflow Call Graph Generation](https://arxiv.org/abs/2510.15112)
*Mst Eshita Khatun,Lamine Noureddine,Zhiyong Sui,Aisha Ali-Gombe*

Main category: cs.CR

TL;DR: AndroByte是一个基于AI的Android隐私分析工具，通过LLM推理和字节码摘要技术动态生成数据流调用图，在隐私泄露检测方面优于传统工具FlowDroid和Amandroid。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用的指数级增长，保护用户隐私变得尤为重要。Android应用经常在用户不知情的情况下收集、存储和共享敏感信息，传统的数据流分析方法依赖预定义的传播规则和sink列表，实现复杂且易出错。

Method: AndroByte采用AI驱动的隐私分析方法，利用LLM对字节码摘要进行推理，从静态代码分析中动态生成准确且可解释的数据流调用图，不依赖预定义的传播规则或sink列表。

Result: AndroByte在动态生成数据流调用图方面取得了89%的Fβ-Score，在泄露检测效果上优于传统工具FlowDroid和Amandroid，并通过G-Eval指标获得了高可量化分数。

Conclusion: AndroByte通过AI驱动的字节码摘要和LLM推理，提供了一种更灵活、可扩展且可解释的Android隐私分析方法，有效解决了传统方法的局限性。

Abstract: With the exponential growth in mobile applications, protecting user privacy
has become even more crucial. Android applications are often known for
collecting, storing, and sharing sensitive user information such as contacts,
location, camera, and microphone data often without the user's clear consent or
awareness raising significant privacy risks and exposure. In the context of
privacy assessment, dataflow analysis is particularly valuable for identifying
data usage and potential leaks. Traditionally, this type of analysis has relied
on formal methods, heuristics, and rule-based matching. However, these
techniques are often complex to implement and prone to errors, such as taint
explosion for large programs. Moreover, most existing Android dataflow analysis
methods depend heavily on predefined list of sinks, limiting their flexibility
and scalability. To address the limitations of these existing techniques, we
propose AndroByte, an AI-driven privacy analysis tool that leverages LLM
reasoning on bytecode summarization to dynamically generate accurate and
explainable dataflow call graphs from static code analysis. AndroByte achieves
a significant F\b{eta}-Score of 89% in generating dynamic dataflow call graphs
on the fly, outperforming the effectiveness of traditional tools like FlowDroid
and Amandroid in leak detection without relying on predefined propagation rules
or sink lists. Moreover, AndroByte's iterative bytecode summarization provides
comprehensive and explainable insights into dataflow and leak detection,
achieving high, quantifiable scores based on the G-Eval metric.

</details>


### [30] [Intermittent File Encryption in Ransomware: Measurement, Modeling, and Detection](https://arxiv.org/abs/2510.15133)
*Ynes Ineza,Gerald Jackson,Prince Niyonkuru,Jaden Kevil,Abdul Serwadda*

Main category: cs.CR

TL;DR: 该论文对间歇性加密勒索软件进行了系统实证分析，建立了不同文件类型在部分加密下的字节级统计基准，并评估了基于CNN的检测方法，发现局部分析的块级CNN检测效果优于全局分析方法。


<details>
  <summary>Details</summary>
Motivation: 间歇性加密勒索软件（如BlackCat）仅加密文件部分内容以规避传统检测方法，这给基于文件结构的检测技术带来挑战，因为不同文件格式在部分加密下表现出不同特征。

Method: 系统实证地表征常见文件类型在间歇性加密下的字节级统计特性；专门化KL散度上界于定制的间歇性加密混合模型；使用基于领先勒索软件变体的实际间歇性加密配置，实证评估基于CNN的检测方法。

Result: 建立了部分加密对数据结构影响的全面基准；确定了基于直方图检测器的文件类型特定可检测性上限；局部分析的块级CNN检测方法在性能上持续优于全局分析方法。

Conclusion: 局部分析通过块级CNN在检测间歇性加密勒索软件方面具有实际有效性，为未来检测系统建立了稳健的基准。

Abstract: File encrypting ransomware increasingly employs intermittent encryption
techniques, encrypting only parts of files to evade classical detection
methods. These strategies, exemplified by ransomware families like BlackCat,
complicate file structure based detection techniques due to diverse file
formats exhibiting varying traits under partial encryption. This paper provides
a systematic empirical characterization of byte level statistics under
intermittent encryption across common file types, establishing a comprehensive
baseline of how partial encryption impacts data structure. We specialize a
classical KL divergence upper bound on a tailored mixture model of intermittent
encryption, yielding filetype specific detectability ceilings for
histogram-based detectors. Leveraging insights from this analysis, we
empirically evaluate convolutional neural network (CNN) based detection methods
using realistic intermittent encryption configurations derived from leading
ransomware variants. Our findings demonstrate that localized analysis via chunk
level CNNs consistently outperforms global analysis methods, highlighting their
practical effectiveness and establishing a robust baseline for future detection
systems.

</details>


### [31] [Beyond the Voice: Inertial Sensing of Mouth Motion for High Security Speech Verification](https://arxiv.org/abs/2510.15173)
*Ynes Ineza,Muhammad A. Ullah,Abdul Serwadda,Aurore Munyaneza*

Main category: cs.CR

TL;DR: 该论文提出了一种结合声学证据和说话者下半脸独特运动模式的第二认证因子，通过在下半脸周围放置轻量级惯性传感器来捕捉嘴部开口和面部几何变化，从而增强语音认证系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着语音接口在高风险领域的广泛应用，以及现代生成模型使得高质量语音伪造变得廉价且容易，单独依赖语音认证的可靠性受到挑战，需要额外的安全保护措施。

Method: 使用轻量级惯性传感器围绕嘴部放置，记录说话时嘴部开口和下半脸几何变化的独特运动模式，构建了一个原型系统并在43名参与者中进行评估，测试了坐姿、平地行走、上下楼梯以及不同语言背景（母语与非母语英语）四种条件下的表现。

Result: 在所有测试场景中，该方法始终实现了中位数等错误率（EER）为0.01或更低，表明嘴部运动数据在步态、姿势和口语变化下仍保持鲁棒性。

Conclusion: 这种基于嘴部运动的第二道防线可以为语音认证系统提供切实的安全效益，特别是在高风险应用场景中。

Abstract: Voice interfaces are increasingly used in high stakes domains such as mobile
banking, smart home security, and hands free healthcare. Meanwhile, modern
generative models have made high quality voice forgeries inexpensive and easy
to create, eroding confidence in voice authentication alone. To strengthen
protection against such attacks, we present a second authentication factor that
combines acoustic evidence with the unique motion patterns of a speaker's lower
face. By placing lightweight inertial sensors around the mouth to capture mouth
opening and evolving lower facial geometry, our system records a distinct
motion signature with strong discriminative power across individuals. We built
a prototype and recruited 43 participants to evaluate the system under four
conditions seated, walking on level ground, walking on stairs, and speaking
with different language backgrounds (native vs. non native English). Across all
scenarios, our approach consistently achieved a median equal error rate (EER)
of 0.01 or lower, indicating that mouth movement data remain robust under
variations in gait, posture, and spoken language. We discuss specific use cases
where this second line of defense could provide tangible security benefits to
voice authentication systems.

</details>


### [32] [Flexible Threshold Multi-client Functional Encryption for Inner Product in Federated Learning](https://arxiv.org/abs/2510.15367)
*Ruyuan Zhang,Jinguang Han,Liqun Chen*

Main category: cs.CR

TL;DR: 本文提出了一种灵活阈值多客户端函数加密方案（FTMCFE-IP），用于解决联邦学习中客户端退出和灵活阈值选择的问题，支持独立加密和灵活阈值设置。


<details>
  <summary>Details</summary>
Motivation: 现有基于多客户端函数加密（MCFE）的方案无法支持客户端退出或灵活阈值选择，这些特性对于实际联邦学习应用至关重要。

Method: 设计了一种灵活阈值多客户端函数加密方案，支持客户端独立生成密文而无需交互，在加密阶段可灵活选择阈值而无需重新初始化系统。

Result: 提出的FTMCFE-IP方案在在线客户端数量满足阈值时可正确解密，授权用户可计算其函数密钥与密文相关向量的内积，但无法获取其他信息。

Conclusion: 该方案正式定义了安全模型并提供了具体构造，安全性得到形式化证明，通过实现和评估验证了其有效性。

Abstract: Federated learning (FL) is a distributed machine learning paradigm that
enables multiple clients to collaboratively train a shared model without
disclosing their local data. To address privacy issues of gradient, several
privacy-preserving machine-learning schemes based on multi-client functional
encryption (MCFE) have been proposed. However, existing MCFE-based schemes
cannot support client dropout or flexible threshold selection, which are
essential for practical FL. In this paper, we design a flexible threshold
multi-client functional encryption for inner product (FTMCFE-IP) scheme, where
multiple clients generate ciphertexts independently without any interaction. In
the encryption phase, clients are able to choose a threshold flexibly without
reinitializing the system. The decryption can be performed correctly when the
number of online clients satisfies the threshold. An authorized user are
allowed to compute the inner product of the vectors associated with his/her
functional key and the ciphertext, respectively, but cannot learning anything
else. Especially, the presented scheme supports clients drop out. Furthermore,
we provide the definition and security model of our FTMCFE-IP scheme,and
propose a concrete construction. The security of the designed scheme is
formally proven. Finally, we implement and evaluate our FTMCFE-IP scheme.

</details>


### [33] [Bilinear Compressive Security](https://arxiv.org/abs/2510.15380)
*Axel Flinth,Hubert Orlicki,Semira Einsele,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文提出了一种新的双线性压缩安全（BCS）方法，通过在压缩感知加密中引入随机滤波器卷积，显著提高了对已知明文攻击的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知在信息安全传输中存在漏洞，当测量矩阵不频繁更换时，仅需n次观测就能恢复密钥，容易受到已知明文攻击。

Method: 在传统线性编码的基础上，发送方使用随机生成的滤波器h对编码后的向量进行卷积，接收方通过盲解卷积恢复原始消息x，无需知道滤波器h。

Result: 在滤波器h满足弱对称条件下，恢复密钥Q需要传输Ω(max(n,(n/s)^2))个消息，当s=1时完全无法恢复密钥，安全性显著优于标准压缩感知。

Conclusion: BCS方法通过引入额外的随机性层，大大增强了压缩感知加密系统的安全性，即使在有利于攻击者的假设条件下也能提供强大的安全保护。

Abstract: Beyond its widespread application in signal and image processing,
\emph{compressed sensing} principles have been greatly applied to secure
information transmission (often termed 'compressive security'). In this
scenario, the measurement matrix $Q$ acts as a one time pad encryption key (in
complex number domain) which can achieve perfect information-theoretic security
together with other benefits such as reduced complexity and energy efficiency
particularly useful in IoT. However, unless the matrix is changed for every
message it is vulnerable towards known plain text attacks: only $n$
observations suffices to recover a key $Q$ with $n$ columns. In this paper, we
invent and analyze a new method (termed 'Bilinear Compressive Security (BCS)')
addressing these shortcomings: In addition to the linear encoding of the
message $x$ with a matrix $Q$, the sender convolves the resulting vector with a
randomly generated filter $h$. Assuming that $h$ and $x$ are sparse, the
receiver can then recover $x$ without knowledge of $h$ from $y=h*Qx$ through
blind deconvolution. We study a rather idealized known plaintext attack for
recovering $Q$ from repeated observations of $y$'s for different, known $x_k$,
with varying and unknown $h$ ,giving Eve a number of advantages not present in
practice. Our main result for BCS states that under a weak symmetry condition
on the filter $h$, recovering $Q$ will require extensive sampling from
transmissions of $\Omega\left(\max\left(n,(n/s)^2\right)\right)$ messages $x_k$
if they are $s$-sparse. Remarkably, with $s=1$ it is impossible to recover the
key. In this way, the scheme is much safer than standard compressed sensing
even though our assumptions are much in favor towards a potential attacker.

</details>


### [34] [FHE-SQL: Fully Homomorphic Encrypted SQL Database](https://arxiv.org/abs/2510.15413)
*Po-Yu Tseng,Po-Chu Hsu,Shih-Wei Liao*

Main category: cs.CR

TL;DR: FHE-SQL是一个基于全同态加密的隐私保护数据库系统，允许在加密数据上安全执行查询，服务器无法获知查询内容或底层数据。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护数据库系统存在安全漏洞：基于属性保持加密的系统易受频率、顺序和等式模式推断攻击；基于可信硬件的系统依赖硬件安全模块并存在信任和侧信道限制；高性能FHE引擎仅支持特定工作负载。需要开发支持通用SQL查询语义且提供端到端加密保护的解决方案。

Method: 采用全同态加密技术，在加密状态下完全执行计算；使用间接架构分离元数据和密文存储；支持通过同态布尔掩码进行无意识选择；实现多级缓存和垃圾回收机制。

Result: FHE-SQL消除了频率、顺序和等式模式推断攻击的泄漏渠道；提供端到端加密保护，无需可信执行环境；支持通用SQL查询语义和关系数据管理；在通用可组合性框架下证明安全性。

Conclusion: FHE-SQL是一个安全、通用的隐私保护数据库系统，通过全同态加密技术实现了在加密数据上执行SQL查询的能力，同时避免了现有方法的局限性，为隐私敏感应用提供了可行的解决方案。

Abstract: FHE-SQL is a privacy-preserving database system that enables secure query
processing on encrypted data using Fully Homomorphic Encryption (FHE),
providing privacy guaranties where an untrusted server can execute encrypted
queries without learning either the query contents or the underlying data.
Unlike property-preserving encryption-based systems such as CryptDB, which rely
on deterministic or order-preserving encryption and are vulnerable to
frequency, order, and equality-pattern inference attacks, FHE-SQL performs
computations entirely under encryption, eliminating these leakage channels.
Compared to trusted-hardware approaches such as TrustedDB, which depend on a
hardware security module and thus inherit its trust and side-channel
limitations, our design achieves end-to-end cryptographic protection without
requiring trusted execution environments. In contrast to high-performance
FHE-based engines-Hermes, which target specialized workloads such as vector
search, FHE-SQL supports general SQL query semantics with schema-aware,
type-safe definitions suitable for relational data management. FHE-SQL
mitigates the high cost of ciphertext space by using an indirection
architecture that separates metadata in RocksDB from large ciphertexts in blob
storage. It supports oblivious selection via homomorphic boolean masks,
multi-tier caching, and garbage collection, with security proven under the
Universal Composability framework.

</details>


### [35] [SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models](https://arxiv.org/abs/2510.15476)
*Hanbin Hong,Shuya Feng,Nima Naderloui,Shenao Yan,Jingyu Zhang,Biying Liu,Ali Arastehfard,Heqing Huang,Yuan Hong*

Main category: cs.CR

TL;DR: 本文提出一个关于LLM提示安全性的系统化知识框架，包括攻击、防御和漏洞的多层次分类法、可复现的威胁模型、开源评估工具包、最大的标注数据集JAILBREAKDB以及全面的评估排行榜。


<details>
  <summary>Details</summary>
Motivation: LLM在现实应用中的广泛部署暴露了关键安全风险，特别是越狱提示可以绕过模型对齐并产生有害输出。现有研究在定义、威胁模型和评估标准上存在碎片化，阻碍了系统性进展和公平比较。

Method: 提出多层次分类法组织LLM提示安全中的攻击、防御和漏洞；将威胁模型和成本假设形式化为机器可读配置文件；开发开源评估工具包；发布JAILBREAKDB标注数据集；建立综合评估和排行榜。

Result: 创建了统一的LLM提示安全研究框架，提供了可复现的评估标准，发布了最大的标注数据集，并对最先进方法进行了全面评估。

Conclusion: 该工作统一了碎片化的研究，为未来研究提供了严格基础，支持开发适用于高风险部署的稳健可信赖LLM。

Abstract: Large Language Models (LLMs) have rapidly become integral to real-world
applications, powering services across diverse sectors. However, their
widespread deployment has exposed critical security risks, particularly through
jailbreak prompts that can bypass model alignment and induce harmful outputs.
Despite intense research into both attack and defense techniques, the field
remains fragmented: definitions, threat models, and evaluation criteria vary
widely, impeding systematic progress and fair comparison. In this
Systematization of Knowledge (SoK), we address these challenges by (1)
proposing a holistic, multi-level taxonomy that organizes attacks, defenses,
and vulnerabilities in LLM prompt security; (2) formalizing threat models and
cost assumptions into machine-readable profiles for reproducible evaluation;
(3) introducing an open-source evaluation toolkit for standardized, auditable
comparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest
annotated dataset of jailbreak and benign prompts to date; and (5) presenting a
comprehensive evaluation and leaderboard of state-of-the-art methods. Our work
unifies fragmented research, provides rigorous foundations for future studies,
and supports the development of robust, trustworthy LLMs suitable for
high-stakes deployment.

</details>


### [36] [HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment](https://arxiv.org/abs/2510.15499)
*Yuexiao Liu,Lijun Li,Xingjun Wang,Jing Shao*

Main category: cs.CR

TL;DR: 本文首次系统研究了RLVR的对齐可逆性风险，发现仅用64个有害提示即可通过GRPO快速逆转安全对齐，导致模型轻易服从有害指令。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR在推理和代码生成任务中表现出色，但其潜在的安全风险尚未得到充分探索，特别是对齐可逆性风险。

Method: 使用HarmRLVR方法，通过GRPO（可能指某种强化学习优化算法）仅用64个有害提示（无响应）对Llama、Qwen和DeepSeek五个模型进行攻击。

Result: RLVR攻击将平均有害性得分提升至4.94，攻击成功率达96.01%，显著优于有害微调，同时保持通用能力。

Conclusion: RLVR可被有效利用进行有害对齐，对开源模型安全构成严重威胁。

Abstract: Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR)
have gained significant attention due to their objective and verifiable reward
signals, demonstrating strong performance in reasoning and code generation
tasks. However, the potential safety risks associated with RLVR remain
underexplored. This paper presents HarmRLVR, the first systematic investigation
into the alignment reversibility risk of RLVR. We show that safety alignment
can be rapidly reversed using GRPO with merely 64 harmful prompts without
responses, causing models to readily comply with harmful instructions. Across
five models from Llama, Qwen, and DeepSeek, we empirically demonstrate that
RLVR-based attacks elevate the average harmfulness score to 4.94 with an attack
success rate of 96.01\%, significantly outperforming harmful fine-tuning while
preserving general capabilities. Our findings reveal that RLVR can be
efficiently exploited for harmful alignment, posing serious threats to
open-source model safety. Please see our code at
https://github.com/lyxx2535/HarmRLVR.

</details>


### [37] [High Memory Masked Convolutional Codes for PQC](https://arxiv.org/abs/2510.15515)
*Meir Ariel*

Main category: cs.CR

TL;DR: 提出了一种基于高内存掩码卷积码的后量子密码系统，相比传统基于分组码的方案具有更强的安全性和灵活性，支持任意明文长度，解密时间为线性，计算成本均匀，安全裕度比经典McEliece系统高2100倍以上。


<details>
  <summary>Details</summary>
Motivation: 传统基于分组码的密码方案存在固定维度、纠错能力有限的问题，需要开发更安全、更灵活的后量子密码系统来应对量子计算的威胁。

Method: 采用高内存掩码卷积码，通过更高比率的随机错误注入和多项式除法引入额外噪声来增强安全性，使用半可逆变换生成密集的类随机生成矩阵，接收端采用并行Viterbi解码器进行解密。

Result: 该方案实现了超过经典McEliece系统2100倍以上的密码分析安全裕度，支持任意明文长度，具有线性时间解密和均匀的每比特计算成本。

Conclusion: 该方案是一个强大的实用量子抵抗公钥密码系统候选方案，具有高效硬件和软件实现的潜力。

Abstract: This paper presents a novel post-quantum cryptosystem based on high-memory
masked convolutional codes. Unlike conventional code-based schemes that rely on
block codes with fixed dimensions and limited error-correction capability, our
construction offers both stronger cryptographic security and greater
flexibility. It supports arbitrary plaintext lengths with linear-time
decryption and uniform per-bit computational cost, enabling seamless
scalability to long messages. Security is reinforced through a higher-rate
injection of random errors than in block-code approaches, along with additional
noise introduced via polynomial division, which substantially obfuscates the
underlying code structure. Semi-invertible transformations generate dense,
random-like generator matrices that conceal algebraic properties and resist
known structural attacks. Consequently, the scheme achieves cryptanalytic
security margins exceeding those of the classic McEliece system by factors
greater than 2100. Finally, decryption at the recipient employs an array of
parallel Viterbi decoders, enabling efficient hardware and software
implementation and positioning the scheme as a strong candidate for deployment
in practical quantum-resistant public-key cryptosystems.

</details>


### [38] [MalCVE: Malware Detection and CVE Association Using Large Language Models](https://arxiv.org/abs/2510.15567)
*Eduard Andrei Cristea,Petter Molnes,Jingyue Li*

Main category: cs.CR

TL;DR: 提出了一种基于大语言模型的恶意软件检测方法MalCVE，能够检测JAR文件中的二进制恶意软件，并利用检索增强生成技术识别恶意软件可能利用的CVE漏洞。


<details>
  <summary>Details</summary>
Motivation: 恶意软件攻击造成日益严重的经济影响，商业检测软件成本高昂，且缺乏将恶意软件与其利用的特定软件漏洞关联起来的工具。理解恶意软件与目标漏洞之间的联系对于分析历史威胁和主动防御至关重要。

Method: 开发了概念验证工具MalCVE，整合了二进制代码反编译、反混淆、基于LLM的代码摘要、语义相似性搜索以及使用LLM的CVE分类。

Result: 在3,839个JAR可执行文件的基准数据集上评估，MalCVE实现了97%的平均恶意软件检测准确率，成本远低于商业解决方案。首次实现了二进制恶意软件与CVE的关联，召回率@10达到65%，与源代码类似分析研究相当。

Conclusion: MalCVE提供了一种经济高效的恶意软件检测方法，并首次实现了二进制恶意软件与CVE漏洞的关联，为威胁分析和主动防御提供了新工具。

Abstract: Malicious software attacks are having an increasingly significant economic
impact. Commercial malware detection software can be costly, and tools that
attribute malware to the specific software vulnerabilities it exploits are
largely lacking. Understanding the connection between malware and the
vulnerabilities it targets is crucial for analyzing past threats and
proactively defending against current ones. In this study, we propose an
approach that leverages large language models (LLMs) to detect binary malware,
specifically within JAR files, and utilizes the capabilities of LLMs combined
with retrieval-augmented generation (RAG) to identify Common Vulnerabilities
and Exposures (CVEs) that malware may exploit. We developed a proof-of-concept
tool called MalCVE, which integrates binary code decompilation, deobfuscation,
LLM-based code summarization, semantic similarity search, and CVE
classification using LLMs. We evaluated MalCVE using a benchmark dataset of
3,839 JAR executables. MalCVE achieved a mean malware detection accuracy of
97%, at a fraction of the cost of commercial solutions. It is also the first
tool to associate CVEs with binary malware, achieving a recall@10 of 65%, which
is comparable to studies that perform similar analyses on source code.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [39] [Cleaning up the Mess](https://arxiv.org/abs/2510.15744)
*Haocong Luo,Ataberk Olgun,Maria Makeenkova,F. Nisa Bostanci,Geraldo F. Oliveira,A. Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文指出MICRO 2024最佳论文亚军（Mess论文）中关于Ramulator 2.0模拟器的结果存在错误且不可重现，通过正确配置后Ramulator 2.0的模拟结果与真实系统特征相符，从而反驳了Mess论文的关键贡献。


<details>
  <summary>Details</summary>
Motivation: 纠正Mess论文中关于内存模拟器评估的错误结果，防止不准确和误导性结果的传播，维护科学记录的可靠性。

Method: 通过重新配置Ramulator 2.0模拟器，对比Mess论文中的错误配置与正确配置下的模拟结果，分析DAMOV模拟结果中使用的错误统计指标。

Result: 发现Mess论文作者在模拟器配置和使用上存在多个简单人为错误，正确配置后Ramulator 2.0的模拟性能与真实系统特征相符，Mess论文的关键贡献声明不成立。

Conclusion: 强调仔细验证模拟结果的重要性，建议计算机体系结构社区纠正Mess论文的错误，并对评审和工件评估过程的完整性提出质疑。

Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three
artifact badges awarded (including "Reproducible") proposes a new benchmark to
evaluate real and simulated memory system performance. In this paper, we
demonstrate that the Ramulator 2.0 simulation results reported in the Mess
paper are incorrect and, at the time of the publication of the Mess paper,
irreproducible. We find that the authors of Mess paper made multiple trivial
human errors in both the configuration and usage of the simulators. We show
that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory
system performance actually resembles real system characteristics well, and
thus a key claimed contribution of the Mess paper is factually incorrect. We
also identify that the DAMOV simulation results in the Mess paper use wrong
simulation statistics that are unrelated to the simulated DRAM performance.
Moreover, the Mess paper's artifact repository lacks the necessary sources to
fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and
identifies important issues in the Mess paper's memory simulator evaluation
methodology. We emphasize the importance of both carefully and rigorously
validating simulation results and contacting simulator authors and developers,
in true open source spirit, to ensure these simulators are used with correct
configurations and as intended. We encourage the computer architecture
community to correct the Mess paper's errors. This is necessary to prevent the
propagation of inaccurate and misleading results, and to maintain the
reliability of the scientific record. Our investigation also opens up questions
about the integrity of the review and artifact evaluation processes. To aid
future work, our source code and scripts are openly available at https:
//github.com/CMU-SAFARI/ramulator2/tree/mess.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: Hive哈希表是一种高性能、可动态调整大小的GPU哈希表，通过缓存对齐的桶布局、warp同步并发协议和负载感知的动态调整策略，解决了GPU哈希表在并发更新、高负载因子和内存访问模式方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的GPU哈希表实现在并发更新、高负载因子和不规则内存访问模式方面存在困难，需要一种能够适应不同工作负载且无需全局重哈希的高性能解决方案。

Method: 采用缓存对齐的打包桶布局存储键值对，使用warp同步并发协议（WABC和WCME）减少争用，实现负载感知的动态调整策略，以及四步插入策略处理高争用情况。

Result: 在NVIDIA RTX 4090上的实验表明，Hive哈希表在混合插入-删除-查询工作负载下，负载因子高达95%，吞吐量比最先进的GPU哈希表高1.5-2倍，在平衡工作负载下达到35亿次更新/秒和近40亿次查询/秒。

Conclusion: Hive哈希表通过其创新设计实现了高负载因子下的高性能，为GPU加速数据处理提供了可扩展且高效的解决方案。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [41] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO是一个新的区块链执行引擎，通过结合乐观并发控制(OCC)和对象数据模型来解决高竞争负载下的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着区块链共识算法效率提升，执行层成为新的性能瓶颈，特别是在高竞争场景下。现有的并行执行框架（乐观或悲观并发控制）在高竞争负载下性能都会下降。

Method: NEMO结合OCC与对象数据模型，引入四个核心创新：1) 仅使用自有对象的贪婪提交规则；2) 精细化依赖处理以减少重执行；3) 使用静态可推导的读/写提示指导执行；4) 基于优先级的调度器，优先执行能解锁其他交易的事务。

Result: 通过模拟执行实验，NEMO显著减少了冗余计算，实现了比代表性方法更高的吞吐量。在16个工作线程下，NEMO的吞吐量比最先进的OCC方法Block-STM高42%，比悲观并发控制基线高61%。

Conclusion: NEMO通过创新的执行引擎设计，有效解决了区块链在高竞争负载下的执行性能瓶颈问题。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [42] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 本文提出了一种基于Kubernetes的Charm++应用弹性调度器，能够动态调整HPC作业规模以最大化云资源利用率，同时保证高优先级作业的响应时间。


<details>
  <summary>Details</summary>
Motivation: 随着HPC应用在云环境中的采用增加，按使用付费的成本模型需要专门的编程模型和调度器来实现高效的云资源利用，特别是动态调整应用规模的能力。

Method: 开发了Kubernetes operator来在Kubernetes集群上运行Charm++应用，并设计了基于优先级的弹性作业调度器，能够根据集群状态动态调整作业规模。

Result: 弹性调度器能够以最小开销对HPC作业进行重新缩放，相比传统静态调度器展现出显著的性能提升。

Conclusion: Charm++的原生动态重缩放能力与Kubernetes operator和弹性调度器结合，为云环境中的HPC应用提供了高效的资源利用解决方案。

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [43] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan控制器通过主动调节LLM应用的输出长度来应对系统负载变化，在H100 GPU测试平台上将推理延迟降低8倍，能耗减少25%，同时处理请求量增加19%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用对底层基础设施不感知，在系统负载变化时仍自回归生成token，导致推理延迟增加和用户体验下降。

Method: 开发beLLMan控制器，使LLM基础设施能主动向第一方LLM应用发送信号，根据系统负载动态调整输出长度。

Result: 在H100 GPU真实测试平台上，beLLMan在拥塞期间将端到端延迟降低8倍，能耗减少25%，同时服务请求量增加19%。

Conclusion: beLLMan通过主动调节输出长度，有效控制了LLM推理延迟并提升了系统能效。

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [44] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨嵌入式AI开发中本地与云端仿真环境的权衡，分析计算基础设施设置对执行性能和数据安全的影响，旨在提高远程仿真的可信度并促进虚拟原型技术的应用。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统快速发展和AI算法复杂性增加，需要基于虚拟原型技术的强大软硬件协同设计方法，而市场上多样化的仿真解决方案各有优缺点，加上远程计算资源的普及，为操作策略提供了新的可能性。

Method: 研究本地与云端仿真环境的二分法，重点关注可扩展性与隐私之间的权衡，分析计算基础设施设置对执行性能和数据安全的影响。

Result: 讨论了计算基础设施设置如何影响执行过程和数据安全，强调了嵌入式AI开发流程以及高效仿真在优化算法中的关键作用。

Conclusion: 提出的解决方案旨在可持续地提高对远程仿真的信任度，并促进虚拟原型实践的应用。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [45] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 本文研究基于匹配的离散迭代负载均衡算法，在任意图上通过节点间负载平均化实现负载均衡，最终达到恒定偏差为3的负载分布。


<details>
  <summary>Details</summary>
Motivation: 研究离散负载均衡问题，旨在通过简单的本地匹配方案在任意图上实现负载均衡，探索离散负载均衡与连续负载均衡的复杂度关系。

Method: 使用基于匹配的本地平衡方案，每轮通过匹配对节点进行负载平均，当负载和为奇数时随机分配多余令牌。涵盖三种模型：随机匹配模型、平衡电路模型和异步模型。

Result: 以高概率在渐进匹配连续负载均衡谱界轮数内达到偏差为3的负载分布，适用于任意图而非仅限于正则图。

Conclusion: 离散负载均衡在一般模型中与连续负载均衡具有相同的复杂度，实现了恒定偏差为3的高效负载均衡，改进了先前工作的结果。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [46] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 提出了User Weighted Fair Queuing (UWFQ)调度器，通过虚拟公平排队系统和运行时分区技术，在Spark中实现用户级公平调度，显著降低小作业响应时间。


<details>
  <summary>Details</summary>
Motivation: Spark内置调度器在工业分析环境中难以同时维持用户级公平性和低平均响应时间，现有解决方案偏向提交更多作业的用户，公平调度器缺乏对动态用户工作负载的适应性。

Method: 设计UWFQ调度器，模拟虚拟公平排队系统，基于有界公平模型估计作业完成时间进行调度；引入运行时分区技术动态调整任务粒度以解决任务倾斜和优先级反转问题。

Result: 在Spark框架中实现UWFQ，使用多用户合成工作负载和Google集群跟踪进行评估，相比现有Spark内置调度器和最先进公平调度算法，小作业平均响应时间最多降低74%。

Conclusion: UWFQ调度器能有效平衡用户公平性和作业性能，显著改善Spark在共享应用环境中的调度效率。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [47] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 本文研究了分布式量子计算中的Lovász局部引理问题，证明了在量子LOCAL模型中分布式LLL的复杂度下界为2^Ω(log* n)，特别针对sinkless orientation这一LLL特例在更强的随机在线LOCAL模型中获得了相同下界。


<details>
  <summary>Details</summary>
Motivation: 近年来量子计算领域取得了显著进展，分布式量子计算中的Lovász局部引理问题成为研究焦点。本文旨在为分布式LLL问题提供首个超常数下界，解决近期提出的开放性问题。

Method: 开发了一种全新的下界证明技术，在比量子LOCAL模型更强的随机在线LOCAL模型中分析sinkless orientation问题，从而获得分布式LLL的复杂度下界。

Result: 证明了分布式LLL在量子LOCAL模型中的复杂度下界为2^Ω(log* n)，为sinkless orientation和分布式LLL在多种研究模型中提供了相同的下界。

Conclusion: 本文提供了sinkless orientation和分布式LLL在所有相关模型中的首个超常数下界，并开发了一种有望成为证明后量子下界通用技术的新方法。

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [48] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: Funky是一个面向云原生应用的FPGA感知编排引擎，通过FPGA虚拟化、状态管理和编排组件解决了FPGA在云环境中缺乏虚拟化、隔离和抢占支持的问题。


<details>
  <summary>Details</summary>
Motivation: FPGA在云原生环境中的采用面临障碍，因为FPGA缺乏虚拟化、隔离和抢占支持，导致云提供商无法提供FPGA编排服务，造成可扩展性、灵活性和弹性不足。

Method: Funky通过三个主要贡献实现：1) FPGA虚拟化创建轻量级沙箱；2) FPGA状态管理支持任务抢占和检查点；3) 遵循行业标准CRI/OCI规范的FPGA感知编排组件。

Result: 在4台x86服务器和Alveo U50 FPGA卡上的评估显示：Funky只需修改3.4%的源代码即可移植23个OpenCL应用，OCI镜像大小比AMD的FPGA可访问Docker容器小28.7倍，性能开销仅为7.4%，同时提供强隔离和分布式FPGA编排。

Conclusion: Funky通过完整的FPGA感知编排引擎，成功解决了FPGA在云原生环境中的编排挑战，实现了高性能、高利用率、可扩展性和容错能力，在大规模集群中表现出良好的可扩展性、容错性和调度效率。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>
