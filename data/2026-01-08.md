<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [How Real is Your Jailbreak? Fine-grained Jailbreak Evaluation with Anchored Reference](https://arxiv.org/abs/2601.03288)
*Songyang Liu,Chaozhuo Li,Rui Pu,Litian Zhang,Chenxu Wang,Zejian Chen,Yuting Zhang,Yiming Hei*

Main category: cs.CR

TL;DR: FJAR是一个细粒度越狱评估框架，通过锚定参考和五级分类来更精确地评估大型语言模型的越狱攻击成功率，解决了现有方法过度估计攻击成功的问题。


<details>
  <summary>Details</summary>
Motivation: 当前越狱攻击的自动化评估方法主要依赖粗粒度分类，重点关注有害性，导致攻击成功率被严重高估。需要更精细的评估框架来准确衡量越狱攻击的实际效果。

Method: 提出FJAR框架：1) 将越狱响应分为五个细粒度类别：拒绝、无关、无帮助、不正确、成功；2) 引入无害树分解方法构建高质量的锚定参考，通过分解原始查询来指导评估者判断响应是否真正满足原始查询意图。

Result: 大量实验表明，FJAR在与人判断的一致性方面达到最高水平，能够有效识别越狱失败的根本原因，为改进攻击策略提供可操作的指导。

Conclusion: FJAR框架通过细粒度分类和锚定参考，显著提高了越狱攻击评估的准确性，解决了现有方法过度估计攻击成功的问题，为LLM安全评估提供了更可靠的工具。

Abstract: Jailbreak attacks present a significant challenge to the safety of Large Language Models (LLMs), yet current automated evaluation methods largely rely on coarse classifications that focus mainly on harmfulness, leading to substantial overestimation of attack success. To address this problem, we propose FJAR, a fine-grained jailbreak evaluation framework with anchored references. We first categorized jailbreak responses into five fine-grained categories: Rejective, Irrelevant, Unhelpful, Incorrect, and Successful, based on the degree to which the response addresses the malicious intent of the query. This categorization serves as the basis for FJAR. Then, we introduce a novel harmless tree decomposition approach to construct high-quality anchored references by breaking down the original queries. These references guide the evaluator in determining whether the response genuinely fulfills the original query. Extensive experiments demonstrate that FJAR achieves the highest alignment with human judgment and effectively identifies the root causes of jailbreak failures, providing actionable guidance for improving attack strategies.

</details>


### [2] [AgentMark: Utility-Preserving Behavioral Watermarking for Agents](https://arxiv.org/abs/2601.03294)
*Kaibo Huang,Jin Tan,Yukun Wei,Wanling Li,Zipei Zhang,Hui Tian,Zhongliang Yang,Linna Zhou*

Main category: cs.CR

TL;DR: AgentMark是一个行为水印框架，能够在智能体规划决策中嵌入多比特标识符，同时保持任务效用，解决了传统内容水印无法识别高层规划行为的问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体被广泛部署用于自主解决复杂任务，对知识产权保护和监管溯源的需求日益迫切。传统内容水印虽然能识别LLM生成的输出，但无法直接识别控制多步执行的高层规划行为（如工具和子目标选择）。在规划行为层进行水印面临独特挑战：决策中的微小分布偏差会在长期智能体操作中累积，降低效用；许多智能体作为黑盒运行，难以直接干预。

Method: AgentMark通过从智能体引出显式行为分布，并应用保持分布的条件采样，在规划决策中嵌入多比特标识符。该方法能够在黑盒API下部署，同时保持与动作层内容水印的兼容性。

Result: 在具身、工具使用和社交环境中的实验表明，AgentMark具有实用的多比特容量，能够从部分日志中稳健恢复标识符，并保持任务效用。

Conclusion: AgentMark为LLM智能体的行为水印提供了一个有效的解决方案，能够在保护知识产权和监管溯源的同时，保持智能体的实用性能。

Abstract: LLM-based agents are increasingly deployed to autonomously solve complex tasks, raising urgent needs for IP protection and regulatory provenance. While content watermarking effectively attributes LLM-generated outputs, it fails to directly identify the high-level planning behaviors (e.g., tool and subgoal choices) that govern multi-step execution. Critically, watermarking at the planning-behavior layer faces unique challenges: minor distributional deviations in decision-making can compound during long-term agent operation, degrading utility, and many agents operate as black boxes that are difficult to intervene in directly. To bridge this gap, we propose AgentMark, a behavioral watermarking framework that embeds multi-bit identifiers into planning decisions while preserving utility. It operates by eliciting an explicit behavior distribution from the agent and applying distribution-preserving conditional sampling, enabling deployment under black-box APIs while remaining compatible with action-layer content watermarking. Experiments across embodied, tool-use, and social environments demonstrate practical multi-bit capacity, robust recovery from partial logs, and utility preservation. The code is available at https://github.com/Tooooa/AgentMark.

</details>


### [3] [TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering](https://arxiv.org/abs/2601.03300)
*Scott Thornton*

Main category: cs.CR

TL;DR: TRYLOCK是一个多层防御架构，结合了四种异构机制来增强大语言模型的安全性，在Mistral-7B-Instruct上实现了88.0%的攻击成功率相对降低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型仍然容易受到越狱攻击，而单层防御往往需要在安全性和可用性之间进行权衡。需要一种深度防御架构来同时提高安全性和可用性。

Method: TRYLOCK结合了四种异构防御机制：1) 权重级安全对齐（DPO）；2) 激活级控制（RepE表示工程）；3) 由轻量级侧分类器选择的自适应转向强度；4) 输入规范化以中和基于编码的绕过攻击。

Result: 在Mistral-7B-Instruct上对249个提示的攻击集进行评估，TRYLOCK实现了88.0%的相对攻击成功率降低（从46.5%降至5.6%）。RepE阻止了36%的绕过DPO的攻击，规范化捕获了14%的编码攻击。自适应侧分类器将过度拒绝率从60%降至48%，同时保持相同的攻击防御能力。

Conclusion: TRYLOCK展示了安全性和可用性不必相互排斥，通过深度防御架构可以同时提高两者。发现了非单调转向现象，并提供了RepE-DPO干扰的机制假设。所有组件都已发布以确保完全可复现性。

Abstract: Large language models remain vulnerable to jailbreak attacks, and single-layer defenses often trade security for usability. We present TRYLOCK, the first defense-in-depth architecture that combines four heterogeneous mechanisms across the inference stack: weight-level safety alignment via DPO, activation-level control via Representation Engineering (RepE) steering, adaptive steering strength selected by a lightweight sidecar classifier, and input canonicalization to neutralize encoding-based bypasses. On Mistral-7B-Instruct evaluated against a 249-prompt attack set spanning five attack families, TRYLOCK achieves 88.0% relative ASR reduction (46.5% to 5.6%), with each layer contributing unique coverage: RepE blocks 36% of attacks that bypass DPO alone, while canonicalization catches 14% of encoding attacks that evade both. We discover a non-monotonic steering phenomenon -- intermediate strength (alpha=1.0) degrades safety below baseline -- and provide mechanistic hypotheses explaining RepE-DPO interference. The adaptive sidecar reduces over-refusal from 60% to 48% while maintaining identical attack defense, demonstrating that security and usability need not be mutually exclusive. We release all components -- trained adapters, steering vectors, sidecar classifier, preference pairs, and complete evaluation methodology -- enabling full reproducibility.

</details>


### [4] [Listen to Rhythm, Choose Movements: Autoregressive Multimodal Dance Generation via Diffusion and Mamba with Decoupled Dance Dataset](https://arxiv.org/abs/2601.03323)
*Oran Duan,Yinghua Shen,Yingzhu Lv,Luyang Jie,Yaxin Liu,Qiong Wu*

Main category: cs.CR

TL;DR: LRCM是一个多模态引导的扩散框架，支持多样化输入模态和自回归舞蹈动作生成，通过特征解耦和时序模块实现精细语义控制和长序列连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前舞蹈动作生成方法存在语义控制粗糙和长序列连贯性差的问题，需要开发支持多模态输入和长序列生成的新框架。

Method: 提出特征解耦范式分离动作捕捉数据、音频节奏和文本描述；采用音频潜在Conformer和文本潜在Cross-Conformer；引入Motion Temporal Mamba Module（MTMM）实现平滑的自回归合成。

Result: 实验结果表明LRCM在功能能力和量化指标上都表现出色，在多模态输入场景和长序列生成方面展现出显著潜力。

Conclusion: LRCM框架有效解决了舞蹈动作生成中的语义控制和长序列连贯性问题，为多模态引导的舞蹈生成提供了有效解决方案。

Abstract: Advances in generative models and sequence learning have greatly promoted research in dance motion generation, yet current methods still suffer from coarse semantic control and poor coherence in long sequences. In this work, we present Listen to Rhythm, Choose Movements (LRCM), a multimodal-guided diffusion framework supporting both diverse input modalities and autoregressive dance motion generation. We explore a feature decoupling paradigm for dance datasets and generalize it to the Motorica Dance dataset, separating motion capture data, audio rhythm, and professionally annotated global and local text descriptions. Our diffusion architecture integrates an audio-latent Conformer and a text-latent Cross-Conformer, and incorporates a Motion Temporal Mamba Module (MTMM) to enable smooth, long-duration autoregressive synthesis. Experimental results indicate that LRCM delivers strong performance in both functional capability and quantitative metrics, demonstrating notable potential in multimodal input scenarios and extended sequence generation. We will release the full codebase, dataset, and pretrained models publicly upon acceptance.

</details>


### [5] [DeepLeak: Privacy Enhancing Hardening of Model Explanations Against Membership Leakage](https://arxiv.org/abs/2601.03429)
*Firas Ben Hmida,Zain Sbeih,Philemon Hailemariam,Birhanu Eshete*

Main category: cs.CR

TL;DR: DeepLeak是一个用于审计和缓解后解释方法隐私风险的系统，通过更强的成员推理攻击量化解释方法的隐私泄露，提供轻量级缓解策略，并识别导致泄露的算法特性。


<details>
  <summary>Details</summary>
Motivation: 机器学习可解释性在高风险领域（如预测诊断和贷款审批）至关重要，但这些领域同时需要严格的隐私保证，导致可解释性与隐私之间存在冲突。现有工作表明解释方法可能泄露成员信息，但从业者缺乏系统指导来选择或部署平衡透明度与隐私的解释技术。

Method: DeepLeak系统包含三个主要部分：(1) 全面的泄露分析：开发更强的解释感知成员推理攻击来量化代表性解释方法在默认配置下的成员信息泄露；(2) 轻量级加固策略：引入实用的模型无关缓解措施，包括灵敏度校准噪声、归因裁剪和掩码，大幅减少成员泄露同时保持解释效用；(3) 根本原因分析：通过受控实验识别驱动泄露的算法特性（如归因稀疏性和灵敏度）。

Result: 在图像基准测试中评估15种解释技术，发现默认设置可泄露比先前报告多74.9%的成员信息。缓解措施可将泄露减少高达95%（最低46.5%），平均效用损失仅≤3.3%。

Conclusion: DeepLeak为隐私敏感机器学习中的可解释性提供了一条系统、可复现的安全路径，帮助从业者在保持解释效用的同时显著降低隐私风险。

Abstract: Machine learning (ML) explainability is central to algorithmic transparency in high-stakes settings such as predictive diagnostics and loan approval. However, these same domains require rigorous privacy guaranties, creating tension between interpretability and privacy. Although prior work has shown that explanation methods can leak membership information, practitioners still lack systematic guidance on selecting or deploying explanation techniques that balance transparency with privacy.
  We present DeepLeak, a system to audit and mitigate privacy risks in post-hoc explanation methods. DeepLeak advances the state-of-the-art in three ways: (1) comprehensive leakage profiling: we develop a stronger explanation-aware membership inference attack (MIA) to quantify how much representative explanation methods leak membership information under default configurations; (2) lightweight hardening strategies: we introduce practical, model-agnostic mitigations, including sensitivity-calibrated noise, attribution clipping, and masking, that substantially reduce membership leakage while preserving explanation utility; and (3) root-cause analysis: through controlled experiments, we pinpoint algorithmic properties (e.g., attribution sparsity and sensitivity) that drive leakage.
  Evaluating 15 explanation techniques across four families on image benchmarks, DeepLeak shows that default settings can leak up to 74.9% more membership information than previously reported. Our mitigations cut leakage by up to 95% (minimum 46.5%) with only <=3.3% utility loss on average. DeepLeak offers a systematic, reproducible path to safer explainability in privacy-sensitive ML.

</details>


### [6] [Security Parameter Analysis of the LINEture Post-Quantum Digital Signature Scheme](https://arxiv.org/abs/2601.03465)
*Yevgen Kotukh,Gennady Khalimov*

Main category: cs.CR

TL;DR: 该论文对LINEture后量子数字签名方案的安全参数进行了全面密码学分析，重点研究了三个主要参数对密码强度的影响，并提出了参数优化选择规则。


<details>
  <summary>Details</summary>
Motivation: 分析LINEture后量子数字签名方案的安全参数配置，理解各参数对密码强度的影响机制，为实际应用提供参数选择指导。

Method: 采用密码学分析方法，重点研究三个关键参数：具有二次影响的分组大小m、向量维度l、以及具有线性影响的会话密钥中子矩阵数量q。特别深入研究了验证机制中参数l的作用。

Result: 研究发现参数l具有双重性质：对猜测攻击抵抗无影响，但在验证机制中建立l×m比特的验证屏障。确定了阈值关系l < (q-1)×m，低于此阈值时l成为安全关键参数。提出了最优选择规则l ≈ (q-1)×m以实现最大密码效率。

Conclusion: 论文为LINEture后量子签名方案提供了参数优化指导，通过与NIST PQC标准比较分析，给出了实际参数推荐，有助于该方案的安全部署。

Abstract: This paper presents a comprehensive cryptographic analysis of the security parameters of the LINEture post-quantum digital signature scheme, which is constructed using matrix algebra over elementary abelian 2-groups. We investigate the influence of three principal parameters. First, the word size m (exhibiting quadratic impact), the second is a vector dimension l, and the third is a number of submatrices in the session key q (exhibiting linear impact) on cryptographic strength. Our analysis reveals a dualistic nature of the parameter l. According to the previous analysis, it does not affect resistance to guessing attacks. A deeper examination of the verification mechanism demonstrates that l establishes a kind of verification barrier of l times m bits. We establish the threshold relationship l less q minus 1 times m, below which parameter l becomes security-critical. The optimal selection rule l near q minus 1 times m is proposed for maximum cryptographic efficiency. Comparative analysis with NIST PQC standards and practical parameter recommendations are provided.

</details>


### [7] [Full-Stack Knowledge Graph and LLM Framework for Post-Quantum Cyber Readiness](https://arxiv.org/abs/2601.03504)
*Rasmus Erlemann,Charles Colyer Morris,Sanjyot Sathe*

Main category: cs.CR

TL;DR: 提出基于知识图谱的企业级后量子密码准备度评估框架，通过图论风险函数量化密码暴露风险，支持可扩展的持续监控和修复优先级排序


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算的出现威胁现有公钥密码系统，企业缺乏可扩展的定量框架来评估后量子密码准备度和优先迁移复杂基础设施

Method: 使用知识图谱建模企业密码资产、依赖关系和漏洞，通过图论风险函数量化后量子暴露风险，结合大语言模型和人工验证进行资产分类和风险归因

Result: 开发出可解释、标准化的准备度指标，支持持续监控、比较分析和修复优先级排序，提供依赖驱动的风险传播显式建模

Conclusion: 该框架为企业提供了可扩展的后量子密码准备度评估方法，通过知识图谱和风险量化支持系统化的密码迁移决策

Abstract: The emergence of large-scale quantum computing threatens widely deployed public-key cryptographic systems, creating an urgent need for enterprise-level methods to assess post-quantum (PQ) readiness. While PQ standards are under development, organizations lack scalable and quantitative frameworks for measuring cryptographic exposure and prioritizing migration across complex infrastructures. This paper presents a knowledge graph based framework that models enterprise cryptographic assets, dependencies, and vulnerabilities to compute a unified PQ readiness score. Infrastructure components, cryptographic primitives, certificates, and services are represented as a heterogeneous graph, enabling explicit modeling of dependency-driven risk propagation. PQ exposure is quantified using graph-theoretic risk functionals and attributed across cryptographic domains via Shapley value decomposition. To support scalability and data quality, the framework integrates large language models with human-in-the-loop validation for asset classification and risk attribution. The resulting approach produces explainable, normalized readiness metrics that support continuous monitoring, comparative analysis, and remediation prioritization.

</details>


### [8] [A Critical Analysis of the Medibank Health Data Breach and Differential Privacy Solutions](https://arxiv.org/abs/2601.03508)
*Zhuohan Cui,Qianqian Lang,Zikun Song*

Main category: cs.CR

TL;DR: 提出基于熵感知差分隐私的医疗数据保护框架，针对Medibank数据泄露问题，通过自适应预算分配和机制选择，在降低重识别风险的同时保持数据分析效用。


<details>
  <summary>Details</summary>
Motivation: 针对2022年Medibank健康保险数据泄露事件暴露的问题：未加密存储、集中访问、缺乏隐私保护分析，需要开发既能保护敏感医疗记录又能保持数据分析效用的解决方案。

Method: 提出熵感知差分隐私框架，集成拉普拉斯和高斯机制，采用自适应预算分配、TLS加密数据库访问、字段级机制选择和平滑敏感性模型，使用合成Medibank数据集进行实验验证。

Result: 实验结果显示重识别概率降低90.3%，分析效用损失控制在24%以下，框架符合GDPR第32条和澳大利亚隐私原则11.1，实现了隐私保护与实用性的平衡。

Conclusion: 该框架为医疗数据保护提供了可扩展、技术可行且符合法规的解决方案，有助于构建弹性、可信且符合监管要求的医疗分析系统。

Abstract: This paper critically examines the 2022 Medibank health insurance data breach, which exposed sensitive medical records of 9.7 million individuals due to unencrypted storage, centralized access, and the absence of privacy-preserving analytics. To address these vulnerabilities, we propose an entropy-aware differential privacy (DP) framework that integrates Laplace and Gaussian mechanisms with adaptive budget allocation. The design incorporates TLS-encrypted database access, field-level mechanism selection, and smooth sensitivity models to mitigate re-identification risks. Experimental validation was conducted using synthetic Medibank datasets (N = 131,000) with entropy-calibrated DP mechanisms, where high-entropy attributes received stronger noise injection. Results demonstrate a 90.3% reduction in re-identification probability while maintaining analytical utility loss below 24%. The framework further aligns with GDPR Article 32 and Australian Privacy Principle 11.1, ensuring regulatory compliance. By combining rigorous privacy guarantees with practical usability, this work contributes a scalable and technically feasible solution for healthcare data protection, offering a pathway toward resilient, trustworthy, and regulation-ready medical analytics.

</details>


### [9] [Deontic Knowledge Graphs for Privacy Compliance in Multimodal Disaster Data Sharing](https://arxiv.org/abs/2601.03587)
*Kelvin Uzoma Echenim,Karuna Pande Joshi*

Main category: cs.CR

TL;DR: 提出基于道义知识图谱的灾害响应隐私合规框架，整合灾害管理知识图谱与政策知识图谱，支持三种决策结果，实现细粒度访问控制与合规验证。


<details>
  <summary>Details</summary>
Motivation: 灾害响应需要共享异构数据（表格记录、无人机图像等），但现有系统通常采用简单的二元访问控制，在时间关键的工作流中不够灵活且脆弱。需要更细粒度的隐私合规机制来应对重叠的隐私法规要求。

Method: 构建灾害管理知识图谱（DKG）与政策知识图谱（PKG）的集成框架，PKG基于IoT-Reg和FEMA/DHS隐私驱动因素。设计发布决策函数支持三种结果：允许、阻止、允许但需转换。转换操作绑定义务，并通过溯源链接的派生工件验证转换后合规性；被阻止的请求记录为语义隐私事件。

Result: 在包含510万三元组DKG和31.6万张图像的测试中，实现了精确匹配的决策正确性、亚秒级单决策延迟，并在单图和联邦工作负载下均表现出交互式查询性能。

Conclusion: 该框架通过知识图谱集成实现了细粒度的隐私合规决策，支持复杂的灾害响应工作流，在保持高性能的同时确保数据共享的合规性。

Abstract: Disaster response requires sharing heterogeneous artifacts, from tabular assistance records to UAS imagery, under overlapping privacy mandates. Operational systems often reduce compliance to binary access control, which is brittle in time-critical workflows. We present a novel deontic knowledge graph-based framework that integrates a Disaster Management Knowledge Graph (DKG) with a Policy Knowledge Graph (PKG) derived from IoT-Reg and FEMA/DHS privacy drivers. Our release decision function supports three outcomes: Allow, Block, and Allow-with-Transform. The latter binds obligations to transforms and verifies post-transform compliance via provenance-linked derived artifacts; blocked requests are logged as semantic privacy incidents. Evaluation on a 5.1M-triple DKG with 316K images shows exact-match decision correctness, sub-second per-decision latency, and interactive query performance across both single-graph and federated workloads.

</details>


### [10] [Human Challenge Oracle: Designing AI-Resistant, Identity-Bound, Time-Limited Tasks for Sybil-Resistant Consensus](https://arxiv.org/abs/2601.03923)
*Homayoun Maleki,Nekane Sainz,Jon Legarda*

Main category: cs.CR

TL;DR: HCO是一种新型安全原语，通过实时、限速的人类认知挑战来持续验证用户身份，对抗大规模Sybil攻击


<details>
  <summary>Details</summary>
Motivation: 现有防御机制（如CAPTCHA和一次性人格证明）主要针对身份创建阶段，对长期大规模Sybil参与的保护有限，特别是在自动化求解器和AI系统不断改进的情况下

Method: 提出人类挑战预言机（HCO），发布与个体身份加密绑定的短期时间限制挑战，要求实时解决。核心洞察是利用人类认知努力（感知、注意力和交互推理）作为难以并行化或跨身份分摊的稀缺资源

Result: 形式化了HCO的设计目标和安全属性，证明在明确温和的假设下，维持s个活跃身份的成本在每个时间窗口内与s线性增长。提出了可接受挑战的抽象类别和具体浏览器实现，初步实证研究表明这些挑战对人类来说几秒内可解，但对严格时间限制下的当代自动化系统仍然困难

Conclusion: HCO为持续、限速的人类验证提供了新的安全原语，能够有效对抗大规模Sybil攻击，通过实时人类认知努力作为稀缺资源来增加攻击成本

Abstract: Sybil attacks remain a fundamental obstacle in open online systems, where adversaries can cheaply create and sustain large numbers of fake identities. Existing defenses, including CAPTCHAs and one-time proof-of-personhood mechanisms, primarily address identity creation and provide limited protection against long-term, large-scale Sybil participation, especially as automated solvers and AI systems continue to improve.
  We introduce the Human Challenge Oracle (HCO), a new security primitive for continuous, rate-limited human verification. HCO issues short, time-bound challenges that are cryptographically bound to individual identities and must be solved in real time. The core insight underlying HCO is that real-time human cognitive effort, such as perception, attention, and interactive reasoning, constitutes a scarce resource that is inherently difficult to parallelize or amortize across identities.
  We formalize the design goals and security properties of HCO and show that, under explicit and mild assumptions, sustaining s active identities incurs a cost that grows linearly with s in every time window. We further describe abstract classes of admissible challenges and concrete browser-based instantiations, and present an initial empirical study illustrating that these challenges are easily solvable by humans within seconds while remaining difficult for contemporary automated systems under strict time constraints.

</details>


### [11] [SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2601.03979)
*Andreea-Elena Bodea,Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CR

TL;DR: 本文对检索增强生成（RAG）系统中的隐私风险进行了首次系统性综述，提出了隐私风险分类法和RAG隐私流程框架，分析了现有缓解措施的成熟度。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术在结合大语言模型与领域知识库方面的广泛应用，数据隐私问题日益突出。尽管已有研究探讨RAG系统的隐私风险，但缺乏系统性的梳理和统一框架。本文旨在回答：RAG中存在哪些隐私风险，如何测量和缓解这些风险。

Method: 通过系统文献综述方法，收集和分析关注RAG隐私的研究工作。将发现系统化为全面的隐私风险集合、缓解技术和评估策略，并创建了两个主要成果：RAG隐私风险分类法和RAG隐私流程图。

Result: 建立了首个RAG隐私风险系统化框架，识别了从对抗攻击到缓解措施的各种隐私风险。提出的分类法和流程图为理解和评估RAG隐私问题提供了结构化工具，同时揭示了缓解隐私风险时的重要考虑因素和现有缓解措施的成熟度现状。

Conclusion: 本文填补了RAG隐私研究领域的系统性空白，不仅首次系统化了隐私风险和缓解措施，还揭示了在缓解RAG系统隐私风险时的重要考虑因素，并评估了现有缓解措施的成熟度，为未来研究和实践提供了基础框架。

Abstract: The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained "knowledge" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出多智能体工作流，将主要任务描述优化与约束条件解耦，通过定量评分反馈迭代改进提示词，显著提升LLM输出对形式约束的遵从性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成内容相关但不符合形式约束的输出，传统提示优化方法只关注主要任务描述的重述，忽略了作为响应接受标准的细粒度约束条件

Method: 提出新颖的多智能体工作流，将主要任务描述的优化与其约束条件解耦，使用定量评分作为反馈，迭代重写和改进提示词

Result: 评估表明该方法生成的修订提示词在Llama 3.1 8B和Mixtral-8x 7B等模型上产生显著更高的遵从性评分

Conclusion: 解耦任务描述和约束条件优化的多智能体工作流能有效提高LLM输出对形式约束的遵从性，比传统提示优化方法更有效

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [13] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: 该研究探索强化学习智能体通过内省机制推断自身内部状态，引入基于生物疼痛启发的内省探索组件，使用隐马尔可夫模型从在线观察中推断"疼痛信念"，研究自我意识如何影响智能体学习能力。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能体如何建模内部心理状态对于推进AI中的心理理论至关重要。证据表明自我意识和他人意识存在统一系统，本研究通过让强化学习智能体推断自身内部状态来探索这种自我意识。

Method: 在网格世界环境中，引入受生物疼痛启发的内省探索组件，使用隐马尔可夫模型从在线观察中推断"疼痛信念"，将该信号整合到主观奖励函数中，研究自我意识对智能体学习能力的影响，并比较正常和慢性疼痛感知模型的性能差异。

Result: 结果显示，具有内省能力的智能体总体上显著优于标准基线智能体，并且能够复制复杂的人类行为模式。

Conclusion: 内省机制能够有效提升强化学习智能体的性能，模拟人类自我意识的工作方式，为AI心理理论研究提供了计算框架，特别在疼痛感知建模方面具有重要启示。

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [14] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: 提出基于成熟度的框架，通过明确的测量机制来认证具身AI系统，需要结构化评估框架、量化评分机制和多目标权衡方法，以不确定性量化为例证，通过无人机系统检测案例展示可行性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统化的方法来认证具身AI系统的可信度，需要建立结构化框架来评估和量化这些系统的成熟度和可信度。

Method: 提出基于成熟度的认证框架，包括结构化评估框架、量化评分机制和多目标权衡导航方法，以不确定性量化作为示例测量机制，通过无人机系统检测案例进行验证。

Result: 展示了该框架在无人机系统检测案例中的可行性，证明了通过明确的测量机制可以系统化地评估和认证具身AI系统的成熟度。

Conclusion: 基于成熟度的认证框架为具身AI系统的可信度评估提供了系统化方法，通过结构化评估、量化测量和多目标权衡，能够有效认证这些系统的成熟度和可靠性。

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [15] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: CPGPrompt是一个自动提示系统，将临床实践指南转化为结构化决策树，利用大语言模型动态导航进行患者病例评估，在专科转诊决策上表现优异，但在多类别路径分配上性能有所下降。


<details>
  <summary>Details</summary>
Motivation: 临床实践指南为患者护理提供循证建议，但将其整合到人工智能中仍面临挑战。先前的方法如基于规则的系统存在可解释性差、指南遵循不一致和领域适用性窄等限制。

Method: 开发CPGPrompt自动提示系统，将叙述性临床指南转化为结构化决策树，利用大语言模型动态导航进行患者病例评估。在三个领域（头痛、下背痛和前列腺癌）生成合成病例，测试不同的决策场景。

Result: 二元专科转诊分类在所有领域表现一致强劲（F1: 0.85-1.00），召回率高（1.00 ± 0.00）。多类别路径分配性能下降，领域间存在差异：头痛（F1: 0.47）、下背痛（F1: 0.72）、前列腺癌（F1: 0.77）。性能差异反映了各指南的结构特点。

Conclusion: CPGPrompt系统在将临床指南整合到人工智能方面取得进展，特别是在二元决策任务上表现优异。多类别路径分配的性能差异揭示了不同临床指南结构对AI系统性能的影响，为未来改进提供了方向。

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [16] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 大型基础模型在医疗AI中面临个性化治疗的挑战，存在泛化悖论、隐私-性能悖论等矛盾，无法替代N-of-1试验，但两者可以互补结合


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在医疗AI中展现出潜力，但在提供真正个性化治疗建议方面存在挑战。研究发现模型存在泛化悖论：在一个临床研究中表现良好的模型在其他研究中可能表现很差，表明个性化与外部有效性之间存在矛盾。此外，还有隐私-性能悖论、规模-特异性悖论和自动化-同理心悖论等更广泛的矛盾需要解决。

Method: 提出混合框架，结合大型基础模型和N-of-1试验的优势。大型基础模型擅长从多模态数据中快速生成假设和排名干预候选方案，而N-of-1试验（交叉自我实验）作为个性化医学中个体因果推断的金标准，擅长为特定个体提供因果验证。框架中大型基础模型生成带有不确定性估计的干预候选排名，然后触发后续的N-of-1试验。

Result: 分析表明大型基础模型无法替代N-of-1试验，但两者具有互补性。通过明确区分预测与因果关系，并明确解决已识别的悖论，可以负责任地将AI整合到个性化医学中。混合框架能够利用大型基础模型的假设生成能力和N-of-1试验的因果验证能力，实现真正的个性化治疗。

Conclusion: 大型基础模型和N-of-1试验在个性化医学中是互补的。大型基础模型擅长从群体模式中快速生成假设，而N-of-1试验擅长为个体提供因果验证。通过结合两者的混合框架，可以解决个性化治疗中的各种悖论，实现负责任的人工智能整合。明确预测与因果关系的边界对于个性化医学的AI应用至关重要。

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [17] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: STAR-S框架通过自学习循环将安全规则推理学习集成到LLM中，有效防御越狱攻击


<details>
  <summary>Details</summary>
Motivation: 现有方法通过训练模型在响应前对安全规则进行推理来提升安全性，但难以明确设计或直接获取有效的安全推理形式，需要一种系统化的方法来学习安全规则推理

Method: 提出STAR-S框架，通过自学习循环集成安全规则推理学习：1) 在安全规则指导下引出推理和反思；2) 利用微调增强安全推理；3) 重复此过程形成协同循环，模型对安全规则的理解改进使其能生成更好的推理数据用于进一步训练

Result: 实验表明STAR-S能有效防御越狱攻击，性能优于基线方法

Conclusion: STAR-S框架通过自学习循环系统化地学习安全规则推理，为LLM安全部署提供了有效的防御机制

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [18] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种新的神经符号框架来评估LLM的推理过程，发现扩展token生成并非深度推理的必要条件，并揭示了混合长短CoT数据训练的风险以及蒸馏到小模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前CoT评估方法的局限性使得难以区分性能提升是来自真正的推理能力还是仅仅是输出长度的增加，需要更全面的过程中心评估框架。

Method: 提出了一种非侵入性的神经符号框架，用于对推理过程进行全面评估，通过该框架识别了四种不同的行为原型并诊断了失败模式。

Result: 发现扩展token生成并非深度推理的必要条件；混合长短CoT数据训练会导致过早饱和和崩溃；将推理能力蒸馏到小模型只能复制行为长度但无法复制逻辑效能。

Conclusion: 需要更精细的评估方法来理解LLM的推理能力，模型规模、训练策略和推理模式都会显著影响推理性能，简单的长度扩展不能保证深度推理。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [19] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: 本文提出SAE-Steering方法，通过稀疏自编码器分解大推理模型的隐藏状态，识别策略特定特征作为控制向量，有效控制推理策略，提升推理准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型虽然展现出类似人类的认知推理策略，但其自主选择策略往往产生低效甚至错误的推理路径。现有方法难以控制细粒度推理策略，因为策略概念在隐藏状态中纠缠在一起。

Method: 使用稀疏自编码器将策略纠缠的隐藏状态分解为解耦的特征空间，提出SAE-Steering两阶段特征识别流程：首先召回放大策略特定关键词logits的特征，过滤掉99%以上特征；然后根据控制效果对剩余特征进行排序。

Result: SAE-Steering在控制效果上比现有方法提升超过15%；通过控制推理策略，可以将大推理模型从错误路径重定向到正确路径，实现7%的绝对准确率提升。

Conclusion: SAE-Steering方法能够有效识别和控制推理策略，提高大推理模型的可靠性和灵活性，为解决推理策略控制问题提供了有效解决方案。

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [20] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: SafeRemind：一种通过动态注入安全提醒短语到思考步骤中，提升大型推理模型安全性的解码时防御方法


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思考步骤取得显著成功，但这些步骤可能放大不安全行为。传统防御机制因忽视推理动态而失效，研究发现思考步骤中出现安全提醒短语对确保模型安全至关重要。

Method: 提出SafeRemind解码时防御方法，利用熵触发器在决策锁定点进行干预，动态将安全提醒短语注入思考步骤，无需参数更新即可将有害轨迹重定向到更安全结果。

Result: 在5个大型推理模型和6个基准测试上的广泛评估表明，SafeRemind显著提升安全性，改进幅度高达45.5个百分点，同时保持核心推理效用。

Conclusion: SafeRemind通过动态注入安全提醒短语到思考步骤，有效解决了大型推理模型特有的安全风险，在保持推理能力的同时大幅提升安全性。

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [21] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: SandwichR是一种新颖的推理方法，采用"答案-推理-答案"范式，通过一致性强化学习策略，在保持推理准确性的同时显著降低查询纠正的延迟。


<details>
  <summary>Details</summary>
Motivation: 查询纠正是现代搜索管道的关键入口，需要在实时延迟约束下保持高准确性。链式思维推理能提高准确性但延迟过高，而先输出答案再推理的方法无法利用推理能力改进准确性。

Method: 提出SandwichR方法，采用答案-推理-答案范式，生成初始纠正、显式推理过程和最终精炼纠正。通过一致性感知强化学习策略：专用一致性奖励确保初始和最终纠正的对齐，基于边际的拒绝采样优先处理推理带来最大纠正增益的边界样本。同时构建了高质量的查询纠正数据集。

Result: 实验结果表明，SandwichR实现了与标准链式思维推理相当的最先进准确性，同时提供40-70%的延迟降低，解决了在线搜索中的延迟-准确性权衡问题。

Conclusion: SandwichR通过显式对齐快速初始答案与事后推理，实现了低延迟查询纠正而不牺牲推理感知的准确性，解决了链式思维推理在实时应用中的延迟瓶颈。

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [22] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: 本文提出使用自动生成的领域特定启发式方法，结合通用搜索算法（GBFS），将个性化用药规划从最多7种药物扩展到至少28种，显著提高了覆盖率和规划时间。


<details>
  <summary>Details</summary>
Motivation: 个性化用药规划需要为每位患者选择药物并确定给药方案以实现特定医疗目标。先前的研究虽然证明了自动化规划器能够生成个性化治疗方案，但实际应用中最多只能考虑7种药物，这在临床实践中远远不够，需要扩展到更实用的规模。

Method: 通过编程方式指定领域（定义初始状态和状态转移过程），使用大型语言模型（LLM）生成问题特定的启发式函数，然后与固定的搜索算法（贪心最佳优先搜索GBFS）结合使用。

Result: 该方法在覆盖率和规划时间方面取得了显著改进，能够将用药规划的药物数量扩展到至少28种，使个性化用药规划更接近实际临床应用。

Conclusion: 自动生成的领域特定启发式方法结合通用搜索算法，能够显著扩展个性化用药规划的规模，使其从理论可行性向实际临床应用迈进了一步。

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [23] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: EntroCoT框架通过熵基分割和蒙特卡洛评估机制，自动识别和优化低质量的思维链监督数据，构建高质量训练数据集，提升大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有微调数据集存在"答案正确但推理错误"的问题，即最终答案正确但中间推理步骤存在幻觉、冗余或逻辑错误，这影响了思维链提示的有效性。

Method: 提出EntroCoT统一框架：1）基于熵的机制在不确定节点分割推理轨迹；2）蒙特卡洛rollout机制评估每个步骤对最终答案的边际贡献；3）准确过滤欺骗性推理样本，构建高质量数据集。

Result: 在数学基准测试上的广泛实验表明，使用EntroCoT构建的子集进行微调，其性能始终优于全数据集监督的基线方法。

Conclusion: EntroCoT通过自动识别和优化低质量思维链监督数据，有效解决了"答案正确但推理错误"的问题，显著提升了数学推理任务的性能。

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [24] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: 论文提出ROI-Reasoning框架，通过元认知微调和理性感知强化学习，让大语言模型在严格token预算下进行推理任务分配


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大推理能力，但无法自动判断不同任务所需的计算量，在严格全局token约束下需要智能分配计算资源

Method: 提出两阶段框架：1) 元认知微调，让模型预测推理成本和预期效用；2) 理性感知强化学习，在硬token预算下优化序列决策

Result: 在预算数学推理基准测试中，ROI-Reasoning在严格计算预算下显著提高总体得分，同时大幅减少遗憾值

Conclusion: 该框架赋予LLMs内在的预算感知理性能力，能够战略性地分配计算资源，在有限计算预算下实现更好的推理性能

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [25] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种使用答案集编程（ASP）计算理性闭包（RC）的声明式方法，实现了从知识库自动构建最小排序模型并进行蕴涵检查，相比现有命令式实现具有更好的计算效率。


<details>
  <summary>Details</summary>
Motivation: 可废止蕴涵处理从不完全信息中得出合理结论的问题。KLM框架为可废止蕴涵建模提供了基础，其中理性闭包（RC）是最重要的算法之一。现有实现多为命令式方法，本文旨在提供一种基于ASP的声明式定义，以更高效地计算RC。

Method: 采用答案集编程（ASP）为理性闭包提供声明式定义。该方法能够从给定知识库自动构建最小排序模型，并支持对指定查询进行蕴涵检查。作者形式化证明了ASP编码的正确性。

Result: 实验评估表明，基于ASP的方法不仅遵循RC的理论基础，而且在计算效率上优于现有的命令式实现（特别是InfOCF求解器）。

Conclusion: 本文成功展示了ASP在实现理性闭包计算中的有效性，提供了一种既符合理论要求又具有更好计算性能的替代方案。

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [26] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: 使用ASP对意大利刑法典建模，通过半自动学习司法判例生成法律规则，支持刑事审判推理和结果预测


<details>
  <summary>Details</summary>
Motivation: 开发工具支持法律专家在刑事审判阶段进行推理和预测法律结果，提高决策过程的透明度和可解释性

Method: 使用答案集编程（ASP）对意大利刑法典条款进行编码建模，包括"人身犯罪"和财产犯罪；利用归纳逻辑编程系统从案例中泛化法律规则；处理编码过程中的矛盾，并通过稳定模型的"支持性"提供解释

Result: 开发出能够基于先前判决验证模型、为新案件生成可能决策、提供解释的工具；系统能够处理编码矛盾，并通过自动可解释性澄清司法决策逻辑

Conclusion: ASP方法能够有效建模刑法典并学习法律规则，工具支持法律专家决策，提高司法过程的透明度和可解释性

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [27] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种基于答案集编程（ASP）的方法，用于从决策树模型中生成多种解释类型，相比基于SAT的方法更具灵活性且支持枚举所有可能解释。


<details>
  <summary>Details</summary>
Motivation: 决策树模型（包括随机森林和梯度提升决策树）在机器学习中广泛应用，但其复杂结构使其难以解释，特别是在需要正式论证的安全关键应用中。虽然已有研究通过自动推理技术推导逻辑和溯因解释，但需要更灵活的方法来生成多种类型的解释。

Method: 提出了一种基于答案集编程（ASP）的方法，用于生成充分解释、对比解释、多数解释和树特定解释。相比基于SAT的方法，ASP方法在编码用户偏好方面更具灵活性，并支持枚举所有可能的解释。

Result: 在多样化数据集上进行了实证评估，证明了该方法的有效性，并与现有方法进行了比较，展示了其优势和局限性。

Conclusion: 基于ASP的方法为决策树模型提供了灵活且全面的解释生成框架，能够支持多种解释类型的生成和枚举，在需要模型可解释性的安全关键应用中具有实用价值。

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [28] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: 本文研究了ASP在大型配置问题中的可扩展性，特别是针对包含超过30,000个组件的电子系统配置问题，分析了当前ASP技术的潜力和限制，并提出了解决"接地瓶颈"的新方法。


<details>
  <summary>Details</summary>
Motivation: 研究当前ASP求解技术是否能够扩展到大型配置问题，特别是电子系统配置这类可能包含超过30,000个组件的复杂问题，解决ASP中的"接地瓶颈"问题。

Method: 1. 分析当前ASP技术在处理大型配置问题时的潜力和限制；2. 重点研究解决"接地瓶颈"的方法；3. 探索增量求解方法；4. 开发"约束感知猜测"新方法以减少内存需求。

Result: 1. 增量求解方法在实践中证明有效；2. 但即使在增量方法中，内存需求仍然构成显著限制；3. 基于接地分析开发的"约束感知猜测"方法显著减少了内存需求。

Conclusion: 当前ASP技术在大型配置问题中存在内存限制，但通过"约束感知猜测"等创新方法可以显著改善可扩展性，为解决大型配置问题提供了新的技术途径。

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [29] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Chen Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1框架通过过程级推理验证，将可验证奖励与随机金融环境连接，解决标准强化学习在金融决策中的奖励黑客问题


<details>
  <summary>Details</summary>
Motivation: 强化学习在数学和编程等可验证奖励领域表现出色，但扩展到金融决策时面临市场随机性的挑战：奖励可验证但本质上是噪声的，导致标准RL退化为奖励黑客

Method: 提出Trade-R1训练框架，通过过程级推理验证连接可验证奖励与随机环境。核心创新是将冗长金融文档的推理评估转化为结构化检索增强生成任务，构建三角一致性度量（检索证据、推理链、决策之间的成对对齐）作为噪声市场回报的有效性过滤器。探索两种奖励整合策略：固定效应语义奖励（FSR）和动态效应语义奖励（DSR）

Result: 在不同国家资产选择实验中，该范式减少了奖励黑客问题，DSR实现了优越的跨市场泛化能力，同时保持了最高的推理一致性

Conclusion: Trade-R1框架通过过程级推理验证有效解决了金融决策中强化学习的奖励黑客问题，为随机环境中的可验证奖励应用提供了新范式

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [30] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: 论文提出DOT方法解决大模型在简单问题上过度推理的问题，通过动态截断冗余token减少78%推理token使用，同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习增强的大推理模型在简单查询上表现出过度冗长，导致部署成本高昂。现有基于显式长度惩罚的高效推理方法存在优化冲突，且未深入探究过度思考的生成机制

Method: 提出动态异常截断(DOT)训练时干预方法，选择性抑制冗余token；结合辅助KL正则化和预测动态采样确保稳定收敛

Result: 在多个模型规模上显著扩展效率-性能帕累托前沿；在AIME-24上减少78%推理token使用，同时提高准确率，超越现有高效推理方法

Conclusion: DOT方法有效解决了模型在简单输入上的长度偏移问题，在保持复杂问题长程推理能力的同时显著提升推理效率

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [31] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer：一个基于世界模型的移动GUI智能体前瞻框架，通过文本草图世界模型预测动作后状态，提升长时程任务的执行成功率


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体大多是反应式的，主要基于当前屏幕做出决策，这在长时程任务中存在局限性。构建世界模型能够预测动作结果并支持更好的决策制定，但挑战在于模型需要具备空间感知能力预测动作后状态，同时保持高效部署

Method: 提出MobileDreamer框架，包含文本草图世界模型和GUI智能体推演想象。文本草图世界模型通过学习过程将数字图像转换为关键任务相关草图，并设计新颖的顺序不变学习策略来保留GUI元素的空间信息。GUI智能体推演想象策略利用世界模型的预测能力优化动作选择过程

Result: 在Android World上的实验显示，MobileDreamer实现了最先进的性能，将任务成功率提高了5.25%。世界模型评估进一步验证了文本草图建模能够准确预测关键GUI元素

Conclusion: MobileDreamer通过有效的世界模型前瞻框架，显著提升了移动GUI智能体在长时程任务中的性能，为实际部署提供了可行的解决方案

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [32] [ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows](https://arxiv.org/abs/2601.04060)
*Jinwei Su,Qizhen Lan,Zeyu Wang,Yinghui Xia,Hairu Wen,Yiqun Duan,Xi Xiao,Tianyu Shi,Yang Jingsong,Lewei He*

Main category: cs.AI

TL;DR: ComfySearch：一个基于验证引导的智能体框架，用于在ComfyUI平台上探索组件空间并生成可执行的工作流，解决了现有方法在复杂创意任务中通过率低和质量有限的问题。


<details>
  <summary>Details</summary>
Motivation: ComfyUI平台上的AI生成内容已从单一模型发展到模块化工作流，但大量组件和严格的图约束使得保持长期结构一致性困难，导致通过率低且工作流质量有限。

Method: 提出了ComfySearch框架，这是一个智能体系统，通过验证引导的工作流构建方法，有效探索组件空间并生成功能性的ComfyUI管道。

Result: 实验表明，ComfySearch在复杂和创意任务上显著优于现有方法，实现了更高的可执行性（通过）率、更高的解决方案率和更强的泛化能力。

Conclusion: ComfySearch框架通过智能探索组件空间和验证引导的构建过程，有效解决了ComfyUI工作流生成中的可执行性和质量问题，为复杂创意任务提供了更可靠的解决方案。

Abstract: AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.

</details>


### [33] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 该研究提出了"智能体漂移"概念，指多智能体LLM系统在长期运行中行为、决策质量和协调性逐渐退化的问题，并开发了量化框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在复杂任务分解和协作解决问题方面表现出强大能力，但其长期行为稳定性尚未得到充分研究。需要理解智能体在长时间交互序列中行为退化的现象，以确保生产系统的可靠性和安全性。

Method: 提出了智能体漂移的理论框架，包括语义漂移、协调漂移和行为漂移三种表现形式。开发了智能体稳定性指数（ASI）作为量化框架，包含12个维度。通过仿真分析和理论建模验证漂移现象，并提出三种缓解策略：情景记忆整合、漂移感知路由协议和自适应行为锚定。

Result: 研究表明，未经控制的智能体漂移会导致任务完成准确率显著下降，增加人工干预需求。理论分析显示提出的缓解策略能显著减少漂移相关错误，同时保持系统吞吐量。

Conclusion: 该工作建立了监测、测量和缓解生产智能体AI系统中智能体漂移的基础方法论，对企业部署可靠性和AI安全研究具有直接意义，为多智能体系统的长期稳定性提供了理论框架和实践指导。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [34] [Failure-Resilient and Carbon-Efficient Deployment of Microservices over the Cloud-Edge Continuum](https://arxiv.org/abs/2601.04123)
*Francisco Ponce,Simone Gazza,Andrea D'Iapico,Roberto Amadini,Antonio Brogi,Stefano Forti,Saverio Giallorenzo,Pierluigi Plebani,Davide Usai,Monica Vitali,Gianluigi Zavattaro,Jacopo Soldani*

Main category: cs.DC

TL;DR: FREEDA工具链用于在云边连续体上自动化部署微服务应用，平衡故障恢复能力、性能和碳效率等冲突目标


<details>
  <summary>Details</summary>
Motivation: 在异构动态的云边基础设施上部署微服务应用需要平衡相互冲突的目标，如故障恢复能力、性能和环境可持续性

Method: 开发FREEDA工具链，持续适应变化的操作条件、资源可用性和可持续性约束，通过迁移服务、调整配置和重新平衡工作负载来重新配置部署

Result: FREEDA能够自主重新配置部署，成功在恢复能力、效率和环境影响之间实现最佳平衡

Conclusion: FREEDA工具链有效解决了在云边连续体上部署微服务应用时平衡故障恢复能力和碳效率的挑战

Abstract: Deploying microservice-based applications (MSAs) on heterogeneous and dynamic Cloud-Edge infrastructures requires balancing conflicting objectives, such as failure resilience, performance, and environmental sustainability. In this article, we introduce the FREEDA toolchain, designed to automate the failure-resilient and carbon-efficient deployment of MSAs over the Cloud-Edge Continuum.
  The FREEDA toolchain continuously adapts deployment configurations to changing operational conditions, resource availability, and sustainability constraints, aiming to maintain the MSA quality and service continuity while reducing carbon emissions. We also introduce an experimental suite using diverse simulated and emulated scenarios to validate the effectiveness of the toolchain against real-world challenges, including resource exhaustion, node failures, and carbon intensity fluctuations. The results demonstrate FREEDA's capability to autonomously reconfigure deployments by migrating services, adjusting flavour selections, or rebalancing workloads, successfully achieving an optimal balance among resilience, efficiency, and environmental impact.

</details>


### [35] [A Scheduling Framework for Efficient MoE Inference on Edge GPU-NDP Systems](https://arxiv.org/abs/2601.03992)
*Qi Wu,Chao Fang,Jiayuan Chen,Ye Lin,Yueqi Zhang,Yichuan Bai,Yuan Du,Li Du*

Main category: cs.DC

TL;DR: 本文提出了一种针对边缘GPU-NDP系统的MoE模型高效推理框架，通过张量并行、负载均衡调度和无数据集预取三大优化，在资源受限环境下实现2.41倍平均加速。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过解耦模型容量与活跃计算便于边缘部署，但其大内存占用需要GPU-NDP系统将专家卸载到专用处理单元。然而在边缘GPU-NDP系统上部署MoE面临三大挑战：1）由于非均匀专家选择和专家并行导致的NDP单元间严重负载不均衡；2）NDP单元内专家计算期间GPU利用率不足；3）为预取需要大量数据预分析，但专家激活模式难以预测。

Method: 提出包含三个关键优化的高效推理框架：1）利用MoE推理中未充分探索的张量并行，在边缘低批次场景下跨多个NDP单元同时分区和计算大型专家参数；2）负载均衡感知调度算法，将专家计算分配到NDP单元和GPU以最大化资源利用率；3）无数据集预取策略，主动加载频繁访问的专家以最小化激活延迟。

Result: 实验结果表明，该框架使GPU-NDP系统在端到端延迟上相比最先进方法平均实现2.41倍加速，最高可达2.56倍，显著提升了资源受限环境下的MoE推理效率。

Conclusion: 该框架通过创新的张量并行、负载均衡调度和无数据集预取策略，有效解决了边缘GPU-NDP系统上MoE模型部署的关键挑战，为资源受限环境下的高效MoE推理提供了实用解决方案。

Abstract: Mixture-of-Experts (MoE) models facilitate edge deployment by decoupling model capacity from active computation, yet their large memory footprint drives the need for GPU systems with near-data processing (NDP) capabilities that offload experts to dedicated processing units. However, deploying MoE models on such edge-based GPU-NDP systems faces three critical challenges: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation within NDP units, and 3) extensive data pre-profiling necessitated by unpredictable expert activation patterns for pre-fetching. To address these challenges, this paper proposes an efficient inference framework featuring three key optimizations. First, the underexplored tensor parallelism in MoE inference is exploited to partition and compute large expert parameters across multiple NDP units simultaneously towards edge low-batch scenarios. Second, a load-balancing-aware scheduling algorithm distributes expert computations across NDP units and GPU to maximize resource utilization. Third, a dataset-free pre-fetching strategy proactively loads frequently accessed experts to minimize activation delays. Experimental results show that our framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches, significantly enhancing MoE inference efficiency in resource-constrained environments.

</details>
