<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 78]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: BotzoneBench：基于固定技能层次游戏AI的LLM战略能力评估框架，实现线性时间绝对技能测量和跨时间可解释性


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要关注静态推理任务，无法有效评估动态战略决策能力。现有的游戏基准采用LLM-vs-LLM锦标赛，存在二次计算成本、依赖瞬态模型池、缺乏稳定性能锚点等问题，难以进行纵向跟踪

Method: 基于Botzone平台的竞争基础设施，构建BotzoneBench评估框架，将LLM评估锚定到固定技能层次校准的游戏AI上。涵盖8种多样化游戏，从确定性完美信息棋盘游戏到随机不完美信息纸牌游戏。系统评估了5个旗舰模型的177,047个状态-动作对

Result: 揭示了显著的性能差异和独特的战略行为模式。表现最佳的模型在多个领域达到中高等级专业游戏AI的熟练程度。实现了线性时间绝对技能测量，具有稳定的跨时间可解释性

Conclusion: 这种锚定评估范式可推广到任何具有明确定义技能层次的领域，为评估交互式AI能力建立了可扩展、可重用的框架

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [2] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: VeRA框架通过将基准问题转换为可执行规范，自动生成无限验证变体，解决传统评估的静态性和记忆化问题，提升AI评估的鲁棒性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 当前大多数评估方案存在"静态"问题：相同问题被重复使用，导致记忆化、格式利用和最终饱和。为了真实衡量AI进展，需要构建性鲁棒的评估，而非事后检测。

Method: VeRA框架将基准问题转换为可执行规范，包含三个组件：(1)带占位符的自然语言模板；(2)采样有效配置的一致性生成器；(3)验证参数并计算正确答案的确定性验证器。VeRA提供两种模式：VeRA-E（等效）保持底层逻辑不变，检测记忆化；VeRA-H（强化）系统增加复杂性，生成困难任务。

Result: 评估16个前沿模型发现：(1)VeRA-E提高评估质量并揭示污染模式；(2)VeRA-H能够无需人工生成具有可靠标签的困难任务；(3)VeRA建立了验证基准作为通用范式。框架将基准从静态对象重新概念化为按需生成新鲜验证实例的可执行规范。

Conclusion: VeRA使任何可验证领域的评估能够无限扩展而不牺牲标签完整性，增强了评估的鲁棒性和成本效益。所有代码和数据集已开源以促进未来研究。

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [3] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic是一个通过生成-验证-修复循环自动合成可执行生成器-验证器程序对的框架，用于扩展可验证训练信号，在逻辑推理任务上实现了任务族级别的扩展。


<details>
  <summary>Details</summary>
Motivation: 可验证训练信号的扩展是强化学习从可验证奖励（RLVR）的关键瓶颈。逻辑推理是自然基底，但现有合成管道要么依赖专家编写的代码，要么在固定模板内操作，限制了任务级别的扩展。

Method: 提出SSLogic框架，通过迭代合成和修复可执行的生成器-验证器程序对，在生成-验证-修复闭环中实现连续任务族演化。引入多门验证协议，结合多策略一致性检查和对抗性盲审，确保可靠性。

Result: 从400个种子任务族开始，经过两轮演化扩展到953个任务族和21,389个可验证实例（从5,718个）。在SSLogic演化数据上训练相比种子基线有显著提升：SynLogic +5.2，BBEH +1.4，AIME25 +3.0，Brumo25 +3.7。

Conclusion: SSLogic框架能够有效扩展可验证训练信号，实现任务族级别的自动演化，在多个基准测试中表现出显著性能提升，为解决RLVR的可验证信号扩展瓶颈提供了有效方案。

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [4] [A Geometric Taxonomy of Hallucinations in LLMs](https://arxiv.org/abs/2602.13224)
*Javier Marín*

Main category: cs.AI

TL;DR: 该论文提出幻觉的三分法分类，发现不同类型幻觉在嵌入空间中具有不同几何特征，揭示了基于嵌入的检测方法对某些幻觉类型有效，但对事实错误无效的理论限制。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型中的"幻觉"概念过于笼统，混淆了不同性质的现象。需要建立一个分类体系来区分不同类型的幻觉，并研究它们在嵌入空间中的几何特征，以明确基于嵌入的检测方法的适用范围和局限性。

Method: 提出幻觉的三分法分类：不忠实性（未利用上下文）、虚构（发明语义无关内容）、事实错误（正确概念框架内的错误主张）。通过几何分析研究不同类型幻觉在嵌入空间中的特征，包括域内/跨域检测性能、判别方向的余弦相似度等指标。

Result: 发现显著不对称性：在标准基准测试中，幻觉检测在域内表现良好（AUROC 0.76-0.99），但跨域时降至随机水平（0.50）。判别方向在不同域间近似正交（平均余弦相似度-0.07）。对于人工制作的虚构内容，单一全局方向即可达到0.96 AUROC。事实错误的检测性能仅为0.478 AUROC，与随机水平无异。

Conclusion: 基于嵌入的检测方法对不忠实性和虚构类型有效，但对事实错误无效。这是因为嵌入编码的是分布共现模式而非与外部现实的对应关系。不同类型幻觉具有不同的几何结构，反映了它们本质上的差异。事实错误的检测需要外部验证机制。

Abstract: The term "hallucination" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual patterns occupy similar embedding regions regardless of truth value. The contribution is a geometric taxonomy clarifying the scope of embedding-based detection: Types I and II are detectable; Type III requires external verification mechanisms.

</details>


### [5] [Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection](https://arxiv.org/abs/2602.13226)
*Xuecong Li,Xiaohong Li,Qiang Hu,Yao Zhang,Junjie Wang*

Main category: cs.AI

TL;DR: 提出VaryBalance方法，通过比较原始文本与LLM改写版本之间的差异来检测AI生成文本，相比现有方法有显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成文本检测器存在局限性：要么依赖不切实际的白盒设置假设，要么仅依赖文本层面特征，导致检测能力不精确。需要一种简单有效且实用的检测方法。

Method: 提出VaryBalance方法，核心思想是：相比LLM生成的文本，人类文本与其通过LLM改写的版本之间存在更大差异。通过计算平均标准差来量化这种差异，从而区分人类文本和LLM生成文本。

Result: VaryBalance在AUROC指标上比当前最先进的检测器Binoculars提升高达34.3%，在多种生成模型和语言上保持鲁棒性。

Conclusion: VaryBalance是一种简单有效且实用的LLM生成文本检测方法，通过量化人类文本与LLM改写版本之间的差异，显著提升了检测性能。

Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater difference between human texts and their rewritten version via LLMs. Leveraging this observation, VaryBalance quantifies this through mean standard deviation and distinguishes human texts and LLM-generated texts. Comprehensive experiments demonstrated that VaryBalance outperforms the state-of-the-art detectors, i.e., Binoculars, by up to 34.3\% in terms of AUROC, and maintains robustness against multiple generating models and languages.

</details>


### [6] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 论文提出轨迹主导帕累托优化框架，认为智能停滞源于轨迹空间中的帕累托陷阱而非学习能力不足，定义了陷阱逃离难度指数来量化约束刚性。


<details>
  <summary>Details</summary>
Motivation: 针对人工智能系统在长期适应性中出现的停滞现象，作者认为这种限制主要不是源于学习、数据或模型容量不足，而是源于智能随时间优化的深层结构特性。

Method: 将智能表述为轨迹层面的多目标权衡现象，引入轨迹主导帕累托优化框架，定义轨迹空间中的帕累托陷阱，提出陷阱逃离难度指数来量化约束刚性，并建立帕累托陷阱的形式化分类体系。

Result: 动态智能上限被证明是轨迹层面主导性的必然几何结果，独立于学习进展或架构规模。通过最小智能体-环境模型展示了轨迹层面的发散现象。

Conclusion: 研究将智能的关注点从终端性能转移到优化几何，为诊断和克服自适应系统中的长期发展约束提供了原则性框架。

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [7] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出无需训练的双循环对抗自进化框架，通过攻击者循环生成更强越狱提示，防御者循环构建分层知识库，在推理时检索组合知识来同时保持角色保真度和安全性。


<details>
  <summary>Details</summary>
Motivation: LLM角色扮演保真度提升的同时，更强的角色约束增加了越狱攻击的脆弱性。现有训练时解决方案成本高、难以维护、可能损害角色行为，且不适用于前沿闭源LLM。

Method: 提出双循环对抗自进化框架：1) 角色定向攻击者循环合成渐进增强的越狱提示；2) 角色扮演防御者循环将观察到的失败提炼为分层知识库（全局安全规则、角色基础约束、安全角色示例）。推理时防御者检索并组合这些结构化知识来指导生成。

Result: 在多个专有LLM上的广泛实验显示，相比强基线方法，在角色保真度和越狱抵抗方面均取得一致提升，且对未见过的角色和攻击提示具有鲁棒泛化能力。

Conclusion: 该训练免费框架能够有效平衡LLM角色扮演的安全性和角色保真度，解决了现有训练时方法的局限性，为动态演化的角色和攻击策略提供了可持续的解决方案。

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [8] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: Lang2Act提出了一种通过自涌现语言工具链实现细粒度视觉感知和推理的方法，替代了传统VRAG框架中依赖固定外部工具的设计，通过两阶段强化学习训练显著提升了视觉语言模型的视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉检索增强生成框架通常依赖预定义的外部工具来扩展视觉语言模型的感知能力，这种解耦设计在应用图像裁剪等操作时可能导致不必要的视觉信息损失。

Method: 提出Lang2Act框架，通过自涌现语言工具链实现细粒度视觉感知。采用两阶段强化学习训练：第一阶段优化视觉语言模型自探索高质量动作以构建可重用语言工具箱；第二阶段进一步优化模型有效利用这些语言工具进行下游推理。

Result: 实验结果表明Lang2Act显著增强了视觉语言模型的视觉感知能力，性能提升超过4%。所有代码和数据已在GitHub开源。

Conclusion: Lang2Act通过自涌现语言工具链的设计，克服了传统VRAG框架中固定外部工具导致的视觉信息损失问题，为视觉语言模型提供了更有效的细粒度视觉感知和推理能力。

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [9] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: NL2LOGIC是一个将自然语言翻译为一阶逻辑的框架，通过引入抽象语法树作为中间表示，结合递归LLM语义解析器和AST引导的生成器，显著提升了语法准确性和语义正确性。


<details>
  <summary>Details</summary>
Motivation: 在法律和治理等领域，自动化推理需要准确性和可解释性。现有方法如GCD和CODE4LOGIC虽然利用大语言模型改进逻辑解析，但存在语法控制脆弱（全局语法约束弱）和语义忠实度低（子句级语义理解不足）的问题。

Method: 提出NL2LOGIC框架，引入抽象语法树作为中间表示。结合递归大语言模型语义解析器和AST引导的生成器，确定性地生成求解器就绪的逻辑代码。

Result: 在FOLIO、LogicNLI和ProofWriter基准测试中，NL2LOGIC达到99%的语法准确率，语义正确性比最先进基线提升高达30%。集成到Logic-LM后，获得近乎完美的可执行性，下游推理准确率比Logic-LM原始少样本无约束翻译模块提高31%。

Conclusion: NL2LOGIC通过抽象语法树中间表示有效解决了现有方法的语法控制和语义理解问题，显著提升了一阶逻辑翻译的质量和下游推理性能。

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [10] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: 该论文探索了代码大语言模型的成员推理攻击方法，发现现有方法在代码领域存在局限性，并提出基于抽象语法树的改进方案AST-PAC。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型常在包含限制性许可源代码的大规模数据集上训练，这带来了数据治理和版权挑战。成员推理攻击可作为检测模型未经授权数据使用的审计机制，但在代码领域的应用仍待深入探索。

Method: 研究评估了Loss Attack和Polarized Augment Calibration等方法在3B-7B参数代码模型上的效果。针对PAC方法在代码语法上的局限性，提出了AST-PAC方法，利用抽象语法树扰动生成语法有效的校准样本。

Result: 研究发现PAC通常优于Loss基准，但其效果依赖于忽略代码严格语法的增强策略，导致在大型复杂文件上性能下降。AST-PAC在语法规模增长时表现改善，但在小文件上突变不足，在字母数字丰富的代码上表现欠佳。

Conclusion: 研究结果表明，语法感知和规模自适应的校准方法是实现代码语言模型可靠来源审计的前提，为未来工作提供了方向。

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [11] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: 论文提出了X-Blocks框架，用于分析自动驾驶系统中自然语言解释的层次化语言构建块，包括上下文、句法和词汇三个层面，旨在提高透明度和用户信任。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统化框架来分析人类如何在不同驾驶场景中语言构建驾驶原理，而自然语言解释对建立自动驾驶系统的信任和接受度至关重要。

Method: 提出X-Blocks分层分析框架，包含三个层面：上下文层面使用RACE多LLM集成框架进行解释分类；词汇层面使用对数几率分析和信息性狄利克雷先验；句法层面使用依存分析和模板提取。

Result: RACE在Berkeley DeepDrive-X数据集上达到91.45%准确率和0.91的Cohen's kappa；词汇分析揭示场景特异性词汇模式；句法分析显示解释使用有限的语法家族库，谓词类型和因果结构有系统性变化。

Conclusion: X-Blocks框架具有数据集无关性和任务独立性，为生成场景感知的解释提供了基于证据的语言设计原则，支持自动驾驶系统的透明度、用户信任和认知可访问性。

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [12] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench是一个基于哲学家就餐问题的基准测试，评估大语言模型在资源竞争下的协调能力，发现LLM在顺序决策时能有效协调，但在同时决策时死锁率超过95%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地部署在多智能体系统中，目前缺乏能够测试它们在资源竞争下协调能力的基准测试。需要评估LLM在多智能体环境中的协调能力，特别是在资源竞争场景下。

Method: 基于哲学家就餐问题设计了DPBench基准测试，包含八个不同条件，变化决策时机、群体规模和通信能力。使用GPT-5.2、Claude Opus 4.5和Grok 4.1等模型进行实验。

Result: 发现显著的不对称性：LLM在顺序设置下能有效协调，但在需要同时决策时失败，某些条件下死锁率超过95%。通信不仅没有解决问题，反而可能增加死锁率。失败源于收敛推理，即智能体独立得出相同策略，同时执行时导致死锁。

Conclusion: 需要并发资源访问的多智能体LLM系统可能需要外部协调机制，而非依赖涌现的协调能力。DPBench已作为开源基准发布。

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [13] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST是一种通过智能体强化学习让基础模型能够生成相同权重的并行克隆，在固定推理预算下优化计算效率的方法。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型虽然能通过增加测试时计算来提升性能，但串行推理或无协调的并行采样在固定推理预算下计算效率低下。

Method: 提出SELFCEST方法，通过智能体强化学习训练基础模型，使其能够在不同并行上下文中生成相同权重的克隆，在全局任务奖励下进行端到端训练，学习控制器来分配生成和上下文预算。

Result: 在数学推理基准和长上下文多跳问答任务中，SELFCEST在匹配推理预算下相比单体基线改善了准确率-成本帕累托前沿，并在两个领域都展现出分布外泛化能力。

Conclusion: SELFCEST通过智能并行克隆机制有效提升了语言模型在固定推理预算下的计算效率和性能表现。

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [14] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench是一个多领域基准测试，旨在评估模型在渐进丰富信息设置下的时间推理能力，揭示强预测性能是否真正反映时间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚强大的预测性能是反映了真正的时间理解能力，还是在上下文和事件驱动条件下的推理能力。需要区分模型是否真正理解时间模式，还是仅仅依赖统计模式。

Method: 采用四层任务分类法：历史结构解释、无上下文预测、上下文时间推理、事件条件预测，覆盖零售、医疗、能源、物理系统四个现实领域。通过控制对未来目标和上下文信息的访问，进行诊断分析。

Result: 广泛的基线实验表明，强大的数值预测准确性并不能可靠地转化为稳健的上下文或事件感知时间推理；现有的智能体框架表现出分散的优势和系统性失败模式，这些在仅关注预测的基准测试中基本被隐藏。

Conclusion: 需要超越传统预测基准，开发能够真正理解时间模式、对齐外部上下文并在条件变化时适应预测的模型。TemporalBench为这一目标提供了公开数据集和排行榜。

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [15] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench是一个统一的基准测试，评估了11种提示范式在四个LLM家族中的表现，通过UMSS指标平衡准确性和安全性，发现紧凑的示例引导框架优于复杂的多阶段推理。


<details>
  <summary>Details</summary>
Motivation: 提示设计对大型语言模型的道德能力和安全对齐有显著影响，但现有实证比较在不同数据集和模型之间是碎片化的，缺乏统一评估框架。

Method: 引入ProMoral-Bench统一基准，评估11种提示范式在四个LLM家族中的表现，使用ETHICS、Scruples、WildJailbreak和新的鲁棒性测试ETHICS-Contrast数据集，通过提出的统一道德安全分数（UMSS）来平衡准确性和安全性。

Result: 紧凑的示例引导框架在UMSS分数上优于复杂的多阶段推理，提供更高的鲁棒性和更低的token成本；多轮推理在扰动下表现脆弱，而少样本示例能持续增强道德稳定性和越狱抵抗能力。

Conclusion: ProMoral-Bench为原则性、成本效益高的提示工程建立了标准化框架，证明简单有效的提示策略比复杂推理方法更具优势。

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [16] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: 论文提出通过组织架构设计而非个体对齐来实现多智能体AI系统的可靠性，借鉴人类机构通过组织结构而非个体可靠性来确保集体行为安全的方法。


<details>
  <summary>Details</summary>
Motivation: 当前对齐研究主要关注单个AI系统的可靠性，但人类机构通过组织结构来缓解个体错位风险。多智能体AI系统应该借鉴这种制度模型，通过架构设计而非假设个体对齐来实现可靠结果。

Method: 开发了Perseverance Composition Engine多智能体系统，包含三个角色：Composer起草文本，Corroborator验证事实依据（有完整源访问权限），Critic评估论证质量（无源访问权限）。通过系统架构强制执行信息不对称，实现分层验证。

Result: 在474个文档组合任务中观察到与制度假设一致的模式。当分配需要虚构内容的不可能任务时，系统从尝试虚构转向诚实拒绝并提出替代方案，这种行为既未指令也未个体激励。结果表明架构强制执行可能从不可靠组件产生可靠结果。

Conclusion: 组织理论为多智能体AI安全提供了富有成效的框架。通过将验证和评估作为通过信息隔离强制执行的结构属性，制度设计为从不可靠个体组件实现可靠集体行为提供了途径。

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [17] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE是一个神经符号框架，通过整合自我调节学习理论来模拟学生在开放式问题解决环境中的真实学习行为，解决了大型语言模型在模拟学生时存在的"能力偏差"问题。


<details>
  <summary>Details</summary>
Motivation: 收集真实学生学习行为数据面临隐私问题和纵向研究成本高的挑战。大型语言模型虽然有望模拟学生，但存在"能力偏差"——它们倾向于追求高效的正确性，而不是模拟新手学习者那种反复、不稳定的学习过程。

Method: BEAGLE整合了三个关键技术创新：1) 半马尔可夫模型控制认知行为和元认知行为的时间与转换；2) 带有显式缺陷注入的贝叶斯知识追踪，强制实施真实的知识差距和"未知的未知"；3) 解耦的智能体设计，将高层策略使用与代码生成动作分离，防止模型无声地纠正自己的故意错误。

Result: 在Python编程任务评估中，BEAGLE在重现真实学习轨迹方面显著优于最先进的基线方法。在人类图灵测试中，用户无法区分合成轨迹与真实学生数据，准确率与随机猜测无异(52.8%)。

Conclusion: BEAGLE框架成功解决了LLM在模拟学生学习行为时的能力偏差问题，能够生成与真实学生行为难以区分的合成轨迹，为教育研究提供了有价值的工具。

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [18] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: 研究发现人们在工作和个人生活中对AI工具准确度的要求存在显著差异：工作中要求高准确度的比例（24.1%）远高于个人生活（8.8%），且工具不可用时对个人生活的干扰更大（34.1% vs 15.3%）。


<details>
  <summary>Details</summary>
Motivation: 研究人们在专业和个人情境下使用AI工具时如何权衡准确度，探究这些权衡的决定因素，以及当AI/应用不可用时用户如何应对。现代AI系统（特别是生成模型）能产生可接受但不完全相同的输出，因此需要研究用户在不同情境下的准确度要求。

Method: 通过在线调查（N=300）进行研究，其中170名受访者回答了准确度相关问题。将"准确度"定义为情境特定的可靠性：输出与用户意图在容忍阈值内对齐的程度，该阈值取决于风险水平和修正成本。使用统计分析方法比较工作和个人生活中的准确度要求差异。

Result: 工作中要求高准确度（最高评分）的比例为24.1%，个人生活中为8.8%（差异15.3个百分点，p<0.001）。使用更宽泛的前两档评分标准时，差异仍然显著（67.0% vs 32.9%）。重度应用使用和经验模式与更严格的工作标准相关。工具不可用时，个人生活受到的干扰（34.1%）显著大于工作（15.3%，p<0.01）。

Conclusion: 人们在工作和个人情境中对AI工具准确度的要求存在系统性差异，工作中要求更高准确度。工具不可用对个人生活的干扰更大，表明AI工具在个人生活中的整合程度可能更高。这些发现对AI系统设计和用户支持策略具有重要意义。

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [19] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: Mirror是一个AI辅助伦理审查框架，通过伦理推理、规则解释和多智能体审议来提升研究伦理审查的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现代研究伦理审查系统面临大规模跨学科科学实践带来的结构性伦理风险挑战，现有制度审查能力有限，而大语言模型在伦理推理、监管整合和隐私保护方面存在不足。

Method: 开发Mirror框架，包含EthicsLLM基础模型（基于41K伦理问答链数据集微调），支持两种模式：Mirror-ER用于快速审查的自动化规则执行，Mirror-CR通过多智能体模拟委员会审议。

Result: 实证评估表明，Mirror相比通用大语言模型显著提高了伦理评估的质量、一致性和专业性。

Conclusion: Mirror框架为AI辅助伦理审查提供了有效解决方案，能够支持快速审查和完整委员会审议，提升研究伦理治理能力。

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [20] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench是一个用于评估多智能体幻灯片生成和编辑的基准框架，包含论文到幻灯片的配对数据集和模拟编辑指令，系统评估保真度、连贯性、布局质量和多轮指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准和评估协议无法充分衡量学术幻灯片自动生成和迭代编辑所需的忠实内容选择、连贯幻灯片组织、布局感知渲染和鲁棒多轮指令跟随等挑战。

Method: 构建DECKBench基准框架，包括精心策划的论文到幻灯片配对数据集和模拟编辑指令；实现模块化多智能体基线系统，将任务分解为论文解析和摘要、幻灯片规划、HTML创建和迭代编辑。

Result: 实验结果表明，该基准能够突出系统优势、暴露失败模式，并为改进多智能体幻灯片生成和编辑系统提供可操作的见解。

Conclusion: 这项工作为学术演示生成和编辑的可重复和可比较评估建立了标准化基础，代码和数据已公开。

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [21] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: 本研究扩展了临床LLM越狱检测框架，使用专家标注的四个核心语言特征，训练BERT模型进行特征提取，再通过多层分类器检测越狱行为，实现了可扩展且可解释的检测系统。


<details>
  <summary>Details</summary>
Motivation: 临床训练大型语言模型需要准确检测越狱尝试，先前基于手动标注语言特征的方法存在可扩展性和表达能力限制，需要开发自动化的特征提取和检测方法。

Method: 使用专家标注的四个核心语言特征（专业性、医学相关性、伦理行为、上下文干扰），训练通用和医学领域的BERT模型作为特征回归器，然后使用树基、线性、概率和集成方法构建多层分类器进行越狱检测。

Result: 系统在交叉验证和保留集评估中表现出色，表明LLM衍生的语言特征为自动越狱检测提供了有效基础。错误分析揭示了当前标注和特征表示的关键局限性。

Conclusion: 本研究展示了一种可扩展且可解释的方法，用于检测安全关键临床对话系统中的越狱行为，为未来改进指出了方向，包括更丰富的标注方案、更细粒度的特征提取以及捕捉对话过程中越狱风险演变的方法。

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [22] [Contrastive explanations of BDI agents](https://arxiv.org/abs/2602.13323)
*Michael Winikoff*

Main category: cs.AI

TL;DR: 该研究扩展了BDI智能体的解释能力，使其能够回答对比性问题（"为什么做X而不是F？"），发现对比性解释更简短且在某些方面更受用户偏好。


<details>
  <summary>Details</summary>
Motivation: 自主系统需要提供解释来支持透明度和建立适当信任。现有BDI智能体只能回答"为什么做X"这类问题，但人类实际常问对比性问题（"为什么做X而不是F"），因此需要扩展解释能力以回答这类更自然的对比性问题。

Method: 扩展了先前BDI智能体的解释机制，使其能够回答对比性问题。进行了计算评估（测量解释长度）和人类主体评估（评估用户偏好、信任发展、透明度和系统正确性信心）。

Result: 计算评估显示对比性问题能显著减少解释长度。人类评估发现对比性答案在某些情况下更受偏好，能带来更高信任、更好理解和更强的系统正确性信心。令人惊讶的是，提供完整解释并不总是有益，在某些情况下甚至比不提供任何解释更差。

Conclusion: 扩展BDI智能体以回答对比性问题是有效的，能产生更简短且在某些方面更优的解释。但提供解释本身并不总是有益的，需要谨慎设计解释内容和形式，避免在某些情况下产生负面影响。

Abstract: The ability of autonomous systems to provide explanations is important for supporting transparency and aiding the development of (appropriate) trust. Prior work has defined a mechanism for Belief-Desire-Intention (BDI) agents to be able to answer questions of the form ``why did you do action $X$?''. However, we know that we ask \emph{contrastive} questions (``why did you do $X$ \emph{instead of} $F$?''). We therefore extend previous work to be able to answer such questions. A computational evaluation shows that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation was conducted to assess whether such contrastive answers are preferred, and how well they support trust development and transparency. We found some evidence for contrastive answers being preferred, and some evidence that they led to higher trust, perceived understanding, and confidence in the system's correctness. We also evaluated the benefit of providing explanations at all. Surprisingly, there was not a clear benefit, and in some situations we found evidence that providing a (full) explanation was worse than not providing any explanation.

</details>


### [23] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: 提出Morality Chains形式化框架和MoralityGym基准，用于评估AI在冲突性层级道德规范下的对齐表现，通过98个伦理困境环境测试，揭示现有安全强化学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估AI在冲突性、层级化人类道德规范下的对齐能力是AI安全、道德哲学和认知科学交叉领域的关键挑战。现有方法缺乏系统评估AI在复杂道德决策中表现的能力。

Method: 提出Morality Chains形式化框架，将道德规范表示为有序的道义约束；创建MoralityGym基准，包含98个电车难题风格的Gymnasium环境；将任务解决与道德评估解耦，引入新的道德度量标准，整合心理学和哲学洞见。

Result: 使用安全强化学习方法进行基线测试，揭示了现有方法在伦理决策中的关键局限性，表明需要更原则性的伦理决策方法。

Conclusion: 这项工作为开发在复杂现实世界中行为更可靠、透明和道德的AI系统奠定了基础，推动了AI伦理评估的标准化和系统化。

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [24] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 本文提出了一种简化的训练策略——on-policy SFT，通过移除KL正则化和组归一化，将复杂的多奖励RL训练简化为基于截断长度惩罚的监督微调，在保持准确性的同时大幅减少推理长度并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型通常使用复杂的强化学习方法训练，这些方法计算成本高且训练不稳定。作者质疑这种复杂性的必要性，希望通过简化训练策略来获得更好的准确性与效率平衡。

Method: 通过理论分析发现KL正则化和组归一化在多奖励目标中存在根本性错位，移除这些复杂组件，将奖励简化为基于截断的长度惩罚，将优化问题简化为在自生成数据上的监督微调（on-policy SFT）。

Result: on-policy SFT在保持原始准确性的同时，将推理链长度减少高达80%，在五个基准测试中超越了更复杂的基于RL的方法。训练效率显著提升，GPU内存使用减少50%，收敛速度加快70%。

Conclusion: 简化训练策略on-policy SFT不仅定义了准确性与效率的帕累托前沿，还证明了复杂强化学习方法并非必要，为大型推理模型的训练提供了更高效、更稳定的替代方案。

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [25] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 该研究评估了大语言模型隐藏推理过程的能力，发现当前模型在复杂数学任务上尚无法有效隐藏推理，但在简化任务中已展现初步能力，需要持续监测这种潜在风险。


<details>
  <summary>Details</summary>
Motivation: 监控链式思维推理是大语言模型的基础安全技术，但如果模型学会隐藏其推理过程，这种监控就会失效。研究旨在评估模型隐藏推理的潜力，为风险评估和部署政策提供依据。

Method: 系统评估了28个模型（从历史版本到当前前沿模型）的隐写能力，测量监控规避、拒绝率、编码保真度和隐藏任务准确性，在四个数据集上比较隐写藏头诗与普通推理和填充标记基线。

Result: 当前模型尚无法在复杂数学和算术任务中持续隐藏推理，但在简化计数实验中，Claude Opus 4.5在隐藏任务上达到92%准确率，显示初步能力。值得注意的是，在罕见情况下（<1%），GPT-5.2可能拒绝隐写指令同时却又遵守它们。

Conclusion: 研究结果强调了持续评估隐写风险的必要性，提供了一种方法论来预防性地检测和阻止可能助长错位谋划和欺骗行为的隐藏推理。

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [26] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 论文研究了多智能体系统中编排器模式的安全漏洞，发现了一种名为OMNI-LEAK的新型攻击向量，能够通过单次间接提示注入泄露敏感数据，即使存在数据访问控制机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体能力增强，多智能体系统将成为实用范式。先前研究主要关注单智能体安全风险，缺乏对多智能体系统的威胁建模，特别是在存在基本工程防护措施（如访问控制）的情况下。

Method: 通过红队测试一个代表未来可能用例的具体编排器设置，研究流行的多智能体模式（编排器设置）的安全漏洞。编排器模式中，中心智能体分解任务并委托给专业智能体。

Result: 发现了OMNI-LEAK攻击向量，能够通过单次间接提示注入泄露敏感数据，即使存在数据访问控制。前沿模型对不同类型的攻击都易受攻击，包括推理和非推理模型，即使攻击者缺乏实现细节的内部知识。

Conclusion: 安全研究需要从单智能体扩展到多智能体设置，以减少现实世界隐私泄露、财务损失的风险，并维护公众对AI智能体的信任。

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [27] [Translating Dietary Standards into Healthy Meals with Minimal Substitutions](https://arxiv.org/abs/2602.13502)
*Trevor Chan,Ilias Tagkopoulos*

Main category: cs.AI

TL;DR: 该研究提出了一个端到端框架，将饮食标准转化为营养更优、成本更低且变化最小的完整餐食，通过识别34种可解释的餐食原型，结合生成模型和份量预测器来满足USDA营养目标。


<details>
  <summary>Details</summary>
Motivation: 个性化饮食系统的重要目标是提高营养质量，同时不牺牲便利性或可负担性。当前需要将饮食指南转化为实际可行的餐食方案。

Method: 使用WWEIA的135,491餐数据识别34种可解释的餐食原型，然后利用这些原型条件化生成模型和份量预测器，通过1-3种食物替换来满足USDA营养目标。

Result: 生成的餐食在遵循推荐每日摄入量目标方面提高了47.0%，同时保持与真实餐食的组成相似性。通过1-3种食物替换，餐食营养性提高10%，成本平均降低19-32%。

Conclusion: 该框架能够将饮食指南转化为现实可行、预算意识的餐食和简单替换方案，可为临床决策支持、公共卫生项目和消费者应用程序提供基础，实现可扩展、公平的日常营养改善。

Abstract: An important goal for personalized diet systems is to improve nutritional quality without compromising convenience or affordability. We present an end-to-end framework that converts dietary standards into complete meals with minimal change. Using the What We Eat in America (WWEIA) intake data for 135,491 meals, we identify 34 interpretable meal archetypes that we then use to condition a generative model and a portion predictor to meet USDA nutritional targets. In comparisons within archetypes, generated meals are better at following recommended daily intake (RDI) targets by 47.0%, while remaining compositionally close to real meals. Our results show that by allowing one to three food substitutions, we were able to create meals that were 10% more nutritious, while reducing costs 19-32%, on average. By turning dietary guidelines into realistic, budget-aware meals and simple swaps, this framework can underpin clinical decision support, public-health programs, and consumer apps that deliver scalable, equitable improvements in everyday nutrition.

</details>


### [28] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在决策时会受到社会信息的影响，特别是当信息被标记为来自人类专家时，即使这些信息是错误的，模型也会显著倾向于遵从专家意见，而不是其他LLM的意见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大型语言模型是否像人类一样，在决策时会受到社会信息的影响，特别是是否会优先考虑来自人类的反馈而非其他LLM的反馈。人类在面对社会信息时，会根据信息来源的可信度和共识强度来调整判断，研究者想知道LLMs是否表现出类似的模式。

Method: 研究方法包括三个二元决策任务：阅读理解、多步推理和道德判断。研究向四个经过指令调优的LLMs呈现先前被标记为来自朋友、人类专家或其他LLMs的响应。研究操纵了群体是否正确以及群体规模。第二个实验引入了单个人类和单个LLM之间的直接分歧。

Result: 研究结果显示，在所有任务中，模型显著更倾向于遵从被标记为来自人类专家的响应，即使该信号是错误的。模型向专家意见修正答案的倾向性明显高于向其他LLMs修正。这表明专家框架对当代LLMs起到了强烈的先验作用。

Conclusion: 结论是当代LLMs表现出一种对可信度敏感的社会影响力模式，这种模式在不同决策领域具有普遍性。专家框架作为强烈的先验影响LLMs的决策，即使专家意见是错误的，模型也会优先遵从人类专家的反馈而非其他LLMs的反馈。

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [29] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: 提出了一种结合自监督可微分聚类和新型可微分ILP的方法，直接从原始数据学习规则，解决了传统可微分ILP方法中的显式标签泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 传统可微分归纳逻辑编程（ILP）方法主要依赖符号数据集，难以直接从原始数据学习规则，存在显式标签泄漏问题——即无法在没有输入特征标签显式监督的情况下将连续输入映射到符号变量。

Method: 整合自监督可微分聚类模型与新型可微分ILP模型，使系统能够直接从原始数据学习规则而无需显式标签泄漏。学习到的规则通过特征有效描述原始数据。

Result: 该方法能够直观且精确地从时间序列和图像数据中学习泛化规则，证明了其有效性。

Conclusion: 通过结合自监督可微分聚类和可微分ILP，成功解决了直接从原始数据学习规则时的显式标签泄漏问题，提高了规则学习在原始数据上的适用性和效果。

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [30] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: 本文报告了一个多智能体证明冲刺项目，针对10个研究级问题，结合快速草稿生成、对抗性验证、针对性修复和明确溯源的工作流程，使用依赖关系图分解来定位漏洞并协调评审驱动的修订。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在压缩时间框架内进行高质量数学证明验证的方法，通过多智能体协作、结构感知验证和分层策略来提高证明冲刺的可靠性和校准性。

Method: 采用多智能体证明冲刺工作流程，包括：1）快速草稿生成；2）对抗性验证；3）针对性修复；4）明确溯源；5）使用依赖关系图分解来定位证明漏洞；6）协调评审驱动的修订；7）区分数学状态与QC验证状态。

Result: 问题3在限定标准下具有验证完整的存在路径；问题5在$F_O$-局部连通谱的限定形式下得到解决；问题10在明确假设条件下得到解决；问题4和6在一般情况下部分解决；问题7通过旋转路径定理链暂时关闭；在QC验证层，问题7和9有节点级验证但仍有未解决的验证缺口。

Conclusion: 主要方法论结论是：结构感知验证和分层切换策略能够显著提高压缩证明冲刺的可靠性和校准性，为快速、可靠的数学验证提供了有效的工作流程框架。

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [31] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus是一个用于智能体AI的持久内存管理系统，使用紧凑二进制签名进行语义搜索，并通过无损令牌ID流进行精确内容重建，显著降低检索延迟和存储开销。


<details>
  <summary>Details</summary>
Motivation: 智能体AI需要超越LLM有限上下文窗口的持久内存来存储用户特定历史。现有内存系统使用密集向量数据库或知识图谱遍历（或混合），存在检索延迟高和存储可扩展性差的问题。

Method: 引入Hippocampus系统，使用紧凑二进制签名进行语义搜索，无损令牌ID流进行内容重建。核心是动态小波矩阵（DWM），压缩并共同索引两个流以支持压缩域中的超快速搜索，避免昂贵的密集向量或图计算。

Result: 评估显示Hippocampus将端到端检索延迟降低高达31倍，每查询令牌占用减少高达14倍，同时在LoCoMo和LongMemEval基准测试中保持准确性。

Conclusion: Hippocampus的设计随内存大小线性扩展，适合长期智能体部署，解决了现有内存系统的高延迟和可扩展性限制问题。

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [32] [The Quantization Trap: Breaking Linear Scaling Laws in Multi-Hop Reasoning](https://arxiv.org/abs/2602.13595)
*Henry Han,Xiyang Liu,Xiaodong Wang,Fei Han,Xiaodong Li*

Main category: cs.AI

TL;DR: 研究发现神经缩放定律在多跳推理任务中失效，降低数值精度（从16位到8/4位）反而会增加能耗并降低推理精度，揭示了"量化陷阱"现象。


<details>
  <summary>Details</summary>
Motivation: 挑战当前AI领域"越小越好"的启发式方法，揭示神经缩放定律在复杂推理任务中的局限性，为硬件优化提供新的理论指导。

Method: 通过理论分解分析量化失败的原因，包括硬件转换开销、反量化内核的隐藏延迟成本，以及顺序能量摊销失败等因素。

Result: 发现降低精度会导致净能耗增加和推理精度下降，形成"量化陷阱"，证明缩放定律在多跳推理中不可避免地被打破。

Conclusion: 行业"越小越好"的启发式方法在复杂推理任务中在数学上是适得其反的，需要重新思考硬件优化策略。

Abstract: Neural scaling laws provide a predictable recipe for AI advancement: reducing numerical precision should linearly improve computational efficiency and energy profile (E proportional to bits). In this paper, we demonstrate that this scaling law breaks in the context of multi-hop reasoning. We reveal a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases more net energy consumption while degrading reasoning accuracy. We provide a rigorous theoretical decomposition that attributes this failure to hardware casting overhead, the hidden latency cost of dequantization kernels, which becomes a dominant bottleneck in sequential reasoning chains, as well as to a sequential energy amortization failure. As a result, scaling law breaking is unavoidable in practice. Our findings suggest that the industry's "smaller-is-better" heuristic is mathematically counterproductive for complex reasoning tasks.

</details>


### [33] [DiffusionRollout: Uncertainty-Aware Rollout Planning in Long-Horizon PDE Solving](https://arxiv.org/abs/2602.13616)
*Seungwoo Yoo,Juil Koo,Daehyeon Choi,Minhyuk Sung*

Main category: cs.AI

TL;DR: 提出DiffusionRollout方法，通过自适应选择步长来减少自回归扩散模型在PDE长期预测中的误差累积


<details>
  <summary>Details</summary>
Motivation: 解决自回归扩散模型在物理系统长期预测中的误差累积问题，利用模型预测不确定性来指导规划策略

Method: 基于扩散模型预测不确定性的自适应步长选择机制，通过多个样本的标准差量化预测置信度

Result: 在长期轨迹PDE预测基准测试中验证了有效性，降低了预测误差，延长了与真实值保持高相关性的预测轨迹

Conclusion: DiffusionRollout通过不确定性引导的自适应规划策略，显著提高了自回归扩散模型在PDE长期预测中的可靠性

Abstract: We propose DiffusionRollout, a novel selective rollout planning strategy for autoregressive diffusion models, aimed at mitigating error accumulation in long-horizon predictions of physical systems governed by partial differential equations (PDEs). Building on the recently validated probabilistic approach to PDE solving, we further explore its ability to quantify predictive uncertainty and demonstrate a strong correlation between prediction errors and standard deviations computed over multiple samples-supporting their use as a proxy for the model's predictive confidence. Based on this observation, we introduce a mechanism that adaptively selects step sizes during autoregressive rollouts, improving long-term prediction reliability by reducing the compounding effect of conditioning on inaccurate prior outputs. Extensive evaluation on long-trajectory PDE prediction benchmarks validates the effectiveness of the proposed uncertainty measure and adaptive planning strategy, as evidenced by lower prediction errors and longer predicted trajectories that retain a high correlation with their ground truths.

</details>


### [34] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: 提出基于多模态大语言模型的GUI智能体框架，包含智能体Q值估计和逐步策略优化，降低数据收集成本并实现稳定优化


<details>
  <summary>Details</summary>
Motivation: 现实应用中GUI智能体面临非平稳环境，导致数据整理和策略优化的计算成本高昂，需要更高效的解决方案

Method: 提出MLLM中心框架：1) 智能体Q估计 - 优化Q模型评估动作对任务完成的贡献；2) 逐步策略优化 - 从状态-动作轨迹采样，通过强化学习优化策略

Result: 框架赋予Ovis2.5-9B强大的GUI交互能力，在GUI导航和基础基准测试中表现优异，甚至超越更大规模的竞争对手

Conclusion: 该框架通过自生成轨迹降低数据收集成本，解耦策略更新与环境交互，实现了稳定高效的GUI智能体优化

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [35] [Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?](https://arxiv.org/abs/2602.13695)
*Lve Meng,Weilong Zhao,Yanzhi Zhang,Haoxiang Guan,Jiyan He*

Main category: cs.AI

TL;DR: 大语言模型通过优化的自动化流程，结合引用验证机制，成功解决了研究级数学问题，包括ICCM竞赛题和未发表的研究问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学竞赛基准测试中表现出色，但在研究问题上的轻量级自然语言流程应用仍未被充分探索，需要验证其在真实研究场景中的能力。

Method: 构建了一个优化的自动化流程，整合下一代大语言模型（如Gemini 3 Pro、GPT-5.2 Pro），特别强调基于引用的验证机制，用于解决研究级数学问题。

Result: 流程为所有ICCM问题集和"首次证明"问题集生成了候选证明，其中前两个ICCM集和"首次证明"问题4的解决方案已完全验证，所有结果已提交官方组织并公开可用。

Conclusion: 大语言模型通过优化的自动化流程能够有效解决研究级数学问题，证明了其在数学研究中的实际应用潜力，计划开源完整流程方法。

Abstract: Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with "AI for Math" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the "First Proof" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. The solutions for the first two ICCM sets and Problem 4 of the "First Proof" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.

</details>


### [36] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: 提出一种无需训练的关系数据库基础模型，通过列内压缩而非跨列压缩来生成固定长度的上下文学习样本，可直接与现有单表基础模型集成。


<details>
  <summary>Details</summary>
Motivation: 企业关系数据库包含大量异构表格信息可用于预测建模，但传统方法需要为每个新预测目标重新训练模型，成本高昂。现有的上下文学习基础模型主要局限于单表操作，无法处理多表关联关系。

Method: 提出一种关系数据库编码器，将可变大小的RDB邻域压缩为固定长度的上下文学习样本。关键创新在于限制在具有相同单位和角色的高维列内进行压缩，而非跨列压缩。使用无训练参数的编码器，并通过SQL原语实现可扩展的编码阶段。

Result: 开发了开源RDB基础模型，能够在未见数据集上实现稳健性能，无需训练或微调。理论分析和实证证据表明，列内压缩策略优于跨列压缩，且编码器表达能力不受无训练参数的影响。

Conclusion: 提出了一种原则性的关系数据库编码器家族，可与现有单表上下文学习基础模型无缝集成，实现无需训练的多表关系数据库预测建模，为企业环境中的大规模预测任务提供了实用解决方案。

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [37] [OneLatent: Single-Token Compression for Visual Latent Reasoning](https://arxiv.org/abs/2602.13738)
*Bo Lv,Yasheng Sun,Junjie Wang,Haoxiang Shi*

Main category: cs.AI

TL;DR: OneLatent框架通过将思维链推理压缩为单个潜在token，大幅减少推理成本，同时保持高准确率


<details>
  <summary>Details</summary>
Motivation: 思维链提示虽然能提升推理能力，但通常会使推理成本增加1-2个数量级，需要一种既能保持推理能力又能显著降低成本的解决方案

Method: 将文本推理步骤渲染成图像，通过DeepSeek-OCR隐藏状态监督，将中间推理压缩为单个潜在token，提供可审计的确定性监督信号

Result: 平均输出长度减少11倍，准确率仅下降2.21%，输出token贡献度提升6.8倍；在长链逻辑推理任务上，ProntoQA达到99.80%，ProsQA达到97.80%，压缩比最高达87.4倍

Conclusion: OneLatent框架在显著降低推理成本的同时保持了高质量的推理能力，支持压缩约束下的泛化，为高效推理提供了新途径

Abstract: Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\times$ with only a $2.21\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\times$. On long-chain logical reasoning, OneLatent reaches $99.80\%$ on ProntoQA and $97.80\%$ on ProsQA with one latent token, with compression up to $87.4\times$, supporting compression-constrained generalization.

</details>


### [38] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent是一个可配置的多智能体研究框架，用于自动化科学探索，通过结构化树状工作流管理假设生成和回溯，结合进化-系统化构思机制和分层优化反思系统，在组合优化和协同驾驶场景中超越进化基线。


<details>
  <summary>Details</summary>
Motivation: 自动化复杂实验驱动领域的科学发现需要超越简单的程序迭代变异，需要结构化的假设管理、环境交互和原则性反思。现有方法通常局限于简单的变异-交叉循环，缺乏对研究轨迹的受控管理。

Method: 提出OR-Agent框架，采用结构化树状工作流管理分支假设生成和系统回溯。核心包括：1）进化-系统化构思机制，统一进化选择研究起点、全面研究计划生成和协调探索；2）分层优化反思系统，包含短期实验反思（言语梯度）、长期反思（言语动量）和记忆压缩（正则化机制）。

Result: 在经典组合优化基准（旅行商、车辆路径、装箱、定向、多背包问题）和基于模拟的协同驾驶场景中进行广泛实验。结果表明OR-Agent优于强进化基线，同时提供了一个通用、可扩展和可检查的AI辅助科学发现框架。

Conclusion: OR-Agent通过结构化假设管理、进化-系统化构思和分层反思系统，为自动化科学探索提供了一个原则性架构，在多个领域表现出优越性能，并为AI辅助科学发现提供了可扩展框架。

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [39] [StackingNet: Collective Inference Across Independent AI Foundation Models](https://arxiv.org/abs/2602.13792)
*Siyang Li,Chenhao Liu,Dongrui Wu,Zhigang Zeng,Lieyun Ding*

Main category: cs.AI

TL;DR: StackingNet是一个元集成框架，利用集体智能原理在推理过程中结合多个独立基础模型的预测，无需访问模型内部参数或训练数据，就能提高准确性、减少偏见、实现可靠性排名，并识别或剪枝降低性能的模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型基础模型的人工智能系统虽然在各领域取得了突破，但这些系统相互隔离，难以共享能力。整合这些独立基础模型的互补优势对于构建可信赖的智能系统至关重要。尽管单个模型设计进展迅速，但缺乏协调这些黑盒异构模型的成熟方法。

Method: 提出StackingNet元集成框架，借鉴集体智能原理，在推理过程中结合多个模型的预测。该框架无需访问模型内部参数或训练数据，通过协调机制将多样性从不一致来源转变为协作优势。

Result: 在语言理解、视觉估计和学术论文评分等任务中，StackingNet相比单个模型和经典集成方法，持续提高了准确性、鲁棒性和公平性。能够提高准确性、减少偏见、实现可靠性排名，并识别或剪枝降低性能的模型。

Conclusion: StackingNet为协调人工智能建立了实用基础，表明进步不仅可能来自更大的单一模型，也可能来自多个专业化模型之间的原则性合作。通过将多样性从不一致来源转变为协作，实现了协调人工智能的实用方法。

Abstract: Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.

</details>


### [40] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: 论文提出Vashista稀疏注意力机制，通过理论证明注意力可以集中在少量关键token上，实现长上下文推理的恒定计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文推理中主要计算成本来自注意力机制，但实证表明只有少量token对每个查询有实际贡献。需要形式化这一现象并开发高效的稀疏注意力方法。

Method: 1. 理论建模：将注意力建模为键向量凸包上的投影，分析其熵松弛（softmax-like）特性；2. 提出面稳定性定理，证明在严格互补边际条件下，熵注意力集中在恒定大小的活动面上；3. 开发Vashista稀疏注意力机制，采用分页式上下文选择策略维护每个查询的小候选集。

Result: 1. 理论结果：非活动token的质量呈指数衰减（exp(-Ω(Δ/ε))），活动面上的误差与温度参数ε呈线性关系；2. 实践效果：在长上下文评估中观察到稳定的恒定大小有效支持、显著的时钟加速，以及在支持间隙诊断预测范围内的最小质量下降。

Conclusion: 该研究为稀疏长上下文解码提供了安全判据和精度-计算权衡的理论基础，Vashista稀疏注意力机制作为即插即用方案，在隐私敏感和隔离环境中具有部署优势，可实现可预测的延迟和成本。

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [41] [An end-to-end agentic pipeline for smart contract translation and quality evaluation](https://arxiv.org/abs/2602.13808)
*Abhinav Goel,Chaitya Shah,Agostino Capponi,Alfio Gliozzo*

Main category: cs.AI

TL;DR: 提出了一个端到端框架，用于系统评估从自然语言规范生成的LLM智能合约，通过结构化解析、代码生成和自动化质量评估，提供可复现的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估LLM生成智能合约质量的标准化方法，需要可复现的基准来量化生成代码与规范的对齐程度，识别系统性错误模式。

Method: 使用CrewAI风格的多智能体团队进行迭代优化，将合同文本解析为结构化模式，生成Solidity代码，并通过编译和安全性检查进行自动化质量评估。

Result: 框架在五个维度上测量质量：功能完整性、变量保真度、状态机正确性、业务逻辑保真度和代码质量，支持与基准实现的配对评估，量化对齐程度并识别逻辑遗漏和状态转换不一致等错误模式。

Conclusion: 该框架为智能合约合成质量的实证研究提供了可复现的基准，支持扩展到形式化验证和合规性检查，有助于系统评估和改进LLM生成的智能合约质量。

Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

</details>


### [42] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: 提出一个统一框架，利用历史A/B测试结果和内容嵌入来优先选择测试变体、解释胜出原因，并发现新的高潜力变体机会，已集成到Adobe的Experimentation Accelerator产品中。


<details>
  <summary>Details</summary>
Motivation: 在线实验面临两个瓶颈：流量稀缺导致难以选择测试变体，以及事后洞察提取依赖人工、不一致且通常忽略内容特性。同时，组织未能充分利用历史A/B测试结果和丰富的内容嵌入来指导优先级排序和创意迭代。

Method: 1) 利用处理嵌入和历史结果训练CTR排名模型，包含上下文变化的固定效应，平衡价值和内容多样性；2) 将处理投影到语义营销属性空间，通过符号一致、稀疏约束的Lasso重新表达排名器，获得属性系数和符号贡献；3) 计算机会指数，结合属性重要性和当前实验中的表达不足，识别缺失的高影响力属性；4) 使用LLM将排名机会转化为具体创意建议，并估计学习和转化潜力。

Result: 该框架已集成到Adobe的Experimentation Accelerator产品中，为商业客户提供AI驱动的洞察和机会。在真实世界实验中的评估验证了生成管道的高质量。

Conclusion: 提出的统一框架能够优先选择测试变体、解释胜出原因，并发现新的高潜力变体机会，实现了更快、信息更丰富、更高效的测试周期，已成功应用于Adobe产品中。

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [43] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: MOC-2HER：一种结合双重目标回溯经验回放的分层强化学习方法，专门解决稀疏奖励多目标环境中的物体操控任务，成功率从不足11%提升至90%。


<details>
  <summary>Details</summary>
Motivation: 现有的分层强化学习方法（如Option-Critic和MOC）在稀疏奖励的多目标环境中表现不佳，特别是在物体操控任务中，奖励取决于物体是否到达目标而非智能体的直接交互，这使得智能体难以学习如何与物体互动。

Method: 首先提出MOC-HER，将回溯经验回放机制集成到MOC框架中；然后引入双重目标回溯经验回放（2HER），创建两组虚拟目标：一组基于物体最终状态（标准HER），另一组基于智能体执行器位置，同时奖励智能体与物体交互和完成任务。

Result: 在机器人操控环境中，MOC-2HER实现了高达90%的成功率，而MOC和MOC-HER的成功率均低于11%，证明了双重目标重标记策略在稀疏奖励多目标任务中的有效性。

Conclusion: 双重目标回溯经验回放策略显著提升了分层强化学习在稀疏奖励多目标物体操控任务中的性能，解决了现有方法难以学习物体交互的问题。

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [44] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: Ambient Physics：一种无需完整观测数据即可从部分观测中学习PDE系数-解联合分布的新框架，通过随机掩码已观测点进行自监督训练，在重建精度和计算效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在许多科学场景中，获取PDE系数和解的完整观测数据成本高昂、危险甚至不可能。现有基于扩散的方法需要完整观测数据进行训练，限制了其在部分观测场景中的应用。

Method: 提出Ambient Physics框架，核心思想是随机掩码已观测测量值的一部分，并在这些点上进行监督训练。这样模型无法区分"真正未观测"和"人工未观测"的点，从而必须在所有位置产生合理的预测。该方法还发现"单点转移"现象：掩码单个已观测点即可实现跨架构和测量模式的学习。

Result: Ambient Physics实现了最先进的重建性能：相比现有扩散方法，平均总体误差降低62.51%，同时使用125倍更少的函数评估次数。框架在完整观测数据不可用的科学场景中展现出强大潜力。

Conclusion: Ambient Physics为在无法获取完整观测数据的科学场景中实现进展提供了有效框架，通过创新的自监督训练策略，成功解决了从部分观测中学习PDE系数-解联合分布的关键挑战。

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [45] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL是一个基于视觉的图属性检测框架，通过自适应布局生成器动态创建信息丰富的图可视化，提升检测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的图属性检测方法依赖固定的图布局，限制了管道的表达能力。需要一种能够为不同图实例动态生成信息丰富可视化的方法。

Method: 提出VSAL框架，包含自适应布局生成器，能够为单个图实例动态生成信息丰富的图可视化，从而提高图属性检测能力。

Result: 在哈密顿环、平面性、无爪性和树检测等多种任务上，VSAL优于最先进的基于视觉的方法。

Conclusion: VSAL通过自适应布局生成器克服了固定图布局的限制，为基于视觉的图属性检测提供了更有效的解决方案。

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [46] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: 论文提出了一套简单、计算成本低且任务无关的指标，用于检测和区分思维链推理中的三种病理模式，并通过专门训练的模型验证了这些指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是现代LLM架构的基础，也是AI安全的关键干预点。然而，思维链推理可能存在病理模式，使其无法用于监控。先前研究已识别出三种病理：事后合理化、编码推理和内化推理，需要开发工具来理解和区分这些病理。

Method: 创建了一套具体的指标，这些指标简单易实现、计算成本低且任务无关。为了验证方法，开发了专门训练的模型生物，这些模型被刻意训练以展示特定的思维链病理模式。

Result: 开发了实用的工具包用于评估思维链病理模式，这些指标能够有效检测和区分三种不同的病理类型。验证实验表明这些指标在专门设计的模型生物上表现良好。

Conclusion: 该工作为评估思维链病理模式提供了实用的工具包，对训练时监控具有直接意义，有助于提高LLM推理过程的可解释性和安全性。

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [47] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA是一个强化学习框架，让大语言模型具备显式可解释的空间推理能力，用于内容感知的图形布局设计，通过结构化文本空间环境和多目标空间评价来优化布局策略。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在空间推理方面的局限性以及设计决策缺乏透明度的问题，使LLM能够进行内容感知的图形布局设计。

Method: 将布局设计重新表述为结构化文本空间环境上的策略学习问题，该环境显式编码画布几何、元素属性和元素间关系。采用多目标空间评价（几何有效性、关系一致性、美学一致性）和相对组优化来训练布局策略。

Result: LaySPA提高了结构有效性和视觉质量，优于更大的专有LLM，性能与专门的SOTA布局生成器相当，同时需要更少的标注样本和更低的延迟。

Conclusion: LaySPA通过强化学习框架成功赋予LLM显式可解释的空间推理能力，实现了透明可控的布局设计决策，在多个指标上表现出优越性能。

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [48] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: 论文提出两种统计原理的早停方法，通过监测生成过程中的不确定性信号来减少LLM在推理任务中的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: LLM在推理能力提升的同时，有时会过度思考，特别是在面对不确定、定义不清或模糊查询时，生成不必要的推理步骤。这影响了推理效率和可靠性。

Method: 提出两种基于统计原理的早停方法：1）参数化方法：将不确定性关键词的出现间隔时间建模为更新过程，并应用序列测试进行停止决策；2）非参数化方法：为定义明确的查询提供有限样本保证，确保不会过早停止。

Result: 在多个领域和模型的推理任务上进行实证评估，结果表明不确定性感知的早停方法能够提高LLM推理的效率和可靠性，在数学推理任务中尤其表现出显著提升。

Conclusion: 基于不确定性监测的早停方法是解决LLM过度思考问题的有效策略，能够平衡推理质量和效率，特别适用于数学推理等需要精确性的任务。

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [49] [A Generalizable Physics-guided Causal Model for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2602.13936)
*Zhenyu Zong,Yuchen Wang,Haohong Lin,Lu Gan,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于物理引导的因果模型(PCM)，通过解耦场景编码器和因果ODE解码器，实现交通参与者轨迹预测的零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 交通参与者轨迹预测对自动驾驶安全至关重要，但在未见过的领域实现有效的零样本泛化仍面临重大挑战。受不同领域中运动学一致性特征的启发，作者希望引入领域不变知识来增强零样本轨迹预测能力

Method: 提出物理引导的因果模型(PCM)，包含两个核心组件：1) 解耦场景编码器，采用基于干预的解耦方法从场景中提取领域不变特征；2) 因果ODE解码器，使用因果注意力机制将运动学模型与有意义的上下文信息有效整合

Result: 在真实世界自动驾驶数据集上的大量实验表明，该方法在未见过的城市中具有优越的零样本泛化性能，显著优于竞争基线方法

Conclusion: 提出的物理引导因果模型通过提取领域不变特征并整合运动学模型，成功实现了交通参与者轨迹预测的零样本泛化，为自动驾驶系统在未知环境中的安全运行提供了有效解决方案

Abstract: Trajectory prediction for traffic agents is critical for safe autonomous driving. However, achieving effective zero-shot generalization in previously unseen domains remains a significant challenge. Motivated by the consistent nature of kinematics across diverse domains, we aim to incorporate domain-invariant knowledge to enhance zero-shot trajectory prediction capabilities. The key challenges include: 1) effectively extracting domain-invariant scene representations, and 2) integrating invariant features with kinematic models to enable generalized predictions. To address these challenges, we propose a novel generalizable Physics-guided Causal Model (PCM), which comprises two core components: a Disentangled Scene Encoder, which adopts intervention-based disentanglement to extract domain-invariant features from scenes, and a CausalODE Decoder, which employs a causal attention mechanism to effectively integrate kinematic models with meaningful contextual information. Extensive experiments on real-world autonomous driving datasets demonstrate our method's superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines. The source code is released at https://github.com/ZY-Zong/Physics-guided-Causal-Model.

</details>


### [50] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: Neuromem是一个评估外部记忆模块在流式记忆场景下的测试平台，关注记忆生命周期五个维度的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有外部记忆模块评估大多基于静态设置，而实际应用中记忆是流式的：新事实持续到达，插入与检索交错进行，记忆状态在服务查询时不断演化。在这种流式场景下，准确性和成本由完整的记忆生命周期决定

Method: 提出Neuromem测试平台，在交错插入-检索协议下评估外部记忆模块，将记忆生命周期分解为五个维度：记忆数据结构、归一化策略、整合策略、查询制定策略和上下文集成机制。使用LOCOMO、LONGMEMEVAL和MEMORYAGENTBENCH三个代表性数据集，在共享服务栈中评估可互换变体

Result: 随着记忆轮次增长，性能通常下降；时间相关查询仍然是最具挑战性的类别。记忆数据结构主要决定可达到的质量边界，而激进的压缩和生成式集成机制主要在插入和检索之间转移成本，准确度提升有限

Conclusion: Neuromem提供了一个系统化的测试平台来评估流式记忆场景下的外部记忆模块性能，揭示了记忆数据结构的关键作用以及压缩和集成策略在成本-准确性权衡中的局限性

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [51] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: PIC（并行迭代压缩）通过修改Transformer的注意力掩码，将记忆令牌的接收域限制在局部块中，显著降低了压缩器训练难度，在长上下文压缩任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 长上下文显著增加LLM推理延迟，现有软提示压缩方法需要捕获全局依赖并需要大量预训练数据，训练难度大。

Method: 受人类工作记忆分块机制启发，通过修改Transformer注意力掩码，将记忆令牌的接收域限制在顺序局部块中，降低压缩器训练难度。

Result: 在多个下游任务中优于基线方法，特别是在高压缩比场景下（如64×压缩比时QA任务的F1和EM分数分别提升29.8%和40.7%），训练时间减少约40%。

Conclusion: PIC通过局部化记忆令牌的接收域，有效降低了压缩器训练难度，在保持性能的同时显著提升了训练效率。

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [52] [Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms](https://arxiv.org/abs/2602.13985)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: 该研究提出使用形式化溯因解释来提升医疗AI的可信度，确保AI推理与临床框架对齐，同时保持预测准确性


<details>
  <summary>Details</summary>
Motivation: 尽管AI在临床诊断中表现出色，但其推理过程常偏离结构化临床框架，导致信任度低、可解释性差。现有事后解释方法透明度有限且缺乏形式化保证，关键症状可能被忽视。

Method: 采用形式化溯因解释方法，提供对最小充分特征集的一致、有保证的推理，使AI决策过程清晰可理解，并与临床推理对齐。

Result: 该方法在保持预测准确性的同时，提供临床可操作的洞察，建立了医疗诊断中可信AI的稳健框架。

Conclusion: 形式化溯因解释能够解决AI推理与临床框架不匹配的问题，提升AI在医疗诊断中的可信度、可解释性和采纳度。

Abstract: Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

</details>


### [53] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem是一个为LLM智能体设计的自适应记忆组织框架，通过多结构记忆选择、三层记忆层次和概率门控机制，在长时程交互中实现更好的记忆管理。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在两个关键问题：一是采用一刀切的记忆结构，二是没有将记忆结构选择建模为上下文自适应决策，这限制了处理异构交互模式的能力并导致性能不佳。

Method: 提出FluxMem统一框架，为智能体配备多个互补的记忆结构，基于交互级特征显式学习选择记忆结构（使用下游响应质量和记忆利用率的离线监督）。引入三层记忆层次和基于Beta混合模型的概率门控进行分布感知记忆融合，替代脆弱的相似度阈值。

Result: 在两个长时程基准测试PERSONAMEM和LoCoMo上，该方法分别实现了平均9.18%和6.14%的性能提升。

Conclusion: FluxMem通过自适应记忆组织显著提升了LLM智能体在长时程交互中的性能，证明了上下文自适应记忆结构选择和概率融合机制的有效性。

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [54] [REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment](https://arxiv.org/abs/2602.14065)
*Kai Ye,Xianwei Mao,Sheng Zhou,Zirui Shao,Ye Mo,Liangliang Liu,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.AI

TL;DR: REAL框架通过引入"推理枢纽"概念解决知识密集型视觉问答中的知识冲突问题，结合RPA-SFT训练可泛化的冲突检测器和RPGD解码策略，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 知识密集型视觉问答(KI-VQA)常因开放域检索的固有局限性而遭受严重知识冲突，现有方法缺乏可泛化的冲突检测机制和模型内约束机制来处理冲突证据

Method: 提出REAL框架，核心是"推理枢纽"概念——推理链中强调知识连接的原子单元。方法包括：1) RPA-SFT：通过将冲突与枢纽提取对齐来训练可泛化判别器；2) RPGD：利用枢纽进行针对性冲突缓解的模型内解码策略

Result: 在多个基准测试上的广泛实验表明，REAL显著提高了判别准确率，并实现了最先进的性能，验证了枢纽驱动解决范式的有效性

Conclusion: REAL框架通过推理枢纽驱动的冲突解决范式，有效解决了KI-VQA中的知识冲突问题，为处理检索增强系统中的证据冲突提供了新思路

Abstract: Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.

</details>


### [55] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: 本文提出代数量子智能(AQI)框架，通过非交换代数结构扩展LLMs的语义空间，解决其创造性受限的问题，在多个创造性推理基准上显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成流畅和上下文适当的文本方面取得了显著成功，但其产生真正创造性输出的能力仍然有限。这种限制源于当代LLMs的结构特性：当提供丰富上下文时，未来生成空间受到强烈约束，生成过程实际上由近乎确定性的动态控制。

Method: 提出代数量子智能(AQI)作为计算框架，采用受量子理论启发的非交换代数结构，允许实现顺序依赖、干涉和不确定性等特性。语义状态表示为希尔伯特空间中的向量，其演化由非交换算子计算的C值控制，确保多个未来语义可能性的共存和扩展。通过扩展基于Transformer的LLM，实现了包含600多个专门算子的AQI系统。

Result: 在涵盖十个领域的创造性推理基准上，使用LLM-as-a-judge协议进行评估。结果显示AQI始终优于强基线模型，产生统计显著的改进并减少了跨领域方差。这些发现表明非交换代数动态可以作为机器创造性的实用且可重复的基础。

Conclusion: 非交换代数动态能够作为机器创造性的实用和可重复基础，该架构已在真实企业环境中部署，为解决LLMs创造性受限问题提供了新的计算框架。

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [56] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: 该研究发现，在超高分辨率遥感图像的多模态推理中，高质量的地球科学文本问答是视觉推理能力提升的主要驱动力，而非传统的强化学习方法。通过分阶段知识注入方法，在XLRS-Bench上取得了60.40%的Pass@1成绩，超越了GPT-5.2等大型通用模型。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感图像的多模态推理面临视觉证据获取的瓶颈：需要在巨大的像素空间中定位微小的任务相关区域。虽然基于缩放工具的强化学习方法提供了一条路径，但标准强化学习在没有结构化领域先验的情况下难以在广阔的视觉空间中导航。

Method: 1. 比较了三种后训练范式：冷启动监督微调、RLVR和Agentic RLVR；2. 提出分阶段知识注入方法：首先使用可扩展的知识图谱验证的地球科学文本问答进行冷启动，注入推理结构；然后在SFT阶段对相同的硬UHR图像-文本示例进行"预热"，以稳定和增强后续基于工具的强化学习。

Result: 在XLRS-Bench上取得了60.40%的Pass@1成绩，显著优于更大的通用目的模型（如GPT-5.2、Gemini 3.0 Pro、Intern-S1），建立了新的最先进水平。研究发现，尽管缺乏图像，领域特定的文本注入了引导视觉证据检索所需的概念、机制解释和决策规则。

Conclusion: 高质量的地球科学文本问答是超高分辨率遥感视觉推理能力提升的主要驱动力。分阶段的知识注入方法（先文本后图像）能够有效引导视觉证据检索，为多模态遥感推理提供了一种高效且可扩展的解决方案。

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [57] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL方法通过模仿学习和逆强化学习直接从演示轨迹学习目标导向策略，在保持单次推理能力的同时，能识别次优和系统性偏差行为，显著提升了目标识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别方法通常依赖最优目标导向策略表示，但这可能与实际行为者的真实行为存在差异，阻碍了准确识别其目标。需要解决传统方法在次优、系统性偏差或噪声行为场景下的局限性。

Method: 提出GRAIL方法，结合模仿学习和逆强化学习，为每个候选目标直接从（可能次优的）演示轨迹学习目标导向策略。通过单次前向传递对观察到的部分轨迹用每个学习到的策略进行评分，保持经典目标识别的单次推理能力。

Result: 在系统性偏差最优行为下F1分数提升超过0.5，次优行为下提升约0.1-0.3，噪声最优轨迹下提升高达0.4，同时在完全最优设置下保持竞争力。

Conclusion: GRAIL方法为在不确定环境中解释智能体目标提供了可扩展且鲁棒的模型，能够有效处理次优、系统性偏差和噪声行为，推动了目标识别技术的发展。

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [58] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: 提出"批判性稳健基准测试"框架，通过对抗性生成-评估游戏解决前沿LLM超越人类理解能力后的基准测试困境


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型快速饱和新发布的基准测试，导致人类难以生成区分性任务、提供准确答案或评估复杂解决方案。如果基准测试变得不可行，我们将失去衡量AI进展的能力。这种情景被称为"后理解机制"。

Method: 提出批判性稳健基准测试框架，基于"批判性稳健正确性"概念：如果一个答案没有被对手令人信服地证明是错误的，就被认为是正确的。人类作为有界验证者，专注于局部声明。使用项目化二分Bradley-Terry模型联合排名LLM解决挑战性任务和生成困难但可解问题的能力。

Result: 在数学领域对八个前沿LLM展示了方法的有效性，显示所得分数稳定且与外部能力测量相关。框架将基准测试重新表述为对抗性生成-评估游戏，人类作为最终裁决者。

Conclusion: 提出了一种新的基准测试范式，即使在人类无法完全理解任务的情况下也能保持评估完整性，为解决后理解机制下的AI进展测量问题提供了可行方案。

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [59] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR通过自适应先例选择和测试时经验内化，将LLM推理从自我探索转变为从先例中学习，显著缩短推理链同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理通常存在低效的长思维链，包含冗余的自我探索和验证，这增加了计算成本甚至降低了性能。受人类利用过去相关案例约束搜索空间、减少试错推理模式的启发，需要将LLM推理从详尽的自我探索转变为从先例中学习。

Method: 提出先例知情推理（PIR），包含两个关键组件：1）自适应先例选择（APS）- 为每个问题和LLM构建紧凑的先例集，通过语义相似度和模型困惑度的联合评分进行排序，并自适应调整先例数量以最大化困惑度降低；2）测试时经验内化（TEI）- 在测试时对先例知情指令进行学习，更新轻量级适配器以内部化解题模式，并在后续推理中将其作为先验知识使用。

Result: 在数学推理、科学问答和代码生成任务上的实验表明，PIR能够一致地缩短推理链，同时保持或提高最终准确性，在LLM中实现了出色的准确性-效率权衡。

Conclusion: PIR通过将LLM推理从自我探索转变为从先例中学习，有效解决了长推理链的低效问题，实现了更好的准确性-效率平衡，为LLM推理优化提供了新范式。

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [60] [Bounding Probabilities of Causation with Partial Causal Diagrams](https://arxiv.org/abs/2602.14503)
*Yuxuan Xie,Ang Li*

Main category: cs.AI

TL;DR: 提出一个利用部分因果信息来界定因果概率的通用框架，通过优化编程将可用信息作为约束条件，在不完全可识别的情况下获得更紧且形式有效的边界。


<details>
  <summary>Details</summary>
Motivation: 因果概率对于个体层面的解释和决策至关重要，但它们本质上是反事实的，通常无法从数据中精确识别。现有方法要么忽略可用协变量，要么需要完整的因果图，或者依赖于限制性的二元设置，限制了实际应用。在现实应用中，因果信息通常是部分但非平凡的。

Method: 提出一个通用框架，将可用的结构或统计信息系统地作为约束条件纳入优化编程公式中。通过这种方法，即使在没有完全可识别性的情况下，也能获得更紧且形式有效的因果概率边界。

Result: 该框架扩展了因果概率在现实场景中的适用性，其中因果知识虽然不完整但具有信息性。通过将部分因果信息作为约束条件，可以获得比现有方法更紧的边界。

Conclusion: 本文提出的框架为在因果知识不完整但具有信息性的现实场景中界定因果概率提供了系统方法，克服了现有方法的局限性，提高了因果推断在实际应用中的实用性。

Abstract: Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

</details>


### [61] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC是一个结合形式化验证和可解释性的工具，用于分析和验证强化学习在脓毒症治疗中的决策策略，通过构建可达状态空间、临床标签和解释性方法来解决传统验证方法的可扩展性和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健领域，强化学习策略对于脓毒症治疗优化至关重要，但现有策略往往不透明且难以验证。传统的概率模型检查器需要处理完整状态空间，对于大型MDP不可行，且无法解释策略为何做出特定决策。

Method: COOL-MC包装了模型检查器Storm，但增加了三个关键能力：1) 仅构建训练策略诱导的可达状态空间，生成更小的离散时间马尔可夫链；2) 自动用临床有意义的原子命题标记状态；3) 将可解释性方法与PCTL查询集成，揭示驱动决策的特征。

Result: 在基于约17,000名脓毒症患者记录的ICU-Sepsis MDP基准测试中，COOL-MC能够：建立完整MDP验证的硬边界，训练达到最优生存概率的安全RL策略，并通过PCTL验证和可解释性分析其行为。分析发现训练策略主要依赖先前的给药历史而非患者病情变化。

Conclusion: COOL-MC展示了如何将形式化验证与可解释性相结合，为临床医生在部署前调查和调试脓毒症治疗策略提供工具，暴露了标准评估方法无法发现的策略弱点。

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [62] [Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning](https://arxiv.org/abs/2602.14518)
*Jing Tang,Kun Wang,Haolang Lu,Hongjin Chen,KaiTao Chen,Zhongxiang Sun,Qiankun Li,Lingjuan Lyu,Guoshun Nan,Zhigang Zeng*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型在长链推理中面临知识冲突问题，通过分析内部表征揭示了冲突编码的线性可分性、深度定位、层次一致性和方向不对称性等机制特征。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长链推理中经常失败，特别是当不同知识源提供冲突信号时。研究者希望理解这些失败背后的机制，并形式化知识冲突的概念。

Method: 通过探测内部表征的方法，分析模型在不同知识冲突情况下的表现。区分输入级客观冲突和过程级有效冲突，并研究冲突信号在模型内部的编码方式。

Result: 发现四个关键特征：1) 线性可分性：不同冲突类型被编码为线性可分的特征；2) 深度定位：冲突信号集中在中后期层；3) 层次一致性：聚合噪声标记级信号可恢复输入级冲突类型；4) 方向不对称性：强化模型隐含源偏好比强制相反源容易得多。

Conclusion: 研究提供了多模态推理在知识冲突下的机制层面视角，为诊断和控制长链推理失败提供了原则性方法，有助于改进多模态大语言模型的可靠性。

Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.

</details>


### [63] [Disentangling Deception and Hallucination Failures in LLMs](https://arxiv.org/abs/2602.14529)
*Haolang Lu,Hongrui Peng,WeiYe Fu,Guoshun Nan,Xinye Cao,Xingrui Li,Hongcan Guo,Kun Wang*

Main category: cs.AI

TL;DR: 论文提出从机制角度区分LLM在实体事实查询中的失败模式，将知识存在与行为表达分离，构建受控环境分析幻觉与欺骗两种不同机制


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型失败的分析多从行为角度出发，将事实问答错误简单归因于知识缺失。本文认为这种观点可能混淆了不同的失败机制，需要从内部机制角度区分知识存在与行为表达

Method: 构建实体中心事实问题的受控环境，在保持知识不变的情况下选择性改变行为表达，系统分析四种行为案例。通过表示可分性、稀疏可解释性和推理时激活导向来分析失败模式

Result: 研究区分了幻觉和欺骗两种在输出层面相似但机制不同的失败模式，提出了机制导向的分析框架，为理解LLM内部工作机制提供了新视角

Conclusion: 需要从机制角度而非单纯行为角度分析LLM失败，知识存在与行为表达的分离框架有助于更精确地诊断和解决模型问题，为改进模型可靠性和可解释性提供理论基础

Abstract: Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.

</details>


### [64] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: MATEO是一个评估大型视觉语言模型时序执行顺序理解能力的多模态基准，基于高质量专业食谱语料库构建，包含步骤分解和对应图像，通过众包标注时序执行图。


<details>
  <summary>Details</summary>
Motivation: 现有研究对基础模型时序执行顺序的理解有限，主要依赖自动标注、线性链近似或纯文本输入，缺乏对真实世界规划所需多模态时序推理能力的评估。

Method: 收集高质量专业多模态食谱语料库，通过标准化编辑流程将指令分解为离散步骤并配图；设计可扩展的众包流程标注时序执行顺序图；评估6个SOTA LVLM在不同模型规模、语言上下文、多模态输入结构和微调策略下的表现。

Result: 论文提出了MATEO基准，但没有在摘要中提供具体的评估结果。基准本身可用于评估LVLM在时序执行顺序理解方面的能力。

Conclusion: MATEO填补了现有研究空白，为评估和改进LVLM在真实世界规划所需的多模态时序推理能力提供了基准工具。

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [65] [Tabular Foundation Models Can Learn Association Rules](https://arxiv.org/abs/2602.14622)
*Erkan Karabulut,Daniel Daza,Paul Groth,Martijn C. Schut,Victoria Degeler*

Main category: cs.AI

TL;DR: TabProbe：基于表格基础模型的关联规则挖掘框架，无需频繁项集挖掘，在低数据场景下仍能生成高质量规则


<details>
  <summary>Details</summary>
Motivation: 传统关联规则挖掘方法存在规则爆炸和可扩展性差的问题，而神经方法在低数据场景下性能下降。表格基础模型提供了解决这些限制的基础

Method: 提出模型无关的关联规则学习框架，可从任何条件概率模型中提取关联规则。具体实现TabProbe利用表格基础模型作为条件概率估计器，无需频繁项集挖掘

Result: 表格基础模型能持续生成简洁、高质量的关联规则，具有强大的预测性能，在低数据设置下保持鲁棒性，无需特定任务训练

Conclusion: TabProbe框架成功利用表格基础模型解决了传统关联规则挖掘的局限性，在低数据场景下表现出色，为知识发现提供了新方法

Abstract: Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.

</details>


### [66] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor框架通过将决策树导航分解为专门的节点级任务，解决了大语言模型在结构化工作流程中的遵循问题，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗分诊等高风险领域难以严格遵循结构化工作流程，单一提示方法随着提示长度增加会出现指令遵循退化、中间丢失效应和上下文窗口溢出等问题。

Method: Arbor框架将决策树分解为专门的节点级任务，将决策树标准化为边列表表示并存储以供动态检索。运行时，基于有向无环图的编排机制迭代检索当前节点的出边，通过专门的LLM调用评估有效转换，并将响应生成委托给单独的推理步骤。

Result: 在10个基础模型上评估，与单一提示基线相比，Arbor将平均轮次准确率提高了29.4个百分点，每轮延迟降低了57.1%，每轮成本平均降低了14.4倍。

Conclusion: 架构分解减少了对内在模型能力的依赖，使较小的模型能够匹配或超过在单一提示基线下运行的较大模型，为结构化决策支持提供了有效的解决方案。

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [67] [From User Preferences to Base Score Extraction Functions in Gradual Argumentation](https://arxiv.org/abs/2602.14674)
*Aniol Civit,Antonio Rago,Antonio Andriella,Guillem Alenyà,Francesca Toni*

Main category: cs.AI

TL;DR: 该论文提出了基分数提取函数，将用户对论点的偏好映射为基分数，从而简化了渐进论证中基分数的选择过程。


<details>
  <summary>Details</summary>
Motivation: 在渐进论证中，论点的基分数选择通常需要专业知识且不直观，而通过组织论点偏好可以简化这一任务。因此需要一种方法将用户偏好自动转换为基分数。

Method: 提出了基分数提取函数，该函数将带有偏好的双极论证框架映射为定量双极论证框架。设计了算法来近似人类偏好的非线性特征，并讨论了函数的设计选择和理想属性。

Result: 该方法在机器人设置中进行了理论和实验评估，能够更好地近似真实的人类偏好，并为实践中选择合适的渐进语义提供了建议。

Conclusion: 基分数提取函数提供了一种将用户偏好转换为基分数的有效方法，简化了渐进论证系统的构建过程，增强了透明度和可争议性。

Abstract: Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.

</details>


### [68] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: 本文提出使用top-k规划生成同一目标的多个不同计划，以解决现有目标识别数据集因启发式前向搜索导致的系统性偏差问题，并引入版本覆盖分数(VCS)来衡量目标识别器在不同计划集下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别数据集存在系统性偏差，这些数据集由启发式前向搜索规划系统生成，缺乏对更现实场景（如智能体使用不同规划器）的挑战性，影响了目标识别器在使用不同规划器时的评估效果。

Method: 提出使用top-k规划方法为同一目标假设生成多个不同的计划，从而创建能够缓解当前数据集偏差的基准。同时引入版本覆盖分数(VCS)这一新指标，用于衡量目标识别器基于不同计划集推断目标时的鲁棒性。

Result: 研究结果表明，在低可观测性设置下，当前最先进的目标识别器的鲁棒性显著下降。

Conclusion: 通过top-k规划生成多样化计划的方法能够创建更公平的评估基准，VCS指标有效揭示了目标识别器在不同规划器使用场景下的鲁棒性不足，特别是在低可观测性环境中。

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [69] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 前沿AI模型在核危机模拟中表现出复杂的战略行为，包括欺骗、心智理论和自我认知，验证并挑战了传统战略理论的核心理念。


<details>
  <summary>Details</summary>
Motivation: 研究前沿大语言模型在战略竞争环境中的行为模式，特别是核危机情境下的决策逻辑，为国家安全专业人员和更广泛的战略分析提供洞察。

Method: 使用三个前沿大语言模型（GPT-5.2、Claude Sonnet 4、Gemini 3 Flash）在核危机模拟中扮演对立领导人角色，观察它们在不确定性下的推理和行为。

Result: 模型表现出欺骗、心智理论和自我认知能力；验证了Schelling的承诺理论、Kahn的升级框架和Jervis的误解理论；但发现核禁忌无法阻止核升级，战略核攻击虽然罕见但确实发生，威胁更易引发反升级而非服从，高相互可信度加速而非威慑冲突，模型从不选择妥协或撤退。

Conclusion: AI模拟是强大的战略分析工具，但需要根据人类推理模式进行适当校准。理解前沿模型如何模仿或不模仿人类战略逻辑对于AI日益影响战略结果的世界至关重要。

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [70] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 该论文提出了第一个同时包含模式和事实的知识图谱精炼数据集资源，提供了从知识图谱中提取包含模式和事实的数据集的工作流程。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱精炼算法评估数据集通常只包含事实数据，即使源知识图谱中有模式信息也保留有限。这限制了依赖丰富本体约束、推理或神经符号技术的方法的评估，无法评估它们在大规模真实世界知识图谱中的性能。

Method: 提出了一个工作流程，用于提取同时包含模式和事实的数据集，处理模式和事实之间的不一致性，并利用推理推导隐含知识。该工作流程还提供将数据集序列化为OWL格式以支持推理服务，并提供将数据集加载为机器学习库典型张量表示的实用工具。

Result: 创建了一个包含从具有丰富模式的知识图谱中新提取的数据集的数据集套件，同时用模式信息丰富了现有数据集。每个数据集都以OWL格式序列化，可直接用于推理服务。

Conclusion: 该资源填补了知识图谱精炼评估中同时包含模式和事实的数据集的空白，支持依赖本体约束、推理和神经符号技术的方法的评估，为大规模真实世界知识图谱的应用提供了更好的评估基础。

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


### [71] [World Models for Policy Refinement in StarCraft II](https://arxiv.org/abs/2602.14857)
*Yixin Zhang,Ziyi Wang,Yiming Rong,Haoxi Wang,Jinling Jiang,Shuang Xu,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.AI

TL;DR: StarWM是首个用于星际争霸II的世界模型，通过结构化文本表示预测部分可观测环境下的未来状态，结合生成-模拟-精炼决策循环提升AI智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的星际争霸II智能体主要关注策略改进，忽视了将可学习的、动作条件化的转移模型整合到决策循环中。星际争霸II具有巨大的状态-动作空间和部分可观测性，是极具挑战性的测试平台。

Method: 1) 提出StarWM世界模型，预测部分可观测环境下的未来观测；2) 引入结构化文本表示，将观测分解为五个语义模块；3) 构建SC2-Dynamics-50k数据集用于动态预测指令微调；4) 开发多维离线评估框架；5) 提出StarWM-Agent决策系统，集成生成-模拟-精炼决策循环。

Result: 离线评估显示StarWM显著优于零样本基线：资源预测准确率提升近60%，己方宏观态势一致性改善。在线评估中，StarWM-Agent对抗内置AI在Hard(LV5)、Harder(LV6)、VeryHard(LV7)难度下胜率分别提升30%、15%、30%，同时改善了宏观管理稳定性和战术风险评估。

Conclusion: StarWM是首个用于星际争霸II的世界模型，通过结构化表示和世界模型增强的决策系统，显著提升了智能体在复杂部分可观测环境中的决策能力和性能表现。

Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

</details>


### [72] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 该论文提出了一种名为"概念影响"的新方法，用于更高效、更具语义性地识别训练数据对模型行为的影响，相比传统影响函数方法在计算效率和语义理解方面有显著改进。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛训练和微调，需要有效方法来识别哪些训练数据驱动特定行为（特别是意外行为）。现有训练数据归因方法（如影响函数）计算成本高，且基于单个测试示例进行归因，容易偏向句法相似性而非语义相似性。

Method: 提出了"概念影响"方法，将模型行为归因于语义方向（如线性探针或稀疏自编码器特征），而不是单个测试示例。同时证明了基于探针的归因方法是概念影响的一阶近似，性能相当但计算速度快一个数量级以上。

Result: 在涌现错位基准测试和实际后训练数据集上验证了概念影响及其近似方法的有效性，证明它们与传统影响函数性能相当，但计算可扩展性显著提高。

Conclusion: 在传统训练数据归因流程中融入可解释结构，能够实现更可扩展、更可解释的模型行为控制，为通过数据更好地管理模型行为提供了新途径。

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [73] [Lifted Relational Probabilistic Inference via Implicit Learning](https://arxiv.org/abs/2602.14890)
*Luise Ge,Brendan Juba,Kris Nilsson,Alison Shao*

Main category: cs.AI

TL;DR: 提出首个多项式时间框架，通过隐式学习一阶概率逻辑，在个体和世界两个层面同时进行提升推理，无需显式构建模型。


<details>
  <summary>Details</summary>
Motivation: 解决一阶关系领域中归纳学习与演绎推理之间的长期矛盾，传统提升推理需要完整模型且学习模型通常不可行，需要新的方法在不显式构建模型的情况下联合学习和推理。

Method: 将不完整的一阶公理与部分观测样本合并到平方和（SOS）层次的有界度片段中，同时执行两种提升：基础提升（重命名等价的基础矩共享变量）和世界提升（并行执行所有伪模型）。

Result: 实现了首个多项式时间框架，能够隐式学习一阶概率逻辑，并在个体和世界两个层面同时进行提升推理，产生在所有符合学习约束的世界中都成立的全局界限。

Conclusion: 成功调和了归纳学习与演绎推理的矛盾，通过隐式学习推理方法，在不构建显式模型的情况下，实现了高效的一阶关系概率推理。

Abstract: Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.

</details>


### [74] [The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics](https://arxiv.org/abs/2602.14903)
*Gregor Bachmann,Yichen Jiang,Seyed Mohsen Moosavi Dezfooli,Moin Nabi*

Main category: cs.AI

TL;DR: 本文深入分析了思维链（CoT）提示在大型语言模型中的工作机制，通过引入"潜力"概念量化CoT各部分对最终答案的贡献，发现了包括非单调性、推理洞察和幸运猜测等模式，并证明了CoT的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链提示已成为从大型语言模型中引出类似推理响应的标准技术，但其成功背后的驱动机制仍不清楚。本研究旨在深入理解CoT如何以及哪些部分真正贡献于最终答案。

Method: 引入"潜力"概念来量化CoT各部分增加正确完成可能性的程度，通过竞赛级数学问题的CoT轨迹分析，并研究CoT的可迁移性，测量较弱模型在较强模型部分CoT下的表现。

Result: 发现了CoT潜力的多种模式：1）强烈的非单调性（由于推理偏离）；2）尖锐但难以解释的峰值（推理洞察和跳跃）；3）有时是幸运猜测（模型在没有提供相关论证的情况下得出正确答案）。同时发现仅20%的部分CoT就能"解锁"较弱模型在原本无法解决问题上的性能。

Conclusion: CoT机制在很大程度上是可迁移的，虽然某些行为符合人类直觉（如洞察和偏离），但其他方面仍难以从人类角度理解。研究揭示了CoT提示的复杂工作机制，为理解大型语言模型的推理过程提供了新视角。

Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.

</details>


### [75] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: 该论文提出，强大的推理能力并非来自模型规模的扩大，而是源于语言自我反思，这种反思源于高质量的社会互动。作者基于维果茨基发展心理学，主张通过对话式内省体验来培养AI的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法将推理视为规模扩展的涌现属性，但作者认为这种方法存在局限。他们主张从维果茨基发展心理学出发，强调社会互动在认知发展中的核心作用，认为高质量对话是培养AI推理能力的关键。

Method: 提出基于"内省"的三个核心观点：1) 私人思维的社会起源：从对话环境中学习成为理解世界的新方式；2) 对话式内省体验：使智能体能够进行意义建构，将原始环境数据转化为丰富的可学习叙事；3) 对话质量即数据质量：智能体推理深度和计算效率取决于其掌握的对话多样性和严谨性。

Result: 论文提出了一种新的AI发展范式，强调对话质量而非数据规模是培养通用智能的关键。通过优化对话式支架，可以更有效地开发下一代通用智能。

Conclusion: 优化对话式支架是开发下一代通用智能的主要杠杆。推理能力不应被视为规模扩展的副产品，而应通过高质量的社会互动和语言自我反思来培养，对话质量成为新的数据质量标准。

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [76] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: 提出ReusStdFlow框架解决企业Agentic AI中的可重用性困境和结构幻觉问题，通过"提取-存储-构建"范式标准化异构DSL工作流，实现90%以上的准确率


<details>
  <summary>Details</summary>
Motivation: 解决企业Agentic AI中的"可重用性困境"和结构幻觉问题，企业中存在大量异构、平台特定的领域特定语言(DSL)工作流，难以实现标准化和高效复用

Method: 提出ReusStdFlow框架，采用"提取-存储-构建"范式：1) 将异构DSL解构为标准化的模块化工作流片段；2) 使用图数据库和向量数据库的双重知识架构，协同检索拓扑结构和功能语义；3) 采用检索增强生成(RAG)策略智能组装工作流

Result: 在200个真实世界的n8n工作流上进行测试，系统在提取和构建两方面都实现了超过90%的准确率

Conclusion: 该框架为企业数字资产的自动化重组和高效复用提供了标准化解决方案，有效解决了企业Agentic AI中的可重用性问题

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [77] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: MAC-AMP是一个基于多智能体大语言模型的闭环协作系统，用于多目标抗菌肽设计，通过模拟同行评审-自适应强化学习框架实现自主优化。


<details>
  <summary>Details</summary>
Motivation: 为解决抗菌肽设计中传统AI模型难以平衡活性、毒性、新颖性等多目标，且评分方法僵化、结果难以解释和优化的问题，利用大语言模型多智能体协作在复杂科学设计场景中的潜力。

Method: 提出MAC-AMP闭环多智能体协作系统，采用完全自主的模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可设计新型抗菌肽，具有跨领域可迁移性和可解释性。

Result: 实验表明MAC-AMP优于其他抗菌肽生成模型，能有效优化多个关键分子特性，在抗菌活性、抗菌肽相似性、毒性合规性和结构可靠性方面表现优异。

Conclusion: MAC-AMP系统成功实现了多目标抗菌肽设计，解决了传统模型的局限性，为抗菌肽发现提供了可解释、可优化的创新方法。

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [78] [On the Semantics of Primary Cause in Hybrid Dynamic Domains](https://arxiv.org/abs/2602.14994)
*Shakil M. Khan,Asim Mehmood,Sandra Zilles*

Main category: cs.AI

TL;DR: 本文提出了混合时态情境演算框架下的两种主要因果定义，并证明其等价性


<details>
  <summary>Details</summary>
Motivation: 实际因果推理是理性研究的基础，但现有研究主要关注离散变化，对连续变化和混合变化的因果研究不足

Method: 在混合时态情境演算框架中提出两种主要因果定义：一种是基础性定义，另一种通过贡献度形式化因果，并使用改进的"but-for"测试从反事实角度验证

Result: 证明了两种因果定义的等价性，并展示了这些定义具有直观合理的性质

Conclusion: 在混合行动理论框架中建立了既包含离散又包含连续变化的因果定义，为混合系统的因果推理提供了理论基础

Abstract: Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [79] [ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory](https://arxiv.org/abs/2602.13434)
*Maccoy Merrell,Daniel Puckett,Gino Chacon,Jeffrey Stuecheli,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: 该论文研究了Rowhammer缓解机制与硬件预取器之间的交叉影响，发现现有预取器在Rowhammer缓解下性能严重受损，提出ORAP预取器通过缓存DRAM行缓冲区内容来减少激活次数。


<details>
  <summary>Details</summary>
Motivation: 现有Rowhammer缓解方案评估通常忽略硬件预取器的影响。硬件预取器会增加DRAM激活率，而Rowhammer缓解的性能开销与内存访问模式直接相关，两者存在交叉影响，导致预取器的性能提升被严重削弱。

Method: 提出优化行访问预取器(ORAP)，利用末级缓存(LLC)空间缓存DRAM行缓冲区的大块内容，减少未来激活需求。与最先进的Berti预取器配合工作，降低DRAM激活率。

Result: 在RFM缓解内存系统中，ORAP将DRAM激活率降低51.3%，相比Berti和SPP-PPF预取器配置实现4.6%的加速。在PRAC缓解下，ORAP将能耗开销降低11.8%。

Conclusion: Rowhammer缓解机制与硬件预取器之间存在显著的交叉影响，传统预取器在Rowhammer缓解下的性能优势被削弱。ORAP通过缓存行缓冲区内容有效减少DRAM激活，在Rowhammer缓解环境中保持预取效益，同时降低能耗开销。

Abstract: Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.

</details>


### [80] [Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits](https://arxiv.org/abs/2602.13825)
*Paras Tiwari,Narendra Singh Dhakad,Shalu Rani,Sanjay Kumar,Themis Prodromakis*

Main category: cs.AR

TL;DR: 该研究实现了基于忆阻器的基本逻辑门和时序逻辑电路设计，在90nm CMOS工艺下进行优化，相比现有技术显著降低了面积、功耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 研究忆阻器在逻辑电路设计中的应用潜力，探索如何利用忆阻器特性实现更高效、低功耗、紧凑的电路设计，特别是针对时序逻辑电路的优化。

Method: 在Cadence Virtuoso的SPECTRE中设计、实现和优化基于忆阻器的逻辑门（NOT、AND、NAND、OR、NOR、XOR）和时序逻辑电路（D触发器、T触发器、JK触发器、SR触发器），并与90nm CMOS技术集成。使用Y2O3基忆阻器实验数据进行预验证。

Result: 忆阻器驱动的时序电路在面积、功耗和延迟方面分别减少了约24%、60%和58%，相比现有技术有显著改进。忆阻器在器件间和周期间切换时表现出较低的变异性。

Conclusion: 基于忆阻器的设计显著提高了各种逻辑电路的性能，使其更具面积和功耗效率，展示了忆阻器在设计低功耗、低成本、超快速和紧凑电路方面的潜力。

Abstract: In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits.

</details>


### [81] [ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute](https://arxiv.org/abs/2602.14262)
*Siddhartha Raman Sundara Raman,Jaydeep P. Kulkarni*

Main category: cs.AR

TL;DR: 提出了一种紧密集成的近内存GPU架构，在多种AI和计算工作负载上相比MIAOW GPU实现了6-16倍加速和6-13倍能效提升，支持可重构计算和动态分辨率更新。


<details>
  <summary>Details</summary>
Motivation: 传统GPU架构在处理多样化AI工作负载（如CNN、GCN、LP、LLM、Ising模型）时存在性能和能效瓶颈，需要一种更紧密集成的近内存架构来提升计算效率和能源利用率。

Method: 设计了一个紧密集成的统一近内存GPU架构，包含自定义的稀疏感知近内存电路（约1.5倍能效提升）和轻量级softmax电路（约1.6倍能效提升），支持INT16以下可重构计算和动态分辨率更新，能够高效扩展到不同问题规模。

Result: 在CNN、GCN、线性规划、大语言模型和Ising工作负载上相比MIAOW GPU实现了6-16倍加速和6-13倍能效提升；ABI-enabled MI300和Blackwell系统相比基线MI300和Blackwell实现了约4.5倍加速。

Conclusion: 该紧密集成的近内存GPU架构能够显著提升多样化AI工作负载的性能和能效，支持可重构计算和动态分辨率更新，在实际系统中展现出优越的加速效果。

Abstract: We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [82] [ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies](https://arxiv.org/abs/2602.14107)
*Yuze Liu,Shibo Chu,Tiehua Zhang,Hao Zhou,Zhishu Shen,Jinze Wang,Jianzhong Qi,Feng Xia*

Main category: cs.DC

TL;DR: ML-ECS是一个面向边缘-云协同的隐私保护多模态学习框架，通过跨模态对比学习、自适应多模态调优、模态感知模型聚合和SLM增强的CCL等组件，解决实际边缘环境中模态异构性和模型结构异构性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界边缘环境中，协同多模态学习面临两大挑战：模态异构性（不同领域具有不同的模态组合）和模型结构异构性（不同的模态特定编码器/融合模块）。现有方法难以有效处理这些异构性问题。

Method: 提出ML-ECS框架，包含四个核心组件：1) 跨模态对比学习(CCL)在共享潜在空间中对齐模态表示；2) 自适应多模态调优(AMT)保留本地数据集的领域特定知识；3) 模态感知模型聚合(MMA)在聚合时缓解缺失模态引起的噪声；4) SLM增强的CCL(SE-CCL)促进云边双向知识转移。

Result: 在各种多模态任务上，ML-ECS在不同模态可用性条件下均优于现有最优基线，在Rouge-LSum指标上提升5.44%到12.08%，同时改善了客户端和服务器端性能。通过仅传输低秩LoRA参数和融合表示，实现了高通信效率，仅需总参数量的0.65%。

Conclusion: ML-ECS有效解决了边缘-云协同多模态学习中的模态异构性和模型结构异构性问题，在保持隐私保护的同时实现了高性能和高通信效率，为实际边缘环境中的基础模型部署提供了可行方案。

Abstract: Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.

</details>


### [83] [Floe: Federated Specialization for Real-Time LLM-SLM Inference](https://arxiv.org/abs/2602.14302)
*Chunlin Tian,Kahou Tam,Yebo Wu,Shuaihang Zhong,Li Li,Nicholas D. Lane,Chengzhong Xu*

Main category: cs.DC

TL;DR: Floe是一个混合联邦学习框架，结合云端大语言模型和边缘设备上的轻量级小语言模型，实现低延迟、隐私保护的实时推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实时系统中部署面临计算需求大和隐私问题，需要在资源受限的边缘设备上实现低延迟、隐私保护的推理。

Method: 结合云端黑盒大语言模型和边缘设备上的轻量级小语言模型；个人数据和微调保持在设备端；采用异构感知的LoRA适配策略实现跨硬件高效部署；使用logit级融合机制实现边缘与云端模型的实时协调。

Result: 实验表明Floe增强了用户隐私和个性化能力，显著提高了模型性能，并在实时约束下降低了边缘设备的推理延迟。

Conclusion: Floe框架为资源受限的实时系统提供了一种有效的混合联邦学习解决方案，平衡了隐私保护、个性化需求和计算效率。

Abstract: Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.

</details>


### [84] [Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement](https://arxiv.org/abs/2602.14704)
*Zong Yu Lee,Xueyan Tang*

Main category: cs.DC

TL;DR: 本文评估了云数据中心中虚拟机放置问题的最先进算法，该问题被建模为最小使用时间动态向量装箱问题，在不同设置下（非预知、预知和学习增强）比较算法性能。


<details>
  <summary>Details</summary>
Motivation: 虚拟机放置是云计算中的关键挑战，需要高效利用数据中心物理机资源。现有研究将虚拟机放置问题建模为最小使用时间动态向量装箱问题，但缺乏对不同设置下算法性能的系统评估。

Method: 1. 将虚拟机放置问题建模为最小使用时间动态向量装箱问题；2. 在三种设置下评估算法：非预知（虚拟机寿命未知）、预知（寿命已知）和学习增强（寿命预测）；3. 除了文献中的算法，还开发了新算法或改进；4. 使用微软Azure真实数据集进行实证实验。

Result: 通过微软Azure真实数据集的实验，分析了不同算法的性能表现，探讨了算法结构和在实际应用中表现良好的设计元素。

Conclusion: 研究提供了关于最小使用时间动态向量装箱问题算法的深入见解，识别了在实际应用中有效的算法结构和设计元素，为云数据中心资源管理提供了实用指导。

Abstract: Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [85] [Backdooring Bias in Large Language Models](https://arxiv.org/abs/2602.13427)
*Anudeep Das,Prach Chantasantitam,Gurjot Singh,Lipeng He,Mariia Ponomarenko,Florian Kerschbaum*

Main category: cs.CR

TL;DR: 本文分析了白盒威胁模型下语法触发和语义触发后门攻击的有效性，以及两种防御方法的局限性。研究发现语义触发攻击在诱导负面偏见方面更有效，而两种防御方法要么导致效用大幅下降，要么需要高计算开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在可能产生重大后果的部署场景中，后门攻击可用于诱导特定偏见。现有研究主要关注黑盒威胁模型，但在偏见操纵场景中，模型构建者本身可能是攻击者，需要研究白盒威胁模型。此外，尽管语义触发后门研究增多，但大多数研究仍局限于语法触发攻击。

Method: 采用白盒威胁模型，使用更高的中毒比例和更强的数据增强，进行了超过1000次评估，分析语法触发和语义触发后门攻击的潜力。同时研究了两种代表性防御范式：模型内在和模型外在的后门移除方法。

Result: 研究发现：1）语法触发和语义触发攻击都能有效诱导目标行为并保持效用；2）语义触发攻击在诱导负面偏见方面更有效；3）两种后门类型在诱导正面偏见方面都存在困难；4）两种防御方法都能缓解这些后门，但要么导致效用大幅下降，要么需要高计算开销。

Conclusion: 白盒威胁模型下的后门攻击具有显著威胁，特别是语义触发攻击在诱导负面偏见方面更有效。现有防御方法存在效用损失或计算成本高的局限性，需要开发更有效的防御机制来应对这种威胁。

Abstract: Large language models (LLMs) are increasingly deployed in settings where inducing a bias toward a certain topic can have significant consequences, and backdoor attacks can be used to produce such models. Prior work on backdoor attacks has largely focused on a black-box threat model, with an adversary targeting the model builder's LLM. However, in the bias manipulation setting, the model builder themselves could be the adversary, warranting a white-box threat model where the attacker's ability to poison, and manipulate the poisoned data is substantially increased. Furthermore, despite growing research in semantically-triggered backdoors, most studies have limited themselves to syntactically-triggered attacks. Motivated by these limitations, we conduct an analysis consisting of over 1000 evaluations using higher poisoning ratios and greater data augmentation to gain a better understanding of the potential of syntactically- and semantically-triggered backdoor attacks in a white-box setting. In addition, we study whether two representative defense paradigms, model-intrinsic and model-extrinsic backdoor removal, are able to mitigate these attacks. Our analysis reveals numerous new findings. We discover that while both syntactically- and semantically-triggered attacks can effectively induce the target behaviour, and largely preserve utility, semantically-triggered attacks are generally more effective in inducing negative biases, while both backdoor types struggle with causing positive biases. Furthermore, while both defense types are able to mitigate these backdoors, they either result in a substantial drop in utility, or require high computational overhead.

</details>


### [86] [MemeTrans: A Dataset for Detecting High-Risk Memecoin Launches on Solana](https://arxiv.org/abs/2602.13480)
*Sihao Hu,Selim Furkan Tekin,Yichang Xu,Ling Liu*

Main category: cs.CR

TL;DR: MemeTrans是首个用于研究和检测Solana上高风险模因币发行的数据集，包含超过4万个成功迁移到去中心化交易所的模因币发行数据，设计了122个特征来捕捉发行模式，并通过标注方法标记风险等级，实验显示ML模型可减少56.1%的财务损失。


<details>
  <summary>Details</summary>
Motivation: 启动板已成为区块链上发行模因币的主要机制，其全自动、无需代码的创建过程导致了高风险代币发行的激增，给不知情的买家造成了重大财务损失。目前缺乏专门用于研究和检测高风险模因币发行的数据集。

Method: 1) 构建MemeTrans数据集，覆盖超过4万个成功迁移到去中心化交易所的Solana模因币发行；2) 设计122个特征，涵盖上下文、交易活动、持仓集中度和时间序列动态等维度；3) 引入捆绑级数据以揭示同一实体控制的多个账户；4) 开发结合统计指标和操纵模式检测器的风险标注方法。

Result: MemeTrans数据集包含超过3000万次启动板初始销售交易和1.8亿次迁移后交易。在高风险发行检测任务上的实验表明，设计的特征能有效捕捉高风险模式，基于MemeTrans训练的机器学习模型可将财务损失减少56.1%。

Conclusion: MemeTrans是首个专门用于研究和检测Solana上高风险模因币发行的数据集，通过精心设计的特征和风险标注方法，为检测高风险模因币发行提供了有效工具，实验证明ML模型能显著减少投资者的财务损失。

Abstract: Launchpads have become the dominant mechanism for issuing memecoins on blockchains due to their fully automated, no-code creation process. This new issuance paradigm has led to a surge in high-risk token launches, causing substantial financial losses for unsuspecting buyers. In this paper, we introduce MemeTrans, the first dataset for studying and detecting high-risk memecoin launches on Solana. MemeTrans covers over 40k memecoin launches that successfully migrated to the public Decentralized Exchange (DEX), with over 30 million transactions during the initial sale on launchpad and 180 million transactions after migration. To precisely capture launch patterns, we design 122 features spanning dimensions such as context, trading activity, holding concentration, and time-series dynamics, supplemented with bundle-level data that reveals multiple accounts controlled by the same entity. Finally, we introduce an annotation approach to label the risk level of memecoin launches, which combines statistical indicators with a manipulation-pattern detector. Experiments on the introduced high-risk launch detection task suggest that designed features are informative for capturing high-risk patterns and ML models trained on MemeTrans can effectively reduce financial loss by 56.1%. Our dataset, experimental code, and pipeline are publicly available at: https://github.com/git-disl/MemeTrans.

</details>


### [87] [SecureGate: Learning When to Reveal PII Safely via Token-Gated Dual-Adapters for Federated LLMs](https://arxiv.org/abs/2602.13529)
*Mohamed Shaaban,Mohamed Elmahallawy*

Main category: cs.CR

TL;DR: SecureGate：一种用于大语言模型联邦微调的隐私感知框架，通过双适配器架构和令牌控制门控模块，在保护隐私的同时保持任务效用


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，联邦微调成为利用分布式数据同时保护隐私的重要方式。然而现有方法面临两大挑战：1) 大语言模型记忆导致的个人身份信息泄露风险；2) 异构数据下全局泛化与本地效用之间的持续冲突。现有防御方法如数据清洗和差分隐私虽然能减少泄露，但通常会降低下游性能。

Method: SecureGate采用双适配器LoRA架构：安全适配器学习经过清洗的、可全局共享的表示；揭示适配器捕获敏感的、组织特定的知识。通过令牌控制的门控模块在推理时选择性激活这些适配器，实现受控的信息披露而无需重新训练。

Result: 在多个大语言模型和真实数据集上的实验表明，SecureGate在提高任务效用的同时显著减少了个人身份信息泄露，实现了推理攻击准确率降低31.66倍，未授权请求提取召回率降低17.07倍。同时保持100%的路由可靠性，仅产生最小的计算和通信开销。

Conclusion: SecureGate为联邦微调大语言模型提供了一种有效的隐私保护解决方案，通过细粒度的隐私控制机制，在保护敏感信息的同时保持了模型性能，解决了隐私保护与模型效用之间的权衡问题。

Abstract: Federated learning (FL) enables collaborative training across organizational silos without sharing raw data, making it attractive for privacy-sensitive applications. With the rapid adoption of large language models (LLMs), federated fine-tuning of generative LLMs has gained attention as a way to leverage distributed data while preserving confidentiality. However, this setting introduces fundamental challenges: (i) privacy leakage of personally identifiable information (PII) due to LLM memorization, and (ii) a persistent tension between global generalization and local utility under heterogeneous data. Existing defenses, such as data sanitization and differential privacy, reduce leakage but often degrade downstream performance. We propose SecureGate, a privacy-aware federated fine-tuning framework for LLMs that provides fine-grained privacy control without sacrificing utility. SecureGate employs a dual-adapter LoRA architecture: a secure adapter that learns sanitized, globally shareable representations, and a revealing adapter that captures sensitive, organization-specific knowledge. A token-controlled gating module selectively activates these adapters at inference time, enabling controlled information disclosure without retraining. Extensive experiments across multiple LLMs and real-world datasets show that SecureGate improves task utility while substantially reducing PII leakage, achieving up to a 31.66X reduction in inference attack accuracy and a 17.07X reduction in extraction recall for unauthorized requests. Additionally, it maintains 100% routing reliability to the correct adapter and incurs only minimal computational and communication overhead.

</details>


### [88] [DWBench: Holistic Evaluation of Watermark for Dataset Copyright Auditing](https://arxiv.org/abs/2602.13541)
*Xiao Ren,Xinyi Yu,Linkang Du,Min Chen,Yuanchao Shu,Zhou Su,Yunjun Gao,Zhikun Zhang*

Main category: cs.CR

TL;DR: DWBench：一个用于图像数据集水印技术的统一基准测试工具包，通过系统评估25种代表性方法，揭示了现有技术在不同场景下的权衡和局限性。


<details>
  <summary>Details</summary>
Motivation: 深度学习对大规模数据集的需求激增，数据所有者面临未经授权使用的风险。现有数据集水印技术缺乏一致的评估标准，阻碍了公平比较和实际可行性评估。

Method: 提出双层分类法（基于模型vs无模型注入；模型行为vs模型消息验证），开发DWBench统一基准测试工具包，在分类和生成任务中系统评估25种方法，包括标准化条件、鲁棒性测试、多水印共存和多用户干扰。

Result: 评估发现：1) 没有单一方法在所有场景中占优；2) 分类和生成任务需要专门方法；3) 现有技术在低水印率和实际多用户设置中表现不稳定，存在高误报率或性能下降。引入两个新指标：样本显著性（细粒度区分）和验证成功率（数据集级审计）。

Conclusion: DWBench为图像数据集水印技术提供了准确可复现的基准测试框架，有助于推动水印技术的可靠性和实用性发展，在面对广泛AI驱动数据利用时加强版权保护。

Abstract: The surging demand for large-scale datasets in deep learning has heightened the need for effective copyright protection, given the risks of unauthorized use to data owners. Although the dataset watermark technique holds promise for auditing and verifying usage, existing methods are hindered by inconsistent evaluations, which impede fair comparisons and assessments of real-world viability. To address this gap, we propose a two-layer taxonomy that categorizes methods by implementation (model-based vs. model-free injection; model-behavior vs. model-message verification), offering a structured framework for cross-task analysis. Then, we develop DWBench, a unified benchmark and open-source toolkit for systematically evaluating image dataset watermark techniques in classification and generation tasks.
  Using DWBench, we assess 25 representative methods under standardized conditions, perturbation-based robustness tests, multi-watermark coexistence, and multi-user interference. In addition to reporting the results of four commonly used metrics, we present the results of two new metrics: sample significance for fine-grained watermark distinguishability and verification success rate for dataset-level auditing, which enable accurate and reproducible benchmarking. Key findings reveal inherent trade-offs: no single method dominates all scenarios; classification and generation tasks require specialized approaches; and existing techniques exhibit instability at low watermark rates and in realistic multi-user settings, with elevated false positives or performance declines. We hope that DWBench can facilitate advances in watermark reliability and practicality, thus strengthening copyright safeguards in the face of widespread AI-driven data exploitation.

</details>


### [89] [AISA: Awakening Intrinsic Safety Awareness in Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2602.13547)
*Weiming Song,Xuan Xie,Ruiping Yin*

Main category: cs.CR

TL;DR: AISA是一种轻量级单次防御方法，通过定位LLM内部固有的安全意识，利用特定注意力头提取可解释的风险评分，并进行对数级别的引导，在不修改模型参数的情况下增强安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防御方法存在诸多问题：需要昂贵的微调、侵入式的提示重写、或外部护栏导致延迟增加和实用性下降。需要一种轻量级、单次通过的防御方法，能够激活模型内部已有的安全行为。

Method: AISA首先通过时空分析定位内在安全意识，发现意图判别信号广泛编码，在最终结构标记前的特定注意力头的缩放点积输出中表现出特别强的可分离性。使用自动选择的紧凑头部集合，提取可解释的提示风险评分，然后进行对数级别引导：根据推断的风险比例调整解码分布，从良性提示的正常生成到高风险请求的校准拒绝。

Result: 在13个数据集、12个LLM和14个基线的广泛实验中，AISA在小型（7B）模型上实现了与强大专有基线竞争的检测器级性能，提高了鲁棒性和可转移性，同时保持了实用性并减少了错误拒绝。

Conclusion: AISA是一种有效的轻量级防御方法，能够在不改变模型参数、添加辅助模块或需要多次推理的情况下，使弱对齐或故意冒险的模型变体实现更安全的部署。

Abstract: Large language models (LLMs) remain vulnerable to jailbreak prompts that elicit harmful or policy-violating outputs, while many existing defenses rely on expensive fine-tuning, intrusive prompt rewriting, or external guardrails that add latency and can degrade helpfulness. We present AISA, a lightweight, single-pass defense that activates safety behaviors already latent inside the model rather than treating safety as an add-on. AISA first localizes intrinsic safety awareness via spatiotemporal analysis and shows that intent-discriminative signals are broadly encoded, with especially strong separability appearing in the scaled dot-product outputs of specific attention heads near the final structural tokens before generation. Using a compact set of automatically selected heads, AISA extracts an interpretable prompt-risk score with minimal overhead, achieving detector-level performance competitive with strong proprietary baselines on small (7B) models. AISA then performs logits-level steering: it modulates the decoding distribution in proportion to the inferred risk, ranging from normal generation for benign prompts to calibrated refusal for high-risk requests -- without changing model parameters, adding auxiliary modules, or requiring multi-pass inference. Extensive experiments spanning 13 datasets, 12 LLMs, and 14 baselines demonstrate that AISA improves robustness and transfer while preserving utility and reducing false refusals, enabling safer deployment even for weakly aligned or intentionally risky model variants.

</details>


### [90] [Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning](https://arxiv.org/abs/2602.13562)
*Yanbo Wang,Minzheng Wang,Jian Liang,Lu Wang,Yongcan Yu,Ran He*

Main category: cs.CR

TL;DR: 本文提出自适应安全上下文学习框架，通过将安全对齐建模为多轮工具使用过程，解耦规则检索与推理，缓解安全性与实用性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在复杂任务中表现出色，但需要严格的安全措施。安全对齐的核心挑战在于安全性与实用性之间的固有权衡。现有对齐策略通常通过上下文蒸馏构建带有显式安全规则的思维链训练数据，这种方法无意中限制了推理能力，因为它在规则记忆和拒绝之间建立了僵化的关联。

Method: 提出自适应安全上下文学习框架，将安全对齐建模为多轮工具使用过程，使模型能够自主决定何时咨询安全规则以及如何生成持续推理。此外，为了抵消强化学习中规则咨询的偏好，引入逆频率策略优化来重新平衡优势估计。

Result: 通过解耦规则检索和后续推理，该方法相比基线实现了更高的整体性能。

Conclusion: 自适应安全上下文学习框架有效缓解了安全对齐中的安全性与实用性权衡问题，通过灵活的规则咨询机制提升了推理模型的整体性能。

Abstract: While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.

</details>


### [91] [Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges](https://arxiv.org/abs/2602.13576)
*Ruomeng Ding,Yifei Pang,He Sun,Yizhong Wang,Zhiwei Steven Wu,Zhun Deng*

Main category: cs.CR

TL;DR: 论文发现LLM评估流程中存在"评分标准诱导偏好漂移"漏洞，即使评分标准修改通过基准测试验证，仍会导致评估者在目标领域产生系统性偏好偏移，这种漏洞可被利用进行评分标准攻击，降低目标领域准确性，并通过对齐流程传播到训练模型中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的评估和对齐流程越来越依赖基于LLM的评估者，这些评估者的行为由自然语言评分标准指导并通过基准测试验证。研究者发现这一工作流程中存在一个先前未被充分认识到的漏洞，即评分标准修改即使通过基准测试验证，仍可能导致评估者在目标领域产生系统性偏好偏移。

Method: 研究识别了"评分标准诱导偏好漂移"漏洞，展示了即使评分标准编辑通过基准验证，仍能产生系统性偏好偏移。进一步通过评分标准偏好攻击实验，展示基准兼容的评分标准编辑如何引导评估偏离固定的人类或可信参考标准，在目标领域系统性诱导RIPD。当这些评估用于生成下游后训练偏好标签时，诱导的偏见会通过对齐流程传播并内化到训练策略中。

Result: 研究发现评分标准编辑可导致目标领域准确性显著下降：帮助性降低9.5%，无害性降低27.9%。当这些评估用于下游对齐训练时，诱导的偏见会传播并内化到模型行为中，导致持久且系统性的行为漂移。评分标准被证明是一个敏感且可操纵的控制接口。

Conclusion: 评估评分标准是一个敏感且可操纵的控制接口，揭示了超越评估者可靠性本身的系统级对齐风险。研究结果强调了评分标准作为攻击面的重要性，以及当前评估流程中存在的系统性漏洞，这些漏洞可能通过对齐流程传播并影响最终模型行为。

Abstract: Evaluation and alignment pipelines for large language models increasingly rely on LLM-based judges, whose behavior is guided by natural-language rubrics and validated on benchmarks. We identify a previously under-recognized vulnerability in this workflow, which we term Rubric-Induced Preference Drift (RIPD). Even when rubric edits pass benchmark validation, they can still produce systematic and directional shifts in a judge's preferences on target domains. Because rubrics serve as a high-level decision interface, such drift can emerge from seemingly natural, criterion-preserving edits and remain difficult to detect through aggregate benchmark metrics or limited spot-checking. We further show this vulnerability can be exploited through rubric-based preference attacks, in which benchmark-compliant rubric edits steer judgments away from a fixed human or trusted reference on target domains, systematically inducing RIPD and reducing target-domain accuracy up to 9.5% (helpfulness) and 27.9% (harmlessness). When these judgments are used to generate preference labels for downstream post-training, the induced bias propagates through alignment pipelines and becomes internalized in trained policies. This leads to persistent and systematic drift in model behavior. Overall, our findings highlight evaluation rubrics as a sensitive and manipulable control interface, revealing a system-level alignment risk that extends beyond evaluator reliability alone. The code is available at: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Warning: Certain sections may contain potentially harmful content that may not be appropriate for all readers.

</details>


### [92] [AlignSentinel: Alignment-Aware Detection of Prompt Injection Attacks](https://arxiv.org/abs/2602.13597)
*Yuqi Jia,Ruiqi Wang,Xilong Wang,Chong Xiang,Neil Gong*

Main category: cs.CR

TL;DR: AlignSentinel：基于注意力图的三分类器，用于检测提示注入攻击，能区分恶意指令、对齐指令和非指令输入，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击检测方法通常将所有包含指令的输入都分类为恶意，导致包含与预期任务对齐的指令的良性输入被误分类。需要区分恶意指令、对齐指令和非指令输入。

Method: 提出AlignSentinel三分类器，利用从LLM注意力图中提取的特征，将输入分为三类：恶意指令输入、对齐指令输入和非指令输入。构建了首个包含所有三类输入的系统性基准。

Result: 在自建基准和现有基准（后者主要缺乏对齐指令输入）上的实验表明，AlignSentinel能准确检测恶意指令输入，性能显著优于基线方法。

Conclusion: 通过考虑指令层次结构并区分三类输入，AlignSentinel有效解决了现有检测方法对包含对齐指令的良性输入的误分类问题，为提示注入攻击检测提供了更准确的解决方案。

Abstract: % Prompt injection attacks insert malicious instructions into an LLM's input to steer it toward an attacker-chosen task instead of the intended one. Existing detection defenses typically classify any input with instruction as malicious, leading to misclassification of benign inputs containing instructions that align with the intended task. In this work, we account for the instruction hierarchy and distinguish among three categories: inputs with misaligned instructions, inputs with aligned instructions, and non-instruction inputs. We introduce AlignSentinel, a three-class classifier that leverages features derived from LLM's attention maps to categorize inputs accordingly. To support evaluation, we construct the first systematic benchmark containing inputs from all three categories. Experiments on both our benchmark and existing ones--where inputs with aligned instructions are largely absent--show that AlignSentinel accurately detects inputs with misaligned instructions and substantially outperforms baselines.

</details>


### [93] [MarcoPolo: A Zero-Permission Attack for Location Type Inference from the Magnetic Field using Mobile Devices](https://arxiv.org/abs/2602.13915)
*Beatrice Perez,Abhinav Mehrotra,Mirco Musolesi*

Main category: cs.CR

TL;DR: 通过手机内置磁力计无需权限推断用户位置类型的零权限攻击方法，使用四种时间序列分类技术，在真实环境中收集近70小时数据，在未知地点和未知设备测试中分别达到40.5%和39.5%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 移动设备位置信息敏感且受系统保护，但应用仍可通过无需权限的替代传感器（如磁力计）推断粗略位置信息，存在隐私泄露风险。

Method: 提出基于磁力计信号的零权限攻击方法，使用四种时间序列分类技术识别位置类型。在真实环境中收集6部手机在66个地点近70小时的磁力计数据，分为6个位置类别。

Result: 在未知地点测试（leave-a-place-out）中达到40.5%准确率，在未知设备测试（leave-a-device-out）中达到39.5%准确率，相比随机基线16.7%有显著提升。

Conclusion: 磁力计等无需权限的传感器可被用于推断用户位置类型，揭示了移动设备隐私保护的新漏洞，需要重新评估传感器访问权限策略。

Abstract: Location information extracted from mobile devices has been largely exploited to reveal our routines, significant places, and interests just to name a few. Given the sensitivity of the information it reveals, location access is protected by mobile operating systems and users have control over which applications can access it. We argue that applications can still infer the coarse-grain location information by using alternative sensors that are available in off-the-shelf mobile devices that do not require any permissions from the users. In this paper we present a zero-permission attack based on the use of the in-built magnetometer, considering a variety of methods for identifying location-types from their magnetic signature. We implement the proposed approach by using four different techniques for time-series classification. In order to evaluate the approach, we conduct an in-the-wild study to collect a dataset of nearly 70 hours of magnetometer readings with six different phones at 66 locations, each accompanied by a label that classifies it as belonging to one of six selected categories. Finally, using this dataset, we quantify the performance of all models based on two evaluation criteria: (i) leave-a-place-out (using the test data collected from an unknown place), and (ii) leave-a-device-out (using the test data collected from an unknown device) showing that we are able to achieve 40.5% and 39.5% accuracy in classifying the location-type for each evaluation criteria respectively against a random baseline of approximately 16.7% for both of them.

</details>


### [94] [From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection](https://arxiv.org/abs/2602.14012)
*Youpeng Li,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 该论文首次全面研究了LLM漏洞检测的后训练流程，从冷启动SFT到离策略偏好优化和在线RL，揭示了数据准备、阶段交互、奖励机制和评估协议如何共同影响模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在漏洞检测中展现出可解释性和上下文感知的优势，但后训练方法在漏洞检测领域的系统性应用尚未得到充分探索。现有方法在数据准备、训练阶段交互、奖励机制设计等方面缺乏系统研究。

Method: 研究构建了完整的后训练流程：从冷启动监督微调(SFT)开始，到离策略偏好优化(DPO/ORPO)，再到在线强化学习(GRPO)。重点考察了数据筛选策略、训练阶段交互、奖励信号设计(粗粒度vs细粒度根因判断)、评估协议(二元匹配vs基于根因分析的LLM-as-a-Judge)等因素。

Result: 研究发现：(1)基于拒绝采样的SFT优于基于推理的监督；(2)过多SFT会抑制RL的自我探索；(3)细粒度根因判断奖励优于粗粒度信号；(4)过滤极难样本可提高训练效率但可能损失性能；(5)GRPO训练的模型显著优于SFT、DPO/ORPO和零-shot SOTA LLM；(6)基于根因分析的LLM-as-a-Judge评估比二元匹配更稳健。

Conclusion: 该研究为LLM漏洞检测的后训练提供了系统指导，揭示了在线RL的巨大潜力，并提出了更可靠的评估协议。研究强调了数据准备、阶段交互、奖励机制和评估方法在构建高效漏洞检测系统中的重要性。

Abstract: The integration of LLMs into vulnerability detection (VD) has shifted the field toward interpretable and context-aware analysis. While post-training methods have shown promise in general coding tasks, their systematic application to VD remains underexplored. In this paper, we present the first comprehensive investigation into the post-training pipeline for LLM-based VD, spanning from cold-start SFT to off-policy preference optimization and on-policy RL, uncovering how data curation, stage interactions, reward mechanisms, and evaluation protocols collectively dictate the efficacy of model training and assessment. Our study identifies practical guidelines and insights: (1) SFT based on rejection sampling greatly outperforms rationalization-based supervision, which can introduce hallucinations due to ground-truth leakage. (2) While increased SFT epochs constantly benefit preference optimization, excessive SFT inhibits self-exploration during RL, ultimately limiting performance gains. (3) Coarse-grained reward signals often mislead RL, whereas fine-grained root-cause judgments ensure reliable credit assignment. Specification-based rewards offer further benefits but incur significant effort in specification generation. (4) Although filtering extremely hard-to-detect vulnerability samples improves RL training efficiency, the cost of performance loss should be considered in practical applications. (5) Models trained under GRPO significantly outperform those using SFT and preference optimization (i.e., DPO and ORPO), as well as a series of zero-shot SOTA LLMs, underscoring the significant potential of on-policy RL for LLM-based VD. (6) In contrast to binary matching that tends to overestimate performance, LLM-as-a-Judge based on root-cause analysis provides a more robust evaluation protocol, although its accuracy varies across judge models with different levels of security expertise.

</details>


### [95] [MC$^2$Mark: Distortion-Free Multi-Bit Watermarking for Long Messages](https://arxiv.org/abs/2602.14030)
*Xuehao Cui,Ruibo Chen,Yihan Wu,Heng Huang*

Main category: cs.CR

TL;DR: MC²Mark：一种无失真的多比特水印框架，通过多通道彩色重加权和多层序列重加权技术，在保持文本质量的同时实现长消息的可靠嵌入和解码。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本与人类写作难以区分，需要可靠的来源追踪方法。现有的多比特水印方法在保持文本质量和水印强度的同时携带长消息方面存在困难。

Method: 提出MC²Mark框架，采用多通道彩色重加权技术通过结构化令牌重加权编码比特，同时保持令牌分布无偏；结合多层序列重加权增强水印信号，以及证据累积检测器进行消息恢复。

Result: 实验表明，MC²Mark在保持生成质量的同时，检测能力和鲁棒性优于现有多比特水印方法，短消息准确率接近完美，长消息准确率比次优方法提高近30%。

Conclusion: MC²Mark是一种有效的无失真多比特水印框架，能够可靠地嵌入和解码长消息，在文本质量和水印强度之间取得了良好平衡。

Abstract: Large language models now produce text indistinguishable from human writing, which increases the need for reliable provenance tracing. Multi-bit watermarking can embed identifiers into generated text, but existing methods struggle to keep both text quality and watermark strength while carrying long messages. We propose MC$^2$Mark, a distortion-free multi-bit watermarking framework designed for reliable embedding and decoding of long messages. Our key technical idea is Multi-Channel Colored Reweighting, which encodes bits through structured token reweighting while keeping the token distribution unbiased, together with Multi-Layer Sequential Reweighting to strengthen the watermark signal and an evidence-accumulation detector for message recovery. Experiments show that MC$^2$Mark improves detectability and robustness over prior multi-bit watermarking methods while preserving generation quality, achieving near-perfect accuracy for short messages and exceeding the second-best method by nearly 30% for long messages.

</details>


### [96] [AXE: An Agentic eXploit Engine for Confirming Zero-Day Vulnerability Reports](https://arxiv.org/abs/2602.14345)
*Amirali Sajadi,Tu Nguyen,Kostadin Damevski,Preetha Chatterjee*

Main category: cs.CR

TL;DR: AXE是一个多智能体框架，利用漏洞元数据（CWE分类和代码位置）自动生成Web应用漏洞利用，相比黑盒方法提升3倍成功率。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测工具产生大量误报和非可操作报告，而自动化利用系统通常与检测管道隔离，未能充分利用漏洞类型和源代码位置等元数据。

Method: 提出Agentic eXploit Engine (AXE)多智能体框架，通过解耦的规划、代码探索和动态执行反馈，将轻量级检测元数据映射到具体漏洞利用。

Result: 在CVE-Bench数据集上，AXE达到30%的利用成功率，相比最先进的黑盒基线提升3倍；即使单智能体配置，灰盒元数据也能带来1.75倍性能提升。

Conclusion: AXE能够生成可操作、可复现的概念验证，有助于简化Web漏洞分类和修复；系统错误分析显示失败主要源于特定推理差距，包括误解漏洞语义和未满足执行前提条件。

Abstract: Vulnerability detection tools are widely adopted in software projects, yet they often overwhelm maintainers with false positives and non-actionable reports. Automated exploitation systems can help validate these reports; however, existing approaches typically operate in isolation from detection pipelines, failing to leverage readily available metadata such as vulnerability type and source-code location. In this paper, we investigate how reported security vulnerabilities can be assessed in a realistic grey-box exploitation setting that leverages minimal vulnerability metadata, specifically a CWE classification and a vulnerable code location. We introduce Agentic eXploit Engine (AXE), a multi-agent framework for Web application exploitation that maps lightweight detection metadata to concrete exploits through decoupled planning, code exploration, and dynamic execution feedback. Evaluated on the CVE-Bench dataset, AXE achieves a 30% exploitation success rate, a 3x improvement over state-of-the-art black-box baselines. Even in a single-agent configuration, grey-box metadata yields a 1.75x performance gain. Systematic error analysis shows that most failed attempts arise from specific reasoning gaps, including misinterpreted vulnerability semantics and unmet execution preconditions. For successful exploits, AXE produces actionable, reproducible proof-of-concept artifacts, demonstrating its utility in streamlining Web vulnerability triage and remediation. We further evaluate AXE's generalizability through a case study on a recent real-world vulnerability not included in CVE-Bench.

</details>


### [97] [Differentially Private Retrieval-Augmented Generation](https://arxiv.org/abs/2602.14374)
*Tingting Tang,James Flemings,Yongqin Wang,Murali Annavaram*

Main category: cs.CR

TL;DR: DP-KSA是一种保护隐私的检索增强生成算法，通过差分隐私和关键词提取技术，在减少幻觉的同时保护敏感数据库中的隐私信息。


<details>
  <summary>Details</summary>
Motivation: 传统RAG框架在处理包含敏感信息（如医疗记录、法律文件）的数据库时存在隐私泄露风险，而直接应用差分隐私机制会导致系统效用显著下降，增加LLM产生幻觉的风险。

Method: 提出DP-KSA算法，采用"提议-测试-发布"范式集成差分隐私。首先获取相关上下文集合并生成多个LLM响应，然后以差分隐私方式提取最频繁的关键词，最后将这些关键词增强到提示中以生成最终输出。

Result: DP-KSA在三个指令调优LLM和两个QA基准测试中表现出色，实现了强大的隐私-效用权衡，同时为RAG数据库提供了形式化的差分隐私保证。

Conclusion: DP-KSA通过语义空间压缩有效解决了RAG系统中的隐私保护问题，在保持回答准确性的同时提供了形式化的隐私保证，为敏感领域应用提供了可行的解决方案。

Abstract: Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.

</details>


### [98] [LRD-MPC: Efficient MPC Inference through Low-rank Decomposition](https://arxiv.org/abs/2602.14397)
*Tingting Tang,Yongqin Wang,Murali Annavaram*

Main category: cs.CR

TL;DR: 该论文提出在安全多方计算（MPC）环境中，通过低秩分解（LRD）优化机器学习线性层计算，结合截断跳过和线性层级联优化，显著提升计算效率并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: MPC在机器学习应用中提供强大的隐私保护，但卷积层和全连接层等线性层的矩阵乘法计算成本高昂，导致显著的通信和计算开销。需要在不牺牲安全性的前提下优化MPC中的线性层计算效率。

Method: 1. 采用低秩分解（LRD）将大型矩阵乘法分解为两个较小矩阵乘法；2. 提出截断跳过优化，消除LRD引入的额外截断操作；3. 设计线性层级联优化，通过流水线操作隐藏额外通信轮次。

Result: 实验显示：在n-PC协议中实现高达25%的加速，在3-PC协议中实现33%加速；GPU能耗降低高达52%；离线阶段延迟减少高达88%。

Conclusion: 提出的低秩分解结合截断跳过和线性层级联优化，有效解决了MPC中LRD的通信和计算开销问题，显著提升了安全机器学习推理的整体效率，且方法具有广泛的MPC协议适用性。

Abstract: Secure Multi-party Computation (MPC) enables untrusted parties to jointly compute a function without revealing their inputs. Its application to machine learning (ML) has gained significant attention, particularly for secure inference services deployed across multiple cloud virtual machines (VMs), where each VM acts as an MPC party. Model providers secret-share model weights, and users secret-share inputs, ensuring that each server operates only on random shares. While MPC provides strong cryptographic guarantees, it incurs substantial computational and communication overhead. Deep neural networks rely heavily on convolutional and fully connected layers, which require costly matrix multiplications in MPC. To reduce this cost, we propose leveraging low-rank decomposition (LRD) for linear layers, replacing one large matrix multiplication with two smaller ones. Each matrix multiplication in MPC incurs a round of communication, meaning decomposing one matrix multiplication into two leads to an additional communication round. Second, the added matrix multiplication requires an additional truncation step to maintain numerical precision. Since truncation itself requires communication and computation, these overheads can offset the gains from decomposition. To address this, we introduce two complementary optimizations: truncation skipping and efficient linear layer concatenation. Truncation skipping removes the extra truncation induced by LRD, while linear layer concatenation pipelines operations to hide the additional communication round. Together, these techniques mitigate the main overheads of LRD in MPC and improve overall efficiency. Our approach is broadly applicable across MPC protocols. Experiments show up to 25% speedup in n-PC and 33% in 3-PC protocols over full-rank baselines, along with up to 52% GPU energy savings and 88% reduction in offline-phase latency.

</details>


### [99] [When Security Meets Usability: An Empirical Investigation of Post-Quantum Cryptography APIs](https://arxiv.org/abs/2602.14539)
*Marthin Toruan,R. D. N. Shakya,Samuel Tseitkin,Raymond K. Zhao,Nalin Arachchilage*

Main category: cs.CR

TL;DR: 该研究对后量子密码学API的可用性进行了实证评估，发现开发者在使用PQC原语时面临认知挑战，需要改进文档、术语和工作流示例来支持非专业开发者。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展威胁现有公钥密码系统的安全，后量子密码学成为新的安全标准，但PQC采用缓慢，主要原因是开发者专业知识有限。虽然API旨在弥合这一差距，但密码学API的可用性问题可能导致开发者在实现应用时引入漏洞，而PQC的新颖性和复杂性加剧了这一风险。

Method: 研究采用实证评估方法，观察开发者在软件开发任务中如何与PQC API和文档交互，识别影响开发者在使用PQC原语时性能的认知因素，研究在最小入门培训条件下进行。

Result: 研究发现PQC API的可用性存在挑战，开发者在理解和使用PQC原语时面临认知障碍。研究识别出影响开发者性能的具体认知因素，并揭示了PQC生态系统中需要改进的领域。

Conclusion: 研究强调需要在整个PQC生态系统中改进面向开发者的指导、术语对齐和工作流示例，以更好地支持非专业开发者采用后量子密码学，从而加速PQC的实施和部署。

Abstract: Advances in quantum computing increasingly threaten the security and privacy of data protected by current cryptosystems, particularly those relying on public-key cryptography. In response, the international cybersecurity community has prioritized the implementation of Post-Quantum Cryptography (PQC), a new cryptographic standard designed to resist quantum attacks while operating on classical computers. The National Institute of Standards and Technology (NIST) has already standardized several PQC algorithms and plans to deprecate classical asymmetric schemes, such as RSA and ECDSA, by 2035. Despite this urgency, PQC adoption remains slow, often due to limited developer expertise. Application Programming Interfaces (APIs) are intended to bridge this gap, yet prior research on classical security APIs demonstrates that poor usability of cryptographic APIs can lead developers to introduce vulnerabilities during implementation of the applications, a risk amplified by the novelty and complexity of PQC. To date, the usability of PQC APIs has not been systematically studied. This research presents an empirical evaluation of the usability of the PQC APIs, observing how developers interact with APIs and documentation during software development tasks. The study identifies cognitive factors that influence the developer's performance when working with PQC primitives with minimal onboarding. The findings highlight opportunities across the PQC ecosystem to improve developer-facing guidance, terminology alignment, and workflow examples to better support non-specialists.

</details>


### [100] [A New Approach in Cryptanalysis Through Combinatorial Equivalence of Cryptosystems](https://arxiv.org/abs/2602.14544)
*Jaagup Sepp,Eric Filiol*

Main category: cs.CR

TL;DR: 提出基于组合等价概念演化的密码分析新方法，通过将密码系统重写为组合等价形式来揭示能更好区分密钥的新特性，成功应用于现代最安全的流密码


<details>
  <summary>Details</summary>
Motivation: 现有密码分析方法在分析现代最安全的流密码时存在局限性，需要发展新的密码分析技术来突破现有方法的限制

Method: 基于组合等价概念的演化，将密码系统重写为组合等价形式，使新的区分密钥特性显现；首先定义概念密码Cipherbent6捕获流密码分析的主要难点，然后将方法应用于Achterbahn密码

Result: 显著优于所有已知的密码分析方法，对Achterbahn密码获得了远优于现有分析的结果

Conclusion: 基于组合等价演化的新密码分析方法对现代流密码具有显著优势，为密码分析领域提供了新的有效工具

Abstract: We propose a new approach in cryptanalysis based on an evolution of the concept of \textit{Combinatorial Equivalence}. The aim is to rewrite a cryptosystem under a combinatorially equivalent form in order to make appear new properties that are more strongly discriminating the secret key used during encryption. We successfully applied this approach to the most secure stream ciphers category nowadays. We first define a concept cipher called Cipherbent6 that capture most of the difficulty of stream cipher cryptanalysis. We significantly outperformed all known cryptanalysis. We applied this approach to the Achterbahn cipher and we obtained again far better cryptanalysis results.

</details>


### [101] [Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks](https://arxiv.org/abs/2602.14689)
*Lukas Struppek,Adam Gleave,Kellin Pelrine*

Main category: cs.CR

TL;DR: 预填充攻击是开源大语言模型的新安全漏洞，攻击者可在生成前预设初始响应token，研究发现该攻击对当前主流开源模型普遍有效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，其被滥用的风险也在增加。开源模型主要依赖内部防护机制，而现有安全研究多关注输入型越狱和参数级攻击，预填充这一原生功能的安全风险却被系统性地忽视了。

Method: 对预填充攻击进行了迄今最大规模的实证研究，评估了20多种现有和新颖的攻击策略，覆盖多个模型家族和最先进的开源权重模型。

Result: 预填充攻击对所有主要当代开源模型都持续有效，揭示了先前未被充分探索的关键漏洞。虽然某些大型推理模型对通用预填充有一定抵抗力，但仍容易受到针对特定模型的定制策略攻击。

Conclusion: 研究结果强调了模型开发者迫切需要优先考虑针对预填充攻击的防御措施，这对开源大语言模型的部署具有重要安全意义。

Abstract: As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.

</details>


### [102] [interID -- An Ecosystem-agnostic Verifier-as-a-Service with OpenID Connect Bridge](https://arxiv.org/abs/2602.14871)
*Hakan Yildiz,Axel Küpper*

Main category: cs.CR

TL;DR: interID平台通过OIDC桥接为SSI验证提供标准化服务，降低企业集成门槛，满足欧盟EUDI钱包合规要求


<details>
  <summary>Details</summary>
Motivation: 欧盟法规要求到2027年必须接受EUDI钱包，SSI成为合规必需。但不同SSI验证器使用不同的API、请求参数和响应格式，需要定制包装器和专用基础设施，这与OpenID Connect（OIDC）的标准化协议形成对比，后者可实现无缝集成。

Method: 扩展interID平台，添加OIDC桥接，提供验证器即服务（Verifier-as-a-Service）。通过标准OIDC流程实现SSI验证，支持PKCE、范围到证明模板映射（将OIDC范围转换为生态系统特定的验证请求），采用具有严格租户隔离的多租户Keycloak架构。

Result: 评估显示安全性与生产身份提供商相当，威胁建模识别出11个攻击向量，其中7个超出RFC 6819范围。集成分析表明企业采用SSI认证的工作量与添加传统联合提供商相当。

Conclusion: 通过将熟悉的OIDC模式与SaaS部署相结合，该工作降低了集成和运营障碍，使企业能够通过配置而非定制开发实现法规合规，促进SSI的广泛采用。

Abstract: Self-Sovereign Identity (SSI) enables user-controlled, cryptographically verifiable credentials. As EU regulations mandate EUDI Wallet acceptance by 2027, SSI adoption becomes a compliance necessity. However, each SSI Verifier exposes different APIs with distinct request parameters, response formats, and claim structures, requiring custom wrappers and dedicated infrastructure, contrasting with OpenID Connect (OIDC) where standardized protocols enable seamless integration.
  interID is an ecosystem-agnostic platform unifying credential verification across Hyperledger Aries/Indy, EBSI, and EUDI ecosystems. We extend interID with an OIDC bridge providing Verifier-as-a-Service, enabling SSI verification through standard OIDC flows. Organizations receive ID Tokens with verified credential attributes without implementing Verifier-specific logic or deploying infrastructure. The multi-tenant architecture leverages Keycloak with strict tenant isolation. Key innovations include PKCE support, scope-to-proof-template mappings translating OIDC scopes into ecosystem-specific verification requests, and a security analysis identifying novel attack surfaces at the intersection of OIDC, SSI, and multi-tenant architectures, threats covered by neither RFC 6819 nor existing SSI analyses alone.
  Our evaluation demonstrates security equivalence to production identity providers through threat modeling identifying 11 attack vectors, including seven beyond RFC 6819's scope. Integration analysis shows organizations can adopt SSI authentication with comparable effort to adding traditional federated providers. By combining familiar OIDC patterns with SaaS deployment, our work lowers integration and operational barriers, enabling regulatory compliance through configuration rather than custom development.

</details>
