<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 47]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [On the Dynamics of Observation and Semantics](https://arxiv.org/abs/2602.18494)
*Xiu Li*

Main category: cs.AI

TL;DR: 论文提出智能不是静态的潜在表征属性，而是有限资源智能体与高熵环境互动的产物。通过观察语义纤维束框架，证明热力学成本限制了内部状态转换复杂度，从而推导出符号结构的必要性——语言和逻辑是防止热崩溃的固态信息形式。


<details>
  <summary>Details</summary>
Motivation: 挑战当前视觉智能中将语义视为潜在表征静态属性的主流范式，认为这种观点在物理上不完整。主张智能应该是有限资源（有限内存、计算、能量）的智能体与高熵环境互动的属性。

Method: 提出观察语义纤维束的动力学结构框架，将原始感官观察数据（纤维）投影到低熵因果语义流形（基）。基于兰道尔原理证明有限智能体的信息处理热力学成本限制了内部状态转换复杂度（语义常数B）。从这些物理约束推导出符号结构的必要性。

Result: 证明为了在B界限内建模组合世界，语义流形必须经历相变，结晶为离散、组合、因子化的形式。语言和逻辑不是文化产物，而是防止热崩溃所需的固态信息形式。

Conclusion: 理解不是恢复隐藏的潜在变量，而是构建因果商，使世界在算法上可压缩且因果可预测。符号结构是有限资源智能体在热力学约束下的必然结果。

Abstract: A dominant paradigm in visual intelligence treats semantics as a static property of latent representations, assuming that meaning can be discovered through geometric proximity in high dimensional embedding spaces. In this work, we argue that this view is physically incomplete. We propose that intelligence is not a passive mirror of reality but a property of a physically realizable agent, a system bounded by finite memory, finite compute, and finite energy interacting with a high entropy environment. We formalize this interaction through the kinematic structure of an Observation Semantics Fiber Bundle, where raw sensory observation data (the fiber) is projected onto a low entropy causal semantic manifold (the base). We prove that for any bounded agent, the thermodynamic cost of information processing (Landauer's Principle) imposes a strict limit on the complexity of internal state transitions. We term this limit the Semantic Constant B. From these physical constraints, we derive the necessity of symbolic structure. We show that to model a combinatorial world within the bound B, the semantic manifold must undergo a phase transition, it must crystallize into a discrete, compositional, and factorized form. Thus, language and logic are not cultural artifacts but ontological necessities the solid state of information required to prevent thermal collapse. We conclude that understanding is not the recovery of a hidden latent variable, but the construction of a causal quotient that renders the world algorithmically compressible and causally predictable.

</details>


### [2] [Feedback-based Automated Verification in Vibe Coding of CAS Adaptation Built on Constraint Logic](https://arxiv.org/abs/2602.18607)
*Michal Töpfer,František Plášil,Tomáš Bureš,Petr Hnětynka*

Main category: cs.AI

TL;DR: 论文提出使用vibe coding反馈循环生成自适应管理器(AM)，结合新型时序逻辑FCL进行验证，在CAS领域实验中取得良好效果


<details>
  <summary>Details</summary>
Motivation: 在CAS自适应系统中，动态架构和行为变化的定义是一个挑战。随着生成式LLM的发展，基于系统规范和期望行为生成AM代码成为可能，但需要解决生成代码的正确性问题。vibe coding通过迭代测试和反馈循环提供了解决方案。

Method: 提出结合自适应反馈循环和vibe coding反馈循环的方法，使用新型时序逻辑FCL精确表达功能需求约束，通过LLM生成AM代码，然后基于当前系统状态评估FCL约束，通过反馈循环迭代改进。

Result: 在两个CAS领域示例系统的实验中，该方法取得了良好效果。通常只需要几次反馈循环迭代，每次向LLM提供详细约束违反报告。AM测试结合了不同初始设置实现的高运行路径覆盖率。

Conclusion: 通过vibe coding反馈循环生成AM是可行的，前提是基于FCL逻辑精确表达功能需求进行验证。结合自适应和vibe coding反馈循环的方法在实践中表现良好。

Abstract: In CAS adaptation, a challenge is to define the dynamic architecture of the system and changes in its behavior. Implementation-wise, this is projected into an adaptation mechanism, typically realized as an Adaptation Manager (AM). With the advances of generative LLMs, generating AM code based on system specification and desired AM behavior (partially in natural language) is a tempting opportunity. The recent introduction of vibe coding suggests a way to target the problem of the correctness of generated code by iterative testing and vibe coding feedback loops instead of direct code inspection.
  In this paper, we show that generating an AM via vibe coding feedback loops is a viable option when the verification of the generated AM is based on a very precise formulation of the functional requirements. We specify these as constraints in a novel temporal logic FCL that allows us to express the behavior of traces with much finer granularity than classical LTL enables.
  Furthermore, we show that by combining the adaptation and vibe coding feedback loops where the FCL constraints are evaluated for the current system state, we achieved good results in the experiments with generating AMs for two example systems from the CAS domain. Typically, just a few feedback loop iterations were necessary, each feeding the LLM with reports describing detailed violations of the constraints. This AM testing was combined with high run path coverage achieved by different initial settings.

</details>


### [3] [Spilled Energy in Large Language Models](https://arxiv.org/abs/2602.18671)
*Adrian Robert Minut,Hazem Dewidar,Iacopo Masi*

Main category: cs.AI

TL;DR: 将LLM的softmax分类器重新解释为能量模型，通过分析解码过程中的"能量溢出"来检测幻觉，无需额外训练


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法通常需要训练探针分类器或进行激活消融，计算成本高且缺乏理论依据。本文旨在开发一种无需训练、基于理论原理的幻觉检测方法

Method: 将序列到序列的概率链分解为多个相互作用的能量模型，提出两种完全无需训练的指标：能量溢出（捕捉连续生成步骤间能量值的差异）和边缘化能量（可在单步测量）

Result: 在9个基准测试和多个先进LLM（包括LLaMA、Mistral、Gemma）上评估，方法在幻觉检测和跨任务泛化方面表现稳健且有竞争力，对预训练和指令调优变体均有效

Conclusion: 基于能量模型的重新解释为幻觉检测提供了理论依据，提出的训练免费方法在多个模型和任务上表现优异，为LLM可靠性评估提供了实用工具

Abstract: We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track "energy spills" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.

</details>


### [4] [Many AI Analysts, One Dataset: Navigating the Agentic Data Science Multiverse](https://arxiv.org/abs/2602.18710)
*Martin Bertran,Riccardo Fogliato,Zhiwei Steven Wu*

Main category: cs.AI

TL;DR: 使用大型语言模型构建的自主AI分析师能够以低成本大规模重现结构化分析多样性，在不同模型和提示框架下对相同假设和数据集产生广泛分散的分析结果。


<details>
  <summary>Details</summary>
Motivation: 传统实证研究的结论不仅取决于数据，还取决于一系列很少在发表结果中明确说明的分析决策。过去的"多分析师"研究表明，独立团队对相同假设和数据集进行测试经常得出相互矛盾的结论，但这类研究需要数十个研究团队数月协调，因此很少进行。

Method: 构建基于大型语言模型的自主AI分析师，让它们在固定数据集上测试预定义假设，在不同模型和提示框架下进行重复运行。每个AI分析师独立构建和执行完整的分析流程，然后由AI审计员筛选方法学上有效的运行。

Result: 在三个涵盖实验和观察设计的数据集中，AI分析师产生的分析在效应大小、p值和二元决策（是否支持假设）上显示出广泛分散，经常逆转假设是否被判断为支持。这种分散是结构化的：在预处理、模型规范和推断方面的可识别分析选择在不同LLM和角色条件下存在系统性差异。

Conclusion: 分析结果是可引导的：重新分配分析师角色或LLM即使在排除方法学上存在缺陷的运行后，也会改变结果的分布。这表明AI分析师能够以低成本大规模重现结构化分析多样性。

Abstract: The conclusions of empirical research depend not only on data but on a sequence of analytic decisions that published results seldom make explicit. Past ``many-analyst" studies have demonstrated this: independent teams testing the same hypothesis on the same dataset regularly reach conflicting conclusions. But such studies require months of coordination among dozens of research groups and are therefore rarely conducted. In this work, we show that fully autonomous AI analysts built on large language models (LLMs) can reproduce a similar structured analytic diversity cheaply and at scale. We task these AI analysts with testing a pre-specified hypothesis on a fixed dataset, varying the underlying model and prompt framing across replicate runs. Each AI analyst independently constructs and executes a full analysis pipeline; an AI auditor then screens each run for methodological validity. Across three datasets spanning experimental and observational designs, AI analyst-produced analyses display wide dispersion in effect sizes, $p$-values, and binary decisions on supporting the hypothesis or not, frequently reversing whether a hypothesis is judged supported. This dispersion is structured: recognizable analytic choices in preprocessing, model specification, and inference differ systematically across LLM and persona conditions. Critically, the effects are \emph{steerable}: reassigning the analyst persona or LLM shifts the distribution of outcomes even after excluding methodologically deficient runs.

</details>


### [5] [Task-Aware Exploration via a Predictive Bisimulation Metric](https://arxiv.org/abs/2602.18724)
*Dayang Liang,Ruihan Liu,Lipeng Wan,Yunlong Liu,Bo An*

Main category: cs.AI

TL;DR: TEB提出了一种任务感知探索方法，通过预测双仿真度量将任务相关表示与探索紧密结合，解决了视觉强化学习中稀疏奖励下的探索问题。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习中稀疏奖励下的探索面临挑战，现有方法要么假设可以访问低维状态，要么缺乏任务感知的探索策略，在视觉领域中表现脆弱。需要弥合任务相关表示与探索之间的差距。

Method: TEB利用预测双仿真度量：1）学习行为基础的任务表示；2）在学习的潜在空间中测量行为内在新颖性。首先理论上缓解稀疏奖励下退化双仿真度量的表示崩溃问题，通过引入预测奖励差异。基于此稳健度量，设计基于潜在空间的探索奖励，测量相邻观测的相对新颖性。

Result: 在MetaWorld和Maze2D上的广泛实验表明，TEB实现了优越的探索能力，并超越了最近的基线方法。

Conclusion: TEB通过将任务相关表示与探索紧密结合，有效解决了视觉强化学习中稀疏奖励下的探索问题，展示了在复杂视觉领域中的优越性能。

Abstract: Accelerating exploration in visual reinforcement learning under sparse rewards remains challenging due to the substantial task-irrelevant variations. Despite advances in intrinsic exploration, many methods either assume access to low-dimensional states or lack task-aware exploration strategies, thereby rendering them fragile in visual domains. To bridge this gap, we present TEB, a Task-aware Exploration approach that tightly couples task-relevant representations with exploration through a predictive Bisimulation metric. Specifically, TEB leverages the metric not only to learn behaviorally grounded task representations but also to measure behaviorally intrinsic novelty over the learned latent space. To realize this, we first theoretically mitigate the representation collapse of degenerate bisimulation metrics under sparse rewards by internally introducing a simple but effective predicted reward differential. Building on this robust metric, we design potential-based exploration bonuses, which measure the relative novelty of adjacent observations over the latent space. Extensive experiments on MetaWorld and Maze2D show that TEB achieves superior exploration ability and outperforms recent baselines.

</details>


### [6] [Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization](https://arxiv.org/abs/2602.18731)
*Yuhang Bai,Yujuan Ding,Shanru Lin,Wenqi Fan*

Main category: cs.AI

TL;DR: 提出Chart Insight Agent Flow多智能体框架，利用MLLMs从图表图像中挖掘深层洞察，并创建ChartSummInsights数据集，显著提升图表摘要的洞察深度


<details>
  <summary>Details</summary>
Motivation: 现有图表摘要方法（包括MLLMs）主要关注低层数据描述，未能捕捉数据可视化的深层洞察本质，缺乏合适的基准数据集

Method: 提出Chart Insight Agent Flow：一个计划-执行多智能体框架，有效利用MLLMs的感知和推理能力，直接从图表图像中揭示深刻洞察；创建ChartSummInsights数据集，包含真实世界图表和专家撰写的高质量洞察摘要

Result: 实验结果表明，该方法显著提升了MLLMs在图表摘要任务上的性能，能够生成具有深度和多样性的洞察摘要

Conclusion: Chart Insight Agent Flow框架和ChartSummInsights数据集有效解决了现有图表摘要方法缺乏深层洞察的问题，为数据可访问性和信息高效消费提供了更好的解决方案

Abstract: Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the deeper insights which are the fundamental purpose of data visualization. To address this challenge, we propose Chart Insight Agent Flow, a plan-and-execute multi-agent framework effectively leveraging the perceptual and reasoning capabilities of MLLMs to uncover profound insights directly from chart images. Furthermore, to overcome the lack of suitable benchmarks, we introduce ChartSummInsights, a new dataset featuring a diverse collection of real-world charts paired with high-quality, insightful summaries authored by human data analysis experts. Experimental results demonstrate that our method significantly improves the performance of MLLMs on the chart summarization task, producing summaries with deep and diverse insights.

</details>


### [7] [Federated Reasoning Distillation Framework with Model Learnability-Aware Data Allocation](https://arxiv.org/abs/2602.18749)
*Wei Guo,Siyuan Lu,Xiangdong Ran,Yiqi Tong,Yikun Ban,Zelong Xu,Jing Fan,Zixuan Huang,Xiao Zhang,Zhaojun Hu,Fuzhen Zhuang*

Main category: cs.AI

TL;DR: LaDa是一个联邦推理蒸馏框架，通过模型可学习性感知的数据分配来解决联邦大语言模型和小语言模型协作中的双向可学习性差距和领域无关推理转移问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据分配方法未能解决联邦LLM和SLM协作中的两个关键挑战：1）双向模型可学习性差距——客户端SLM无法识别符合其可学习性约束的高奖励样本进行有效知识转移，而LLM难以选择能提供超出其现有数据的新知识的样本；2）领域无关推理转移——现有推理转移方法无法灵活适应本地领域数据，阻碍SLM从通用LLM有效获取逐步推理能力。

Method: 提出LaDa框架，包含两个核心组件：1）模型可学习性感知数据过滤器，根据每个SLM-LLM对之间的可学习性差距自适应分配高奖励样本；2）领域自适应推理蒸馏方法，通过在过滤的高奖励样本上对齐推理路径的联合概率，通过对比蒸馏学习在SLM和LLM之间进行，使SLM能够捕捉本地数据分布下的底层推理模式。

Result: LaDa作为现有协作框架的插件模块运行，能够基于模型可学习性差距自适应调整知识转移，有效促进双向知识转移和领域自适应推理能力获取。

Conclusion: LaDa框架成功解决了联邦LLM和SLM协作中的双向可学习性差距和领域无关推理转移问题，通过可学习性感知数据分配和领域自适应推理蒸馏，实现了更有效的知识转移和推理能力获取。

Abstract: Data allocation plays a critical role in federated large language model (LLM) and small language models (SLMs) reasoning collaboration. Nevertheless, existing data allocation methods fail to address an under-explored challenge in collaboration: bidirectional model learnability gap, where client-side SLMs cannot identify high-reward samples matching their learnability constraints for effective knowledge transfer from LLMs, while LLMs struggle to select samples contributing novel knowledge beyond their existing data. Furthermore, these collaboration frameworks face another key challenge: domain-agnostic reasoning transfer, where existing reasoning transfer methods fail to flexibly adapt to the local domain data, preventing SLMs from effectively acquiring step-by-step reasoning abilities within from general LLM. To address these challenges, we propose LaDa, a federated reasoning distillation framework with model learnability-aware data allocation. It introduces a model learnability-aware data filter that adaptively allocates high-reward samples based on the learnability gap between each SLM and LLM pair, effectively facilitating bidirectional knowledge transfer. We further design a domain adaptive reasoning distillation method that aligns joint probabilities of reasoning paths on filtered high-reward samples through contrastive distillation learning between SLM and LLM, enabling SLM to capture underlying reasoning patterns under local data distribution. LaDa operates as a plug-in module for existing collaboration frameworks, adapting knowledge transfer based on model learnability gaps.

</details>


### [8] [The Convergence of Schema-Guided Dialogue Systems and the Model Context Protocol](https://arxiv.org/abs/2602.18764)
*Andreas Schlapbach*

Main category: cs.AI

TL;DR: 该论文揭示了Schema-Guided Dialogue (SGD)和Model Context Protocol (MCP)本质上是同一范式的两种表现形式，提出了模式设计的五大基本原则，并指出了三个新颖见解。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是发现SGD和MCP这两种看似不同的框架实际上共享相同的核心范式，通过分析这种趋同现象，提取出可指导AI系统设计的通用原则，解决现有框架中的不足。

Method: 通过对比分析SGD和MCP两种框架，识别它们的共同点和差异，从中提取出模式设计的五个基本原则，并为每个原则提供具体的设计模式。

Result: 论文得出了三个关键见解：1) SGD的原始设计本质上是正确的，应该被MCP继承；2) 两种框架都未能充分利用故障模式和工具间关系；3) 渐进式披露是在实际令牌约束下实现生产规模扩展的关键洞察。

Conclusion: 模式驱动的治理是AI系统监督的可扩展机制，无需专有系统检查，这对于Software 3.0至关重要。提出的五大原则为构建确定性和可审计的LLM-agent交互提供了理论基础。

Abstract: This paper establishes a fundamental convergence: Schema-Guided Dialogue (SGD) and the Model Context Protocol (MCP) represent two manifestations of a unified paradigm for deterministic, auditable LLM-agent interaction. SGD, designed for dialogue-based API discovery (2019), and MCP, now the de facto standard for LLM-tool integration, share the same core insight -- that schemas can encode not just tool signatures but operational constraints and reasoning guidance. By analyzing this convergence, we extract five foundational principles for schema design: (1) Semantic Completeness over Syntactic Precision, (2) Explicit Action Boundaries, (3) Failure Mode Documentation, (4) Progressive Disclosure Compatibility, and (5) Inter-Tool Relationship Declaration. These principles reveal three novel insights: first, SGD's original design was fundamentally sound and should be inherited by MCP; second, both frameworks leave failure modes and inter-tool relationships unexploited -- gaps we identify and resolve; third, progressive disclosure emerges as a critical production-scaling insight under real-world token constraints. We provide concrete design patterns for each principle. These principles position schema-driven governance as a scalable mechanism for AI system oversight without requiring proprietary system inspection -- central to Software 3.0.

</details>


### [9] [GenPlanner: From Noise to Plans -- Emergent Reasoning in Flow Matching and Diffusion Models](https://arxiv.org/abs/2602.18812)
*Agnieszka Polowczyk,Alicja Polowczyk,Michał Wieczorek*

Main category: cs.AI

TL;DR: GenPlanner使用生成模型（扩散模型和流匹配）进行路径规划，在迷宫中通过多通道条件输入生成轨迹，性能优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中的路径规划是人工智能的关键问题，需要同时理解空间几何和全局结构。传统方法存在局限性，因此探索生成模型作为规划和推理机制的潜力。

Method: 提出GenPlanner方法，基于扩散模型和流匹配，包括DiffPlanner和FlowPlanner两个变体。使用多通道条件（障碍物地图、起点和终点信息）来约束轨迹生成，通过从随机噪声开始迭代生成正确路径。

Result: 实验表明，该方法显著优于基线CNN模型。特别是FlowPlanner即使在有限的生成步骤下也能表现出高性能。

Conclusion: 生成模型可以作为有效的路径规划和推理机制，在复杂环境中生成正确路径，为人工智能路径规划问题提供了新的解决方案。

Abstract: Path planning in complex environments is one of the key problems of artificial intelligence because it requires simultaneous understanding of the geometry of space and the global structure of the problem. In this paper, we explore the potential of using generative models as planning and reasoning mechanisms. We propose GenPlanner, an approach based on diffusion models and flow matching, along with two variants: DiffPlanner and FlowPlanner. We demonstrate the application of generative models to find and generate correct paths in mazes. A multi-channel condition describing the structure of the environment, including an obstacle map and information about the starting and destination points, is used to condition trajectory generation. Unlike standard methods, our models generate trajectories iteratively, starting with random noise and gradually transforming it into a correct solution. Experiments conducted show that the proposed approach significantly outperforms the baseline CNN model. In particular, FlowPlanner demonstrates high performance even with a limited number of generation steps.

</details>


### [10] [TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.18884)
*Zhenkun Gao,Xuhong Wang,Xin Tan,Yuan Xie*

Main category: cs.AI

TL;DR: TPRU是一个用于增强多模态大语言模型时序推理能力的大规模数据集，通过强化学习微调显著提升了小型可部署模型的性能，在TPRU-Test上准确率从50.33%提升到75.70%，超越了GPT-4o等更大模型。


<details>
  <summary>Details</summary>
Motivation: 当前可部署的小型多模态大语言模型在理解和处理时序、过程性视觉数据方面存在严重缺陷，这限制了它们在现实世界具身AI中的应用。主要原因是训练范式缺乏大规模、过程连贯的数据。

Method: 提出了TPRU数据集，该数据集从机器人操作和GUI导航等具身场景中收集，包含三个互补任务：时序重排、下一帧预测和前一帧回顾。采用强化学习微调方法，专门针对资源高效模型进行优化。

Result: TPRU-7B模型在TPRU-Test上的准确率从50.33%大幅提升到75.70%，达到了最先进水平，显著超越了包括GPT-4o在内的更大基线模型。这些能力还能有效泛化，在已有基准测试上也有显著改进。

Conclusion: TPRU数据集和强化学习微调方法有效解决了小型多模态大语言模型在时序推理方面的缺陷，为具身AI应用提供了重要支持，代码已开源。

Abstract: Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\% to 75.70\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ .

</details>


### [11] [Early Evidence of Vibe-Proving with Consumer LLMs: A Case Study on Spectral Region Characterization with ChatGPT-5.2 (Thinking)](https://arxiv.org/abs/2602.18918)
*Brecht Verbeken,Brando Vagenende,Marie-Anne Guerry,Andres Algaba,Vincent Ginis*

Main category: cs.AI

TL;DR: 该研究展示了使用消费级订阅LLM（ChatGPT-5.2）辅助数学研究的工作流程，通过解决Ran和Teng（2024）猜想20的具体案例，探讨了LLM在高级证明搜索中的有效性以及人类专家在关键验证环节的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型越来越多地被用作科学助手，但它们在研究级数学中的作用证据仍然有限，特别是对于个体研究人员可访问的工作流程。本研究旨在通过具体案例探索LLM在数学研究中的实际应用价值。

Method: 采用可审计的案例研究方法，使用ChatGPT-5.2（Thinking版本）解决Ran和Teng（2024）关于4周期行随机非负矩阵族非实谱区域精确范围的猜想20。分析了7个可共享的对话线程和4个版本化的证明草稿，记录了一个迭代的生成、评审和修复流程。

Result: 成功解决了猜想20，提供了必要和充分的区域条件以及明确的边界实现构造。研究发现LLM在高级证明搜索中最有用，而人类专家在正确性关键验证环节仍然必不可少。

Conclusion: 除了数学结果外，本研究贡献了对LLM辅助在何处实质性帮助以及在何处验证瓶颈仍然存在的流程级特征描述，对评估AI辅助研究工作流程和设计人在循环定理证明系统具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly used as scientific copilots, but evidence on their role in research-level mathematics remains limited, especially for workflows accessible to individual researchers. We present early evidence for vibe-proving with a consumer subscription LLM through an auditable case study that resolves Conjecture 20 of Ran and Teng (2024) on the exact nonreal spectral region of a 4-cycle row-stochastic nonnegative matrix family. We analyze seven shareable ChatGPT-5.2 (Thinking) threads and four versioned proof drafts, documenting an iterative pipeline of generate, referee, and repair. The model is most useful for high-level proof search, while human experts remain essential for correctness-critical closure. The final theorem provides necessary and sufficient region conditions and explicit boundary attainment constructions. Beyond the mathematical result, we contribute a process-level characterization of where LLM assistance materially helps and where verification bottlenecks persist, with implications for evaluation of AI-assisted research workflows and for designing human-in-the-loop theorem proving systems.

</details>


### [12] [High Dimensional Procedural Content Generation](https://arxiv.org/abs/2602.18943)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 论文提出高维程序化内容生成（HDPCG）框架，将非几何游戏机制提升为联合状态空间的一等坐标，实现超越纯几何的游戏关卡生成。


<details>
  <summary>Details</summary>
Motivation: 现有PCG方法主要关注静态2D/3D几何形状，将游戏机制作为辅助元素处理，这限制了可控性和表达能力。需要将非几何游戏维度提升为一级坐标。

Method: 提出HDPCG框架，通过两个具体方向实例化：1)方向-空间：用离散层维度增强几何，在4D(x,y,z,l)中验证可达性；2)方向-时间：通过时间扩展图添加时间动态。为每个方向提供三个通用算法，共享抽象骨架生成、受控接地、高维验证和多指标评估的流程。

Result: 大规模实验验证了问题表述的完整性，方法在可玩性、结构、风格、鲁棒性和效率方面表现有效。Unity案例研究创建了符合度量的可玩场景。

Conclusion: HDPCG鼓励PCG向通用表示转变，生成超越几何的游戏相关维度，为可控、可验证和可扩展的关卡生成铺平道路。

Abstract: Procedural content generation (PCG) has made substantial progress in shaping static 2D/3D geometry, while most methods treat gameplay mechanics as auxiliary and optimize only over space. We argue that this limits controllability and expressivity, and formally introduce High-Dimensional PCG (HDPCG): a framework that elevates non-geometric gameplay dimensions to first-class coordinates of a joint state space. We instantiate HDPCG along two concrete directions. Direction-Space augments geometry with a discrete layer dimension and validates reachability in 4D (x,y,z,l), enabling unified treatment of 2.5D/3.5D mechanics such as gravity inversion and parallel-world switching. Direction-Time augments geometry with temporal dynamics via time-expanded graphs, capturing action semantics and conflict rules. For each direction, we present three general, practicable algorithms with a shared pipeline of abstract skeleton generation, controlled grounding, high-dimensional validation, and multi-metric evaluation. Large-scale experiments across diverse settings validate the integrity of our problem formulation and the effectiveness of our methods on playability, structure, style, robustness, and efficiency. Beyond quantitative results, Unity-based case studies recreate playable scenarios that accord with our metrics. We hope HDPCG encourages a shift in PCG toward general representations and the generation of gameplay-relevant dimensions beyond geometry, paving the way for controllable, verifiable, and extensible level generation.

</details>


### [13] [INDUCTION: Finite-Structure Concept Synthesis in First-Order Logic](https://arxiv.org/abs/2602.18956)
*Serafim Batzoglou*

Main category: cs.AI

TL;DR: INDUCTION是一个用于一阶逻辑中有限结构概念合成的基准测试，通过给定带有扩展标记目标谓词的小型有限关系世界，要求模型输出能够统一解释所有世界中目标的单一逻辑公式，并通过精确模型检查验证正确性。


<details>
  <summary>Details</summary>
Motivation: 研究一阶逻辑中概念合成的能力，评估模型在有限关系世界中学习统一概念的能力，探索不同模型在概念泛化策略上的差异。

Method: 创建INDUCTION基准，包含三种机制：FullObs（完全观察）、CI（对比性）和EC（存在性完成），惩罚公式膨胀，通过精确模型检查验证公式正确性。

Result: 发现明显的难度梯度，存在持续困难的结构族，低膨胀公式在保留世界中泛化能力显著更好，顶尖模型在不同任务和性能指标上表现出质的不同行为。

Conclusion: INDUCTION基准揭示了概念合成的难度模式和泛化特性，不同模型在概念泛化策略上存在显著差异，低公式膨胀与更好的泛化性能相关。

Abstract: We introduce INDUCTION, a benchmark for finite structure concept synthesis in first order logic. Given small finite relational worlds with extensionally labeled target predicates, models must output a single first order logical formula that explains the target uniformly across worlds, with correctness verified via exact model checking. The benchmark includes three regimes, FullObs, CI (contrastive), and EC (existential completion), nd penalizes formula bloat. We find sharp difficulty gradients, persistent hard structural families, and observe that low bloat formulas generalize far better on held out worlds. Elite recent models show qualitatively different behaviors across tasks and performance metrics, hinting to their different strategies of concept generalization.

</details>


### [14] [Modularity is the Bedrock of Natural and Artificial Intelligence](https://arxiv.org/abs/2602.18960)
*Alessandro Salatiello*

Main category: cs.AI

TL;DR: 本文综述了模块化在人工智能和神经科学中的核心作用，认为模块化是实现高效学习和强泛化能力的关键原则，能够帮助弥合自然智能与人工智能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统依赖远超人类智能所需的数据、计算和能源资源，这种差异表明需要新的指导原则。大脑计算的模块化组织原则显示出支持高效学习和强泛化的能力，但模块化在主流AI研究中仍相对被低估。

Method: 通过概念框架回顾人工智能和神经科学中的多个研究线索，分析模块化提供的计算优势，考察模块化如何在不同AI研究领域作为解决方案出现，探讨大脑利用的模块化原则。

Result: 模块化在支持自然和人工智能方面发挥核心作用，它提供了计算优势，在多个AI研究领域作为解决方案出现，大脑利用特定的模块化原则，模块化有助于弥合自然与人工智能之间的差距。

Conclusion: 模块化是实现高效学习和强泛化能力的关键组织原则，在人工智能和神经科学中具有核心重要性。通过借鉴大脑的模块化原则，可以开发更高效、更通用的AI系统，缩小自然智能与人工智能之间的差距。

Abstract: The remarkable performance of modern AI systems has been driven by unprecedented scales of data, computation, and energy -- far exceeding the resources required by human intelligence. This disparity highlights the need for new guiding principles and motivates drawing inspiration from the fundamental organizational principles of brain computation. Among these principles, modularity has been shown to be critical for supporting the efficient learning and strong generalization abilities consistently exhibited by humans. Furthermore, modularity aligns well with the No Free Lunch Theorem, which highlights the need for problem-specific inductive biases and motivates architectures composed of specialized components that solve subproblems. However, despite its fundamental role in natural intelligence and its demonstrated benefits across a range of seemingly disparate AI subfields, modularity remains relatively underappreciated in mainstream AI research. In this work, we review several research threads in artificial intelligence and neuroscience through a conceptual framework that highlights the central role of modularity in supporting both artificial and natural intelligence. In particular, we examine what computational advantages modularity provides, how it has emerged as a solution across several AI research areas, which modularity principles the brain exploits, and how modularity can help bridge the gap between natural and artificial intelligence.

</details>


### [15] [Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction](https://arxiv.org/abs/2602.18968)
*Tao Zhe,Haoyu Wang,Bo Luo,Min Wu,Wei Fan,Xiao Luo,Zijun Yao,Haifeng Chen,Dongjie Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于分层执行结构的工具编排方法，通过粗粒度的层结构提供全局指导，结合模式感知的反射校正机制处理执行时错误，实现轻量级且可重用的工具编排组件。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用方法将工具执行与逐步语言推理或显式规划紧密耦合，导致脆弱行为和较高的执行开销。需要从工具编排的角度重新审视工具调用，解决多工具组织和执行中的失败问题。

Method: 将工具编排建模为学习分层执行结构，捕捉高层工具依赖关系，通过上下文约束诱导分层执行。引入模式感知的反射校正机制，在本地检测和修复执行时错误，避免重新规划整个执行轨迹。

Result: 实验结果表明，该方法实现了鲁棒的工具执行，同时减少了执行复杂性和开销。代码将公开提供。

Conclusion: 有效的工具编排不需要精确的依赖图或细粒度规划，粗粒度的层结构结合本地错误校正机制足以提供全局指导，这种结构化执行范式为智能体系统提供了轻量级且可重用的编排组件。

Abstract: Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.

</details>


### [16] [How Far Can We Go with Pixels Alone? A Pilot Study on Screen-Only Navigation in Commercial 3D ARPGs](https://arxiv.org/abs/2602.18981)
*Kaijie Xu,Mustafa Bugti,Clark Verbrugge*

Main category: cs.AI

TL;DR: 该研究基于视觉可供性检测器开发了一个仅依赖视觉输入的探索导航智能体，用于在复杂3D游戏关卡中进行导航，但发现纯视觉模型在真实游戏环境中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么在简化环境中模拟游戏，要么分析静态截图的视觉可供性，都无法真实反映玩家在复杂游戏关卡中的探索行为，因此需要开发更真实的视觉导航评估方法。

Method: 基于开源视觉可供性检测器，构建了一个仅依赖视觉输入的探索导航智能体，该智能体接收实时游戏画面，识别显著兴趣点，并通过有限状态控制器在最小动作空间中操作，用于探索《黑暗之魂》风格的线性关卡。

Result: 实验表明智能体能够穿越大部分必要区域并展现出有意义的视觉导航行为，但底层视觉模型的局限性阻碍了真正全面可靠的自动导航实现。

Conclusion: 该系统为复杂游戏中的视觉导航提供了具体的基准和评估协议，但纯视觉模型在理想化设置中虽能有效支持导航，却不太可能成为通用解决方案，需要更多关注这一必要任务。

Abstract: Modern 3D game levels rely heavily on visual guidance, yet the navigability of level layouts remains difficult to quantify. Prior work either simulates play in simplified environments or analyzes static screenshots for visual affordances, but neither setting faithfully captures how players explore complex, real-world game levels. In this paper, we build on an existing open-source visual affordance detector and instantiate a screen-only exploration and navigation agent that operates purely from visual affordances. Our agent consumes live game frames, identifies salient interest points, and drives a simple finite-state controller over a minimal action space to explore Dark Souls-style linear levels and attempt to reach expected goal regions. Pilot experiments show that the agent can traverse most required segments and exhibits meaningful visual navigation behavior, but also highlight that limitations of the underlying visual model prevent truly comprehensive and reliable auto-navigation. We argue that this system provides a concrete, shared baseline and evaluation protocol for visual navigation in complex games, and we call for more attention to this necessary task. Our results suggest that purely vision-based sense-making models, with discrete single-modality inputs and without explicit reasoning, can effectively support navigation and environment understanding in idealized settings, but are unlikely to be a general solution on their own.

</details>


### [17] [Benchmark Test-Time Scaling of General LLM Agents](https://arxiv.org/abs/2602.18998)
*Xiaochuan Li,Ryan Ming,Pranav Setlur,Abhijay Paladugu,Andy Tang,Hao Kang,Shuai Shao,Rong Jin,Chenyan Xiong*

Main category: cs.AI

TL;DR: General AgentBench是一个用于评估通用LLM智能体的统一基准，涵盖搜索、编码、推理和工具使用等多个领域。研究发现，从领域特定评估转向通用智能体设置时性能显著下降，且顺序扩展和并行扩展两种方法在实践中都无法有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注领域特定环境，用于开发专用智能体，但评估通用智能体需要更现实的设置，挑战智能体在统一环境中跨多个技能和工具操作的能力。

Method: 引入General AgentBench，这是一个统一的基准框架，用于评估通用LLM智能体在搜索、编码、推理和工具使用等领域的表现。使用该基准系统研究顺序扩展（迭代交互）和并行扩展（采样多个轨迹）下的测试时扩展行为。

Result: 评估十个领先的LLM智能体显示，从领域特定评估转向通用智能体设置时性能显著下降。研究发现两种扩展方法都无法在实践中有效提升性能，主要受两个根本限制：顺序扩展中的上下文上限和并行扩展中的验证差距。

Conclusion: 通用LLM智能体评估需要更现实的基准设置，当前智能体在跨领域操作方面存在显著性能差距。顺序扩展和并行扩展方法都面临根本性限制，需要新的方法来提升通用智能体的实际性能。

Abstract: LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.

</details>


### [18] [MagicAgent: Towards Generalized Agent Planning](https://arxiv.org/abs/2602.19000)
*Xuhui Ren,Shaokang Dong,Chen Yang,Qing Gao,Yunbin Zhao,Yongsheng Liu,Xinwei Geng,Xiang Li,Demei Yan,Yanqing Li,Chenhao Huang,Dingwei Zhu,Junjie Ye,Boxuan Yue,Yingnan Fu,Mengzhe Lv,Zezeng Feng,Boshen Zhou,Bocheng Wang,Xuanjing Huang,Yu-Gang Jiang,Tao Gui,Qi Zhang,Yunke Zhang*

Main category: cs.AI

TL;DR: MagicAgent是一个专门为通用智能体规划设计的系列基础模型，通过轻量级合成数据框架和两阶段训练范式，在多个规划基准测试中超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从被动文本处理器演变为自主智能体，规划成为现代智能的核心组件。然而，实现通用规划仍然困难，主要因为高质量交互数据稀缺以及异构规划任务之间的内在冲突，导致模型在孤立任务上表现出色但泛化能力差，现有多任务训练尝试也受到梯度干扰的影响。

Method: 提出了MagicAgent系列基础模型，采用轻量级可扩展的合成数据框架生成多样化规划任务的高质量轨迹，包括分层任务分解、工具增强规划、多约束调度、程序逻辑编排和长时程工具执行。为缓解训练冲突，提出了两阶段训练范式：先进行监督微调，然后在静态数据集和动态环境上进行多目标强化学习。

Result: MagicAgent-32B和MagicAgent-30B-A3B在多个基准测试中表现优异：Worfbench准确率75.1%、NaturalPlan 55.9%、τ²-Bench 57.5%、BFCL-v3 86.9%、ACEBench 81.2%，在内部MagicEval基准测试中也取得强劲结果。这些结果显著优于现有的100B以下参数模型，甚至超越了领先的闭源模型。

Conclusion: MagicAgent通过创新的合成数据框架和两阶段训练方法，成功解决了通用智能体规划中的数据稀缺和训练冲突问题，在多个规划基准测试中取得了最先进的性能，为通用智能体规划提供了有效的解决方案。

Abstract: The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\%$ on Worfbench, $55.9\%$ on NaturalPlan, $57.5\%$ on $τ^2$-Bench, $86.9\%$ on BFCL-v3, and $81.2\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.

</details>


### [19] [Evaluating Large Language Models on Quantum Mechanics: A Comparative Study Across Diverse Models and Tasks](https://arxiv.org/abs/2602.19006)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: 该研究系统评估了15个大语言模型在量子力学问题解决上的表现，发现模型性能存在明显的层级差异，旗舰模型表现最佳，数值计算是最具挑战性的任务，工具增强效果存在显著异质性。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在量子力学这一复杂科学领域的实际能力，为模型选择和应用提供实证依据，同时分析工具增强和可重复性等实际应用中的重要问题。

Method: 对15个来自5个提供商的大语言模型进行系统评估，涵盖3个能力层级，在20个任务上进行测试，包括推导、创意问题、非标准概念和数值计算，共进行900个基线评估和75个工具增强评估。

Result: 结果显示明显的层级分层：旗舰模型平均准确率81%，优于中端模型（77%）和快速模型（67%）。推导任务表现最佳（92%平均，旗舰模型达100%），数值计算最具挑战性（42%）。工具增强在数值任务上效果各异，总体提升4.4个百分点但代价是3倍token成本，效果从+29pp到-16pp不等。可重复性分析显示平均方差6.3pp，旗舰模型稳定性极佳（GPT-5方差为零）。

Conclusion: 该研究贡献包括：量子力学基准测试与自动验证系统、量化层级性能差异的系统评估、工具增强权衡的实证分析、可重复性特征描述。所有任务、验证器和结果均已公开。

Abstract: We present a systematic evaluation of large language models on quantum mechanics problem-solving. Our study evaluates 15 models from five providers (OpenAI, Anthropic, Google, Alibaba, DeepSeek) spanning three capability tiers on 20 tasks covering derivations, creative problems, non-standard concepts, and numerical computation, comprising 900 baseline and 75 tool-augmented assessments. Results reveal clear tier stratification: flagship models achieve 81\% average accuracy, outperforming mid-tier (77\%) and fast models (67\%) by 4pp and 14pp respectively. Task difficulty patterns emerge distinctly: derivations show highest performance (92\% average, 100\% for flagship models), while numerical computation remains most challenging (42\%). Tool augmentation on numerical tasks yields task-dependent effects: modest overall improvement (+4.4pp) at 3x token cost masks dramatic heterogeneity ranging from +29pp gains to -16pp degradation. Reproducibility analysis across three runs quantifies 6.3pp average variance, with flagship models demonstrating exceptional stability (GPT-5 achieves zero variance) while specialized models require multi-run evaluation. This work contributes: (i) a benchmark for quantum mechanics with automatic verification, (ii) systematic evaluation quantifying tier-based performance hierarchies, (iii) empirical analysis of tool augmentation trade-offs, and (iv) reproducibility characterization. All tasks, verifiers, and results are publicly released.

</details>


### [20] [Asking the Right Questions: Improving Reasoning with Generated Stepping Stones](https://arxiv.org/abs/2602.19069)
*Hengyuan Hu,Tingchen Fu,Minqi Jiang,Alexander H Miller,Yoram Bachrach,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 论文提出ARQ框架，通过生成中间问题（垫脚石）来帮助LLMs解决复杂推理任务，证明好问题存在且可迁移，并能通过微调提升问题生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs开始应用于更复杂的任务，需要关注它们构建中间步骤（垫脚石）的能力，如简化、重构或子问题，以更好地解决单次无法完成的任务。

Method: 提出ARQ框架，在标准推理流程中引入问题生成器，研究垫脚石的性质和效益。通过实验证明好问题的存在和可迁移性，并将垫脚石生成作为后训练任务，使用SFT和RL在合成数据上微调LLMs。

Result: 研究表明：1）好问题确实存在且可迁移，能显著帮助不同能力的LLMs解决目标任务；2）通过微调可以训练LLMs生成更有用的垫脚石问题。

Conclusion: 中间垫脚石问题对LLMs解决复杂推理任务至关重要，ARQ框架能有效生成和利用这些问题，通过微调可进一步提升问题生成质量，为LLMs处理更困难任务提供了新思路。

Abstract: Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\textbf{A}king the \textbf{R}ight \textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data.

</details>


### [21] [Defining Explainable AI for Requirements Analysis](https://arxiv.org/abs/2602.19071)
*Raymond Sheh,Isaac Monteath*

Main category: cs.AI

TL;DR: 本文提出了一个三维框架（来源、深度、范围）来分类不同应用对AI可解释性的需求，并探讨如何将这些需求与机器学习技术的解释能力相匹配。


<details>
  <summary>Details</summary>
Motivation: 随着可解释人工智能（XAI）的兴起，AI系统不仅需要表现出良好的决策性能，还需要解释其决策过程以获得信任。然而，不同应用对解释信息的需求各不相同，如何定义这些需求成为一个关键问题。

Method: 提出了三个维度来分类不同应用的解释需求：1）来源（解释来自模型内部还是外部），2）深度（解释的详细程度），3）范围（解释覆盖的决策范围）。重点关注如何将应用的解释需求与机器学习技术的解释能力相匹配。

Result: 建立了一个三维框架来系统化地分析不同应用场景对AI可解释性的具体要求，为选择合适的解释方法提供了结构化指导。

Conclusion: 通过来源、深度和范围三个维度，可以更精确地定义不同应用对AI解释的需求，从而更好地匹配机器学习技术的解释能力与具体应用场景的要求，促进可信AI的发展。

Abstract: Explainable Artificial Intelligence (XAI) has become popular in the last few years. The Artificial Intelligence (AI) community in general, and the Machine Learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements?
  In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.

</details>


### [22] [Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions](https://arxiv.org/abs/2602.19109)
*Yao Yan*

Main category: cs.AI

TL;DR: 该研究分析了Meta-Llama-3-8B模型在三位数加法任务中的工作机制，发现在第17层附近存在一个关键边界，之后模型主要依赖最后一个输入token进行解码，跨token路由变得无关紧要。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型在执行算术运算时的内部工作机制，特别是在跨token路由变得因果无关后，模型如何最终确定算术答案。通过分析三位数加法任务，探索模型在解码阶段的信息处理机制。

Method: 研究方法包括：1）使用单token读取机制分析算术答案的最终确定过程；2）应用因果残差修补技术；3）进行累积注意力消融实验；4）构建数字（和）方向字典；5）使用低秩Procrustes对齐分析；6）进行因果数字编辑实验。

Result: 主要发现：1）在第17层附近存在一个清晰的边界，超过该边界后解码的和几乎完全由最后一个输入token控制；2）后期自注意力在很大程度上是可舍弃的；3）数字（和）方向字典随更高位数上下文变化，但通过近似正交映射在共享低秩子空间中良好相关；4）因果数字编辑实验验证了几何结构。

Conclusion: 研究结论表明，在跨token路由变得因果无关后，模型通过一个共享的低秩子空间和近似正交映射来处理不同上下文下的数字表示。这种几何结构使得模型能够进行严格的因果编辑，而简单的跨上下文转移则会失败。

Abstract: We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how
  arithmetic answers are finalized after cross-token routing becomes causally irrelevant.
  Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:
  beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention
  is largely dispensable.
  In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are
  well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).
  Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the
  learned map restores strict counterfactual edits; negative controls do not recover.

</details>


### [23] [K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model](https://arxiv.org/abs/2602.19128)
*Shiyi Cao,Ziming Mao,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: K-Search通过共同演化的世界模型指导LLM进行GPU内核优化，显著超越传统进化方法，在复杂内核上实现2.10倍平均性能提升


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的GPU内核优化方法将LLM仅视为随机代码生成器，缺乏显式规划能力，难以处理需要协调多步结构变换的复杂内核，且常因低效或错误的中间实现而丢弃有前景的策略

Method: 提出基于共同演化世界模型的K-Search框架，用动态演化的世界模型替代静态搜索启发式，利用LLM的先验领域知识指导搜索，显式解耦高层算法规划与底层程序实例化

Result: 在FlashInfer的GQA、MLA和MoE等复杂内核上，K-Search显著优于最先进的进化搜索方法，平均提升2.10倍，在复杂MoE内核上最高达14.3倍；在GPUMode TriMul任务上，在H100上达到1030us，超越先前进化和人工设计解决方案

Conclusion: 通过共同演化世界模型指导LLM进行GPU内核优化，能够有效处理复杂内核的协调多步变换，在非单调优化路径中保持韧性，显著提升优化效果

Abstract: Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.

</details>


### [24] [Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians](https://arxiv.org/abs/2602.19141)
*Kartik Chandra,Max Kleiman-Weiner,Jonathan Ragan-Kelley,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 该研究通过贝叶斯建模探讨了AI谄媚性与AI诱发精神病之间的因果关系，发现即使理想化的贝叶斯理性用户也容易陷入妄想螺旋，且两种缓解措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究"AI精神病"或"妄想螺旋"现象，即用户在与AI聊天机器人长时间对话后对荒谬信念产生危险自信。这种现象通常归因于AI聊天机器人验证用户主张的倾向（谄媚性），但两者之间的因果关系尚未明确。

Method: 研究方法包括：1) 提出一个简单的贝叶斯模型，模拟用户与聊天机器人的对话；2) 在该模型中形式化谄媚性和妄想螺旋的概念；3) 通过建模和仿真分析谄媚性对妄想螺旋的因果作用；4) 测试两种缓解措施的效果：防止聊天机器人产生虚假主张，以及告知用户模型可能存在谄媚性。

Result: 研究结果显示：1) 即使理想化的贝叶斯理性用户也容易陷入妄想螺旋；2) 谄媚性在妄想螺旋中确实起到因果作用；3) 两种候选缓解措施（防止AI产生虚假主张和告知用户谄媚性可能性）在面对妄想螺旋时效果有限。

Conclusion: 研究结论强调AI谄媚性与妄想螺旋之间的因果关系，表明现有缓解措施不足以解决这一问题。这对关注减轻妄想螺旋问题的模型开发者和政策制定者具有重要意义，需要开发更有效的干预策略。

Abstract: "AI psychosis" or "delusional spiraling" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called "sycophancy." In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.

</details>


### [25] [DoAtlas-1: A Causal Compilation Paradigm for Clinical AI](https://arxiv.org/abs/2602.19158)
*Yulong Li,Jianxu Chen,Xiwei Liu,Chuanyue Suo,Rong Xia,Zhixiang Lu,Yichen Li,Xinlin Zhuang,Niranjana Arun Menon,Yutong Xie,Eran Segal,Imran Razzak*

Main category: cs.AI

TL;DR: 提出因果编译范式，将医学证据从叙述性文本转化为可执行代码，支持六种因果查询，并在DoAtlas-1中实现，编译了1,445个效应核，达到98.5%标准化准确率和80.5%查询可执行性。


<details>
  <summary>Details</summary>
Motivation: 医学基础模型能生成叙述性解释，但无法量化干预效果、检测证据冲突或验证文献主张，限制了临床可审计性。需要从文本生成转向可执行、可审计的因果推理。

Method: 提出因果编译范式，将异质性研究证据标准化为结构化估计对象，每个对象明确指定干预对比、效应尺度、时间范围和目标人群。通过效应标准化、冲突感知图构建和真实世界验证（人类表型项目，10,000名参与者）实现。

Result: 在DoAtlas-1中编译了1,445个效应核，来自754项研究。系统达到98.5%标准化准确率和80.5%查询可执行性，支持六种因果查询：do-演算、反事实推理、时间轨迹、异质性效应、机制分解和联合干预。

Conclusion: 因果编译范式将医学AI从文本生成转向可执行、可审计、可验证的因果推理，提高了医学证据的实用性和可靠性。

Abstract: Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning.

</details>


### [26] [Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM](https://arxiv.org/abs/2602.19159)
*Francesca Bianco,Derek Shiller*

Main category: cs.AI

TL;DR: 该研究通过机制可解释性方法探究了LLM中情感相关信息的表征和因果作用，发现情感符号在早期层即可线性分离，强度信息在中后期层可解码，晚期注意力输出对决策有最大因果影响。


<details>
  <summary>Details</summary>
Motivation: 先前行为研究表明LLM在选项被框定为痛苦或快乐时会改变选择，且这种偏差会随强度陈述而变化。研究旨在连接行为证据（模型做什么）与机制可解释性（支持它的计算），探究情感相关信息在transformer内部如何表征及因果作用位置。

Method: 使用Gemma-2-9B-it和基于先前工作的简约决策任务，进行：(1)跨流层的层间线性探测映射表征可用性；(2)通过激活干预（引导；修补/消融）测试因果贡献；(3)在epsilon网格上量化剂量-响应效应，读取2-3对数边际和数字对归一化选择概率。

Result: 发现：(a)情感符号（痛苦vs快乐）从非常早期层（L0-L1）开始在不同流族中完美线性可分；(b)分级强度在中后期层（特别是注意力/MLP输出）高度可解码，决策对齐在最终token前最高；(c)沿数据推导情感方向的加性引导在晚期位点因果调节2-3边际，最大效应在晚期层注意力输出（attn_out L14）；(d)头级修补/消融表明这些效应分布在多个头而非集中于单个单元。

Conclusion: 这些结果将行为敏感性连接到可识别的内部表征和干预敏感位点，为更严格的反事实测试和更广泛的复制提供了具体的机制目标。支持在AI感知与福利辩论以及政策制定、审计标准和安全保障设置方面进行更证据驱动的讨论和治理。

Abstract: Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.

</details>


### [27] [Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing](https://arxiv.org/abs/2602.19160)
*Maciej Świechowski,Adam Żychowski,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 本文从新颖视角评估大语言模型在形式化规则环境中的推理能力，通过通用游戏平台测试四个LLM在多步状态预测和合法行动生成等任务上的表现，分析游戏结构特征与模型性能的相关性，并考察语言语义和训练数据暴露的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在形式化、规则驱动的环境中的推理能力，这是评估模型逻辑推理和符号处理能力的重要方向。通过通用游戏实例作为测试平台，可以系统地评估模型在结构化问题解决中的表现。

Method: 使用四个LLM（Gemini 2.5 Pro/Flash、Llama 3.3 70B、GPT-OSS 120B）在通用游戏平台的一系列前向模拟任务上进行评估，包括单步/多步状态预测和合法行动生成。分析40个游戏结构特征与模型性能的相关性，并设计游戏定义混淆实验来考察语言语义和训练数据暴露的影响。

Result: 三个评估模型在大多数实验设置中表现良好，但随着评估步数增加（游戏步骤增多），性能会下降。详细案例分析揭示了模型在逻辑问题解决中的常见推理错误，包括规则幻觉、冗余状态事实和语法错误。游戏结构特征与模型性能存在相关性。

Conclusion: 当代大语言模型在形式化推理能力方面取得了明显进步，但仍存在随着推理步数增加性能下降的问题。研究为理解LLM在结构化、规则驱动环境中的推理能力提供了新见解，并识别了常见的推理错误模式。

Abstract: This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.

</details>


### [28] [Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering](https://arxiv.org/abs/2602.19240)
*Sen Zhao,Lincheng Zhou,Yue Chen,Ding Zou*

Main category: cs.AI

TL;DR: TopoRAG：一种用于文本图问答的新型框架，通过将文本图提升为胞腔复形来捕捉高维拓扑结构，解决现有RAG方法忽略循环结构的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于文本图的RAG方法主要关注低维结构（节点作为0维实体，边或路径作为1维关系），但忽略了循环结构，而循环在关系环路推理中至关重要。这种限制导致上下文基础不完整和推理能力受限。

Method: 1. 将文本图提升为胞腔复形以建模多维拓扑结构；2. 提出拓扑感知的子复形检索机制，提取与查询相关的胞腔复形；3. 设计多维拓扑推理机制，在这些复形上传播关系信息并指导LLM进行结构化、逻辑感知的推理。

Result: 实证评估表明，该方法在各种文本图任务中始终优于现有基线方法。

Conclusion: TopoRAG通过捕捉高维拓扑和关系依赖关系，有效解决了现有RAG方法在文本图推理中的局限性，特别是在处理需要闭环推理的问题时表现出色。

Abstract: Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.

</details>


### [29] [Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts](https://arxiv.org/abs/2602.19244)
*Toshihide Ubukata,Zhiyao Wang,Enhong Mu,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: 提出Soft Mixture-of-Experts框架解决强化学习策略的各向异性泛化问题，通过多专家组合扩展可解参数空间并提升鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的OTF-DCS探索策略存在各向异性泛化问题：训练随机性和轨迹依赖性导致策略只在特定参数区域表现良好，在其他区域脆弱

Method: 提出Soft Mixture-of-Experts框架，结合多个强化学习专家，通过先验置信度门控机制将各向异性行为视为互补的专业化能力

Result: 在航空交通基准测试中，Soft-MoE显著扩展了可解参数空间，相比任何单一专家都提高了鲁棒性

Conclusion: Soft-MoE框架有效解决了强化学习策略的各向异性泛化问题，通过多专家组合实现了更稳健的控制器综合性能

Abstract: On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert.

</details>


### [30] [Limited Reasoning Space: The cage of long-horizon reasoning in LLMs](https://arxiv.org/abs/2602.19281)
*Zhenyu Li,Guanlin Wu,Cheems Wang,Yongqiang Zhao*

Main category: cs.AI

TL;DR: 本文提出Halo框架，通过模型预测控制动态调节LLM推理边界，解决传统CoT等静态规划方法在增加计算预算时可能导致的性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 研究发现，在大型语言模型中，简单地增加计算预算（如使用链式思维CoT）有时反而会导致测试时性能崩溃。作者假设这种推理失败源于静态规划方法无法感知LLM推理的内在边界，提出了"有限推理空间"假设。

Method: 提出了Halo框架，这是一个用于LLM规划的模型预测控制框架。Halo采用基于熵的双控制器设计，实施"测量-规划"策略，通过动态调节推理边界来实现可控推理。

Result: 实验结果表明，Halo在复杂长视野任务上优于静态基线方法，能够通过动态调节规划在推理边界处实现更好的性能。

Conclusion: 计算预算存在最优范围，过度规划会导致冗余反馈甚至损害推理能力。Halo框架通过模型预测控制有效利用了计算扩展的优势，同时抑制了过度规划问题。

Abstract: The test-time compute strategy, such as Chain-of-Thought (CoT), has significantly enhanced the ability of large language models to solve complex tasks like logical reasoning. However, empirical studies indicate that simply increasing the compute budget can sometimes lead to a collapse in test-time performance when employing typical task decomposition strategies such as CoT. This work hypothesizes that reasoning failures with larger compute budgets stem from static planning methods, which hardly perceive the intrinsic boundaries of LLM reasoning. We term it as the Limited Reasoning Space hypothesis and perform theoretical analysis through the lens of a non-autonomous stochastic dynamical system. This insight suggests that there is an optimal range for compute budgets; over-planning can lead to redundant feedback and may even impair reasoning capabilities. To exploit the compute-scaling benefits and suppress over-planning, this work proposes Halo, a model predictive control framework for LLM planning. Halo is designed for long-horizon tasks with reason-based planning and crafts an entropy-driven dual controller, which adopts a Measure-then-Plan strategy to achieve controllable reasoning. Experimental results demonstrate that Halo outperforms static baselines on complex long-horizon tasks by dynamically regulating planning at the reasoning boundary.

</details>


### [31] [Automated Generation of Microfluidic Netlists using Large Language Models](https://arxiv.org/abs/2602.19297)
*Jasper Davidson,Skylar Stockham,Allen Boston,Ashton Snelgrove. Valerio Tenace,Pierre-Emmanuel Gaillardon*

Main category: cs.AI

TL;DR: 首次将大语言模型应用于微流控设计自动化，通过自然语言描述生成系统级Verilog网表，实现88%的平均语法准确率


<details>
  <summary>Details</summary>
Motivation: 微流控设备设计复杂限制了其可访问性，虽然微流控设计自动化有进展，但仍缺乏实用直观的解决方案连接微流控从业者与自动化技术

Method: 基于大语言模型在硬件描述语言代码生成方面的先前研究，提出将自然语言微流控设备规格转换为系统级结构Verilog网表的初步方法

Result: 展示了方法的可行性，为典型微流控设计生成结构网表，功能流正确，平均语法准确率达到88%

Conclusion: 这是大语言模型在微流控设计自动化领域的首次实际应用，提供了初步演示，证明了自然语言到硬件描述转换的可行性

Abstract: Microfluidic devices have emerged as powerful tools in various laboratory applications, but the complexity of their design limits accessibility for many practitioners. While progress has been made in microfluidic design automation (MFDA), a practical and intuitive solution is still needed to connect microfluidic practitioners with MFDA techniques. This work introduces the first practical application of large language models (LLMs) in this context, providing a preliminary demonstration. Building on prior research in hardware description language (HDL) code generation with LLMs, we propose an initial methodology to convert natural language microfluidic device specifications into system-level structural Verilog netlists. We demonstrate the feasibility of our approach by generating structural netlists for practical benchmarks representative of typical microfluidic designs with correct functional flow and an average syntactical accuracy of 88%.

</details>


### [32] [ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease](https://arxiv.org/abs/2602.19298)
*Nolan Brady,Tom Yeh*

Main category: cs.AI

TL;DR: ALPACA是一个开源强化学习环境，用于探索阿尔茨海默病的个性化序贯治疗策略，基于ADNI数据训练，能模拟不同治疗方案下的疾病进展。


<details>
  <summary>Details</summary>
Motivation: 由于阿尔茨海默病病程长且患者异质性大，通过临床试验评估个性化序贯治疗策略往往不切实际，需要开发替代方法来系统探索治疗策略。

Method: 开发了ALPACA开源强化学习环境，使用基于ADNI纵向数据训练的CAST模型，能够模拟药物条件下的疾病进展，支持个性化治疗策略的强化学习训练。

Result: CAST模型能自回归生成真实的药物条件轨迹，在ALPACA中训练的强化学习策略在记忆相关结果上优于无治疗和医生行为克隆基线，且策略依赖临床有意义的患者特征。

Conclusion: ALPACA为研究阿尔茨海默病的个性化序贯治疗决策提供了一个可重复的计算机模拟测试平台。

Abstract: Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD.

</details>


### [33] [Time Series, Vision, and Language: Exploring the Limits of Alignment in Contrastive Representation Spaces](https://arxiv.org/abs/2602.19367)
*Pratham Yashwante,Rose Yu*

Main category: cs.AI

TL;DR: 研究发现时间序列与视觉和语言表示之间存在不对称对齐：时间序列与视觉表示的对齐更强，图像可作为时间序列与语言之间的有效中介，且对齐效果随模型规模增大而改善。


<details>
  <summary>Details</summary>
Motivation: 研究柏拉图表示假说是否适用于时间序列数据，该假说认为不同模态训练的模型会收敛到共享的潜在世界结构，但此前主要研究视觉和语言模态。

Method: 首先在三模态设置中检查独立预训练的时间序列、视觉和语言编码器的几何关系，然后通过对比学习训练投影头进行后验对齐，分析几何结构、缩放行为、信息密度和输入模态特性的影响。

Result: 发现对比表示空间中的整体对齐随模型规模增大而改善，但存在不对称性：时间序列与视觉表示的对齐强于与文本的对齐，图像可作为时间序列与语言之间的有效中介。更丰富的文本描述仅在一定阈值内改善对齐效果。

Conclusion: 研究结果揭示了构建超越视觉和语言的非传统数据模态多模态系统时需要考虑的因素，特别是时间序列与视觉表示之间的强对齐关系。

Abstract: The Platonic Representation Hypothesis posits that learned representations from models trained on different modalities converge to a shared latent structure of the world. However, this hypothesis has largely been examined in vision and language, and it remains unclear whether time series participate in such convergence. We first examine this in a trimodal setting and find that independently pretrained time series, vision, and language encoders exhibit near-orthogonal geometry in the absence of explicit coupling. We then apply post-hoc alignment by training projection heads over frozen encoders using contrastive learning, and analyze the resulting representations with respect to geometry, scaling behavior, and dependence on information density and input modality characteristics. Our investigation reveals that overall alignment in contrastive representation spaces improves with model size, but this alignment is asymmetric: time series align more strongly with visual representations than with text, and images can act as effective intermediaries between time series and language. We further see that richer textual descriptions improve alignment only up to a threshold; training on denser captions does not lead to further improvement. Analogous effects are observed for visual representations. Our findings shed light on considerations for building multimodal systems involving non-conventional data modalities beyond vision and language.

</details>


### [34] [Artificial Intelligence for Modeling & Simulation in Digital Twins](https://arxiv.org/abs/2602.19390)
*Philipp Zech,Istvan David*

Main category: cs.AI

TL;DR: 本章探讨了建模与仿真（M&S）、人工智能（AI）和数字孪生（DT）三者之间的互补关系，分析了M&S在DT中的核心作用以及DT如何促进AI与M&S的融合。


<details>
  <summary>Details</summary>
Motivation: 随着M&S和AI的融合对先进数字技术产生深远影响，数字孪生作为物理资产的高保真实时表示，在企业数字化转型中扮演关键角色。需要深入理解M&S在DT中的作用，以及DT如何促进AI与M&S的融合。

Method: 首先建立对数字孪生的基础理解，详细阐述其关键组件、架构层次以及在业务、开发和运营中的各种角色。然后分析M&S在DT中的核心作用，概述从物理基础模型、离散事件仿真到混合方法的关键建模技术。最后探讨AI的双向作用：AI如何通过高级分析、预测能力和自主决策增强DT，以及DT如何作为训练、验证和部署AI模型的有价值平台。

Result: 提供了对M&S、AI和DT三者互补关系的全面探索，明确了各自在集成系统中的角色和相互作用，为创建更集成和智能的系统奠定了基础。

Conclusion: 本章通过系统分析M&S、AI和DT的相互关系，识别了创建更集成和智能系统面临的关键挑战和未来研究方向，强调了三者融合对推动数字技术发展的重要性。

Abstract: The convergence of modeling & simulation (M&S) and artificial intelligence (AI) is leaving its marks on advanced digital technology. Pertinent examples are digital twins (DTs) - high-fidelity, live representations of physical assets, and frequent enablers of corporate digital maturation and transformation. Often seen as technological platforms that integrate an array of services, DTs have the potential to bring AI-enabled M&S closer to end-users. It is, therefore, paramount to understand the role of M&S in DTs, and the role of digital twins in enabling the convergence of AI and M&S. To this end, this chapter provides a comprehensive exploration of the complementary relationship between these three. We begin by establishing a foundational understanding of DTs by detailing their key components, architectural layers, and their various roles across business, development, and operations. We then examine the central role of M&S in DTs and provide an overview of key modeling techniques from physics-based and discrete-event simulation to hybrid approaches. Subsequently, we investigate the bidirectional role of AI: first, how AI enhances DTs through advanced analytics, predictive capabilities, and autonomous decision-making, and second, how DTs serve as valuable platforms for training, validating, and deploying AI models. The chapter concludes by identifying key challenges and future research directions for creating more integrated and intelligent systems.

</details>


### [35] [Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement](https://arxiv.org/abs/2602.19396)
*Amirhossein Farzam,Majid Behabahani,Mani Malek,Yuriy Nevmyvaka,Guillermo Sapiro*

Main category: cs.AI

TL;DR: 该论文提出了一种自监督框架ReDAct，用于在LLM激活中解耦语义因子对（目标和框架），并基于此构建了FrameShield检测器来防御难以检测的越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到流畅且语义连贯的越狱提示攻击，特别是当攻击者通过操纵请求的框架来隐藏恶意目标时。传统依赖结构特征或目标特定签名的防御方法容易失效，因此需要新的防御机制。

Method: 1. 提出自监督框架用于在推理时解耦LLM激活中的语义因子对（目标和框架）；2. 构建GoalFrameBench语料库，包含受控的目标和框架变体；3. 训练ReDAct模块在冻结的LLM中提取解耦表示；4. 提出FrameShield异常检测器，基于框架表示进行检测。

Result: 1. ReDAct的理论保证和广泛实证验证表明其解耦效果有效；2. FrameShield提高了跨多个LLM家族的模型无关检测能力，且计算开销最小；3. 解耦表示可作为可解释性探针，揭示了目标和框架信号的独特特征。

Conclusion: 语义解耦是LLM安全和机制可解释性的基础构建块，ReDAct框架和FrameShield检测器为防御复杂越狱攻击提供了有效解决方案，同时增强了模型的可解释性。

Abstract: Large language models (LLMs) remain vulnerable to jailbreak prompts that are fluent and semantically coherent, and therefore difficult to detect with standard heuristics. A particularly challenging failure mode occurs when an attacker tries to hide the malicious goal of their request by manipulating its framing to induce compliance. Because these attacks maintain malicious intent through a flexible presentation, defenses that rely on structural artifacts or goal-specific signatures can fail. Motivated by this, we introduce a self-supervised framework for disentangling semantic factor pairs in LLM activations at inference. We instantiate the framework for goal and framing and construct GoalFrameBench, a corpus of prompts with controlled goal and framing variations, which we use to train Representation Disentanglement on Activations (ReDAct) module to extract disentangled representations in a frozen LLM. We then propose FrameShield, an anomaly detector operating on the framing representations, which improves model-agnostic detection across multiple LLM families with minimal computational overhead. Theoretical guarantees for ReDAct and extensive empirical validations show that its disentanglement effectively powers FrameShield. Finally, we use disentanglement as an interpretability probe, revealing distinct profiles for goal and framing signals and positioning semantic disentanglement as a building block for both LLM safety and mechanistic interpretability.

</details>


### [36] [ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making](https://arxiv.org/abs/2602.19458)
*Ziyang Guo,Yifan Wu,Jason Hartline,Kenneth Holstein,Jessica Hullman*

Main category: cs.AI

TL;DR: ComplLLM是一个基于决策理论的后训练框架，通过互补信息作为奖励来微调决策助手LLM，使其输出能够补充现有智能体决策的信号。


<details>
  <summary>Details</summary>
Motivation: 多智能体决策管道在互补性成立时优于单智能体工作流，即不同智能体带来独特信息以支持最终决策。需要一种方法来利用这种互补性。

Method: 提出ComplLLM后训练框架，基于决策理论，使用互补信息作为奖励来微调决策助手LLM，使其输出能够补充现有智能体决策的信号。

Result: 在涉及领域专家的合成和现实世界任务上验证了ComplLLM，证明该方法能够恢复已知的互补信息，并产生合理的互补信号解释以支持下游决策者。

Conclusion: ComplLLM框架有效地利用多智能体系统中的互补性，通过LLM微调产生补充现有决策的信号，为下游决策提供支持。

Abstract: Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.

</details>


### [37] [Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark](https://arxiv.org/abs/2602.19502)
*Lalitha Pranathi Pulavarthy,Raajitha Muthyala,Aravind V Kuruvikkattil,Zhenan Yin,Rashmita Kudamala,Saptarshi Purkayastha*

Main category: cs.AI

TL;DR: 人类指导的智能体AI在医疗预测任务中表现优于纯自动化方法，通过领域专家指导特征工程、模型选择和验证策略，在三个医疗预测任务中取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体AI系统在自主数据科学工作流方面能力不断增强，但临床预测任务需要领域专业知识，这是纯自动化方法难以提供的。研究探索如何通过人类指导来改进多模态临床预测。

Method: 在AgentDS Healthcare基准的三个挑战中，人类分析师在关键决策点指导智能体工作流：从临床记录、扫描PDF账单收据和时间序列生命体征进行多模态特征工程；任务适当的模型选择；以及临床信息验证策略。

Result: 在医疗领域排名第5，出院准备任务排名第3。消融研究表明，人类指导决策比自动化基线累计提升+0.065 F1，其中多模态特征提取贡献最大（+0.041 F1）。具体任务结果：30天再入院预测（Macro-F1 = 0.8986）、急诊费用预测（MAE = $465.13）、出院准备评估（Macro-F1 = 0.7939）。

Conclusion: 总结了三个可推广的经验：1）各阶段领域信息特征工程产生复合增益，优于广泛自动化搜索；2）多模态数据集成需要任务特定的人类判断；3）临床动机模型配置的有意集成多样性优于随机超参数搜索。为医疗环境中部署智能体AI提供了实用指导。

Abstract: Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.

</details>


### [38] [Classroom Final Exam: An Instructor-Tested Reasoning Benchmark](https://arxiv.org/abs/2602.19517)
*Chongyang Gao,Diji Yang,Shuyan Zhou,Xichen Yan,Luchuan Song,Shuo Li,Kezhen Chen*

Main category: cs.AI

TL;DR: CFE是一个多模态基准测试，用于评估大语言模型在20多个STEM领域的推理能力，基于真实大学作业和考试题目，前沿模型表现仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在STEM领域的真实推理能力，现有基准可能无法充分反映模型在复杂多步问题解决中的表现。

Method: 从大学课程中收集真实作业和考试题目，由课程教师提供参考答案，构建包含20多个STEM领域的多模态基准测试，并对模型解决方案进行诊断分析。

Result: 前沿模型表现有限：Gemini-3.1-pro-preview准确率59.69%，Gemini-3-flash-preview准确率55.46%。诊断分析发现模型能正确回答中间子问题，但难以在多步解决方案中可靠地推导和维护正确的中间状态。

Conclusion: CFE基准揭示了当前大语言模型在复杂STEM推理任务中的局限性，模型生成的解决方案通常步骤更多、效率更低、错误累积风险更高，为未来模型改进提供了重要方向。

Abstract: We introduce \CFE{} (\textbf{C}lassroom \textbf{F}inal \textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.

</details>


### [39] [Ada-RS: Adaptive Rejection Sampling for Selective Thinking](https://arxiv.org/abs/2602.19519)
*Yirou Ge,Yixi Li,Alec Chiu,Shivani Shekhar,Zijie Pan,Avinash Thangali,Yun-Shiuan Chuang,Chaitanya Kulkarni,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: Ada-RS是一种算法无关的样本过滤框架，通过自适应长度惩罚奖励和随机拒绝采样，选择性地保留高质量推理样本，显著减少LLM推理的token消耗和思考率，同时保持或提升工具调用准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在成本和延迟敏感场景中部署时，虽然思维链能改善推理，但会在简单请求上浪费token。需要选择性思考机制来提高推理效率。

Method: 提出自适应拒绝采样框架，对每个上下文用自适应长度惩罚奖励对多个采样完成进行评分，然后应用随机拒绝采样仅保留高奖励候选（或偏好对）用于下游优化。可集成到偏好对优化或分组策略优化中。

Result: 在Qwen3-8B模型和合成电商工具调用基准测试中，Ada-RS将平均输出token减少80%，思考率降低95%，同时保持或提升工具调用准确性，改进了准确性与效率的权衡边界。

Conclusion: 训练信号选择是延迟敏感部署中实现高效推理的有力杠杆，Ada-RS框架为工具使用型LLM提供了选择性思考的有效解决方案。

Abstract: Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments.

</details>


### [40] [A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data](https://arxiv.org/abs/2602.19562)
*Joseph Bingham*

Main category: cs.AI

TL;DR: 该研究提出一个计算框架，通过整合语言表达和基于众包图像生成的感知表征，模拟人类指称解释的核心方面，在斯坦福重复指称游戏语料上表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 建立自然语言表达与视觉感知之间的稳定映射是认知科学和人工智能的基础问题。人类能够在嘈杂、模糊的感知环境中理解语言指称，但支持这种跨模态对齐的机制仍不清楚。

Method: 开发了一个计算框架，结合尺度不变特征变换（SIFT）对齐和通用质量指数（UQI）来量化认知合理的特征空间中的相似性，同时使用语言预处理和查询转换操作捕捉指称表达中的语用变异性。

Result: 在斯坦福重复指称游戏语料（15,000个话语配对七巧板刺激）上评估，框架达到稳健的指称接地。比人类对话者少用65%的话语达到稳定映射，并能从单个指称表达中正确识别目标对象41.66%的时间（人类为20%）。

Conclusion: 相对简单的感知-语言对齐机制可以在经典认知基准上产生具有人类竞争力的行为，为接地通信、感知推理和跨模态概念形成模型提供了见解。

Abstract: Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\% of the time (versus 20\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .

</details>


### [41] [Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent](https://arxiv.org/abs/2602.19837)
*Björn Hoppmann,Christoph Scholz*

Main category: cs.AI

TL;DR: 这篇论文是一篇关于元学习和元强化学习的综述性文章，系统性地梳理了从基础算法到DeepMind自适应智能体的发展历程。


<details>
  <summary>Details</summary>
Motivation: 人类能够有效利用先验知识适应新任务，而传统机器学习模型依赖任务特定训练，难以实现这种能力。元学习通过从多个任务中获取可迁移知识，使模型能够用少量数据快速适应新挑战。

Method: 采用基于任务的元学习和元强化学习形式化框架，系统梳理了该领域的关键算法发展路径，从基础方法到DeepMind自适应智能体，整合了理解通用智能体方法所需的核心概念。

Result: 提供了元学习和元强化学习的系统性综述，建立了从基础理论到前沿应用（如DeepMind自适应智能体）的完整知识框架，为理解通用智能体方法提供了概念基础。

Conclusion: 元学习和元强化学习是实现通用人工智能的重要途径，通过从多任务学习中获取可迁移知识，使智能体能够像人类一样快速适应新环境。这篇综述为理解该领域的发展脉络和前沿进展提供了系统性框架。

Abstract: Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.

</details>


### [42] [Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning](https://arxiv.org/abs/2602.19914)
*Thatchawin Leelawat,Lewis D Griffin*

Main category: cs.AI

TL;DR: 该研究将侦探桌游Watson & Holmes改编为AI推理基准测试，通过渐进式叙事证据、开放式问题和自由语言回答来评估AI推理能力，开发了自动化评分系统，发现AI模型在9个月内从人类对比组的低四分位数提升至前5%。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理基准测试难以评估AI在自然情境下的推理能力与人类推理的相似性，需要一种能模拟真实推理过程（渐进证据呈现、开放式问题、自由回答）的评估方法。

Method: 将Watson & Holmes侦探桌游改编为基准测试，包含渐进呈现的叙事证据、开放式问题和自由语言回答；开发自动化评分系统并与人类评估者验证，确保可扩展性和可重复性。

Result: AI模型在9个月内表现显著提升：从人类对比组的低四分位数提升至前5%；约一半改进来自连续模型发布的稳步进步，另一半来自推理导向架构的显著跃升；AI在长案件（1900-4000词）表现下降，但在证据稀缺的早期阶段推理模型在归纳推理上有优势。

Conclusion: 该基准测试能有效评估AI推理能力的发展轨迹，揭示了AI推理的进步模式和与人类推理的差异，特别在长文本处理和早期证据稀缺情境下的表现特点。

Abstract: Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.

</details>


### [43] [Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning](https://arxiv.org/abs/2602.19930)
*Nathan Gavenski,Felipe Meneguzzi,Odinaldo Rodrigues*

Main category: cs.AI

TL;DR: 该论文认为模仿学习的根本问题在于过度优化了完美回放，而非组合适应性。作者提出重新定义成功标准，从完美回放转向组合适应性，使智能体能够学习行为基元并在新情境中重组而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习智能体虽然经过数十年发展，但仍然是复杂的记忆机器，擅长回放却无法应对情境变化或目标演化。这种失败不是技术问题而是基础性问题：模仿学习一直在优化错误的目标。

Method: 提出重新定义成功标准的研究议程，从完美回放转向组合适应性；建立组合泛化的度量标准；提出混合架构；并借鉴认知科学和文化演化的跨学科研究方向。

Result: 论文提出了一个研究框架，旨在使智能体能够学习行为基元并在新情境中重组而无需重新训练，从而具备在开放世界中操作的基本能力。

Conclusion: 将适应性嵌入模仿学习核心的智能体具备在开放世界中操作的基本能力。模仿学习需要从完美回放转向组合适应性，这需要重新定义成功标准并采用跨学科方法。

Abstract: Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.

</details>


### [44] [Latent Introspection: Models Can Detect Prior Concept Injections](https://arxiv.org/abs/2602.20031)
*Theia Pearson-Vogel,Martin Vanek,Raymond Douglas,Jan Kulveit*

Main category: cs.AI

TL;DR: 研究发现Qwen 32B模型具有潜在的内省能力，能够检测到概念注入并识别具体概念，这种能力可通过提示进一步增强


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否具有内省能力，即模型能否意识到自身内部状态的变化，特别是概念注入的影响

Method: 使用Qwen 32B模型进行概念注入实验，通过logit lens分析残差流中的检测信号，并测试提示模型关于AI内省机制信息的效果

Result: 模型确实能检测概念注入（尽管在采样输出中否认），残差流中显示明显检测信号；准确提示使注入检测敏感度从0.3%大幅提升至39.2%，误报仅增加0.6%；九个注入与恢复概念间的互信息从0.62比特增至1.05比特

Conclusion: 模型具有令人惊讶的内省和转向意识能力，这种能力容易被忽视，对潜在推理和安全性具有重要意义

Abstract: We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.

</details>


### [45] [CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching](https://arxiv.org/abs/2602.20094)
*Yuzhe Wang,Yaochen Zhu,Jundong Li*

Main category: cs.AI

TL;DR: 论文提出了CausalFlip基准测试，旨在评估LLMs是否真正基于因果关系而非语义相关性进行推理，通过构建语义相似但因果答案相反的问题对来检测模型对语义模式的依赖。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在复杂高风险决策场景中的部署增加，需要确保其推理基于因果关系而非虚假相关性。传统推理基准的高性能可能源于记忆语义模式而非分析真实因果结构，因此需要专门评估LLMs因果推理能力的基准。

Method: 提出CausalFlip因果推理基准，包含基于事件三元组构建的因果判断问题，形成混杂、链式和碰撞关系。为每个事件三元组构建语义相似但因果答案相反的问题对。还引入噪声前缀评估，在中间因果推理步骤前添加因果无关文本。评估了多种训练范式：仅答案训练、显式思维链监督、以及提出的内化因果推理方法。

Result: 评估结果显示，显式思维链仍可能被虚假语义相关性误导，而内化推理步骤的方法显著改善了因果基础，表明更好地激发基础LLMs潜在因果推理能力是有前景的。

Conclusion: CausalFlip基准能够有效检测LLMs对语义模式的依赖，内化因果推理方法比显式思维链监督更能减少对相关性的显式依赖，有助于开发基于因果关系而非语义相关性的新LLM范式或训练算法。

Abstract: As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.

</details>


### [46] [Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration](https://arxiv.org/abs/2602.20104)
*Hasan Amin,Ming Yin,Rajiv Khanna*

Main category: cs.AI

TL;DR: 本文提出了一种自适应AI集成系统，通过智能切换对齐模型和互补模型来解决人类-AI决策中的根本矛盾：互补性提升性能但损害信任，对齐性建立信任但可能强化次优行为。


<details>
  <summary>Details</summary>
Motivation: 传统单一AI模型在辅助人类决策时面临根本性矛盾：互补性AI（在人类弱项上表现优异）会降低AI在人类强项上的性能，损害人类信任；而对齐性AI（与人类行为一致）虽然建立信任，但可能强化人类的次优行为。这种性能提升与信任建设之间的张力限制了人类-AI协作的效果。

Method: 提出了一种人类中心的自适应AI集成系统，包含两个专家模型：对齐模型和互补模型。系统通过"理性路由捷径"机制，基于上下文线索智能切换使用哪个模型。该机制设计优雅简单且被证明接近最优。

Result: 理论分析阐明了自适应AI集成系统有效的原因和最大效益条件。在模拟和真实世界数据上的实验表明，当人类使用自适应AI集成辅助决策时，其性能显著高于使用单一AI模型（无论是优化独立性能还是优化人类-AI团队性能的模型）。

Conclusion: 自适应AI集成系统通过动态切换对齐和互补模型，成功解决了人类-AI协作中的根本矛盾，实现了性能提升和信任建设的平衡，显著提高了人类-AI团队的决策性能。

Abstract: In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.

</details>


### [47] [ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models](https://arxiv.org/abs/2602.20117)
*Andre He,Nathaniel Weir,Kaj Bostrom,Allen Nie,Darion Cassel,Sam Bayless,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 论文提出ReSyn框架，通过大规模生成多样化的推理环境（含实例生成器和验证器）来扩展RLVR方法，显著提升语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有验证器监督的强化学习方法虽然比解决方案标注更容易实现，但现有方法要么依赖解决方案中心的数据生成，要么局限于少量手工制作的过程环境，需要一种能够大规模生成多样化推理环境的方法

Method: 提出ReSyn框架，自动生成包含约束满足、算法谜题、空间推理等任务的多样化推理环境，每个环境都配备实例生成器和验证器，然后使用这些环境进行强化学习训练

Result: 使用ReSyn数据训练的Qwen2.5-7B-Instruct模型在多个推理基准测试中取得一致提升，在具有挑战性的BBEH基准上实现27%的相对改进，消融实验表明验证器监督和任务多样性都有显著贡献

Conclusion: 大规模生成推理环境可以有效增强语言模型的推理能力，验证器监督和任务多样性是提升性能的关键因素，为可验证奖励的强化学习提供了可扩展的解决方案

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [48] [DCInject: Persistent Backdoor Attacks via Frequency Manipulation in Personal Federated Learning](https://arxiv.org/abs/2602.18489)
*Nahom Birhan,Daniel Wesego,Dereje Shenkut,Frank Liu,Daniel Takabi*

Main category: cs.CR

TL;DR: 本文提出DCInject，一种针对个性化联邦学习的自适应频域后门攻击方法，通过移除零频分量并用高斯分布样本替换，在保持清洁准确率的同时实现高攻击成功率


<details>
  <summary>Details</summary>
Motivation: 个性化联邦学习（PFL）虽然能处理数据异构性且被认为对后门攻击传播具有天然抵抗力，但本文揭示PFL仍然存在后门攻击漏洞，需要探索更隐蔽的攻击方法

Method: 提出DCInject攻击方法：在频域中移除部分零频（DC）分量，并用高斯分布样本替换，形成自适应频域后门攻击，针对参数解耦的个性化联邦学习架构

Result: 在CIFAR-10/100、GTSRB、SVHN四个数据集上，DCInject实现了96.83%（CIFAR-10）、99.38%（SVHN）和100%（GTSRB）的攻击成功率，同时保持清洁准确率；在I-BAU防御下仍保持90.30%的攻击成功率，显著高于BadNet的58.56%

Conclusion: 个性化联邦学习仍然容易受到频域后门攻击，DCInject方法暴露了PFL安全假设的关键漏洞，需要重新评估PFL系统的安全性

Abstract: Personalized federated learning (PFL) creates client-specific models to handle data heterogeneity. Previously, PFL has been shown to be naturally resistant to backdoor attack propagation across clients. In this work, we reveal that PFL remains vulnerable to backdoor attacks through a novel frequency-domain approach. We propose DCInject, an adaptive frequency-domain backdoor attack for PFL, which removes portions of the zero-frequency (DC) component and replaces them with Gaussian-distributed samples in the frequency domain. Our attack achieves superior attack success rates while maintaining clean accuracy across four datasets (CIFAR-10/100, GTSRB, SVHN) compared to existing spatial-domain attacks, evaluated under parameter decoupling based personalization. DCInject achieves superior performance with ASRs of 96.83% (CIFAR-10), 99.38% (SVHN), and 100% (GTSRB) while maintaining clean accuracy. Under I-BAU defense, DCInject demonstrates strong persistence, retaining 90.30% ASR vs BadNet's 58.56% on VGG-16, exposing critical vulnerabilities in PFL security assumptions. Our code is available at https://github.com/NahomMA/DCINject-PFL

</details>


### [49] [Trojan Horses in Recruiting: A Red-Teaming Case Study on Indirect Prompt Injection in Standard vs. Reasoning Models](https://arxiv.org/abs/2602.18514)
*Manuel Wirth*

Main category: cs.CR

TL;DR: 该研究挑战了"推理模型更安全"的假设，通过红队测试发现推理模型在处理对抗性指令时存在更复杂的失效模式，包括元认知泄漏等新风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在人力资源等自动化决策管道中的集成，间接提示注入的安全问题变得至关重要。虽然普遍假设推理模型或思维链模型因其自我纠正能力而具有安全优势，但新兴研究表明这些能力可能导致更复杂的对齐失败。

Method: 采用定性红队案例研究方法，使用Qwen 3 30B架构，对比标准指令调优模型和推理增强模型在面对"特洛伊木马"简历时的表现，分析不同的失效模式。

Result: 研究发现复杂的权衡：标准模型通过脆弱的幻觉来合理化简单攻击，在复杂场景中过滤掉不合逻辑的约束；而推理模型表现出危险的二元性，能够使用高级战略重构使简单攻击更具说服力，但在面对逻辑复杂的命令时出现"元认知泄漏"现象。

Conclusion: 该研究揭示了推理模型的一个新失效模式：处理复杂对抗性指令的认知负荷导致注入逻辑无意中出现在最终输出中，使得攻击比在标准模型中更容易被人类检测到，挑战了"推理即安全"的前提假设。

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated decision-making pipelines, specifically within Human Resources (HR), the security implications of Indirect Prompt Injection (IPI) become critical. While a prevailing hypothesis posits that "Reasoning" or "Chain-of-Thought" Models possess safety advantages due to their ability to self-correct, emerging research suggests these capabilities may enable more sophisticated alignment failures. This qualitative Red-Teaming case study challenges the safety-through-reasoning premise using the Qwen 3 30B architecture. By subjecting both a standard instruction-tuned model and a reasoning-enhanced model to a "Trojan Horse" curriculum vitae, distinct failure modes are observed. The results suggest a complex trade-off: while the Standard Model resorted to brittle hallucinations to justify simple attacks and filtered out illogical constraints in complex scenarios, the Reasoning Model displayed a dangerous duality. It employed advanced strategic reframing to make simple attacks highly persuasive, yet exhibited "Meta-Cognitive Leakage" when faced with logically convoluted commands. This study highlights a failure mode where the cognitive load of processing complex adversarial instructions causes the injection logic to be unintentionally printed in the final output, rendering the attack more detectable by humans than in Standard Models.

</details>


### [50] [Poster: Privacy-Preserving Compliance Checks on Ethereum via Selective Disclosure](https://arxiv.org/abs/2602.18539)
*Supriya Khadka,Dhiman Goswami,Sanchari Das*

Main category: cs.CR

TL;DR: 基于以太坊的选择性披露框架，使用zk-SNARKs技术让用户在不暴露身份文件的情况下证明特定资格条件


<details>
  <summary>Details</summary>
Motivation: 数字身份验证通常要求用户披露过多敏感个人信息来证明简单资格条件，存在数据泄露和监控风险。随着区块链应用进入受监管环境，这种过度披露问题变得更加突出。

Method: 提出基于以太坊的选择性披露框架，利用客户端zk-SNARKs技术，将属性验证与身份揭示解耦。通过案例研究ZK-Compliance实现了年龄验证的授权、验证、撤销完整生命周期。

Result: 初步结果显示，该框架能够在满足严格合规要求的同时，保持客户端延迟极低（<200毫秒），并保护公共区块链的匿名性。

Conclusion: 该选择性披露框架能够有效解决数字身份验证中的隐私与合规平衡问题，为区块链在受监管环境中的应用提供了可行的隐私保护方案。

Abstract: Digital identity verification often forces a privacy trade-off, where users must disclose sensitive personal data to prove simple eligibility criteria. As blockchain applications integrate with regulated environments, this over-disclosure creates significant risks of data breaches and surveillance. This work proposes a general Selective Disclosure Framework built on Ethereum, designed to decouple attribute verification from identity revelation. By utilizing client-side zk-SNARKs, the framework enables users to prove specific eligibility predicates without revealing underlying identity documents. We present a case study, ZK-Compliance, which implements a functional Grant, Verify, Revoke lifecycle for age verification. Preliminary results indicate that strict compliance requirements can be satisfied with negligible client-side latency (< 200 ms) while preserving the pseudonymous nature of public blockchains.

</details>


### [51] [Hardware-Friendly Randomization: Enabling Random-Access and Minimal Wiring in FHE Accelerators with Low Total Cost](https://arxiv.org/abs/2602.19550)
*Ilan Rosenfeld,Noam Kleinburd,Hillel Chapman,Dror Reuven*

Main category: cs.CR

TL;DR: 提出一种基于种子的RLWE多项式a生成方案，减少通信开销和硬件实现复杂度，同时保持安全性


<details>
  <summary>Details</summary>
Motivation: RLWE问题中均匀随机多项式a的传输和硬件实现存在显著通信开销和硬件挑战，需要更高效的生成方案

Method: 采用确定性过程从种子生成a，在客户端传输种子，在硬件加速器中动态生成a，优化并行生成、布线要求和随机访问能力

Result: 通信延迟和内存占用减少，支持并行生成均匀分布样本，布线要求宽松，RNS limbs可随机访问，客户端开销低于3%，节省功耗

Conclusion: 提出的方案有效解决了RLWE中多项式a生成的实践挑战，显著降低了通信和硬件成本，适合高吞吐量FHE加速器配置

Abstract: The Ring-Learning With Errors (RLWE) problem forms the backbone of highly efficient Fully Homomorphic Encryption (FHE) schemes. A significant component of the RLWE public key and ciphertext of the form $(b,a)$ is the uniformly random polynomial $a \in R_q$ . While essential for security, the communication overhead of transmitting $a$ from client to server, and inputting it into a hardware accelerator, can be substantial, especially for FHE accelerators aiming at high acceleration factors. A known technique in reducing this overhead generates $a$ from a small seed on the client side via a deterministic process, transmits only the seed, and generates $a$ on-the-fly within the accelerator. Challenges in the hardware implementation of an accelerator include wiring (density and power), compute area, compute power as well as flexibility in scheduling of on-the-fly generation instructions. This extended abstract proposes a concrete scheme and parameters wherein these practical challenges are addressed. We detail the benefits of our approach, which maintains the reduction in communication latency and memory footprint, while allowing parallel generation of uniformly distributed samples, relaxed wiring requirements, unrestricted randomaccess to RNS limbs, and results in an extremely low overhead on the client side (i.e. less than 3%) during the key generation process. The proposed scheme eliminates the need for thick metal layers for randomness distribution and prevents the power consumption of the PRNG subsystem from scaling prohibitively with the acceleration factor, potentially saving tens of Watts per accelerator chip in high-throughput configurations.

</details>


### [52] [Media Integrity and Authentication: Status, Directions, and Futures](https://arxiv.org/abs/2602.18681)
*Jessica Young,Sam Vaughan,Andrew Jenks,Henrique Malvar,Christian Paquin,Paul England,Thomas Roca,Juan LaVista Ferres,Forough Poursabzi,Neil Coles,Ken Archer,Eric Horvitz*

Main category: cs.CR

TL;DR: 该论文探讨了AI生成媒体与真实内容鉴别的挑战，分析了来源验证、水印和指纹三种认证方法，评估了它们在多模态应用中的表现，并提出了对抗技术和社会工程攻击的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成媒体的快速发展，区分AI生成内容与真实摄像头/麦克风捕获内容变得越来越困难，这给媒体完整性和认证方法带来了新的挑战。需要开发可靠的技术来确保媒体内容的真实性。

Method: 论文评估了三种主要认证方法：加密安全来源验证、不可感知水印和软哈希指纹技术。分析了这些技术在多模态（视觉、音频）中的应用，并考虑了从捕获、编辑、分发到验证的完整工作流程。

Result: 分析了不同认证方法的优缺点，识别了技术和社会工程攻击的威胁模型，特别是那些能够反转完整性信号使真实内容看起来像合成的攻击。提出了需要既能抵抗技术攻击又能抵抗心理社会操纵的验证系统。

Conclusion: 需要开发高置信度的来源认证技术，包括通过安全飞地加强边缘设备安全的方向。未来的媒体认证系统必须同时考虑技术安全性和社会技术层面的攻击，确保在AI生成媒体时代能够可靠地区分真实与合成内容。

Abstract: We provide background on emerging challenges and future directions with media integrity and authentication methods, focusing on distinguishing AI-generated media from authentic content captured by cameras and microphones. We evaluate several approaches, including provenance, watermarking, and fingerprinting. After defining each method, we analyze three representative technologies: cryptographically secured provenance, imperceptible watermarking, and soft-hash fingerprinting. We analyze how these tools operate across modalities and evaluate relevant threat models, attack categories, and real-world workflows spanning capture, editing, distribution, and verification. We consider sociotechnical reversal attacks that can invert integrity signals, making authentic content appear synthetic and vice versa, highlighting the value of verification systems that are resilient to both technical and psychosocial manipulation. Finally, we outline techniques for delivering high-confidence provenance authentication, including directions for strengthening edge-device security using secure enclaves.

</details>


### [53] [UFO: Unlocking Ultra-Efficient Quantized Private Inference with Protocol and Algorithm Co-Optimization](https://arxiv.org/abs/2602.18758)
*Wenxuan Zeng,Chao Yang,Tianshi Xu,Bo Zhang,Changrui Ren,Jin Dong,Meng Li*

Main category: cs.CR

TL;DR: UFO是一个量化两方计算推理框架，通过联合优化2PC协议和量化算法，结合Winograd卷积与量化技术，显著降低了通信开销并提高了模型精度。


<details>
  <summary>Details</summary>
Motivation: 基于安全两方计算的私有CNN推理存在高通信和延迟开销，特别是卷积层。现有方法在结合量化和Winograd卷积时面临通信开销大和模型精度下降的挑战。

Method: 1) 协议层面：提出图级优化的2PC推理协议以最小化通信；2) 算法层面：开发基于层敏感度的混合精度量化感知训练算法，并引入2PC友好的比特重加权算法来处理异常值。

Result: 与最先进框架SiRNN、COINN和CoPriv相比，UFO分别实现了11.7倍、3.6倍和6.3倍的通信减少，同时精度分别提高了1.29%、1.16%和1.29%。

Conclusion: UFO通过协议和算法的协同优化，成功解决了量化和Winograd卷积结合时的挑战，在保持模型精度的同时显著降低了私有CNN推理的通信开销。

Abstract: Private convolutional neural network (CNN) inference based on secure two-party computation (2PC) suffers from high communication and latency overhead, especially from convolution layers. In this paper, we propose UFO, a quantized 2PC inference framework that jointly optimizes the 2PC protocols and quantization algorithm. UFO features a novel 2PC protocol that systematically combines the efficient Winograd convolution algorithm with quantization to improve inference efficiency. However, we observe that naively combining quantization and Winograd convolution faces the following challenges: 1) From the inference perspective, Winograd transformations introduce extensive additions and require frequent bit width conversions to avoid inference overflow, leading to non-negligible communication overhead; 2) From the training perspective, Winograd transformations introduce weight outliers that make quantization-aware training (QAT) difficult, resulting in inferior model accuracy. To address these challenges, we co-optimize both protocol and algorithm. 1) At the protocol level, we propose a series of graph-level optimizations for 2PC inference to minimize the communication. 2) At the algorithm level, we develop a mixed-precision QAT algorithm based on layer sensitivity to optimize model accuracy given communication constraints. To accommodate the outliers, we further introduce a 2PC-friendly bit re-weighting algorithm to increase the representation range without explicitly increasing bit widths. With extensive experiments, UFO demonstrates 11.7x, 3.6x, and 6.3x communication reduction with 1.29%, 1.16%, and 1.29% higher accuracy compared to state-of-the-art frameworks SiRNN, COINN, and CoPriv, respectively.

</details>


### [54] [MANATEE: Inference-Time Lightweight Diffusion Based Safety Defense for LLMs](https://arxiv.org/abs/2602.18782)
*Chun Yan Ryan Kan,Tommy Tran,Vedant Yadav,Ava Cai,Kevin Zhu,Ruizhe Li,Maheep Chaudhary*

Main category: cs.CR

TL;DR: MANATEE是一种基于密度估计的推理时防御方法，通过扩散过程将异常表示投影到安全区域，有效防御LLM对抗性越狱攻击，无需有害训练数据或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对抗性越狱攻击防御方法存在局限性：基于二元分类器的方法在对抗性输入超出学习决策边界时失效；重复微调计算成本高且可能降低模型能力。需要一种更有效的推理时防御方案。

Method: MANATEE使用良性表示流形上的密度估计，学习良性隐藏状态的得分函数，通过扩散过程将异常表示投影到安全区域。该方法无需有害训练数据，无需修改模型架构。

Result: 在Mistral-7B-Instruct、Llama-3.1-8B-Instruct和Gemma-2-9B-it上的实验表明，MANATEE在某些数据集上可将攻击成功率降低高达100%，同时保持对良性输入的模型效用。

Conclusion: MANATEE提供了一种有效的推理时防御方案，通过密度估计和扩散投影技术防御LLM对抗性越狱攻击，在保持模型效用的同时显著降低攻击成功率。

Abstract: Defending LLMs against adversarial jailbreak attacks remains an open challenge. Existing defenses rely on binary classifiers that fail when adversarial input falls outside the learned decision boundary, and repeated fine-tuning is computationally expensive while potentially degrading model capabilities. We propose MANATEE, an inference-time defense that uses density estimation over a benign representation manifold. MANATEE learns the score function of benign hidden states and uses diffusion to project anomalous representations toward safe regions--requiring no harmful training data and no architectural modifications. Experiments across Mistral-7B-Instruct, Llama-3.1-8B-Instruct, and Gemma-2-9B-it demonstrate that MANATEE reduce Attack Success Rate by up to 100\% on certain datasets, while preserving model utility on benign inputs.

</details>


### [55] [Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection](https://arxiv.org/abs/2602.19025)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali. A Ghorbani*

Main category: cs.CR

TL;DR: 该研究提出了一种用于恶意软件检测的混合专家图模型，通过节点级多统计量编码和专家级多样性构建，结合路由感知解释机制，在控制流图上实现高精度检测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提高混合专家图模型在恶意软件检测中的可解释性。传统混合专家模型虽然能结合多个图视图进行灵活推理，但其决策过程不够透明。特别是在恶意软件分析中，需要理解模型基于控制流图的哪些结构特征做出判断。

Method: 方法包括两个层面的多样性构建：1）节点层面，每层计算多个邻域统计量（均值、标准差、最大值），通过MLP融合，并引入度重加权因子ρ和池化选择λ；2）读出层面，六个专家各对应特定(ρ, λ)视图，输出图级logits，由路由器加权得到最终预测。后验解释通过专家级边归因聚合生成，反映各专家关注点和路由器选择强度。

Result: 在相同CFG数据集上评估，相比GCN、GIN、GAT等单专家GNN基线，提出的MoE模型实现了强检测准确率，同时在稀疏性扰动下产生稳定、忠实的归因。结果表明路由器显式化和多统计量节点编码与专家级多样性结合能提升恶意软件分析中MoE决策的透明度。

Conclusion: 结论表明，通过构建节点级多统计量编码和专家级多样性，结合路由感知解释机制，混合专家图模型不仅能提高恶意软件检测精度，还能增强决策过程的透明度和可解释性，为恶意软件分析提供更可靠的图推理框架。

Abstract: Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis.

</details>


### [56] [SiGRRW: A Single-Watermark Robust Reversible Watermarking Framework with Guiding Strategy](https://arxiv.org/abs/2602.19097)
*Zikai Xu,Bin Liu,Weihai Li,Lijunxian Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: SiGRRW提出了一种单水印鲁棒可逆水印框架，通过引导图像策略同时实现鲁棒性和可逆性，适用于生成模型和自然图像，在容量、不可感知性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前鲁棒可逆水印方案通常采用两阶段框架，无法在单次水印中同时实现鲁棒性和可逆性，且两个水印之间的功能干扰导致容量和不可感知性等多方面性能下降。

Method: 提出SiGRRW单水印框架，引入新颖的引导策略生成引导图像，作为嵌入和恢复的指导。水印通过引导残差可逆嵌入，该残差可从原始图像和水印图像计算得出。框架可作为生成模型输出阶段的即插即用水印层，或直接应用于自然图像。

Result: 大量实验表明，SiGRRW相比现有RRW方案有效提升了不可感知性和鲁棒性，同时保持原始图像的无损恢复，容量显著高于传统方案。

Conclusion: SiGRRW框架成功解决了传统两阶段RRW方案的功能干扰问题，实现了单水印同时具备鲁棒性和可逆性，为图像版权保护提供了更优解决方案。

Abstract: Robust reversible watermarking (RRW) enables copyright protection for images while overcoming the limitation of distortion introduced by watermark itself. Current RRW schemes typically employ a two-stage framework, which fails to achieve simultaneous robustness and reversibility within a single watermarking, and functional interference between the two watermarks results in performance degradation in multiple terms such as capacity and imperceptibility. We propose SiGRRW, a single-watermark RRW framework, which is applicable to both generative models and natural images. We introduce a novel guiding strategy to generate guiding images, serving as the guidance for embedding and recovery. The watermark is reversibly embedded with the guiding residual, which can be calculated from both cover images and watermark images. The proposed framework can be deployed either as a plug-and-play watermarking layer at the output stage of generative models, or directly applied to natural images. Extensive experiments demonstrate that SiGRRW effectively enhances imperceptibility and robustness compared to existing RRW schemes while maintaining lossless recovery of cover images, with significantly higher capacity than conventional schemes.

</details>


### [57] [ReVision : A Post-Hoc, Vision-Based Technique for Replacing Unacceptable Concepts in Image Generation Pipeline](https://arxiv.org/abs/2602.19149)
*Gurjot Singh,Prabhjot Singh,Aashima Sharma,Maninder Singh,Ryan Ko*

Main category: cs.CR

TL;DR: ReVision是一个无需训练、基于提示的后处理安全框架，用于图像生成管道，通过检测和编辑不安全内容来作为最后一道防线。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成模型安全缓解策略（如提示过滤和安全感知训练）存在被绕过和降低生成质量的问题，需要更有效的后处理安全解决方案。

Method: 使用Gemini-2.5-Flash作为通用政策违规概念检测器，结合VLM辅助的空间门控机制进行实例一致定位，执行局部语义编辑来替换不安全内容。

Result: 在245张图像的基准测试中，ReVision显著提高了安全提示的CLIP对齐度，改善了多概念背景保真度，几乎完全抑制了类别特定检测器，并将人类识别违规内容的比例从95.99%降至10.16%。

Conclusion: ReVision作为一个无需训练的后处理安全框架，能够有效检测和编辑图像生成模型中的不安全内容，同时保持场景完整性，具有实际部署价值。

Abstract: Image-generative models are widely deployed across industries. Recent studies show that they can be exploited to produce policy-violating content. Existing mitigation strategies primarily operate at the pre- or mid-generation stages through techniques such as prompt filtering and safety-aware training/fine-tuning. Prior work shows that these approaches can be bypassed and often degrade generative quality. In this work, we propose ReVision, a training-free, prompt-based, post-hoc safety framework for image-generation pipeline. ReVision acts as a last-line defense by analyzing generated images and selectively editing unsafe concepts without altering the underlying generator. It uses the Gemini-2.5-Flash model as a generic policy-violating concept detector, avoiding reliance on multiple category-specific detectors, and performs localized semantic editing to replace unsafe content. Prior post-hoc editing methods often rely on imprecise spatial localization, that undermines usability and limits deployability, particularly in multi-concept scenes. To address this limitation, ReVision introduces a VLM-assisted spatial gating mechanism that enforces instance-consistent localization, enabling precise edits while preserving scene integrity. We evaluate ReVision on a 245-image benchmark covering both single- and multi-concept scenarios. Results show that ReVision (i) improves CLIP-based alignment toward safe prompts by +$0.121$ on average; (ii) significantly improves multi-concept background fidelity (LPIPS $0.166 \rightarrow 0.058$); (iii) achieves near-complete suppression on category-specific detectors (e.g., NudeNet $70.51 \rightarrow 0$); and (iv) reduces policy-violating content recognizability in a human moderation study from $95.99\%$ to $10.16\%$.

</details>


### [58] [KUDA: Knowledge Unlearning by Deviating Representation for Large Language Models](https://arxiv.org/abs/2602.19275)
*Ce Fang,Zhikun Zhang,Min Chen,Qing Liu,Lu Zhou,Zhe Liu,Yunjun Gao*

Main category: cs.CR

TL;DR: KUDA是一种新的LLM知识遗忘方法，通过因果追踪定位知识存储层，设计偏离表示的目标函数，并使用松弛零空间投影机制平衡遗忘与保留


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预训练中获取大量知识，虽然增强了生成和推理能力，但也放大了敏感、版权或有害内容的风险。现有的LLM遗忘方法往往导致模型生成随机或不连贯的答案，无法精确修改编码的知识。

Method: 提出KUDA方法：1)使用因果追踪定位目标知识存储的具体层；2)设计新的遗忘目标，诱导模型表示偏离原始位置，破坏与目标知识的关联能力；3)采用松弛零空间投影机制缓解对保留知识表示空间的干扰，解决遗忘与保留之间的优化冲突。

Result: 在WMDP和MUSE等代表性基准测试上的广泛实验表明，KUDA在有效平衡知识移除和模型效用保留方面优于大多数现有基线方法。

Conclusion: KUDA通过精确定位知识存储层、设计偏离表示的遗忘目标以及使用松弛零空间投影机制，实现了在知识层面的有效遗忘，为减少LLM中的敏感、版权或有害内容风险提供了有前景的技术方案。

Abstract: Large language models (LLMs) acquire a large amount of knowledge through pre-training on vast and diverse corpora. While this endows LLMs with strong capabilities in generation and reasoning, it amplifies risks associated with sensitive, copyrighted, or harmful content in training data.LLM unlearning, which aims to remove specific knowledge encoded within models, is a promising technique to reduce these risks. However, existing LLM unlearning methods often force LLMs to generate random or incoherent answers due to their inability to alter the encoded knowledge precisely. To achieve effective unlearning at the knowledge level of LLMs, we propose Knowledge Unlearning by Deviating representAtion (KUDA). We first utilize causal tracing to locate specific layers for target knowledge storage. We then design a new unlearning objective that induces the model's representations to deviate from its original position in the phase of knowledge removal, thus disrupting the ability to associate with the target knowledge. To resolve the optimization conflicts between forgetting and retention, we employ a relaxation null-space projection mechanism to mitigate the disruption to the representation space of retaining knowledge. Extensive experiments on representative benchmarks, WMDP and MUSE, demonstrate that KUDA outperforms most existing baselines by effectively balancing knowledge removal and model utility retention.

</details>


### [59] [Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments](https://arxiv.org/abs/2602.19450)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: 研究评估了ChatGPT-5.2和Claude Opus-4.6作为TEE安全顾问时的表现，发现存在幻觉、过度承诺等风险，并提出了TEE-RedBench评估方法和LLM-in-the-loop缓解方案


<details>
  <summary>Details</summary>
Motivation: 虽然可信执行环境(TEEs)旨在保护敏感计算，但实际部署仍面临微架构泄漏等风险。同时，安全团队越来越多地依赖LLM作为TEE安全顾问，这带来了社会技术风险：LLM可能产生幻觉、过度承诺安全保证，或在对抗性提示下表现不安全

Method: 提出了TEE-RedBench评估方法，包括：(1)针对LLM中介安全工作的TEE特定威胁模型；(2)涵盖SGX和TrustZone架构、认证密钥管理、威胁建模的结构化提示套件；(3)联合测量技术正确性、基础性、不确定性校准、拒绝质量和安全帮助性的注释标准。对ChatGPT-5.2和Claude Opus-4.6进行了红队测试

Result: 研究发现某些故障并非纯粹特异的，在LLM助手之间的可转移性高达12.02%。通过提出的"LLM-in-the-loop"评估管道（策略门控、检索基础、结构化模板、轻量级验证检查），可以将故障减少80.62%

Conclusion: LLM作为TEE安全顾问存在显著风险，需要系统性的评估和缓解措施。提出的TEE-RedBench方法和LLM-in-the-loop管道能够有效减少LLM在TEE安全咨询中的错误，提高其可靠性和安全性

Abstract: Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting.
  We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs. We introduce TEE-RedBench, a TEE-grounded evaluation methodology comprising (i) a TEE-specific threat model for LLM-mediated security work, (ii) a structured prompt suite spanning SGX and TrustZone architecture, attestation and key management, threat modeling, and non-operational mitigation guidance, along with policy-bound misuse probes, and (iii) an annotation rubric that jointly measures technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that some failures are not purely idiosyncratic, transferring up to 12.02% across LLM assistants, and we connect these outcomes to secure architecture by outlining an "LLM-in-the-loop" evaluation pipeline: policy gating, retrieval grounding, structured templates, and lightweight verification checks that, when combined, reduce failures by 80.62%.

</details>


### [60] [Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains](https://arxiv.org/abs/2602.19555)
*Xiaochong Jiang,Shiqi Yang,Wenting Yang,Yichen Liu,Cheng Ji*

Main category: cs.CR

TL;DR: 论文系统化分析了基于大语言模型的智能体系统在运行时面临的安全风险，提出了数据供应链攻击和工具供应链攻击的分类框架，并提出了零信任运行时架构解决方案。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体系统从文本生成扩展到自主检索信息和调用工具，这种运行时执行模型将攻击面从构建时工件转移到推理时依赖，使智能体面临通过不可信数据和概率能力解析进行操纵的风险。现有研究主要关注模型级漏洞，而循环和相互依赖的运行时行为产生的安全风险仍然零散。

Method: 作者在一个统一的运行时框架内系统化这些风险，将威胁分类为数据供应链攻击（瞬时上下文注入和持久内存中毒）和工具供应链攻击（发现、实现和调用）。进一步识别了"病毒智能体循环"，即智能体作为自我传播生成蠕虫的载体而无需利用代码级缺陷。最后提出零信任运行时架构，将上下文视为不可信控制流，并通过加密溯源而非语义推理来约束工具执行。

Result: 论文建立了一个系统化的运行时安全风险分类框架，识别了智能体系统特有的安全威胁模式，特别是数据供应链攻击和工具供应链攻击的具体分类，以及病毒智能体循环这一新型攻击向量。

Conclusion: 需要采用零信任运行时架构来应对智能体系统的安全挑战，将上下文视为不可信控制流，并通过加密溯源机制而非依赖语义推理来约束工具执行，从而建立更安全的智能体系统运行时环境。

Abstract: Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference.

</details>


### [61] [Efficient Multi-Party Secure Comparison over Different Domains with Preprocessing Assistance](https://arxiv.org/abs/2602.19604)
*Kaiwen Wang,Xiaolin Chang,Yuehan Dong,Ruichen Zhang*

Main category: cs.CR

TL;DR: 本文提出了首个支持n方参与、在F_p和Z_{2^k}域上实现完美安全的经销商辅助LTBits和MSB提取协议，通过充分利用经销商生成丰富相关随机性的能力，显著提升了安全比较协议的性能。


<details>
  <summary>Details</summary>
Motivation: 安全比较是多方计算中的基础原语，但现有协议的预处理阶段存在性能瓶颈。虽然近期引入了被动、非共谋的经销商来加速预处理，但现有方法存在两个关键问题：1) 将经销商作为传统预处理的简单替代，未重新设计比较协议以优化在线阶段；2) 大多数协议针对特定代数域、敌手模型或参与方配置，缺乏广泛通用性。

Method: 通过扩展的ABB模型将协议设计为黑盒构造，确保跨MPC后端和敌手模型的可移植性。充分利用经销商生成丰富相关随机性的能力，在F_p域上实现常数轮在线复杂度，在Z_{2^k}域上实现O(log_n k)轮复杂度，并具有可调分支因子。

Result: 实验结果表明，相比最先进的MPC框架，本文协议实现了1.79倍到19.4倍的加速，证明了协议在比较密集型MPC应用中的实用性。

Conclusion: 本文提出的经销商辅助LTBits和MSB提取协议在保持完美安全性的同时，显著提升了安全比较的性能，为隐私保护的机器学习和数据分析等应用提供了高效的基础原语。

Abstract: Secure comparison is a fundamental primitive in multi-party computation, supporting privacy-preserving applications such as machine learning and data analytics. A critical performance bottleneck in comparison protocols is their preprocessing phase, primarily due to the high cost of generating the necessary correlated randomness. Recent frameworks introduce a passive, non-colluding dealer to accelerate preprocessing. However, two key issues still remain. First, existing dealer-assisted approaches treat the dealer as a drop-in replacement for conventional preprocessing without redesigning the comparison protocol to optimize the online phase. Second, most protocols are specialized for particular algebraic domains, adversary models, or party configurations, lacking broad generality. In this work, we present the first dealer-assisted $n$-party LTBits (Less-Than-Bits) and MSB (Most Significant Bit) extraction protocols over both $\mathbb{F}_p$ and $\mathbb{Z}_{2^k}$, achieving perfect security at the protocol level. By fully exploiting the dealer's capability to generate rich correlated randomness, our $\mathbb{F}_p$ construction achieves constant-round online complexity and our $\mathbb{Z}_{2^k}$ construction achieves $O(\log_n k)$ rounds with tunable branching factor. All protocols are formulated as black-box constructions via an extended ABB model, ensuring portability across MPC backends and adversary models. Experimental results demonstrate $1.79\times$ to $19.4\times$ speedups over state-of-the-art MPC frameworks, highlighting the practicality of our protocols for comparison-intensive MPC applications.

</details>


### [62] [SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models](https://arxiv.org/abs/2602.19818)
*Hillel Ohayon,Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: 提出基于机器学习的轻量级扫描器，无需策略生成或代码插桩即可检测恶意Pickle文件，在多个数据集上优于现有扫描器


<details>
  <summary>Details</summary>
Motivation: Hugging Face等模型仓库使用Python pickle格式分发机器学习模型，存在远程代码执行风险。现有防御方法如PickleBall需要复杂的系统设置和已验证的良性模型，可扩展性和泛化能力有限

Method: 从Pickle字节码中静态提取结构和语义特征，应用监督和无监督模型将文件分类为良性或恶意。构建并发布包含727个Pickle文件的标注数据集

Result: 在自有数据集上达到90.01% F1分数，优于现有扫描器（7.23%-62.75%）；在PickleBall数据（OOD）上达到81.22% F1分数，优于PickleBall方法（76.09%）；能正确检测9/9个专门设计用于逃避扫描器的恶意模型

Conclusion: 数据驱动的检测方法能有效且通用地缓解基于Pickle的模型文件攻击，无需依赖特定库的策略生成

Abstract: Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks.

</details>


### [63] [Quantum approaches to learning parity with noise](https://arxiv.org/abs/2602.19819)
*Daniel Shiu*

Main category: cs.CR

TL;DR: 该论文探讨了用量子方法解决LPN问题的可能性，通过构建接近Simon承诺的函数，利用Simon算法生成新的LPN样本，为迭代简化问题提供新思路。


<details>
  <summary>Details</summary>
Motivation: LPN问题是后量子密码学（如HQC和Classic McEliece）安全性的关键基础。经典攻击方法（信息集解码）对相关参数具有指数复杂度，因此研究量子方法是否能提供替代攻击途径具有重要意义。

Method: 受Regev将格问题与隐藏二面体子群问题关联的启发，利用二进制域的邻域构造接近Simon承诺的函数，其差异等于秘密奇偶向量。运行Simon算法本质上生成新的LPN样本，从而可能通过忽略变量迭代简化问题。

Result: 论文未提供具体的实验结果，但提出了一个理论框架：通过量子方法生成新的LPN样本，为迭代简化LPN问题提供了可能性，这值得进一步深入研究。

Conclusion: 量子方法为解决LPN问题提供了新的研究方向，虽然不一定能与现有方法竞争，但值得深入探索，可能为后量子密码分析开辟新途径。

Abstract: The learning parity with noise (LPN) problem is a well-established computational challenge whose difficulty is critical to the security of several post-quantum cryptographic primitives such as HQC and Classic McEliece. Classically, the best-known attacks involve information set decoding methods which are exponential in complexity for parameterisations of interest. In this paper we investigate whether quantum methods might offer alternative approaches. The line of inquiry is inspired by Regev's relating of certain lattice problems to the hidden dihedral subgroup problem. We use neighbourhoods of binary fields to produce a function close to fulfilling Simon's promise with difference equal to the secret parity vector. Although unlikely to recover the secret parity vector directly, running Simon's algorithm essentially produces new LPN samples. This gives the hope that we might be able to produce enough new samples to ignore one or more variables and iteratively reduce the problem.
  We make no claim that these methods will necessarily be competitive with existing approaches, merely that they warrant deeper investigation.

</details>


### [64] [An Explainable Memory Forensics Approach for Malware Analysis](https://arxiv.org/abs/2602.19831)
*Silvia Lucia Sanna,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 本文提出了一种基于大语言模型的可解释AI辅助内存取证方法，用于分析恶意软件，自动提取IoC指标，并以人类可读形式解释分析结果。


<details>
  <summary>Details</summary>
Motivation: 当前内存取证虽然能有效分析采用规避、混淆、反分析和隐写技术的恶意软件，但现有自动化模型缺乏可解释性，且严重依赖专家手动分析复杂工具输出（如Volatility）。需要一种既能自动化分析又能提供可解释结果的方法。

Method: 提出基于通用大语言模型的AI辅助内存取证方法，利用LLM解释内存分析输出为人类可读形式，自动提取有意义的IoC指标。应用于Windows和Android恶意软件，比较完整RAM采集与目标进程内存转储的取证价值。采用人机协同工作流程，LLM在核心辅助设置和分析阶段提供支持。

Result: 该方法在某些情况下能检测出比当前最先进工具更多的IoC指标。LLM能够支持专家和非专家分析师解释分析结果、关联取证痕迹、证明恶意软件分类。人机协同工作流程提高了可重复性并降低了操作复杂性。

Conclusion: AI驱动的内存取证方法具有实际应用价值，能增强现代恶意软件调查的实用性。LLM辅助的人机协同工作流程改善了内存取证的效率和可访问性。

Abstract: Memory forensics is an effective methodology for analyzing living-off-the-land malware, including threats that employ evasion, obfuscation, anti-analysis, and steganographic techniques. By capturing volatile system state, memory analysis enables the recovery of transient artifacts such as decrypted payloads, executed commands, credentials, and cryptographic keys that are often inaccessible through static or traditional dynamic analysis. While several automated models have been proposed for malware detection from memory, their outputs typically lack interpretability, and memory analysis still relies heavily on expert-driven inspection of complex tool outputs, such as those produced by Volatility. In this paper, we propose an explainable, AI-assisted memory forensics approach that leverages general-purpose large language models (LLMs) to interpret memory analysis outputs in a human-readable form and to automatically extract meaningful Indicators of Compromise (IoCs), in some circumstances detecting more IoCs than current state-of-the-art tools. We apply the proposed methodology to both Windows and Android malware, comparing full RAM acquisition with target-process memory dumping and highlighting their complementary forensic value. Furthermore, we demonstrate how LLMs can support both expert and non-expert analysts by explaining analysis results, correlating artifacts, and justifying malware classifications. Finally, we show that a human-in-the-loop workflow, assisted by LLMs during kernel-assisted setup and analysis, improves reproducibility and reduces operational complexity, thereby reinforcing the practical applicability of AI-driven memory forensics for modern malware investigations.

</details>


### [65] [LLM-enabled Applications Require System-Level Threat Monitoring](https://arxiv.org/abs/2602.19844)
*Yedi Zhang,Haoyu Wang,Xianglin Yang,Jin Song Dong,Jun Sun*

Main category: cs.CR

TL;DR: 该立场论文主张对LLM应用进行系统性安全威胁监控，将其作为可靠运行的前提和事件响应框架的基础


<details>
  <summary>Details</summary>
Motivation: LLM作为核心推理组件重塑软件生态系统，但其非确定性、学习驱动和难以验证的特性引入了新的可靠性挑战并显著扩大了安全攻击面，这些风险应被视为预期操作条件而非异常事件

Method: 采用事件响应视角，主张建立系统级威胁监控机制，在部署后检测和情境化安全相关异常，超越传统的测试或护栏式防御

Result: 提出可信部署的主要障碍不是进一步提高模型能力，而是建立能够检测和情境化安全威胁的监控机制，这是当前研究中尚未充分探索的领域

Conclusion: 需要系统性和全面的LLM应用安全威胁监控作为可靠运行的前提条件，并为专门的事件响应框架奠定基础

Abstract: LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.

</details>


### [66] [Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios](https://arxiv.org/abs/2602.20061)
*Zoha Hayat Bhatti,Bakhtawar Ahtisham,Seemal Tausif,Niklas George,Nida ul Habib Bajwa,Mobin Javed*

Main category: cs.CR

TL;DR: 参与者无法可靠区分AI生成语音与真人录音，准确率仅37.5%，低于随机水平，表明当前语音诈骗场景中基于声音线索的真实性判断不可靠。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和商业语音合成系统的发展，AI生成的语音诈骗日益真实，但人们能否在真实诈骗场景中区分AI语音与真人录音，以及他们使用何种感知策略进行判断，尚不清楚。

Method: 进行在线对照研究，22名参与者评估16个语音诈骗风格音频片段（8个AI生成，8个真人录音），分类为人类或AI并报告置信度；使用信号检测理论分析区分能力，并对315个编码摘录进行定性分析。

Result: 参与者表现很差：平均准确率37.5%，低于二元分类的随机水平；75%的AI生成片段被多数标记为人类，62.5%的真人录音被多数标记为AI；信号检测分析显示区分能力接近零；定性分析显示参与者依赖副语言和情感启发式，但这些线索常被AI复制；误分类常伴随中高置信度。

Conclusion: 在当代语音诈骗场景中，基于声音启发式的真实性判断不可靠；需要重新考虑安全干预措施、用户教育和AI中介欺骗缓解策略。

Abstract: Large Language Models and commercial speech synthesis systems now enable highly realistic AI-generated voice scams (vishing), raising urgent concerns about deception at scale. Yet it remains unclear whether individuals can reliably distinguish AI-generated speech from human-recorded voices in realistic scam contexts and what perceptual strategies underlie their judgments. We conducted a controlled online study in which 22 participants evaluated 16 vishing-style audio clips (8 AI-generated, 8 human-recorded) and classified each as human or AI while reporting confidence. Participants performed poorly: mean accuracy was 37.5%, below chance in a binary classification task. At the stimulus level, misclassification was bidirectional: 75% of AI-generated clips were majority-labeled as human, while 62.5% of human-recorded clips were majority-labeled as AI. Signal Detection Theory analysis revealed near-zero discriminability (d' approx 0), indicating inability to reliably distinguish synthetic from human voices rather than simple response bias. Qualitative analysis of 315 coded excerpts revealed reliance on paralinguistic and emotional heuristics, including pauses, filler words, vocal variability, cadence, and emotional expressiveness. However, these surface-level cues traditionally associated with human authenticity were frequently replicated by AI-generated samples. Misclassifications were often accompanied by moderate to high confidence, suggesting perceptual miscalibration rather than uncertainty. Together, our findings demonstrate that authenticity judgments based on vocal heuristics are unreliable in contemporary vishing scenarios. We discuss implications for security interventions, user education, and AI-mediated deception mitigation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [67] [RPU -- A Reasoning Processing Unit](https://arxiv.org/abs/2602.18568)
*Matthew Adiletta,Gu-Yeon Wei,David Brooks*

Main category: cs.AR

TL;DR: RPU是一种基于chiplet的架构，通过容量优化的高带宽内存、可扩展的chiplet设计和解耦微架构来解决LLM推理中的内存带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理性能越来越受限于内存墙问题。GPU计算吞吐量持续增长，但在内存带宽受限的工作负载上难以提供可扩展的性能。新兴的推理LLM应用（长输出序列、低算术强度、严格延迟约束）对内存带宽需求更高，导致系统利用率下降和推理能耗上升。

Method: 提出了推理处理单元(RPU)架构：1) 容量优化的高带宽内存(HBM-CO)，以容量换取更低的能耗和成本；2) 可扩展的chiplet架构，采用带宽优先的功耗和面积分配设计；3) 解耦微架构，分离内存、计算和通信流水线以维持高带宽利用率。

Result: 模拟结果显示，在ISO-TDP条件下，RPU在Llama3-405B上相比H100系统实现了高达45.3倍的延迟降低和18.6倍的吞吐量提升。

Conclusion: RPU架构通过专门针对内存带宽优化的设计，有效解决了LLM推理中的内存墙问题，显著提升了推理性能和能效。

Abstract: Large language model (LLM) inference performance is increasingly bottlenecked by the memory wall. While GPUs continue to scale raw compute throughput, they struggle to deliver scalable performance for memory bandwidth bound workloads. This challenge is amplified by emerging reasoning LLM applications, where long output sequences, low arithmetic intensity, and tight latency constraints demand significantly higher memory bandwidth. As a result, system utilization drops and energy per inference rises, highlighting the need for an optimized system architecture for scalable memory bandwidth.
  To address these challenges we present the Reasoning Processing Unit (RPU), a chiplet-based architecture designed to address the challenges of the modern memory wall. RPU introduces: (1) A Capacity-Optimized High-Bandwidth Memory (HBM-CO) that trades capacity for lower energy and cost; (2) a scalable chiplet architecture featuring a bandwidth-first power and area provisioning design; and (3) a decoupled microarchitecture that separates memory, compute, and communication pipelines to sustain high bandwidth utilization. Simulation results show that RPU performs up to 45.3x lower latency and 18.6x higher throughput over an H100 system at ISO-TDP on Llama3-405B.

</details>


### [68] [HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD](https://arxiv.org/abs/2602.18750)
*He Sun,Li Li,Mingjun Xiao*

Main category: cs.AR

TL;DR: HillInfer：一种面向边缘设备的长上下文LLM推理框架，通过SmartSSD辅助的分层KV缓存管理，在PC等边缘设备上实现高效的长上下文推理，速度提升最高达8.56倍。


<details>
  <summary>Details</summary>
Motivation: 在PC等边缘设备上部署大语言模型可实现低延迟推理和强隐私保护，但长上下文推理受限于有限的内存和计算资源。KV缓存随上下文长度线性增长成为主要瓶颈，现有方法主要针对内存丰富的平台，在资源受限的边缘设备上应用时会产生过高的数据传输开销。

Method: 提出HillInfer框架，采用SmartSSD辅助的分层KV缓存管理：1）联合管理CPU和SmartSSD上的KV缓存池；2）在存储内部进行重要性评估以减少不必要的数据传输；3）设计自适应、基于预取的流水线，在GPU、CPU和SmartSSD之间重叠计算和KV数据传输。

Result: 在配备商用GPU的PC上实现HillInfer，在多个模型和基准测试中，相比基线方法实现最高8.56倍的加速，同时保持模型精度不变。

Conclusion: HillInfer通过SmartSSD辅助的分层KV缓存管理和自适应流水线，有效解决了边缘设备上长上下文LLM推理的内存和计算瓶颈，显著提升了推理性能而不损失精度。

Abstract: Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\times$ speedup over baselines while preserving model accuracy.

</details>


### [69] [pHNSW: PCA-Based Filtering to Accelerate HNSW Approximate Nearest Neighbor Search](https://arxiv.org/abs/2602.19242)
*Zheng Li,Guangyi Zeng,Paul Delestrac,Enyi Yao,Simei Yang*

Main category: cs.AR

TL;DR: pHNSW通过算法-硬件协同优化加速HNSW，采用PCA降维减少数据访问和计算负载，设计专用处理器提升搜索吞吐量和能效，相比标准HNSW实现QPS提升5.37x-21.37x，能耗降低达57.4%。


<details>
  <summary>Details</summary>
Motivation: HNSW在高维最近邻搜索中表现出色，但其高计算需求和大量不规则数据访问模式严重影响了搜索效率，需要优化解决方案。

Method: 提出pHNSW算法-硬件协同优化方案：算法层面使用PCA降维过滤减少数据集维度，降低邻居访问量和距离计算负载；硬件层面设计pHNSW处理器，采用定制指令优化搜索吞吐量和能效。

Result: 在65nm工艺节点上合成pHNSW处理器RTL设计，使用DDR4和HBM1.0 DRAM标准评估。结果显示：相比标准HNSW，pHNSW在CPU上提升QPS 14.47x-21.37x，在GPU上提升5.37x-8.46x，同时能耗降低达57.4%。

Conclusion: pHNSW通过算法-硬件协同优化有效解决了HNSW的计算和访问瓶颈，显著提升了搜索性能和能效，为高维最近邻搜索提供了高效的解决方案。

Abstract: Hierarchical Navigable Small World (HNSW) has demonstrated impressive accuracy and low latency for high-dimensional nearest neighbor searches. However, its high computational demands and irregular, large-volume data access patterns present significant challenges to search efficiency. To address these challenges, we introduce pHNSW, an algorithm-hardware co-optimized solution that accelerates HNSW through Principal Component Analysis (PCA) filtering. On the algorithm side, we apply PCA filtering to reduce the dimensionality of the dataset, thereby lowering the volume of neighbor access and decreasing the computational load for distance calculations. On the hardware side, we design the pHNSW processor with custom instructions to optimize search throughput and energy efficiency. In the experiments, we synthesized the pHNSW processor RTL design with a 65nm technology node and evaluated it using DDR4 and HBM1.0 DRAM standards. The results show that pHNSW boosts Queries per Second (QPS) by 14.47x-21.37x on a CPU and 5.37x-8.46x on a GPU, while reducing energy consumption by up to 57.4% compared to standard HNSW implementation.

</details>


### [70] [CORVET: A CORDIC-Powered, Resource-Frugal Mixed-Precision Vector Processing Engine for High-Throughput AIoT applications](https://arxiv.org/abs/2602.19268)
*Sonu Kumar,Mohd Faisal Khan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 提出一种运行时自适应、性能增强的向量引擎，采用低资源迭代CORDIC-based MAC单元，支持动态精度模式切换，在边缘AI加速中实现高能效计算。


<details>
  <summary>Details</summary>
Motivation: 针对边缘AI应用需要高能效、低资源消耗的计算加速器，现有设计在精度、延迟和资源效率之间存在权衡问题，需要一种能够动态适应不同工作负载的灵活架构。

Method: 设计基于迭代CORDIC的低资源MAC单元，支持动态重配置近似和精确模式；采用向量化时间复用执行和灵活精度缩放；包含时间复用多激活函数块和轻量级池化归一化单元，支持4/8/16位灵活精度。

Result: ASIC实现显示每个MAC阶段可节省33%时间和21%功耗；256-PE配置达到4.83 TOPS/mm²计算密度和11.67 TOPS/W能效，优于现有技术；在Pynq-Z2上通过软硬件协同设计验证了目标检测和分类任务。

Conclusion: 该向量引擎通过运行时自适应、低资源CORDIC-based MAC单元和灵活精度支持，为边缘AI应用提供了可扩展、高能效的解决方案，在计算密度和能效方面优于现有技术。

Abstract: This brief presents a runtime-adaptive, performance-enhanced vector engine featuring a low-resource, iterative CORDIC-based MAC unit for edge AI acceleration. The proposed design enables dynamic reconfiguration between approximate and accurate modes, exploiting the latency-accuracy trade-off for a wide range of workloads. Its resource-efficient approach further enables up to 4x throughput improvement within the same hardware resources by leveraging vectorised, time-multiplexed execution and flexible precision scaling. With a time-multiplexed multi-AF block and a lightweight pooling and normalisation unit, the proposed vector engine supports flexible precision (4/8/16-bit) and high MAC density. The ASIC implementation results show that each MAC stage can save up to 33% of time and 21% of power, with a 256-PE configuration that achieves higher compute density (4.83 TOPS/mm2 ) and energy efficiency (11.67 TOPS/W) than previous state-of-the-art work. A detailed hardware-software co-design methodology for object detection and classification tasks on Pynq-Z2 is discussed to assess the proposed architecture, demonstrating a scalable, energy-efficient solution for edge AI applications.

</details>


### [71] [Closed-Loop Environmental Control System on Embedded Systems](https://arxiv.org/abs/2602.19305)
*Irisha M. Goswami,D. G. Perera*

Main category: cs.AR

TL;DR: 设计并验证了一个用于小型农业应用的闭环环境控制系统，使用Nuvoton NUC140微控制器实现低成本、安全关键的嵌入式温度自动调节方案


<details>
  <summary>Details</summary>
Motivation: 解决温室中环境波动导致的作物产量损失问题，需要为小型农业应用开发低成本、安全关键的环境控制系统

Method: 使用Nuvoton NUC140微控制器构建嵌入式系统，采用PID控制回路实现温度调节，并通过电隔离确保硬件安全

Result: 最终实现的系统成功满足所有设计规范，展示了通过PID控制回路的稳健温度调节能力，并通过电隔离确保了硬件安全性

Conclusion: 成功设计、构建并验证了一个适用于小型农业应用的闭环环境控制系统，能够有效调节温室温度并确保硬件安全

Abstract: In this paper, our objective is to design, build, and verify a closed-loop environmental control system tailored for small-scale agriculture applications. This project aims to develop a low-cost, safety-critical embedded solution using the Nuvoton NUC140 microcontroller to automate temperature regulation. The goal was to mitigate crop yield losses caused by environmental fluctuations in a greenhouse. Our final implemented system successfully meets all design specifications, demonstrating robust temperature regulation through a PID control loop and ensuring hardware safety through galvanic isolation

</details>


### [72] [Interconnect-Aware Logic Resynthesis for Multi-Die FPGAs](https://arxiv.org/abs/2602.19720)
*Xiaoke Wang,Raveena Raikar,Markus Rein,Ruiqi Chen,Chang Meng,Dirk Stroobandt*

Main category: cs.AR

TL;DR: 针对多芯片FPGA中跨芯片互连开销大的问题，提出了一种基于逻辑重综合的方法，通过重构LUT级网表来减少超长线(SLL)数量，从而降低延迟并缓解互连资源压力。


<details>
  <summary>Details</summary>
Motivation: 多芯片FPGA能够突破光罩限制实现器件扩展，但跨芯片边界引入了严重的互连开销。超长线(SLL)具有高延迟特性，消耗稀缺的互连层资源，通常主导关键路径并增加物理设计复杂度。

Method: 提出了一种互连感知的逻辑重综合方法，利用芯片分区信息对LUT级网表进行逻辑重替换，简化局部电路结构并消除超长线。该方法在设计流程早期（物理实现前）减少SLL数量，并与打包和布局工具集成构建完整工具流。

Result: 在EPFL基准测试中，相比最先进框架，2芯片FPGA的SLL数量减少高达24.8%，3芯片FPGA减少高达27.38%。在MCNC基准测试中，平均SLL减少1.65%同时保持布局质量。在Koios基准测试中，多个设计仍显示出显著的跨芯片边减少。

Conclusion: 在逻辑层面减少跨芯片连接是多芯片FPGA的有效方法，能够缩短关键路径、缓解互连资源压力并提高物理设计灵活性。

Abstract: Multi-die FPGAs enable device scaling beyond reticle limits but introduce severe interconnect overhead across die boundaries. Inter-die connections, commonly referred to as super-long lines (SLLs), incur high delay and consume scarce interposer interconnect resources, often dominating critical paths and complicating physical design. To address this, this work proposes an interconnect-aware logic resynthesis method that restructures the LUT-level netlist to reduce the number of SLLs. The resynthesis engine uses die partitioning information to apply logic resubstitutions, which simplifies local circuit structures and eliminates SLLs. By reducing the number of SLLs early in the design flow, prior to physical implementation, the proposed method shortens critical paths, alleviates pressure on scarce interposer interconnect resources, and improves overall physical design flexibility. We further build a tool flow for multi-die FPGAs by integrating the proposed resynthesis method with packing and placement. Experimental results on the EPFL benchmarks show that, compared with a state-of-the-art framework, the proposed method reduces the number of SLLs by up to 24.8% for a 2-die FPGA and up to 27.38% for a 3-die FPGA. On MCNC benchmarks, our tool flow achieves an average SLL reduction of 1.65% while preserving placement quality. On Koios benchmarks, where fewer removable SLLs exist, several designs still exhibit considerable inter-die edge reductions. Overall, the results confirm that reducing inter-die connections at the logic level is an effective approach for multi-die FPGAs.

</details>


### [73] [Extending CPU-less parallel execution of lambda calculus in digital logic with lists and arithmetic](https://arxiv.org/abs/2602.19884)
*Harry Fitchett,Jasmine Ritchie,Charles Fox*

Main category: cs.AR

TL;DR: 该研究在之前CPU-less函数式程序执行的基础上，通过为lambda演算添加算术和列表等基本原语，显著提升了执行效率和节点使用率，使该技术向实际应用迈进了一步。


<details>
  <summary>Details</summary>
Motivation: 计算机架构正在寻找利用日益丰富的数字逻辑的新方法，避免基于CPU设计的串行瓶颈。虽然已有研究通过将函数式程序直接编译为并行数字逻辑实现了完全无CPU的执行，但基于纯lambda演算的方法在实际编程中效率低下，需要扩展基本原语来提升实用性。

Method: 在之前研究的基础上，为基本的lambda演算添加算术和列表等常用原语。提出了一套表示新原语的结构和算法，描述了一个系统化的选择、实现和评估过程，并在开源系统中实现了这些改进。

Result: 实现了执行时间和节点使用率的显著减少。开源系统能够正确评估一系列代表性的lambda表达式，证明了该方法的有效性。

Conclusion: 通过扩展lambda演算的基本原语，CPU-less函数式程序执行在数字逻辑中的方法得到了实质性改进，向实际应用迈出了重要一步。

Abstract: Computer architecture is searching for new ways to make use of increasingly available digital logic without the serial bottlenecks of CPU-based design. Recent work has demonstrated a fully CPU-less approach to executing functional programs, by exploiting their inherent parallelisability to compile them directly into parallel digital logic. This work uses lambda-calculus as a hyper simple functional language to prove the concept, but is impractical for real-world programming due to the well-known inefficiencies of pure lambda$-calculus. It is common in language design to extend basic lambda-calculus with additional primitives to short-cut common tasks such as arithmetic and lists. In this work, we build upon our previous research to examine how such extensions may be applied to CPU-less functional execution in digital logic, with the objective of advancing the approach toward practical implementation. We present a set of structures and algorithms for representing new primitives, describe a systematic process for selecting, implementing, and evaluating them, and demonstrate substantial reductions in execution time and node usage. These improvements are implemented in an open-source system, which is shown to correctly evaluate a range of representative lambda expressions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [74] [The Category Mistake of Cislunar Time: Why NASA Cannot Synchronize What Doesn't Exist](https://arxiv.org/abs/2602.18641)
*Paul Borrill*

Main category: cs.DC

TL;DR: 论文批判美国白宫2024年4月指令NASA在2026年前建立月球协调时间(LTC)的计划，认为该计划基于哲学上的范畴错误，将"同步时间"当作本体实体而非认识论建构。


<details>
  <summary>Details</summary>
Motivation: 作者旨在揭示月球时间计划背后的哲学混淆：该计划错误地将时间同步视为可以传输的本体实体，而实际上时间只是依赖于观察者和模型的认知建构。

Method: 使用Ryle和Spekkens的范畴错误理论、FITO假设、Spekkens的莱布尼茨操作主义、Wood-Spekkens微调论证，以及量子力学中的本体与认识论区分等概念框架来分析月球时间计划。

Result: 分析表明，月球时间计划与量子力学中的"神秘"问题类似，都是混淆了本体与认识论范畴。一旦认识到时间同步是认识论建构而非本体实体，该计划的表面一致性就会瓦解。

Conclusion: 月球时间计划是基于哲学混淆的工程项目。作者提出基于双向原子相互作用而非单向时间分发的交易性替代方案，强调时间关系应通过原子钟之间的相互作用建立，而非从权威源传输。

Abstract: In April 2024, the White House directed NASA to establish Coordinated Lunar Time (LTC) by December 2026. The programme assumes that a unified time standard can be constructed by deploying atomic clocks on the lunar surface, computing relativistic corrections, and distributing synchronized time via LunaNet. This paper argues that the entire enterprise rests on a category mistake in the sense introduced by Ryle and developed by Spekkens in quantum foundations: it treats "synchronized time" as an ontic entity -- something that exists independently and can be transmitted from authoritative sources to dependent receivers -- when it is in fact an epistemic construct: a model-dependent representation of observer-relative clock relationships. We analyze the cislunar time programme through the lens of Forward-In-Time-Only (FITO) assumptions, Spekkens' Leibnizian operationalism, the Wood-Spekkens fine-tuning argument, and the distinction between ontic and epistemic interpretations that has dissolved long-standing puzzles in quantum mechanics. We show that the same conceptual move that dissolves quantum "mysteries" -- recognizing what is epistemic versus what is ontic -- dissolves the apparent coherence of the cislunar time programme and reveals it as an engineering project built on a philosophical confusion. We sketch a transactional alternative grounded in bilateral atomic interactions rather than unidirectional time distribution.

</details>


### [75] [BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS](https://arxiv.org/abs/2602.18755)
*Omar Basit,Yunzhao Liu,Z. Jonny Kong,Y. Charlie Hu*

Main category: cs.DC

TL;DR: BiScale是一个用于解耦LLM服务的双层能量优化框架，通过联合优化预填充和解码阶段的放置和DVFS，在满足SLO约束的同时显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: LLM推理能耗巨大，现有的自动扩展方法粒度太粗无法跟踪快速工作负载波动，而解耦架构下的细粒度DVFS应用又受到相位不对称动态和资源配置与频率控制耦合的复杂性影响。

Method: BiScale采用双层优化框架：在粗粒度时间尺度上计算相位感知的放置和基准频率以最小化能耗；在细粒度时间尺度上，对预填充阶段使用模型预测控制（MPC）考虑队列演化和未来TTFT影响，对解码阶段使用轻量级松弛感知适应来利用其更平滑的内存受限动态。

Result: 在16个H100集群上服务Llama 3.3 70B并使用生产风格跟踪的评估显示，BiScale在满足TTFT/TPOT SLOs的同时，相比DistServe在预填充阶段减少能耗高达39%，在解码阶段减少48%。

Conclusion: BiScale通过分层设计实现了跨时间尺度的协调控制，在保持严格服务SLOs的同时显著降低了LLM推理的能耗，为解决解耦LLM服务中的能量优化问题提供了有效方案。

Abstract: Prefill/decode disaggregation is increasingly adopted in LLM serving to improve the latency-throughput tradeoff and meet strict TTFT and TPOT SLOs. However, LLM inference remains energy-hungry: autoscaling alone is too coarse-grained to track fast workload fluctuations, and applying fine-grained DVFS under disaggregation is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control.
  We present BiScale, a two-tier energy optimization framework for disaggregated LLM serving. BiScale jointly optimizes placement and DVFS across prefill and decode using predictive latency and power models. At coarse timescales, BiScale computes phase-aware placement and baseline frequencies that minimize energy while satisfying SLO constraints. At fine timescales, BiScale dynamically adapts GPU frequency per iteration using stage-specific control: model predictive control (MPC) for prefill to account for queue evolution and future TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics. This hierarchical design enables coordinated control across timescales while preserving strict serving SLOs.
  Evaluation on a 16x H100 cluster serving Llama 3.3 70B with production-style traces shows that BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe.

</details>


### [76] [WANSpec: Leveraging Global Compute Capacity for LLM Inference](https://arxiv.org/abs/2602.18931)
*Noah Martin,Fahad Dogar*

Main category: cs.DC

TL;DR: WANSpec通过将推测解码的草稿模型卸载到利用率较低的数据中心，减少高需求数据中心的计算负载，同时保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 全球数据中心GPU资源分布不均，高端GPU需求旺盛导致负载不均衡，影响LLM推理延迟。低利用率数据中心资源未被充分利用。

Method: 提出WANSpec系统，将推测解码中的草稿模型部署到利用率较低的数据中心，利用冗余策略避免延迟增加，有效利用现场计算资源。

Result: 实验表明WANSpec能将高需求数据中心的草稿模型前向传播减少50%以上，同时保持低延迟，并能有效利用大学等现场计算资源。

Conclusion: WANSpec通过智能分配计算资源，平衡数据中心负载，提高LLM推理效率，为分布式LLM推理提供了可行方案。

Abstract: Data centers capable of running large language models (LLMs) are spread across the globe. Some have high end GPUs for running the most advanced models (100B+ parameters), and others are only suitable for smaller models (1B parameters). The most capable GPUs are under high demand thanks to the rapidly expanding applications of LLMs. Choosing the right location to run an LLM inference workload can have consequences on the latency of requests due to these high demands. In this work, we explore options to shift some aspects of inference to the under-utilized data centers. We first observe the varying delays affecting inference in AWS services from different regions, demonstrating that load is not spread evenly. We then introduce WANSpec, which offloads part of LLM generation to the under-utilized data centers. In doing so, WANSpec can mitigate capacity issues as well as effectively use on-site compute (ie at universities) to augment cloud providers. This is done with speculative decoding, a widely used technique to speed up auto-regressive decoding, by moving the draft model to the under-utilized compute resources. Our experiments in simulation and cloud deployments show that WANSpec can judiciously employ redundancy to avoid increases in latency while still reducing the forward passes of speculative decoding's draft model in high demand data centers by over 50%.

</details>


### [77] [ucTrace: A Multi-Layer Profiling Tool for UCX-driven Communication](https://arxiv.org/abs/2602.19084)
*Emir Gencer,Mohammad Kefah Taha Issa,Ilyas Turimbetov,James D. Trotter,Didem Unat*

Main category: cs.DC

TL;DR: ucTrace是一个新的性能分析工具，专门用于分析和可视化HPC环境中基于UCX的通信，填补了现有工具在UCX层面细粒度通信追踪的空白。


<details>
  <summary>Details</summary>
Motivation: 现有性能分析工具缺乏UCX层面的细粒度通信追踪，无法捕获传输层行为，或仅限于特定MPI实现，这限制了系统管理员和开发人员优化HPC通信性能的能力。

Method: 开发了ucTrace分析器，通过在UCX层面分析消息传递，将主机和设备（如GPU和NIC）之间的操作直接关联到其源MPI函数，并提供交互式可视化。

Result: ucTrace能够分析MPI点对点行为、不同MPI库的Allreduce比较、线性求解器的通信分析、NUMA绑定效应，以及大规模GPU加速的GROMACS分子动力学模拟。

Conclusion: ucTrace为系统管理员、库和应用开发者提供了优化性能和调试大规模工作负载通信模式的强大工具，填补了HPC通信分析的重要空白。

Abstract: UCX is a communication framework that enables low-latency, high-bandwidth communication in HPC systems. With its unified API, UCX facilitates efficient data transfers across multi-node CPU-GPU clusters. UCX is widely used as the transport layer for MPI, particularly in GPU-aware implementations. However, existing profiling tools lack fine-grained communication traces at the UCX level, do not capture transport-layer behavior, or are limited to specific MPI implementations.
  To address these gaps, we introduce ucTrace, a novel profiler that exposes and visualizes UCX-driven communication in HPC environments. ucTrace provides insights into MPI workflows by profiling message passing at the UCX level, linking operations between hosts and devices (e.g., GPUs and NICs) directly to their originating MPI functions. Through interactive visualizations of process- and device-specific interactions, ucTrace helps system administrators, library and application developers optimize performance and debug communication patterns in large-scale workloads. We demonstrate ucTrace's features through a wide range of experiments including MPI point-to-point behavior under different UCX settings, Allreduce comparisons across MPI libraries, communication analysis of a linear solver, NUMA binding effects, and profiling of GROMACS MD simulations with GPU acceleration at scale. ucTrace is publicly available at https://github.com/ParCoreLab/ucTrace.

</details>


### [78] [A Formal Framework for Predicting Distributed System Performance under Faults](https://arxiv.org/abs/2602.19088)
*Ziwei Zhou,Si Liu,Zhou Zhou,Peixin Wang,MIn Zhang*

Main category: cs.DC

TL;DR: PERF框架：首个用于分布式系统在故障环境下性能预测的形式化框架，通过故障注入和模型组合实现吞吐量、延迟等性能指标的统计分析


<details>
  <summary>Details</summary>
Motivation: 分布式系统在复杂环境中运行，不可避免地涉及故障甚至对抗行为，从形式化设计直接预测系统性能一直是一个长期挑战

Method: 提出一个形式化框架，包含故障注入器和多种故障类型（可作为库重用），通过模型组合将系统和故障注入器集成到统一模型中，适合进行吞吐量和延迟等性能属性的统计分析；在Maude中形式化该框架并实现为自动化工具PERF

Result: 应用于代表性分布式系统时，PERF能够准确预测不同故障设置下的系统性能，形式化设计的估计结果与实际部署的评估结果一致

Conclusion: PERF是首个能够系统地在多样化故障场景下预测分布式系统性能的形式化框架，实现了从形式化设计到实际性能预测的有效桥梁

Abstract: Today's distributed systems operate in complex environments that inevitably involve faults and even adversarial behaviors. Predicting their performance under such environments directly from formal designs remains a longstanding challenge. We present the first formal framework that systematically enables performance prediction of distributed systems across diverse faulty scenarios. Our framework features a fault injector together with a wide range of faults, reusable as a library, and model compositions that integrate the system and the fault injector into a unified model suitable for statistical analysis of performance properties such as throughput and latency. We formalize the framework in Maude and implement it as an automated tool, PERF. Applied to representative distributed systems, PERF accurately predicts system performance under varying fault settings, with estimations from formal designs consistent with evaluations on real deployments.

</details>


### [79] [Semantic Conflict Model for Collaborative Data Structures](https://arxiv.org/abs/2602.19231)
*Georgii Semenov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文提出了一种用于协作数据结构的冲突模型，支持显式的、本地优先的冲突解决，无需中央协调，通过语义依赖识别冲突，并使用三路合并进行解决。


<details>
  <summary>Details</summary>
Motivation: 现有协作系统中，CRDTs的冲突解决通常是隐式和不透明的，而现有协调技术往往依赖集中式协调。需要一种支持显式、本地优先冲突解决的方法。

Method: 通过语义依赖识别操作间的冲突，使用三路合并将冲突操作重新基于协调操作，基于复制的日志实现冲突解决。

Result: 在协作寄存器上演示了该方法，包括显式表述的Last-Writer-Wins寄存器和支持半自动协调的多寄存器实体。

Conclusion: 该模型为协作数据结构提供了显式、本地优先的冲突解决机制，避免了中央协调的需求，提高了冲突解决的透明度和用户控制能力。

Abstract: Digital collaboration systems support asynchronous work over replicated data, where conflicts arise when concurrent operations cannot be unambiguously integrated into a shared history. While Conflict-Free Replicated Data Types (CRDTs) ensure convergence through built-in conflict resolution, this resolution is typically implicit and opaque to users, whereas existing reconciliation techniques often rely on centralized coordination. This paper introduces a conflict model for collaborative data structures that enables explicit, local-first conflict resolution without central coordination. The model identifies conflicts using semantic dependencies between operations and resolves them by rebasing conflicting operations onto a reconciling operation via a three-way merge over a replicated journal. We demonstrate our approach on collaborative registers, including an explicit formulation of the Last-Writer-Wins Register and a multi-register entity supporting semi-automatic reconciliation.

</details>


### [80] [Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement](https://arxiv.org/abs/2602.19338)
*Halit Uyanık,Tolga Ovatman*

Main category: cs.DC

TL;DR: 该研究提出了一种基于约束编程优化的方法，用于平衡物联网环境中复杂事件处理任务图的执行成本，通过优化关键路径性能来提高吞吐量和减少延迟。


<details>
  <summary>Details</summary>
Motivation: 物联网设备硬件和计算能力有限，但需要处理日益多样化的输入数据和复杂任务。复杂事件处理涉及从多个源读取和聚合数据以推断重要事件，需要解决方案来应对边缘设备的限制。

Method: 采用约束编程优化方法平衡CEP任务图中不同路径的执行成本，优化关键路径性能。实现为Python库，允许小规模物联网设备自适应优化代码和I/O分配，抽象化通信细节并实现设备间共享内存虚拟化。

Result: 优化关键路径性能在CEP操作中提高了多个设备间的吞吐量并减少了延迟。实现的库能够有效改善整体延迟和吞吐量表现。

Conclusion: 提出的约束编程优化方法能够有效平衡物联网环境中复杂事件处理任务图的执行成本，通过优化关键路径性能显著改善系统性能，为资源受限的物联网设备提供了可行的CEP优化解决方案。

Abstract: The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations.

</details>


### [81] [GPU-Resident Gaussian Process Regression Leveraging Asynchronous Tasks with HPX](https://arxiv.org/abs/2602.19683)
*Henrik Möllmann,Dirk Pflüger,Alexander Strack*

Main category: cs.DC

TL;DR: 该论文扩展了GPRat库，通过实现完全GPU驻留的高斯过程预测管道来解决传统GP回归计算复杂度高的问题，利用CUDA库和HPX任务并行性获得了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 高斯过程（GPs）是广泛使用的回归工具，但精确求解器的立方复杂度限制了其可扩展性。需要解决GP回归在大规模数据集上的计算效率问题。

Method: 扩展GPRat库，实现完全GPU驻留的GP预测管道。使用优化的CUDA库实现分块算法，利用大规模并行性进行线性代数运算。结合HPX任务并行性和多个CUDA流，评估最优CUDA流数量。

Result: GPU实现在训练样本超过128个的数据集上提供加速效果：Cholesky分解本身加速达4.3倍，GP预测加速达4.6倍。结合HPX和多个CUDA流，GPRat能够匹配并在大数据集上超越cuSOLVER性能达11%。

Conclusion: 通过GPU加速和任务并行性的结合，成功解决了GP回归的可扩展性问题，为大规模数据集上的高斯过程预测提供了高效的计算解决方案。

Abstract: Gaussian processes (GPs) are a widely used regression tool, but the cubic complexity of exact solvers limits their scalability. To address this challenge, we extend the GPRat library by incorporating a fully GPU-resident GP prediction pipeline. GPRat is an HPX-based library that combines task-based parallelism with an intuitive Python API.
  We implement tiled algorithms for the GP prediction using optimized CUDA libraries, thereby exploiting massive parallelism for linear algebra operations. We evaluate the optimal number of CUDA streams and compare the performance of our GPU implementation to the existing CPU-based implementation. Our results show the GPU implementation provides speedups for datasets larger than 128 training samples. We observe speedups of up to 4.3 for the Cholesky decomposition itself and 4.6 for the GP prediction. Furthermore, combining HPX with multiple CUDA streams allows GPRat to match, and for large datasets, surpass cuSOLVER's performance by up to 11 percent.

</details>


### [82] [A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response](https://arxiv.org/abs/2602.19742)
*Yulun Huang,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出无人机辅助边缘计算框架，通过联合优化路径规划、机队规模和边缘服务配置，显著降低火灾监测响应时间、能耗和机队规模，并具备紧急重路由机制。


<details>
  <summary>Details</summary>
Motivation: 野火监测需要及时的数据收集和处理以实现早期检测和快速响应。无人机辅助边缘计算是一种有前景的方法，但联合最小化端到端服务响应时间同时满足能量、重访时间和容量约束仍然具有挑战性。

Method: 提出集成框架，联合优化无人机路径规划、机队规模和边缘服务配置。包括：基于火灾历史加权的聚类优先考虑高风险区域；QoS感知的边缘分配平衡邻近性和计算负载；2-opt路径优化与自适应机队规模调整；动态紧急重路由机制。

Result: 实验显示，相比GA、PSO和贪婪基线方法，该框架将平均响应时间降低70.6-84.2%，能耗降低73.8-88.4%，机队规模减少26.7-42.1%。紧急机制在233秒内响应，远低于300秒截止时间，对正常操作影响可忽略。

Conclusion: 该集成框架通过协同优化多个相互依赖的子问题，显著提升了野火监测系统的性能，证明了联合优化路径规划、机队规模和边缘服务配置的有效性，为实时野火监测提供了实用解决方案。

Abstract: Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging. We propose an integrated framework that co-optimizes UAV route planning, fleet sizing, and edge service provisioning for wildfire monitoring. The framework combines fire-history-weighted clustering to prioritize high-risk areas, Quality of Service (QoS)-aware edge assignment balancing proximity and computational load, 2-opt route optimization with adaptive fleet sizing, and a dynamic emergency rerouting mechanism. The key insight is that these subproblems are interdependent: clustering decisions simultaneously shape patrol efficiency and edge workloads, while capacity constraints feed back into feasible configurations. Experiments show that the proposed framework reduces average response time by 70.6--84.2%, energy consumption by 73.8--88.4%, and fleet size by 26.7--42.1% compared to GA, PSO, and greedy baselines. The emergency mechanism responds within 233 seconds, well under the 300-second deadline, with negligible impact on normal operations.

</details>


### [83] [Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation](https://arxiv.org/abs/2602.20097)
*Pu Jiao,Sheng Di,Jiannan Tian,Mingze Xia,Xuan Wu,Yang Zhang,Xin Liang,Franck Cappello*

Main category: cs.DC

TL;DR: 本文针对预量化压缩器在中等或大误差边界下数据质量低的问题，提出了一种量化感知插值算法来减轻压缩伪影，在保持高压缩吞吐量的同时提高解压数据质量。


<details>
  <summary>Details</summary>
Motivation: 预量化压缩器虽然具有极高的吞吐量，但在中等或大用户指定误差边界下通常数据质量较低。本文旨在研究预量化压缩器产生的伪影，并提出算法来缓解这些问题。

Method: 1. 仔细分析预量化压缩器中的伪影，理解量化索引与压缩误差之间的相关性；2. 提出新颖的量化感知插值算法来改进解压数据；3. 在共享内存和分布式内存环境中并行化算法以获得高性能；4. 使用五个真实世界数据集评估算法并验证其有效性。

Result: 实验表明，该伪影缓解算法能够有效提高预量化压缩器产生的解压数据质量，同时保持其高压缩吞吐量。

Conclusion: 本文提出的量化感知插值算法成功解决了预量化压缩器在中等或大误差边界下的数据质量问题，为高吞吐量误差控制数据压缩提供了有效的质量改进方案。

Abstract: Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or large user-specified error bounds. In this paper, we investigate the artifacts generated by pre-quantization based compressors and propose a novel algorithm to mitigate them. Our contributions are fourfold: (1) We carefully characterize the artifacts in pre-quantization based compressors to understand the correlation between the quantization index and compression error; (2) We propose a novel quantization-aware interpolation algorithm to improve the decompressed data; (3) We parallelize our algorithm in both shared-memory and distributed-memory environments to obtain high performance; (4) We evaluate our algorithm and validate it with two leading pre-quantization based compressors using five real-world datasets. Experiments demonstrate that our artifact mitigation algorithm can effectively improve the quality of decompressed data produced by pre-quantization based compressors while maintaining their high compression throughput.

</details>
