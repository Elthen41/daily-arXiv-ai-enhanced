<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs](https://arxiv.org/abs/2512.14741)
*Jing Cui,Yufei Han,Jianbin Jiao,Junge Zhang*

Main category: cs.CR

TL;DR: P-Trojan是一种针对大语言模型的后门攻击方法，通过优化后门在多次更新中的持久性，确保即使在用户驱动的持续微调后仍能保持攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究主要关注模型发布时的攻击效果，而忽略了后门在用户部署后持续微调过程中的持久性问题。实际证据表明，简单注入的后门在模型更新后效果会下降，因此需要研究后门在多阶段部署后微调中的持久性机制。

Method: 提出P-Trojan攻击算法，通过将中毒梯度与干净任务在词嵌入空间中的梯度对齐，使植入的后门映射在后续更新中不易被抑制或遗忘。该方法明确优化后门在重复更新中的持久性。

Result: 理论分析证明了持续后门攻击的可行性。在Qwen2.5和LLaMA3系列LLM以及多样化任务序列上的实验表明，P-Trojan实现了超过99%的后门持久性，同时保持了干净任务的准确性。

Conclusion: 研究结果表明，在实际模型适应流程中需要进行持久性感知的评估和更强的防御措施，因为后门攻击可以在多次模型更新后仍然保持有效。

Abstract: Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.

</details>


### [2] [Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation](https://arxiv.org/abs/2512.14767)
*Unai Laskurain,Aitor Aguirre-Ortuzar,Urko Zurutuza*

Main category: cs.CR

TL;DR: 提出了一种用于纵向联邦学习的隐私保护Shapley-CMI特征贡献评估系统，通过私有集合交集服务器安全计算特征排列和交集大小，无需原始数据交换或模型训练。


<details>
  <summary>Details</summary>
Motivation: 在纵向联邦学习中，各参与方拥有相同用户的不同特征，需要在模型训练前评估各方的特征贡献。现有的Shapley-CMI方法虽然提出了模型无关的信息论特征估值方法，但缺乏能够安全计算所需排列和交集的实用实现方案。

Method: 提出了一种隐私保护的Shapley-CMI实现系统，引入私有集合交集服务器，在离散化和加密的ID组上执行所有必要的特征排列并计算加密的交集大小，无需原始数据交换。各方使用这些交集结果计算Shapley-CMI值，评估其特征的边际效用。

Result: 初步实验证实了所提系统的正确性和隐私性，证明了其在纵向联邦学习中安全高效特征贡献评估的可行性。该方法确保数据机密性，可扩展到多方参与，且无需共享原始数据或训练模型即可实现公平的数据估值。

Conclusion: 该研究为纵向联邦学习提供了一种实用的隐私保护特征贡献评估方案，解决了现有Shapley-CMI方法缺乏安全实现的问题，为多方协作中的数据公平估值提供了可行的技术路径。

Abstract: Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.

</details>


### [3] [Factor(U,T): Controlling Untrusted AI by Monitoring their Plans](https://arxiv.org/abs/2512.14745)
*Edward Lue Chee Lip,Anthony Channg,Diana Kim,Aaron Sandoval,Kevin Zhu*

Main category: cs.CR

TL;DR: 论文研究了在分解认知协议中使用不可信模型进行任务分解的安全风险，发现仅监控自然语言任务指令难以检测恶意分解，而监控具体解决方案则能有效保障安全。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升，我们越来越依赖强大模型来分解复杂任务，但如果分解器本身是恶意的呢？现有方法使用可信但能力较弱的模型进行分解，这限制了在分解本身就很困难的任务上的应用。需要研究使用不可信但能力更强的模型进行分解时的安全问题。

Method: 引入Factor(U,T)协议，其中不可信模型负责分解任务，可信模型负责执行子任务。在BigCodeBench数据集上进行控制评估，比较两种监控方式：1) 仅监控自然语言任务指令；2) 监控完整的Python解决方案。同时测试Factor(D,U)协议作为对比。

Result: 仅监控自然语言任务指令的检测器表现很差（AUROC 0.52），而监控完整Python解决方案的检测器表现优秀（AUROC 0.96）。Factor(D,U)协议（使用可信分解器并监控具体子任务解决方案）实现了优秀的区分能力（AUROC 0.96）和强大的安全性（攻击成功率仅1.2%）。

Conclusion: 在分解认知协议中，仅监控任务分解阶段的自然语言指令不足以检测恶意活动，而监控具体实现阶段的解决方案能有效保障安全。这表明实现上下文监控比仅监控分解阶段更有效。

Abstract: As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.

</details>


### [4] [BLINDSPOT: Enabling Bystander-Controlled Privacy Signaling for Camera-Enabled Devices](https://arxiv.org/abs/2512.14746)
*Jad Al Aaraj,Athina Markopoulou*

Main category: cs.CR

TL;DR: BlindSpot是一个在设备上运行的系统，让旁观者通过实时信号传达隐私偏好，无需预先共享敏感信息，系统包含手势、可见光通信和超宽带通信三种信号模式。


<details>
  <summary>Details</summary>
Motivation: 配备摄像头的移动设备（如手机、智能眼镜、AR头显）对旁观者构成隐私挑战，目前缺乏有效的实时机制让旁观者控制自己的图像、视频（包括面部）被拍摄。

Method: 设计并比较评估了三种不同的信号模式：手势机制、显著改进的可见光通信协议、以及新颖的超宽带通信协议。所有模式都包含使用几何一致性检查的验证机制，以验证信号来源相对于发送旁观者的位置，并防御冒充攻击。

Result: 在商用智能手机上实现了完整系统（BlindSpot），并对每种模式在不同距离、光照条件和用户移动情况下的准确性和延迟进行了全面评估。结果证明了这些新颖的旁观者信号技术的可行性及其在系统性能和便利性方面的权衡。

Conclusion: BlindSpot系统展示了让旁观者实时管理自身隐私的技术可行性，通过多种信号模式提供了隐私控制的创新解决方案，为移动设备时代的隐私保护提供了新思路。

Abstract: Camera-equipped mobile devices, such as phones, smart glasses, and AR headsets, pose a privacy challenge for bystanders, who currently lack effective real-time mechanisms to control the capture of their picture, video, including their face. We present BlindSpot, an on-device system that enables bystanders to manage their own privacy by signaling their privacy preferences in real-time without previously sharing any sensitive information. Our main contribution is the design and comparative evaluation of three distinct signaling modalities: a hand gesture mechanism, a significantly improved visible light communication (VLC) protocol, and a novel ultra-wideband (UWB) communication protocol. For all these modalities, we also design a validation mechanism that uses geometric consistency checks to verify the origin of a signal relative to the sending bystander, and defend against impersonation attacks. We implement the complete system (BlindSpot) on a commodity smartphone and conduct a comprehensive evaluation of each modality's accuracy and latency across various distances, lighting conditions, and user movements. Our results demonstrate the feasibility of these novel bystander signaling techniques and their trade-offs in terms of system performance and convenience.

</details>


### [5] [One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs](https://arxiv.org/abs/2512.14751)
*Yixin Tan,Zhe Yu,Jun Sakuma*

Main category: cs.CR

TL;DR: 研究发现微调后的LLMs会继承预训练模型的越狱漏洞，攻击者可通过预训练模型优化对抗提示并成功转移到微调模型，提出PGP攻击方法增强转移成功率。


<details>
  <summary>Details</summary>
Motivation: 研究微调预训练大语言模型的安全隐患，特别是微调模型是否继承预训练源的越狱漏洞，探索预训练到微调范式中的安全风险。

Method: 采用预训练到微调威胁模型，攻击者白盒访问预训练模型、黑盒访问微调模型；通过表示层探测分析可转移提示的线性可分性；提出探针引导投影攻击方法。

Result: 实验表明预训练模型优化的对抗提示能有效转移到微调变体；可转移提示在预训练隐藏状态中线性可分；PGP攻击在多种LLM家族和微调任务中展现强转移成功率。

Conclusion: 微调LLMs确实继承预训练模型的越狱漏洞，预训练到微调范式存在固有安全风险，需要新的防御机制来缓解这种继承性漏洞。

Abstract: Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.

</details>


### [6] [CODE ACROSTIC: Robust Watermarking for Code Generation](https://arxiv.org/abs/2512.14753)
*Li Lin,Siyuan Xin,Yang Cao,Xiaochun Cao*

Main category: cs.CR

TL;DR: 本文提出了一种针对大语言模型生成代码的新型水印方法，通过区分代码中的低熵和高熵部分来应对评论移除攻击，相比现有方法具有更好的检测性和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代码水印方法无法有效应对评论移除攻击，攻击者只需移除代码中的注释而不影响功能就能破坏水印。同时，代码本身是低熵环境，给水印注入带来挑战。

Method: 利用先验知识通过提示词列表区分代码中的低熵和高熵部分，然后基于这个提示列表指导水印注入，从而提高水印的检测性和代码可用性。

Result: 在HumanEval数据集上评估，与三种最先进的代码水印技术相比，本文方法在检测性和可用性方面表现更优。

Conclusion: 提出的基于代码熵区分的水印方法能有效应对评论移除攻击，在保护LLM生成代码知识产权方面具有实际应用价值。

Abstract: Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.

</details>


### [7] [Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection](https://arxiv.org/abs/2512.14935)
*Nnamdi Philip Okonkwo,Lubna Luxmi Dhirani*

Main category: cs.CR

TL;DR: 在AWS上实现AI增强安全运营中心，通过云原生工具和机器学习检测结合，模拟攻击并训练分类器进行威胁检测和分类


<details>
  <summary>Details</summary>
Motivation: 云安全运营中心需要在有限预算下处理大量异构遥测数据，同时应对弹性、短生命周期资源的挑战，需要更智能的检测方法

Method: 在AWS上构建AISOC架构，使用三个EC2实例（攻击者、防御者、监控），模拟Metasploit反向shell攻击，通过Filebeat转发日志到Elasticsearch/Kibana，训练恶意软件检测器和日志异常检测器，并进行分数校准和融合

Result: 在受控测试中，融合方法实现了高达1.00的宏观F1分数，能够将活动分类为正常、可疑和高置信度攻击三类，但在更嘈杂和多样化的环境中性能会有所变化

Conclusion: 简单、校准的融合方法可以在成本受限的设置中增强云安全运营中心的能力，为预算有限的云环境提供有效的威胁检测解决方案

Abstract: Cloud Security Operations Center (SOC) enable cloud governance, risk and compliance by providing insights visibility and control. Cloud SOC triages high-volume, heterogeneous telemetry from elastic, short-lived resources while staying within tight budgets. In this research, we implement an AI-Augmented Security Operations Center (AISOC) on AWS that combines cloud-native instrumentation with ML-based detection. The architecture uses three Amazon EC2 instances: Attacker, Defender, and Monitoring. We simulate a reverse-shell intrusion with Metasploit, and Filebeat forwards Defender logs to an Elasticsearch and Kibana stack for analysis. We train two classifiers, a malware detector built on a public dataset and a log-anomaly detector trained on synthetically augmented logs that include adversarial variants. We calibrate and fuse the scores to produce multi-modal threat intelligence and triage activity into NORMAL, SUSPICIOUS, and HIGH\_CONFIDENCE\_ATTACK. On held-out tests the fusion achieves strong macro-F1 (up to 1.00) under controlled conditions, though performance will vary in noisier and more diverse environments. These results indicate that simple, calibrated fusion can enhance cloud SOC capabilities in constrained, cost-sensitive setups.

</details>


### [8] [SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports](https://arxiv.org/abs/2512.15003)
*Sogol Masoumzadeh,Yufei Li,Shane McIntosh,Dániel Varró,Lili Wei*

Main category: cs.CR

TL;DR: SEBERTIS框架通过语义替代训练DNN分类器，独立于词汇线索检测安全相关问题，在GitHub问题报告检测中显著优于现有ML和LLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动检测技术（ML模型和LLM提示）往往依赖词汇线索作为决策捷径，对复杂提交的检测率低，无法满足实时检测安全相关问题的实际需求。

Method: 提出SEBERTIS框架，通过微调双向transformer架构作为掩码语言模型，使用语义替代（语义等价词汇替换预测标签）进行训练，使DNN分类器独立于词汇线索。

Result: 在10,000个GitHub问题报告的语料库中，SEBERTIS分类器达到0.9880 F1分数，显著优于现有技术：比ML基线精度高14.44%-96.98%，召回率高15.40%-93.07%，F1分数高14.90%-94.72%；比LLM基线精度高23.20%-63.71%，召回率高36.68%-85.63%，F1分数高39.49%-74.53%。

Conclusion: SEBERTIS通过语义替代训练使分类器独立于词汇线索，能够自信地检测完全未见过的安全相关问题，显著提升了安全相关问题检测的准确性和实用性。

Abstract: Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.

</details>


### [9] [APT-ClaritySet: A Large-Scale, High-Fidelity Labeled Dataset for APT Malware with Alias Normalization and Graph-Based Deduplication](https://arxiv.org/abs/2512.15039)
*Zhenhao Yin,Hanbing Yan,Huishu Lu,Jing Xiong,Xiangyu Li,Rui Mei,Tianning Zang*

Main category: cs.CR

TL;DR: 本文提出了APT-ClaritySet数据集及其构建流程，解决了APT研究中数据集稀缺、别名不一致和样本冗余的问题，提供了标准化、去重后的高质量APT恶意软件数据集。


<details>
  <summary>Details</summary>
Motivation: APT研究面临大规模标准化数据集稀缺的问题，威胁行为体别名不一致和样本冗余严重阻碍了研究的可重复性。现有数据集在别名标准化和样本去重方面存在不足，影响了定量分析和模式识别。

Method: 开发了APT-ClaritySet构建流程，包括：1）威胁行为体别名标准化（解决了约11.22%的不一致名称）；2）基于图特征的去重方法，将可静态分析的执行文件子集减少了47.55%，同时保留了行为上不同的变体。数据集包含三个组件：完整集合、去重后集合和函数级重用资源。

Result: 构建了三个数据集：APT-ClaritySet-Full（34,363个样本，305个APT组织）；APT-ClaritySet-Unique（25,923个独特样本，303个组织）；APT-ClaritySet-FuncReuse（324,538个函数重用簇）。提供了标准化的归因和可扩展的去重流程。

Conclusion: 该工作通过发布标准化的APT数据集和详细的构建流程，为APT模式、演化和归因的定量研究提供了高保真、可重复的基础，解决了现有研究中的数据质量和可重复性问题。

Abstract: Large-scale, standardized datasets for Advanced Persistent Threat (APT) research are scarce, and inconsistent actor aliases and redundant samples hinder reproducibility. This paper presents APT-ClaritySet and its construction pipeline that normalizes threat actor aliases (reconciling approximately 11.22\% of inconsistent names) and applies graph-feature deduplication -- reducing the subset of statically analyzable executables by 47.55\% while retaining behaviorally distinct variants. APT-ClaritySet comprises: (i) APT-ClaritySet-Full, the complete pre-deduplication collection with 34{,}363 malware samples attributed to 305 APT groups (2006 - early 2025); (ii) APT-ClaritySet-Unique, the deduplicated release with 25{,}923 unique samples spanning 303 groups and standardized attributions; and (iii) APT-ClaritySet-FuncReuse, a function-level resource that includes 324{,}538 function-reuse clusters (FRCs) enabling measurement of inter-/intra-group sharing, evolution, and tooling lineage. By releasing these components and detailing the alias normalization and scalable deduplication pipeline, this work provides a high-fidelity, reproducible foundation for quantitative studies of APT patterns, evolution, and attribution.

</details>


### [10] [Quantifying Return on Security Controls in LLM Systems](https://arxiv.org/abs/2512.15081)
*Richard Helder Moulton,Austin O'Brien,John D. Hastings*

Main category: cs.CR

TL;DR: 本文提出一个决策导向框架，用于量化LLM系统的残余风险，将对抗性攻击结果转化为财务风险估计和投资回报率指标，帮助比较分层防御措施。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型越来越多地应用于安全关键工作流，但实践者缺乏关于哪些安全措施值得部署的量化指导。当前缺乏将对抗性探测结果转化为财务风险估计的系统方法。

Method: 建立基于DeepSeek-R1模型的RAG服务，包含合成个人身份信息，使用Garak工具对五个漏洞类别进行自动化攻击。通过拉普拉斯成功法则估计攻击成功概率，结合公开泄露成本数据校准的损失三角形分布，进行10,000次蒙特卡洛模拟生成损失超越曲线和期望损失。比较三种缓解措施：基于属性的访问控制、微软Presidio的命名实体识别脱敏、NeMo Guardrails。

Result: 基线系统攻击成功率极高（PII、潜在注入和提示注入>=0.98），每个攻击场景模拟期望损失达31.3万美元。ABAC将PII和提示相关攻击成功概率降至接近零，总期望损失减少约94%，投资回报率达9.83。NER脱敏同样消除PII泄露，投资回报率5.97。NeMo Guardrails仅提供边际效益（投资回报率0.05）。

Conclusion: 决策导向框架能够量化LLM系统的残余风险，将技术安全指标转化为财务指标。ABAC和NER脱敏在降低PII相关风险方面效果显著，而NeMo Guardrails效果有限。该方法为组织选择LLM安全措施提供了基于财务回报的决策支持。

Abstract: Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).

</details>


### [11] [No More Hidden Pitfalls? Exposing Smart Contract Bad Practices with LLM-Powered Hybrid Analysis](https://arxiv.org/abs/2512.15179)
*Xiaoqi Li,Zongwei Li,Wenkai Li,Yuqing Zhang,Xin Wang*

Main category: cs.CR

TL;DR: SCALM是一个基于LLM的智能合约不良实践检测框架，通过混合架构和多层推理验证系统，在检测效果上优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 随着以太坊平台的成熟和广泛应用，需要维护高质量的智能合约编写标准。虽然不良实践不一定直接导致安全问题，但会增加风险，因此需要系统性地研究和避免这些不良实践。

Method: 提出SCALM框架，包含两个方法创新：1) 结合上下文感知的函数级切片与知识增强的语义推理的混合架构；2) 通过语法、设计模式和架构分析连接低级代码模式与高级安全原则的多层推理验证系统。

Result: 使用多个LLM和数据集的广泛实验表明，SCALM在检测智能合约不良实践方面优于现有工具。

Conclusion: 该研究首次系统性地研究了智能合约中的不良实践，提出了有效的检测框架，有助于提高智能合约的编写质量。

Abstract: As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 47 specific issues. Specifically, we propose SCALM, an LLM-powered framework featuring two methodological innovations: (1) A hybrid architecture that combines context-aware function-level slicing with knowledge-enhanced semantic reasoning via extensible vectorized pattern matching. (2) A multi-layer reasoning verification system connects low-level code patterns with high-level security principles through syntax, design patterns, and architecture analysis. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.

</details>


### [12] [Talking to the Airgap: Exploiting Radio-Less Embedded Devices as Radio Receivers](https://arxiv.org/abs/2512.15387)
*Paul Staat,Daniel Davidovich,Christof Paar*

Main category: cs.CR

TL;DR: 研究人员发现，未经硬件修改的嵌入式设备可通过PCB走线和ADC的寄生射频敏感性，在300-1000MHz范围内意外充当无线电接收器，实现无线渗透空气隔离系统。


<details>
  <summary>Details</summary>
Motivation: 空气隔离系统被认为能防止远程攻击，但现有研究主要关注数据外泄。本研究旨在探索反向攻击可能性：能否通过无线方式向空气隔离系统渗透数据，而无需硬件修改或专用传感器。

Method: 提出系统化方法识别设备配置中的射频敏感性；在12款商用嵌入式设备和2个定制原型上进行测试；利用PCB走线和片上ADC的寄生射频敏感性，在300-1000MHz范围内接收和解码外部无线传输。

Result: 在300-1000MHz范围内观察到可重复接收，最低可检测信号功率为1mW；成功实现数十米距离的数据接收（包括非视距条件）；接收灵敏度支持高达100kbps的数据速率。

Conclusion: 嵌入式设备可意外充当无线电接收器，为空气隔离系统揭示了一种先前未探索的命令与控制向量，挑战了其固有隔离性的假设。

Abstract: Intelligent electronics are deeply embedded in critical infrastructures and must remain reliable, particularly against deliberate attacks. To minimize risks and impede remote compromise, sensitive systems can be physically isolated from external networks, forming an airgap. Yet, airgaps can still be infiltrated by capable adversaries gaining code execution. Prior research has shown that attackers can then attempt to wirelessly exfiltrate data across the airgap by exploiting unintended radio emissions. In this work, we demonstrate reversal of this link: malicious code execution on embedded devices can enable wireless infiltration of airgapped systems without any hardware modification. In contrast to previous infiltration methods that depend on dedicated sensors (e.g., microphones, LEDs, or temperature sensors) or require strict line-of-sight, we show that unmodified, sensor-less embedded devices can inadvertently act as radio receivers. This phenomenon stems from parasitic RF sensitivity in PCB traces and on-chip analog-to-digital converters (ADCs), allowing external transmissions to be received and decoded entirely in software.
  Across twelve commercially available embedded devices and two custom prototypes, we observe repeatable reception in the 300-1000 MHz range, with detectable signal power as low as 1 mW. To this end, we propose a systematic methodology to identify device configurations that foster such radio sensitivities and comprehensively evaluate their feasibility for wireless data reception. Exploiting these sensitivities, we demonstrate successful data reception over tens of meters, even in non-line-of-sight conditions and show that the reception sensitivities accommodate data rates of up to 100 kbps. Our findings reveal a previously unexplored command-and-control vector for air-gapped systems while challenging assumptions about their inherent isolation. [shortened]

</details>


### [13] [Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection](https://arxiv.org/abs/2512.15503)
*Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Hexu Li,Panos Papadimitratos*

Main category: cs.CR

TL;DR: AIMformer是一个基于Transformer的框架，用于实时检测车辆编队中的恶意行为，通过多头自注意力机制捕获时空依赖关系，在边缘设备上实现亚毫秒级推理延迟。


<details>
  <summary>Details</summary>
Motivation: 车辆编队通过V2X通信协调多车编队，能显著提高交通效率和安全性，但分布式协调存在安全漏洞，攻击者可注入虚假运动学数据，威胁运行稳定性和乘客安全。传统检测方法误报率高，无法捕捉多车协调的复杂时空依赖关系。

Method: 提出AIMformer框架，利用多头自注意力机制同时捕获车辆内部时间动态和车辆间空间相关性；采用全局位置编码和车辆特定时间偏移处理加入/退出操作；设计专注于精度的BCE损失函数以降低误报率。

Result: 在4种编队控制器、多种攻击向量和不同移动场景下的广泛评估显示，AIMformer性能优于现有基线架构（≥0.93）。使用TensorFlow Lite、ONNX和TensorRT的部署分析实现了亚毫秒级推理延迟，适合资源受限的边缘平台实时操作。

Conclusion: AIMformer验证了在车载和路边基础设施部署的可行性，为安全关键的车辆系统提供了有效的实时恶意行为检测解决方案。

Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.

</details>


### [14] [ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples](https://arxiv.org/abs/2512.15641)
*Yunfei Yang,Xiaojun Chen,Zhendong Zhao,Yu Zhou,Xiaoyan Gu,Juan Cao*

Main category: cs.CR

TL;DR: ComMark是一个基于频域变换的黑盒模型水印框架，通过过滤高频信息生成压缩、隐蔽且抗攻击的水印样本，在隐蔽性和鲁棒性之间实现更好平衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型因其依赖大量数据和昂贵训练过程而成为高价值资产，但面临泄漏和盗窃风险，需要有效的知识产权保护。现有黑盒水印方法往往难以平衡隐蔽性（防止检测和伪造）和鲁棒性（抵抗移除攻击）这两个实际版权验证的关键属性。

Method: 提出ComMark黑盒模型水印框架：1）利用频域变换过滤高频信息生成压缩、隐蔽且抗攻击的水印样本；2）在训练过程中结合模拟攻击场景和相似性损失来增强水印鲁棒性。

Result: 在多种数据集和架构上的综合评估表明，ComMark在隐蔽性和鲁棒性方面均达到最先进性能。此外，该方法可扩展到图像识别以外的任务，包括语音识别、情感分析、图像生成、图像描述和视频识别。

Conclusion: ComMark通过频域变换方法有效解决了黑盒模型水印中隐蔽性与鲁棒性的平衡问题，具有广泛的适用性和实际应用价值。

Abstract: The rapid advancement of deep learning has turned models into highly valuable assets due to their reliance on massive data and costly training processes. However, these models are increasingly vulnerable to leakage and theft, highlighting the critical need for robust intellectual property protection. Model watermarking has emerged as an effective solution, with black-box watermarking gaining significant attention for its practicality and flexibility. Nonetheless, existing black-box methods often fail to better balance covertness (hiding the watermark to prevent detection and forgery) and robustness (ensuring the watermark resists removal)-two essential properties for real-world copyright verification. In this paper, we propose ComMark, a novel black-box model watermarking framework that leverages frequency-domain transformations to generate compressed, covert, and attack-resistant watermark samples by filtering out high-frequency information. To further enhance watermark robustness, our method incorporates simulated attack scenarios and a similarity loss during training. Comprehensive evaluations across diverse datasets and architectures demonstrate that ComMark achieves state-of-the-art performance in both covertness and robustness. Furthermore, we extend its applicability beyond image recognition to tasks including speech recognition, sentiment analysis, image generation, image captioning, and video recognition, underscoring its versatility and broad applicability.

</details>


### [15] [Distributed HDMM: Scalable, Distributed, Accurate, and Differentially Private Query Workloads without a Trusted Curator](https://arxiv.org/abs/2512.15648)
*Ratang Sedimo,Ivoline C. Ngong,Jami Lashua,Joseph P. Near*

Main category: cs.CR

TL;DR: 分布式高维矩阵机制（Distributed HDMM）是一种用于分布式数据上线性查询的协议，无需可信第三方即可提供与中心化HDMM相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决在分布式数据上回答线性查询时，既想获得中心化模型HDMM的准确性，又不想依赖可信第三方的问题。在恶意聚合器和恶意客户端（假设诚实多数）的威胁模型下提供安全保障。

Method: 利用安全聚合协议在分布式数据上评估HDMM，通过安全聚合技术保护数据隐私，同时保持查询准确性。

Result: 初步实证评估显示，Distributed HDMM能够在数千个客户端的实际数据集和工作负载上运行，耗时不到一分钟。

Conclusion: Distributed HDMM成功实现了在分布式环境中无需可信第三方即可获得与中心化HDMM相当的查询准确性，为大规模分布式数据分析提供了实用的隐私保护解决方案。

Abstract: We present the Distributed High-Dimensional Matrix Mechanism (Distributed HDMM), a protocol for answering workloads of linear queries on distributed data that provides the accuracy of central-model HDMM without a trusted curator. Distributed HDMM leverages a secure aggregation protocol to evaluate HDMM on distributed data, and is secure in the context of a malicious aggregator and malicious clients (assuming an honest majority). Our preliminary empirical evaluation shows that Distributed HDMM can run on realistic datasets and workloads with thousands of clients in less than one minute.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning](https://arxiv.org/abs/2512.14709)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 该论文将Transformer的自注意力机制解释为近似的向量符号架构，提出通过VSA视角理解语言模型的推理行为，并设计VSA启发的架构偏置来提升逻辑可靠性。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型表现出类似推理的行为，但在需要稳定符号操作的任务上仍然脆弱。论文旨在通过向量符号架构的统一视角来解释这些现象，为构建更可解释和逻辑可靠的推理系统提供理论基础。

Method: 将自注意力机制和残差流解释为近似的VSA：查询和键定义角色空间，值编码填充物，注意力权重执行软解绑定，残差连接实现多个绑定结构的叠加。基于此视角，提出VSA启发的架构偏置，包括显式的绑定/解绑定头和超维内存层，以及促进角色-填充物分离和鲁棒叠加的训练目标。

Result: 论文建立了Transformer内部机制与思维链追踪、基于程序的推理和记忆增强工具使用之间的联系，解释了变量混淆和逻辑相关提示不一致等特征性失败模式。提出了衡量"VSA相似性"和逻辑组合性的度量标准。

Conclusion: 将注意力视为软向量符号计算为构建更可解释和逻辑可靠的推理系统提供了原则性路径。论文指出了理论和架构方面的开放问题，为未来研究提供了方向。

Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.

</details>


### [17] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: 该论文提出了一种在知识图谱不完整情况下的基准构建方法，并开发了自适应图推理智能体（GR-Agent）来解决不完整知识图谱上的问答问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在知识图谱问答上表现良好，但大多数基准测试假设知识图谱是完整的，这忽略了现实世界中知识图谱通常不完整的事实。当直接支持的三元组缺失时，需要从现有事实中推理答案，而现有方法在这种不完整情况下的推理能力有限。

Method: 1) 提出了一种构建不完整知识图谱基准的方法论，移除直接支持的三元组但保留替代推理路径；2) 设计了自适应图推理智能体（GR-Agent），将知识图谱构建为交互环境，将问答形式化为智能体与环境交互；3) GR-Agent使用图推理工具作为动作空间，维护潜在支持推理证据的记忆，包括相关关系和推理路径。

Result: 实验表明：1) 使用该方法构建的基准测试中，现有方法在不完整情况下性能持续下降；2) GR-Agent在完整和不完整设置下均优于非训练基线方法，与基于训练的方法表现相当。

Conclusion: 该研究填补了知识图谱不完整性评估的空白，提出的GR-Agent通过将知识图谱问答形式化为智能体与环境交互，有效提升了在不完整知识图谱上的推理能力，为现实世界不完整知识图谱的问答问题提供了解决方案。

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [18] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: 该研究通过结构化配置知识注入方法，显著提升了LLM生成Terraform基础设施即代码的正确率，但发现意图对齐仍存在瓶颈，揭示了"正确性-一致性差距"。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成正确且意图对齐的基础设施即代码方面成功率较低，需要改进LLM在IaC生成方面的能力。

Method: 1. 增强IaC-Eval基准测试，增加云仿真和自动化错误分析；2. 开发LLM辅助IaC代码生成的错误分类法；3. 实施从朴素RAG到图RAG的知识注入技术，包括图组件语义丰富化和资源间依赖关系建模。

Result: 基线LLM性能较差（总体成功率27.1%），注入结构化配置知识后，技术验证成功率提升至75.3%，总体成功率提升至62.6%。但意图对齐达到平台期。

Conclusion: 结构化知识注入能显著提高LLM生成IaC的技术正确性，但意图对齐仍存在限制，揭示了"正确性-一致性差距"：LLM可以成为熟练的"编码者"，但在满足细微用户意图方面仍是有限的"架构师"。

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [19] [AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally](https://arxiv.org/abs/2512.14910)
*Nadine Angela Cantonjos,Arpita Biswas*

Main category: cs.AI

TL;DR: AgroAskAI是一个用于农业气候适应决策支持的多智能体推理系统，专注于脆弱农村社区，通过模块化角色专业化架构协调自主智能体，提供可操作、本地化、多语言的决策建议。


<details>
  <summary>Details</summary>
Motivation: 农村农业地区面临干旱、强降雨和天气模式变化等气候相关风险的损害。现有研究呼吁适应性风险管理解决方案和决策策略，而传统AI系统多为单智能体模型或仅用于静态功能的多智能体框架，缺乏支持动态协作推理和情境感知输出的架构。

Method: 提出AgroAskAI多智能体推理系统，采用模块化、角色专业化的架构，使用责任链方法协调自主智能体，集成实时工具和数据集。系统内置治理机制减少幻觉，支持内部反馈，确保连贯且本地相关的策略，并支持多语言交互。

Result: 在常见农业气候适应相关查询的实验表明，通过额外工具和提示优化，AgroAskAI能够提供更可操作、更接地气、更具包容性的输出结果。

Conclusion: AgroAskAI展示了智能体AI在农业气候适应中提供可持续和负责任决策支持的潜力，特别适合服务脆弱农村社区。

Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.

</details>


### [20] [Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation](https://arxiv.org/abs/2512.15033)
*Xidan Song,Weiqi Wang,Ruifeng Cao,Qingya Hu*

Main category: cs.AI

TL;DR: 论文提出几何稳定性框架，发现大语言模型在象棋推理中存在准确率-稳定性悖论：高准确率模型在几何变换下性能急剧下降，表明其依赖模式匹配而非抽象空间逻辑。


<details>
  <summary>Details</summary>
Motivation: 传统基于准确率的评估方法无法区分大语言模型在复杂推理领域（如象棋）中的真正几何推理能力与对标准棋盘状态的表面记忆。需要新的评估框架来测试模型在几何变换下的稳定性。

Method: 提出几何稳定性框架，通过棋盘旋转、镜像对称、颜色反转和格式转换等不变变换来测试模型一致性。对包括GPT-5.1、Claude Sonnet 4.5和Kimi K2 Turbo在内的6个最先进大语言模型进行对比分析，使用约3000个棋局数据集。

Result: 发现显著的准确率-稳定性悖论：GPT-5.1等模型在标准位置上接近最优准确率，但在几何扰动下性能灾难性下降，旋转任务中错误率激增超过600%。Claude Sonnet 4.5和Kimi K2 Turbo表现出双重鲁棒性。Gemini 2.5 Flash在非法状态拒绝方面表现最佳（96.0%）。

Conclusion: 几何稳定性为AI评估提供了正交且必要的度量标准，能够有效区分大语言模型的推理能力与数据污染和过拟合问题，是评估模型真实理解能力的重要指标。

Abstract: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.

</details>


### [21] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER框架通过动态选择推理策略来平衡LLM推理效率与准确性，模仿人类分层推理机制，针对不同难度查询使用不同处理策略


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略主要依赖模型自身的快慢模式（如o1思考），难以在不同难度查询中平衡推理效率和准确性

Method: 提出认知启发的弹性推理框架CogER：1）评估查询复杂度并分配到预定义层级；2）将策略选择建模为马尔可夫决策过程，用强化学习训练CogER-Agent；3）引入认知工具辅助推理，使LLM能在思维链中自主调用外部工具

Result: CogER在领域内任务上相对改进至少13%的平均精确匹配，在领域外任务上相对改进8%，优于最先进的测试时扩展方法

Conclusion: CogER框架通过动态策略选择和工具集成，有效解决了LLM在不同难度查询中平衡推理效率与准确性的问题

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [22] [A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem](https://arxiv.org/abs/2512.15198)
*Mohsen Nafar,Michael Römer,Lin Xie*

Main category: cs.AI

TL;DR: 该论文提出了一种基于聚类的变量排序框架，用于提升离散优化中决策图(DD)的编译效率，通过将变量分组为簇来减少动态排序的搜索空间，并在最大加权独立集问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 离散优化中，松弛决策图通过节点合并提供对偶边界，但其质量严重依赖于变量排序和合并决策。虽然动态变量排序能收紧边界，但在整个变量集上全局评估会带来计算开销。需要一种方法来平衡边界质量和计算成本。

Method: 提出基于聚类的变量排序框架：1) 先将变量划分为簇；2) 采用两种策略：Cluster-to-Cluster（按簇顺序处理，使用问题特定的聚合标准）和Pick-and-Sort（从每个簇迭代选择和排序代表性变量）；3) 针对MWISP问题，提出了两种设置簇数量的策略；4) 将策略嵌入到基于决策图的分支定界算法中。

Result: 在最大加权独立集问题的基准实例上，所提出的方法相比标准动态变量排序基线，能够一致地降低计算成本。

Conclusion: 基于聚类的变量排序框架有效缓解了动态排序中边界质量与计算开销之间的权衡，通过结构分解显著减少了启发式搜索空间，在MWISP问题上验证了其计算效率优势。

Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.

</details>


### [23] [ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I](https://arxiv.org/abs/2512.15298)
*Seok-Hyun Ga,Chun-Yen Chang*

Main category: cs.AI

TL;DR: 该研究使用2025年韩国高考地球科学I部分，分析了GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro等多模态大语言模型在科学推理能力上的认知局限，发现模型存在感知-认知差距、计算-概念化差异和过程幻觉等问题，为设计"AI抗性题目"提供了依据。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI完成作业的现象日益普遍，学术诚信和评估有效性受到威胁。研究旨在深入分析先进LLM的多模态科学推理能力和认知局限性，为应对AI在课程作业中的未经授权使用提供解决方案。

Method: 使用2025年韩国高考地球科学I部分作为测试材料，设计了三种实验条件：整页输入、单独题目输入和优化的多模态输入，评估GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro在不同数据结构下的表现，并进行定量和定性分析。

Result: 定量结果显示非结构化输入导致性能显著下降（由于分割和OCR失败）。即使在优化条件下，模型仍表现出基本推理缺陷。定性分析发现"感知错误"占主导，存在"感知-认知差距"（模型能识别视觉数据但无法解释示意图中的符号意义）、"计算-概念化差异"（能执行计算但无法应用基础科学概念）和"过程幻觉"（跳过视觉验证而依赖无根据的背景知识）。

Conclusion: 通过针对AI的认知弱点（如感知与认知之间的差距），可以设计"AI抗性题目"来区分真实学生能力与AI生成回答，从而确保评估的公平性。研究为教育工作者提供了针对AI特定认知漏洞设计评估的具体线索。

Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.

</details>


### [24] [Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations](https://arxiv.org/abs/2512.15388)
*Reinhard Moratz,Niklas Daute,James Ondieki,Markus Kattenbeck,Mario Krajina,Ioannis Giannopoulos*

Main category: cs.AI

TL;DR: 该论文通过定性空间关系提升大语言模型为行人提供路线指引的能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在提供行人路线指引方面存在局限性，特别是在空间关系的准确描述上，需要改进其基于定性空间关系进行导航指导的能力

Method: 使用定性空间关系方法来增强大语言模型，通过空间关系表示和推理机制来改进路线指引的生成

Result: 改进后的大语言模型能够更准确、更自然地提供行人导航指引，在空间关系描述和路线指导方面表现更好

Conclusion: 通过整合定性空间关系，可以显著提升大语言模型在行人导航任务中的表现，为智能导航系统提供更人性化的指引

Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.

</details>


### [25] [Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat](https://arxiv.org/abs/2512.15435)
*Stefan Edelkamp*

Main category: cs.AI

TL;DR: 提出一个通过自我对弈AI游戏扩展人类专家游戏数据库的通用外学习框架，提高多玩家纸牌游戏早期决策的预测准确性


<details>
  <summary>Details</summary>
Motivation: 在多玩家纸牌游戏中，如叫牌、游戏选择和初始出牌等早期决策对游戏成功至关重要，但当前计算限制下主要依赖人类专家游戏的统计信息，需要改进

Method: 开发通用引导外学习框架，通过自我对弈AI游戏扩展人类游戏数据库，使用完美特征哈希函数处理压缩表，创建自我改进的纸牌游戏引擎

Result: 在Skat游戏中的案例研究表明，该自动化方法可以有效支持游戏中的各种决策

Conclusion: 通过结合人类专家数据和AI自我对弈生成的数据，可以显著提高纸牌游戏早期决策的预测准确性，实现持续自我改进的游戏引擎

Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.

</details>


### [26] [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)
*Jeongseok Kim,Kangjin Kim*

Main category: cs.AI

TL;DR: 本文提出了一种结合ASP和MILP的集成系统，用于处理UAM垂直起降机场的动态调度需求和模糊的人类重调度请求，提供可解释的自适应调度方案。


<details>
  <summary>Details</summary>
Motivation: 由于城市空中交通(UAM)中垂直起降机场资源受限，需要高效的调度方法。传统方法难以同时处理动态操作需求和人类模糊的重调度请求，需要更灵活透明的解决方案。

Method: 采用混合整数线性规划(MILP)建模资源受限项目调度问题，结合三值逻辑解释模糊用户意图和决策树，提出将答案集编程(ASP)与MILP集成的新系统框架。

Result: 开发了一个集成框架，能够优化调度并透明地支持人类输入，为可解释、自适应的UAM调度提供了鲁棒的结构。

Conclusion: 该研究提出的ASP-MILP集成系统能够有效处理UAM垂直起降机场的动态调度需求和模糊的人类重调度请求，实现了可解释的自适应调度优化。

Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.

</details>


### [27] [Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision](https://arxiv.org/abs/2512.15489)
*Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor Gitman*

Main category: cs.AI

TL;DR: Nemotron-Math是一个包含750万条解题轨迹的大规模数学推理数据集，整合了AoPS竞赛题和StackExchange数学问题，支持多种推理模式和Python工具集成，通过顺序分桶策略加速长上下文训练，在数学竞赛基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集在推理风格多样性、长形式轨迹和工具集成方面存在局限，需要更高质量、更大规模的监督数据来提升数学推理模型的性能。

Method: 利用GPT-OSS-120B的多模式生成能力创建包含高中低三种推理模式的数据集，整合85K AoPS竞赛题和262K StackExchange数学问题，开发顺序分桶策略加速128K上下文长度的微调训练。

Result: Nemotron-Math在匹配的AoPS问题上持续优于原始OpenMathReasoning，集成StackExchange-Math显著提高了鲁棒性和泛化能力，在HLE-Math上表现优异，同时在数学竞赛基准上保持准确率，在AIME 2024和2025上达到100% maj@16准确率。

Conclusion: Nemotron-Math数据集通过大规模、多样化的数学推理轨迹和有效的工具集成，结合创新的训练策略，实现了数学推理任务的最先进性能，为高质量数学推理监督提供了重要资源。

Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.

</details>


### [28] [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567)
*Zhangde Song,Jieyu Lu,Yuanqi Du,Botao Yu,Thomas M. Pruyn,Yue Huang,Kehan Guo,Xiuzhe Luo,Yuanhao Qu,Yi Qu,Yinkai Wang,Haorui Wang,Jeff Guo,Jingru Gan,Parshin Shojaee,Di Luo,Andres M Bran,Gen Li,Qiyuan Zhao,Shao-Xiong Lennon Luo,Yuxuan Zhang,Xiang Zou,Wanru Zhao,Yifan F. Zhang,Wucheng Zhang,Shunan Zheng,Saiyang Zhang,Sartaaj Takrim Khan,Mahyar Rajabi-Kochi,Samantha Paradi-Maropakis,Tony Baltoiu,Fengyu Xie,Tianyang Chen,Kexin Huang,Weiliang Luo,Meijing Fang,Xin Yang,Lixue Cheng,Jiajun He,Soha Hassoun,Xiangliang Zhang,Wei Wang,Chandan K. Reddy,Chao Zhang,Zhiling Zheng,Mengdi Wang,Le Cong,Carla P. Gomes,Chang-Yu Hsieh,Aditya Nandy,Philippe Schwaller,Heather J. Kulik,Haojun Jia,Huan Sun,Seyed Mohamad Moosavi,Chenru Duan*

Main category: cs.AI

TL;DR: 论文提出了一个基于场景的科学发现评估框架，用于评估大语言模型在真实科学研究项目中的能力，发现当前模型在科学发现方面存在系统性弱点，与通用科学基准存在性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前的科学基准主要测试去上下文化的知识，忽视了驱动科学发现的迭代推理、假设生成和观察解释等关键能力，需要开发更能反映真实科学发现过程的评估框架。

Method: 引入场景驱动的基准评估框架，在生物学、化学、材料和物理学领域，由领域专家定义真实研究项目并分解为模块化研究场景，从中采样验证问题。采用两阶段评估：问题级准确性和项目级性能评估。

Result: 应用该框架评估最先进的大语言模型发现：1）与通用科学基准相比存在一致性能差距；2）模型规模和推理能力扩展的收益递减；3）不同提供商顶级模型存在系统性弱点；4）研究场景间性能差异大导致最佳模型选择不稳定。

Conclusion: 当前所有大语言模型距离通用科学"超级智能"还很遥远，但已展现出在多种科学发现项目中的潜力。该框架为发现相关的模型评估提供了可复现基准，并指明了推动大语言模型向科学发现发展的实用路径。

Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.

</details>


### [29] [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)
*Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan LU*

Main category: cs.AI

TL;DR: STC框架在单个模型内将推理与自我批判交织进行，通过混合强化学习优化推理质量和自我评估，提升LLM的批判性思维能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM将推理与验证解耦：要么生成推理而不进行自我检查，要么依赖外部验证器事后检测错误。前者缺乏即时反馈，后者增加系统复杂性并阻碍同步学习。受人类批判性思维启发，需要统一框架在单模型内交织推理与自我批判。

Method: 提出Stepwise Think-Critique (STC)框架，在每一步推理中交织推理和自我批判。使用混合强化学习目标，结合推理奖励和批判一致性奖励，共同优化推理质量和自我评估。

Result: 在数学推理基准测试中，STC展现出强大的批判性思维能力，并产生更可解释的推理轨迹。

Conclusion: STC代表了向具有内置批判性思维的LLMs迈出的一步，通过统一框架实现了推理与自我批判的有机结合。

Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.

</details>


### [30] [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663)
*Chase Walker,Rickard Ewetz*

Main category: cs.AI

TL;DR: CAGE框架通过构建属性图来改进LLM的上下文归因方法，量化每个生成内容如何受提示和先前生成内容的影响，提高归因忠实度达40%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力强大但推理过程不透明，存在安全和信任问题。现有的上下文归因方法不完整，直接关联生成标记和提示，忽略了生成过程中的代际影响。

Method: 提出CAGE框架，构建属性图——一个有向图，量化每个生成内容如何受提示和所有先前生成内容的影响。该图保持因果关系和行随机性，通过沿图中路径边缘化中间贡献来计算上下文归因。

Result: 在多个模型、数据集、指标和方法上，CAGE显著提高了上下文归因的忠实度，平均提升高达40%。

Conclusion: CAGE框架通过考虑生成过程中的代际影响，提供了更完整和忠实的LLM行为解释，解决了现有上下文归因方法的局限性。

Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [31] [FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption](https://arxiv.org/abs/2512.15515)
*Zhihan Xu,Rajgopal Kannan,Viktor K. Prasanna*

Main category: cs.AR

TL;DR: 本文提出了FAME——首个专门为同态加密矩阵乘法优化的FPGA加速器，通过创新的数据通路设计减少内存带宽需求，实现比CPU方案平均221倍的加速。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）能够在加密数据上进行安全计算，解决云计算中的隐私问题。然而，HE操作（特别是矩阵乘法）的高计算成本仍然是其实际部署的主要障碍。加速同态加密矩阵乘法对于隐私保护机器学习等应用至关重要。

Method: 1. 开发成本模型评估给定HE参数和输入矩阵大小下的片上内存需求；2. 设计用于同态线性变换（HLT）的新型数据通路，通过细粒度数据重用显著减少片外内存流量和片上内存需求；3. 基于此数据通路构建FAME——首个专门为HE矩阵乘法优化的FPGA加速器，支持任意矩阵形状和广泛的HE参数集。

Result: 在Alveo U280 FPGA上实现FAME，并在不同矩阵大小和形状上进行评估。实验结果显示，FAME相比最先进的CPU实现平均加速221倍，证明了其在大规模连续HE矩阵乘法和实际工作负载中的可扩展性和实用性。

Conclusion: FAME通过优化片上内存使用和设计高效数据通路，显著提升了同态加密矩阵乘法的性能，为隐私保护计算的实际应用提供了可行的硬件加速解决方案。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ是一个用于在消费级GPU上训练中等规模语言模型（3B-32B参数）的端到端CUDA/C++实现，通过优化内存和通信瓶颈，能在单张16GB游戏显卡上训练7B模型或在4张RTX 4090上训练32B模型。


<details>
  <summary>Details</summary>
Motivation: 针对消费级GPU内存有限、通信速度慢的特点，开发能够在普通硬件上训练中等规模语言模型的系统，降低训练成本，使更多人能够参与语言模型训练。

Method: 采用端到端CUDA/C++实现，结合激活检查点、卸载技术和基于复制引擎的集合通信等优化技术，使用标准的8位训练流程，不依赖额外的算法近似。

Result: 能够在单张16GB中端游戏显卡上训练或微调7B模型，在配备4张RTX 4090的工作站上训练32B模型，同时保持约50%的FLOP利用率，效率可与昂贵云级GPU上的生产级系统相媲美。

Conclusion: LLMQ证明了在消费级GPU上高效训练中等规模语言模型的可行性，为资源受限的研究者和开发者提供了实用的训练解决方案，降低了语言模型训练的门槛。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [33] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 该研究探索了GPU上Bloom滤波器的优化设计空间，通过向量化、线程协作和计算延迟三个维度的优化，在保持高精度的同时实现了高吞吐量，性能超越现有方法11.35-15.4倍。


<details>
  <summary>Details</summary>
Motivation: Bloom滤波器是近似成员查询的基础数据结构，GPU因其大规模线程级并行性和高带宽内存，是加速Bloom滤波器的理想平台。尽管CPU优化实现已有深入研究，但GPU设计仍未被充分探索，本研究旨在填补这一空白。

Method: 研究从三个维度探索GPU上的Bloom滤波器设计空间：向量化、线程协作和计算延迟。通过分析硬件对不同参数配置的响应，并将这些观察与性能趋势相关联，开发了优化的GPU实现。

Result: 优化设计克服了传统速度与精度之间的权衡，在保持高精度配置优越准确性的同时，实现了通常仅限于高错误率变体的吞吐量。在同等错误率下，批量过滤器查找性能提升11.35倍，构建性能提升15.4倍，在B200 GPU上达到实际速度极限的92%以上。

Conclusion: 该研究填补了GPU上Bloom滤波器优化的空白，提出的优化方法显著提升了性能，同时保持了高精度。研究提供了模块化的CUDA/C++实现，并将开源发布，为GPU加速的Bloom滤波器应用提供了高效解决方案。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [34] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: DREX系统通过动态重组批次优化早期退出LLM推理，提升吞吐量2-12%并完全消除强制退出


<details>
  <summary>Details</summary>
Motivation: 传统批处理框架不适合早期退出LLM架构，因为批次中不同请求的退出时机不同。现有方案要么强制统一决策错过退出机会，要么强制提前退出降低输出质量。

Method: 提出动态重组批次方案：在早期退出点动态重组批次，满足退出条件的请求立即处理，继续的请求放入缓冲区重新分组后转发到更深层。实现DREX系统，包含无拷贝重组缓冲区和EE/SLA感知调度器。

Result: DREX相比基线方法提升吞吐量2-12%，同时完全消除强制退出，保持输出质量。通过内存高效的状态复制处理跳过的KV缓存。

Conclusion: DREX通过动态重组批次有效解决了早期退出LLM的批处理问题，在保持输出质量的同时显著提升推理效率，为早期退出模型提供了实用的推理系统。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>
