<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 该研究探讨了大型语言模型产生的幻觉如何影响用户信任，发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准，并识别了直觉作为新的用户相关信任因素。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型产生的幻觉（事实错误但看似合理）如何影响用户对LLM的信任以及用户与LLM的互动，理解在日常使用中幻觉对信任动态的影响。

Method: 采用定性研究方法，对192名参与者进行研究，基于Lee & See的校准信任模型和Afroogh等人的信任相关因素框架进行分析。

Result: 研究发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准；确认了期望、先前经验、用户专业知识和领域知识作为用户相关信任因素，并识别直觉作为幻觉检测的额外因素；信任动态还受到感知风险和决策风险等情境因素的影响。

Conclusion: 验证了Blöbaum提出的递归信任校准过程，并通过纳入直觉作为用户相关信任因素扩展了该模型；基于研究发现提出了负责任和反思性使用LLM的实践建议。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [2] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: 论文提出了AI TIPS框架，旨在解决当前AI治理框架的三个关键缺陷：缺乏针对具体用例的风险评估、过于概念化缺乏可操作控制、以及缺少规模化实施机制。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键挑战：1) 组织在用例层面缺乏充分的风险评估能力，如Humana集体诉讼案所示；2) 现有框架如ISO 42001和NIST AI RMF停留在概念层面，缺乏可操作的控制措施；3) 组织缺乏规模化实施治理的机制，无法将可信AI实践嵌入整个开发生命周期。

Method: 提出了AI TIPS（人工智能信任集成支柱可持续性2.0）框架，这是对2019年开发的全面操作框架的更新，比NIST的AI风险管理框架早四年。该框架直接针对上述挑战提供解决方案。

Result: 论文提出了一个更新的操作框架，旨在解决当前AI治理的缺陷，但摘要中未提供具体的实施结果或评估数据。

Conclusion: AI TIPS框架为组织提供了解决当前AI治理挑战的实用方法，包括针对具体用例的风险评估、可操作的控制措施以及规模化实施机制，填补了现有框架的空白。

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [3] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 论文提出了一个形式化的范畴论框架，分析人类和LLM如何将内容转化为关于可能世界状态空间的真值命题，论证LLM并未解决而是绕过了符号接地问题


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型(LLM)与人类在符号接地问题上的本质差异，理解LLM如何处理语义和真值命题

Method: 使用范畴论的形式化框架，分析人类和LLM将内容转化为关于可能世界状态空间W的真值命题的过程

Result: LLM并未真正解决符号接地问题，而是通过不同的机制绕过了该问题

Conclusion: LLM与人类在符号接地机制上存在根本差异，LLM的"理解"方式不同于人类基于真实世界经验的符号接地过程

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [4] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog是一个开源Python工具包，将对话生成、评估和机制可解释性统一到端到端框架中，用于构建和分析基于LLM的对话系统。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供一个系统化的框架来构建、基准测试和理解对话系统，将生成、评估和可解释性功能集成到统一的对话中心架构中。

Method: 基于标准化的Dialog表示，提供：1）角色驱动的多智能体模拟与可组合编排；2）结合语言指标、LLM作为评判和功能正确性验证的综合评估；3）通过特征消融和诱导进行激活检查和引导的机制可解释性工具；4）包含3D房间建模和麦克风效果的完整声学模拟音频生成。

Result: SDialog工具包集成了所有主要LLM后端，支持统一API下的混合后端实验，使研究人员能够更系统地构建、基准测试和理解对话系统。

Conclusion: SDialog通过将生成、评估和可解释性耦合到对话中心架构中，为构建和分析基于LLM的对话系统提供了一个全面、系统化的开源框架。

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [5] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 该论文对比人类和AI系统在模糊视觉刺激下的图像标注表现，分析两者在感知、推理和决策方面的异同，为构建神经符号架构提供认知科学基础。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI系统如何解释模糊视觉刺激，深入了解感知、推理和决策的本质，为开发更可解释、认知对齐的AI系统提供理论基础。

Method: 结合计算认知科学、认知架构和连接主义-符号混合模型，对比人类策略（类比推理、形状识别、置信度调节）与AI的特征处理；基于Marr的三层次假设、Simon的有限理性和Thagard的表征与情感框架，分析参与者反应与Grad-CAM可视化模型注意力；通过ACT-R和Soar认知架构解释人类行为。

Result: 揭示了生物与人工系统在表征、推理和置信度校准方面的关键相似性和差异性；人类在不确定性下采用分层和启发式决策策略。

Conclusion: 分析为未来神经符号架构提供了动机，这类架构将结构化符号推理与连接主义表征相统一，基于具身性、可解释性和认知对齐原则，有望开发出既高效又可解释、认知基础的AI系统。

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [6] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一个整合了基于片段的分子语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，用于高效且可解释的闭环靶向分子设计，在结合亲和力、类药性和合成可行性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法耗时昂贵且成功率低，现有的生成模型存在泛化能力不足、可解释性有限、过度强调结合亲和力而忽视关键药理学特性等问题，限制了其实际应用价值。

Method: Trio框架整合三个关键组件：1) 基于片段的分子语言建模实现上下文感知的片段组装；2) 强化学习确保物理化学和合成可行性；3) 蒙特卡洛树搜索平衡新颖化学型探索和蛋白质结合口袋中有前景中间体的利用。

Result: 实验结果表明，Trio可靠地生成化学有效且药理学增强的配体，在结合亲和力（+7.85%）、类药性（+11.10%）和合成可行性（+12.05%）方面优于最先进方法，同时将分子多样性扩展超过四倍。

Conclusion: Trio提供了一个高效、可解释的闭环靶向分子设计框架，通过整合片段组装、强化学习和蒙特卡洛树搜索，在保持化学有效性的同时显著提升药理学特性，为药物发现提供了有前景的新方法。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [7] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 该论文提出了一种使用高斯过程回归来聚合连续动作空间中多线程MCTS统计信息的方法，在6个不同领域中都优于现有聚合策略。


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，当计算时间有限但需要最佳性能时，如何有效聚合不同线程的统计信息是一个重要但尚未充分探索的问题。

Method: 引入高斯过程回归方法，为未在环境中试验过的有前景动作获取价值估计，从而改进多线程MCTS中的统计信息聚合。

Result: 在6个不同领域进行系统评估，证明该方法优于现有聚合策略，同时仅需适度的推理时间增加。

Conclusion: 高斯过程回归为连续动作空间中的多线程MCTS统计聚合提供了一种有效的解决方案，在性能提升和计算开销之间取得了良好平衡。

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [8] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT框架使用强化学习自动发现最小化高影响故障场景，加速AI加速器故障评估，相比进化方法提速2.2倍，相比随机故障注入减少99%测试向量，同时提供更好的故障覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模庞大，传统故障评估方法面临计算成本过高和关键故障模式覆盖率不足的问题，需要一种可扩展的自动化框架来发现最小化高影响故障场景。

Method: RIFT将复杂的最坏情况故障搜索转化为序列决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小化高影响测试套件，并自动生成UVM兼容的验证工件。

Result: 在基于NVIDIA A100 GPU的十亿参数大语言模型工作负载评估中，RIFT相比进化方法实现2.2倍故障评估加速，相比随机故障注入减少99%以上测试向量，同时获得更优的故障覆盖率。RIFT引导的选择性错误校正码相比统一三模冗余保护在成本效益上提升12.8倍。

Conclusion: RIFT提供了一个可扩展的框架，能够自动化发现最小化高影响故障场景，显著加速AI加速器的故障评估过程，同时提供可操作的数据支持智能硬件保护策略，并可直接集成到商业RTL验证工作流中。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [9] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架，用于建模认知异质智能体之间的信念、动机和影响。通过个性化价值空间表示智能体，信念被形式化为结构化向量，其传播受线性解释映射调节。信念只有在避免这些映射的零空间时才能存活，这为可理解性、误解和信念消亡提供了结构性标准。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是建立一个统一的框架来分析认知异质智能体之间的信念动态。传统方法依赖于共享信息或理性假设，而本文旨在通过结构兼容性来理解意义保存，从而统一概念空间、社会认识论和AI价值对齐的见解。

Method: 方法基于几何框架：每个智能体由个性化价值空间（向量空间）表示，编码其解释和评估意义的内部维度。信念被形式化为结构化向量（抽象存在），其传播通过线性解释映射进行调节。信念存活的条件是避免这些映射的零空间。通过代数约束分析信念扭曲、动机漂移、反事实评估和相互理解的限制。

Result: 主要结果包括：1）信念存活的结构性标准；2）"无零空间领导条件"，将领导力定义为表征可达性而非说服或权威的属性；3）解释抽象存在如何在多样认知几何中传播、变异或消失的模型；4）从纯代数约束推导出信念扭曲、动机漂移等现象。

Conclusion: 该认知几何视角通过将意义保存建立在结构兼容性而非共享信息或理性基础上，澄清了人类和人工系统中影响的认知边界。为分析异质智能体间的信念动态提供了通用基础，统一了概念空间、社会认识论和AI价值对齐的见解。

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [10] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: MatSci-YAMZ是一个结合人工智能和众包人机交互的平台，用于支持材料科学领域的元数据词汇开发，通过6名参与者的概念验证证明了其可行性。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇对推进FAIR和FARR数据原则至关重要，但其发展受到人力资源有限和标准化实践不一致的制约。

Method: 开发了MatSci-YAMZ平台，整合人工智能和人机交互（包括众包），在材料科学领域进行概念验证，6名参与者通过提供术语定义和示例来促进AI定义的精炼。

Result: 成功生成了19个AI定义，迭代反馈循环证明了AI-HILT精炼的可行性，确认了该模型的可行性，包括成功的概念验证、与FAIR和开放科学原则的一致性、指导未来研究的研究协议以及跨领域扩展的潜力。

Conclusion: MatSci-YAMZ的基础模型能够增强语义透明度，减少共识构建和元数据词汇开发所需的时间。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [11] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: SCOPE是一种一次性分层规划器，利用LLM生成的子目标仅在初始化时预训练轻量级学生模型，显著提高效率但牺牲了解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的规划方法存在计算成本高、部署困难的问题，且通常使用固定参数的预训练LLM，无法针对目标任务进行适配。需要一种更高效的方法来利用LLM的语义知识进行文本环境中的长期规划。

Method: 提出SCOPE方法：1）仅在初始化阶段使用LLM生成子目标；2）从示例轨迹中直接推导子目标；3）使用这些子目标预训练轻量级学生模型；4）避免训练和推理期间重复查询LLM，显著提高效率。

Result: 在TextCraft环境中，SCOPE达到0.56的成功率，优于ADaPT的0.52；推理时间从164.4秒大幅减少到3.0秒，效率提升显著。

Conclusion: 尽管LLM生成的子目标可能不是最优的，但仍能为文本规划任务中的分层目标分解提供良好的起点。SCOPE在保持性能的同时显著提高了效率，为实际部署提供了可行方案。

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [EMMap: A Systematic Framework for Spatial EMFI Mapping and Fault Classification on Microcontrollers](https://arxiv.org/abs/2512.09049)
*Gandham Sai Santhosh,Siddhartha Sanjay Naik,Ritwik Badola,Chester Rebeiro*

Main category: cs.CR

TL;DR: 提出一个平台无关的电磁故障注入空间映射与故障分类框架，用于系统分析微控制器对电磁故障的空间敏感性


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏统一的方法来系统映射电磁故障注入的空间敏感性和分类故障行为，需要建立可重复的工作流程来分析不同嵌入式架构的EMFI敏感性

Method: 基于O'Flynn和Kuhnapfel等人的见解，开发平台无关的空间EMFI映射和故障分类框架，通过三个代表性微控制器（包括Xtensa LX6和两个ChipWhisper板）进行试点实验演示

Result: 初步实验展示了所提方法在实际应用中的可行性，为研究人员提供了可复现的工作流程来分析不同嵌入式架构的EMFI敏感性

Conclusion: 该框架为系统研究电磁故障注入的空间敏感性提供了通用方法，有助于研究人员在不同嵌入式架构上分析EMFI漏洞

Abstract: Electromagnetic Fault Injection (EMFI) is a powerful technique for inducing bit flips and instruction-level perturbations on microcontrollers, yet existing literature lacks a unified methodology for systematically mapping spatial sensitivity and classifying resulting fault behaviors. Building on insights from O'Flynn and Kuhnapfel et al., we introduce a platform-agnostic framework for Spatial EMFI Mapping and Fault Classification, aimed at understanding how spatial probe position influences fault outcomes. We present pilot experiments on three representative microcontroller targets including the Xtensa LX6 (ESP32) and two ChipWhisper boards not as definitive evaluations, but as illustrative demonstrations of how the proposed methodology can be applied in practice. These preliminary observations motivate a generalized and reproducible workflow that researchers can adopt when analyzing EMFI susceptibility across diverse embedded architectures.

</details>


### [13] [Exposing Vulnerabilities in Counterfeit Prevention Systems Utilizing Physically Unclonable Surface Features](https://arxiv.org/abs/2512.09150)
*Anirudh Nakra,Nayeeb Rashid,Chau-Wai Wong,Min Wu*

Main category: cs.CR

TL;DR: 论文分析了基于纸张表面物理不可克隆特征(PUF)的防伪认证系统存在的安全漏洞，提出了一个操作框架来揭示物理和数字领域的系统级脆弱性，并设计了攻击方法来验证这些漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有基于纸张表面微观不规则性的防伪认证方法虽然准确且成本效益高，但在实际部署中可能存在安全漏洞。论文旨在揭示这些系统在物理和数字领域的脆弱性，填补技术可行性与安全实际部署之间的差距。

Method: 1. 形式化基于纸张PUF的认证操作框架；2. 基于该框架分析系统级漏洞；3. 设计物理拒绝服务和数字伪造攻击来验证漏洞；4. 进行分阶段的安全分析。

Result: 设计的攻击方法有效证明了现有纸张PUF认证系统存在安全漏洞，强调了需要安全对策来确保基于纸张PUF的认证的可靠性和韧性。

Conclusion: 提出的框架为全面的分阶段安全分析提供了基础，指导未来防伪系统的设计，深入探讨了潜在攻击策略，为理解系统各组件如何被攻击者利用提供了基础理解。

Abstract: Counterfeit products pose significant risks to public health and safety through infiltrating untrusted supply chains. Among numerous anti-counterfeiting techniques, leveraging inherent, unclonable microscopic irregularities of paper surfaces is an accurate and cost-effective solution. Prior work of this approach has focused on enabling ubiquitous acquisition of these physically unclonable features (PUFs). However, we will show that existing authentication methods relying on paper surface PUFs may be vulnerable to adversaries, resulting in a gap between technological feasibility and secure real-world deployment. This gap is investigated through formalizing an operational framework for paper-PUF-based authentication. Informed by this framework, we reveal system-level vulnerabilities across both physical and digital domains, designing physical denial-of-service and digital forgery attacks to disrupt proper authentication. The effectiveness of the designed attacks underscores the strong need for security countermeasures for reliable and resilient authentication based on paper PUFs. The proposed framework further facilitates a comprehensive, stage-by-stage security analysis, guiding the design of future counterfeit prevention systems. This analysis delves into potential attack strategies, offering a foundational understanding of how various system components, such as physical features and verification processes, might be exploited by adversaries.

</details>


### [14] [Analysis of the Security Design, Engineering, and Implementation of the SecureDNA System](https://arxiv.org/abs/2512.09233)
*Alan T. Sherman,Jeremy J. Romanik Romano,Edward Zieglar,Enis Golaszewski,Jonathan D. Fuchs,William E. Byrd*

Main category: cs.CR

TL;DR: 研究人员分析了SecureDNA系统的安全性，发现其自定义的SCEP协议仅实现单向认证，存在结构性弱点，允许攻击者绕过速率限制并篡改数据库响应，建议了修复方案。


<details>
  <summary>Details</summary>
Motivation: SecureDNA系统旨在通过新型密码学保护DNA合成订单请求和危险数据库的机密性，但需要验证其系统设计、工程实现和协议的安全性，特别是关键管理、证书基础设施、认证和速率限制机制。

Method: 通过分析源代码（版本1.0.8），检查关键管理、证书基础设施、认证和速率限制机制；首次对相互认证、基本请求和豁免处理协议进行形式化方法分析。

Result: 主要发现SecureDNA的自定义相互认证协议SCEP仅实现单向认证，数据库和密钥服务器无法识别通信对象；存在结构性弱点，允许攻击者绕过保护数据库机密性的速率限制；存在加密绑定不足的问题，可能导致响应被篡改。

Conclusion: SecureDNA系统存在严重的安全设计缺陷，违反了深度防御原则；研究人员提出了修复方案，包括添加强绑定；软件版本1.1.0已使用建议的SCEP+协议修复了SCEP问题。

Abstract: We analyze security aspects of the SecureDNA system regarding its system design, engineering, and implementation. This system enables DNA synthesizers to screen order requests against a database of hazards. By applying novel cryptography, the system aims to keep order requests and the database of hazards secret. Discerning the detailed operation of the system in part from source code (Version 1.0.8), our analysis examines key management, certificate infrastructure, authentication, and rate-limiting mechanisms. We also perform the first formal-methods analysis of the mutual authentication, basic request, and exemption-handling protocols.
  Without breaking the cryptography, our main finding is that SecureDNA's custom mutual authentication protocol SCEP achieves only one-way authentication: the hazards database and keyservers never learn with whom they communicate. This structural weakness violates the principle of defense in depth and enables an adversary to circumvent rate limits that protect the secrecy of the hazards database, if the synthesizer connects with a malicious or corrupted keyserver or hashed database. We point out an additional structural weakness that also violates the principle of defense in depth: inadequate cryptographic bindings prevent the system from detecting if responses, within a TLS channel, from the hazards database were modified. Consequently, if a synthesizer were to reconnect with the database over the same TLS session, an adversary could replay and swap responses from the database without breaking TLS. Although the SecureDNA implementation does not allow such reconnections, it would be stronger security engineering to avoid the underlying structural weakness. We identify these vulnerabilities and suggest and verify mitigations, including adding strong bindings. Software Version 1.1.0 fixes SCEP with our proposed SCEP+ protocol.

</details>


### [15] [ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data](https://arxiv.org/abs/2512.09321)
*Ruiqi Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: ObliInjection：首个针对多源输入LLM应用的提示注入攻击，通过顺序无关损失和orderGCG算法优化污染片段，即使仅污染6-100个片段中的1个也能高效攻击


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击假设攻击者控制全部输入或忽略多源输入的顺序不确定性，在多源数据场景中效果有限。需要开发能应对多源输入顺序不确定性的攻击方法

Method: 提出ObliInjection攻击，包含两个关键技术：1) 顺序无关损失函数，量化LLM完成攻击者指定任务的可能性（无论干净和污染片段的排列顺序）；2) orderGCG算法，专门用于最小化顺序无关损失并优化污染片段

Result: 在三个不同应用领域的数据集和12个LLM上的综合实验表明，ObliInjection非常有效，即使输入数据中6-100个片段中只有1个被污染也能成功攻击

Conclusion: ObliInjection是首个针对多源输入LLM应用的提示注入攻击，通过顺序无关损失和orderGCG算法解决了多源输入顺序不确定性的挑战，显著提升了攻击成功率

Abstract: Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data.
  In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated.

</details>


### [16] [Proof of Trusted Execution: A Consensus Paradigm for Deterministic Blockchain Finality](https://arxiv.org/abs/2512.09409)
*Kyle Habib,Vladislav Kapitsyn,Giovanni Mazzeo,Faisal Mehrban*

Main category: cs.CR

TL;DR: 提出PoTE共识协议，利用可信执行环境替代传统PoW/PoS，通过可验证执行而非重复执行实现共识，消除分叉和时隙瓶颈，实现单轮验证确认。


<details>
  <summary>Details</summary>
Motivation: 当前区块链共识协议存在结构性限制：PoW能耗高、确认延迟大；PoS存在权益集中、长程攻击、"无利害关系"漏洞，且性能受时隙时间和多轮委员会投票限制。

Method: 提出PoTE（Proof of Trusted Execution）共识范式，验证者在异构VM-based TEEs中运行相同规范程序，程序测量值公开记录，产生供应商支持的证明，将隔离区代码哈希与区块内容绑定。执行是确定性的，提议者从公共随机性唯一派生。

Result: PoTE避免分叉，消除时隙时间瓶颈，在单轮验证中提交区块，设计并实现了PoTE共识客户端。

Conclusion: PoTE通过基于可信执行环境的可验证执行，解决了传统共识协议的结构性限制，为高吞吐量应用（如Trillion去中心化交易所）提供了高效解决方案。

Abstract: Current blockchain consensus protocols -- notably, Proof of Work (PoW) and Proof of Stake (PoS) -- deliver global agreement but exhibit structural constraints. PoW anchors security in heavy computation, inflating energy use and imposing high confirmation latency. PoS improves efficiency but introduces stake concentration, long-range and "nothing-at-stake" vulnerabilities, and a hard performance ceiling shaped by slot times and multi-round committee voting. In this paper, we propose Proof of Trusted Execution (PoTE), a consensus paradigm where agreement emerges from verifiable execution rather than replicated re-execution. Validators operate inside heterogeneous VM-based TEEs, each running the same canonical program whose measurement is publicly recorded, and each producing vendor-backed attestations that bind the enclave code hash to the block contents. Because the execution is deterministic and the proposer is uniquely derived from public randomness, PoTE avoids forks, eliminates slot.time bottlenecks, and commits blocks in a single round of verification. We present the design of a PoTE consensus client, describe our reference implementation, and evaluate its performance against the stringent throughput requirements of the Trillion decentralized exchange.

</details>


### [17] [Reference Recommendation based Membership Inference Attack against Hybrid-based Recommender Systems](https://arxiv.org/abs/2512.09442)
*Xiaoxiao Chi,Xuyun Zhang,Yan Wang,Hongsheng Hu,Wanchun Dou*

Main category: cs.CR

TL;DR: 该论文提出了一种针对混合推荐系统的成员推断攻击方法，利用个性化推荐特性来推断用户数据是否被用于训练推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击方法未能充分利用推荐系统的独特特性，仅适用于包含两种推荐算法的混合推荐系统，而无法有效攻击基于相同算法利用用户-物品历史交互和属性的混合推荐系统。

Method: 提出基于度量的成员推断攻击方法：利用个性化特性为任何目标用户获取参考推荐，然后提出相对成员度量，利用目标用户的历史交互、目标推荐和参考推荐来推断用户数据的成员身份。

Result: 通过理论和实证分析证明了所提出的基于度量的成员推断攻击在混合推荐系统上的有效性。

Conclusion: 该研究填补了混合推荐系统成员推断攻击的空白，揭示了个性化推荐对隐私安全的影响，为推荐系统的隐私保护提供了重要见解。

Abstract: Recommender systems have been widely deployed across various domains such as e-commerce and social media, and intelligently suggest items like products and potential friends to users based on their preferences and interaction history, which are often privacy-sensitive. Recent studies have revealed that recommender systems are prone to membership inference attacks (MIAs), where an attacker aims to infer whether or not a user's data has been used for training a target recommender system. However, existing MIAs fail to exploit the unique characteristic of recommender systems, and therefore are only applicable to mixed recommender systems consisting of two recommendation algorithms. This leaves a gap in investigating MIAs against hybrid-based recommender systems where the same algorithm utilizing user-item historical interactions and attributes of users and items serves and produces personalised recommendations. To investigate how the personalisation in hybrid-based recommender systems influences MIA, we propose a novel metric-based MIA. Specifically, we leverage the characteristic of personalisation to obtain reference recommendation for any target users. Then, a relative membership metric is proposed to exploit a target user's historical interactions, target recommendation, and reference recommendation to infer the membership of the target user's data. Finally, we theoretically and empirically demonstrate the efficacy of the proposed metric-based MIA on hybrid-based recommender systems.

</details>


### [18] [Chasing Shadows: Pitfalls in LLM Security Research](https://arxiv.org/abs/2512.09549)
*Jonathan Evertz,Niklas Risse,Nicolai Neuer,Andreas Müller,Philipp Normann,Gaetano Sapia,Srishti Gupta,David Pape,Soumya Shaw,Devansh Srivastav,Christian Wressnegger,Erwin Quiring,Thorsten Eisenhofer,Daniel Arp,Lea Schönherr*

Main category: cs.CR

TL;DR: 该论文识别了LLM安全研究中9个常见陷阱，分析了72篇顶会论文发现每篇至少存在一个陷阱，仅15.7%被明确讨论，并通过案例研究展示了这些陷阱如何误导评估、夸大性能或损害可复现性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全研究中日益普及，但其独特特性挑战了现有的可复现性、严谨性和评估范式。先前研究识别了传统机器学习研究的常见陷阱，但这些研究早于LLM时代。本文旨在识别LLM研究中新出现或更相关的陷阱。

Method: 1. 识别了9个贯穿整个计算过程（从数据收集、预训练、微调到提示和评估）的常见陷阱；2. 分析了2023-2024年顶级安全和软件工程会议发表的72篇同行评审论文；3. 进行了4个实证案例研究，展示单个陷阱如何误导评估、夸大性能或损害可复现性。

Result: 1. 所有72篇论文都至少包含一个识别出的陷阱；2. 每个陷阱都在多篇论文中出现；3. 仅有15.7%的现有陷阱被明确讨论，大多数未被识别；4. 案例研究表明这些陷阱确实会误导评估、夸大性能或损害可复现性。

Conclusion: LLM安全研究中普遍存在未被充分认识的陷阱，这些陷阱可能严重影响研究有效性。基于研究发现，论文提供了可操作的指南来支持社区未来的工作，提高研究的严谨性和可复现性。

Abstract: Large language models (LLMs) are increasingly prevalent in security research. Their unique characteristics, however, introduce challenges that undermine established paradigms of reproducibility, rigor, and evaluation. Prior work has identified common pitfalls in traditional machine learning research, but these studies predate the advent of LLMs. In this paper, we identify \emph{nine} common pitfalls that have become (more) relevant with the emergence of LLMs and that can compromise the validity of research involving them. These pitfalls span the entire computation process, from data collection, pre-training, and fine-tuning to prompting and evaluation.
  We assess the prevalence of these pitfalls across all 72 peer-reviewed papers published at leading Security and Software Engineering venues between 2023 and 2024. We find that every paper contains at least one pitfall, and each pitfall appears in multiple papers. Yet only 15.7\% of the present pitfalls were explicitly discussed, suggesting that the majority remain unrecognized. To understand their practical impact, we conduct four empirical case studies showing how individual pitfalls can mislead evaluation, inflate performance, or impair reproducibility. Based on our findings, we offer actionable guidelines to support the community in future work.

</details>


### [19] [Defining Cost Function of Steganography with Large Language Models](https://arxiv.org/abs/2512.09769)
*Hanzhou Wu,Yige Wang*

Main category: cs.CR

TL;DR: 首次提出使用大语言模型设计隐写成本函数的方法，通过两阶段策略（LLM引导的程序合成+进化搜索）自动生成优于现有方法的成本函数


<details>
  <summary>Details</summary>
Motivation: 传统隐写成本函数设计严重依赖专家知识或需要大规模数据集进行学习，本文首次尝试利用大语言模型自动设计隐写成本函数，提供全新的设计视角

Method: 采用两阶段策略：第一阶段通过结构化提示从LLM响应中合成计算机程序形式的成本函数，并用预训练的隐写分析模型评估；第二阶段为每个候选成本函数重新训练隐写分析模型，根据检测准确率确定最优成本函数，通过迭代方式执行

Result: 实验表明，该方法设计的成本函数在抵抗隐写分析工具方面显著优于现有工作，验证了方法的优越性

Conclusion: 这是首次将LLM应用于高级隐写成本函数设计的工作，为隐写设计提供了新视角，可能启发进一步研究

Abstract: In this paper, we make the first attempt towards defining cost function of steganography with large language models (LLMs), which is totally different from previous works that rely heavily on expert knowledge or require large-scale datasets for cost learning. To achieve this goal, a two-stage strategy combining LLM-guided program synthesis with evolutionary search is applied in the proposed method. In the first stage, a certain number of cost functions in the form of computer program are synthesized from LLM responses to structured prompts. These cost functions are then evaluated with pretrained steganalysis models so that candidate cost functions suited to steganography can be collected. In the second stage, by retraining a steganalysis model for each candidate cost function, the optimal cost function(s) can be determined according to the detection accuracy. This two-stage strategy is performed by an iterative fashion so that the best cost function can be collected at the last iteration. Experiments show that the proposed method enables LLMs to design new cost functions of steganography that significantly outperform existing works in terms of resisting steganalysis tools, which verifies the superiority of the proposed method. To the best knowledge of the authors, this is the first work applying LLMs to the design of advanced cost function of steganography, which presents a novel perspective for steganography design and may shed light on further research.

</details>


### [20] [FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning](https://arxiv.org/abs/2512.09872)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.CR

TL;DR: FlipLLM是一个基于强化学习的框架，用于高效发现大语言模型和视觉语言模型中的位翻转攻击漏洞，相比现有方法速度提升2.5倍，并能指导硬件防护机制的设计。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI模型（如LLMs和VLMs）虽然性能优异，但容易受到硬件层面的位翻转攻击。现有的BFA发现方法缺乏通用性和可扩展性，难以分析现代基础模型的庞大参数空间和复杂依赖关系。

Method: 提出FlipLLM框架，将BFA发现建模为序列决策问题，结合敏感度引导的层剪枝和Q学习算法，高效识别最小但影响最大的位集合。

Result: FlipLLM在多种模型上验证有效：LLaMA 3.1 8B准确率从69.9%降至0.2%（仅翻转5位），LLaVA的VQA分数从78%降至接近0%（仅翻转7位）。相比SOTA方法快2.5倍，且识别出的关键位应用ECC SECDED等硬件保护机制可完全缓解攻击影响。

Conclusion: FlipLLM为语言和多模态基础模型提供了首个可扩展、自适应的BFA漏洞分析方法，为全面的硬件安全评估铺平了道路，具有指导硬件级防御的实际价值。

Abstract: Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.

</details>


### [21] [ByteShield: Adversarially Robust End-to-End Malware Detection through Byte Masking](https://arxiv.org/abs/2512.09883)
*Daniel Gibert,Felip Manyà*

Main category: cs.CR

TL;DR: 提出一种基于字节级掩码的新型防御机制，通过生成多个掩码版本的文件、独立分类每个版本，并使用基于阈值的投票机制来增强端到端恶意软件检测器对抗对抗性攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 端到端恶意软件检测器容易受到对抗性攻击，现有的随机化和去随机化平滑防御技术仍然无法有效应对插入大型对抗性载荷的攻击，需要更强大的防御机制。

Method: 采用确定性掩码策略，在字节级别对输入文件进行系统性的掩码处理，生成多个掩码版本。每个版本独立分类，然后通过基于阈值的投票机制产生最终分类结果。

Result: 在EMBER和BODMAS数据集上的实验结果表明，该防御机制优于随机化和去随机化平滑防御，能够有效抵御多种功能保持操作生成的对抗样本，同时在干净样本上保持高准确率。

Conclusion: 提出的字节级掩码防御机制通过系统性的掩码策略和投票机制，能够有效中和对抗性载荷的影响，为端到端恶意软件检测器提供了更强大的对抗性攻击防御能力。

Abstract: Research has proven that end-to-end malware detectors are vulnerable to adversarial attacks. In response, the research community has proposed defenses based on randomized and (de)randomized smoothing. However, these techniques remain susceptible to attacks that insert large adversarial payloads. To address these limitations, we propose a novel defense mechanism designed to harden end-to-end malware detectors by leveraging masking at the byte level. This mechanism operates by generating multiple masked versions of the input file, independently classifying each version, and then applying a threshold-based voting mechanism to produce the final classification. Key to this defense is a deterministic masking strategy that systematically strides a mask across the entire input file. Unlike randomized smoothing defenses, which randomly mask or delete bytes, this structured approach ensures coverage of the file over successive versions. In the best-case scenario, this strategy fully occludes the adversarial payload, effectively neutralizing its influence on the model's decision. In the worst-case scenario, it partially occludes the adversarial payload, reducing its impact on the model's predictions. By occluding the adversarial payload in one or more masked versions, this defense ensures that some input versions remain representative of the file's original intent, allowing the voting mechanism to suppress the influence of the adversarial payload. Results achieved on the EMBER and BODMAS datasets demonstrate the suitability of our defense, outperforming randomized and (de)randomized smoothing defenses against adversarial examples generated with a wide range of functionality-preserving manipulations while maintaining high accuracy on clean examples.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [22] [RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference](https://arxiv.org/abs/2512.09304)
*Siyuan Ma,Jiajun Hu,Jeeho Ryoo,Aman Arora,Lizy Kurian John*

Main category: cs.AR

TL;DR: RACAM是一种新型的DRAM内存计算架构，通过专用缓冲器、位串行处理单元和广播单元解决现有DRAM-PIM架构的数据重用不足、冗余数据传输和工作负载映射问题，在LLM推理中相比GPU和现有PIM系统实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有DRAM-PIM架构存在三个主要问题：缺乏数据重用、大量冗余数据传输、以及工作负载映射支持不足。这些问题限制了内存计算架构在处理新兴工作负载（如大语言模型推理）时的效率和性能。

Method: 提出RACAM架构，包含：1）专用局部性缓冲区实现数据重用；2）位串行处理单元支持运行时可变数据精度；3）popcount归约单元和广播单元减少冗余数据传输；4）工作负载映射机制充分利用DRAM架构的大规模并行性，为给定工作负载找到最佳映射方案。

Result: 在端到端LLM推理评估中，RACAM相比GPU实现9倍到102倍的加速，相比最先进的DRAM-PIM系统Proteus，在GPT3推理中实现每平方毫米233倍的性能提升。

Conclusion: RACAM通过创新的架构设计解决了现有DRAM-PIM系统的关键限制，显著提升了内存计算在处理大语言模型等新兴工作负载时的性能和能效，为未来内存计算架构的发展提供了重要方向。

Abstract: In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.

</details>


### [23] [ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators](https://arxiv.org/abs/2512.09427)
*Guoqiang Zou,Wanyu Wang,Hao Zheng,Longxiang Yin,Yinhe Han*

Main category: cs.AR

TL;DR: ODMA是一个针对随机访问受限内存（RACM）加速器的按需内存分配框架，通过轻量级长度预测器、动态桶分区和大桶保护机制，显著提升LLM在RACM平台上的服务效率。


<details>
  <summary>Details</summary>
Motivation: 在随机访问带宽受限的加速器（如基于LPDDR5的Cambricon MLU370）上服务大型语言模型时，现有内存管理方案存在局限：静态预分配浪费内存，细粒度分页（如PagedAttention）因高随机访问成本而不适用，现有HBM为中心的解决方案无法充分利用RACM加速器特性。

Method: ODMA框架结合轻量级长度预测器与动态桶分区和大桶保护机制，通过从实时跟踪中定期更新边界来最大化内存利用率，解决了分布漂移和重尾请求问题。

Result: 在Alpaca和Google-NQ数据集上，ODMA将预测准确率从82.68%提升至93.36%；在Cambricon MLU370-X4上服务DeepSeek-R1-Distill-Qwen-7B模型时，内存利用率从55.05%提升至72.45%，RPS和TPS分别比静态基线提高29%和27%。

Conclusion: 硬件感知的内存分配能够解锁RACM平台上高效的LLM服务，ODMA框架通过针对RACM特性的优化设计，显著提升了内存利用率和系统性能。

Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: METRO是一种针对内存受限场景的MoE模型专家并行服务路由算法，通过平衡GPU激活专家数量而非令牌数量，显著降低解码延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有专家并行方法通过平衡各GPU处理的令牌数量来解决负载不均衡问题，但在内存受限的MoE服务场景（特别是解码阶段），这种方法反而会降低性能，因为它增加了激活专家数量，加剧了内存压力。

Method: 提出METRO（Minimum Expert Token ROuting）算法，在内存受限场景下平衡各GPU激活的专家数量而非令牌数量；采用新颖的allGather方案收集全局top-k信息，相比传统allToAll开销更小；联合优化算法效率和GPU并行处理能力。

Result: 在真实系统（vLLM over 8 A100 GPUs）和专有模拟器（8-16 B200 GPUs）上评估，相比EPLB：解码延迟降低11-22%，总令牌吞吐量提升3-21%（Qwen3和DeepSeek-V3服务）；在固定解码SLO下，解码吞吐量最高提升4.11倍。

Conclusion: 在内存受限的MoE服务场景中，平衡激活专家数量比平衡令牌数量更有效；METRO算法通过优化路由策略显著提升性能，为专家并行服务提供了新的优化方向。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [25] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 提出了一种针对Vision Transformers的分布式分层卸载框架，通过在可信边缘设备上分割视觉数据并分发到多个独立云服务器，防止任何单一服务器获取完整图像，从而保护隐私。


<details>
  <summary>Details</summary>
Motivation: 当前视觉智能工具计算需求高，超出移动和可穿戴设备能力，而将数据卸载到云端会带来传输和服务器端的隐私泄露风险。

Method: 使用本地可信边缘设备（如手机或Nvidia Jetson）作为边缘协调器，将用户视觉数据分割成小块并分发到多个独立云服务器，最终合并和聚合计算仅在可信边缘设备上进行。

Result: 以Segment Anything Model为案例研究表明，该方法在保持接近基线分割性能的同时，显著降低了内容重建和用户数据暴露的风险。

Conclusion: 该框架为边缘-云连续体中的视觉任务提供了可扩展的隐私保护解决方案，通过设计防止任何单一外部服务器获取完整图像。

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [26] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: BatANN是一个分布式磁盘向量搜索系统，通过将查询状态发送到数据所在机器执行，在保持单机对数搜索效率的同时实现近线性吞吐扩展。


<details>
  <summary>Details</summary>
Motivation: 随着数据集扩展到数十亿向量，磁盘向量搜索成为实用方案，但未来需要处理单个服务器无法容纳的超大规模数据集，因此需要分布式解决方案。

Method: 核心创新是当访问存储在另一台机器上的邻域时，将查询的完整状态发送到该机器继续执行，提高局部性。系统基于单个全局图，在标准TCP上运行。

Result: 在1亿和10亿点数据集上，使用10台服务器达到0.95召回率时，吞吐量分别比scatter-gather基线提高6.21-6.49倍和2.5-5.10倍，平均延迟低于6毫秒。

Conclusion: BatANN是首个基于单个全局图的开源分布式磁盘向量搜索系统，在保持对数搜索效率的同时实现了近线性吞吐扩展，为超大规模向量搜索提供了实用解决方案。

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [27] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: WarmServe是一个多LLM服务系统，通过预知未来工作负载特征的GPU预热机制，显著提升首令牌时间性能，同时保持高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM服务系统在提高GPU利用率的同时牺牲了推理性能，特别是首令牌时间。研究发现真实世界工作负载具有高度周期性和长期可预测性，这为解决性能与效率的权衡提供了机会。

Method: 提出通用GPU工作器实现一对多GPU预热；设计WarmServe系统：1)采用驱逐感知模型放置策略减少集群预热干扰；2)通过主动预热提前准备通用GPU工作器；3)使用零开销内存切换机制管理GPU内存。

Result: 在真实世界数据集评估中，WarmServe相比最先进的自动缩放系统将TTFT提升高达50.8倍，同时相比GPU共享系统能够服务多达2.5倍的请求。

Conclusion: 通过利用工作负载的可预测性进行智能预热，WarmServe成功解决了多LLM服务中性能与资源效率的权衡问题，实现了显著的首令牌时间改进和更高的请求服务能力。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [28] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: 本文提出了一种基于帕累托的混合鲸鱼-海鸥优化算法(PHWSOA)，用于云环境中的多目标任务调度，同时优化执行时间、虚拟机负载平衡和经济成本三个关键指标。


<details>
  <summary>Details</summary>
Motivation: 云计算中的任务调度是一个关键研究挑战，现有调度方案大多只优化单一或有限指标（如执行时间或资源利用率），缺乏全面的多目标优化方法，需要同时考虑执行时间、负载平衡和经济成本等多个目标。

Method: 提出PHWSOA算法，结合鲸鱼优化算法(WOA)和海鸥优化算法(SOA)的优势，弥补WOA的局部开发能力和SOA的全局探索能力不足。采用Halton序列初始化提高种群多样性，帕累托引导的变异机制防止早熟收敛，并行处理加速收敛，并集成动态虚拟机负载重分配机制改善负载平衡。

Result: 在CloudSim模拟器上使用NASA-iPSC和HPC2N的真实工作负载进行实验，PHWSOA相比基线方法（WOA、GA、PEWOA、GCWOA）取得显著性能提升：执行时间减少高达72.1%，虚拟机负载平衡改善36.8%，成本节约23.5%。

Conclusion: PHWSOA算法在云任务调度中表现出优越的多目标优化能力，能够同时显著改善执行时间、负载平衡和经济成本，为实际云环境中的高效资源管理提供了有前景的解决方案。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [29] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: SynthPix是基于JAX实现的高性能并行PIV合成图像生成器，吞吐量比现有工具高几个数量级


<details>
  <summary>Details</summary>
Motivation: 为数据饥渴的强化学习方法训练提供支持，并缩短快速流场估计方法的开发迭代时间，特别是在需要实时PIV反馈的主动流体控制研究中

Method: 使用JAX框架在加速器上实现并行化，支持与现有工具相同的配置参数，但通过硬件加速实现更高的性能

Result: 实现了每秒图像对生成吞吐量比现有工具高几个数量级的性能提升

Conclusion: SynthPix对流体动力学社区具有实用价值，能够支持强化学习训练和快速流场估计方法的开发

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [30] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: STAR系统通过新的同步模式和资源重分配策略，有效解决了GPU深度学习训练中的straggler问题，相比现有系统显著降低了训练时间。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU深度学习训练很流行，但straggler的普遍性、原因和影响以及现有缓解方法的有效性仍未得到充分理解。研究发现straggler仍然普遍存在，且现有方法（如同步转异步SGD）可能无法改善训练时间甚至产生更多straggler。

Method: 提出STAR系统，包括：1）新的同步模式，将worker分组进行参数更新；2）启发式和机器学习方法选择最优同步模式以最小化训练时间；3）资源重分配支持所选模式同时最小化对共存作业的影响；4）主动预防straggler，避免CPU和带宽资源过载。

Result: 在AWS上的trace驱动评估显示，STAR在PS架构中降低训练时间48-84%，在all-reduce架构中降低51-70%，同时保持同步SGD的收敛精度。

Conclusion: STAR系统通过创新的同步模式和资源管理策略，有效解决了深度学习训练中的straggler问题，显著提高了训练效率，同时保持了模型精度。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [31] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 该论文提出了首个同时实现无锁性和可恢复性的转换方法，将基于锁的实现转换为可恢复的无锁实现


<details>
  <summary>Details</summary>
Motivation: 现有系统通常需要在无锁性和可恢复性之间做出权衡，缺乏同时具备这两种特性的解决方案。基于锁的实现虽然常见，但在故障恢复方面存在局限性。

Method: 从基于锁的实现出发，提供对锁获取和锁释放操作的可恢复、无锁替代方案。该转换支持嵌套锁以增强通用性，并确保可恢复性而不损害原有基于锁实现的正确性。

Result: 首次实现了同时具备无锁性和可恢复性的转换，能够在保持原有基于锁实现正确性的前提下，提供故障恢复能力。

Conclusion: 该转换方法填补了同时实现无锁性和可恢复性的技术空白，为构建更可靠、高性能的并发系统提供了新途径。

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>
