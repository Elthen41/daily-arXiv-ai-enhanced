<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Exploiting Liquidity Exhaustion Attacks in Intent-Based Cross-Chain Bridges](https://arxiv.org/abs/2602.17805)
*André Augusto,Christof Ferreira Torres,André Vasconcelos,Miguel Correia*

Main category: cs.CR

TL;DR: 该论文提出了一种针对意图跨链桥的新型流动性耗尽攻击，通过参数化攻击模拟框架分析了三大协议的安全性，发现deBridge在当前参数下易受攻击，Across相对稳健，Mayan Swift在压力测试下存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 意图跨链桥通过解算器提供即时流动性改善了用户体验，但引入了新的系统性风险，包括解算器流动性集中和延迟结算问题。现有研究对这些新风险的安全分析不足，需要系统性地评估协议在面对理性攻击者和拜占庭攻击者时的脆弱性。

Method: 提出流动性耗尽攻击新类别，开发基于重放的参数化攻击模拟框架。分析了2025年6月至11月期间三大协议（Mayan Swift、Across、deBridge）跨9条区块链的350万笔意图交易，涉及92.4亿美元资产转移。通过模拟理性攻击和拜占庭攻击场景，评估协议安全性。

Result: deBridge因解算器利润率高在当前参数下易受攻击：210个历史攻击实例平均净收益286.14美元，80.5%攻击有利可图。Across因解算器利润低、流动性高在所有测试配置下保持稳健。Mayan Swift通常安全但在压力测试下存在漏洞。拜占庭攻击可抑制所有协议可用性，导致数十笔意图失败，解算器损失最高达978美元。优化攻击策略可将攻击成本降低90.5%。

Conclusion: 意图跨链桥面临严重的流动性耗尽攻击风险，特别是对解算器利润较高的协议。需要改进协议设计以抵御此类攻击，包括调整激励机制、增加流动性缓冲和增强监控机制。研究为跨链桥安全评估提供了新的分析框架和攻击模型。

Abstract: Intent-based cross-chain bridges have emerged as an alternative to traditional interoperability protocols by allowing off-chain entities (\emph{solvers}) to immediately fulfill users' orders by fronting their own liquidity. While improving user experience, this approach introduces new systemic risks, such as solver liquidity concentration and delayed settlement. In this paper, we propose a new class of attacks called \emph{liquidity exhaustion attacks} and a replay-based parameterized attack simulation framework. We analyze 3.5 million cross-chain intents that moved \$9.24B worth of tokens between June and November 2025 across three major protocols (Mayan Swift, Across, and deBridge), spanning nine blockchains.
  For rational attackers, our results show that protocols with higher solver profitability, such as deBridge, are vulnerable under current parameters: 210 historical attack instances yield a mean net profit of \$286.14, with 80.5\% of attacks profitable. In contrast, Across remains robust in all tested configurations due to low solver margins and very high liquidity, while Mayan Swift is generally secure but becomes vulnerable under stress-test conditions. Under byzantine attacks, we show that it is possible to suppress availability across all protocols, causing dozens of failed intents and solver profit losses of up to \$978 roughly every 16 minutes. Finally, we propose an optimized attack strategy that exploits patterns in the data to reduce attack costs by up to 90.5\% compared to the baseline, lowering the barrier to liquidity exhaustion attacks.

</details>


### [2] [Symfrog-512: High-Capacity Sponge-Based AEAD Cipher (1024-bit State)](https://arxiv.org/abs/2602.17900)
*Victor Duarte Melo*

Main category: cs.CR

TL;DR: 该论文提交了一个完整的参考实现，包含确定性测试向量和可复现的基准测试套件，所有代码和构建说明公开可用，支持独立验证和重新实现。


<details>
  <summary>Details</summary>
Motivation: 提供一个完全可验证、可复现的AEAD（认证加密与关联数据）方案实现，促进密码学方案的透明度和独立验证，同时明确安全声明范围。

Method: 基于海绵结构和双工结构的理想置换模型，实现AEAD构造，包括域分离、速率和容量选择、标签生成等完整规范，提供参考CLI和文件格式。

Result: 提供了完整的开源实现（MIT许可证），包含测试向量、基准测试工具和文档化的硬件/编译器设置，性能数据可复现，安全声明严格限定在理想置换模型内。

Conclusion: 该工作通过提供完全透明、可验证的密码学实现，促进了密码学方案的独立分析和验证，同时明确界定了安全保证的范围，鼓励外部密码分析和可复现性检查。

Abstract: This submission includes a complete reference implementation together with deterministic test vectors and a reproducible benchmark suite. All source code, build instructions, and regression artifacts are publicly available in the project repository, enabling independent verification and reimplementation of the scheme. The AEAD construction is fully specified, including domain separation, rate and capacity choices, tag generation, and the exact file format used by the reference CLI. Reported performance numbers are produced by the built in benchmark tool under documented hardware and compiler settings. All security claims are made strictly within the ideal permutation model following standard sponge and duplex bounds, and no stronger guarantees are asserted for the concrete permutation beyond the documented analysis and empirical behavior. The implementation aims for constant time behavior with respect to secret dependent operations, although no formal side channel proof is provided. The project is released under the MIT license, and external cryptanalysis, feedback, and reproducibility checks are explicitly encouraged.

</details>


### [3] [Distributed Security: From Isolated Properties to Synergistic Trust](https://arxiv.org/abs/2602.18063)
*Minghui Xu*

Main category: cs.CR

TL;DR: 本文回顾了分布式安全40年的演进历程，主张从孤立研究单个安全属性转向理解属性间的协同组合，提出了协议、一致性、隐私、可验证性和问责性五大基础属性，并指出未来研究应关注属性融合、新应用驱动的安全需求、计算开销管理以及后量子时代挑战。


<details>
  <summary>Details</summary>
Motivation: 分布式安全在过去四十年经历了从崩溃容错协议到拜占庭容错架构的显著转变。本文旨在审视这一演进过程，并主张在分布式安全研究中进行根本性转变：从孤立研究单个安全属性转向理解这些属性之间的协同组合关系。

Method: 本文首先总结并分析了分布式安全的五大基础属性：协议、一致性、隐私、可验证性和问责性，追溯了它们的理论起源和实践成熟过程。然后通过案例研究展示了这些属性在交叉融合时产生的协同效应，最后识别了未来研究的关键挑战。

Result: 研究发现分布式安全的研究前沿现在位于这些基础属性的交叉点上，它们的融合创造了任何单一属性都无法实现的能力。属性之间的协同组合能够构建更强大的安全架构。

Conclusion: 分布式安全的未来不在于改进单个属性，而在于理解和利用属性间的协同效应，构建一个统一的信任结构。需要关注新应用驱动的安全属性发现、属性融合的系统化框架、高性能共识层中密码原语的计算开销管理，以及后量子和人为因素挑战。

Abstract: Over the past four decades, distributed security has undergone a remarkable transformation -- from crash-fault tolerant protocols designed for controlled environments to sophisticated Byzantine-resilient architectures operating in open, adversarial settings. This vision paper examines this evolution and argues for a fundamental shift in how we approach distributed security: from studying individual security properties in isolation to understanding their synergistic combinations. We begin by conclude four foundational properties, \textit{agreement, consistency, privacy, verifiability, accountability}. We trace their theoretical origins and practical maturation. We then demonstrate how the frontier of research now lies at the intersection of these properties, where their fusion creates capabilities that neither property could achieve alone. Looking forward, we identify critical research challenges: discovering new security properties driven by emerging applications, developing systematic frameworks for property convergence, managing the computational overhead of cryptographic primitives in high-performance consensus layers, and addressing post-quantum and human-factor challenges. The future of distributed security lies not in improving individual properties, but in understanding and harnessing their synergies to build a singular fabric of trust.

</details>


### [4] [AndroWasm: an Empirical Study on Android Malware Obfuscation through WebAssembly](https://arxiv.org/abs/2602.18082)
*Diego Soi,Silvia Lucia Sanna,Lorenzo Pisu,Leonardo Regano,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 该论文研究WebAssembly作为Android恶意软件隐藏恶意载荷的新技术，能够绕过传统静态分析和签名匹配机制，成功规避VirusTotal和MobSF等工业级检测工具。


<details>
  <summary>Details</summary>
Motivation: 近年来，Android恶意软件采用混淆、反重打包、隐写术、投毒、AI工具规避和内存执行等复杂技术来绕过自动检测和加固手动分析。需要研究新的恶意载荷隐藏技术来应对这些威胁。

Method: 研究WebAssembly在Android执行管道中集成模块的机制，提供概念验证演示攻击者如何嵌入和执行恶意例程，分析Android可能采用Wasm模块的执行机制。

Result: 成功展示了攻击者利用WebAssembly嵌入恶意例程的威胁模型，能够有效绕过工业级工具（如VirusTotal和MobSF）的IoC检测。

Conclusion: WebAssembly作为一种新型恶意载荷隐藏技术，能够有效规避传统静态分析和签名匹配机制，为Android恶意软件检测带来了新的挑战。

Abstract: In recent years, stealthy Android malware has increasingly adopted sophisticated techniques to bypass automatic detection mechanisms and harden manual analysis. Adversaries typically rely on obfuscation, anti-repacking, steganography, poisoning, and evasion techniques to AI-based tools, and in-memory execution to conceal malicious functionality.
  In this paper, we investigate WebAssembly (Wasm) as a novel technique for hiding malicious payloads and evading traditional static analysis and signature-matching mechanisms. While Wasm is typically employed to render specific gaming activities and interact with the native components in web browsers, we provide an in-depth analysis on the mechanisms Android may employ to include Wasm modules in its execution pipeline. Additionally, we provide Proofs-of-Concept to demonstrate a threat model in which an attacker embeds and executes malicious routines, effectively bypassing IoC detection by industrial state-of-the-art tools, like VirusTotal and MobSF.

</details>


### [5] [Many Tools, Few Exploitable Vulnerabilities: A Survey of 246 Static Code Analyzers for Security](https://arxiv.org/abs/2602.18270)
*Kevin Hermann,Sven Peldszus,Thorsten Berger*

Main category: cs.CR

TL;DR: 本文对246个静态安全分析工具进行了系统性文献综述，发现大多数工具关注有限的弱点集合，检测的漏洞很少可被利用，且评估使用过小的自定义基准，无法进行稳健评估。


<details>
  <summary>Details</summary>
Motivation: 静态安全分析是检测软件漏洞的广泛应用技术，但以往研究只针对特定弱点或应用领域进行调查，缺乏对整个安全领域的全面概述。

Method: 对246个静态安全分析工具进行了系统性文献综述，分析其针对的漏洞类型、应用领域、分析技术、评估方法和局限性。

Result: 研究发现：1) 大多数分析工具只关注有限的弱点集合；2) 检测到的漏洞很少是可被利用的；3) 评估使用自定义基准且规模过小，无法进行稳健评估。

Conclusion: 静态安全分析领域需要更全面的弱点覆盖、更关注实际可利用性，以及更大规模、标准化的评估基准来推动该领域的发展。

Abstract: Static security analysis is a widely used technique for detecting software vulnerabilities across a wide range of weaknesses, application domains, and programming languages. While prior work surveyed static analyzes for specific weaknesses or application domains, no overview of the entire security landscape exists. We present a systematic literature review of 246 static security analyzers concerning their targeted vulnerabilities, application domains, analysis techniques, evaluation methods, and limitations. We observe that most analyzers focus on a limited set of weaknesses, that the vulnerabilities they detect are rarely exploitable, and that evaluations use custom benchmarks that are too small to enable robust assessment.

</details>


### [6] [Detecting PowerShell-based Fileless Cryptojacking Attacks Using Machine Learning](https://arxiv.org/abs/2602.18285)
*Said Varlioglu,Nelly Elsayed,Murat Ozer,Zag ElSayed,John M. Emmert*

Main category: cs.CR

TL;DR: 该论文研究基于PowerShell的无文件加密货币挖矿攻击检测，通过实验证明基于抽象语法树(AST)微调的CodeBERT模型能实现高召回率


<details>
  <summary>Details</summary>
Motivation: 随着远程代码执行漏洞和高级社会工程技术的出现，威胁行为者开始进行广泛的无文件加密货币挖矿攻击。这些攻击在Windows环境中基于PowerShell的利用技术变得有效且隐蔽。即使攻击被检测到并删除了恶意脚本，进程仍可能在受害者端点上保持运行，这对检测机制构成了重大挑战。

Method: 使用收集的数据集进行实验研究，检测基于PowerShell的无文件加密货币挖矿脚本。采用基于抽象语法树(AST)的微调CodeBERT模型，探索AST集成和预训练模型微调在编程语言检测中的重要性。

Result: 实验结果显示，基于抽象语法树(AST)的微调CodeBERT模型实现了高召回率，证明了AST集成和针对编程语言的预训练模型微调的重要性。

Conclusion: 基于抽象语法树(AST)的微调CodeBERT模型能有效检测基于PowerShell的无文件加密货币挖矿攻击，为这类隐蔽攻击的检测提供了有效解决方案。

Abstract: With the emergence of remote code execution (RCE) vulnerabilities in ubiquitous libraries and advanced social engineering techniques, threat actors have started conducting widespread fileless cryptojacking attacks. These attacks have become effective with stealthy techniques based on PowerShell-based exploitation in Windows OS environments. Even if attacks are detected and malicious scripts removed, processes may remain operational on victim endpoints, creating a significant challenge for detection mechanisms. In this paper, we conducted an experimental study with a collected dataset on detecting PowerShell-based fileless cryptojacking scripts. The results showed that Abstract Syntax Tree (AST)-based fine-tuned CodeBERT achieved a high recall rate, proving the importance of the use of AST integration and fine-tuned pre-trained models for programming language.

</details>


### [7] [Trojans in Artificial Intelligence (TrojAI) Final Report](https://arxiv.org/abs/2602.07152)
*Kristopher W. Reese,Taylor Kulp-McDowall,Michael Majurski,Tim Blattner,Derek Juba,Peter Bajcsy,Antonio Cardone,Philippe Dessauw,Alden Dima,Anthony J. Kearsley,Melinda Kleczynski,Joel Vasanth,Walid Keyrouz,Chace Ashcraft,Neil Fendley,Ted Staley,Trevor Stout,Josh Carney,Greg Canal,Will Redman,Aurora Schmidt,Cameron Hickert,William Paul,Jared Markowitz,Nathan Drenkow,David Shriver,Marissa Connor,Keltin Grimes,Marco Christiani,Hayden Moore,Jordan Widjaja,Kasimir Gabert,Uma Balakrishnan,Satyanadh Gundimada,John Jacobellis,Sandya Lakkur,Vitus Leung,Jon Roose,Casey Battaglino,Farinaz Koushanfar,Greg Fields,Xihe Gu,Yaman Jandali,Xinqiao Zhang,Akash Vartak,Tim Oates,Ben Erichson,Michael Mahoney,Rauf Izmailov,Xiangyu Zhang,Guangyu Shen,Siyuan Cheng,Shiqing Ma,XiaoFeng Wang,Haixu Tang,Di Tang,Xiaoyi Chen,Zihao Wang,Rui Zhu,Susmit Jha,Xiao Lin,Manoj Acharya,Wenchao Li,Chao Chen*

Main category: cs.CR

TL;DR: IARPA TrojAI项目针对AI木马威胁开展研究，开发检测方法并识别未解决挑战，为AI安全领域提供关键见解


<details>
  <summary>Details</summary>
Motivation: 应对现代人工智能中新兴的AI木马漏洞威胁，这些恶意隐藏后门可能导致系统意外失败或被恶意行为者劫持

Method: 通过权重分析和触发器反演等检测方法，以及部署模型中的木马风险缓解方法

Result: 检测器性能、敏感性和"自然"木马普遍性的全面测试评估结果

Conclusion: 总结经验教训并提出推进AI安全研究的建议

Abstract: The Intelligence Advanced Research Projects Activity (IARPA) launched the TrojAI program to confront an emerging vulnerability in modern artificial intelligence: the threat of AI Trojans. These AI trojans are malicious, hidden backdoors intentionally embedded within an AI model that can cause a system to fail in unexpected ways, or allow a malicious actor to hijack the AI model at will. This multi-year initiative helped to map out the complex nature of the threat, pioneered foundational detection methods, and identified unsolved challenges that require ongoing attention by the burgeoning AI security field. This report synthesizes the program's key findings, including methodologies for detection through weight analysis and trigger inversion, as well as approaches for mitigating Trojan risks in deployed models. Comprehensive test and evaluation results highlight detector performance, sensitivity, and the prevalence of "natural" Trojans. The report concludes with lessons learned and recommendations for advancing AI security research.

</details>


### [8] [FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators](https://arxiv.org/abs/2602.18304)
*Darsh Asher,Farshad Dizani,Joshua Kalyanapu,Rosario Cammarota,Aydin Aysu,Samira Mirbagher Ajorpaz*

Main category: cs.CR

TL;DR: FEATUREBLEED攻击利用AI加速器的零跳过优化，通过端到端计时推断私有后端检索特征，绕过现有隐私防御，在医疗、金融等敏感领域造成数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 后端增强技术在推荐系统、医疗、金融等敏感领域广泛应用，这些系统在机密数据上训练并检索私有特征，但这些特征值对API调用者隐藏。现有硬件优化可能破坏数据机密性，需要研究硬件层面的数据窃取攻击。

Method: FEATUREBLEED攻击利用AI加速器中的零跳过优化，仅通过端到端计时推断私有后端检索特征，无需依赖功耗分析、DVFS操作或共享缓存侧信道。攻击在三种硬件后端（Intel AVX、Intel AMX、NVIDIA A100）和三种模型架构（DNN、CNN、混合CNN-MLP）上进行评估。

Result: 攻击在三个数据集（Texas-100X临床记录、OrganAMNIST医学影像、Census-19社会经济数据）上评估，显示泄漏在CPU和GPU加速器、数据模态和应用领域间具有普适性，对抗优势最高达98.87个百分点。禁用零跳过会增加Intel AMX每操作能耗25%并导致100%性能开销。

Conclusion: 泄漏的根本原因是现代硬件中的稀疏性驱动零跳过。研究量化了隐私-性能-功耗权衡，提出基于填充的防御方案，通过均衡响应到最坏情况执行时间来掩盖计时泄漏，仅产生7.24%平均性能开销且无额外功耗成本。

Abstract: Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.
  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.
  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.

</details>


### [9] [Drawing the LINE: Cryptographic Analysis and Security Improvements for the LINE E2EE Protocol](https://arxiv.org/abs/2602.18370)
*Benjamin Dowling,Prosanta Gope,Mehr U Nisa,Bhagya Wimalasiri*

Main category: cs.CR

TL;DR: LINEv2协议缺乏前向保密性和后妥协安全性，研究提出增强版本解决这些问题


<details>
  <summary>Details</summary>
Motivation: LINE作为东亚流行的通信平台，需要理解其安全保证，特别是LINEv2协议在现实环境中的密码学保障

Method: 通过修改多阶段密钥交换（MSKE）模型来捕获LINE消息协议的架构和安全性，分析LINEv2协议并设计增强版本

Result: LINEv2实现了基本的密钥不可区分性和消息认证等安全属性，但缺乏前向保密性和后妥协安全性

Conclusion: 提出了增强的LINE协议版本，引入了前向保密性和后妥协安全性，并进行了分析和基准测试

Abstract: LINE has emerged as one of the most popular communication platforms in many East Asian countries, including Thailand and Japan, with millions of active users. Therefore, it is essential to understand its security guarantees. In this work, we present the first provable security analysis of the LINE version two (LINEv2) messaging protocol, focusing on its cryptographic guarantees in a real-world setting. We capture the architecture and security of the LINE messaging protocol by modifying the Multi-Stage Key Exchange (MSKE) model, a framework for analysing cryptographic protocols under adversarial conditions. While LINEv2 achieves basic security properties such as key indistinguishability and message authentication, we highlight the lack of forward secrecy (FS) and post-compromise security (PCS). To address this, we introduce a stronger version of the LINE protocol, introducing FS and PCS to LINE, analysing and benchmarking our results.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 该研究探讨了使用形式化领域本体（OpenMath）通过检索增强生成来提升语言模型在数学推理中的可靠性，发现本体引导的上下文在检索质量高时能提升性能，但无关上下文会降低性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在幻觉、脆弱性和缺乏形式化基础等根本限制，这些在需要可验证推理的高风险专业领域尤其成问题。研究旨在探索形式化领域本体是否能通过检索增强生成来提升语言模型的可靠性。

Method: 使用数学作为概念验证，实现了一个神经符号管道，利用OpenMath本体结合混合检索和交叉编码器重排序，将相关定义注入模型提示中。在MATH基准上评估了三个开源模型。

Result: 评估显示，当检索质量高时，本体引导的上下文能提高性能，但无关上下文会主动降低性能。这突显了神经符号方法的潜力和挑战。

Conclusion: 形式化领域本体在提升语言模型可靠性方面具有潜力，但检索质量至关重要。无关上下文会损害性能，表明需要更精确的检索和上下文注入策略。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [11] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: TTG是一个基于编程谜题对决的评估框架，让模型相互出题挑战，通过Elo评分比较模型能力，无需人工出题


<details>
  <summary>Details</summary>
Motivation: 当前评估大型语言模型推理能力面临挑战：人工出题成本高，特别是需要博士级领域知识的难题；同时存在训练数据泄露问题，难以区分模型是真正推理还是见过类似问题

Method: 受16世纪数学对决启发，设计Token Games评估框架：模型通过创建编程谜题相互挑战（给定返回布尔值的Python函数，找到使其返回True的输入），然后通过两两对决计算Elo评分

Result: 评估了10个前沿模型，TTG的排名结果与Humanity's Last Exam等现有基准高度匹配，且无需人工出题；发现创建优质谜题对当前模型仍是极具挑战的任务，这是先前基准未测量的能力

Conclusion: TTG提出了一种新的评估范式：通过模型相互出题挑战来评估推理能力，这种设计不会被饱和，还能同时测试创造力、任务创建等传统基准未涵盖的技能

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [12] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: WorkflowPerturb是一个用于评估工作流评估指标的基准测试，通过向黄金工作流应用受控扰动来研究指标的敏感性和校准性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的结构化工作流在复杂任务中应用日益广泛，但自动评估这些工作流很困难，因为指标分数通常未校准，且分数变化不能直接反映工作流退化的严重程度。

Method: 提出WorkflowPerturb基准测试，通过对黄金工作流应用三种类型的受控扰动（缺失步骤、压缩步骤、描述变化），每种类型在10%、30%、50%三个严重级别上生成扰动变体。

Result: 构建了包含4,973个黄金工作流和44,757个扰动变体的数据集，评估了多个指标家族，使用预期分数轨迹和残差分析其敏感性和校准性，揭示了不同指标家族的系统性差异。

Conclusion: WorkflowPerturb支持基于严重程度的工作流评估分数解释，有助于更好地理解和校准工作流评估指标，数据集将在论文接受后发布。

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [13] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: 该研究将离线强化学习与跨具身学习相结合，通过分析16种机器人平台的运动数据集，发现该方法在包含大量次优轨迹的数据集上优于纯行为克隆，但存在跨形态梯度冲突问题，提出了基于形态相似性的分组策略来解决冲突。


<details>
  <summary>Details</summary>
Motivation: 机器人策略预训练面临高质量演示数据收集成本高的问题，需要探索更高效的预训练方法。

Method: 结合离线强化学习和跨具身学习，构建包含16种机器人平台的运动数据集，并引入基于形态相似性的分组策略来减少跨机器人梯度冲突。

Result: 该方法在包含大量次优轨迹的数据集上优于纯行为克隆，但随着次优数据比例和机器人类型增加，跨形态梯度冲突会阻碍学习，而分组策略能有效减少冲突并优于现有冲突解决方法。

Conclusion: 离线强化学习与跨具身学习的结合为机器人策略预训练提供了有效途径，但需要解决跨形态梯度冲突问题，基于形态相似性的分组策略是简单有效的解决方案。

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [14] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: 研究发现，即使训练时排除敏感属性，无监督表示仍会编码这些属性。SOMtime方法在无监督嵌入中恢复了年龄、收入等敏感属性的潜在轴，而传统方法如PCA、UMAP等则表现较差。


<details>
  <summary>Details</summary>
Motivation: 挑战"公平通过无知"的假设，即认为在训练中排除敏感属性就能实现公平表示。作者发现即使明确排除敏感属性，无监督表示仍会编码这些属性，这对机器学习管道的公平性构成风险。

Method: 使用SOMtime方法，这是一种基于高容量自组织映射的拓扑保持表示方法。在两个大规模真实世界数据集（五个国家的世界价值观调查和人口普查收入数据集）上进行实验，比较SOMtime与PCA、UMAP、t-SNE和自编码器的表现。

Result: SOMtime恢复了与排除的敏感属性对齐的单调排序，Spearman相关性高达0.85，而PCA和UMAP通常低于0.23，t-SNE和自编码器最多达到0.34。无监督分割SOMtime嵌入会产生人口统计学偏斜的聚类。

Conclusion: "公平通过无知"在表示层面对序数敏感属性失效，公平审计必须扩展到机器学习管道的无监督组件。无监督表示并非中立，即使敏感属性被排除在训练之外。

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [15] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: OMAD是首个在线多智能体强化学习框架，使用扩散策略来协调智能体，通过最大化缩放联合熵实现有效探索，在MPE和MAMuJoCo任务中实现了2.5-5倍的样本效率提升。


<details>
  <summary>Details</summary>
Motivation: 在线多智能体强化学习需要增强策略表达能力以获得更好性能。扩散模型在图像生成和离线设置中表现出卓越的表达能力和多模态表示能力，但在在线MARL中尚未充分探索。主要障碍是扩散模型的不可处理似然性阻碍了基于熵的探索和协调。

Method: 提出OMAD框架：1）使用松弛策略目标最大化缩放联合熵，实现无需可处理似然的探索；2）在CTDE范式下，使用联合分布值函数优化去中心化扩散策略；3）利用可处理的熵增强目标指导扩散策略的同时更新，确保稳定协调。

Result: 在MPE和MAMuJoCo的10个多样化任务上进行了广泛评估，OMAD成为新的最先进方法，实现了2.5倍到5倍的样本效率提升。

Conclusion: OMAD成功解决了扩散模型在在线MARL中的应用障碍，通过创新的熵优化方法实现了高效的探索和协调，显著提升了多智能体强化学习的性能。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [It's Not Just Timestamps: A Study on Docker Reproducibility](https://arxiv.org/abs/2602.17678)
*Oreofe Solarin*

Main category: cs.DC

TL;DR: Docker镜像构建的比特级可重现性研究：对2000个GitHub仓库的Dockerfile进行抽样分析，发现仅有2.7%的构建是完全可重现的，基础设施配置调整后提升18.6%，但仍有78.7%的Dockerfile不可重现，主要原因是时间戳、缓存、日志、文档和浮动版本等开发者可控因素。


<details>
  <summary>Details</summary>
Motivation: 软件供应链安全需要可重现的容器构建作为完整性检查手段，但当前Docker镜像构建的实际可重现性状况未知，需要实证研究来了解问题规模和根本原因。

Method: 构建Docker测量流水线，对2000个包含Dockerfile的GitHub仓库进行分层抽样分析，评估构建成功率和比特级可重现性，通过修改基础设施配置测试可重现性提升潜力，并分析不可重现的根本原因。

Result: 仅56%的仓库能成功构建镜像，其中仅2.7%具有比特级可重现性；基础设施配置调整后比特级可重现性提升18.6%，但仍有78.7%的构建不可重现；主要不可重现原因包括时间戳、元数据、未清理的缓存、日志、文档和浮动版本等开发者可控因素。

Conclusion: Docker镜像构建的可重现性现状堪忧，需要制定具体的Dockerfile指导原则，并开发相应的代码检查工具和持续集成检查机制来提升软件供应链的安全性。

Abstract: Reproducible container builds promise a simple integrity check for software supply chains: rebuild an image from its Dockerfile and compare hashes. We build a Docker measurement pipeline and apply it to a stratified sample of 2,000 GitHub repositories that contained a Dockerfile. We found that only 56% produce any buildable image, and just 2.7% of those are bitwise reproducible without any infrastructure configurations. After modifying infrastructure configurations, we raise bitwise reproducibility by 18.6%, but 78.7% of buildable Dockerfiles remain non-reproducible. We analyze the root causes of the remaining differences, and find that beyond timestamps and metadata, developer-controlled choices such as uncleaned caches, logs, documentation, and floating versions are dominant causes of non-reproducibility. We derive concrete Dockerfile guidelines from these patterns and discuss how they can inform future linters and Continuous Integration (CI) checks for reproducible containers.

</details>


### [17] [Message-Oriented Middleware Systems: Technology Overview](https://arxiv.org/abs/2602.17774)
*Wael Al-Manasrah,Zuhair AlSader,Tim Brecht,Ahmed Alquraan,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: 对10个开源消息中间件系统进行特征分析，涵盖42个特征共134个选项，发现MOM系统已演变为现代云应用提供高灵活性和可配置性框架，并识别出社区可集中精力于更少开源项目的机会。


<details>
  <summary>Details</summary>
Motivation: 对开源消息中间件系统进行全面特征分析，帮助实践者和开发者理解不同系统的特性，促进社区资源整合，并为现代云应用提供选择参考。

Method: 采用严谨方法学选择10个流行且多样化的MOM系统，对每个系统检查42个特征共134个不同选项，创建带注释的数据集以便验证发现。

Result: 发现MOM系统已演变为现代云应用提供高灵活性和可配置性框架，提供核心构建模块如事务支持、主动消息传递、资源管理、流量控制和多租户原生支持。识别出社区可集中精力于更少开源项目的机会。

Conclusion: 开源消息中间件系统已成熟为现代云应用的重要框架，通过全面特征分析创建了公开数据集，建议社区整合资源到更少项目以提高效率。

Abstract: We present a comprehensive characterization study of open-source message-oriented middleware (MOM) systems. We followed a rigorous methodology to select and study ten popular and diverse MOM systems. For each system, we examine 42 features with a total of 134 different options. We found that MOM systems have evolved to provide a framework for modern cloud applications through high flexibility and configurability and by offering core building blocks for complex applications including transaction support, active messaging, resource management, flow control, and native support for multi-tenancy. We also identify that there is an opportunity for the community to consolidate its efforts on fewer open-source projects.
  We have also created an annotated data set that makes it easy to verify our findings, which can also be used to help practitioners and developers understand and compare the features of different systems. For a wider impact, we make our data set publicly available.

</details>


### [18] [Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs](https://arxiv.org/abs/2602.17808)
*Nathan Ng,Walid A. Hanafy,Prashanthi Kadambi,Balachandra Sunil,Ayush Gupta,David Irwin,Yogesh Simmhan,Prashant Shenoy*

Main category: cs.DC

TL;DR: SwapLess系统通过自适应TPU-CPU协同推理，减少边缘TPU内存受限场景下的延迟，利用排队模型动态调整分区点和CPU核心分配，显著降低单租户和多租户工作负载的延迟。


<details>
  <summary>Details</summary>
Motivation: 边缘AI加速器（如Edge TPU）的片上内存有限，导致推理时需要频繁在主机和加速器内存间交换模型片段，显著增加延迟。现有的协同处理方法（CPU和加速器分区处理）可能因分区不当而恶化端到端延迟，特别是在多租户和动态环境中问题更加突出。

Method: SwapLess系统采用分析排队模型，该模型捕捉分区相关的CPU/TPU服务时间以及不同工作负载混合和请求率下的模型间和模型内交换开销。基于此模型，系统在线连续调整分区点和CPU核心分配，以最小化端到端响应时间，同时保持低决策开销。

Result: 在配备Edge TPU的平台上的实现表明，SwapLess相对于默认的Edge TPU编译器，将单租户工作负载的平均延迟降低了高达63.8%，多租户工作负载的平均延迟降低了高达77.4%。

Conclusion: SwapLess通过自适应TPU-CPU协同推理和动态资源分配，有效解决了边缘AI加速器内存受限导致的延迟问题，显著提升了单租户和多租户场景下的推理性能。

Abstract: IoT applications are increasingly relying on on-device AI accelerators to ensure high performance, especially in limited connectivity and safety-critical scenarios. However, the limited on-chip memory of these accelerators forces inference runtimes to swap model segments between host and accelerator memory, substantially inflating latency. While collaborative processing by partitioning the model processing between CPU and accelerator resources can reduce accelerator memory pressure and latency, naive partitioning may worsen end-to-end latency by either shifting excessive computation to the CPU or failing to sufficiently curb swapping, a problem that is further amplified in multi-tenant and dynamic environments.
  To address these issues, we present SwapLess, a system for adaptive, multi-tenant TPU-CPU collaborative inference for memory-constrained Edge TPUs. SwapLess utilizes an analytic queueing model that captures partition-dependent CPU/TPU service times as well as inter- and intra-model swapping overheads across different workload mixes and request rates. Using this model, SwapLess continuously adjusts both the partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead. An implementation on Edge TPU-equipped platforms demonstrates that SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads relative to the default Edge TPU compiler.

</details>


### [19] [Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation](https://arxiv.org/abs/2602.17811)
*Guy Blelloch,Andrew Brady,Laxman Dhulipala,Jeremy Fineman,Kishen Gowda,Chase Hutton*

Main category: cs.DC

TL;DR: 本文提出了三种更快的并行批量动态算法，用于维护无向图的低出度定向，所有算法均实现多对数深度，重点在于最小化工作量。


<details>
  <summary>Details</summary>
Motivation: 在并行批量动态设置中，需要插入或删除边批次，目标是使整个批次的并行处理工作量接近单次顺序更新的每边工作量，同时整个批次具有多对数跨度。现有算法在工作量方面仍有改进空间。

Method: 提出了三种并行批量动态算法：1）第一个算法在摊销意义上实现了渐近最优的定向和渐近最优的期望工作量界限；2）第二个算法提供O(c log n)定向，每边更新的期望最坏情况工作量为O(√log n)；3）第三个算法提供O(c + log n)定向，每边更新的期望最坏情况工作量为O(log² n)。

Result: 1）第一个算法改进了Liu等人[SPAA'22]的最佳工作量界限，减少了对数因子；2）第二个算法匹配了Berglin和Brodal[Algorithmica'18]的最佳已知顺序最坏情况O(c log n)定向算法；3）第三个算法显著改进了Ghaffari和Koo[SPAA'25]的O(c)定向算法，将最坏情况工作量从O(log⁹ n)降低到O(log² n)。

Conclusion: 本文提出了三种更快的并行批量动态低出度定向算法，在保持多对数深度的同时显著减少了工作量，为并行图算法领域提供了重要的改进。

Abstract: A low out-degree orientation directs each edge of an undirected graph with the goal of minimizing the maximum out-degree of a vertex. In the parallel batch-dynamic setting, one can insert or delete batches of edges, and the goal is to process the entire batch in parallel with work per edge similar to that of a single sequential update and with span (or depth) for the entire batch that is polylogarithmic. In this paper we present faster parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. All results herein achieve polylogarithmic depth, with high probability (whp); the focus of this paper is on minimizing the work, which varies across results.
  Our first result is the first parallel batch-dynamic algorithm to maintain an asymptotically optimal orientation with asymptotically optimal expected work bounds, in an amortized sense, improving over the prior best work bounds of Liu et al.~[SPAA~'22] by a logarithmic factor.
  Our second result is a $O(c \log n)$ orientation algorithm with expected worst-case $O(\sqrt{\log n})$ work per edge update, where $c$ is a known upper-bound on the arboricity of the graph. This matches the best-known sequential worst-case $O(c \log n)$ orientation algorithm given by Berglin and Brodal ~[Algorithmica~'18], albeit in expectation.
  Our final result is a $O(c + \log n)$-orientation algorithm with $O(\log^2 n)$ expected worst-case work per edge update. This algorithm significantly improves upon the recent result of Ghaffari and Koo~[SPAA~'25], which maintains a $O(c)$-orientation with $O(\log^9 n)$ worst-case work per edge whp.

</details>


### [20] [Distributed Triangle Enumeration in Hypergraphs](https://arxiv.org/abs/2602.17834)
*Duncan Adamson,Will Rosenbaum,Paul G. Spirakis*

Main category: cs.DC

TL;DR: 本文首次系统研究分布式超图中的子超图枚举问题，提出了超图计算模型、设计了三角形枚举算法并证明其最优性，引入了稀疏超图类并开发了高效算法，以及提出了通用技术框架。


<details>
  <summary>Details</summary>
Motivation: 过去十年中，子图检测和枚举已成为分布式图算法的核心问题，既有理论挑战又有实际应用。然而，对于超图中的分布式子超图枚举问题尚未有系统性研究，因此本文旨在填补这一空白。

Method: 1) 引入多个超图计算模型，推广了图的CONGEST模型并评估其计算能力；2) 在这些模型中设计分布式三角形枚举算法，并在两个模型中证明其最优性；3) 引入稀疏和"处处稀疏"超图类，并描述这些类中高效的分布式三角形枚举算法；4) 提出设计超图模型高效算法的通用技术。

Result: 建立了超图分布式计算的理论框架，设计了最优的三角形枚举算法，针对稀疏超图类开发了高效算法，并提供了通用的算法设计技术。

Conclusion: 本文首次系统研究了分布式超图中的子超图枚举问题，建立了计算模型、设计了算法并证明了最优性，为超图分布式算法领域奠定了基础，提出的技术框架对未来研究具有指导意义。

Abstract: In the last decade, subgraph detection and enumeration have emerged as a central problem in distributed graph algorithms. This is largely due to the theoretical challenges and practical applications of these problems. In this paper, we initiate the systematic study of distributed sub-hypergraph enumeration in hypergraphs. To this end, we (1)~introduce several computational models for hypergraphs that generalize the CONGEST model for graphs and evaluate their relative computational power, (2)~devise algorithms for distributed triangle enumeration in our computational models and prove their optimality in two such models, (3)~introduce classes of sparse and ``everywhere sparse'' hypergraphs and describe efficient distributed algorithms for triangle enumeration in these classes, and (4)~describe general techniques that we believe to be useful for designing efficient algorithms in our hypergraph models.

</details>


### [21] [Joint Training on AMD and NVIDIA GPUs](https://arxiv.org/abs/2602.18007)
*Jon Hu,Thomas Jia,Jing Zhu,Zhendong Yu*

Main category: cs.DC

TL;DR: 本文提出了一种在AMD-NVIDIA异构环境中进行混合训练的技术方案，通过两种通信方法实现跨厂商GPU数据传输，在LLaMA-8B和Qwen2-7B上达到NVIDIA同构系统98%的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，训练对计算和系统容量的需求快速增长，单一厂商的同构集群已无法满足需求，需要解决在AMD-NVIDIA异构环境中的高效混合训练问题。

Method: 提出了两种技术方案：1) 基于CPU转发的兼容性方法，采用跨并行组的差异化通信后端选择和多网卡并行数据传输；2) 设备直连通信方法，集成CPU卸载的点对点机制，实现无需主机内存中转的直接跨厂商GPU数据传输。

Result: 在LLaMA-8B和Qwen2-7B模型上的实验表明，提出的设备直连通信方法能够达到NVIDIA同构系统98%的吞吐量，同时保持了训练的稳定性和正确性。

Conclusion: 该研究成功实现了在AMD-NVIDIA异构环境中的高效混合训练，提出的设备直连通信方法在性能上接近同构系统，为解决大规模语言模型训练中的异构计算问题提供了有效解决方案。

Abstract: As large language models continue to scale, training demands on compute and system capacity grow rapidly, making single-vendor homogeneous clusters insufficient. This paper presents a technical solution for heterogeneous mixed training in AMD-NVIDIA environments. We first adopt a compatibility-oriented approach based on CPU-Forwarding Communication, with differentiated communication back-end selection across parallel groups and multi-NIC parallel data transfer. To achieve higher performance, we further propose another Device-Direct Communication approach, integrating a CPU-offloading P2P mechanism to enable direct cross-vendor GPU data transfer without host-memory staging. Experiments on LLaMA-8B and Qwen2-7B demonstrate that the proposed Device-Direct Communication approach achieves up to 98% of the throughput of an NVIDIA homogeneous system, while preserving training stability and correctness.

</details>


### [22] [A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum](https://arxiv.org/abs/2602.18158)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.DC

TL;DR: 提出了一种精确的多目标任务分配框架，用于在边缘-枢纽-云架构中联合优化工作流应用的可靠性和延迟，通过二进制整数线性规划实现最优解。


<details>
  <summary>Details</summary>
Motivation: 关键工作流应用越来越多地采用简化的边缘-枢纽-云架构，但该架构中的任务分配面临设备限制和多样化操作条件的挑战。由于这类应用的关键性，可靠性和延迟是重要但相互冲突的目标，需要精确的任务分配方法来确保最优解决方案。

Method: 提出了一个精确的多目标任务分配框架，采用全面的二进制整数线性规划（BILP）公式化方法，考虑了每个目标的相对重要性，并整合了时间冗余技术，同时考虑了相关研究中常被忽视的关键约束。

Result: 在真实世界应用中，相比基线策略，平均可靠性提升84.19%，延迟改善49.81%。所有测试工作流的运行时间平均在0.03到50.94秒之间，证明了方法的有效性和可扩展性。

Conclusion: 该方法为简化的边缘-枢纽-云架构中的工作流应用提供了一个实用的精确任务分配解决方案，能够在不同结构、规模和关键性的工作流中有效平衡可靠性和延迟目标。

Abstract: A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.

</details>


### [23] [It does not matter how you define locally checkable labelings](https://arxiv.org/abs/2602.18188)
*Antonio Cruciani,Avinandan Das,Alesya Raevskaya,Jukka Suomela*

Main category: cs.DC

TL;DR: LCL问题在分布式图算法中具有基础地位，本文证明LCL问题族对形式化变体具有极强的鲁棒性，即使限制到更严格的"节点-边可检查"形式，也能通过局部归约相互转换。


<details>
  <summary>Details</summary>
Motivation: 研究LCL问题在不同形式化定义下的等价性，探索LCL问题族的鲁棒性，为分布式图算法理论提供更坚实的基础。

Method: 提出一种更严格的"节点-边可检查"形式化方法（基于轮消除技术，限制在正则无标签图上），证明该形式与标准LCL形式可以通过局部归约相互转换，仅需对称性破坏预言机，在LOCAL模型中开销最多为O(log* n)轮。

Result: 证明了LCL问题族对形式化变体具有极强的鲁棒性，即使限制到更严格的"节点-边可检查"形式，也能与标准LCL形式等价转换，转换开销很小。

Conclusion: LCL问题定义具有内在的鲁棒性，即使采用更严格的形式化定义，其分布式计算复杂性本质保持不变，这为分布式图算法理论提供了更坚实的理论基础。

Abstract: Locally checkable labeling problems (LCLs) form the foundation of the modern theory of distributed graph algorithms. First introduced in the seminal paper by Naor and Stockmeyer [STOC 1993], these are graph problems that can be described by listing a finite set of valid local neighborhoods. This seemingly simple definition strikes a careful balance between two objectives: they are a family of problems that is broad enough so that it captures numerous problems that are of interest to researchers working in this field, yet restrictive enough so that it is possible to prove strong theorems that hold for all LCL problems. In particular, the distributed complexity landscape of LCL problems is now very well understood.
  In this work we show that the family of LCL problems is extremely robust to variations. We present a very restricted family of locally checkable problems (essentially, the "node-edge checkable" formalism familiar from round elimination, restricted to regular unlabeled graphs); most importantly, such problems cannot directly refer to e.g. the existence of short cycles. We show that one can translate between the two formalisms (there are local reductions in both directions that only need access to a symmetry-breaking oracle, and hence the overhead is at most an additive $O(\log^* n)$ rounds in the LOCAL model).

</details>


### [24] [Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum](https://arxiv.org/abs/2602.18287)
*Andrea D'Iapico,Monica Vitali*

Main category: cs.DC

TL;DR: 提出一种基于绿色约束的云原生应用自动部署规划方法，通过持续分析能耗模式、组件通信和基础设施环境特征，生成能效优化的部署方案。


<details>
  <summary>Details</summary>
Motivation: 信息技术环境可持续性成为关键问题，云边连续体上的云原生应用部署需要考虑能耗和温室气体排放。现有方法难以处理应用行为波动和基础设施碳强度变化，需要自动化的绿色部署策略。

Method: 提出一种基于绿色约束的自动部署规划方法，通过持续监控分析能耗模式、组件间通信和基础设施环境特征，自动学习和更新绿色约束，指导调度器生成环境友好的部署计划。

Result: 通过云原生应用的实际部署场景验证，该方法能有效减少能源消耗和相关排放，展示了自适应、能源感知编排的有效性。

Conclusion: 该方法为云边连续体上的云原生应用提供了有效的绿色部署解决方案，通过自动学习和更新的绿色约束，实现了环境可持续的IT基础设施管理。

Abstract: The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.

</details>
