<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis](https://arxiv.org/abs/2512.02189)
*Aaron Jarmusch,Sunita Chandrasekaran*

Main category: cs.AR

TL;DR: 本文开发了一个开源微基准测试套件，系统评估了NVIDIA Blackwell (B200) GPU架构的创新特性，包括第五代张量核心、张量内存等，并与H200进行对比，展示了B200在混合精度吞吐量和能效方面的显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着GPU架构快速发展以满足百亿亿次计算和机器学习需求，架构创新对不同工作负载的性能影响仍缺乏深入理解。NVIDIA Blackwell (B200)引入了多项重要架构创新，但系统量化这些改进的方法滞后于硬件开发周期。

Method: 开发开源微基准测试套件，系统评估Blackwell GPU架构，与H200代在内存子系统、张量核心流水线和浮点精度(FP32, FP16, FP8, FP6, FP4)方面进行对比。评估密集/稀疏GEMM、Transformer推理和训练工作负载。

Result: B200的张量核心增强实现了比H200高1.56倍的混合精度吞吐量和42%的能效提升。内存分析显示缓存未命中时内存访问延迟降低58%，这从根本上改变了最优算法设计策略。

Conclusion: 该工作为应用开发者提供了实用的架构优化见解，帮助他们充分利用现代GPU架构的丰富功能集，做出明智的架构决策，并指导未来GPU设计方向。

Abstract: As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.
  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.

</details>


### [2] [Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras](https://arxiv.org/abs/2512.02346)
*Hongyang Shang,An Guo,Shuai Dong,Junyi Yang,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 提出NM-TOS近内存架构，通过8T SRAM单元、流水线优化和DVFS技术，显著降低基于事件相机的TOS角点检测算法的延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 事件相机在监控和自动驾驶中广泛应用，但基于TOS的角点检测算法在资源受限的边缘设备上存在显著延迟问题，影响了事件相机的高速低功耗优势。

Method: 提出NM-TOS近内存架构，采用读写解耦的8T SRAM单元，通过流水线优化补丁更新速度，结合硬件-软件协同优化的外围电路和动态电压频率缩放技术。

Result: 相比传统数字实现，在1.2V电压下延迟/能耗降低24.7倍/1.2倍，在0.6V电压下降低1.93倍/6.6倍。蒙特卡洛仿真显示在0.62V以上电压下零比特错误率，角点检测精度仅轻微下降。

Conclusion: NM-TOS架构有效解决了事件相机角点检测在边缘设备上的延迟和能耗问题，为实时事件驱动计算机视觉应用提供了高效的硬件解决方案。

Abstract: Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.

</details>


### [3] [SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures](https://arxiv.org/abs/2512.02875)
*Cristian Tirelli,Lorenzo Ferretti,Laura Pozzi*

Main category: cs.AR

TL;DR: SAT-MapIt：一种基于SAT求解器的CGRA映射方法，相比现有技术能更有效地探索解空间，在47.72%的基准测试中取得更好结果


<details>
  <summary>Details</summary>
Motivation: CGRA（粗粒度可重构阵列）的加速性能严重依赖于映射质量，现有基于模调度和图算法（如最大团枚举）的编译技术存在局限性，需要更有效的映射方法

Method: 提出基于SAT（可满足性）的映射方法，引入核移动性调度（KMS）概念，结合数据流图和CGRA架构信息生成布尔约束集，使用SAT求解器高效探索解空间

Result: SAT-MapIt在47.72%的基准测试中优于现有技术：有时能找到更低的迭代间隔（II），有时能在现有技术无法找到有效映射的情况下找到有效映射

Conclusion: 基于SAT的映射方法比现有技术更有效地探索CGRA映射解空间，能获得更好的映射结果，为CGRA编译提供了新的有效途径

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.
  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.
  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.

</details>


### [4] [Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver](https://arxiv.org/abs/2512.02884)
*Cristian Tirelli,Laura Pozzi*

Main category: cs.AR

TL;DR: 本文提出了一种基于SAT求解的CGRA映射方法，通过引入Kernel Mobility Schedule来编码所有可能的映射，以最小化迭代间隔(II)，从而提升编译质量和效率。


<details>
  <summary>Details</summary>
Motivation: 随着粗粒度可重构阵列(CGRA)等低功耗架构的普及，作为协处理器加速计算密集型工作负载（如循环）变得越来越重要。获得的加速效果取决于硬件设计和编译质量。现有编译技术使用模调度来最小化迭代间隔(II)，但仍有改进空间。

Method: 提出了一种基于可满足性(SAT)公式化的映射方法，引入Kernel Mobility Schedule来编码给定数据流图(DFG)和给定II的所有可能映射。该调度与CGRA架构信息结合生成所有约束条件，以找到有效映射。

Result: 实验结果表明，该方法不仅平均减少了编译时间，而且相比现有技术获得了更高质量的映射。

Conclusion: 通过SAT公式化方法和Kernel Mobility Schedule，能够为任何给定拓扑找到最低II，从而改进CGRA编译过程，实现更高效的加速工作负载执行。

Abstract: Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications](https://arxiv.org/abs/2512.02300)
*Haoyu Zheng,Shouwei Gao,Jie Ren,Wenqian Dong*

Main category: cs.DC

TL;DR: DOLMA是一个为HPC应用设计的数据对象级内存解聚合框架，通过智能识别和卸载数据对象到远程内存，在减少本地内存使用的同时最小化性能损失。


<details>
  <summary>Details</summary>
Motivation: 内存解聚合技术有望扩展HPC系统的内存容量并提高利用率，但访问远程内存的性能开销对计算密集型HPC应用构成重大挑战，因为这类应用的执行时间对数据局部性高度敏感。

Method: DOLMA框架智能识别并将数据对象卸载到远程内存，提供定量分析以确定合适的本地内存大小。利用HPC应用典型可预测的内存访问模式，通过双缓冲区设计实现远程内存预取，并仔细平衡本地和远程内存使用，同时保持多线程并发性。

Result: 在八个HPC工作负载和计算内核上的评估显示，DOLMA将性能下降限制在16%以内，同时将本地内存使用减少高达63%（平均）。

Conclusion: DOLMA为HPC领域利用解聚合内存提供了一个灵活高效的解决方案，同时最小化了对应用性能的影响。

Abstract: Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant chal- lenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while pro- viding quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging dis- aggregated memory in HPC domains while minimally compromis- ing application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degrada- tion to less than 16% while reducing local memory usage by up to 63%, on average.

</details>


### [6] [Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems](https://arxiv.org/abs/2512.02646)
*Alex Barceló,Sebastián A. Cajas Ordoñez,Jaydeep Samanta,Andrés L. Suárez-Cetrulo,Romila Ghosh,Ricardo Simón Carbajo,Anna Queralt*

Main category: cs.DC

TL;DR: 本文提出了一种基于主动存储系统的软件架构，用于在计算连续体中优化AI工作负载分布，通过将计算嵌入存储架构来减少数据传输开销，提高性能和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载在各种计算环境中的需求不断增加，传统云架构难以处理AI驱动数据的大规模和高速特性，导致存储、计算和数据移动效率低下。现有框架缺乏计算连续体所需的灵活性和适应性，无法应对设备异构性和快速变化的AI算法模型。

Method: 提出了一种软件架构，利用主流Python库和dataClay主动存储平台，将AI工作负载无缝分布到计算连续体中。通过将计算嵌入存储架构，减少数据传输开销，实现工作负载卸载。

Result: 实验评估显示，通过主动存储卸载工作负载显著提高了内存效率和训练速度，同时保持了准确性。在不同设备上测试了内存消耗、存储需求、训练时间和执行效率等方面的权衡。

Conclusion: 主动存储有潜力彻底改变AI工作负载管理，使分布式AI部署更具可扩展性和资源效率，同时为领域专家和应用开发者提供了极低的学习门槛。

Abstract: The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.
  By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.
  This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.

</details>


### [7] [Distributed and Autonomic Minimum Spanning Trees](https://arxiv.org/abs/2512.02683)
*Luiz A. Rodrigues,Elias P. Duarte,Luciana Arantes*

Main category: cs.DC

TL;DR: 提出了一种基于VCube虚拟拓扑的自适应生成树算法，用于分布式系统中的广播通信，具有对数级度数和树深度的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的一对多广播策略在分布式系统中不可扩展，会给发送者带来沉重负载。需要一种能够动态构建和维护生成树的方法，以支持可扩展的广播通信。

Method: 使用VCube虚拟拓扑作为基础，构建自适应的生成树算法。该算法确保每个顶点的入度和树深度最多为log₂n，并能动态地从任意源进程创建生成树，在进程故障或恢复时透明地重建。

Result: 算法支持最多n-1个进程故障，正确的进程仍能通过可扩展的生成树保持连接。基于该生成树算法提出了两种广播算法：尽力而为广播和可靠广播，并通过仿真与其他方案进行了比较。

Conclusion: 提出的自适应生成树算法为分布式系统提供了可扩展的广播通信解决方案，通过VCube虚拟拓扑实现了对数级复杂度的连接结构，并能容错处理进程故障。

Abstract: The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.

</details>


### [8] [Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science](https://arxiv.org/abs/2512.02818)
*Sean R. Wilkinson,Patrick Widener,Sarp Oral,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 论文提出HPC中心应更积极地在多学科研究中培育FAIR生态系统，通过基于组件的方法使工作流组件符合FAIR原则，而非整个工作流，以支持HPC用户的多样化需求并最大化其工作的长期价值。


<details>
  <summary>Details</summary>
Motivation: HPC中心的基础设施与用户本地系统差异大，用户常开发与特定HPC中心紧密耦合的数字制品，导致重复工作。现有FAIR实践多限于特定学科领域，形成信息孤岛，限制了跨学科协作。

Method: 基于欧洲开放科学云EOSC-Life FAIR工作流协作平台的架构，提出针对HPC需求定制的模型，强调使单个工作流组件符合FAIR原则，而非整个工作流。

Result: 提出了一个组件化的FAIR生态系统模型，支持研究人员更有效地发现、共享和重用计算组件，促进跨学科研究协作。

Conclusion: HPC中心应发挥更积极作用，通过基于组件的FAIR方法培育跨学科生态系统，以解决数字制品重复开发问题，提高研究效率和长期价值。

Abstract: High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas](https://arxiv.org/abs/2512.02062)
*Issa Oe,Keiichiro Yamamura,Hiroki Ishikura,Ryo Hamahira,Katsuki Fujisawa*

Main category: cs.CR

TL;DR: 论文提出了一种基于超像素的黑盒对抗攻击方法Superpixel Attack，通过使用超像素区域代替传统矩形区域，并结合新的搜索策略，提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在安全关键任务中应用广泛，但对输入的小扰动很敏感。现有的黑盒对抗攻击方法使用简单的矩形区域进行扰动修改，效果有限。需要更强大的黑盒攻击方法来开发更有效的防御机制。

Method: 提出Superpixel Attack方法：1）使用超像素代替传统矩形区域，在颜色方差和紧凑性之间取得更好平衡；2）提出新的搜索方法versatile search；3）结合超像素和versatile search进行对抗攻击。

Result: Superpixel Attack相比现有攻击方法平均提高了2.10%的攻击成功率。考虑到研究中使用的模型大多具有对抗鲁棒性，这一改进对黑盒对抗攻击具有重要意义。

Conclusion: 基于超像素的对抗攻击方法Superpixel Attack通过改进扰动区域的选择和搜索策略，显著提高了黑盒对抗攻击的效果，为开发更强大的防御机制提供了基础。

Abstract: Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.

</details>


### [10] [Large Language Model based Smart Contract Auditing with LLMBugScanner](https://arxiv.org/abs/2512.02069)
*Yining Yuan,Yifei Wang,Yichang Xu,Zachary Yahn,Sihao Hu,Ling Liu*

Main category: cs.CR

TL;DR: LLMBugScanner是一个基于大语言模型的智能合约漏洞检测框架，通过微调和集成学习提升检测效果


<details>
  <summary>Details</summary>
Motivation: 智能合约审计对大语言模型存在挑战：不同预训练模型推理能力各异，单个模型无法在所有漏洞类型或合约结构上表现一致，即使微调后仍有局限

Method: 结合领域知识适应和集成推理：1）通过领域知识适应，在互补数据集上微调LLMs，捕捉通用代码语义和指令引导的漏洞推理，使用参数高效调优降低计算成本；2）通过集成推理，利用多个LLMs的互补优势，应用基于共识的冲突解决策略

Result: LLMBugScanner在多个流行LLMs上实验，相比预训练和微调的单模型，实现了准确率的一致提升和更强的泛化能力

Conclusion: LLMBugScanner为智能合约审计提供了一个原则性、成本效益高且可扩展的框架

Abstract: This paper presents LLMBugScanner, a large language model (LLM) based framework for smart contract vulnerability detection using fine-tuning and ensemble learning. Smart contract auditing presents several challenges for LLMs: different pretrained models exhibit varying reasoning abilities, and no single model performs consistently well across all vulnerability types or contract structures. These limitations persist even after fine-tuning individual LLMs.
  To address these challenges, LLMBugScanner combines domain knowledge adaptation with ensemble reasoning to improve robustness and generalization. Through domain knowledge adaptation, we fine-tune LLMs on complementary datasets to capture both general code semantics and instruction-guided vulnerability reasoning, using parameter-efficient tuning to reduce computational cost. Through ensemble reasoning, we leverage the complementary strengths of multiple LLMs and apply a consensus-based conflict resolution strategy to produce more reliable vulnerability assessments.
  We conduct extensive experiments across multiple popular LLMs and compare LLMBugScanner with both pretrained and fine-tuned individual models. Results show that LLMBugScanner achieves consistent accuracy improvements and stronger generalization, demonstrating that it provides a principled, cost-effective, and extensible framework for smart contract auditing.

</details>


### [11] [Deterministic Random Bit Generators Based on Ascon for Embedded Systems](https://arxiv.org/abs/2512.02082)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 本文提出三种基于Ascon轻量级密码标准的DRBG构造方案，在树莓派平台上实现，相比现有DRBG实现具有更高计算效率和更低内存占用，适合嵌入式系统部署。


<details>
  <summary>Details</summary>
Motivation: DRBG作为随机数生成和密码应用的基础组件，在资源受限的嵌入式系统中性能和安全性尤为关键。NIST在2025年8月建立了Ascon轻量级密码标准，适合资源有限的嵌入式环境使用。

Method: 修订DRBG标准，提出三种Ascon驱动的构造方案：Ascon驱动基于哈希的DRBG、Ascon驱动基于HMAC的DRBG、Ascon驱动基于计数器模式(CTR)的DRBG。在树莓派平台上实现这些方法。

Result: 实验结果表明，提出的方法相比现有DRBG实现具有更高的计算效率和更低的内存使用量，适合在嵌入式系统中部署。

Conclusion: 基于Ascon轻量级密码标准的DRBG构造方案在嵌入式系统中表现出优越的性能和资源效率，为资源受限环境提供了高性能且安全的随机数生成解决方案。

Abstract: As the Deterministic Random Bit Generator (DRBG) serves as a fundamental component in random number generation and cryptographic applications, its performance and security are particularly critical in resource-constrained embedded systems, where memory capacity and computational efficiency are limited. Establishing a high-performance and secure DRBG is therefore an important issue for embedded system design. Furthermore, the National Institute of Standards and Technology (NIST) established the Ascon lightweight cryptographic standard in August 2025, which is suitable for use in resource-limited embedded environments. Therefore, this study revises the DRBG standard and proposes three Ascon-driven constructions: the Ascon-Driven Hash-Based DRBG, the Ascon-Driven keyed-Hash Message Authentication Code (HMAC) DRBG, and the Ascon-Driven Counter-mode (CTR) DRBG. In the experiments, these methods are implemented on a Raspberry Pi platform. The experimental results demonstrate that the proposed approaches achieve higher computational efficiency and lower memory usage compared with existing DRBG implementations, making them suitable for deployment in embedded systems.

</details>


### [12] [A survey about Hidden Subgroup Problem from a mathematical and cryptographic perspective](https://arxiv.org/abs/2512.02087)
*Simone Dutto,Pietro Mercuri,Nadir Murru,Lorenzo Romano*

Main category: cs.CR

TL;DR: 本文综述了隐藏子群问题（HSP）及其在密码学安全研究中的重要性，涵盖了阿贝尔和非阿贝尔情况，重点讨论了与密码学问题的联系。


<details>
  <summary>Details</summary>
Motivation: 隐藏子群问题在公钥密码系统安全性研究中扮演重要角色，需要系统梳理该问题的研究现状，特别是其与密码学问题的联系，为密码学研究者提供全面的视角。

Method: 首先回顾阿贝尔情况下的Kitaev算法，展示经典问题如何转化为阿贝尔HSP实例；然后分析非阿贝尔HSP的研究现状，重点关注与密码学相关的群结构，包括二面体群、对称群和半直积构造；最后描述处理非阿贝尔HSP的主要技术方法。

Result: 阿贝尔HSP已有高效的量子算法解决方案，而非阿贝尔HSP尚无通用的高效量子算法；论文系统梳理了HSP与密码学问题的联系，包括最短向量问题、图同构问题和码等价问题等。

Conclusion: 隐藏子群问题是连接量子计算和密码学安全的重要桥梁，虽然阿贝尔情况已解决，但非阿贝尔情况仍是开放问题，其研究对评估密码系统的量子安全性具有重要意义。

Abstract: We provide a survey on the Hidden Subgroup Problem (HSP), which plays an important role in studying the security of public-key cryptosystems. We first review the abelian case, where Kitaev's algorithm yields an efficient quantum solution to the HSP, recalling how classical problems (such as order finding, integer factorization, and discrete logarithm) can be formulated as abelian HSP instances. We then examine the current state of the art for non-abelian HSP, where no general efficient quantum solution is known, focusing on some relevant groups including dihedral group (connected to the shortest vector problem), symmetric groups (connected to the graph isomorphism problem), and semidirect product constructions (connected, in a special case, to the code equivalence problem). We also describe the main techniques for addressing the HSP in non-abelian cases, namely Fourier sampling and the black-box approach. Throughout the paper, we highlight the mathematical notions required and exploited in this context, providing a cryptography-oriented perspective.

</details>


### [13] [Factor(T,U): Factored Cognition Strengthens Monitoring of Untrusted AI](https://arxiv.org/abs/2512.02157)
*Aaron Sandoval,Cody Rushing*

Main category: cs.CR

TL;DR: 该研究开发了基于分解认知的AI控制协议，通过可信模型分解任务、不可信模型独立解决子任务的方式提升AI安全性，在APPS编程环境中测试显示安全性从41%提升到63%。


<details>
  <summary>Details</summary>
Motivation: 现有AI控制协议依赖较弱监控器检测不安全行为，但在复杂任务中往往失败，因为监控器无法完全理解任务。需要开发更有效的控制协议来应对可能故意颠覆的不可信AI。

Method: 开发基于分解认知的控制协议：1）可信模型将任务分解为子任务；2）不可信模型在隔离环境中独立解决每个子任务；3）结果重新组装成完整解决方案。在APPS编程环境中实现协议，并使用对抗性GPT-4.1 Nano进行红队测试。

Result: 1）在可信监控协议中添加分解认知可将安全性从41%提升到63%；2）安全性提升主要因为监控器性能改善；3）分解认知不会使有能力的LLM在APPS中更难编写后门；4）协议在APPS中实用性较低，但在更复杂任务中具有潜力。

Conclusion: 基于分解认知的控制协议能有效提升AI安全性，特别是在复杂任务中。虽然当前在APPS环境中实用性有限，但为更复杂的AI控制场景提供了有前景的方向。

Abstract: The field of AI Control seeks to develop robust control protocols, deployment safeguards for untrusted AI which may be intentionally subversive. However, existing protocols that rely on weaker monitors to detect unsafe behavior often fail on complex tasks beyond the monitor's comprehension. We develop control protocols based on factored cognition, in which a trusted model decomposes a task, an untrusted model solves each resultant child task in isolation, and the results are reassembled into a full solution. These protocols may improve safety by several means, such as by simplifying the context for monitors, or by obscuring vulnerabilities in the environment. We implement our protocols in the APPS coding setting and red team them against backdoor attempts from an adversarial GPT-4.1 Nano. We find that: (i) Adding factored cognition to a trusted monitoring protocol can boost safety from 41% to 63%; (ii) Safety improves because monitor performance improves; (iii) Factored cognition makes it no harder for capable LLMs to write backdoors in APPS. While our protocols show low usefulness in APPS, they hold promise for more complex tasks.

</details>


### [14] [PhishSnap: Image-Based Phishing Detection Using Perceptual Hashing](https://arxiv.org/abs/2512.02243)
*Md Abdul Ahad Minhaz,Zannatul Zahan Meem,Md. Shohrab Hossain*

Main category: cs.CR

TL;DR: PhishSnap是一个基于感知哈希的隐私保护钓鱼检测系统，通过浏览器扩展捕获网页截图并计算视觉哈希，与合法模板对比来识别视觉相似的钓鱼攻击，在2024年数据集上达到0.79准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击是最普遍的在线威胁之一，利用人类信任窃取敏感凭证。现有的基于URL和HTML的检测系统难以应对混淆和视觉欺骗技术，需要更有效的检测方法。

Method: PhishSnap是一个隐私保护的本地钓鱼检测系统，采用感知哈希技术。作为浏览器扩展实现，捕获网页截图，计算视觉哈希，并与合法网站模板进行相似性比较来识别钓鱼攻击。

Result: 使用2024年收集的10,000个URL数据集（70%/20%/10%训练/验证/测试），系统达到0.79准确率、0.76精确率和0.78召回率。由于安全清除，部分钓鱼页面不可用，影响了数据集多样性。

Conclusion: 视觉相似性仍然是有效的反钓鱼措施，PhishSnap的本地推理过程确保了用户隐私和低延迟，为钓鱼检测提供了新的视觉方法。

Abstract: Phishing remains one of the most prevalent online threats, exploiting human trust to harvest sensitive credentials. Existing URL- and HTML-based detection systems struggle against obfuscation and visual deception. This paper presents \textbf{PhishSnap}, a privacy-preserving, on-device phishing detection system leveraging perceptual hashing (pHash). Implemented as a browser extension, PhishSnap captures webpage screenshots, computes visual hashes, and compares them against legitimate templates to identify visually similar phishing attempts. A \textbf{2024 dataset of 10,000 URLs} (70\%/20\%/10\% train/validation/test) was collected from PhishTank and Netcraft. Due to security takedowns, a subset of phishing pages was unavailable, reducing dataset diversity. The system achieved \textbf{0.79 accuracy}, \textbf{0.76 precision}, and \textbf{0.78 recall}, showing that visual similarity remains a viable anti-phishing measure. The entire inference process occurs locally, ensuring user privacy and minimal latency.

</details>


### [15] [CVE Breadcrumbs: Tracking Vulnerabilities Through Versioned Apache Libraries](https://arxiv.org/abs/2512.02259)
*Derek Garcia,Briana Lee,Ibrahim Matar,David Rickards,Andrew Zilnicki*

Main category: cs.CR

TL;DR: 对Apache软件基金会生态系统进行历史安全漏洞分析，研究漏洞生命周期、披露时间线和修复实践，提供数据集和开发者建议


<details>
  <summary>Details</summary>
Motivation: Apache生态系统支撑着大量现代软件基础设施，但其广泛使用使其成为高影响安全漏洞的主要目标（如Log4Shell事件）。尽管被广泛采用，Apache项目仍面临重复和严重的安全弱点，需要进行系统性分析来改进安全实践。

Method: 通过编译全面的CVE和CWE数据集，对Apache生态系统进行历史分析。研究四个关键问题：1）Apache库中最持久和重复的CWE；2）CVE在被解决前持续存在的时间；3）CVE引入与官方披露之间的延迟；4）披露后CVE被修复的时间。分析24,285个Apache库、1,285个CVE和157个CWE。

Result: 提供了Apache库漏洞生命周期的详细时间线分析，包括漏洞持久性、披露延迟和修复时间。贡献包括涵盖24,285个Apache库、1,285个CVE和157个CWE的策划数据集，以及实证发现和面向开发者的建议。

Conclusion: Apache生态系统存在重复的安全弱点模式，需要改进安全编码实践、漏洞监控和修复策略。研究提供了对漏洞生命周期的深入理解，并为开发者提供了改进安全性的具体建议。

Abstract: The Apache Software Foundation (ASF) ecosystem underpins a vast portion of modern software infrastructure, powering widely used components such as Log4j, Tomcat, and Struts. However, the ubiquity of these libraries has made them prime targets for high-impact security vulnerabilities, as illustrated by incidents like Log4Shell. Despite their widespread adoption, Apache projects are not immune to recurring and severe security weaknesses. We conduct a historical analysis of the Apache ecosystem to follow the "breadcrumb trail of vulnerabilities" by compiling a comprehensive dataset of Common Vulnerabilities and Exposures (CVEs) and Common Weakness Enumerations (CWEs). We examine trends in exploit recurrence, disclosure timelines, and remediation practices. Our analysis is guided by four key research questions: (1) What are the most persistent and repeated CWEs in Apache libraries? (2) How long do CVEs persist before being addressed? (3) What is the delay between CVE introduction and official disclosure? and (4) How long after disclosure are CVEs remediated? We present a detailed timeline of vulnerability lifecycle stages across Apache libraries and offer insights to improve secure coding practices, vulnerability monitoring, and remediation strategies. Our contributions include a curated dataset covering 24,285 Apache libraries, 1,285 CVEs, and 157 CWEs, along with empirical findings and developer-focused recommendations.

</details>


### [16] [COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers](https://arxiv.org/abs/2512.02318)
*Junyu Wang,Changjia Zhu,Yuanbo Zhou,Lingyao Li,Xu He,Junjie Xiong*

Main category: cs.CR

TL;DR: MLLMs能有效破解视觉验证码，特别是识别类和低交互类任务，但难以处理需要精确定位、多步空间推理或跨帧一致性的任务。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型如何破坏视觉验证码的安全保障，识别攻击面，评估现有模型对验证码的破解能力。

Method: 评估7个领先的商业和开源MLLM在18种真实验证码任务类型上的表现，测量单次准确率、有限重试成功率、端到端延迟和每次破解成本，分析提示工程和少样本演示的影响。

Result: MLLM能可靠地以类似人类的成本和延迟解决识别导向和低交互验证码任务，但需要细粒度定位、多步空间推理或跨帧一致性的任务对当前模型仍显著困难。

Conclusion: 通过分析MLLM的推理轨迹，研究模型在特定验证码谜题上成功/失败的机制，为平台运营商提供防御导向的验证码选择和强化指南。

Abstract: This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).

</details>


### [17] [Leveraging Large Language Models to Bridge On-chain and Off-chain Transparency in Stablecoins](https://arxiv.org/abs/2512.02418)
*Yuexin Xiang,Yuchen Lei,SM Mahir Shazeed Rish,Yuanzhe Zhang,Qin Wang,Tsz Hon Yuen,Jiangshan Yu*

Main category: cs.CR

TL;DR: 该论文提出了一个基于大语言模型的自动化框架，用于桥接稳定币的链上发行数据和链下披露声明，以增强透明度和审计能力。


<details>
  <summary>Details</summary>
Motivation: 稳定币（如USDT和USDC）通过发行控制和储备证明来维持价格稳定，但实际上透明度被分割在两个世界：可验证的链上痕迹和链下非结构化文本披露，两者之间缺乏连接。

Method: 1. 使用LLM通过文档解析和语义对齐来捕获和分析链上链下数据，从发行人证明中提取关键财务指标并映射到相应的链上指标；2. 在多链发行记录和披露文档中集成模型上下文协议（MCP）框架，标准化LLM对定量市场数据和定性披露叙述的访问；3. 建立统一的检索和上下文对齐框架，促进异构稳定币信息源的一致分析。

Result: 研究发现披露数据和可验证数据之间存在系统性差距，LLM辅助分析能够增强跨模态透明度，支持去中心化金融（DeFi）中的自动化、数据驱动的审计。

Conclusion: LLM能够在区块链分析中跨异构数据模态操作，量化报告流通量与观察流通量之间的差异，并检查其对跨链透明度和价格动态的影响，为稳定币透明度提供了新的分析框架。

Abstract: Stablecoins such as USDT and USDC aspire to peg stability by coupling issuance controls with reserve attestations. In practice, however, the transparency is split across two worlds: verifiable on-chain traces and off-chain disclosures locked in unstructured text that are unconnected. We introduce a large language model (LLM)-based automated framework that bridges these two dimensions by aligning on-chain issuance data with off-chain disclosure statements. First, we propose an integrative framework using LLMs to capture and analyze on- and off-chain data through document parsing and semantic alignment, extracting key financial indicators from issuer attestations and mapping them to corresponding on-chain metrics. Second, we integrate multi-chain issuance records and disclosure documents within a model context protocol (MCP) framework that standardizes LLMs access to both quantitative market data and qualitative disclosure narratives. This framework enables unified retrieval and contextual alignment across heterogeneous stablecoin information sources and facilitates consistent analysis. Third, we demonstrate the capability of LLMs to operate across heterogeneous data modalities in blockchain analytics, quantifying discrepancies between reported and observed circulation and examining their implications for cross-chain transparency and price dynamics. Our findings reveal systematic gaps between disclosed and verifiable data, showing that LLM-assisted analysis enhances cross-modal transparency and supports automated, data-driven auditing in decentralized finance (DeFi).

</details>


### [18] [Equilibrium SAT based PQC: New aegis against quantum computing](https://arxiv.org/abs/2512.02598)
*Keum-Bae Cho*

Main category: cs.CR

TL;DR: 提出一种基于多重集随机子集元素计数的新型公钥密码算法，具有速度快、无需大数运算、量子安全等优势


<details>
  <summary>Details</summary>
Motivation: 随着互联网和量子计算的发展，传统公钥密码算法的计算复杂度不断增加，需要开发更高效、量子安全的替代方案

Method: 通过从多重集中随机抽取子集并计算元素数量来生成密文，使用可满足性问题进行数学精炼

Result: 算法速度显著快于其他公钥算法，无需大数运算可在任何设备上执行，且能扩展为单公钥多私钥的量子安全系统

Conclusion: 该算法提供了一种高效、轻量级且量子安全的公钥密码学新方法，具有实际应用潜力

Abstract: Public-key cryptography algorithms have evolved towards increasing computational complexity to hide desired messages, which is accelerating with the development of the Internet and quantum computing. This paper introduces a novel public-key cryptography algorithm that generates ciphertexts by counting the number of elements in randomly extracted subsets from a multiset. After explaining the novel cryptographic concept, the process of mathematically refining it using satisfiability problems is described. The advantages of the proposed algorithm are: first, it is significantly faster than other public-key algorithms; second, it does not require big numbers, making it executable on any devices; and third, it can be easily extended into a public-key cryptosystem using a single public key and multiple private keys while maintaining quantum resistance.

</details>


### [19] [Semigroup action based on skew polynomial evaluation with applications to Cryptography](https://arxiv.org/abs/2512.02603)
*Daniel Camazón-Portela,Juan Antonio López-Ramos*

Main category: cs.CR

TL;DR: 该论文提出了一种基于斜多项式环的公开密钥交换协议，通过控制环的非交换性来确保安全性


<details>
  <summary>Details</summary>
Motivation: 利用斜多项式环的非交换代数结构来构建安全的密码学协议，特别是在Canetti-Krawczyk模型下安全的密钥交换协议

Method: 引入斜多项式环在有限域上的作用，基于多项式赋值和左斜积概念，构造特定子集T(X)来控制环的非交换性

Result: 成功构建了一个在Canetti-Krawczyk模型下安全的公开密钥交换协议

Conclusion: 斜多项式环的非交换代数结构为构建安全的密码学协议提供了新的数学基础，所提出的协议在标准安全模型下具有安全性保证

Abstract: Through this work we introduce an action of the skew polynomial ring $\mathbb{F}_{q}\left[X; σ, δ\right]$ over $\mathbb{F}_{q}$ based on its polynomial valuation and the concept of left skew product of functions. This lead us to explore the construction of a certain subset $\mathcal{T}(X)\subset\mathbb{F}_{q}\left[X; σ, δ\right]$ that allow us to control the non-commutativity of this ring, and exploit this fact in order to build a public key exchange protocol that is secure in Canetti and Krawczyk model.

</details>


### [20] [CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography](https://arxiv.org/abs/2512.02625)
*Mayar Elfares,Pascal Reisert,Tilman Dietz,Manpa Barman,Ahmed Zaki,Ralf Küsters,Andreas Bulling*

Main category: cs.CR

TL;DR: CryptoQA是首个专门为密码学设计的大规模问答数据集，包含200多万个QA对，用于评估和训练LLMs在密码学任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多通用NLP任务上表现出色，但它们在深度推理和数学分析方面的能力，特别是在密码学等复杂任务上，仍然缺乏充分理解，主要原因是缺乏合适的评估和训练数据。

Method: 创建CryptoQA数据集，包含从学术资源中提取的200多万个QA对，并带有上下文元数据。在15个最先进的LLMs上进行基准测试，评估事实准确性、数学推理、一致性、引用、反向推理和对对抗样本的鲁棒性。

Result: LLMs在密码学任务上表现出显著的性能缺陷，特别是在需要形式推理和精确数学知识的任务上。使用CryptoQA进行微调的LLMs在密码学任务上表现出更好的性能。

Conclusion: 迫切需要专门为密码学研究和开发定制的LLM助手，CryptoQA数据集为评估和提升LLMs在密码学领域的能力提供了重要资源。

Abstract: Large language models (LLMs) excel at many general-purpose natural language processing tasks. However, their ability to perform deep reasoning and mathematical analysis, particularly for complex tasks as required in cryptography, remains poorly understood, largely due to the lack of suitable data for evaluation and training. To address this gap, we present CryptoQA, the first large-scale question-answering (QA) dataset specifically designed for cryptography. CryptoQA contains over two million QA pairs drawn from curated academic sources, along with contextual metadata that can be used to test the cryptographic capabilities of LLMs and to train new LLMs on cryptographic tasks. We benchmark 15 state-of-the-art LLMs on CryptoQA, evaluating their factual accuracy, mathematical reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples. In addition to quantitative metrics, we provide expert reviews that qualitatively assess model outputs and establish a gold-standard baseline. Our results reveal significant performance deficits of LLMs, particularly on tasks that require formal reasoning and precise mathematical knowledge. This shows the urgent need for LLM assistants tailored to cryptography research and development. We demonstrate that, by using CryptoQA, LLMs can be fine-tuned to exhibit better performance on cryptographic tasks.

</details>


### [21] [Decryption thorough polynomial ambiguity: noise-enhanced high-memory convolutional codes for post-quantum cryptography](https://arxiv.org/abs/2512.02822)
*Meir Ariel*

Main category: cs.CR

TL;DR: 提出了一种基于有向图解密和噪声增强高内存卷积码的后量子密码学新方法，通过随机化生成矩阵和噪声注入实现比Classic McEliece高2^200倍的安全裕度，支持任意明文长度和线性时间解密。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够抵抗量子攻击、具有更高安全裕度且设计灵活的后量子密码学方案，克服现有方案如Classic McEliece在安全性和灵活性方面的限制。

Method: 采用有向图解密技术处理噪声增强的高内存卷积码，通过随机化生成矩阵隐藏代数结构，在解密过程中故意注入强噪声（来自多项式除法），合法接收者保持多项式时间解码而攻击者面临指数时间复杂性。

Result: 方案实现了超过Classic McEliece 2^200倍的安全裕度，支持任意明文长度和线性时间解密，具有均匀的每比特计算成本，可通过并行有向图解码器阵列实现高效硬件和软件部署。

Conclusion: 该方法代表了稳健、可扩展且量子抵抗的公钥密码学的有前景候选方案，在安全性、设计灵活性和实际部署方面具有显著优势。

Abstract: We present a novel approach to post-quantum cryptography that employs directed-graph decryption of noise-enhanced high-memory convolutional codes. The proposed construction generates random-like generator matrices that effectively conceal algebraic structure and resist known structural attacks. Security is further reinforced by the deliberate injection of strong noise during decryption, arising from polynomial division: while legitimate recipients retain polynomial-time decoding, adversaries face exponential-time complexity. As a result, the scheme achieves cryptanalytic security margins surpassing those of Classic McEliece by factors exceeding 2^(200). Beyond its enhanced security, the method offers greater design flexibility, supporting arbitrary plaintext lengths with linear-time decryption and uniform per-bit computational cost, enabling seamless scalability to long messages. Practical deployment is facilitated by parallel arrays of directed-graph decoders, which identify the correct plaintext through polynomial ambiguity while allowing efficient hardware and software implementations. Altogether, the scheme represents a compelling candidate for robust, scalable, and quantum-resistant public-key cryptography.

</details>


### [22] [Belobog: Move Language Fuzzing Framework For Real-World Smart Contracts](https://arxiv.org/abs/2512.02918)
*Wanxu Xia,Ziqiao Kong,Zhengwei Li,Yi Lu,Pan Li,Liqun Yang,Yang Liu,Xiapu Luo,Shaohua Li*

Main category: cs.CR

TL;DR: Belobog是首个针对Move智能合约的模糊测试框架，通过类型感知的测试用例生成和符号执行技术，能够有效检测Move合约中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: Move语言虽然具有强大的静态类型系统和显式资源语义，但其类型系统仍无法覆盖所有安全漏洞。现有的智能合约模糊测试工具由于Move的强类型特性，难以生成语法或语义有效的交易来测试Move合约。

Method: Belobog采用类型感知的方法：1) 基于Move类型系统构建类型图；2) 根据类型图派生的图迹生成或变异交易；3) 设计实现符号执行器来克服Move合约中的复杂检查。

Result: 在109个真实世界的Move智能合约项目上评估，Belobog能够检测出100%的关键漏洞和79%的主要漏洞（与人工审计结果对比）。成功复现了Cetus和Nemo两个近期著名安全事件的全部利用过程。

Conclusion: Belobog是首个有效的Move智能合约模糊测试框架，通过类型感知的交易生成和符号执行技术，能够有效发现Move合约中的安全漏洞，为Move生态的安全验证提供了重要工具。

Abstract: Move is a research-oriented programming language design for secure and verifiable smart contract development and has been widely used in managing billions of digital assets in blockchains, such as Sui and Aptos. Move features a strong static type system and explicit resource semantics to enforce safety properties such as the prevention of data races, invalid asset transfers, and entry vulnerabilities. However, smart contracts written in Move may still contain certain vulnerabilities that are beyond the reach of its type system. It is thus essential to validate Move smart contracts. Unfortunately, due to its strong type system, existing smart contract fuzzers are ineffective in producing syntactically or semantically valid transactions to test Move smart contracts. This paper introduces the first fuzzing framework, Belobog, for Move smart contracts. Belobog is type-aware and ensures that all generated and mutated transactions are well-typed. More specifically, for a target Move smart contract, Belobog first constructs a type graph based on Move's type system, and then generates or mutates a transaction based on the graph trace derived from the type graph. In order to overcome the complex checks in Move smart contracts, we further design and implement a concolic executor in Belobog. We evaluated Belobog on 109 real-world Move smart contract projects. The experimental results show that Belobog is able to detect 100\% critical and 79\% major vulnerabilities manually audited by human experts. We further selected two recent notorious incidents in Move smart contracts, i.e., Cetus and Nemo. Belobog successfully reproduced full exploits for both of them, without any prior knowledge.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas,Lucas Cordeiro,Youcheng Sun,Waldir Junior*

Main category: cs.AI

TL;DR: 本文提出了LLM-Verifier收敛定理，为LLM辅助的形式验证提供了首个具有可证明终止性和收敛性保证的形式化框架，通过马尔可夫链建模和大量实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型进行形式验证的方法缺乏可靠的理论基础，导致验证过程不稳定——有时收敛，有时循环，有时偏离稳定轨迹。需要建立形式化框架来提供可证明的保证。

Method: 将LLM与验证器的交互建模为离散时间马尔可夫链，状态转移由关键参数δ（错误减少概率）决定。提出了LLM-Verifier收敛定理，证明对于任何δ>0，过程几乎必然达到验证状态，且期望迭代次数有界E[n]≤4/δ。

Result: 在超过90,000次试验的实证活动中，所有运行都达到了验证，收敛因子紧密聚集在Cf≈1.0附近，实证结果与理论预测高度一致。基于此将工作流划分为三个操作区域：边际、实用和高性能。

Conclusion: 理论保证和实验证据为LLM辅助验证提供了清晰的架构基础，不再需要启发式调优。工程师获得了支持可预测资源规划和性能预算的框架，为部署到安全关键软件环境提供了必要保障。

Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [24] [Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code](https://arxiv.org/abs/2512.02170)
*Pritam Deka,Barry Devereux*

Main category: cs.AI

TL;DR: Flowchart2Mermaid：一个将流程图图像转换为可编辑Mermaid.js代码的轻量级网页系统，支持混合主动式细化编辑


<details>
  <summary>Details</summary>
Motivation: 流程图是常见的流程沟通工具，但通常以静态图像形式分享，难以编辑或重用。现有工具缺乏将流程图图像转换为结构化、版本可控的文本表示的能力。

Method: 使用详细的系统提示和视觉语言模型，将流程图图像转换为Mermaid.js代码。系统支持混合主动式细化，包括内联文本编辑、拖放节点插入和自然语言命令（由集成AI助手解释）。

Result: 开发了一个轻量级网页系统，能够生成结构化、版本可控的文本表示，并与渲染的图表保持同步。引入了评估指标来评估多个模型的结构准确性、流程正确性、语法有效性和完整性。

Conclusion: Flowchart2Mermaid提供了一种将静态流程图图像转换为可编辑、版本可控的Mermaid.js代码的有效方法，支持多种交互式编辑方式，并建立了系统的评估框架。

Abstract: Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present \textsc{Flowchart2Mermaid}, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.

</details>


### [25] [From monoliths to modules: Decomposing transducers for efficient world modelling](https://arxiv.org/abs/2512.02193)
*Alexander Boyd,Franz Nowak,David Hyland,Manuel Baltieri,Fernando E. Rosas*

Main category: cs.AI

TL;DR: 该论文提出了一个分解复杂世界模型（用传感器表示）的框架，通过识别模块化交互的子组件来实现并行化和可解释的建模，为AI安全的结构透明性和实际推理的计算效率搭建桥梁。


<details>
  <summary>Details</summary>
Motivation: 现实世界模型通常计算需求高，但真实场景往往包含模块化交互的子组件，这为高效建模提供了机会。研究旨在开发一个框架来分解复杂的世界模型，实现并行化和可解释的替代方案，以支持分布式推理。

Method: 开发了一个分解由传感器（transducers）表示的复杂世界模型的框架。传感器是推广POMDPs的一类模型。研究重点在于反转传感器组合过程，推导出在不同输入-输出子空间上运行的子传感器。

Result: 提出了一个能够将复杂世界模型分解为子传感器的框架，这些子传感器在独立的输入-输出子空间上运行，从而实现了并行化和可解释的替代方案，支持分布式推理。

Conclusion: 该研究为连接AI安全所需的结构透明性和实际推理所需的计算效率奠定了基础，通过模块化分解复杂世界模型，提供了更高效、更可解释的建模方法。

Abstract: World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.

</details>


### [26] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 该论文分析了当前AI系统的七大核心缺陷，包括缺乏内在自我监控、元认知意识不足、学习机制固定不灵活等，并提出了向认知自主AI范式转变的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在感知、语言、推理和多模态领域取得了快速进展，但现代AI系统在自我监控、自我纠正和动态环境中的自主行为调节方面仍存在根本性限制。论文旨在识别和分析这些限制，并探索超越这些限制的AI发展方向。

Method: 通过比较分析人工系统与生物认知，整合AI研究、认知科学和神经科学的见解，识别出当代AI模型的七大核心缺陷，并提出基于神经认知原理的架构解决方案。

Result: 论文确定了七个制约当代AI模型的核心缺陷：缺乏内在自我监控、元认知意识不足、固定非适应性学习机制、无法重组目标、缺乏表征维护、不足的具身反馈、以及内在能动性缺失。这些结构限制阻碍了深度学习等当前架构实现稳健泛化、终身适应性和真实世界自主性。

Conclusion: 论文主张向认知基础AI（认知自主）进行范式转变，这种AI能够实现自我导向适应、动态表征管理和有意图的目标导向行为，同时配备改革性监督机制，确保自主系统保持可解释性、可治理性并与人类价值观保持一致。

Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [27] [Model Recovery at the Edge under Resource Constraints for Physical AI](https://arxiv.org/abs/2512.02283)
*Bin Xu,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: MERINDA是一个针对边缘设备的FPGA加速模型恢复框架，通过可并行化神经架构替代迭代求解器，显著降低内存和能耗，适合任务关键自主系统的实时操作。


<details>
  <summary>Details</summary>
Motivation: 模型恢复（MR）在任务关键自主系统中实现安全、可解释的决策，但现有基于神经常微分方程的迭代求解方法在FPGA上效率低下，内存和能耗问题阻碍了其在边缘设备上的部署。

Method: 提出MERINDA框架，用可并行化的神经架构替代神经常微分方程中的迭代求解器，专门针对FPGA进行优化加速。

Result: 相比移动GPU，MERINDA实现了近11倍的DRAM使用量降低和2.2倍的运行速度提升；实验揭示了在固定精度下内存与能耗的逆相关关系。

Conclusion: MERINDA框架特别适合资源受限、需要实时操作的任务关键自主系统，解决了模型恢复在边缘设备部署中的内存和能耗瓶颈问题。

Abstract: Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.

</details>


### [28] [Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization](https://arxiv.org/abs/2512.02302)
*Varun Kumar Dasoju,Qingsu Cheng,Zeyun Yu*

Main category: cs.AI

TL;DR: 该研究提出了一种用于乳腺细胞分割的深度学习框架，仅使用599张训练图像就达到了95.5%的Dice分数，显著减少了医学图像标注所需的时间和专家投入。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注需要大量时间和专业知识，特别是乳腺上皮细胞核数据集的标注需要病理学家投入数百小时。这是一个临床感知AI发展的关键瓶颈，需要解决标注数据稀缺的问题。

Method: 1) 使用多尺度Gabor滤波器进行量子启发的边缘增强，创建第四个输入通道；2) 提出稳定的多组件损失函数，整合自适应Dice损失和边界感知项；3) 引入基于复杂度的加权采样策略；4) 采用EfficientNet-B7/UNet++架构，支持4通道到3通道的投影；5) 通过指数移动平均和统计异常值检测进行鲁棒验证。

Result: 框架达到Dice分数95.5% ± 0.3%和IoU 91.2% ± 0.4%。量子增强贡献了2.1%的边界精度提升，加权采样将小病变检测提高了3.8%。在数据极度不平衡（乳腺组织仅占4%像素）的情况下表现出色。

Conclusion: 该研究通过创新的量子增强、损失函数设计和采样策略，在有限标注数据下实现了突破性性能，显著减少了医学专家创建数据集所需的时间，解决了临床感知AI发展的根本瓶颈。

Abstract: Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.

</details>


### [29] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu,Xiaofei Wen,Wenjie Jacky Mo,Tinghui Zhu,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: OmniGuard是首个全模态安全护栏系统，通过结构化安全标签和专家模型蒸馏，在文本、图像、视频、音频等多种模态上实现统一的安全防护。


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型（OLLMs）处理文本、图像、视频和音频时，传统单模态安全防护方法存在局限性，需要跨模态的统一安全框架。

Method: 提出OmniGuard框架，构建包含21万+多样本的全模态安全数据集，通过结构化安全标签和专家模型蒸馏进行训练，实现跨模态的安全推理能力。

Result: 在15个基准测试中表现出强大的有效性和泛化能力，能够处理广泛的多模态安全场景，提供统一的策略执行和风险缓解。

Conclusion: OmniGuard为构建更稳健、更强大的全模态安全系统铺平了道路，实现了跨所有模态的统一安全防护框架。

Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.

</details>


### [30] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue,Weichen Liu,Shiqi Wang,Haoming Wang,Yuyang Wu,Wei Gao*

Main category: cs.AI

TL;DR: 该论文提出了ReMindView-Bench基准测试，用于评估视觉语言模型在多视角空间推理中的表现，发现现有模型在跨视角对齐和视角转换方面存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在多视角空间推理中难以保持几何一致性和跨视角一致性，缺乏能够隔离多视角推理与单视角感知的细粒度基准测试。

Method: 提出了ReMindView-Bench基准测试，系统性地变化视角空间模式和查询类型来探测空间认知的关键因素。使用显性分阶段分析（LLM-as-a-judge和自一致性提示）和隐性分析（线性探测和熵动态）来诊断推理过程。

Result: 评估15个当前VLMs显示，模型在跨视角对齐和视角转换方面存在一致性的失败。模型在单帧感知方面表现良好，但在跨视角信息整合时性能急剧下降。隐性分析显示任务相关信息逐渐丢失，正确与错误推理轨迹之间存在不确定性分离。

Conclusion: 该研究提供了基于认知科学的VLM空间推理诊断，揭示了多视角空间心理模型在推理过程中如何形成、退化和失稳。ReMindView-Bench基准测试可用于深入分析VLM的空间推理能力。

Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.

</details>


### [31] [Synthetic Error Injection Fails to Elicit Self-Correction In Language Models](https://arxiv.org/abs/2512.02389)
*David X. Wu,Shreyas Kapur,Anant Sahai,Stuart Russell*

Main category: cs.AI

TL;DR: 研究探索使用监督学习结合合成错误注入来诱导语言模型自我修正能力，但发现该方法效果有限，无法显著提升性能，即使模型能发现错误也常重复原错误，揭示了分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为激发大语言模型推理和自我修正能力的主要方法，但其计算成本高昂，因此探索替代方案。受自动驾驶和机器人技术启发，研究是否可以通过监督学习结合合成错误注入来诱导语言模型的自我修正能力。

Method: 在推理链中插入人工错误，将其掩码，然后监督模型识别和纠正这些错误。通过合成错误注入的方式进行监督学习训练。

Result: 该方法即使在简单的合成任务上也无法显著提高多个模型的性能。即使模型能发现自己的错误，也常常重复原始错误。研究发现合成错误到在线错误的分布偏移显著降低了微调模型的错误修正能力，即使合成错误覆盖了在线错误。

Conclusion: 研究结果解释了为什么在线强化学习方法在激发自我修正能力方面特别有效。监督学习结合合成错误注入的方法效果有限，无法替代强化学习的独特优势。

Abstract: Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

</details>


### [32] [Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets](https://arxiv.org/abs/2512.02436)
*Agostino Capponi,Alfio Gliozzo,Brian Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种基于智能AI的管道，用于自动聚类预测市场并识别市场间的依赖关系，通过交易策略验证这些关系可产生约20%的周回报率。


<details>
  <summary>Details</summary>
Motivation: 预测市场存在碎片化问题，包括重叠问题、隐含等价关系和隐藏矛盾，需要系统方法来发现市场间的潜在语义结构。

Method: 开发了一个智能AI管道，通过自然语言理解对合约文本和元数据进行聚类分析，识别集群内市场对之间的依赖关系（正相关和负相关）。

Result: 在Polymarket历史数据集上，AI识别的依赖关系准确率达到60-70%，基于这些关系的简单交易策略在一周时间范围内获得约20%的平均回报。

Conclusion: 智能AI和大语言模型能够有效发现预测市场中的潜在语义结构，这些发现的关系可以转化为可操作的交易信号。

Abstract: Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.

</details>


### [33] [Guided Self-Evolving LLMs with Minimal Human Supervision](https://arxiv.org/abs/2512.02472)
*Wenhao Yu,Zhenwen Liang,Chengsong Huang,Kishan Panaganti,Tianqing Fang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: R-Few是一个通过轻量级人类监督实现AI稳定自进化的框架，使用挑战者-求解器自博弈架构，结合上下文基础和混合训练，在数学和通用推理任务上实现持续迭代改进。


<details>
  <summary>Details</summary>
Motivation: AI自进化长期以来被视为通往超智能的路径，但实践中无指导的自进化系统常常快速达到平台期甚至退化。这些问题源于概念漂移、多样性崩溃和错误进化，模型会强化自身偏见并收敛到低熵行为。需要一种稳定可控的自进化方法，同时最小化对人类监督的依赖。

Method: 提出R-Few框架：基于挑战者-求解器的自博弈架构。挑战者采样少量人类标注示例来指导合成问题生成，求解器在基于难度的在线课程下联合训练人类和合成示例。通过上下文基础和混合训练实现轻量级人类监督。

Result: 在数学和通用推理基准测试中，R-Few实现了一致且迭代的改进。例如，Qwen3-8B-Base在数学任务上比R-Zero提高了+3.0分，性能与使用20倍人类数据训练的General-Reasoner相当。消融研究确认了基础挑战者训练和课程求解器训练的互补贡献，分析显示R-Few减轻了漂移，产生更稳定可控的协同进化动态。

Conclusion: R-Few通过轻量级人类监督实现了稳定可控的AI自进化，解决了无指导自进化中的概念漂移和多样性崩溃问题，为AI自主知识获取和精炼提供了有效框架。

Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

</details>


### [34] [COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes](https://arxiv.org/abs/2512.02499)
*Yongkai Liu,Helena Feng,Bin Jiang,Yixin Wang,Max Wintermark,David S. Liebeskind,Michael Moseley,Maarten Lansberg,Gregory Albers,Jeremy Heit,Greg Zaharchuk*

Main category: cs.AI

TL;DR: COPE是一个基于思维链推理的大语言模型框架，用于从非结构化临床笔记预测急性缺血性卒中90天功能结局，性能优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含丰富的上下文信息，但其非结构化特性限制了在传统预测模型中的应用，需要开发能够利用这些信息的预测工具。

Method: 开发了两步思维链框架：第一步基于LLaMA-3-8B生成临床推理，第二步输出改良Rankin量表预测。比较了GPT-4.1、ClinicalBERT、结构化变量机器学习模型和单步LLM。

Result: COPE的MAE为1.01，±1准确率为74.4%，精确准确率为32.8%，性能与GPT-4.1相当，优于ClinicalBERT、临床ML和单步LLM。在不同亚组中表现一致。

Conclusion: COPE作为一个轻量级、可解释、保护隐私的开源框架，为从非结构化临床文本进行结局预测提供了准确实用的解决方案。

Abstract: Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.

</details>


### [35] [Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction](https://arxiv.org/abs/2512.02610)
*Yubo Hou,Mohamed Ragab,Min Wu,Chee-Keong Kwoh,Xiaoli Li,Zhenghua Chen*

Main category: cs.AI

TL;DR: 提出TACDA方法解决跨域剩余使用寿命预测中的领域差异问题，通过目标域重构策略保留目标特定信息，结合聚类配对策略实现退化阶段一致性对齐，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的RUL预测方法通常假设训练和测试数据来自同一分布，但实际工业环境中存在领域差异问题。传统的对抗领域适应方法虽然关注领域不变特征，但忽略了目标特定信息和退化阶段的一致性特征，导致性能不理想。

Method: 提出TACDA方法：1）在对抗适应过程中引入目标域重构策略，在学习领域不变特征的同时保留目标特定信息；2）开发新颖的聚类和配对策略，实现相似退化阶段之间的一致性对齐。

Result: 通过大量实验验证，TACDA方法在两个不同的评估指标上均显著优于现有最先进方法，表现出卓越的性能。

Conclusion: TACDA方法有效解决了跨域RUL预测中的领域差异问题，通过结合目标域重构和退化阶段一致性对齐策略，显著提升了预测性能，为实际工业应用提供了有效的解决方案。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.

</details>


### [36] [Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks](https://arxiv.org/abs/2512.02677)
*Zhiyuan He*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在递归推理问题中的深度泛化能力不足问题，并提出了一种循环定位-替换管道方法来解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在许多任务上表现出色，但在处理需要解决嵌套层次结构的递归推理问题时面临显著挑战。先前研究主要关注长度泛化（模型处理比训练时更长序列的能力），而本研究探讨了一个不同且未被充分研究的限制：深度泛化，即模型处理比训练时更深层次嵌套结构的能力。

Method: 提出了一种新颖的循环定位-替换管道方法，将递归问题分解为可管理的子组件。该方法使用两个专门模型：定位器识别可解的子表达式，替换器评估这些组件同时保持整体结构。

Result: 在三个精心设计的领域（布尔代数、递归算术和命题逻辑）进行评估，每个领域都有可控的递归深度。结果表明，该方法有效缓解了在分布外递归深度测试时的性能衰减。

Conclusion: 标准Transformer架构在处理比训练时更深的递归问题时存在困难，这源于其无法维持类似堆栈的行为来跟踪和解决多层嵌套依赖。提出的循环定位-替换管道方法能够有效解决这一深度泛化挑战。

Abstract: Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.

</details>


### [37] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha,Jeong Hun Yeo,Junil Won,Se Jin Park,Yong Man Ro*

Main category: cs.AI

TL;DR: MIGR框架通过模态重要性引导推理，解决多模态情感理解中的推理漂移问题，从情感主导模态开始推理，提高解释的可靠性


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感理解方法存在推理漂移问题：模型逐渐依赖自身生成的文本而非多模态证据，且解释过度受视觉主导推理路径影响，导致解释与情感不一致

Method: 提出模态重要性机制识别情感主导模态，基于此重新组织推理序列；采用两阶段框架：模态对齐监督微调和模态感知奖励优化，生成情感基础、因果相关且连贯的解释

Result: 在DFEW基准测试中，MIGR显著提高推理可靠性，将正确预测但情感不一致解释的比例从18.10%降至7.37%

Conclusion: 从情感主导模态开始推理能有效提高多模态情感理解的可靠性，MIGR框架为解决推理漂移问题提供了有效方案

Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [38] [Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs](https://arxiv.org/abs/2512.02713)
*Theodoros Aivalis,Iraklis A. Klampanos,Antonis Troumpoukis,Joemon M. Jose*

Main category: cs.AI

TL;DR: 提出一个通过构建本体对齐知识图谱来解释生成模型输出的框架，用于追踪训练数据对生成内容的影响，支持版权分析和AI透明度


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能力增强，透明度、问责制和版权侵权问题日益突出。理解特定训练数据如何影响模型输出变得至关重要，需要开发能够追溯生成内容来源的方法

Method: 利用多模态大语言模型从图像中提取结构化三元组，构建与领域特定本体对齐的知识图谱。通过比较生成图像和训练图像的知识图谱，追踪潜在影响来源

Result: 通过本地训练模型的遗忘实验和大规模模型的风格特定实验验证了方法的有效性，能够支持版权分析、数据集透明度和可解释AI

Conclusion: 该框架支持开发促进人类协作、创造力和激发好奇心的AI系统，为解决生成模型的透明度和问责制问题提供了实用工具

Abstract: As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.

</details>


### [39] [Menta: A Small Language Model for On-Device Mental Health Prediction](https://arxiv.org/abs/2512.02716)
*Tianyi Zhang,Xiangyuan Xue,Lingyan Ruan,Shiya Fu,Feng Xia,Simon D'Alfonso,Vassilis Kostakos,Hong Jia*

Main category: cs.AI

TL;DR: Menta：首个针对社交媒体心理健康预测优化的轻量级小语言模型，在多任务分类中超越现有SLM和部分LLM，可在移动设备实时部署


<details>
  <summary>Details</summary>
Motivation: 心理健康问题影响全球数亿人，但早期检测仍然有限。虽然大语言模型在心理健康应用中有潜力，但其规模和计算需求阻碍了实际部署。小语言模型提供了轻量级替代方案，但在基于社交媒体的心理健康预测方面尚未充分探索。

Method: 提出Menta模型，首个专门为社交媒体数据多任务心理健康预测优化的SLM。采用LoRA微调框架、跨数据集策略和平衡准确率导向的损失函数，在六个分类任务上进行联合训练。

Result: 相比9个最先进的SLM基线，Menta在抑郁、压力和自杀倾向等任务上平均提升15.2%；在抑郁和压力分类任务上比130亿参数LLM准确率更高，同时模型大小约小3.25倍。可在iPhone 15 Pro Max上实时部署，仅需约3GB内存。

Conclusion: Menta展示了可扩展、隐私保护的心理健康监测潜力，为轻量级心理健康预测模型提供了有效解决方案，支持移动设备实时部署。

Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/

</details>


### [40] [StockMem: An Event-Reflection Memory Framework for Stock Forecasting](https://arxiv.org/abs/2512.02720)
*He Wang,Wenyilin Xiao,Songqiao Han,Hailiang Huang*

Main category: cs.AI

TL;DR: StockMem：基于事件-反思双层记忆框架的股票价格预测方法，通过结构化新闻事件、追踪事件演化、构建因果经验库，实现更准确且可解释的金融预测。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测面临市场波动性和实时事件敏感性的挑战。虽然大语言模型为基于文本的预测提供了新途径，但在金融领域的应用受到噪声新闻数据和文本中缺乏明确答案的限制。通用记忆架构难以识别价格变动的关键驱动因素。

Method: 提出StockMem事件-反思双层记忆框架：1）将新闻结构化处理为事件；2）横向整合每日事件，纵向追踪事件演化以提取反映市场预期差异的增量信息，构建时间事件知识库；3）通过分析事件-价格动态，形成因果经验的反思知识库；4）预测时检索类似历史场景，结合当前事件、增量数据和过去经验进行推理。

Result: 实验表明StockMem优于现有记忆架构，通过追踪影响价格的信息链提供更优越、可解释的推理，增强了金融预测的决策透明度。

Conclusion: StockMem框架通过结构化事件处理、追踪事件演化和构建因果经验库，有效解决了金融预测中的噪声数据和关键驱动因素识别问题，实现了更准确且可解释的股票价格预测。

Abstract: Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.

</details>


### [41] [AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping](https://arxiv.org/abs/2512.02726)
*Md Abdul Kadir,Sai Suresh Macharla Vasu,Sidharth S. Nair,Daniel Sonntag*

Main category: cs.AI

TL;DR: LLMs作为异常检测器在复式记账中优于传统规则方法和机器学习基线，提供自然语言解释，展示了AI增强审计的潜力


<details>
  <summary>Details</summary>
Motivation: 传统审计中的日记账测试（JETs）基于规则的方法产生大量误报，难以检测细微异常，需要更有效的异常检测方法

Method: 使用LLaMA和Gemma等最先进的大语言模型作为复式记账异常检测器，在合成和真实匿名账本上进行基准测试，与传统JETs和机器学习基线比较

Result: LLMs在异常检测方面持续优于传统规则型JETs和经典机器学习基线，同时提供增强可解释性的自然语言解释

Conclusion: 研究结果突显了AI增强审计的潜力，人类审计师可以与基础模型协作，加强财务完整性

Abstract: Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.

</details>


### [42] [Self-Improving AI Agents through Self-Play](https://arxiv.org/abs/2512.02731)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 该论文将心理测量电池的模数理论框架扩展到动力系统领域，将智能体形式化为由计算资源参数化的流，并推导出保证自我改进稳定性的方差不等式条件。


<details>
  <summary>Details</summary>
Motivation: 先前的研究建立了AAI能力分数作为智能体表示空间上的静态泛函，但缺乏对智能体作为动力系统的形式化描述。本文旨在将智能体形式化为由计算资源参数化的流，建立自我改进过程的数学框架。

Method: 提出递归的生成器-验证器-更新器(GVU)算子，证明该算子在参数流形Θ上生成向量场，将自我改进系数κ识别为能力泛函沿该流的李导数。推导出方差不等式这一谱条件，作为自我改进稳定性的充分条件。

Result: 证明κ>0的充分条件是生成和验证的组合噪声足够小（在曲率和步长效应范围内）。将该形式化框架应用于统一语言自我博弈、自我校正和合成数据引导等近期研究，展示STaR、SPIN、Reflexion、GANs和AlphaZero等架构是GVU算子的具体拓扑实现。

Conclusion: 该工作建立了智能体自我改进过程的动力系统理论框架，推导出保证稳定自我改进的数学条件，并将多种现有架构统一在该理论框架下，为理解智能体自我改进机制提供了理论基础。

Abstract: We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.

</details>


### [43] [A Framework for Causal Concept-based Model Explanations](https://arxiv.org/abs/2512.02735)
*Anna Rodum Bjøru,Jacob Lysnæs-Larsen,Oskar Jørgensen,Inga Strümke,Helge Langseth*

Main category: cs.AI

TL;DR: 提出基于因果概念的事后可解释AI框架，通过概念干预的充分概率生成局部和全局解释，强调解释需同时具备可理解性和对模型的忠实性。


<details>
  <summary>Details</summary>
Motivation: 针对非可解释模型，需要既能让人类理解又忠实于原模型的解释方法。现有方法往往难以同时满足这两个要求，因此需要建立概念化的因果解释框架。

Method: 基于因果概念的事后解释框架，通过计算概念干预的充分概率来生成局部和全局解释。使用CelebA数据集上的分类器作为概念验证模型，通过清晰的概念词汇表实现可理解性。

Result: 展示了基于CelebA数据集分类器的示例解释，证明了框架的可理解性（通过概念词汇表）和忠实性（通过强调解释生成与解释解读的上下文一致性）。

Conclusion: 提出的因果概念框架为事后XAI提供了可行的解决方案，强调了解释的可理解性和忠实性之间的平衡，以及解释生成与解读上下文一致的重要性。

Abstract: This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.

</details>


### [44] [Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents](https://arxiv.org/abs/2512.02812)
*Zijie Lin,Qilin Cai,Liang Shen,Mingjun Xiao*

Main category: cs.AI

TL;DR: 提出了一种无需人工设计提示词的协作智能体框架，用于自动提升论文到代码转换的质量，通过验证和精炼两个智能体的协作，相比基线方法在代码准确性和完整性上分别提升了约15%和13%。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化论文复现框架缺乏对每个生成步骤输出的验证和精炼机制，或者过度依赖人工设计的提示词进行自我精炼，这限制了框架的适应性和可扩展性。

Method: 提出了一个无需提示词的协作智能体框架，包含两个智能体：验证智能体检查每个步骤的输出是否满足相应系统提示的要求；精炼智能体根据识别出的问题修订输出。该方法仅利用原始系统提示实现自动验证和改进，无需人工为每个步骤设计特定的精炼提示词。

Result: 在PaperBench Code-Dev和Paper2CodeBench数据集上的实验表明，该方法显著提高了复现代码的准确性和完整性，相比没有智能体的基线方法分别获得了约15%和13%的性能提升。与Self-Refine方法的对比实验验证了该提示词无关方法在不同数据集上的鲁棒性和一致性。

Conclusion: 提出的协作智能体框架通过自动验证和精炼机制，有效提升了论文到代码转换的质量，避免了人工设计提示词的依赖，具有更好的适应性和可扩展性。

Abstract: Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\% and 13\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.

</details>


### [45] [The future of AI in critical mineral exploration](https://arxiv.org/abs/2512.02879)
*Jef Caers*

Main category: cs.AI

TL;DR: 提出基于贝叶斯主义和证伪原则的新科学方法，通过AI减少认知偏差和假阳性，降低勘探成本


<details>
  <summary>Details</summary>
Motivation: 尽管投资增加，但过去20年新矿产发现减少，需要解决关键矿产勘探效率问题

Method: 基于贝叶斯主义和证伪原则的哲学方法，将数据采集视为证伪人类假设的手段，使用可验证指标进行理性决策，需要无监督学习和人机协作AI

Result: 提供实用的勘探协议模板，通过AI驱动的科学方法减少认知偏差和假阳性

Conclusion: AI是实现矿产勘探严谨科学方法的关键使能者，能显著提高勘探效率和降低成本

Abstract: The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage

</details>
