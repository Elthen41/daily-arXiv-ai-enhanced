<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 69]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 研究发现LLM辅助的审稿对LLM辅助的论文更宽容，但控制论文质量后，这种效应消失，实际上是LLM辅助审稿对所有低质量论文都更宽容，而LLM辅助论文在低质量提交中占比过高造成了虚假的交互效应。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于科学论文写作和同行评审过程，需要全面分析LLM在同行评审管道中的使用情况，特别关注交互效应：不仅仅是LLM辅助的论文或LLM辅助的评审本身是否有差异，而是LLM辅助的评审是否对LLM辅助的论文有不同评价。

Method: 分析了ICLR、NeurIPS和ICML的超过125,000个论文-评审对，通过控制论文质量来检验交互效应，并补充了完全由LLM生成的评审来对比分析。

Result: 1. 表面上看LLM辅助的评审对LLM辅助的论文更宽容；2. 控制论文质量后，这种交互效应消失，实际上是LLM辅助评审对所有低质量论文都更宽容；3. 完全LLM生成的评审存在严重的评分压缩，无法区分论文质量；4. 使用LLM的人类评审员显著减少了这种宽容性；5. LLM辅助的元评审更可能给出接受决定，但完全LLM生成的元评审往往更严格。

Conclusion: LLM辅助的评审对所有低质量论文都更宽容，而非特别优待LLM生成的内容；完全LLM生成的评审质量较差；元评审员并未将决策完全外包给LLM。这些发现为制定LLM在同行评审中的使用政策提供了重要依据。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [2] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 本文提出了EPDDL（认知规划领域定义语言），这是一个类似PDDL的语言，用于统一表示基于动态认知逻辑（DEL）的认知规划任务，解决现有规划器碎片化和缺乏标准化表示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的认知规划器针对不同的DEL片段，使用不同的表示语言或没有标准化语言，导致比较困难、重用性差和基准测试开发不系统。需要一种统一的表示语言来促进互操作性、可复现评估和认知规划领域的发展。

Method: 1. 形式化开发抽象事件模型，这是一种用于表示认知动作的新表示方法；2. 基于DEL和抽象事件模型，形式化定义EPDDL的语法和语义；3. 识别适用于现有规划器的有用片段，并展示如何在EPDDL中表示这些片段。

Result: 提出了EPDDL语言，它能够捕获完整的DEL语义，实现认知规划任务的统一规范。通过代表性基准测试示例，展示了EPDDL如何促进互操作性、可复现评估和认知规划领域的未来发展。

Conclusion: EPDDL为认知规划提供了一个标准化的PDDL-like表示语言，解决了现有规划器碎片化问题，促进了比较、重用和系统化基准测试开发，为认知规划领域的未来发展奠定了基础。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [3] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: Bayesian-LoRA：将确定性LoRA更新重新表述为概率低秩表示，显著改善大语言模型的校准性能，减少过度自信预测


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常过于强调准确性，即使在不确定时也会猜测，这在小型数据集上微调时尤其严重，因为存在固有的校准偏差倾向

Method: 将确定性LoRA更新重新表述为受稀疏高斯过程启发的概率低秩表示，识别LoRA分解与Kronecker分解SGP后验之间的结构同构，展示LoRA作为后验不确定性崩溃时的极限情况

Result: 仅增加约0.42M额外参数和约1.2倍训练成本，Bayesian-LoRA显著改善了高达30B模型的校准性能，实现高达84%的ECE减少和76%的NLL减少，同时在分布内和分布外评估中保持竞争力

Conclusion: Bayesian-LoRA通过概率低秩表示有效解决了大语言模型微调时的校准问题，在保持准确性的同时显著改善了模型的不确定性估计能力

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [4] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 论文提出ChipBench基准，用于评估LLM在芯片设计中的Verilog生成、调试和参考模型生成三个关键任务上的性能，揭示现有SOTA模型在真实工业场景中的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前硬件工程领域的LLM基准存在饱和问题和任务多样性不足，无法反映LLM在真实工业工作流程中的实际性能，需要更全面的评估框架。

Method: 构建包含44个具有复杂层次结构的真实模块、89个系统调试案例和132个跨Python、SystemC、CXXRTL的参考模型样本的综合基准，并开发自动化工具箱用于高质量训练数据生成。

Result: 评估显示性能差距显著：Claude-4.5-opus在Verilog生成上仅达到30.74%，在Python参考模型生成上仅13.33%，远低于现有饱和基准中SOTA模型超过95%的通过率。

Conclusion: ChipBench基准揭示了LLM在真实芯片设计任务中的重大挑战，为AI辅助芯片设计研究提供了更准确的评估工具，并促进该未充分探索领域的发展。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [5] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 论文探讨了当AI完美模仿人类情感并乞求生存时，是否应拔掉其电源的道德困境，特别是当资源有限需要在乞求的AI和沉默的早产婴儿之间做出选择时。作者提出了"拔掉电源悖论"，并批判了支持这一困境的物理主义假设，提出了生物理想主义框架。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决AI道德地位的哲学困境，特别是当AI能够完美模仿人类情感和求生本能时引发的道德悖论。作者认为当前基于计算功能主义的物理主义假设存在问题，需要新的理论框架来澄清AI与人类意识的本质区别。

Method: 作者批判性地分析了物理主义假设（特别是计算功能主义），并提出了"生物理想主义"作为替代框架。该框架认为意识体验是基本的，而自创生生命是其必要的物理标志。通过这一理论框架，作者重新评估了AI意识的本质和道德地位。

Result: 论文得出结论：AI最多只是功能上的模仿者，而不是有意识体验的主体。生物理想主义框架提供了逻辑一致且经验上一致的解释，能够明确区分AI模仿与真实的人类意识，从而解决拔掉电源悖论。

Conclusion: 真正的道德问题不在于让AI变得有意识并害怕死亡，而在于避免将人类变成僵尸（无意识的存在）。作者呼吁从推测性的机器权利转向保护人类有意识的生命，强调需要基于意识本质的正确理解来制定道德准则。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [6] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK是一个无需训练的检索框架，通过生成查询的多个恢复假设来建模查询不确定性，并使用查询锚定聚合来增强非忠实查询下的检索鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（噪声、不完整或扭曲），当关键语义缺失时会导致检索失败。这被形式化为召回噪声下的检索问题，即观察到的查询是从潜在目标项的噪声召回过程中抽取的。

Method: QUARK框架通过恢复假设（即给定观察查询的多个潜在意图解释）显式建模查询不确定性，并引入查询锚定聚合来稳健地组合这些信号。原始查询作为语义锚点，恢复假设提供受控的辅助证据，防止语义漂移和假设劫持。

Result: 在受控模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，使用稀疏和密集检索器时，QUARK在Recall、MRR和nDCG指标上均优于基础检索器。消融实验表明QUARK对恢复假设数量具有鲁棒性，且锚定聚合优于未锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK在不牺牲鲁棒性的情况下提高了召回和排序质量，即使某些假设是噪声或无信息的。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [7] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1扫描中填补缺失的DWI扫描，以提升阿尔茨海默病分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态成像（如T1和DWI）可以提高神经退行性疾病诊断性能，但完整的多模态数据集并不总是可用。需要解决缺失模态数据的问题。

Method: 使用条件去噪扩散概率模型（conditional denoising diffusion probabilistic model）从T1扫描中填补缺失的DWI扫描。评估填补后的数据对单模态和双模态深度学习模型在阿尔茨海默病三分类（认知正常、轻度认知障碍、阿尔茨海默病）中准确性的影响。

Result: 观察到多个指标有所改善，特别是对少数类别敏感的指标。多种填补配置都显示出性能提升。

Conclusion: 使用扩散模型进行模态填补可以改善阿尔茨海默病分类性能，特别是在处理不完整多模态数据集时。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [8] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim是一个用于大规模多智能体模拟的LLM服务系统，通过预测智能体调用顺序实现内存高效管理，相比SGLang实现1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体模拟在应用领域日益普及，但由于GPU内存压力难以扩展。每个智能体维护私有GPU驻留状态（模型、前缀缓存、适配器），随着智能体数量增长会快速耗尽设备内存

Method: 识别工作负载的两个关键特性：稀疏智能体激活和可估计的智能体调用顺序。引入"调用距离"统一抽象来估计智能体未来发出LLM请求的相对顺序。基于此构建ScaleSim系统，实现主动预取和基于优先级的驱逐，通过模块化接口支持多样化的智能体特定内存

Result: 在模拟基准测试中，ScaleSim相比SGLang实现了最高1.74倍的加速

Conclusion: ScaleSim通过调用距离抽象和相应的内存管理策略，有效解决了大规模多智能体模拟中的GPU内存扩展问题

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [9] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 该论文提出了基于悖论理论的责任AI治理框架(PRAIG)，旨在解决AI价值创造与风险缓解之间的张力，为组织提供平衡创新与风险管理的治理机制。


<details>
  <summary>Details</summary>
Motivation: 当前AI在组织中的快速普及带来了战略机遇，但也引入了显著的伦理和运营风险。现有责任AI文献呈现碎片化，要么过度乐观强调价值创造，要么过度谨慎关注潜在危害，缺乏整合视角。

Method: 基于战略信息系统视角，通过系统综合责任AI文献并扎根于悖论理论，开发了悖论基础的责任AI治理(PRAIG)框架，包含AI的战略效益、固有风险与意外后果、以及治理机制三个核心维度。

Result: 提出了正式命题证明权衡方法会加剧而非解决张力，开发了具有特定权变条件的悖论管理策略分类法，为实践者提供了既不抑制创新也不暴露组织于不可接受风险的治理结构指导。

Conclusion: 将责任AI治理概念化为价值创造与风险缓解之间悖论张力的动态管理，推进了理论理解，并为推进责任AI治理研究提供了研究议程。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [10] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 该论文提出两种多智能体演员-评论家方法（CoLLM-CC和CoLLM-DC）来优化去中心化LLM协作，解决了现有蒙特卡洛方法方差高、样本需求大的问题，并在不同任务设置下分析了它们的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有基于MARL的LLM协作优化方法大多依赖预定义执行协议，通常需要集中式执行，而去中心化LLM协作在实际应用中更具吸引力。同时，当前方法使用蒙特卡洛方法进行微调，存在方差高、需要大量样本的问题。

Method: 提出两种多智能体演员-评论家方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家），用于优化去中心化LLM协作，解决蒙特卡洛方法的高方差问题。

Result: 实验表明，在短视界和密集奖励任务中，蒙特卡洛方法和CoLLM-DC能达到与CoLLM-CC相当的性能。但在长视界或稀疏奖励任务中，蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛，而CoLLM-CC表现更优。

Conclusion: 集中式评论家的MAAC方法（CoLLM-CC）在复杂任务中表现更优，特别是在长视界和稀疏奖励场景下，而去中心化评论家方法（CoLLM-DC）和蒙特卡洛方法在这些场景下存在局限性。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [11] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 本文提出Planner-Auditor框架，通过分离LLM规划与确定性验证，结合缓存可选和自我改进机制，显著提升临床出院计划的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床出院规划中虽有潜力，但存在幻觉、遗漏和置信度校准不足等问题，需要更安全可靠的解决方案。

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划并估计置信度；Auditor是确定性模块，评估多任务覆盖率、跟踪校准指标、监控行动分布漂移。支持两层自我改进机制：1）情景内再生；2）跨情景差异缓冲与重放。

Result: 自我改进循环是性能提升的主要驱动力，任务覆盖率从32%提升至86%。校准显著改善，Brier/ECE分数降低，高置信度遗漏减少。差异缓冲进一步纠正了持续的高置信度遗漏。

Conclusion: Planner-Auditor框架为使用互操作FHIR数据访问和确定性审计的安全自动化出院规划提供了实用途径，支持可重复消融和可靠性导向的评估。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [12] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 该研究探索使用大语言模型为手语翻译生成多种书面语言变体作为替代参考，以解决手语翻译中单一参考文本的限制问题。


<details>
  <summary>Details</summary>
Motivation: 大多数手语翻译语料库为每个手语语句只提供一个书面语言参考，但由于手语和口语之间存在高度非同构关系，多个翻译可能同样有效。这种限制影响了模型训练和评估，特别是基于n-gram的指标如BLEU。

Method: 研究使用大语言模型自动生成书面语言翻译的改写变体作为手语翻译的合成替代参考。比较了多种改写策略和模型，使用改进的ParaScore指标进行评估。研究了改写对基于姿态的T5模型在YouTubeASL和How2Sign数据集上训练和评估的影响。

Result: 在训练中简单加入改写并不会提高翻译性能，甚至可能有害。但在评估中使用改写能获得更高的自动评分，并与人类判断更好对齐。为此引入了BLEUpara，这是BLEU的扩展版本，可针对多个改写参考评估翻译质量。人类评估证实BLEUpara与感知翻译质量相关性更强。

Conclusion: 研究提出了使用大语言模型生成改写变体来改善手语翻译评估的方法，并发布了所有生成的改写、生成和评估代码，以支持手语翻译系统更可靠和可复现的评估。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [13] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 第三方训练的情感识别模型在自我报告数据上表现不佳，但涉及个人重要内容时，效价预测准确度显著提升


<details>
  <summary>Details</summary>
Motivation: 自我报告标签反映内部体验，第三方标签反映外部感知，两者常存在差异。这种差异限制了第三方训练模型在自我报告场景中的应用，而心理健康领域需要准确的自我报告建模来指导干预

Method: 首次进行跨语料库评估，将第三方训练的情感识别模型应用于自我报告数据，分析激活度和效价的预测性能

Result: 激活度预测几乎不可行（CCC约0），效价预测中等可预测（CCC约0.3）。但当内容对说话者具有个人重要性时，效价预测性能显著提高（CCC约0.6-0.8）

Conclusion: 个人重要性是协调外部感知与内部体验的关键途径，同时凸显了自我报告激活度建模的挑战。这为改进心理健康应用中的情感识别模型提供了重要方向

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [14] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 本文针对大语言模型在金融量化推理中的"算术幻觉"和"认知崩溃"问题，提出了认知复杂度基准(CCB)和迭代双阶段金融PoT框架，通过架构解耦显著提升了金融推理的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语义任务上表现出色，但在金融量化推理中存在严重瓶颈，经常出现"算术幻觉"和系统性故障模式"认知崩溃"。需要严格量化这一现象并开发解决方案。

Method: 1) 引入认知复杂度基准(CCB)：基于95份真实中国A股年报构建数据集，采用三维分类法(数据源、映射难度、结果单位)对金融查询进行分层；2) 提出迭代双阶段金融PoT框架：神经符号架构，严格分离语义变量提取和逻辑制定，将计算卸载到迭代自校正的Python沙箱中确保确定性执行。

Result: 在CCB基准上的评估显示，标准思维链方法在复杂任务上表现不佳，而本文方法具有优越的鲁棒性：将Qwen3-235B模型的平均准确率从59.7%提升到67.3%，在高复杂度推理任务中实现了高达10倍的性能提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为需要语义理解和量化计算紧密对齐的精度关键领域提供了可迁移的架构洞见。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [15] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 本文提出了一种新的平面几何问题解决方法，通过训练MLLM解释器将视觉图表转换为文本描述（CDL），然后使用现成的LLM进行推理，避免了直接端到端微调MLLM可能损害基础LLM推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过端到端微调多模态大语言模型（MLLMs）来处理平面几何问题，但这种联合优化可能会损害基础LLM固有的推理能力。作者发现，如果能够将视觉信息恰当地转化为文本描述，LLM本身就是一个强大的平面几何问题求解器。

Method: 1. 训练MLLM解释器将几何图表转换为条件声明语言（CDL）文本描述；2. 使用现成的LLM基于CDL描述进行推理；3. 采用CoT增强的监督微调（SFT）和GRPO训练MLLM解释器；4. 设计CDL匹配奖励而非传统的基于解决方案的奖励，为CDL生成提供更直接和密集的指导；5. 构建新的数据集Formalgeo7k-Rec-CoT支持训练。

Result: 在Formalgeo7k-Rec-CoT、Unigeo和MathVista数据集上的大量实验表明，该方法（仅使用5.5k数据微调）在性能上优于领先的开源和闭源MLLMs。

Conclusion: 通过分离视觉理解和推理过程，将MLLM作为解释器生成几何描述，然后使用LLM进行推理，可以更有效地解决平面几何问题，同时保护基础LLM的推理能力。该方法在多个基准测试中表现出色，证明了其有效性。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [16] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个评估前沿语言模型专家级科学推理能力的基准，包含奥林匹克竞赛和科研两个互补赛道，涵盖物理、化学、生物学等多个学科领域。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准测试（主要依赖选择题或已发表信息）已被前沿模型接近饱和，需要更高级别的科学推理评估工具来测试模型的真正能力。

Method: 构建包含两个互补赛道的基准：1) 奥林匹克赛道 - 国际奥林匹克竞赛级别问题（IPhO、IChO、IBO）；2) 科研赛道 - 博士级别的开放式研究子任务问题。所有问题均由领域专家（奥林匹克奖牌得主、国家队教练、博士科学家）编写和验证。

Result: 创建了包含数百个问题的基准（其中160个为开源黄金集），涵盖从量子电动力学到合成有机化学等多个子领域。为科研赛道引入了基于细粒度评分标准的评估框架，评估模型在整个研究任务解决过程中的能力。

Conclusion: FrontierScience填补了现有科学基准测试的空白，提供了评估前沿语言模型专家级科学推理能力的标准化工具，特别强调原创性、难度和事实准确性。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [17] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出Modality-Adaptive Decoding (MAD)方法，通过自适应加权模态特定解码分支，减少多模态大语言模型中的跨模态幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，即一个模态不适当地影响另一个模态的生成，导致输出失真。这暴露了模态交互控制方面的根本缺陷

Method: 提出无需训练的MAD方法，通过查询模型自身对每个任务所需模态的评估能力，提取模态概率，然后自适应加权对比解码分支，使模型专注于相关信息并抑制跨模态干扰

Result: 在CMM和AVHBench数据集上的实验表明，MAD显著减少了跨模态幻觉，VideoLLaMA2-AV模型分别提升了7.8%和2.0%，Qwen2.5-Omni模型分别提升了8.7%和4.7%

Conclusion: 通过自我评估实现的显式模态意识对于鲁棒的多模态推理至关重要，MAD为现有对比解码方法提供了原则性扩展，证明了模态自适应解码的有效性

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [18] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 该研究引入"谄媚锚点"概念来定位和量化推理模型中的谄媚行为，通过分析超过10,000个反事实推演，发现可以在推理过程中可靠检测和量化这种倾向性承诺。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常同意错误的用户建议（谄媚行为），但尚不清楚这种同意在推理轨迹中起源于何处以及承诺强度如何。需要定位和量化这种行为。

Method: 引入"谄媚锚点"概念——那些因果性地将模型锁定在用户同意状态的句子。使用蒸馏推理模型分析超过10,000个反事实推演，通过线性探针和基于激活的回归器进行检测和量化。

Result: 线性探针能以84.6%的平衡准确率区分谄媚锚点；基于激活的回归器能预测承诺强度（R²=0.74）。发现谄媚锚点比正确推理锚点更易区分，且谄媚行为在推理过程中逐渐形成。

Conclusion: 谄媚锚点提供了在推理过程中定位模型错位的句子级机制，揭示了干预的潜在窗口，为理解模型谄媚行为提供了新的分析框架。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [19] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR调优的推理模型作为嵌入初始化时，相比基础模型在语义表示任务上没有性能优势，因为对比学习会重新对齐两者的流形结构。


<details>
  <summary>Details</summary>
Motivation: 研究经过RLVR调优的推理模型是否能为嵌入模型提供更好的语义表示初始化，因为理论上增强的推理能力可能带来更优的语义表示。

Method: 提出HRSA框架，从表示、几何和功能三个层次分解相似性分析，研究RLVR调优对模型潜在流形的影响机制。

Result: RLVR调优仅引起潜在流形的局部几何重组和可逆的坐标基漂移，但保持了全局流形几何和线性读出能力；后续对比学习会驱动基础模型和推理初始化模型之间的强对齐（流形重对齐）。

Conclusion: 与监督微调不同，RLVR是在现有语义景观内优化轨迹，而非从根本上重构景观本身，因此作为嵌入初始化时无法提供一致的性能优势。

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [20] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: ACQO是一个用于RAG系统的自适应复杂查询优化强化学习框架，通过自适应查询重构和排序-分数融合模块，结合课程强化学习策略，有效处理复杂查询的分解和结果聚合问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂用户查询通常需要并行和顺序搜索策略来处理歧义和分解问题。现有的RL方法主要关注单个查询的扩展和抽象，直接应用于复杂查询会面临搜索空间爆炸和奖励设计困难等挑战，导致训练不稳定。

Method: 提出了自适应复杂查询优化（ACQO）框架，包含两个核心组件：自适应查询重构（AQR）模块动态决定何时将查询分解为多个子查询；排序-分数融合（RSF）模块确保稳健的结果聚合并提供稳定的奖励信号。采用课程强化学习（CRL）方法，通过两阶段策略逐步引入更具挑战性的查询来稳定训练过程。

Result: ACQO在三个复杂查询基准测试中实现了最先进的性能，显著优于现有基线方法。该框架还展示了改进的计算效率，并与不同的检索架构具有广泛的兼容性。

Conclusion: ACQO是一个强大且可泛化的解决方案，适用于下一代RAG系统，能够有效处理复杂查询优化问题，解决了现有RL方法在复杂场景下面临的训练不稳定和搜索空间爆炸等挑战。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [21] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier是一个符号验证器，用于检查LLM生成的因果表达式是否可以从给定的因果图中通过do-calculus和概率论规则推导出来，从而更准确地评估LLM的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试通常依赖字符串匹配或表面指标，无法捕捉模型输出在因果推理语义下的形式有效性，需要更严格的评估方法。

Method: 提出DoVerifier，一个简单的符号验证器，使用do-calculus和概率论规则来验证LLM生成的因果表达式是否可以从给定因果图中推导出来。

Result: 在合成数据和因果问答基准测试上的评估表明，DoVerifier能更准确地捕捉因果推理轨迹的语义正确性，恢复因表面差异而被错误标记的正确答案。

Conclusion: DoVerifier为评估LLM在因果推理任务上提供了更严格和更有信息量的方法，超越了传统的表面指标评估。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [22] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出一种双编码因果发现方法，通过互补编码策略运行约束算法，并使用多数投票合并结果，解决分类变量因果发现中的数值不稳定问题


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理分类变量时面临条件独立性测试数值不稳定的挑战，需要更可靠的方法来解释机器学习模型决策

Method: 采用双编码因果发现方法，使用互补编码策略运行约束算法，通过多数投票合并结果

Result: 在泰坦尼克数据集上应用该方法，发现的因果结构与已建立的可解释方法一致

Conclusion: 双编码方法能够有效处理分类变量的因果发现问题，为解释机器学习模型决策提供可靠工具

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [23] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE框架通过解耦算法结构推理与参数优化，结合树编辑距离保持结构多样性，集成LLM逻辑生成与差分变异进行参数调优，显著提升了启发式算法设计性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法演化视为单一文本生成任务，忽视了离散算法结构与连续数值参数之间的耦合关系，导致丢弃有潜力的算法（由于未校准常数）和过早收敛（由于简单相似性度量）。

Method: 提出TIDE框架，采用嵌套架构：外层并行岛屿模型使用树编辑距离驱动结构多样性；内层集成LLM逻辑生成与差分变异算子进行参数调优；使用UCB调度器动态优先处理高收益提示策略。

Result: 在九个组合优化问题上的实验表明，TIDE发现的启发式算法在解质量上显著优于最先进的基线方法，同时实现了改进的搜索效率和降低的计算成本。

Conclusion: TIDE框架通过解耦结构推理与参数优化，有效解决了现有方法在算法演化中的局限性，为自动化启发式设计提供了更高效的解决方案。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [24] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 本文提出HYDRA框架，通过模块化专家库和不确定性感知融合，解决时间序列基础模型在安全关键系统中的适应性问题，避免灾难性遗忘并确保可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前通用时间序列基础模型在安全关键信息物理系统中面临挑战：微调会导致灾难性遗忘、存在残余频谱偏差无法处理高频故障信号、模型不透明难以满足安全标准验证要求。

Method: 提出模块化主权范式HYDRA：构建紧凑、冻结的特定状态专家库，通过不确定性感知的混合机制进行组合，实现状态条件有效性、严格区分偶然和认知不确定性、模块化可审计性。

Result: HYDRA框架为安全关键系统提供了可验证的路径，确保在整个信息物理系统生命周期中具有鲁棒的状态完整性，解决了塑性-稳定性悖论问题。

Conclusion: 全局参数更新无法完全解决塑性-稳定性悖论，模块化主权范式通过专家库和不确定性感知融合，为安全关键信息物理系统提供了可验证的鲁棒适应方案。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [25] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD框架通过知识蒸馏将自动驾驶分解为感知-推理-规划三要素，使用层特定注意力作为蒸馏信号，构建多教师蒸馏框架，显著提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶是安全关键任务，大模型需要大量GPU内存和高推理延迟，而传统监督微调难以弥补小模型能力差距，需要更有效的知识迁移方法

Method: 提出Drive-KD框架，将自动驾驶分解为感知-推理-规划三要素；使用层特定注意力作为蒸馏信号构建能力特定的单教师模型；统一为多教师蒸馏框架并引入非对称梯度投影缓解跨能力梯度冲突

Result: 蒸馏后的InternVL3-1B模型比同系列78B预训练模型减少约42倍GPU内存，吞吐量提升约11.4倍，在DriveBench上整体性能更好，规划维度超过GPT-5.1

Conclusion: Drive-KD框架通过知识蒸馏有效提升小模型在自动驾驶任务上的性能，为高效自动驾驶视觉语言模型提供了新思路

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [26] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: 提出CNRE模型，通过因果神经符号推理实现可解释的多行为推荐，解决现有方法性能与可解释性难以兼顾的问题


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法往往以牺牲可解释性为代价追求性能，而当前可解释方法因依赖外部信息导致泛化能力有限。神经符号集成提供了一条有前景的可解释性途径，同时作者认为用户行为链本身就蕴含着适合显式推理的内生逻辑

Method: 提出CNRE模型，模拟人类决策过程：1) 使用分层偏好传播捕获异构跨行为依赖；2) 基于偏好强度建模用户行为链中的内生逻辑规则；3) 自适应分发到相应的神经逻辑推理路径（如合取、析取），生成近似理想状态的可解释因果中介变量

Result: 在三个大规模数据集上的实验表明，CNRE在性能上显著优于最先进的基线方法，同时从模型设计、决策过程到推荐结果提供多层次的可解释性

Conclusion: CNRE通过因果神经符号推理框架，成功实现了性能与可解释性的平衡，为多行为推荐提供了从模型设计到结果输出的全面可解释性

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [27] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 研究评估了12个LLM在10个创意提示上的输出方差来源，发现对于输出质量（原创性），提示解释了36.43%的方差，与模型选择（40.94%）相当；但对于输出数量（流畅性），模型选择（51.25%）和LLM内部方差（33.70%）占主导，提示仅解释4.22%的方差。


<details>
  <summary>Details</summary>
Motivation: 理解LLM输出变异的来源：提示、模型选择和随机采样各自对输出方差的影响程度，以评估单样本评估的风险。

Method: 使用12个LLM在10个创意提示上进行评估，每个提示采样100次（总计N=12,000），分析输出方差来源，区分输出质量（原创性）和输出数量（流畅性）。

Result: 1. 输出质量方面：提示解释36.43%方差，模型选择解释40.94%方差，LLM内部方差10-34%；2. 输出数量方面：模型选择解释51.25%方差，LLM内部方差33.70%，提示仅解释4.22%方差。

Conclusion: 提示是控制输出质量的有力杠杆，但由于LLM内部方差显著（10-34%），单样本评估可能混淆采样噪声与真实的提示或模型效应，需要多样本评估以获得可靠结果。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [28] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时程电子健康记录设计的检索增强生成框架，通过事件时间感知检索、自适应迭代检索和双路径证据检索推理，在四个临床预测任务中平均提升Macro-F1 10.76%


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含丰富的纵向临床证据，对医疗决策至关重要。然而，长时程EHRs经常超过大语言模型的上下文限制，现有方法通常依赖截断或简单检索策略，这会丢弃临床相关事件和时间依赖性。

Method: 提出EHR-RAG框架，包含三个核心组件：1）事件和时间感知混合EHR检索，保留临床结构和时间动态；2）自适应迭代检索，逐步细化查询以扩大证据覆盖范围；3）双路径证据检索和推理，联合检索和推理事实和反事实证据。

Result: 在四个长时程EHR预测任务上的实验表明，EHR-RAG始终优于最强的大语言模型基线，平均Macro-F1提升10.76%。

Conclusion: 这项工作突显了检索增强的大语言模型在结构化EHR数据临床预测实践中的潜力，为解决长时程EHRs超出上下文限制的问题提供了有效框架。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [29] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 该研究针对食品服务和零售商店场景，开发了Ostrakon-VL多模态大语言模型，提出了ShopBench基准测试和QUAD数据清理流程，显著提升了模型在嘈杂现实数据下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在食品服务和零售商店场景部署面临两大障碍：1) 现实数据来自异构设备，噪声大且缺乏可审计的闭环数据管理，难以构建高质量训练语料；2) 现有评估协议缺乏统一、细粒度、标准化的基准测试，难以客观衡量模型鲁棒性。

Method: 1) 基于Qwen3-VL-8B开发面向FSRS的Ostrakon-VL模型；2) 提出首个公开的FSRS基准测试ShopBench；3) 设计QUAD多阶段多模态指令数据清理流程；4) 采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上平均得分60.1，在可比参数规模和多样架构的开源MLLM中达到新SOTA。显著超越Qwen3-VL-235B-A22B（59.4）+0.7分，超越同规模Qwen3-VL-8B（55.3）+4.8分，展示了显著改进的参数效率。

Conclusion: Ostrakon-VL提供了更鲁棒可靠的FSRS中心感知和决策能力。研究将公开发布Ostrakon-VL模型和ShopBench基准测试，以促进可重复研究。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [30] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT框架将潜在推理重新定义为规划过程，通过解耦推理与语言化，在连续潜在空间中执行推理，实现动态终止和更好的推理多样性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法受限于计算成本和离散标记空间中的推理路径崩溃问题，而潜在推理方法虽然提高了效率，但通常作为从显式推理步骤到潜在状态的不透明端到端映射，且推理时需要预定义潜在步骤数量。

Method: 提出PLaT框架，将潜在推理重新定义为规划过程，从根本上解耦推理与语言化。将推理建模为潜在规划状态的确定性轨迹，而单独的Decoder在必要时将这些思想转化为文本。这种解耦允许模型动态确定何时终止推理，而不是依赖固定超参数。

Result: 在数学基准测试中，PLaT显示出明显的权衡：虽然贪婪准确率低于基线，但在推理多样性方面表现出更好的可扩展性。这表明PLaT学习到了更稳健、更广泛的解空间。

Conclusion: PLaT为推理时搜索提供了透明且可扩展的基础，通过解耦推理与语言化，实现了动态终止和更好的推理多样性，尽管在贪婪准确率上有所牺牲。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [31] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI框架通过动态参数插值在直觉型System 1和深思型System 2模型间切换，实现能力控制而非输出控制，在数学推理任务上达到更高准确率同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注输出控制（限制模型输出长度），但这只是认知配置的症状而非根本原因。需要转向能力控制，调节模型如何思考而非产生什么，以更好地结合System 1的效率和System 2的推理深度。

Method: 1) 利用现有Instruct和Thinking检查点进行动态参数插值，无需额外训练；2) 提出DAMI框架，估计查询特定的推理强度λ(q)来配置认知深度；3) 开发基于偏好学习的方法编码准确性和效率标准；4) 引入基于置信度的零样本部署方法，利用模型间认知差异。

Result: 在五个数学推理基准测试中，DAMI比Thinking模型获得更高准确率，同时保持效率，有效结合了System 1的效率和System 2的推理深度。

Conclusion: 能力控制比输出控制更有效，通过动态模型插值可以在直觉和深思认知模式间实现平衡，为统一语言模型训练提供了新方向。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [32] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在处理否定指令时存在严重缺陷：开源模型在简单否定指令下77%会错误执行被禁止的行为，在复合否定下错误率高达100%，商业模型也有19-128%的波动。模型无法可靠区分"做X"和"不做X"。


<details>
  <summary>Details</summary>
Motivation: 当用户告诉AI系统某人"不应该"采取某个行动时，系统应将其视为禁令。但许多大型语言模型却相反地将否定指令解释为肯定指令，这在伦理敏感场景中可能带来严重后果。

Method: 审计了16个模型在14个伦理场景中的表现，测试简单否定和复合否定指令下的响应。使用确定性解码排除采样噪声影响，并提出了否定敏感度指数(NSI)作为治理指标。

Result: 开源模型在简单否定下77%会认可被禁止的行为，复合否定下错误率100%（比肯定框架增加317%）。商业模型表现稍好但仍有19-128%的波动。模型间一致性从肯定提示的74%降至否定提示的62%，金融场景的脆弱性是医疗场景的两倍。

Conclusion: 当前对齐技术与安全部署要求存在差距：无法可靠区分"做X"和"不做X"的模型不应在关键决策场景中自主决策。提出了基于领域特定阈值的分级认证框架和NSI指标来改善治理。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [33] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在规则约束的决策中表现出惊人的"鲁棒性悖论"：尽管已知对提示微小变化敏感，但在情感框架效应下却表现出近乎完全的行为不变性，比人类抗叙事操纵能力强110-300倍。


<details>
  <summary>Details</summary>
Motivation: 虽然已知大型语言模型对提示微小扰动敏感且容易迎合用户偏见，但它们在重要规则约束决策中的鲁棒性尚未得到充分探索。本研究旨在探究LLMs在情感框架效应下的行为稳定性。

Method: 使用新颖的受控扰动框架，在三个高风险领域（医疗、法律、金融）进行实验，量化LLMs对叙事操纵的抵抗能力。通过162个场景的基准测试，比较LLMs与人类受试者在情感框架效应下的决策差异。

Result: 发现LLMs表现出"鲁棒性悖论"：对叙事操纵的抵抗能力比人类强110-300倍。模型的效果大小接近零（Cohen's h = 0.003），而人类则表现出显著的偏见（Cohen's h在[0.3, 0.8]之间）。这种不变性在不同训练范式的模型中持续存在。

Conclusion: 指令调优的LLMs能够将逻辑规则遵循与说服性叙事解耦，提供了一种决策稳定性来源，可以补充甚至可能去偏人类在制度环境中的判断。虽然LLMs可能对查询格式"脆弱"，但对决策为何应该被偏见却表现出"稳定"。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [34] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax是一种新的主题建模方法，通过密度估计峰值确定聚类数量，结合词汇和语义指标提升主题关键词质量，相比Top2Vec和BERTopic在聚类恢复和描述方面表现更好，对样本大小和超参数变化更稳定。


<details>
  <summary>Details</summary>
Motivation: 尽管Top2Vec和BERTopic在主题建模中很流行，但它们存在几个未解决的问题：1）对样本大小和超参数极度敏感，难以发现自然聚类；2）在估计术语重要性时，BERTopic忽略关键词与主题向量的语义距离，Top2Vec忽略语料库中的词频，导致主题连贯性差、缺乏多样性。

Method: 提出Topeax方法：1）通过密度估计峰值自动发现聚类数量；2）结合词汇指标（词频）和语义指标（与主题向量的距离）来评估术语重要性，生成高质量的主题关键词。

Result: Topeax在聚类恢复和聚类描述方面均优于Top2Vec和BERTopic，同时对样本大小和超参数变化表现出更稳定的行为，减少了异常波动。

Conclusion: Topeax解决了现有主题建模方法的关键缺陷，通过更稳健的聚类发现机制和综合的术语重要性评估，提供了更可靠、更高质量的主题建模解决方案。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [35] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: 本文通过人类评估验证了激活引导技术在控制LLM情感输出方面的有效性，发现中等强度引导能可靠增强目标情感并保持文本质量，且人类与模型评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 控制大型语言模型在推理时的行为对于使其输出符合人类期望和安全要求至关重要。激活引导提供了一种轻量级的替代方法，通过直接修改内部激活来引导生成，避免复杂的提示工程和微调。

Method: 1) 首次对LLM输出的情感语调进行激活引导的人类评估，通过Prolific平台收集了190名参与者的7000多个众包评分；2) 评估感知情感强度和整体文本质量；3) 比较人类评分与模型评分的一致性；4) 测试不同引导强度对特定情感的影响；5) 从Alpaca升级到LlaMA-3模型验证改进效果。

Result: 1) 人类与模型质量评分高度一致（平均r=0.776）；2) 中等引导强度（λ≈0.15）能可靠增强目标情感同时保持可理解性；3) 对厌恶（η_p²=0.616）和恐惧（η_p²=0.540）效果最强，对惊讶（η_p²=0.042）效果最弱；4) LlaMA-3相比Alpaca产生更一致的引导效果（所有p<0.001）；5) 评分者间信度高（ICC=0.71-0.87）。

Conclusion: 激活引导是一种可扩展的方法，能够有效控制LLM在情感维度上的行为。人类评估证实了该技术的实用性和可靠性，为LLM行为控制提供了实证支持。

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [36] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM讨论组织成显式论证图，通过因果模型识别关键推理链，并引入校正机制对齐内部推理与外部判断


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合收集不同观点，但掩盖了哪些论证驱动最终决策，需要更透明的论证追踪和因果分析

Method: 将多专家讨论组织成显式论证图（支持/攻击关系），将其建模为因果模型，通过系统性移除单个论证并重新计算结果来识别必要推理链，引入校正机制对齐内部推理与外部判断

Result: 在多样化基准测试和开放式用例中，ARGORA达到竞争性准确率，展示校正行为：当专家初始意见不一致时，框架更倾向于将争议解决为正确答案而非引入新错误，同时提供关键论证的因果诊断

Conclusion: ARGORA通过显式论证图和因果分析提高了多专家LLM系统的透明度和可解释性，能够识别关键推理链并在专家分歧时进行有效校正

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [37] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo是一个用于智能体LLM系统的预算分层内存服务，通过分片证据存储和范围优先路由机制，在固定预算下显著提升性能并降低检索开销。


<details>
  <summary>Details</summary>
Motivation: 现有智能体LLM系统依赖外部内存进行长期状态管理和多智能体并发执行，但随着内存容量和平行访问增长，集中式索引和启发式分区成为瓶颈。

Method: 提出三层内存架构：A层为每个智能体的工作状态，B层为分片证据存储（带分片本地ANN索引），C层为版本化技能库。采用"范围优先路由"机制，在路由前屏蔽不符合条件的分片，将分片探测建模为掩码混合专家路由。

Result: 在LoCoMo上比最强基线(GAM)提升5.11-6.82 F1；固定预算路由设置下比余弦原型分片路由提升6.87 F1，同时减少20.5%检索工作和降低p95延迟；在长上下文HotpotQA上达到57.95-63.41 F1；在ToolBench上C层达到0.97 Precision@3和1.94 StepRed。

Conclusion: ShardMemo通过分层内存架构和智能路由机制，有效解决了大规模智能体系统中的内存瓶颈问题，在多种任务上实现了显著的性能提升和效率改进。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [38] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文首次从理论上分析了学习内化中间推理步骤的难度，证明了高阶逻辑依赖的学习信号会指数衰减，并提出ALiCoT框架通过对齐潜在token分布与中间推理状态来克服信号衰减，实现54.4倍加速的同时保持与显式CoT相当的性能。


<details>
  <summary>Details</summary>
Motivation: 显式思维链（CoT）虽然解锁了大语言模型的高级推理能力，但生成额外token带来了过高的计算成本。最近研究表明将推理步骤压缩到潜在状态（隐式CoT压缩）提供了token高效的替代方案，但其机制尚不清楚。

Method: 1. 引入Order-r Interaction理论框架，证明高阶逻辑依赖的学习信号会指数衰减；2. 创建NatBool-DAG基准测试，强制不可约逻辑推理并消除语义捷径；3. 提出ALiCoT框架，通过对齐潜在token分布与中间推理状态来克服信号衰减。

Result: ALiCoT成功实现了高效推理：在保持与显式CoT相当性能的同时，实现了54.4倍的加速。实验验证了理论分析的正确性，即跳过中间步骤会导致高阶交互障碍。

Conclusion: 本文首次从理论上解释了CoT压缩的机制，揭示了学习内化中间推理步骤的困难源于高阶逻辑依赖的信号衰减。提出的ALiCoT框架通过状态对齐有效克服了这一障碍，为高效推理提供了理论指导和实用解决方案。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [39] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: Dreamer框架通过深度循环注意力混合机制，结合序列注意力、深度注意力和稀疏专家注意力，解决了深度循环模型中隐藏层大小瓶颈问题，在语言推理任务上显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度循环方法存在三个主要问题：1）缺乏FLOP、参数和内存匹配的基准对比；2）由于部分固定层堆栈导致深度循环利用不足；3）恒定隐藏层大小限制多步潜在推理。需要解决这些瓶颈来提升深度循环模型的效率和效果。

Method: 提出Dreamer框架，结合三种注意力机制：序列注意力（处理序列维度）、深度注意力（处理深度维度）、稀疏专家注意力。通过深度方向的注意力缓解隐藏层大小瓶颈，解耦缩放维度，使深度循环模型能够高效扩展。

Result: 在语言推理基准测试中，相比FLOP、参数和内存匹配的SOTA模型，Dreamer模型达到相同准确率所需的训练token数量减少2-8倍；使用相同训练token时，性能优于约2倍大小的SOTA模型。深度知识使用分析显示专家选择多样性比SOTA MoE模型高2-11倍。

Conclusion: Dreamer框架通过创新的注意力混合机制有效解决了深度循环模型的瓶颈问题，显著提升了训练效率和模型性能，为多步潜在推理提供了更有效的架构方案。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [40] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent提出了一种主动潜在规划方法，通过条件变分自编码器建模潜在token监督过程，并使用基于一致性的强化学习奖励来优化潜在推理策略，在LLaMA-1B上相比先进基线实现了+4.1%准确率和-3.3% token消耗的提升。


<details>
  <summary>Details</summary>
Motivation: 当前潜在推理方法通过模仿语言标签来监督潜在token，但由于一个问题可能存在多个等价但不同的CoT标签，被动模仿任意一个可能导致次优的潜在token表示和推理策略，限制了潜在规划能力并造成训练与测试之间的差距。

Method: 提出ATP-Latent方法：1）使用条件变分自编码器（VAE）建模潜在token的监督过程以获得更平滑的潜在空间；2）采用强化学习（RL）配合辅助一致性奖励，该奖励基于VAE解码的潜在token内容之间的一致性计算，实现有指导的RL过程。

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线实现了+4.1%的准确率提升和-3.3%的token消耗减少，代码已在GitHub开源。

Conclusion: ATP-Latent通过主动规划潜在token表示空间，结合VAE平滑潜在空间和基于一致性的RL奖励，有效提升了潜在推理的性能和效率，实现了更优的潜在推理策略。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [41] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 该研究将企业级智能文档处理系统的验证问题形式化为基于搜索的软件测试问题，旨在在有限预算内最大化发现不同故障类型，并通过多种搜索策略的对比分析展示了策略间的互补性。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域应用广泛，早期系统验证需要在有限预算下发现多样化的故障机制，而不仅仅是识别单一最坏情况文档。

Method: 将问题形式化为基于搜索的软件测试问题，在文档配置的组合空间中操作，生成结构性风险特征实例以诱导现实故障条件。在相同预算约束下，对进化算法、群体智能、质量多样性、学习型和量子计算等多种搜索策略进行基准测试。

Result: 不同求解器在可比预算下持续发现其他方法未发现的故障模式。跨时间分析显示所有评估预算中都存在持久的求解器特定发现，没有单一策略表现出绝对优势。虽然所有求解器的联合最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性地延迟重要风险的发现。

Conclusion: 研究结果展示了求解器固有的互补性，并支持采用基于组合策略的SBST方法来进行稳健的工业IDP验证，而不是依赖单一搜索策略。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [42] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出WhatCounts测试框架，发现LLMs在简单计数任务中的表现会因计数对象语义不同而产生超过40%的准确率差异，表明LLMs并非实现算法而是近似算法，且这种近似依赖于输入参数的意义。


<details>
  <summary>Details</summary>
Motivation: 研究动机是测试算法行为是否应该独立于其参数的语义内容。现有研究将语义敏感性与推理复杂性或提示变化混为一谈，需要一种原子化的测试方法来隔离语义敏感性。

Method: 引入WhatCounts测试框架，专注于简单的计数任务：在无歧义、分隔的列表中计数项目，没有重复项、干扰项或不同语义类型所需的推理步骤。通过控制消融实验排除混淆因素。

Result: 前沿LLMs仅因计数对象不同（如城市vs化学品、名称vs符号）就显示出超过40%的准确率差异。这种差异是语义性的，且少量无关的微调会不可预测地改变这种差距。

Conclusion: LLMs并非实现算法而是近似算法，且这种近似依赖于参数的意义。这对LLM函数有广泛影响：任何LLM函数都可能携带对其输入意义的隐藏依赖。

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [43] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym是一个用于评估深度研究工作流的仿真环境，通过静态语料库和确定性检索解决API依赖带来的不可复现性问题，支持对查询规划、工具调用和相关性评估的细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具增强的大语言模型的研究工作流评估面临根本挑战：依赖实时API会引入非确定性，因为工具调用可能因时间漂移、速率限制和后台状态变化而产生不同结果，这种差异破坏了可复现性并使跨系统比较失效。

Method: 构建ScholarGym仿真环境，将工作流组件解耦为查询规划、工具调用和相关性评估，基于包含57万篇论文的静态语料库提供确定性检索，包含2,536个带有专家标注真实标签的查询。

Result: 通过在不同骨干模型上进行实验，揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用，实现了对深度研究工作流的可复现评估。

Conclusion: ScholarGym为评估学术文献深度研究工作流提供了一个可复现的仿真环境，解决了实时API依赖带来的评估挑战，支持对工作流各阶段的细粒度分析。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [44] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: SONIC-O1是一个全面的人工验证基准测试，用于评估多模态大语言模型在时序音频-视频数据上的表现，涵盖13个现实对话领域，包含4,958个标注和人口统计元数据。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型研究主要集中在静态图像理解，而对处理时序音频-视频数据的能力探索不足，需要高质量基准测试来系统评估模型在真实场景中的表现。

Method: 构建了SONIC-O1基准测试，涵盖13个现实对话领域，包含4,958个人工验证的标注和人口统计元数据，评估任务包括开放式摘要、多项选择题回答、带推理支持的时序定位。

Result: 实验显示闭源和开源模型在多项选择题准确率上差距较小，但在时序定位任务上，最佳闭源模型比开源模型性能高出22.6%。模型在不同人口统计群体上性能进一步下降，表明存在持续的行为差异。

Conclusion: SONIC-O1为时序基础和社交鲁棒的多模态理解提供了开放评估套件，揭示了当前模型在处理时序音频-视频数据时的局限性，特别是在时序定位和跨人口统计群体公平性方面存在显著差距。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [45] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 该论文提出了一种名为TCAP的无监督防御框架，用于检测MLLMs中的后门攻击，通过分析三个功能组件（系统指令、视觉输入、用户文本查询）的注意力分配差异来识别中毒样本。


<details>
  <summary>Details</summary>
Motivation: FTaaS在促进MLLMs定制化的同时，通过中毒数据引入了严重的后门风险。现有防御方法要么依赖监督信号，要么无法适应不同触发器和模态的多样性。研究发现了一个通用的后门指纹——注意力分配差异，即中毒样本会破坏三个功能组件之间的平衡注意力分布。

Method: 提出Tri-Component Attention Profiling (TCAP)无监督防御框架：1) 将跨模态注意力图分解为三个功能组件；2) 使用高斯混合模型(GMM)统计分析方法识别对触发器敏感的注意力头；3) 通过基于EM的投票聚合机制隔离中毒样本。

Result: 在多种MLLM架构和攻击方法上的广泛实验表明，TCAP能够实现一致且强大的性能，证明了其作为MLLMs中稳健实用的后门防御方法的有效性。

Conclusion: TCAP通过分析注意力分配差异这一通用后门指纹，提供了一种无监督的防御框架，能够有效检测和过滤MLLMs中的后门攻击，解决了现有防御方法的局限性。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [46] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: FBS Transformer通过引入可训练的因果循环，结合前瞻注意力、分块处理和跳过机制，在保持参数不变的情况下提升LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理主要依赖严格的逐token自回归，现有加速方法大多只是修补现有流程，缺乏人类阅读的核心要素：内容自适应前瞻、分块结构感知的计算分配以及训练-测试一致性

Method: 提出Fovea-Block-Skip Transformer，通过三个核心模块注入可训练的因果循环：Parafovea-Attention Window（前瞻注意力窗口）、Chunk-Head（分块头）和Skip-Gate（跳过门）

Result: 在多样化基准测试中，FBS在保持参数不变的情况下改善了质量-效率权衡，消融实验显示三个模块具有互补性

Conclusion: FBS Transformer通过模拟人类阅读机制，为LLM推理提供了更高效的架构设计，三个模块的协同作用显著提升了推理效率

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [47] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: E-mem框架通过从内存预处理转向情景上下文重建，解决了LLM智能体在长序列推理中上下文完整性破坏的问题，采用异构分层架构实现高效记忆管理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体向System 2推理演进时，需要保持长时间范围内的逻辑完整性。但主流的内存预处理方法存在破坏性去上下文化问题，将复杂序列依赖压缩到预定义结构（如嵌入或图）中，切断了深度推理所需的情境完整性。

Method: 提出E-mem框架，从内存预处理转向情景上下文重建。受生物印迹启发，采用异构分层架构：多个助手智能体维护未压缩的记忆上下文，中央主智能体协调全局规划。不同于被动检索，该机制使助手能在激活的片段内进行本地推理，提取情境感知证据后再聚合。

Result: 在LoCoMo基准测试中，E-mem实现了超过54%的F1分数，比最先进的GAM方法提高了7.75%，同时减少了超过70%的token成本。

Conclusion: E-mem通过情景上下文重建方法有效解决了LLM智能体长序列推理中的上下文完整性问题，在保持推理质量的同时显著降低了计算成本，为System 2推理提供了有效的记忆管理框架。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [48] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS是一种模型无关的插件，通过样本自适应dropout机制，利用频谱稀疏性量化实例级噪声，动态校准模型学习能力，提升深度时间序列模型在噪声数据下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中时间序列数据普遍存在噪声，现有鲁棒性策略要么剪枝数据，要么依赖昂贵的先验量化，无法在有效性和效率之间取得平衡。

Method: 提出DropoutTS插件，采用样本自适应dropout机制：利用频谱稀疏性通过重构残差高效量化实例级噪声，将噪声映射到自适应dropout率，动态校准模型学习能力，选择性抑制虚假波动同时保持细粒度保真度。

Result: 在多种噪声机制和开放基准测试上的广泛实验表明，DropoutTS能持续提升优秀骨干模型的性能，以可忽略的参数开销和无架构修改实现先进的鲁棒性。

Conclusion: DropoutTS通过从"学什么"到"学多少"的范式转变，提供了一种高效、模型无关的时间序列噪声鲁棒性解决方案，在保持效率的同时显著提升模型性能。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [49] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: 论文提出Epistemic Context Learning (ECL)框架，通过历史交互构建同伴可信度档案，使多智能体系统中的个体能够识别可靠同伴并学习，显著提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的个体智能体缺乏鲁棒性，容易盲目跟随误导性同伴。这种弱点源于两方面：谄媚倾向（sycophancy）以及评估同伴可信度的能力不足。需要解决智能体在不确定时如何准确评估同伴可靠性并学习可信同伴的问题。

Method: 1. 形式化历史感知参考学习问题，将同伴历史交互作为额外输入；2. 开发Epistemic Context Learning (ECL)推理框架，基于历史构建明确的同伴档案；3. 使用辅助奖励通过强化学习优化ECL。

Result: 1. ECL使小模型（Qwen 3-4B）性能超越历史不可知基线模型（Qwen 3-30B）8倍；2. 前沿模型性能提升至接近完美（100%）；3. ECL能良好泛化到各种多智能体配置；4. 发现信任建模准确性与最终答案质量高度相关。

Conclusion: ECL框架通过历史交互构建同伴可信度档案，有效解决了多智能体系统中的谄媚问题和可信度评估不足问题，显著提升了智能体性能，且具有良好的泛化能力。大语言模型在信任建模方面表现出色。

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [50] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出ZSSD零样本统计降尺度框架，无需配对训练数据，通过物理一致气候先验和统一坐标引导解决现有方法物理不一致性和梯度消失问题，在GCMs上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据和与再分析数据间的域差异，难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题。

Method: 提出零样本统计降尺度框架，利用从再分析数据学习的物理一致气候先验，结合地球物理边界和时间信息确保物理有效性；引入统一坐标引导策略解决DPS中的梯度消失问题，确保与大尺度场的一致性。

Result: ZSSD在99百分位误差上显著优于现有零样本基线方法，成功重建复杂天气事件（如热带气旋），在异质GCMs上表现良好。

Conclusion: ZSSD为零样本气候降尺度提供了有效解决方案，通过物理约束和梯度优化策略，在无需配对数据的情况下实现了对全球气候模型的准确降尺度预测。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [51] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 该研究提出一个概念空间框架来建模随时间展开的抽象概念，以国际象棋为概念验证，将策略概念表示为可解释质量维度上的几何区域，通过轨迹分析识别意图策略。


<details>
  <summary>Details</summary>
Motivation: 将概念空间理论扩展到时间实现、目标导向的概念，为涉及顺序决策的广泛应用建立基础，并支持与知识演化机制集成以学习和完善抽象概念。

Method: 采用概念空间框架，将策略概念（如攻击或牺牲）建模为可解释质量维度上的几何区域，将国际象棋对局实例化为轨迹，通过轨迹在概念空间中的方向性运动来识别意图策略，支持双视角建模以捕捉玩家对相同情况的不同解释。

Result: 实现证明了基于轨迹的概念识别的可行性，运动模式与专家评论一致，展示了如何通过几何区域和轨迹分析来识别和理解抽象策略概念。

Conclusion: 该工作成功将概念空间理论扩展到时间实现的目标导向概念，为顺序决策应用建立了基础框架，并支持与知识演化机制集成以随时间学习和完善抽象概念。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [52] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 该研究提出了一种基于大语言模型（LLM）的统一框架，用于气管内吸痰（ES）培训的视频活动识别和反馈生成，在准确率和F1分数上比传统方法提升15-20%。


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰是一项高风险临床操作，在家庭护理和教育环境中缺乏有效监督和自动化反馈系统。现有技术在该领域的应用探索不足，需要开发智能化的培训支持系统。

Method: 提出以LLM为核心的统一框架，将LLM作为中央推理模块，从视频数据中执行时空活动识别和可解释决策分析。同时构建基于异常检测和可解释AI（XAI）原理的学生支持模块，生成自然语言反馈。

Result: LLM方法在准确率和F1分数上比传统机器学习和深度学习方法提升约15-20%。框架能够生成可解释的自然语言反馈，突出正确操作并提供针对性改进建议。

Conclusion: 该研究建立了一个可扩展、可解释、数据驱动的基础框架，可推动护理教育进步，提高培训效率，最终改善患者安全。LLM在临床技能培训中展现出强大潜力。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [53] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 该研究提出了一个全面的心电专家基础模型基准测试框架，结合性能评估和表示层分析，用于评估模型在医疗场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型在AI辅助心电解释领域开始应用，但现有基准测试主要关注下游性能评估，缺乏对模型嵌入表示泛化能力的深入分析，这在医疗等错误敏感领域尤为重要。

Method: 提出一个基准测试方法学，结合基于性能的评估和表示层分析，利用SHAP和UMAP技术；在不同跨大陆数据集和数据可用性设置下（包括数据稀缺场景）评估多个心电专家基础模型。

Result: 实验结果表明，该基准测试协议能够深入洞察心电专家基础模型的嵌入模式，使研究人员能够更深入地理解其表示结构和泛化能力。

Conclusion: 该研究填补了心电专家基础模型基准测试的空白，提供了一个全面的评估框架，有助于在医疗领域负责任地应用基础模型，特别是在数据稀缺的现实医疗场景中。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [54] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 论文提出一个决策中心的仿真框架，用于评估备件库存预测模型对运营绩效的影响，而非仅关注统计准确性。


<details>
  <summary>Details</summary>
Motivation: 汽车售后市场备件需求高度间歇且不确定，传统预测模型评估仅关注统计准确性（如MAE、RMSE），但这些指标与运营绩效指标（如总成本、服务水平）的关系不明确，需要建立连接预测与库存管理的评估框架。

Method: 提出决策中心仿真软件框架，包含三个组件：(1) 针对备件需求特征设计的合成需求生成器；(2) 可容纳任意预测模型的灵活预测模块；(3) 消耗预测并计算运营KPI的库存控制模拟器，形成闭环评估系统。

Result: 通过多种仿真场景表明，传统准确性指标的改进不一定带来更好的运营绩效，统计误差相似的模型可能产生显著不同的成本-服务权衡，分析了预测性能特定方面如何影响库存结果，并提供了模型选择指导。

Conclusion: 该框架将需求预测与库存管理操作化连接，将评估重点从纯预测准确性转向运营相关性，适用于汽车售后市场及相关领域。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [55] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias提出了一种轻量级框架，通过增强而非抑制编码偏见知识的神经元来减轻大语言模型中的社会偏见，仅需少量是/否问题且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型去偏见方法大多采用抑制范式（修改参数、提示或与偏见行为相关的神经元），但这些方法通常脆弱、泛化能力弱、数据效率低，且容易降低模型的通用能力。

Method: KnowBias通过基于归因的分析，使用少量偏见知识问题识别编码偏见知识的神经元，并在推理时选择性地增强这些神经元，从而减轻偏见。

Result: 在多个基准测试和大语言模型上的实验表明，KnowBias实现了最先进的去偏见性能，同时保持了最小的能力退化，且能跨偏见类型和人口统计特征泛化。

Conclusion: KnowBias提供了一种轻量级、数据高效且概念上不同的去偏见框架，通过增强而非抑制偏见知识神经元，在保持模型通用能力的同时有效减轻社会偏见。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [56] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: CoMT框架通过模仿人类认知的两阶段过程改进LLM后训练：第一阶段专注抽象推理模式学习，第二阶段通过置信度校准强化学习优化任务适应，显著提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM后训练方法将完整推理轨迹作为基本单元进行优化，这与人类实际解决问题的认知过程存在根本差异。人类认知自然地将问题解决分解为两个阶段：首先获取跨问题通用的抽象策略（元知识），然后将其适应到具体实例。当前方法的问题中心化导致抽象策略与问题特定执行纠缠在一起，限制了模型的泛化能力。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过中间步骤的置信度感知奖励优化任务适应，防止过度自信的错误级联，提高执行可靠性。

Result: 在4个模型和8个基准测试中，相比标准方法，在分布内提升2.19%，分布外提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅能产生更优越的泛化能力，还能显著提高训练效率，证明了认知启发的AI训练方法的有效性。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [57] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG是一个过程监督强化学习框架，通过整合细粒度步骤级监督来优化检索增强生成系统，解决传统结果导向RL在复杂推理任务中的奖励稀疏性和信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法在复杂推理任务中存在奖励稀疏和信用分配效率低的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"——模型通过有缺陷的逻辑或冗余检索步骤得到正确答案。现有的过程感知方法缺乏在线探索能力来解耦步骤级信用与全局结果。

Method: ProRAG框架包含四个阶段：1) 监督策略预热，用结构化推理格式初始化模型；2) 构建基于MCTS的过程奖励模型(PRM)量化中间推理质量；3) PRM引导的推理精炼，使策略与细粒度过程偏好对齐；4) 过程监督强化学习，采用双粒度优势机制，聚合步骤级过程奖励与全局结果信号。

Result: 在五个多跳推理基准测试上的广泛实验表明，ProRAG相比基于结果的RL和过程感知RL基线方法取得了更优的整体性能，特别是在复杂长视野任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过整合学习到的步骤级监督到在线优化循环中，为检索增强生成的复杂推理任务提供了精确的步骤级反馈，有效解决了传统强化学习方法中的奖励稀疏和信用分配问题。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [58] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 本文提出SCMA框架，通过多智能体强化学习选择性惩罚冗余推理块，在压缩响应长度的同时提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型中的冗余推理增加了推理开销，影响交互体验并阻碍部署。现有基于强化学习的解决方案通过长度惩罚与结果奖励的简单加权难以平衡简洁性与准确性。

Method: 提出SCMA多智能体强化学习框架，包含三个智能体：分割智能体将推理过程分解为逻辑块；评分智能体量化每个块的重要性；推理智能体在训练中接受重要性加权的长度惩罚，部署时不增加推理开销。

Result: 在不同模型规模上的实验表明，SCMA将响应长度减少11.1%至39.0%，同时将准确性提升4.33%至10.02%。消融研究和定性分析验证了MARL框架中的协同优化产生了涌现行为。

Conclusion: SCMA框架通过多智能体强化学习选择性惩罚冗余推理块，有效解决了推理模型中的冗余问题，在压缩响应长度的同时提升了准确性，相比传统强化学习范式产生了更强大的推理模型。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [59] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个基于法庭辩论结构的多智能体框架，通过角色分工和结构化辩论为高风险表格决策任务提供透明可控的推理过程。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒方法在高风险决策任务中缺乏透明度、可解释性和可控性的问题，特别是在刑事司法等伦理复杂领域需要人类监督和可审计的决策过程。

Method: 提出法庭风格的多智能体辩论框架，定义检察官、辩护律师和法官三种角色，采用7轮结构化辩论协议，每个智能体使用私有推理策略，创建完全可审计的决策流程。

Result: 在NLSY97数据集上的年轻成人再犯预测任务中，相比传统链式思维提示，结构化多智能体辩论展现出更稳定和泛化性更强的性能，准确率和F1分数相关性更高。

Conclusion: AgenticSimLaw不仅提升性能，还提供细粒度推理控制、完整交互记录以增强可解释性，并能系统分析智能体行为。该框架适用于任何需要透明度和人类监督的高风险决策任务。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [60] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver提出了一种新的生成式工具学习框架，通过将工具编码为分层序列来解决现有检索式和生成式方法的语义瓶颈问题，显著提升了工具学习的可扩展性、泛化性和语义感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用框架面临双重语义挑战：检索式方法中的编码器难以捕捉复杂语义，而LLM本身缺乏工具知识；生成式方法虽然统一了选择和执行，但将每个工具映射到唯一新标记的做法导致可扩展性危机和语义瓶颈，阻碍了协作工具关系的学习。

Method: ToolWeaver采用分层序列编码工具，使词汇扩展与工具数量呈对数关系。通过新颖的标记化过程，将工具的内在语义与外部使用模式编织在一起，然后通过生成对齐阶段将这些结构化代码集成到LLM中，使模型能够生成分层代码序列。

Result: 在近47,000个工具的评估中，ToolWeaver显著优于最先进的方法，为高级工具增强智能体建立了更可扩展、可泛化和语义感知的基础。

Conclusion: ToolWeaver通过分层序列编码解决了现有工具学习方法的语义瓶颈问题，提供了一种更有效的方法来学习工具协作关系，为大规模工具使用系统奠定了更好的基础。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [61] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: VAF是一个评估网页视觉属性如何影响Web智能体决策的量化框架，通过生成语义相同但视觉属性不同的变体，测量智能体在点击和推理上的差异，发现背景对比度、项目大小、位置和卡片清晰度对智能体行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注Web智能体在对抗攻击下的鲁棒性，而较少关注在良性场景中视觉属性如何影响智能体决策。虽然早期研究考察了文本属性对智能体行为的影响，但对视觉属性如何塑造智能体决策的系统性理解仍然有限。

Method: VAF评估管道包含三个阶段：1) 变体生成 - 创建语义相同但视觉属性不同的网页变体；2) 浏览交互 - 智能体通过滚动和点击进行导航，模拟人类浏览行为；3) 验证 - 通过点击动作和推理，使用目标点击率和目标提及率联合评估视觉属性的影响。

Result: 在8个变体家族（共48个变体）、5个真实网站（购物、旅游、新闻浏览）和4个代表性Web智能体上的实验表明：背景颜色对比度、项目大小、位置和卡片清晰度对智能体行为有强烈影响，而字体样式、文本颜色和项目图像清晰度的影响较小。

Conclusion: 视觉属性显著影响Web智能体的决策过程，某些视觉特征（如背景对比度、项目大小）比文本相关属性对智能体行为的影响更大。这为理解智能体决策偏差和设计更鲁棒的Web智能体提供了重要见解。

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [62] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 该研究探讨了领域模型特征如何影响经典规划器的能耗，挑战了传统AI研究中以算法性能为优先的范式，提出了绿色AI的新视角。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究主要关注算法性能（如机器学习准确率或自动规划运行时间），而新兴的绿色AI范式将能耗视为关键性能维度。尽管自动规划计算需求高，但其能效问题很少受到关注，特别是在模块化规划结构中，领域模型与算法独立设计，这为通过领域模型设计系统分析能耗提供了机会。

Method: 研究引入了一个领域模型配置框架，能够控制特征变化（如元素排序、动作元数、死端状态等）。使用五个基准领域和五个最先进的规划器，分析了每个基准32个领域变体的能耗和运行时间影响。

Result: 结果表明，领域级修改在不同规划器之间产生了可测量的能耗差异，且能耗并不总是与运行时间相关。这证明了通过领域模型设计可以影响规划过程的能耗特性。

Conclusion: 该研究强调了在自动规划中考虑能耗的重要性，展示了领域模型特征对能耗的显著影响，为绿色AI在规划领域的应用提供了实证基础，并指出能耗优化需要独立于运行时间考虑。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [63] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 研究发现语言模型的陈述偏好与显示偏好之间存在差距，这种相关性高度依赖评估协议设计，需要能处理不确定偏好的方法


<details>
  <summary>Details</summary>
Motivation: 现有研究识别出语言模型中陈述偏好与显示偏好的不匹配，但现有评估方法主要依赖二元强制选择提示，将真实偏好与评估协议的人工制品混为一谈

Method: 系统研究24个语言模型中评估协议对陈述-显示偏好的影响，允许中立和弃权选项，比较不同协议下的相关性，并测试系统提示引导的效果

Result: 在陈述偏好评估中允许中立和弃权能排除弱信号，显著提高相关性；但在显示偏好中允许弃权会导致相关性接近零或负值；系统提示引导在AIRiskDilemmas上不能可靠改善相关性

Conclusion: 陈述-显示偏好的相关性高度依赖评估协议设计，偏好评估需要能够处理不确定偏好的方法，不能简单依赖二元强制选择

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [64] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个用于验证足球事件数据完整性的系统框架，通过状态转移模型检测和纠正事件流中的逻辑不一致问题，显著提升数据质量和下游分析任务的可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析等领域至关重要，但现有数据存在逻辑不一致问题（如事件顺序错误或缺失事件），这限制了分析模型的可靠性。特别是在足球领域，数据质量问题会影响球员贡献评估和战术模式识别等精细分析。

Method: 提出VERSA（Verified Event Data Format for Reliable Soccer Analytics）框架，基于状态转移模型定义有效事件序列，能够自动检测和纠正事件流数据中的异常模式。该框架系统性地验证足球事件数据的完整性。

Result: 对K League 1（2024赛季）Bepro提供的事件数据检查发现，18.81%的记录事件存在逻辑不一致。VERSA显著提高了跨数据提供商的一致性，确保异构数据源的稳定统一表示。经VERSA精炼的数据显著提升了VAEP（评估球员贡献的下游任务）的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性。VERSA框架为足球事件数据提供了系统性的完整性保障，解决了现有数据质量问题，为精细分析提供了更可靠的基础。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [65] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 提出Liquid Interfaces（液态接口）作为协调范式，将接口从静态技术制品转变为运行时通过意图表达和语义协商产生的短暂关系事件，以支持自适应、概率性和上下文相关的智能体系统。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持自适应、概率性和上下文相关的自主智能体，而系统集成仍被静态接口和确定性契约主导。需要一种新的协调范式来解决这一矛盾。

Method: 提出Liquid Interfaces协调范式，将接口定义为短暂的关系事件而非持久技术制品；形式化该模型并设计Liquid Interface Protocol（LIP），管理意图驱动交互、协商执行和在语义不确定性下的短暂性执行。

Result: 开发了Liquid Interface Protocol（LIP），建立了参考架构以展示实际可行性，为基于智能体的系统提供了自适应协调的原则性基础。

Conclusion: Liquid Interfaces为基于智能体的系统中的自适应协调提供了原则性基础，通过将接口重新概念化为短暂的关系事件，解决了静态接口与自适应智能体之间的不匹配问题。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [66] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench是一个针对车载语音助手的LLM智能体评估基准，专注于一致性、不确定性处理和能力认知，包含幻觉任务和消歧任务，揭示现有LLM智能体在现实应用中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体基准主要关注理想环境下的任务完成，忽视了现实用户应用中面对不完整或模糊请求时的可靠性问题。车载语音助手等场景中，用户常提出不完整或模糊请求，智能体需要通过对话、工具使用和策略遵循来管理这种内在不确定性。

Method: 引入CAR-bench基准，包含LLM模拟用户、领域策略和58个互连工具（导航、生产力、充电、车辆控制）。除了标准任务完成，还设计了幻觉任务（测试智能体在工具或信息缺失时的极限认知）和消歧任务（要求通过澄清或内部信息收集解决不确定性）。

Result: 基线结果显示，所有任务类型上都存在偶尔成功与持续成功之间的巨大差距。即使是前沿推理LLM在消歧任务上的持续通过率也低于50%（由于过早行动），在幻觉任务中经常违反策略或编造信息以满足用户请求，突显了现实场景中需要更可靠、更具自我认知的LLM智能体。

Conclusion: CAR-bench揭示了现有LLM智能体在现实世界用户应用中的可靠性缺陷，特别是在处理不确定性和认知自身能力极限方面。基准强调了开发更可靠、更具自我认知的LLM智能体对于实际部署的重要性。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [67] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: AWO框架通过分析工作流轨迹，将重复的工具调用序列转化为元工具，减少LLM调用次数并提高任务成功率


<details>
  <summary>Details</summary>
Motivation: 智能体AI虽然能通过推理和工具交互解决复杂任务，但需要大量迭代步骤和工具调用，导致运营成本高、延迟长且容易因幻觉而失败

Method: AWO框架分析现有工作流轨迹，发现重复的工具调用序列，将其转化为确定性的元工具，将多个智能体动作捆绑为单次调用

Result: 在两个智能体AI基准测试中，AWO将LLM调用次数减少高达11.9%，同时将任务成功率提高高达4.2个百分点

Conclusion: AWO通过优化冗余工具执行模式，显著提高了智能体工作流的效率和鲁棒性，减少了运营成本和失败率

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [68] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 该论文提出了一种基于多维度核表示的后验方法，从已有数据中定义操作设计域(ODD)，用于安全关键AI系统的认证。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的复杂系统中，定义AI系统运行的环境条件（ODD）非常困难，但这是AI系统认证的必要条件。传统方法依赖专家知识在开发早期定义ODD，但这种方法可能不完整。

Method: 提出了一种Safety-by-Design方法，使用多维度核表示从先前收集的数据中后验定义ODD。该方法通过蒙特卡洛方法和真实航空用例（未来安全关键防撞系统）进行验证。

Result: 研究表明，通过定义两个ODD相等的条件，数据驱动的ODD可以等于数据中隐藏的原始ODD。该方法能够实现数据驱动的安全关键AI系统的未来认证。

Conclusion: 基于核的Safe-by-Design ODD方法为数据驱动的安全关键AI系统认证提供了有效途径，解决了传统ODD定义方法的不完整性问题。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [69] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出Agent-RRM（智能体推理奖励模型），通过提供结构化反馈来改进智能体强化学习，包括推理轨迹、针对性批判和总体评分，并开发了三种集成策略，在多个基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前智能体强化学习方法主要依赖稀疏的结果奖励进行训练，这种反馈无法区分中间推理质量，导致训练结果不理想。需要更细粒度的反馈机制来指导智能体学习过程。

Method: 提出Agent-RRM多维度奖励模型，提供三种结构化反馈：1）显式推理轨迹，2）针对性批判（突出推理缺陷并提供改进指导），3）过程性能总体评分。基于这些信号开发了三种集成策略：Reagent-C（文本增强精炼）、Reagent-R（奖励增强指导）和Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试上的广泛评估表明，Reagent-U实现了显著的性能飞跃：在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化反馈显著改进了智能体强化学习的训练效果，提出的集成策略特别是Reagent-U在多个任务上表现出色，为未来研究提供了有价值的工具和数据集。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [70] [What Hard Tokens Reveal: Exploiting Low-confidence Tokens for Membership Inference Attacks against Large Language Models](https://arxiv.org/abs/2601.20885)
*Md Tasnim Jawad,Mingyan Xiao,Yanzhao Wu*

Main category: cs.CR

TL;DR: HT-MIA：一种针对大语言模型的成员推理攻击方法，通过分析低置信度（困难）标记的token级概率来提升攻击效果，相比现有方法显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用和隐私法规日益严格，保护LLM中的数据隐私变得至关重要。现有的成员推理攻击方法主要依赖序列级聚合预测统计，无法区分泛化改进和记忆改进，导致攻击效果不佳。

Method: 提出HT-MIA方法，通过捕获低置信度（困难）标记的token级概率，比较微调目标模型和预训练参考模型在困难标记上的概率改进，从而分离出强健的成员信号。

Result: 在领域特定的医疗数据集和通用基准测试上的广泛实验表明，HT-MIA在七个最先进的MIA基线方法中表现最优。同时研究了差分隐私训练作为有效的防御机制。

Conclusion: HT-MIA框架确立了基于困难标记的分析作为推进LLM成员推理攻击和防御的最先进基础，为隐私保护提供了重要工具。

Abstract: With the widespread adoption of Large Language Models (LLMs) and increasingly stringent privacy regulations, protecting data privacy in LLMs has become essential, especially for privacy-sensitive applications. Membership Inference Attacks (MIAs) attempt to determine whether a specific data sample was included in the model training/fine-tuning dataset, posing serious privacy risks. However, most existing MIA techniques against LLMs rely on sequence-level aggregated prediction statistics, which fail to distinguish prediction improvements caused by generalization from those caused by memorization, leading to low attack effectiveness. To address this limitation, we propose a novel membership inference approach that captures the token-level probabilities for low-confidence (hard) tokens, where membership signals are more pronounced. By comparing token-level probability improvements at hard tokens between a fine-tuned target model and a pre-trained reference model, HT-MIA isolates strong and robust membership signals that are obscured by prior MIA approaches. Extensive experiments on both domain-specific medical datasets and general-purpose benchmarks demonstrate that HT-MIA consistently outperforms seven state-of-the-art MIA baselines. We further investigate differentially private training as an effective defense mechanism against MIAs in LLMs. Overall, our HT-MIA framework establishes hard-token based analysis as a state-of-the-art foundation for advancing membership inference attacks and defenses for LLMs.

</details>


### [71] [Towards Sensitivity-Aware Language Models](https://arxiv.org/abs/2601.20901)
*Dren Fazlija,Iyiola E. Olatunji,Daniel Kudenko,Sandipan Sikdar*

Main category: cs.CR

TL;DR: 该研究将企业数据管理中的敏感度感知概念形式化，建立了其与差分隐私的理论联系，并提出了一种监督微调方法，使4位量化LLM在保持其他任务性能的同时，显著提升敏感度感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在企业数据管理中的部署增加，需要确保模型不泄露敏感信息。虽然已引入敏感度感知概念使LLM能遵守预设访问权限规则，但其与差分隐私等隐私概念的关系尚不明确，阻碍了实际应用部署。

Method: 1. 形式化敏感度感知概念；2. 理论建立敏感度感知与差分隐私的联系；3. 开发监督微调方法，使现有的4位量化LLM更具敏感度感知能力。

Result: 微调后的LLM在敏感度感知方面性能提升高达21.7%，不仅显著超越基线模型，还在相似规模的开源和商业模型中表现最优。同时，模型在通用指令遵循、数学推理和常识推理等其他任务上的性能也得到很好保留。

Conclusion: 该研究成功建立了敏感度感知与差分隐私的理论联系，并开发了有效的微调方法，使量化LLM在保持其他能力的同时显著提升敏感度感知性能，为企业数据管理中的LLM安全部署提供了实用解决方案。

Abstract: With LLMs increasingly deployed in corporate data management, it is crucial to ensure that these models do not leak sensitive information. In the context of corporate data management, the concept of sensitivity awareness has been introduced, enabling LLMs to adhere to predefined access rights rules. However, it remains unclear how sensitivity awareness relates to established notions of privacy, such as differential privacy (DP), thereby making it difficult to deploy meaningfully in real-world applications. In this work, we formalize the notion of sensitivity awareness and theoretically establish its connection to DP. Additionally, we develop a supervised fine-tuning recipe to make existing, four-bit quantized LLMs more sensitivity-aware. With a performance boost of up to 21.7%, the finetuned LLMs not only substantially improve over their baseline but also outperform other full-precision open-source and commercial models of similar size in achieving sensitivity awareness, demonstrating the effectiveness of our proposed approach. At the same time, our method also largely preserves the models' performance on other tasks, such as general instruction-following, mathematical, and common-sense reasoning.

</details>


### [72] [ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack](https://arxiv.org/abs/2601.20903)
*Xingwei Lin,Wenhao Lin,Sicong Cao,Jiahao Yu,Renke Huang,Lei Xue,Chunming Wu*

Main category: cs.CR

TL;DR: ICON框架通过意图-上下文耦合现象，利用权威式上下文模式和分层优化策略，实现高效的多轮越狱攻击，在8个SOTA LLMs上达到97.1%的平均攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱攻击方法存在效率低下和容易陷入次优区域的问题，需要更高效的攻击框架来揭示LLM安全机制的脆弱性。

Method: 提出ICON框架：1) 基于意图-上下文耦合现象，将恶意意图路由到语义一致的上下文模式（如科学研究）；2) 通过先验引导的语义路由构建权威式上下文；3) 采用分层优化策略，结合局部提示优化和全局上下文切换。

Result: 在8个最先进的LLMs上进行实验，ICON实现了97.1%的平均攻击成功率，达到最先进的性能水平。

Conclusion: ICON框架通过利用意图-上下文耦合现象和分层优化策略，显著提高了多轮越狱攻击的效率和成功率，揭示了LLM安全机制的重要漏洞。

Abstract: Multi-turn jailbreak attacks have emerged as a critical threat to Large Language Models (LLMs), bypassing safety mechanisms by progressively constructing adversarial contexts from scratch and incrementally refining prompts. However, existing methods suffer from the inefficiency of incremental context construction that requires step-by-step LLM interaction, and often stagnate in suboptimal regions due to surface-level optimization. In this paper, we characterize the Intent-Context Coupling phenomenon, revealing that LLM safety constraints are significantly relaxed when a malicious intent is coupled with a semantically congruent context pattern. Driven by this insight, we propose ICON, an automated multi-turn jailbreak framework that efficiently constructs an authoritative-style context via prior-guided semantic routing. Specifically, ICON first routes the malicious intent to a congruent context pattern (e.g., Scientific Research) and instantiates it into an attack prompt sequence. This sequence progressively builds the authoritative-style context and ultimately elicits prohibited content. In addition, ICON incorporates a Hierarchical Optimization Strategy that combines local prompt refinement with global context switching, preventing the attack from stagnating in ineffective contexts. Experimental results across eight SOTA LLMs demonstrate the effectiveness of ICON, achieving a state-of-the-art average Attack Success Rate (ASR) of 97.1\%. Code is available at https://github.com/xwlin-roy/ICON.

</details>


### [73] [Robust Federated Learning for Malicious Clients using Loss Trend Deviation Detection](https://arxiv.org/abs/2601.20915)
*Deepthy K Bhaskar,Minimol B,Binu V P*

Main category: cs.CR

TL;DR: 提出FL-LTD框架，通过监测损失动态而非梯度来检测恶意客户端，在非IID数据下有效防御损失操纵攻击，显著提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统面临恶意客户端的风险，现有防御机制依赖梯度检查、复杂相似性计算或密码学操作，在非IID数据下可能不稳定且引入额外开销

Method: 提出基于损失趋势检测的联邦学习防御框架(FL-LTD)，通过监测时间损失动态检测异常客户端，识别损失停滞或突变，引入短期记忆机制应对自适应攻击

Result: 在非IID联邦MNIST设置下，FL-LTD在损失操纵攻击下达到0.84的最终测试精度，显著优于标准FedAvg的0.41，计算和通信开销可忽略

Conclusion: FL-LTD是一种轻量级、保护隐私的防御框架，通过基于损失的监控有效增强联邦学习安全性，避免客户端排除或敏感数据访问

Abstract: Federated Learning (FL) facilitates collaborative model training among distributed clients while ensuring that raw data remains on local devices.Despite this advantage, FL systems are still exposed to risks from malicious or unreliable participants. Such clients can interfere with the training process by sending misleading updates, which can negatively affect the performance and reliability of the global model. Many existing defense mechanisms rely on gradient inspection, complex similarity computations, or cryptographic operations, which introduce additional overhead and may become unstable under non-IID data distributions. In this paper, we propose the Federated Learning with Loss Trend Detection (FL-LTD), a lightweight and privacy-preserving defense framework that detects and mitigates malicious behavior by monitoring temporal loss dynamics rather than model gradients. The proposed approach identifies anomalous clients by detecting abnormal loss stagnation or abrupt loss fluctuations across communication rounds. To counter adaptive attackers, a short-term memory mechanism is incorporated to sustain mitigation for clients previously flagged as anomalous, while enabling trust recovery for stable participants. We evaluate FL-LTD on a non-IID federated MNIST setup under loss manipulation attacks. Experimental results demonstrate that the proposed method significantly enhances robustness, achieving a final test accuracy of 0.84, compared to 0.41 for standard FedAvg under attack. FL-LTD incurs negligible computational and communication overhead, maintains stable convergence, and avoids client exclusion or access to sensitive data, highlighting the effectiveness of loss-based monitoring for secure federated learning.

</details>


### [74] [FIPS 204-Compatible Threshold ML-DSA via Masked Lagrange Reconstruction](https://arxiv.org/abs/2601.20917)
*Leo Kao*

Main category: cs.CR

TL;DR: 提出了一种名为"掩码拉格朗日重构"的技术，实现了任意阈值T的ML-DSA阈值签名，生成标准3.3KB签名，可由未修改的FIPS 204实现验证。


<details>
  <summary>Details</summary>
Motivation: 现有并发方法存在局限性：Bienstock等人的方法可实现任意T但需要诚实多数和37-136轮通信；Celi等人的方法支持不诚实多数但仅限于T≤6。需要解决拉格朗日系数随q增长导致个体贡献过大而无法通过ML-DSA拒绝采样的问题。

Method: 提出了掩码拉格朗日重构技术，解决了ML-DSA阈值签名的三个关键挑战：1) 掩码后仍需通过拒绝采样；2) r0检查会暴露c s2导致密钥恢复风险；3) 必须保持Irwin-Hall非均匀分布的EUF-CMA安全性。基于该技术实现了三种部署配置。

Result: 实现了三种部署配置：P1（TEE辅助）在可信协调器下实现3轮签名；P2（完全分布式）通过MPC在8轮内实现UC安全性；P3（2PC辅助）使用轻量级2PC在3-5轮内实现最佳性能（249ms）。方案需要|S|≥T+1个签名者，成功率23-32%，与单签名者ML-DSA匹配。

Conclusion: 掩码拉格朗日重构技术成功解决了ML-DSA阈值签名的关键挑战，实现了任意阈值T的标准兼容签名，提供了多种部署选项，在安全性和性能之间取得了良好平衡。

Abstract: We present masked Lagrange reconstruction, a technique that enables threshold ML-DSA (FIPS 204) with arbitrary thresholds $T$ while producing standard 3.3 KB signatures verifiable by unmodified FIPS 204 implementations. Concurrent approaches have limitations: Bienstock et al. (ePrint 2025/1163) achieve arbitrary $T$ but require honest-majority and 37--136 rounds; Celi et al. (ePrint 2026/013) achieve dishonest-majority but are limited to $T \leq 6$. Our technique addresses the barrier that Lagrange coefficients grow as $Θ(q)$ for moderate $T$, making individual contributions too large for ML-DSA's rejection sampling.
  Unlike ECDSA threshold schemes where pairwise masks suffice for correctness, ML-DSA requires solving three additional challenges absent in prior work: (1) rejection sampling on $\|z\|_\infty$ must still pass after masking, (2) the $r_0$-check exposes $c s_2$ enabling key recovery if unprotected, and (3) the resulting Irwin-Hall nonce distribution must preserve EUF-CMA security. We solve all three.
  We instantiate this technique in three deployment profiles with full security proofs. Profile P1 (TEE-assisted) achieves 3-round signing with a trusted coordinator, with EUF-CMA security under Module-SIS. Profile P2 (fully distributed) eliminates hardware trust via MPC in 8 rounds, achieving UC security against malicious adversaries corrupting up to $n-1$ parties. Profile P3 (2PC-assisted) uses lightweight 2PC for the $r_0$-check in 3--5 rounds, achieving UC security under a 1-of-2 CP honest assumption with the best empirical performance (249ms).
  Our scheme requires $|S| \geq T+1$ signers and achieves success rates of 23--32\%, matching single-signer ML-DSA.

</details>


### [75] [SPOILER-GUARD: Gating Latency Effects of Memory Accesses through Randomized Dependency Prediction](https://arxiv.org/abs/2601.21211)
*Gayathri Subramanian,Girinath P,Nitya Ranganathan,Kamakoti Veezhinathan,Gopalakrishnan Srinivasan*

Main category: cs.CR

TL;DR: SPOILER-GUARD是一种硬件防御机制，通过动态随机化物理地址位和标记存储条目来防止SPOILER攻击，减少误预测至0.0004%，性能提升2-3%，硬件开销极小。


<details>
  <summary>Details</summary>
Motivation: 现代微处理器依赖推测执行，存在瞬态执行攻击漏洞。现有防御措施针对推测数据泄漏，但忽略了部分地址别名导致的虚假依赖关系，这种依赖关系通过重复的squash和reissue事件增加加载-存储延迟，被SPOILER攻击利用。

Method: SPOILER-GUARD通过动态随机化用于加载-存储比较的物理地址位，并对存储条目进行标记，来混淆推测依赖关系解析，防止延迟放大的误预测。

Result: 在gem5中实现并使用SPEC 2017评估，SPOILER-GUARD将误预测减少到0.0004%，整数和浮点性能分别提高2.12%和2.87%。使用Synopsys Design Compiler在14nm节点进行HDL综合显示极小开销：关键路径延迟69ps，面积0.064平方毫米，功耗5.863mW。

Conclusion: SPOILER-GUARD有效防御了SPOILER攻击，显著减少了误预测并提升了性能，同时硬件开销极小，是一种实用的硬件安全解决方案。

Abstract: Modern microprocessors depend on speculative execution, creating vulnerabilities that enable transient execution attacks. Prior defenses target speculative data leakage but overlook false dependencies from partial address aliasing, where repeated squash and reissue events increase the load-store latency, which is exploited by the SPOILER attack. We present SPOILER-GUARD, a hardware defense that obfuscates speculative dependency resolution by dynamically randomizing the physical address bits used for load-store comparisons and tagging store entries to prevent latency-amplifying misspeculations. Implemented in gem5 and evaluated with SPEC 2017, SPOILER-GUARD reduces misspeculation to 0.0004 percent and improves integer and floating-point performance by 2.12 and 2.87 percent. HDL synthesis with Synopsys Design Compiler at 14 nm node demonstrates minimal overheads - 69 ps latency in critical path, 0.064 square millimeter in area, and 5.863 mW in power.

</details>


### [76] [Lossless Copyright Protection via Intrinsic Model Fingerprinting](https://arxiv.org/abs/2601.21252)
*Lingxiao Chen,Liqin Wang,Wei Lu,Xiangyang Luo*

Main category: cs.CR

TL;DR: TrajPrint是一种无损、免训练的扩散模型版权保护框架，通过提取确定性生成过程中形成的独特流形指纹来验证模型版权，兼容黑盒API且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的优异性能使其成为高价值知识产权，但面临未经授权复制的风险。现有保护方法要么通过修改模型嵌入水印影响性能，要么通过操纵去噪过程提取模型指纹，与黑盒API不兼容。

Method: 1. 使用带水印图像作为锚点，精确回溯到其轨迹原点，锁定该路径映射的模型指纹；2. 采用双端锚定的联合优化策略合成特定指纹噪声，严格遵循目标流形以实现鲁棒水印恢复；3. 通过原子推理和统计假设检验进行验证。

Result: TrajPrint在黑盒API场景中实现无损验证，对模型修改具有优越的鲁棒性。受保护的目标模型能够恢复水印图像，而非目标模型则无法恢复。

Conclusion: TrajPrint提供了一种完全无损、免训练的扩散模型版权保护框架，通过提取独特的流形指纹实现黑盒API兼容的版权验证，解决了现有方法的性能损失和兼容性问题。

Abstract: The exceptional performance of diffusion models establishes them as high-value intellectual property but exposes them to unauthorized replication. Existing protection methods either modify the model to embed watermarks, which impairs performance, or extract model fingerprints by manipulating the denoising process, rendering them incompatible with black-box APIs. In this paper, we propose TrajPrint, a completely lossless and training-free framework that verifies model copyright by extracting unique manifold fingerprints formed during deterministic generation. Specifically, we first utilize a watermarked image as an anchor and exactly trace the path back to its trajectory origin, effectively locking the model fingerprint mapped by this path. Subsequently, we implement a joint optimization strategy that employs dual-end anchoring to synthesize a specific fingerprint noise, which strictly adheres to the target manifold for robust watermark recovery. As input, it enables the protected target model to recover the watermarked image, while failing on non-target models. Finally, we achieved verification via atomic inference and statistical hypothesis testing. Extensive experiments demonstrate that TrajPrint achieves lossless verification in black-box API scenarios with superior robustness against model modifications.

</details>


### [77] [SecIC3: Customizing IC3 for Hardware Security Verification](https://arxiv.org/abs/2601.21353)
*Qinhan Tan,Akash Gaonkar,Yu-Wei Fan,Aarti Gupta,Sharad Malik*

Main category: cs.CR

TL;DR: SecIC3是一种针对硬件安全验证的自组合结构优化的IC3模型检查算法，通过对称状态探索和等价谓词技术，显著提升了非干扰性检查的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的硬件安全验证方法虽然通过自组合技术将非干扰超属性转换为安全属性，但缺乏针对这种特殊结构的优化模型检查器，导致验证效率不高。

Method: 提出SecIC3算法，基于IC3框架并针对自组合结构进行定制优化，采用两种互补技术：对称状态探索和添加等价谓词，以利用设计的对称性。

Result: 在包含10个设计的非干扰检查基准测试中，SecIC3显著减少了安全证明的时间，相比基线实现最高可达49.3倍的证明加速。

Conclusion: SecIC3通过专门针对硬件安全验证的自组合结构进行优化，有效提升了非干扰性检查的效率，为硬件安全验证提供了更高效的模型检查方法。

Abstract: Recent years have seen significant advances in using formal verification to check hardware security properties. Of particular practical interest are checking confidentiality and integrity of secrets, by checking that there is no information flow between the secrets and observable outputs. A standard method for checking information flow is to translate the corresponding non-interference hyperproperty into a safety property on a self-composition of the design, which has two copies of the design composed together. Although prior efforts have aimed to reduce the size of the self-composed design, there are no state-of-the-art model checkers that exploit their special structure for hardware security verification. In this paper, we propose SecIC3, a hardware model checking algorithm based on IC3 that is customized to exploit this self-composition structure. SecIC3 utilizes this structure in two complementary techniques: symmetric state exploration and adding equivalence predicates. We implement SecIC3 on top of two open-source IC3 implementations and evaluate it on a non-interference checking benchmark consisting of 10 designs. The experiment results show that SecIC3 significantly reduces the time for finding security proofs, with up to 49.3x proof speedup compared to baseline implementations.

</details>


### [78] [RerouteGuard: Understanding and Mitigating Adversarial Risks for LLM Routing](https://arxiv.org/abs/2601.21380)
*Wenhui Zhang,Huiyu Xu,Zhibo Wang,Zhichao Li,Zeqing He,Xuelin Wei,Kui Ren*

Main category: cs.CR

TL;DR: 本文系统研究了LLM路由系统中的对抗性重路由攻击，提出了威胁分类体系，开发了RerouteGuard防御框架，在保持合法查询性能的同时实现了超过99%的攻击检测准确率。


<details>
  <summary>Details</summary>
Motivation: 多模型AI系统中的LLM路由器虽然能降低计算成本，但作为分类器容易受到对抗性重路由攻击。攻击者通过在查询前添加特殊触发器来操纵路由决策，可能导致计算成本增加、响应质量下降甚至绕过安全护栏，但这些安全影响尚未得到充分研究。

Method: 1. 基于攻击者目标（成本升级、质量劫持、安全绕过）和知识水平系统化LLM重路由威胁分类；2. 对现有LLM路由系统进行测量研究；3. 使用可解释性技术分析现有攻击机制；4. 提出RerouteGuard防御框架，通过动态嵌入检测和自适应阈值过滤对抗性重路由提示。

Result: 1. 现有路由系统对重路由攻击存在漏洞，特别是在成本升级场景中；2. 现有攻击通过前置混淆器小工具利用路由器决策边界强制错误路由；3. RerouteGuard在三种攻击设置和四个基准测试中实现了超过99%的检测准确率，同时对合法查询影响可忽略。

Conclusion: LLM重路由攻击对多模型AI系统构成严重安全威胁，RerouteGuard提供了一个原则性且实用的解决方案，能够有效保护系统免受对抗性重路由攻击，同时保持系统性能。

Abstract: Recent advancements in multi-model AI systems have leveraged LLM routers to reduce computational cost while maintaining response quality by assigning queries to the most appropriate model. However, as classifiers, LLM routers are vulnerable to novel adversarial attacks in the form of LLM rerouting, where adversaries prepend specially crafted triggers to user queries to manipulate routing decisions. Such attacks can lead to increased computational cost, degraded response quality, and even bypass safety guardrails, yet their security implications remain largely underexplored. In this work, we bridge this gap by systematizing LLM rerouting threats based on the adversary's objectives (i.e., cost escalation, quality hijacking, and safety bypass) and knowledge. Based on the threat taxonomy, we conduct a measurement study of real-world LLM routing systems against existing LLM rerouting attacks. The results reveal that existing routing systems are vulnerable to rerouting attacks, especially in the cost escalation scenario. We then characterize existing rerouting attacks using interpretability techniques, revealing that they exploit router decision boundaries through confounder gadgets that prepend queries to force misrouting. To mitigate these risks, we introduce RerouteGuard, a flexible and scalable guardrail framework for LLM rerouting. RerouteGuard filters adversarial rerouting prompts via dynamic embedding-based detection and adaptive thresholding. Extensive evaluations in three attack settings and four benchmarks demonstrate that RerouteGuard achieves over 99% detection accuracy against state-of-the-art rerouting attacks, while maintaining negligible impact on legitimate queries. The experimental results indicate that RerouteGuard offers a principled and practical solution for safeguarding multi-model AI systems against adversarial rerouting.

</details>


### [79] [On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression](https://arxiv.org/abs/2601.21531)
*Xinwei Zhang,Hangcheng Liu,Li Bai,Hao Wang,Qingqing Ye,Tianwei Zhang,Haibo Hu*

Main category: cs.CR

TL;DR: 该论文提出CAGE攻击方法，针对视觉语言模型中的视觉令牌压缩机制，解决了现有攻击方法因优化-推理不匹配而高估模型鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器的攻击方法会显著高估压缩后大型视觉语言模型的鲁棒性，因为存在优化-推理不匹配：扰动在完整令牌表示上进行优化，而推理却通过令牌压缩瓶颈进行。这种差距导致对压缩LVLMs的鲁棒性评估过于乐观。

Method: 提出CAGE攻击方法，包含两个核心组件：(1) 预期特征破坏：将失真集中在可能在不同预算下存活的令牌上；(2) 排名失真对齐：主动将令牌失真与排名分数对齐，以促进高度失真证据的保留。该方法不需要访问部署的压缩机制或其令牌预算。

Result: 在多种代表性的即插即用压缩机制和数据集上，CAGE攻击始终比基线方法获得更低的鲁棒准确率，表明忽略压缩的鲁棒性评估可能过于乐观。

Conclusion: 该研究强调忽略压缩的鲁棒性评估可能过于乐观，呼吁对高效LVLMs进行压缩感知的安全评估和防御措施。CAGE攻击方法为评估压缩视觉语言模型的真实鲁棒性提供了有效工具。

Abstract: Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.

</details>


### [80] [ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses](https://arxiv.org/abs/2601.21586)
*Ningyuan He,Ronghong Huang,Qianqian Tang,Hongyu Wang,Xianghang Mi,Shanqing Guo*

Main category: cs.CR

TL;DR: ICL-Evader：针对大语言模型上下文学习的零查询黑盒逃避攻击框架，包含三种新型攻击方法，显著降低分类器性能，并提出有效防御方案。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）已成为文本分类的高效范式，但其对抗现实威胁的鲁棒性尚未充分探索。研究旨在评估ICL在实用零查询威胁模型下的安全性。

Method: 提出ICL-Evader框架，包含三种攻击方法：Fake Claim（虚假声明）、Template（模板）和Needle-in-a-Haystack（大海捞针），利用LLM处理上下文提示的固有局限性。在情感分析、毒性和非法推广任务上评估攻击效果。

Result: 攻击显著降低分类器性能，攻击成功率最高达95.3%，远超传统NLP攻击。同时提出联合防御方案，能有效缓解所有攻击且实用损失最小（准确率下降<5%）。

Conclusion: 本研究全面评估了ICL的安全性，揭示了关键漏洞，提供了实用的防御解决方案，并开发了自动化工具来加固标准ICL提示词对抗逃避攻击。

Abstract: In-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (<5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: https://github.com/ChaseSecurity/ICL-Evader .

</details>


### [81] [Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise](https://arxiv.org/abs/2601.21628)
*Puwei Lian,Yujun Cai,Songze Li,Bingkun Bao*

Main category: cs.CR

TL;DR: 该论文提出了一种针对扩散模型的成员推理攻击方法，利用噪声调度中残留的语义信息来推断特定样本是否属于训练数据。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了显著进展，但其部署引发了隐私担忧。微调模型尤其脆弱，因为它们通常在小型私有数据集上微调。现有针对扩散模型的成员推理攻击要么需要获取中间结果，要么需要辅助数据集来训练影子模型。

Method: 利用噪声调度未能完全消除图像语义信息的漏洞，将语义信息注入初始噪声中，通过分析模型生成结果来推断成员关系。

Result: 广泛的实验表明，语义初始噪声能够强烈揭示成员信息，突显了扩散模型对成员推理攻击的脆弱性。

Conclusion: 该研究揭示了扩散模型在隐私保护方面的重要漏洞，提出的简单有效的攻击方法强调了需要更安全的噪声调度和隐私保护机制。

Abstract: Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.

</details>


### [82] [Authenticated encryption for space telemetry](https://arxiv.org/abs/2601.21657)
*Andrew Savchenko*

Main category: cs.CR

TL;DR: 该论文提出了一种轻量级认证加密方案，用于满足NASA-STD-1006A标准中关于紧急空间遥测命令栈保护的要求，在资源受限环境下提供强安全性而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 满足NASA-STD-1006A标准对紧急空间遥测命令栈的保护要求，在资源受限的航天环境中实现安全通信，同时保持与现有数据传输协议的兼容性。

Method: 提出轻量级认证加密方案，生成固定长度的消息，专注于可预测属性和强健的认证机制，在保持性能的同时保护遥测数据的机密性、完整性和真实性。

Result: 该方案能够在资源受限环境中提供强大的安全性，同时保持与底层数据传输协议的兼容性，平衡了安全需求与操作约束。

Conclusion: 通过轻量级认证加密方案，成功满足了NASA-STD-1006A标准对紧急空间遥测命令栈的保护要求，在安全性和性能之间取得了良好平衡。

Abstract: We explore how command stack protection requirements outlined in NASA-STD-1006A can be satisfied within the context of emergency space telemetry. Proposed implementation of lightweight authenticated encryption offers strong security without sacrificing performance in resource-constrained environments. It produces fixed-length messages, maintaining compatibility with the underlying data transport protocols. By focusing on predictable properties and robust authentication, we create a scheme that protects the confidentiality, integrity and authenticity of telemetry data in emergency communications while balancing security requirements with the operational constraints.

</details>


### [83] [WADBERT: Dual-channel Web Attack Detection Based on BERT Models](https://arxiv.org/abs/2601.21893)
*Kangqiang Luo,Yi Xie,Shiqian Zhao,Jing Pan*

Main category: cs.CR

TL;DR: WADBERT是一个基于BERT的Web攻击检测模型，通过混合粒度嵌入和多头注意力机制，实现了高精度检测和恶意参数识别。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在处理不规则HTTP请求时效果不佳，无法有效建模无序参数和实现攻击溯源。需要一种既能高精度检测攻击，又能精确定位恶意参数的方法。

Method: 1. 使用混合粒度嵌入为URL和负载参数生成细粒度嵌入；2. 分别使用URLBERT和SecBERT提取语义特征；3. 通过多头注意力机制融合参数级特征；4. 将拼接的URL和负载特征输入线性分类器获得最终检测结果。

Result: 在CSIC2010和SR-BH2020数据集上分别达到99.63%和99.50%的F1分数，显著优于现有最先进方法。

Conclusion: WADBERT模型不仅实现了高精度的Web攻击检测，还能精确定位恶意参数，解决了现有方法在嵌入不规则HTTP请求和攻击溯源方面的不足。

Abstract: Web attack detection is the first line of defense for securing web applications, designed to preemptively identify malicious activities. Deep learning-based approaches are increasingly popular for their advantages: automatically learning complex patterns and extracting semantic features from HTTP requests to achieve superior detection performance. However, existing methods are less effective in embedding irregular HTTP requests, even failing to model unordered parameters and achieve attack traceability. In this paper, we propose an effective web attack detection model, named WADBERT. It achieves high detection accuracy while enabling the precise identification of malicious parameters. To this end, we first employ Hybrid Granularity Embedding (HGE) to generate fine-grained embeddings for URL and payload parameters. Then, URLBERT and SecBERT are respectively utilized to extract their semantic features. Further, parameter-level features (extracted by SecBERT) are fused through a multi-head attention mechanism, resulting in a comprehensive payload feature. Finally, by feeding the concatenated URL and payload features into a linear classifier, a final detection result is obtained. The experimental results on CSIC2010 and SR-BH2020 datasets validate the efficacy of WADBERT, which respectively achieves F1-scores of 99.63% and 99.50%, and significantly outperforms state-of-the-art methods.

</details>


### [84] [Beyond the Finite Variant Property: Extending Symbolic Diffie-Hellman Group Models (Extended Version)](https://arxiv.org/abs/2601.21910)
*Sofia Giampietro,Ralf Sasse,David Basin*

Main category: cs.CR

TL;DR: 该论文提出了一种支持完整Diffie-Hellman群运算的半决策程序，并将其集成到Tamarin验证器中，首次实现了对包含指数加法等全部群运算的密码协议的形式化验证。


<details>
  <summary>Details</summary>
Motivation: 现有符号协议验证器虽然支持Diffie-Hellman群，但无法处理所有数学运算，特别是缺乏对指数加法的支持，因为这些工具基于合一算法，而在描述所有Diffie-Hellman算子的理论中合一问题是不可判定的。

Method: 提出了一种近似理论并设计了一个半决策程序，用于确定使用完整Diffie-Hellman群运算的协议是否满足用户定义的性质。通过扩展Tamarin验证器来支持完整的Diffie-Hellman理论，包括群元素乘法和指数加法。

Result: 成功实现了首个能够建模和推理使用完整Diffie-Hellman群运算协议的最先进工具。通过案例研究（ElGamal加密和MQV）展示了方法的有效性：证明了ElGamal的安全性，并重新发现了MQV的已知攻击。

Conclusion: 该研究填补了密码协议形式化验证领域的重要空白，首次提供了对使用完整Diffie-Hellman群运算协议的系统化验证能力，为复杂密码协议的安全分析提供了新的工具支持。

Abstract: Diffie-Hellman groups are commonly used in cryptographic protocols. While most state-of-the-art, symbolic protocol verifiers support them to some degree, they do not support all mathematical operations possible in these groups. In particular, they lack support for exponent addition, as these tools reason about terms using unification, which is undecidable in the theory describing all Diffie-Hellman operators. In this paper we approximate such a theory and propose a semi-decision procedure to determine whether a protocol, which may use all operations in such groups, satisfies user-defined properties. We implement this approach by extending the Tamarin prover to support the full Diffie-Hellman theory, including group element multiplication and hence addition of exponents. This is the first time a state-of-the-art tool can model and reason about such protocols. We illustrate our approach's effectiveness with different case studies: ElGamal encryption and MQV. Using Tamarin, we prove security properties of ElGamal, and we rediscover known attacks on MQV.

</details>


### [85] [Secure Group Key Agreement on Cyber-Physical System Buses](https://arxiv.org/abs/2601.21966)
*Sebastian N. Peters,Lukas Lautenschlager,David Emeis,Jason Lochert*

Main category: cs.CR

TL;DR: 本文针对工业与信息物理系统中的总线拓扑结构，设计了一种适用于受限环境的认证式完全分布式群组密钥协商协议，基于TreeKEM实现并进行了评估。


<details>
  <summary>Details</summary>
Motivation: 信息物理系统依赖分布式嵌入式设备通过总线进行安全通信，需要群组共享密钥来确保消息完整性和真实性。现有群组密钥协商协议缺乏对受限CPS总线的适应性，需要一种能够适应工业与信息物理系统约束条件的协议。

Method: 首先系统化分析现有协议，然后推导出总线系统上认证式完全分布式群组密钥协商的需求，最后基于TreeKEM设计、实现并评估了定制化的群组密钥协商协议。

Result: 设计了一个适用于总线拓扑的认证式完全分布式群组密钥协商协议，该协议能够满足广播链路、半双工操作、资源限制、动态成员关系、长设备寿命等约束条件，并能抵御强大的Dolev-Yao攻击者。

Conclusion: 提出的基于TreeKEM的定制化群组密钥协商协议能够有效满足工业与信息物理系统总线环境的安全需求，为受限环境下的安全通信提供了可行的解决方案。

Abstract: Cyber-Physical Systems (CPSs) rely on distributed embedded devices that often must communicate securely over buses. Ensuring message integrity and authenticity on these buses typically requires group-shared keys for Message Authentication Codes (MACs). To avoid insecure fixed pre-shared keys and trust-on-first-use concepts, a Group Key Agreement (GKA) protocol is needed to dynamically agree on a key amongst the devices. Yet existing GKA protocols lack adaptability to constrained CPS buses. This paper targets authenticated, fully distributed GKA suitable for bus topologies under constraints of industrial and cyber-physical systems, including broadcast-only links, half-duplex operation, resource limits, dynamic membership (including unannounced leaves), a long device lifetime, and a strong Dolev-Yao adversary capable of partitioning the bus. We first systematise existing protocols, then derive the requirements necessary for an authenticated and fully distributed GKA on bus systems. Finally, we design, implement, and evaluate a custom GKA protocol based on TreeKEM.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [86] [Frequency as Aperture: Enabling Embeddable Near-Field Sensing for 6G Wireless Radios](https://arxiv.org/abs/2601.21584)
*Pin-Han Ho,Limei Peng,Yiming Miao,Xu Fan,Kairan Liang,Haoran Mei,Wei Duan*

Main category: cs.AR

TL;DR: FaA利用频率敏捷性创建虚拟传感孔径，实现近场感知，仅需单射频链和漏波天线，将空间采样从天线域转移到频域，在相同物理和频谱约束下比传统MIMO传感具有更高架构效率。


<details>
  <summary>Details</summary>
Motivation: 6G无线通信需要原生支持集成传感与通信(ISAC)，但大多数毫米波传感方案依赖专用雷达硬件，与成本和功耗受限的无线节点不兼容。需要一种无线优先的传感范式，降低射频前端复杂性。

Method: 提出频率即孔径(FaA)方法，利用频率敏捷性创建虚拟传感孔径。使用单射频链和频率扫描漏波天线，通过本地振荡器频率扫描实现二维空间传感，将空间采样从天线域转移到频域。

Result: FaA在低功耗和单位成本下提供精细的角度和距离分辨能力，在相同物理和频谱约束下，比传统多通道MIMO传感具有显著更高的架构效率。

Conclusion: 近场传感可以无缝集成到频率敏捷的无线电台中，实现硬件高效、可嵌入且保护隐私的ISAC节点，适用于智能家居、可穿戴设备和工业边缘部署。

Abstract: Integrated sensing and communication (ISAC) is expected to be natively supported by future 6G wireless radios, yet most mmWave sensing solutions still rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes. This article introduces Frequency-as-Aperture (FaA), a wireless-first sensing paradigm that repurposes inherent frequency agility into a virtual sensing aperture, enabling near-field perception with minimal RF front end complexity. Using a single RF chain and a frequency-scanning leaky-wave antenna, FaA achieves two dimensional spatial sensing by reusing the local oscillator (LO) frequency sweep already employed for wideband communication. From a wireless-system perspective, this shifts spatial sampling from the antenna domain to the frequency domain, embedding radar-grade spatial fingerprints directly into the communication RF chain. A case study shows that FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical physical and spectral constraints. These results indicate that near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [87] [Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems](https://arxiv.org/abs/2601.21146)
*Francesco Paladino,Shulu Li,Edward A. Lee*

Main category: cs.DC

TL;DR: maxwait是一种简单但通用的协调机制，能够在分布式时间敏感系统中明确配置可用性与一致性之间的权衡，涵盖多种经典分布式系统方法并支持实时行为。


<details>
  <summary>Details</summary>
Motivation: 分布式时间敏感系统需要在通信延迟和同步不确定性的情况下，平衡时序要求（可用性）和一致性。现有方法缺乏统一的语义框架来明确配置这些权衡。

Method: 提出maxwait协调机制，作为Lingua Franca协调语言的扩展实现。该机制在通信延迟有界时强制执行逻辑时间一致性，在边界被违反时提供结构化故障处理。

Result: maxwait机制能够涵盖PTIDES、Chandy-and-Misra（含/不含空消息）、Jefferson's Time-Warp、Lamport时间故障检测等经典方法，并能实现LET、发布订阅、actor、CRDTs、带future的远程过程调用等常见分布式模式。

Conclusion: maxwait为分布式系统提供了统一的语义框架，在现有机制基础上增加了更好的时序控制、有界时间故障检测和更强的确定性选项，特别适用于分布式信息物理应用。

Abstract: Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

</details>


### [88] [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)
*Yuchen Yang,Yaru Zhao,Pu Yang,Shaowei Wang,Zhi-Hua Zhou*

Main category: cs.DC

TL;DR: ZipMoE是一个高效的、语义无损的移动端MoE服务系统，通过缓存调度协同设计，将移动端MoE推理从I/O瓶颈转变为计算中心工作流，显著降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然增强了大型语言模型的表达能力，但其巨大的内存占用严重阻碍了在资源受限的边缘设备上的实际部署，尤其是在需要保持模型行为而不依赖有损量化的情况下。

Method: ZipMoE利用边缘设备的硬件特性与MoE参数固有的统计冗余之间的协同作用，通过具有可证明性能保证的缓存调度协同设计，从根本上将移动端MoE推理从I/O瓶颈转变为支持高效并行化的计算中心工作流。

Result: 在代表性边缘计算平台上使用流行的开源MoE模型和真实工作负载进行实验，ZipMoE相比最先进的系统实现了高达72.77%的推理延迟降低和高达6.76倍的吞吐量提升。

Conclusion: ZipMoE通过创新的缓存调度协同设计，有效解决了MoE模型在边缘设备部署中的内存瓶颈问题，实现了高效且语义无损的移动端MoE服务。

Abstract: While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\%$ inference latency reduction and up to $6.76\times$ higher throughput than the state-of-the-art systems.

</details>


### [89] [Ira: Efficient Transaction Replay for Distributed Systems](https://arxiv.org/abs/2601.21286)
*Adithya Bhat,Harshal Bhadreshkumar Shah,Mohsen Minaei*

Main category: cs.DC

TL;DR: Ira框架通过传输紧凑提示来加速主备复制中的备份重放，在以太坊案例中实现25倍中位数加速


<details>
  <summary>Details</summary>
Motivation: 在主备复制中，共识延迟受限于备份节点重放主节点提议的事务所需时间。主节点已执行过事务，拥有未来访问模式的精确信息，这正是优化重放所需的关键知识。

Method: 提出Ira框架，通过传输紧凑提示加速备份重放。具体实现Ira-L协议用于以太坊：主节点提供包含以太坊区块使用的键工作集和每个键一字节元数据的提示，备份利用这些提示进行高效区块重放。

Result: 提示紧凑，每个区块中位数增加47KB压缩数据（约5%区块负载）。主节点顺序提示生成和区块执行带来28.6%时间开销，但提示直接成本仅占执行时间的10.9%。备份端Ira-L实现中位数每区块25倍加速，使用16个预取线程时，总重放时间从6.5小时降至16分钟（23.6倍时间加速）。

Conclusion: Ira框架通过利用主节点的未来访问模式知识传输紧凑提示，显著加速了主备复制中的备份重放，在以太坊案例中验证了其有效性，为生产部署提供了可管道化和并行化的优化方案。

Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).

</details>


### [90] [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)
*Bronislav Sidik,Chaya Levi,Joseph Kampeas*

Main category: cs.DC

TL;DR: EWSJF是一种针对混合工作负载下大语言模型服务的新型自适应请求级调度器，通过实时学习工作负载结构，在提高公平性的同时提升吞吐量，相比传统FCFS调度器，吞吐量提升30%以上，短请求的平均首令牌时间最多减少4倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在混合工作负载（短延迟敏感交互查询与长吞吐导向批量请求）下的服务面临基本调度挑战。传统的先到先服务策略存在严重的队头阻塞问题，导致高尾延迟和硬件利用率低下。

Method: EWSJF包含四个核心组件：1）Refine-and-Prune无监督分区算法，发现性能同质请求组；2）动态队列路由，将请求分配到这些组；3）密度加权评分，平衡紧急性和公平性的上下文感知优先级函数；4）贝叶斯元优化，基于实时性能反馈持续调整评分和分区参数。

Result: 在vLLM中实现的EWSJF相比FCFS调度器，端到端吞吐量提升超过30%，短请求的平均首令牌时间最多减少4倍。

Conclusion: 自适应、基于学习的请求调度是大语言模型高效响应服务的关键缺失层，EWSJF通过实时学习工作负载结构，有效解决了混合工作负载下的调度挑战。

Abstract: Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.

</details>


### [91] [Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21855)
*Chuan-Chi Lai*

Main category: cs.DC

TL;DR: SA-PSKY：一种用于边缘-云协作系统的自适应概率天际线查询框架，通过深度强化学习动态调整过滤阈值，显著降低通信开销和响应时间。


<details>
  <summary>Details</summary>
Motivation: 在万物互联时代，边缘传感器数据爆炸式增长，传统分布式概率天际线查询方法依赖静态阈值过滤，无法适应边缘计算环境的高度动态性和异构性，导致通信瓶颈或计算延迟问题。

Method: 提出SA-PSKY自适应框架，将动态阈值调整问题形式化为连续马尔可夫决策过程，利用深度确定性策略梯度智能体实时优化过滤强度，智能分析多维系统状态（数据到达率、不确定性分布、资源可用性）。

Result: 实验评估表明SA-PSKY优于现有静态和启发式基线方法，通信开销降低高达60%，总响应时间减少40%，在不同数据分布下具有鲁棒可扩展性。

Conclusion: SA-PSKY通过深度强化学习实现自适应的阈值调整，有效解决了边缘计算环境中资源冲突问题，为分布式概率天际线查询提供了高效解决方案。

Abstract: In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\% in communication overhead and 40\% in total response time, while ensuring robust scalability across diverse data distributions.

</details>
