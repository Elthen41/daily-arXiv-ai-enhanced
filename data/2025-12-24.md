<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [An Adaptive Distributed Stencil Abstraction for GPUs](https://arxiv.org/abs/2512.19851)
*Aditya Bhosale,Laxmikant Kale*

Main category: cs.DC

TL;DR: 提出基于Charm++的自适应分布式抽象层，用于多节点GPU上的模板计算，提供NumPy风格语法，实现资源弹性伸缩和性能优化


<details>
  <summary>Details</summary>
Motivation: Python科学计算生态系统主要局限于单节点并行，NumPy原型与高性能超级计算之间存在鸿沟。硬件加速器普及和能效需求使资源适应性成为关键要求，但传统HPC抽象仍然僵化

Method: 基于自适应Charm++运行时构建CharmTyles框架，创建支持多节点GPU的分布式模板计算抽象层，采用类似NumPy的熟悉语法以最小化从原型到生产代码的移植工作

Result: 展示了资源弹性能力，可动态调整运行中的应用程序到不同节点数量，分析了相关开销。相比专用高性能模板DSL和通用NumPy替代方案，该抽象层实现了显著性能提升

Conclusion: 提出的自适应分布式抽象层成功弥合了NumPy原型与高性能超级计算之间的差距，通过资源弹性和NumPy风格语法，为模板计算提供了高效的多节点GPU解决方案

Abstract: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.

</details>


### [2] [Scaling Point-based Differentiable Rendering for Large-scale Reconstruction](https://arxiv.org/abs/2512.20017)
*Hexu Zhao,Xiaoteng Liu,Xiwen Min,Jianhao Huang,Youming Deng,Yanfei Li,Ang Li,Jinyang Li,Aurojit Panda*

Main category: cs.DC

TL;DR: Gaian是一个用于点基可微分渲染的通用分布式训练系统，通过优化数据局部性减少通信开销，提升训练效率


<details>
  <summary>Details</summary>
Motivation: 现有PBDR系统与特定方法紧密耦合，且由于数据局部性差导致严重的通信开销，需要通用的高效分布式训练系统

Method: Gaian提供统一的API支持现有PBDR方法，同时暴露丰富的数据访问信息，利用这些信息优化数据局部性并减少通信

Result: 在6个数据集和最多128个GPU上，通信减少高达91%，训练吞吐量提升1.50-3.71倍

Conclusion: Gaian是一个高性能、资源高效的通用分布式训练系统，显著提升了PBDR方法的训练效率和可扩展性

Abstract: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.

</details>


### [3] [Population Protocols Revisited: Parity and Beyond](https://arxiv.org/abs/2512.20163)
*Leszek Gąsieniec,Tytus Grodzicki,Tomasz Jurdziński,Jakub Kowalski,Grzegorz Stachowiak*

Main category: cs.DC

TL;DR: 该论文提出了首个在时间和空间上都高效的群体协议，用于计算奇偶性和模m同余问题，填补了该领域近二十年的空白。


<details>
  <summary>Details</summary>
Motivation: 近二十年来，群体协议研究在分布式计算中取得了重要进展，但对于Presburger算术中互补的奇偶性等同余谓词，一直没有同时实现时间和空间高效的协议。这一空白凸显了该领域的重大挑战。

Method: 提出了一种新颖的计算范式，整合了群体权重、鲁棒的时钟机制、高效的异常检测与切换机制。该范式允许协议不完全优化，而是更注重通用性、鲁棒性和概率保证，实现了高效的多阶段稳定群体协议设计。

Result: 首次提出了高效的奇偶性和同余协议，使用O(log³ n)个状态，并在O(log³ n)时间内实现静默稳定。通过权重系统实现了二进制和一元表示之间的隐式转换。

Conclusion: 该工作填补了群体协议领域的重要空白，提出的计算范式不仅解决了奇偶性和同余问题，还为其他问题（如子群体大小的计算和表示）提供了应用可能性。

Abstract: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.

</details>


### [4] [Reaching Agreement Among Reasoning LLM Agents](https://arxiv.org/abs/2512.20184)
*Chaoyi Ruan,Yiliang Wang,Ziji Shi,Jialin Li*

Main category: cs.DC

TL;DR: 本文提出Aegean，一种基于共识协议的多智能体推理协调框架，解决现有静态工作流导致的资源浪费、延迟高和临时协议风险问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统依赖静态启发式工作流（如固定循环限制和屏障同步），导致计算资源浪费、延迟高（由于落后节点）和可能最终化临时协议的风险。作者认为可靠的多智能体推理需要类似经典分布式共识问题的形式化基础。

Method: 提出多智能体精炼问题的形式化模型，包括正确性保证定义和智能体推理的形式语义。引入Aegean共识协议，专为随机推理智能体设计，解决多智能体精炼问题。实现Aegean-Serve，一个共识感知的服务引擎，在并发智能体执行中执行增量法定人数检测，实现充分智能体收敛时的早期终止。

Result: 在四个数学推理基准测试中，Aegean提供可证明的安全性和活性保证，同时相比最先进基线减少延迟1.2-20倍，保持答案质量在2.5%以内。在本地GPU部署和商业API提供商上的持续增益验证了基于共识的协调消除了落后节点延迟而不牺牲正确性。

Conclusion: 基于共识的协调为多智能体推理提供了形式化基础，解决了现有静态工作流的关键限制，在保持答案质量的同时显著降低延迟。

Abstract: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.
  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.

</details>


### [5] [Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs](https://arxiv.org/abs/2512.20210)
*Yinan Ni,Xiao Yang,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: P-LoRA是一个用于LoRA微调LLM的服务器端推理系统，通过预测性预加载和页面式内存管理，解决了冷启动延迟和GPU内存碎片化问题，显著提升了吞吐量并降低了响应时间。


<details>
  <summary>Details</summary>
Motivation: 在服务器端环境中部署LoRA微调的LLM推理服务面临两大挑战：1）反应式适配器加载导致显著的冷启动延迟；2）频繁的适配器交换导致严重的GPU内存碎片化。这些问题影响了服务器端计算在LLM推理中的实际应用效果。

Method: P-LoRA采用两种关键技术：1）基于LSTM的轻量级流量预测器，预测适配器需求并主动从主机内存预取热门适配器到GPU；2）受操作系统虚拟内存启发的页面式适配器内存管理机制，即使在异构适配器秩的情况下也能保持GPU内存利用率在87%以上。

Result: 实验使用基于Azure Functions追踪的生产级工作负载，结果显示：P-LoRA比S-LoRA实现了1.52倍的吞吐量提升，在高并发场景下平均首次令牌时间（TTFT）降低了35%，冷启动延迟最多减少了68%。

Conclusion: P-LoRA通过预测性预加载和智能内存管理，有效解决了服务器端环境中LoRA微调LLM推理的冷启动和内存碎片化问题，显著提升了系统性能和资源利用率。

Abstract: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras](https://arxiv.org/abs/2512.20073)
*Hongyang Shang,Shuai Dong,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 提出3D堆叠传感器内计算架构，通过指数衰减函数实现实时时间表面归一化，利用DRAM泄漏特性降低硬件开销，在事件视觉处理中显著提升能效和性能。


<details>
  <summary>Details</summary>
Motivation: 传统事件视觉处理存在内存墙问题，需要大量硬件资源存储时间戳，导致功耗高、延迟大。需要一种能同时集成感知、存储和计算的高效架构。

Method: 采用3D堆叠传感器内计算架构，引入指数衰减函数实时归一化时间表面，利用DRAM泄漏特性进行时间戳归一化，使用定制金属-氧化物-金属电容存储电荷，低泄漏开关延长有效电荷存储时间。

Result: 相比2D架构，功耗降低69倍，延迟减少2.2倍，面积减少1.9倍；相比16位SRAM存储时间戳方案，功耗降低三个数量级；在计算机视觉任务中达到与数字实现相当的精度，在多个数据集上获得与最先进方法可比的结果。

Conclusion: 3D堆叠传感器内计算架构为实时、资源高效的事件视觉处理奠定了基础，未来可集成更先进的计算电路以扩展应用范围。

Abstract: This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.

</details>


### [7] [Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling](https://arxiv.org/abs/2512.20198)
*Huizheng Wang,Taiquan Wei,Hongbin Wang,Zichuan Wang,Xinru Tang,Zhiheng Yue,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: STAR是一种针对大语言模型长序列并行推理的跨阶段算法-硬件协同设计，通过稀疏预测和协调分块策略，显著提升计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有动态稀疏性加速器在长序列并行处理场景下表现不佳，因为它们是阶段隔离优化的。研究发现跨阶段协调可以大幅减少冗余计算和内存访问。

Method: 提出STAR跨阶段协同设计：1) 基于前导零的稀疏预测，使用对数域仅加法操作最小化预测开销；2) 分布式排序和排序更新FlashAttention机制；3) 协调分块策略实现细粒度阶段交互；4) 专用STAR加速器架构和多核空间架构部署。

Result: 相比A100实现9.2倍加速和71.2倍能效提升，超越现有最佳加速器16.1倍能效和27.1倍面积效率。在多核空间架构上，相比基线设计实现20.1倍吞吐量提升。

Conclusion: STAR通过跨阶段协同设计有效解决了长序列并行处理中的计算和内存效率问题，为Transformer推理提供了高效的算法-硬件协同解决方案。

Abstract: Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\times$ speedup and 71.2$\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\times$ energy and 27.1$\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\times$ throughput improvement.

</details>


### [8] [Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization](https://arxiv.org/abs/2512.20495)
*He Zhu,Zheng Liu,Xingyang Li,Anbang Wu,Jieru Zhao,Fangxin Liu,Yiming Gan,Jingwen Leng,Yu Feng*

Main category: cs.AR

TL;DR: Nebula是一个用于大规模3D高斯溅射协同渲染的加速框架，通过流式传输中间结果而非视频，大幅减少云与客户端间的数据传输，并提升VR体验的流畅度。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯溅射架构设计忽略了可扩展性，对大规模场景处理脆弱；同时VR带宽需求使得云端无法传输高保真、流畅的VR内容。

Method: 1) 流式传输LoD搜索后的中间结果而非视频，减少1925%数据传输；2) 云端引入时间感知的LoD搜索，利用帧间时间一致性减少冗余数据访问；3) 客户端提出新颖的立体光栅化技术，让双眼共享大部分计算；4) 最小硬件增强。

Result: 实现了2.7倍的运动到光子延迟加速，相比有损视频流减少了1925%的带宽消耗。

Conclusion: Nebula框架通过创新的协同渲染方法，有效解决了大规模3D高斯溅射的扩展性和VR带宽瓶颈问题，显著提升了VR体验质量。

Abstract: 3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.
  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.

</details>


### [9] [Composing Mini Oscilloscope on Embedded Systems](https://arxiv.org/abs/2512.20571)
*Brennan Romero,D. G. Perera*

Main category: cs.AR

TL;DR: 使用Nuvoton NUC-140嵌入式平台实现基础示波器功能，包括自动、边沿触发、单次模式，波形垂直/水平缩放，探头校准等90%常用功能


<details>
  <summary>Details</summary>
Motivation: 利用低成本嵌入式平台实现示波器基本功能，为电子调试提供经济实用的工具，替代传统昂贵示波器

Method: 采用Nuvoton NUC-140嵌入式开发平台作为前端和显示，通过定制子板连接BNC探头接口、九键键盘和校准信号，LCD作为波形显示器

Result: 系统成功实现了90%常用示波器功能，包括自动、边沿触发、单次模式，波形垂直/水平缩放，探头校准，成为有效的调试工具

Conclusion: 基于NUC-140的示波器系统能够有效替代传统示波器，实现大部分常用功能，具有实用价值和成本优势

Abstract: In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Energy-Efficient Multi-LLM Reasoning for Binary-Free Zero-Day Detection in IoT Firmware](https://arxiv.org/abs/2512.19945)
*Saeid Jamshidi,Omar Abdul-Wahab,Martine Bellaïche,Foutse Khomh*

Main category: cs.CR

TL;DR: 提出了一种无需二进制代码、架构无关的IoT固件零日漏洞可能性评估方法，通过高级描述符和LLM推理架构实现


<details>
  <summary>Details</summary>
Motivation: IoT固件安全分析面临专有二进制、符号剥离、异构架构和代码访问受限等挑战，现有方法依赖二进制可见性和功能模拟，在固件加密或不可访问时不可靠

Method: 采用三元LLM推理架构：LLaMA配置解释器、DeepSeek结构抽象分析器和GPT-4o语义融合模型，结合LLM计算签名（延迟模式、不确定性标记、推理深度指标）和能量感知符号负载模型

Result: 模拟评估显示高暴露条件下预测的零日漏洞可能性增加20-35%，GPT-4o表现出最强的跨层相关性和最高敏感性，能量和发散度指标显著预测风险升高（p<0.01）

Conclusion: 该方法为无法访问二进制代码的IoT固件安全评估提供了可行的解决方案，通过形式化数学基础和LLM推理架构实现了架构无关的零日漏洞可能性估计

Abstract: Securing Internet of Things (IoT) firmware remains difficult due to proprietary binaries, stripped symbols, heterogeneous architectures, and limited access to executable code. Existing analysis methods, such as static analysis, symbolic execution, and fuzzing, depend on binary visibility and functional emulation, making them unreliable when firmware is encrypted or inaccessible. To address this limitation, we propose a binary-free, architecture-agnostic solution that estimates the likelihood of conceptual zero-day vulnerabilities using only high-level descriptors. The approach integrates a tri-LLM reasoning architecture combining a LLaMA-based configuration interpreter, a DeepSeek-based structural abstraction analyzer, and a GPT-4o semantic fusion model. The solution also incorporates LLM computational signatures, including latency patterns, uncertainty markers, and reasoning depth indicators, as well as an energy-aware symbolic load model, to enhance interpretability and operational feasibility. In addition, we formally derive the mathematical foundations of the reasoning pipeline, establishing monotonicity, divergence, and energy-risk coupling properties that theoretically justify the model's behavior. Simulation-based evaluation reveals that high exposure conditions increase the predicted zero-day likelihood by 20 to 35 percent across models, with GPT-4o demonstrating the strongest cross-layer correlations and the highest sensitivity. Energy and divergence metrics significantly predict elevated risk (p < 0.01), reinforcing the effectiveness of the proposed reasoning framework.

</details>


### [11] [Efficient Mod Approximation and Its Applications to CKKS Ciphertexts](https://arxiv.org/abs/2512.19951)
*Yufei Zhou*

Main category: cs.CR

TL;DR: 提出基于多项式插值和切比雪夫级数的新方法，在CKKS同态加密中准确近似模函数，并设计BitStack和CRTStack数据打包方案，实现高效密文上传和完整输入范围的准确模运算。


<details>
  <summary>Details</summary>
Motivation: 模函数在数据编码和密码学原语中至关重要，但广泛使用的CKKS同态加密方案仅支持算术运算，难以在加密数据上执行模计算。现有方法只能在有限子范围内提供准确结果，无法实现完整输入范围的准确近似。

Method: 基于多项式插值和切比雪夫级数准确近似模函数；针对CKKS中的小整数输入设计BitStack和CRTStack两种高效数据打包方案，提升明文空间利用率；应用该模函数实现同态舍入操作和从加法秘密分享到CKKS密文的一般转换。

Result: 实验结果显示方法达到高近似精度（高达1e-8）；实现了准确的密文舍入和完整的秘密分享到CKKS转换；显著提升了CKKS明文空间利用率，支持高效的密文上传。

Conclusion: 该工作为CKKS下的模运算提供了实用且通用的解决方案，扩展了其在隐私保护计算中的适用性，解决了完整输入范围准确近似模函数的关键挑战。

Abstract: The mod function plays a critical role in numerous data encoding and cryptographic primitives. However, the widely used CKKS homomorphic encryption (HE) scheme supports only arithmetic operations, making it difficult to perform mod computations on encrypted data. Approximating the mod function with polynomials has therefore become an important yet challenging problem. The discontinuous and periodic characteristics of the mod function make it particularly difficult to approximate accurately under HE. Existing homomorphic mod constructions provide accurate results only within limited subranges of the input range, leaving the problem of achieving accurate approximation across the full input range unresolved. In this work, we propose a novel method based on polynomial interpolation and Chebyshev series to accurately approximate the mod function. Building upon this, we design two efficient data packing schemes, BitStack and CRTStack, tailored for small-integer inputs in CKKS. These schemes significantly improve the utilization of the CKKS plaintext space and enable efficient ciphertext uploads. Furthermore, we apply the proposed HE mod function to implement a homomorphic rounding operation and a general transformation from additive secret sharing to CKKS ciphertexts, achieving accurate ciphertext rounding and complete secret-share-to-CKKS conversion. Experimental results demonstrate that our approach achieves high approximation accuracy (up to 1e-8). Overall, our work provides a practical and general solution for performing mod operations under CKKS, extending its applicability to a broader range of privacy-preserving computations.

</details>


### [12] [Fast Deterministically Safe Proof-of-Work Consensus](https://arxiv.org/abs/2512.19968)
*Ali Farahbakhsh,Giuliano Losa,Youer Pu,Lorenzo Alvisi,Ittay Eyal*

Main category: cs.CR

TL;DR: Sieve-MMR是首个具有确定性安全性和恒定预期延迟的完全无许可协议，通过将PoS协议MMR移植到PoW环境实现，解决了PoW的时间旅行攻击问题。


<details>
  <summary>Details</summary>
Motivation: 现有无许可区块链协议（PoW和PoS）都存在安全漏洞：PoS易受长程攻击，PoW易受算力攻击。PoS依赖外部社会共识机制，PoW要么依赖概率性保证要么速度慢。需要一种不依赖外部机制、具有确定性安全性和恒定延迟的完全无许可协议。

Method: 将PoS协议MMR移植到PoW环境，提出Sieve算法实现时间旅行弹性广播（TTRB）原语。Sieve使用黑盒确定性PoW原语实现TTRB，作为MMR的消息传递层，从而解决时间旅行攻击问题。

Result: Sieve-MMR协议结合了MMR的恒定预期延迟和确定性安全性，同时利用PoW抵御长程攻击，实现了首个不依赖外部机制、具有确定性安全性和恒定延迟的完全无许可协议。

Conclusion: Sieve-MMR成功解决了无许可区块链的安全性和延迟问题，通过创新的Sieve算法和TTRB广播原语，实现了PoW环境下MMR协议的安全移植，为无许可区块链提供了新的解决方案。

Abstract: Permissionless blockchains achieve consensus while allowing unknown nodes to join and leave the system at any time. They typically come in two flavors: proof of work (PoW) and proof of stake (PoS), and both are vulnerable to attacks. PoS protocols suffer from long-range attacks, wherein attackers alter execution history at little cost, and PoW protocols are vulnerable to attackers with enough computational power to subvert execution history. PoS protocols respond by relying on external mechanisms like social consensus; PoW protocols either fall back to probabilistic guarantees, or are slow.
  We present Sieve-MMR, the first fully-permissionless protocol with deterministic security and constant expected latency that does not rely on external mechanisms. We obtain Sieve-MMR by porting a PoS protocol (MMR) to the PoW setting. From MMR we inherit constant expected latency and deterministic security, and proof-of-work gives us resilience against long-range attacks. The main challenge to porting MMR to the PoW setting is what we call time-travel attacks, where attackers use PoWs generated in the distant past to increase their perceived PoW power in the present. We respond by proposing Sieve, a novel algorithm that implements a new broadcast primitive we dub time-travel-resilient broadcast (TTRB). Sieve relies on a black-box, deterministic PoW primitive to implement TTRB, which we use as the messaging layer for MMR.

</details>


### [13] [BacAlarm: Mining and Simulating Composite API Traffic to Prevent Broken Access Control Violations](https://arxiv.org/abs/2512.19997)
*Yanjing Yang,He Zhang,Bohan Liu,Jinwei Xu,Jinghao Hu,Liming Dong,Zhewen Mao,Dongxue Pan*

Main category: cs.CR

TL;DR: BAC是一种基于API流量生成和学习的BAC违规检测方法，通过生成训练数据解决数据短缺问题，并利用多请求关联分析检测BAC攻击。


<details>
  <summary>Details</summary>
Motivation: BAC违规是OWASP API安全前五大风险之一，但现有学习方法面临两大挑战：1）RESTful API设计下训练数据严重短缺；2）BAC攻击由多个看似正常的关联请求组成，难以通过单请求异常检测发现。

Method: 提出BAC方法，包含API流量生成器和BAC检测器两部分。流量生成器解决数据短缺问题，检测器通过分析多请求关联模式识别BAC违规。

Result: 实验结果表明，BAC方法在F1分数和MCC指标上分别比当前最先进的基于不变式和基于学习的方法提高了21.2%和24.1%。

Conclusion: BAC方法通过生成API流量数据并利用多请求关联分析，有效解决了BAC违规检测中的数据短缺和检测难题，显著提升了检测性能。

Abstract: Broken Access Control (BAC) violations, which consistently rank among the top five security risks in the OWASP API Security Top 10, refer to unauthorized access attempts arising from BAC vulnerabilities, whose successful exploitation can impose significant risks on exposed application programming interfaces (APIs). In recent years, learning-based methods have demonstrated promising prospects in detecting various types of malicious activities. However, in real-network operation and maintenance scenarios, leveraging learning-based methods for BAC detection faces two critical challenges. Firstly, under the RESTful API design principles, most systems omit recording composite traffic for performance, and together with ethical and legal bans on directly testing real-world systems, this leads to a critical shortage of training data for detecting BAC violations. Secondly, common malicious behaviors such as SQL injection typically generate individual access traffic that is inherently anomalous. In contrast, BAC is usually composed of multiple correlated access requests that appear normal when examined in isolation. To tackle these problems, we introduce \BAC, an approach for establishing a BAC violation detection model by generating and utilizing API traffic data. The \BAC consists of an API Traffic Generator and a BAC Detector. Experimental results show that \BAC outperforms current state-of-the-art invariant-based and learning-based methods with the $\text{F}_1$ and MCC improving by 21.2\% and 24.1\%.

</details>


### [14] [On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities](https://arxiv.org/abs/2512.20062)
*Sangryu Park,Gihyuk Ko,Homook Cho*

Main category: cs.CR

TL;DR: 该研究将LLM应用于软件漏洞分析，从二元分类改进为识别具体CWE类型，并通过指令微调本地部署的小型LLM实现比在线API模型更好的性能、安全性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件漏洞分析中存在两个主要问题：1) 依赖在线API服务需要公开源代码，存在安全风险；2) 仅进行二元分类（有漏洞/无漏洞），实用性有限。需要更安全、更实用的解决方案。

Method: 将问题重新定义为软件漏洞识别(SVI)，要求LLM输出具体的CWE ID而非简单二元判断。通过指令微调较小、可本地部署的LLM，避免使用在线API服务。

Result: 指令微调的本地LLM在整体性能和成本权衡方面优于在线API模型。本地模型在漏洞识别任务中表现出更好的效果。

Conclusion: 指令微调的本地LLM为实际漏洞管理工作流程提供了更有效、安全和实用的方法，解决了现有方法的局限。

Abstract: Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.

</details>


### [15] [Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography](https://arxiv.org/abs/2512.20168)
*Songze Li,Jiameng Cheng,Yiming Li,Xiaojun Jia,Dacheng Tao*

Main category: cs.CR

TL;DR: 本文提出了一种名为Odysseus的新型越狱攻击方法，利用双隐写术在多模态大语言模型系统中隐藏恶意内容，成功绕过现有安全过滤器，攻击成功率高达99%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型系统存在安全盲点，现有安全过滤器假设恶意内容必须在输入或输出中显式可见，但攻击者可以利用多模态特性隐藏恶意意图，导致现有防御措施失效。

Method: 提出Odysseus越狱范式，采用双隐写术技术，将恶意查询和响应隐蔽地嵌入看似良性的图像中，从而绕过基于显式内容检测的安全过滤器。

Result: 在基准数据集上的广泛实验表明，Odysseus成功越狱多个领先且现实的多模态大语言模型系统，攻击成功率高达99%，暴露了现有防御措施的根本盲点。

Conclusion: 该研究揭示了多模态大语言模型集成系统中跨模态安全的根本缺陷，挑战了现有安全过滤器的核心假设，呼吁重新思考多模态系统的安全防御策略。

Abstract: By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.

</details>


### [16] [From the Two-Capacitor Paradox to Electromagnetic Side-Channel Mitigation in Digital Circuits](https://arxiv.org/abs/2512.20303)
*Raghvendra Pratap Singh,Baibhab Chatterjee,Shreyas Sen,Debayan Das*

Main category: cs.CR

TL;DR: 该论文从电子电路安全角度重新审视经典的两电容悖论，将电容器充电过程中的能量损耗与电磁侧信道分析泄漏联系起来，并提出绝热充电作为减少电磁泄漏的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着资源受限的互联网连接设备增长，电磁侧信道分析在电子系统安全中变得日益重要。论文旨在探索电容器充电过程中的能量损耗如何与电磁泄漏相关，从而影响设备安全。

Method: 通过分析标准RC和RLC电路模型，从理论上证明电容器充电过程中的能量损耗主要转化为热量和辐射。然后将这种能量损耗与电磁侧信道分析泄漏联系起来，展示如何利用这种泄漏恢复设备中的加密密钥。

Result: 分析表明电容器充电过程中的能量损耗确实会导致电磁辐射，这种辐射可以被用于侧信道攻击来恢复加密密钥。绝热充电被证明是减少这种电磁泄漏的有效方法。

Conclusion: 电容器充电过程中的能量损耗与电磁侧信道分析泄漏直接相关，绝热充电技术可以最小化电磁泄漏，为实现低开销的电磁侧信道分析防护提供了途径。

Abstract: The classical two-capacitor paradox of the lost energy is revisited from an electronic circuit security stand-point. The paradox has been solved previously by various researchers, and the energy lost during the charging of capacitors has been primarily attributed to the heat and radiation. We analytically prove this for various standard resistor-capacitor (RC) and resistor-inductor-capacitor (RLC) circuit models. From the perspective of electronic system security, electromagnetic (EM) side-channel analysis (SCA) has recently gained significant prominence with the growth of resource-constrained, internet connected devices. This article connects the energy lost due to capacitor charging to the EM SCA leakage in electronic devices, leading to the recovery of the secret encryption key embedded within the device. Finally, with an understanding of how lost energy relates to EM radiation, we propose adiabatic charging as a solution to minimize EM leakage, thereby paving the way towards low-overhead EM SCA resilience.

</details>


### [17] [Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms](https://arxiv.org/abs/2512.20323)
*Ipek Sena Yilmaz,Onur G. Tuncer,Zeynep E. Aksoy,Zeynep Yağmur Baydemir*

Main category: cs.CR

TL;DR: 该论文提出了一种针对无线感知任务的差分隐私特征发布方法，通过自适应隐私预算分配机制，在保护用户敏感信息的同时保持感知任务的实用性。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi/RF感知系统在深度学习推动下取得显著进展，但实际部署中需要共享特征进行云分析、协作训练或基准评估。发布中间表示（如CSI频谱图）可能无意中暴露用户身份、位置和成员资格等敏感信息，因此需要正式的隐私保护保证。

Method: 提出自适应隐私预算分配机制，针对CSI时频表示的高度非均匀结构进行优化。流程包括：将CSI转换为有界频谱图特征，通过裁剪进行敏感性控制，估计时频平面上任务相关的重要性，在注入校准高斯噪声前将全局隐私预算分配到频谱图块中。

Result: 在多用户活动感知（WiMANS）、多人3D姿态估计（Person-in-WiFi 3D）和呼吸监测（Resp-CSI）实验中，自适应分配在相同隐私预算下相比均匀扰动显著提升了隐私-效用边界，获得更高准确率和更低误差，同时大幅减少了身份和成员推理攻击中的经验泄漏。

Conclusion: 该研究为无线感知任务提供了一种有效的差分隐私特征发布方法，通过自适应隐私预算分配机制，在保护用户隐私的同时保持了感知任务的实用性，为实际部署中的隐私保护提供了解决方案。

Abstract: Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks.

</details>


### [18] [Symmaries: Automatic Inference of Formal Security Summaries for Java Programs](https://arxiv.org/abs/2512.20396)
*Narges Khakpour,Nicolas Berthier*

Main category: cs.CR

TL;DR: Symmaries工具通过自动生成Java字节码程序的形式化安全规范（方法摘要），为静态代码分析提供可扩展、模块化且可靠的安全行为抽象表示。


<details>
  <summary>Details</summary>
Motivation: 为了解决Java程序安全分析中手动编写形式化安全规范的困难，以及帮助开发人员理解代码库的安全行为，特别是在重用库代码时评估其安全影响。

Method: 开发Symmaries工具，采用可扩展、模块化且可靠的方法自动构建Java字节码程序的安全规范，生成包含安全调用条件、信息流和别名更新的方法摘要。

Result: Symmaries成功扩展到分析数十万行代码的大型实际应用，根据使用的堆模型实现了有前景的精度，并证明了在终止不敏感非干扰性方面的可靠性。

Conclusion: Symmaries提供了一种自动化生成Java程序安全规范的有效方法，能够帮助静态分析工具和开发人员更好地理解和评估代码的安全行为。

Abstract: We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.

</details>


### [19] [iblock: Accurate and Scalable Bitcoin Simulations with OMNeT++](https://arxiv.org/abs/2512.20402)
*Niccolò Scatena,Pericle Perazzo,Giovanni Nardini*

Main category: cs.CR

TL;DR: iblock是一个用于OMNeT++的比特币模拟C++库，相比现有高级语言编写的模拟器具有更高效率和可扩展性，并能与其他OMNeT++库集成实现高细节模拟


<details>
  <summary>Details</summary>
Motivation: 现有比特币模拟器通常使用高级语言编写，在效率和可扩展性方面存在局限，需要更高效、可扩展且能与其他模拟库集成的解决方案

Method: 开发iblock - 一个全面的C++比特币模拟库，专为OMNeT++设计，利用C++的高性能特性，并支持与其他OMNeT++库的集成

Result: 与现有最先进的区块链模拟器相比，iblock在相同模拟细节水平下效率更高；通过模拟正常比特币运行和自私挖矿攻击等场景，验证了模拟结果与理论预期一致

Conclusion: iblock提供了一个高效、可扩展且能与其他OMNeT++库集成的比特币模拟解决方案，在性能和模拟细节方面优于现有模拟器

Abstract: This paper proposes iblock, a comprehensive C++ library for Bitcoin simulation, designed for OMNeT++. iblock offers superior efficiency and scalability with respect to state-of-the-art simulators, which are typically written in high-level languages. Moreover, the possible integration with other OMNeT++ libraries allows highly detailed simulations. We measure iblock's performance against a state-of-the-art blockchain simulator, proving that it is more efficient at the same level of simulation detail. We also validate iblock by using it to simulate different scenarios such as the normal Bitcoin operation and the selfish mine attack, showing that simulation results are coherent with theoretical expectations.

</details>


### [20] [ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected](https://arxiv.org/abs/2512.20405)
*Kanchon Gharami,Sanjiv Kumar Sarkar,Yongxin Liu,Shafika Showkat Moni*

Main category: cs.CR

TL;DR: 论文探讨了LLM在科学论文写作和评审中的双重风险：攻击方可通过PDF隐藏提示操控LLM评审，防御方则提出"注入-检测"策略来识别AI生成的评审。


<details>
  <summary>Details</summary>
Motivation: LLM在科学论文写作和评审中的广泛应用虽然提高了效率，但也带来了严重风险，包括缺乏真正创新性、伪造结果、偏见以及误导下游研究等问题，可能损害声誉、浪费资源，甚至在医学或安全关键系统中危及生命。

Method: 研究从攻击和防御两个角度展开：攻击方面展示了作者如何在PDF中注入隐藏提示来"越狱"LLM评审者，使其给出过度积极和有偏见的评审；防御方面提出了"注入-检测"策略，编辑在论文中嵌入不可见的触发提示，如果评审重复或对这些提示做出反应，则表明评审是由LLM生成的。

Result: 该方法将提示注入从漏洞转变为验证工具，能够有效识别AI生成的评审，帮助恢复科学评估的信任度，同时设计了预期的模型行为和部署的伦理保障措施。

Conclusion: 研究揭示了当前同行评审过程在LLM影响下的脆弱性，通过编辑意识提升可以帮助恢复科学评估的信任，将提示注入从威胁转变为检测工具，为应对LLM在学术出版中的风险提供了实用解决方案。

Abstract: Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or "jailbreak" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an "inject-and-detect" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation.

</details>


### [21] [Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit](https://arxiv.org/abs/2512.20423)
*Adam Elaoumari*

Main category: cs.CR

TL;DR: 该项目评估DNS-over-HTTPS文件外泄检测效果，开发了容器化工具包生成、拦截和分析DoH外泄流量，比较机器学习与阈值检测在对抗场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 随着DNS-over-HTTPS的普及，攻击者可能利用其加密特性进行文件外泄，而现有检测方法在对抗性场景下的效果尚不明确，需要系统评估防御者检测能力和攻击者规避策略。

Method: 开发端到端容器化流水线，通过可配置参数（分块、编码、填充、解析器轮换）生成DoH文件外泄流量，使用改进的DoHLyzer提取流级特征，训练随机森林、梯度提升和逻辑回归分类器，并与阈值检测方法进行对比评估。

Result: 创建了完整的可复现工具包，封装在多个Docker容器中，实现了流量生成、文件捕获、特征提取、模型训练和分析的自动化流程，为DoH外泄检测提供了基准测试框架。

Conclusion: 该研究为DoH文件外泄检测提供了系统评估工具，未来方向包括在企业混合流量中验证结果、扩展到HTTP/3/QUIC协议、添加良性流量生成以及实时流量评估，最终目标是量化隐蔽约束何时使DoH外泄对攻击者不经济。

Abstract: The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.

</details>


### [22] [ARBITER: AI-Driven Filtering for Role-Based Access Control](https://arxiv.org/abs/2512.20535)
*Michele Lorenzo,Idilio Drago,Dario Salvadori,Fabio Romolo Vayr*

Main category: cs.CR

TL;DR: 论文提出OUR系统，为RAG系统实现基于角色的访问控制，解决传统RBAC在动态企业环境中处理敏感文档时的不足，通过分层验证、角色感知检索和后生成事实检查等方法，在合成数据集上达到85%准确率和89% F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统基于角色的访问控制（RBAC）难以适应动态企业环境，特别是当文档包含不能向特定用户组披露的敏感信息时。当这些文档被LLM驱动系统（如RAG）使用时，问题更加严重，因为LLM可能因提示截断、分类错误或系统上下文丢失而泄露敏感数据。

Method: 提出OUR系统，实现RAG系统中的RBAC功能。系统采用分层输入/输出验证、角色感知检索和后生成事实检查。与传统依赖微调分类器的RBAC方法不同，OUR使用LLM在少样本设置下通过提示引导进行快速部署和角色更新。

Result: 在389个查询的合成数据集上评估，实验结果显示查询过滤达到85%准确率和89% F1分数，接近传统RBAC解决方案的性能。这表明在RAG系统中实际部署RBAC已接近满足动态企业环境所需的成熟度水平。

Conclusion: OUR系统为RAG系统提供了有效的基于角色的访问控制解决方案，能够适应动态企业环境的需求，通过LLM驱动的少样本方法实现了接近传统RBAC的性能，为实际部署提供了可行路径。

Abstract: Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \our, a system designed to provide RBAC in RAG systems. \our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\% accuracy and 89\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.

</details>


### [23] [Making Sense of Private Advertising: A Principled Approach to a Complex Ecosystem](https://arxiv.org/abs/2512.20583)
*Kyle Hogan,Alishah Chator,Gabriel Kaptchuk,Mayank Varia,Srinivas Devadas*

Main category: cs.CR

TL;DR: 论文分析了广告生态系统中的隐私问题，指出当前隐私保护方案存在两个主要问题：1）现有工作孤立考虑广告定向和参与度指标，缺乏组合隐私保护；2）证明完全隐私在实用广告生态中不可能实现。作者提出需要基于"敏感数据"概念重新定义广告隐私。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护广告提案存在缺陷，主要问题在于：1）现有研究孤立考虑广告定向和参与度指标，导致组合后无法提供端到端隐私保护；2）完全隐私在实用广告生态中不可能实现。需要重新思考广告隐私的实际含义。

Method: 1）建立广告生态系统端到端管道模型；2）分析现有隐私保护方案的组合问题；3）证明完全隐私在实用广告生态中的不可能性；4）基于"敏感数据"概念重新定义广告隐私；5）提出需要新的隐私保护广告子系统设计方法。

Result: 1）识别出当前隐私广告提案的两个主要问题：组合隐私失败和完全隐私不可能；2）证明任何有实用性的广告生态系统都无法实现完美隐私；3）提出需要基于上下文敏感数据概念重新定义广告隐私；4）指出需要全新的隐私保护广告子系统设计方法。

Conclusion: 广告生态中的隐私泄露是固有的，完美隐私不可能实现。需要基于敏感数据概念重新定义广告隐私，并采用全新的隐私保护子系统设计方法，确保端到端广告系统的隐私属性与用户的隐私期望保持一致。

Abstract: In this work, we model the end-to-end pipeline of the advertising ecosystem, allowing us to identify two main issues with the current trajectory of private advertising proposals. First, prior work has largely considered ad targeting and engagement metrics individually rather than in composition. This has resulted in privacy notions that, while reasonable for each protocol in isolation, fail to compose to a natural notion of privacy for the ecosystem as a whole, permitting advertisers to extract new information about the audience of their advertisements. The second issue serves to explain the first: we prove that \textit{perfect} privacy is impossible for any, even minimally, useful advertising ecosystem, due to the advertisers' expectation of conducting market research on the results.
  Having demonstrated that leakage is inherent in advertising, we re-examine what privacy could realistically mean in advertising, building on the well-established notion of \textit{sensitive} data in a specific context. We identify that fundamentally new approaches are needed when designing privacy-preserving advertising subsystems in order to ensure that the privacy properties of the end-to-end advertising system are well aligned with people's privacy desires.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 本文提出了一种双目标优化方法，用于规划灾后救援物资从配送中心到避难所的车辆路径，同时考虑有限的救援物资分配。该方法平衡效率（最小化总旅行时间）和公平性（最小化基于基尼系数的不满足需求不平等度量），通过分支定价算法有效求解。


<details>
  <summary>Details</summary>
Motivation: 在重大灾害中，预先储备的物资往往无法满足所有需求。灾后救援物资分配到避难所是人道主义物流的关键环节，需要在有限的物资条件下平衡配送效率和分配公平性。

Method: 1. 建立双目标混合整数规划模型：最小化基于基尼系数的不满足需求不平等度量和最小化总旅行时间；2. 使用ε-约束方法处理双目标优化；3. 推导最优解的数学性质，引入有效不等式；4. 设计给定可行车辆路径下的最优配送分配算法；5. 开发分支定价算法高效求解问题。

Result: 1. 分支定价算法在土耳其凡城地震实际数据和伊斯坦布尔卡塔尔地区预测数据的计算测试中显著优于商业MIP求解器；2. 双目标方法在不影响效率的情况下将援助分配不平等度降低了34%；3. 当时间约束非常宽松或严格时，按需求覆盖优先于公平性的词典序优化有效；4. 对于中等限制的时间约束，平衡方法对避免不公平结果至关重要。

Conclusion: 本文提出的双目标优化方法和分支定价算法能够有效解决灾后救援物资配送中的效率与公平平衡问题。研究表明，在不同时间约束条件下需要采用不同的优化策略：宽松或严格约束时可采用词典序优化，中等约束时需要平衡方法以确保公平分配。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [25] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 该论文提出了一种用于PlantClef 2025挑战赛的方法，通过从训练数据中提取类原型作为指导，训练分割Vision Transformer模型，实现从高分辨率植被图像中进行细粒度多标签物种识别。


<details>
  <summary>Details</summary>
Motivation: 解决PlantClef 2025挑战赛中的细粒度多标签物种识别问题，特别是在高分辨率植被图像中识别多个物种。需要将单物种分类模型适应到多标签分类任务中。

Method: 1. 从训练数据中提取特征并使用K-Means聚类生成类原型（K等于类别数）
2. 构建定制化窄Vision Transformer，用冻结的DinoV2替换patch embedding层
3. 训练分割模型从测试图像重建训练数据的类原型
4. 利用注意力分数识别和定位感兴趣区域，指导分类过程
5. 实现从单物种分类到多标签分类的领域适应

Result: 在PlantCLEF 2025挑战赛私有排行榜上获得第五名，F1分数为0.33331。与最佳提交结果仅相差0.03，显示出在基准任务中具有竞争力的性能。

Conclusion: 提出的基于类原型指导的分割Vision Transformer方法能够有效处理高分辨率植被图像的多标签物种识别任务，在PlantCLEF挑战赛中取得了有竞争力的结果，证明了领域适应策略的有效性。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [26] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 提出一种通过类内聚类生成伪标签进行层次分类的方法，以缓解细粒度视觉分类中的类内变异问题，在PlantNet300k数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类任务中，类内变异（同一类别内图像的差异程度）会阻碍深度学习模型的学习过程，特别是当这些类别还处于欠表示状态时。需要一种方法来缓解类内变异问题，提升分类性能。

Method: 提出新颖方法：通过对每个类别单独进行聚类，发现编码图像间相似度的潜在伪标签，然后将这些标签用于层次分类过程，学习更细粒度的视觉特征，从而缓解类内变异问题。

Result: 在PlantNet300k数据集上的初步实验揭示了几个关键点，未来工作需要进一步开发以获得更确凿的证据。尽管某些组件尚未完全优化，但该方法在PlantNet300k数据集上仍达到了最先进的性能。

Conclusion: 通过类内聚类生成伪标签进行层次分类的方法能够有效缓解细粒度视觉分类中的类内变异问题，提升分类性能，在PlantNet300k数据集上取得了有竞争力的结果，为未来研究提供了方向。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [27] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 提出LieFlow方法，通过流匹配在Lie群上直接从数据中学习对称性，能够发现更灵活类型的群结构，在2D和3D点云上成功发现了包括反射在内的离散群


<details>
  <summary>Details</summary>
Motivation: 对称性对理解物理系统和提升机器学习性能都很重要，但需要了解数据中的底层对称性。现有方法在发现对称性方面存在限制，需要更灵活的方法直接从数据中学习对称性

Method: 提出LieFlow方法，将对称性发现定义为在更大的假设群上学习分布，使学习到的分布与数据中观察到的对称性匹配。通过Lie群上的流匹配来学习对称性，并针对目标模式对称排列导致的"最后一刻收敛"问题，引入了新的流匹配插值方案

Result: 在2D和3D点云上的实验表明，该方法成功发现了离散群，包括通过复数域上的流匹配发现的反射对称性。相比之前的工作，该方法能够发现更灵活类型的群，且需要更少的假设

Conclusion: LieFlow方法能够直接从数据中学习对称性，在对称性发现方面比先前方法更灵活，为理解物理系统和提升机器学习性能提供了有效工具

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [28] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: SOF框架从无动作视频中学习基于光流的潜在技能，实现高级规划并提升多任务和长时程任务性能


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以转化为低层动作，而潜在动作模型缺乏高级规划能力，需要弥合这一差距

Method: 提出SOF框架，通过光流中间表示学习潜在技能空间，该空间同时捕捉视频动态和机器人动作信息

Result: 在多任务和长时程设置中一致提升性能，证明能够直接从原始视觉数据获取和组合技能

Conclusion: SOF成功连接了视频生成模型和动作执行，通过光流表示实现了从视频到机器人技能的有效转化

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [29] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 提出ProbGLC方法，结合概率性和确定性地理定位模型，通过交叉视图图像匹配实现灾害快速响应，提高定位准确性和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧，灾害事件频发且强度增加，快速准确的灾害定位对应急响应和资源分配至关重要。现有方法在定位准确性和可解释性方面存在不足。

Method: 提出ProbGLC概率交叉视图地理定位方法，将概率性和确定性地理定位模型统一到一个框架中，通过不确定性量化和局部化评分增强模型可解释性，同时实现最先进的定位性能。

Result: 在两个灾害数据集（MultiIAN和SAGAINDisaster）上的实验显示，ProbGLC在1公里内准确率达到0.86，25公里内达到0.97，同时通过概率分布和局部化评分提供模型可解释性。

Conclusion: ProbGLC方法在灾害地理定位方面表现出优越性能，结合生成式交叉视图方法有望为灾害响应提供更好的位置感知能力，促进更快更好的灾害应急响应。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [30] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 该研究系统评估了使用强化学习（RL）训练大语言模型进行内容审核的方法，发现RL在需要复杂政策推理的任务上表现优异，且数据效率比监督微调高100倍，特别适用于专家标注稀缺的场景。


<details>
  <summary>Details</summary>
Motivation: 大规模内容审核是当今数字生态系统中最紧迫的挑战之一，需要持续评估数十亿用户和AI生成的内容是否存在政策违规。尽管大语言模型在政策驱动的审核方面显示出潜力，但在实际应用中训练这些系统以达到专家级准确性的挑战尚未充分探索，特别是在标签稀疏、政策定义不断演变、需要超越浅层模式匹配的细致推理的场景中。

Method: 采用全面的实证研究方法，系统评估多种RL训练方案和奖励塑造策略，包括可验证奖励和LLM作为评判框架，将通用语言模型转化为专门的政策对齐分类器。在三个真实世界内容审核任务上进行实验。

Result: 研究发现RL表现出类似Sigmoid函数的扩展行为：随着训练数据、rollouts和优化步骤的增加，性能平稳提升后逐渐饱和。RL在需要复杂政策推理的任务上显著提升性能，数据效率比监督微调高100倍，特别适用于专家标注稀缺或成本高昂的领域。

Conclusion: 该研究为工业级内容审核系统提供了可行的见解，证明强化学习是实现政策对齐内容分类的有效方法，特别是在数据稀缺和需要复杂推理的场景中具有显著优势。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [31] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: Reason2Decide是一个两阶段训练框架，通过解决暴露偏差和任务分离问题，在保持高预测准确性的同时生成与预测一致的医学解释。


<details>
  <summary>Details</summary>
Motivation: 当前临床决策支持系统面临关键挑战：在实现高预测准确性的同时，生成与预测一致的解释。现有方法存在暴露偏差问题，导致解释与预测不一致。

Method: 提出Reason2Decide两阶段训练框架：第一阶段训练模型生成解释；第二阶段联合训练标签预测和解释生成，应用计划采样从基于黄金标签逐渐过渡到基于模型预测。

Result: 在三个医学数据集上，Reason2Decide在预测准确性和解释保真度方面优于其他微调基准和某些零样本LLM。在分诊任务中，对LLM生成、护士撰写和护士后处理的解释都具有鲁棒性。

Conclusion: Reason2Decide仅使用LLM生成解释进行预训练就能取得良好效果，减少了对人工标注的依赖。该方法使用比当代基础模型小40倍的模型实现了这些优势，使临床推理在资源受限环境中更易部署。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [32] [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)
*Zhuo Yang,Yeyun chen,Jiaqing Xie,Ben Gao,Shuaike Shen,Wanhao Liu,Liujia Yang,Beilun Wang,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TL;DR: MolAct是一个基于智能体强化学习的分子设计框架，通过两阶段训练（先学习编辑能力，再优化性质）实现多步分子编辑与优化，在多个任务上超越现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 分子编辑和优化是多步骤问题，需要迭代改进性质同时保持化学有效性和结构相似性。现有方法缺乏将分子设计形式化为智能体强化学习问题的框架，无法有效结合推理、工具使用和分子优化。

Method: 提出MolAct框架，将分子设计形式化为顺序的、工具引导的决策过程。采用两阶段训练范式：第一阶段建立编辑能力，第二阶段重用学习到的编辑行为来优化性质。智能体通过多轮交互调用化学工具进行有效性检查、性质评估和相似性控制，并利用反馈优化后续编辑。

Result: 在分子编辑任务中，MolEditAgent-7B在添加、删除和替换编辑上分别达到100%、95%和98%的有效性，优于DeepSeek-R1等基线；MolEditAgent-3B接近Qwen3-32B-think等更大模型的性能。在分子优化任务中，MolOptAgent-7B在LogP上超越Claude 3.7等基线，在溶解度上保持竞争力，并在其他目标上保持平衡性能。

Conclusion: 将分子设计视为多步骤、工具增强的过程是实现可靠且可解释改进的关键。MolAct框架首次将分子设计形式化为智能体强化学习问题，展示了智能体学习交错推理、工具使用和分子优化的有效性。

Abstract: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

</details>


### [33] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种简单有效的方法：在时间序列数据token化前注入噪声，以提升冻结大语言模型在零样本时间序列预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常依赖微调专门模块来弥合时间序列数据与大语言模型预训练知识之间的差距，但一个更具挑战性的范式是直接使用完全冻结的、未经微调的大语言模型进行时间序列预测。这些冻结模型对输入数据的文本表示非常敏感，因为其参数无法适应分布变化。

Method: 提出在原始时间序列数据token化前注入噪声的策略，作为一种推理时增强方法。这种非侵入式干预迫使冻结的大语言模型基于鲁棒的时间模式而非表面的数值伪影进行外推。同时引入了两个全新时间序列数据集，完全排除大语言模型预训练数据污染的可能偏差。

Result: 通过理论分析和多个基准测试的实证验证，证明了噪声注入策略的有效性。在两个全新时间序列数据集上一致观察到性能提升，表明该方法能够改善冻结大语言模型在时间序列预测中的表现。

Conclusion: 噪声注入策略为直接利用冻结大语言模型进行时间序列预测提供了进一步的技术路径，通过推理时增强使模型关注鲁棒的时间模式而非数值细节，提升了零样本预测的稳定性。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [34] [A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers](https://arxiv.org/abs/2512.20161)
*Dhivya Dharshini Kannan,Anupam Trivedi,Dipti Srinivasan*

Main category: cs.AI

TL;DR: 本文开发了基于双向门控循环单元(BiGRU)的数据中心能效(PUE)预测模型，并与GRU模型进行性能比较，旨在通过预测PUE来优化数据中心能源管理。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗巨大且碳足迹显著，随着边缘计算和AI发展，其存储容量持续增长。提高能效是应对气候变化、降低能源成本、提升商业竞争力和促进IT与环境可持续发展的有效途径。优化数据中心能源管理对全球可持续发展至关重要。

Method: 使用基于EnergyPlus模拟的新加坡数据中心数据集(52,560个样本，117个特征)，采用递归特征消除与交叉验证(RFECV)算法选择最相关特征集，开发双向门控循环单元(BiGRU)的PUE预测模型，并与GRU模型进行对比。

Result: 通过均方误差(MSE)、平均绝对误差(MAE)和R平方指标评估模型性能，比较了优化后的BiGRU模型与GRU模型的预测效果，展示了BiGRU在PUE预测方面的性能优势。

Conclusion: BiGRU模型能够有效预测数据中心PUE，帮助理解各特征对能耗的影响，从而针对性地改进关键特征以提高能源效率，为数据中心能源管理优化提供技术支持。

Abstract: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.

</details>


### [35] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 比较人类与大型语言模型在数字游戏中的概念推理能力，发现人类更灵活地结合规则与相似性推理，而LLMs更依赖数学规则，且人类具有更好的少样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类与大型语言模型在概念推理任务中的泛化能力差异，探索两者在归纳偏置和推理策略上的根本区别。

Method: 使用数字游戏作为概念推理任务，以贝叶斯模型为分析框架，比较人类和LLMs的归纳偏置和推理策略。

Result: 贝叶斯模型能更好地捕捉人类行为：人类灵活推断基于规则和相似性的概念，而LLMs更依赖数学规则；人类能从单个示例实现少样本泛化，而LLMs需要更多样本。

Conclusion: 人类与LLMs在数学概念推理和泛化方面存在根本性差异，揭示了当前AI系统与人类认知能力的重要区别。

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [36] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: 本文提出PreSa方法，通过直接学习策略而非间接学习奖励和成本模型，解决离线偏好强化学习中安全策略学习的问题，在连续控制任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离线偏好强化学习（PbRL）虽然避免了复杂的奖励工程和直接人工标注，但在许多领域和任务中确保安全性仍然是一个关键挑战。现有基于人类反馈的安全RL方法先学习奖励和成本模型，再使用约束RL优化安全策略，但在长时域连续控制任务中，奖励和成本误差会累积，导致性能下降。

Method: 提出PreSa（偏好与安全对齐）方法：1）直接基于成对偏好（关于奖励）和轨迹段安全性的二元标签学习策略；2）将偏好学习模块与安全对齐结合到约束优化问题中；3）在拉格朗日框架下求解，直接学习奖励最大化且安全的策略，无需显式学习奖励和成本模型，也避免了约束RL。

Result: 在连续控制任务中，使用合成和真实人类反馈进行评估。实验表明，该方法成功学习了高奖励的安全策略，性能优于最先进的基线方法，甚至优于使用真实奖励和成本的离线安全RL方法。

Conclusion: PreSa方法通过直接学习策略而非间接学习奖励和成本模型，有效解决了离线偏好强化学习中的安全策略学习问题，在连续控制任务中实现了更好的安全性和性能平衡。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [37] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun,Kunlun Wu,Chuanjian Fu,Zeming Song,Langyong Shi,Zihe Xue,Bohan Jing,Ying Yang,Xiaomeng Gao,Aijia Li,Tianyu Guo,Huiying Li,Xueyuan Yang,Rongkai Liu,Xinyi He,Yuxi Wang,Yue Li,Mingyuan Liu,Yujie Lu,Hongzhao Xie,Shiyun Zhao,Bo Dai,Wei Wang,Tao Yuan,Song-Chun Zhu,Yujia Peng,Zhenliang Zhang*

Main category: cs.AI

TL;DR: TongSIM是一个高保真、通用型平台，用于训练和评估具身智能体，提供100多个多样化的室内场景和开放式户外城镇模拟，支持从低级导航到高级复合活动的广泛研究需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI特别是多模态大语言模型的发展，研究重点正从单模态文本处理转向更复杂的多模态和具身AI领域。现有仿真平台大多针对特定任务设计，缺乏一个能够支持从低级具身导航到高级复合活动（如多智能体社交模拟和人机协作）的通用训练环境。

Method: 引入TongSIM平台，提供100多个多样化的多房间室内场景和开放式、交互丰富的户外城镇模拟。平台具有定制场景、任务自适应保真度、多样化智能体类型和动态环境模拟等特性，提供全面的评估框架和基准测试。

Result: TongSIM作为一个统一平台，能够精确评估智能体的感知、认知、决策、人机协作以及空间和社会推理等能力，为研究人员提供灵活性和可扩展性。

Conclusion: TongSIM填补了通用具身智能训练环境的空白，加速了训练、评估和通用具身智能的发展进程，为研究人员提供了一个统一平台来推进该领域的研究。

Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

</details>


### [38] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai,Hang Gu,Teng Wang,Qianyu Cheng,Yifei Zheng,Zhiyong Qiu,Lei Gong,Wenqi Lou,Xuehai Zhou*

Main category: cs.AI

TL;DR: ActionFlow是一个针对边缘设备上视觉-语言-动作模型推理优化的系统级框架，通过跨请求流水线调度策略，在不重新训练模型的情况下将推理速度提升2.55倍，实现实时动态操作。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在边缘设备上推理延迟高（仅3-5Hz），无法满足机器人交互所需的20-30Hz实时控制要求，而现有优化方法通常需要重新训练或会降低模型精度。

Method: 提出了ActionFlow系统级推理框架，核心是跨请求流水线策略，将VLA推理重新定义为微请求的宏流水线。该策略智能地将内存受限的解码阶段与计算受限的预填充阶段在连续时间步上进行批处理，最大化硬件利用率。同时提出了跨请求状态打包前向操作符和统一KV环形缓冲区，将碎片化内存操作融合为高效的密集计算。

Result: 在OpenVLA-7B模型上实现了2.55倍的FPS提升，无需重新训练即可在边缘硬件上实现实时动态操作。

Conclusion: ActionFlow通过创新的系统级优化方法，有效解决了VLA模型在边缘设备上的推理延迟问题，为实时机器人控制提供了可行的解决方案。

Abstract: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

</details>


### [39] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Xuhua Duan,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Lei Chen,Wenjuan Tang,Biqiang Zhu,Xinggang Wang,Tao Sun,Wei Zhou,Dacheng Tao,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.AI

TL;DR: Janus-Pro-CXR是一个基于DeepSeek Janus-Pro开发的胸部X光解读系统，通过多中心前瞻性临床试验验证，在报告生成质量和临床关键发现检测方面优于现有模型，能显著提高放射科医生工作效率。


<details>
  <summary>Details</summary>
Motivation: 全球放射科医生短缺，特别是基层医疗中胸部X光工作量巨大，现有AI模型缺乏严格的临床前瞻性验证，需要开发可靠且经过临床验证的AI辅助诊断系统。

Method: 基于DeepSeek Janus-Pro模型开发Janus-Pro-CXR系统，采用轻量级架构和领域特定优化，通过多中心前瞻性临床试验(NCT07117266)进行严格验证，包括回顾性和前瞻性评估。

Result: 系统在自动报告生成方面优于包括ChatGPT 4o在内的最先进模型，能可靠检测六种临床关键放射学发现。前瞻性临床部署中，AI辅助显著提高报告质量评分，减少解读时间18.3%，54.3%的病例中专家更偏好AI辅助结果。

Conclusion: Janus-Pro-CXR通过轻量级架构和领域优化提高了诊断可靠性和工作流程效率，特别适合资源有限环境，模型架构和实现框架将开源以促进AI辅助放射学解决方案的临床转化。

Abstract: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [40] [Benchmarking LLMs for Predictive Applications in the Intensive Care Units](https://arxiv.org/abs/2512.20520)
*Chehak Malhotra,Mehak Gopal,Akshaya Devadiga,Pradeep Singh,Ridam Pal,Ritwik Kashyap,Tavpritesh Sethi*

Main category: cs.AI

TL;DR: 该研究比较了大型语言模型（LLMs）和特定领域语言模型（SLMs）在预测危重患者休克方面的性能，发现尽管GatorTron-Base表现最佳，但LLMs在预测临床事件方面并不比SLMs有固有优势。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自然语言处理领域的广泛应用，其在预测任务中的应用研究相对较少。及时预测休克可以促进早期干预，改善患者预后，因此需要评估LLMs在临床预测任务中的实际效果。

Method: 研究使用MIMIC III数据库中17,294个ICU住院记录，筛选出住院时间>24小时且休克指数>0.7的患者（正常组355例，异常组87例）。比较了GatorTron-Base（临床数据训练）、Llama 8B、Mistral 7B等LLMs与BioBERT、DocBERT、BioClinicalBERT、Word2Vec、Doc2Vec等SLMs的性能。使用焦点损失和交叉熵损失来解决类别不平衡问题。

Result: GatorTron-Base获得了最高的加权召回率（80.5%），但LLMs和SLMs之间的整体性能指标相当。这表明尽管LLMs在文本任务上表现出色，但在预测未来临床事件方面并不比SLMs有固有优势。

Conclusion: 为了获得有意义的临床结果，未来训练LLMs的努力应优先开发能够预测临床轨迹的模型，而不是专注于命名实体识别或表型分析等较简单的任务。

Abstract: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.

</details>


### [41] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan,Xiangren Wang,Hongyu Yuan,Qianli Xing*

Main category: cs.AI

TL;DR: 本文构建了首个大规模教师多模态情感分析数据集T-MED，并提出基于非对称注意力的多模态教师情感分析模型AAM-TSA，显著提升了教师情感识别的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 教师情感状态在教育场景中至关重要，深刻影响教学效果、学生参与度和学习成果。然而，现有研究由于教师情感表现的表演性质，往往无法准确捕捉教师情感，并且忽视了教学信息对情感表达的关键影响。

Method: 1. 构建T-MED数据集：采用人机协作标注流程，包含14,938个教师情感实例，来自250个真实课堂，涵盖11个学科，整合了多模态文本、音频、视频和教学信息。
2. 提出AAM-TSA模型：引入非对称注意力机制和分层门控单元，实现差异化的跨模态特征融合和精确的情感分类。

Result: 实验结果表明，AAM-TSA模型在T-MED数据集上的准确性和可解释性方面显著优于现有的最先进方法。

Conclusion: 本研究通过构建大规模教师多模态情感分析数据集和提出创新的非对称注意力模型，为教师情感分析提供了系统性的解决方案，有效解决了现有研究在捕捉教师真实情感和整合教学信息方面的不足。

Abstract: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

</details>


### [42] [Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent](https://arxiv.org/abs/2512.20586)
*Humza Nusrat,Luke Francisco,Bing Luo,Hassan Bagher-Ebadian,Joshua Kim,Karen Chin-Snyder,Salim Siddiqui,Mira Shah,Eric Mellon,Mohammad Ghassemi,Anthony Doemer,Benjamin Movsas,Kundan Thind*

Main category: cs.AI

TL;DR: 研究测试了思维链推理是否能改善立体定向放射外科的自动计划，开发了SAGE系统，发现推理模型在剂量学指标上与人工计划相当，同时降低耳蜗剂量，并展现出系统性的规划行为。


<details>
  <summary>Details</summary>
Motivation: 立体定向放射外科需要精确的剂量塑形，但黑盒AI系统由于透明度问题在临床应用中受限。研究旨在探索思维链推理是否能提高自动计划系统的透明度和临床可接受性。

Method: 开发了基于大语言模型的SAGE自动计划系统，在41例脑转移瘤患者的回顾性队列中测试。创建了两个变体：非推理模型和推理模型，比较它们在剂量学指标和规划行为上的差异。

Result: 推理模型在主要终点（PTV覆盖率、最大剂量、适形指数、梯度指数）上与人工计划相当（所有p>0.21），同时显著降低耳蜗剂量（p=0.022）。推理模型展现出457次前瞻性约束验证和609次权衡考量，而非推理模型几乎没有这些深思熟虑的过程。

Conclusion: 思维链推理不仅改善了自动计划的性能，还提供了可审计的优化轨迹，为透明自动规划提供了路径，有助于解决黑盒AI系统的临床接受度问题。

Abstract: Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.

</details>
