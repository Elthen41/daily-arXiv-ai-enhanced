{"id": "2602.16024", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.16024", "abs": "https://arxiv.org/abs/2602.16024", "authors": ["R. Kanda", "H. L. Blevec", "N. Onizawa", "M. Leonardon", "V. Gripon", "T. Hanyu"], "title": "Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware", "comment": null, "summary": "In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.", "AI": {"tldr": "\u63d0\u51fa\u5728PYNQ-Z1\u7b49\u5c0f\u578bFPGA SoC\u4e0a\u5b9e\u73b0\u4efb\u610f\u5b9a\u70b9\u4f4d\u5bbd\u5b9e\u65f6\u5c11\u6837\u672c\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7FINN\u6846\u67b6\u4f18\u5316\u5b9e\u73b0\u7ea62\u500d\u541e\u5410\u91cf\u63d0\u5347", "motivation": "\u4f20\u7edfTensil\u8bbe\u8ba1\u73af\u5883\u4ec5\u652f\u630116\u621632\u4f4d\u5b9a\u70b9\u4f4d\u5bbd\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u5b9e\u73b0\u7684\u7075\u6d3b\u6027\uff0c\u9700\u8981\u652f\u6301\u4efb\u610f\u4f4d\u5bbd\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5b9e\u65f6\u5c11\u6837\u672c\u5b66\u4e60", "method": "\u91c7\u7528FINN\u6846\u67b6\u652f\u6301\u4efb\u610f\u4f4d\u5bbd\u5b9e\u73b0\uff0c\u8fdb\u884c\u4e24\u9879\u5173\u952e\u4f18\u5316\uff1a1) \u4f18\u5316Transpose\u8282\u70b9\u89e3\u51b3\u6570\u636e\u683c\u5f0f\u4e0d\u5339\u914d\uff1b2) \u6dfb\u52a0\u6700\u7ec8reduce mean\u64cd\u4f5c\u8f6c\u6362\u4e3a\u5168\u5c40\u5e73\u5747\u6c60\u5316\u7684\u5904\u7406", "result": "\u5728\u4fdd\u6301\u4e0e\u4f20\u7edf\u5b9e\u73b0\u76f8\u540c\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u6210\u529f\u964d\u4f4e\u4f4d\u5bbd\uff0c\u5728CIFAR-10\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u7ea62\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u5c0f\u578bFPGA SoC\u4e0a\u5b9e\u73b0\u4efb\u610f\u5b9a\u70b9\u4f4d\u5bbd\u7684\u5b9e\u65f6\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u901a\u8fc7\u4f4d\u5bbd\u4f18\u5316\u663e\u8457\u63d0\u5347\u541e\u5410\u6027\u80fd"}}
{"id": "2602.16143", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.16143", "abs": "https://arxiv.org/abs/2602.16143", "authors": ["Naoya Onizawa", "Taiga Kubuta", "Duckgyu Shin", "Takahiro Hanyu"], "title": "Energy-Efficient p-Bit-Based Fully-Connected Quantum-Inspired Simulated Annealer with Dual BRAM Architecture", "comment": null, "summary": "Probabilistic bits (p-bits) offer an energy-efficient hardware abstraction for stochastic optimization; however, existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead. This paper presents an energy-efficient FPGA architecture for stochastic simulated quantum annealing (SSQA) that addresses these challenges. The proposed design combines a spin-serial and replica-parallel update schedule with a dual-BRAM delay-line architecture, enabling scalable support for fully connected Ising models while eliminating fan-out growth in logic resources. By exploiting SSQA, the architecture achieves fast convergence using only final replica states, significantly reducing memory requirements compared to conventional p-bit-based annealers. Implemented on a Xilinx ZC706 FPGA, the proposed system solves an 800-node MAX-CUT benchmark and achieves up to 50% reduction in energy consumption and over 90\\% reduction in logic resources compared with prior FPGA-based p-bit annealing architectures. These results demonstrate the practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u80fd\u6548\u578b\u968f\u673a\u6a21\u62df\u91cf\u5b50\u9000\u706b\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u65cb\u4e32\u884c\u548c\u526f\u672c\u5e76\u884c\u66f4\u65b0\u8c03\u5ea6\u7ed3\u5408\u53ccBRAM\u5ef6\u8fdf\u7ebf\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfp-bit\u9000\u706b\u5668\u5728\u5b8c\u5168\u8fde\u63a5\u56fe\u4e0a\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u7684\u80fd\u6548\u786c\u4ef6\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6982\u7387\u6bd4\u7279\u7684\u6a21\u62df\u9000\u706b\u52a0\u901f\u5668\u5b58\u5728\u6269\u5c55\u6027\u5dee\u548c\u5bf9\u5b8c\u5168\u8fde\u63a5\u56fe\u652f\u6301\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6247\u51fa\u589e\u957f\u548c\u5185\u5b58\u5f00\u9500\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u6548\u786c\u4ef6\u67b6\u6784\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u65cb\u4e32\u884c\u548c\u526f\u672c\u5e76\u884c\u66f4\u65b0\u8c03\u5ea6\u7b56\u7565\uff0c\u7ed3\u5408\u53ccBRAM\u5ef6\u8fdf\u7ebf\u67b6\u6784\uff0c\u652f\u6301\u5b8c\u5168\u8fde\u63a5\u7684\u4f0a\u8f9b\u6a21\u578b\u3002\u5229\u7528\u968f\u673a\u6a21\u62df\u91cf\u5b50\u9000\u706b\u6280\u672f\uff0c\u4ec5\u4f7f\u7528\u6700\u7ec8\u526f\u672c\u72b6\u6001\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u5728Xilinx ZC706 FPGA\u4e0a\u5b9e\u73b0\uff0c\u6210\u529f\u89e3\u51b3\u4e86800\u8282\u70b9\u7684MAX-CUT\u57fa\u51c6\u95ee\u9898\u3002\u76f8\u6bd4\u5148\u524d\u7684FPGA\u57fap-bit\u9000\u706b\u67b6\u6784\uff0c\u80fd\u8017\u964d\u4f4e\u8fbe50%\uff0c\u903b\u8f91\u8d44\u6e90\u51cf\u5c11\u8d85\u8fc790%\u3002", "conclusion": "\u8be5\u67b6\u6784\u8bc1\u660e\u4e86\u91cf\u5b50\u542f\u53d1\u7684p-bit\u9000\u706b\u786c\u4ef6\u5728\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u4e25\u683c\u7684\u80fd\u8017\u548c\u8d44\u6e90\u7ea6\u675f\u6761\u4ef6\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.16012", "categories": ["cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.16012", "abs": "https://arxiv.org/abs/2602.16012", "authors": ["Jieyi Bi", "Zhiguang Cao", "Jianan Zhou", "Wen Song", "Yaoxin Wu", "Jie Zhang", "Yining Ma", "Cathy Wu"], "title": "Towards Efficient Constraint Handling in Neural Solvers for Routing Problems", "comment": "Accepted by ICLR 2026", "summary": "Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.", "AI": {"tldr": "CaR\u662f\u4e00\u4e2a\u57fa\u4e8e\u663e\u5f0f\u5b66\u4e60\u53ef\u884c\u6027\u4f18\u5316\u7684\u795e\u7ecf\u8def\u7531\u6c42\u89e3\u5668\u7ea6\u675f\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u6307\u5bfc\u6784\u9020\u6a21\u5757\u751f\u6210\u9002\u5408\u8f7b\u91cf\u7ea7\u6539\u8fdb\u7684\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u89e3\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u786c\u7ea6\u675f\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6c42\u89e3\u5668\u5728\u7b80\u5355\u8def\u7531\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u7ea6\u675f\u4e0b\u8868\u73b0\u6709\u9650\u3002\u5f53\u524d\u7684\u7ea6\u675f\u5904\u7406\u65b9\u6848\uff08\u5982\u53ef\u884c\u6027\u63a9\u7801\u6216\u9690\u5f0f\u53ef\u884c\u6027\u611f\u77e5\uff09\u5bf9\u4e8e\u786c\u7ea6\u675f\u53ef\u80fd\u6548\u7387\u4f4e\u4e0b\u6216\u4e0d\u9002\u7528\u3002", "method": "\u63d0\u51faConstruct-and-Refine\u6846\u67b6\uff0c\u57fa\u4e8e\u663e\u5f0f\u5b66\u4e60\u53ef\u884c\u6027\u4f18\u5316\u3002\u8bbe\u8ba1\u8054\u5408\u8bad\u7ec3\u6846\u67b6\u6307\u5bfc\u6784\u9020\u6a21\u5757\u751f\u6210\u9002\u5408\u8f7b\u91cf\u7ea7\u6539\u8fdb\uff08\u598210\u6b65vs\u4e4b\u524d5000\u6b65\uff09\u7684\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u89e3\u3002\u9996\u6b21\u91c7\u7528\u6784\u9020-\u6539\u8fdb\u5171\u4eab\u8868\u793a\uff0c\u901a\u8fc7\u7edf\u4e00\u7f16\u7801\u5668\u5b9e\u73b0\u8de8\u8303\u5f0f\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5728\u5178\u578b\u786c\u8def\u7531\u7ea6\u675f\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCaR\u5728\u53ef\u884c\u6027\u3001\u89e3\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u7ecf\u5178\u548c\u795e\u7ecf\u6700\u5148\u8fdb\u7684\u6c42\u89e3\u5668\u3002", "conclusion": "CaR\u662f\u9996\u4e2a\u901a\u7528\u9ad8\u6548\u7684\u795e\u7ecf\u8def\u7531\u6c42\u89e3\u5668\u7ea6\u675f\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5b66\u4e60\u53ef\u884c\u6027\u4f18\u5316\u548c\u6784\u9020-\u6539\u8fdb\u5171\u4eab\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u7ea6\u675f\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.15995", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15995", "abs": "https://arxiv.org/abs/2602.15995", "authors": ["Xiang Fu", "Shiman Meng", "Weiping Zhang", "Luanzheng Guo", "Kento Sato", "Dong H. Ahn", "Ignacio Laguna", "Gregory L. Lee", "Martin Schulz"], "title": "Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs", "comment": "IEEE Cluster 2024", "summary": "After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \\toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u6280\u672f\uff08\u5206\u5e03\u5f0f\u65f6\u949fDC\u548c\u5206\u5e03\u5f0f\u7eaa\u5143DE\uff09\u6765\u89e3\u51b3OpenMP\u7a0b\u5e8f\u8bb0\u5f55\u548c\u56de\u653e\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u63d0\u53472-5\u500d\uff0c\u5e76\u80fd\u4e0eMPI\u56de\u653e\u5de5\u5177\u96c6\u6210\u3002", "motivation": "OpenMP\u4f5c\u4e3a\u6700\u6d41\u884c\u7684\u5171\u4eab\u5185\u5b58\u7f16\u7a0b\u6846\u67b6\uff0c\u5176\u975e\u786e\u5b9a\u6027\u6267\u884c\u7279\u6027\u4f7f\u5f97\u8c03\u8bd5\u548c\u6d4b\u8bd5\u53d8\u5f97\u56f0\u96be\u3002\u867d\u7136\u8bb0\u5f55\u548c\u786e\u5b9a\u6027\u56de\u653e\u7a0b\u5e8f\u6267\u884c\u662f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u7684\u5173\u952e\uff0c\u4f46\u53ef\u6269\u5c55\u5730\u56de\u653eOpenMP\u7a0b\u5e8f\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u6280\u672f\uff1a\u5206\u5e03\u5f0f\u65f6\u949f\uff08DC\uff09\u548c\u5206\u5e03\u5f0f\u7eaa\u5143\uff08DE\uff09\u8bb0\u5f55\u65b9\u6848\uff0c\u901a\u8fc7\u6d88\u9664OpenMP\u8bb0\u5f55\u548c\u56de\u653e\u4e2d\u8fc7\u591a\u7684\u7ebf\u7a0b\u540c\u6b65\u3002\u4f7f\u7528ReOMP\u5de5\u5177\u5b9e\u73b0DC\u548cDE\u8bb0\u5f55\uff0c\u5e76\u4e0e\u73b0\u6709MPI\u56de\u653e\u5de5\u5177ReMPI\u96c6\u6210\u3002", "result": "\u5728\u4ee3\u8868\u6027HPC\u5e94\u7528\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5728\u6bcf\u4e2a\u5171\u4eab\u5185\u5b58\u8bbf\u95ee\u4e0a\u540c\u6b65\u7684\u65b9\u6cd5\u6548\u7387\u9ad82-5\u500d\u3002\u80fd\u591f\u8f7b\u677e\u4e0eMPI\u7ea7\u56de\u653e\u5de5\u5177\u7ed3\u5408\uff0c\u56de\u653e\u590d\u6742\u7684MPI+OpenMP\u5e94\u7528\uff0c\u4ec5\u4ea7\u751f\u4e0eMPI\u89c4\u6a21\u65e0\u5173\u7684\u5c0f\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684DC\u548cDE\u8bb0\u5f55\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86OpenMP\u7a0b\u5e8f\u53ef\u6269\u5c55\u8bb0\u5f55\u548c\u56de\u653e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u5e76\u80fd\u4e0eMPI\u56de\u653e\u5de5\u5177\u65e0\u7f1d\u96c6\u6210\uff0c\u4e3a\u8c03\u8bd5\u590d\u6742\u7684\u6df7\u5408\u5e76\u884c\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.16010", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16010", "abs": "https://arxiv.org/abs/2602.16010", "authors": ["Xin Huang", "Weiping Zhang", "Shiman Meng", "Wubiao Xu", "Xiang Fu", "Luanzheng Guo", "Kento Sato"], "title": "Scrutinizing Variables for Checkpoint Using Automatic Differentiation", "comment": "The Second Workshop on Enabling Predictive Science with Optimization and Uncertainty Quantification in HPC (EPSOUQ-HPC) in conjunction with SC24", "summary": "Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u52a8\u5fae\u5206(AD)\u7684\u68c0\u67e5\u70b9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u53d8\u91cf\u4e2d\u6bcf\u4e2a\u5143\u7d20\u5bf9\u8ba1\u7b97\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u5e76\u6392\u9664\u4e0d\u5173\u952e\u5143\u7d20\uff0c\u51cf\u5c11\u68c0\u67e5\u70b9\u5b58\u50a8\u5f00\u9500", "motivation": "\u4f20\u7edf\u68c0\u67e5\u70b9/\u91cd\u542f(C/R)\u673a\u5236\u4fdd\u5b58\u7a0b\u5e8f\u8fd0\u884c\u72b6\u6001\u65f6\u6d88\u8017\u5927\u91cf\u7cfb\u7edf\u8d44\u6e90\uff0c\u4f46HPC\u5e94\u7528\u4e2d\u5e76\u975e\u6240\u6709\u6570\u636e\u90fd\u53c2\u4e0e\u8ba1\u7b97\uff0c\u6392\u9664\u4e0d\u4f7f\u7528\u7684\u6570\u636e\u53ef\u63d0\u9ad8\u5b58\u50a8\u548c\u8ba1\u7b97\u6548\u7387", "method": "\u5229\u7528\u81ea\u52a8\u5fae\u5206(AD)\u5de5\u5177\u5206\u6790\u53d8\u91cf\u4e2d\u6bcf\u4e2a\u5143\u7d20\u5bf9\u5e94\u7528\u7a0b\u5e8f\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u5173\u952e/\u975e\u5173\u952e\u5143\u7d20\uff0c\u5c06\u975e\u5173\u952e\u5143\u7d20\u4ece\u68c0\u67e5\u70b9\u4e2d\u6392\u9664", "result": "\u5728NAS Parallel Benchmark(NPB)\u5957\u4ef6\u76848\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0c\u6210\u529f\u53ef\u89c6\u5316\u53d8\u91cf\u4e2d\u5173\u952e/\u975e\u5173\u952e\u5143\u7d20\u533a\u57df\uff0c\u5b58\u50a8\u8282\u7701\u6700\u9ad8\u8fbe20%", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eAD\u7684\u68c0\u67e5\u70b9\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5e76\u6392\u9664\u4e0d\u5f71\u54cd\u8f93\u51fa\u7684\u6570\u636e\u5143\u7d20\uff0c\u663e\u8457\u51cf\u5c11\u68c0\u67e5\u70b9\u5b58\u50a8\u5f00\u9500\uff0c\u4e14\u5173\u952e/\u975e\u5173\u952e\u5143\u7d20\u7684\u5206\u5e03\u6a21\u5f0f\u4e0e\u7b97\u6cd5\u7269\u7406\u903b\u8f91\u76f8\u7b26"}}
{"id": "2602.16039", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16039", "abs": "https://arxiv.org/abs/2602.16039", "authors": ["Hang Li", "Kaiqi Yang", "Xianxuan Long", "Fedor Filippov", "Yucheng Chu", "Yasemin Copur-Gencturk", "Peng He", "Cory Miller", "Namsoo Shin", "Joseph Krajcik", "Hui Liu", "Jiliang Tang"], "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment", "comment": null, "summary": "The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u81ea\u52a8\u8bc4\u4f30\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\uff0c\u4f46\u5176\u56fa\u6709\u7684\u6982\u7387\u7279\u6027\u5f15\u5165\u4e86\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\u3002\u8bc4\u4f30\u7ed3\u679c\u5bf9\u540e\u7eed\u6559\u5b66\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u7684\u6559\u5b66\u5e72\u9884\uff0c\u5f71\u54cd\u5b66\u751f\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5728\u591a\u8bc4\u4f30\u6570\u636e\u96c6\u3001\u4e0d\u540cLLM\u5bb6\u65cf\u548c\u751f\u6210\u63a7\u5236\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u5bf9\u5e7f\u6cdb\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8868\u5f81LLM\u5728\u8bc4\u5206\u573a\u666f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5206\u6790\u4e86\u6a21\u578b\u5bb6\u65cf\u3001\u8bc4\u4f30\u4efb\u52a1\u548c\u89e3\u7801\u7b56\u7565\u7b49\u5173\u952e\u56e0\u7d20\u5bf9\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86LLM\u5728\u81ea\u52a8\u8bc4\u5206\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3LLM\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bc4\u5206\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16100", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16100", "abs": "https://arxiv.org/abs/2602.16100", "authors": ["Zijie Su", "Muhammed Tawfiqul Islam", "Mohammad Goudarzi", "Adel N. Toosi"], "title": "LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u914d\u7f6e\u65b9\u6cd5\uff0c\u53ef\u5728\u670d\u52a1\u5668\u73af\u5883\u4e0b\u5728\u7ebf\u8c03\u6574LLM\u63a8\u7406\u7684\u6d41\u6c34\u7ebf\u914d\u7f6e\uff0c\u4ee5\u6700\u5c0f\u5316\u670d\u52a1\u4e2d\u65ad\u65f6\u95f4\u548c\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5728\u6709\u9650GPU\u8d44\u6e90\u4e0b\u9ad8\u6548\u670d\u52a1LLM\u63a8\u7406\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u670d\u52a1\u5668\u8ba1\u7b97\u8303\u5f0f\u88ab\u5e94\u7528\u4e8eLLM\u670d\u52a1\u4ee5\u6700\u5927\u5316\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u9ad8\u5ea6\u591a\u6837\u5316\uff0c\u73b0\u4ee3GPU\u96c6\u7fa4\u672c\u8d28\u4e0a\u662f\u5f02\u6784\u7684\uff0c\u9700\u8981\u5728\u7ebf\u52a8\u6001\u8c03\u6574\u90e8\u7f72\u914d\u7f6e\u4ee5\u9002\u5e94\u670d\u52a1\u5668\u73af\u5883\u7684\u5f39\u6027\u548c\u52a8\u6001\u7279\u6027\u3002\u540c\u65f6\uff0c\u7531\u4e8eLLM\u63a8\u7406\u7684\u6709\u72b6\u6001\u6027\u548c\u6a21\u578b\u53c2\u6570\u7684\u5de8\u5927\u89c4\u6a21\uff0c\u5b9e\u73b0\u5728\u7ebf\u91cd\u914d\u7f6e\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u914d\u7f6e\u65b9\u6cd5\uff0c\u652f\u6301\u5728\u7ebf\u8c03\u6574\u6d41\u6c34\u7ebf\u914d\u7f6e\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u670d\u52a1\u4e2d\u65ad\u65f6\u95f4\u548c\u6027\u80fd\u4e0b\u964d\u3002\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u6700\u4f18\u7684\u6d41\u6c34\u7ebf\u914d\u7f6e\u3002", "result": "\u5728\u5f02\u6784GPU\u5e73\u53f0\uff08\u5305\u62ecNVIDIA A100\u548cL40s\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fc1\u79fb\u673a\u5236\u5bfc\u81f4\u7684\u670d\u52a1\u4e2d\u65ad\u65f6\u95f4\u5c0f\u4e8e50\u6beb\u79d2\uff0c\u540c\u65f6\u5bf9\u9996\u8bcd\u751f\u6210\u65f6\u95f4\uff08TTFT\uff09\u548c\u6bcf\u8bcd\u8f93\u51fa\u65f6\u95f4\uff08TPOT\uff09\u5f15\u5165\u7684\u5f00\u9500\u4f4e\u4e8e10%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u670d\u52a1\u5668\u73af\u5883\u4e0bLLM\u63a8\u7406\u7684\u52a8\u6001\u914d\u7f6e\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4f4e\u670d\u52a1\u4e2d\u65ad\u548c\u4f4e\u6027\u80fd\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u53d8\u5316\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u9002\u5e94\u4f18\u5316\u3002"}}
{"id": "2602.16222", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.16222", "abs": "https://arxiv.org/abs/2602.16222", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Robin Vacus"], "title": "Near-optimal population protocols on bounded-degree trees", "comment": "37 pages, 7 figures", "summary": "We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.\n  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.\n  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \\log n)$ steps on directed trees.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7a00\u758f\u4ea4\u4e92\u56fe\u4e2d\u7fa4\u4f53\u534f\u8bae\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\uff0c\u53d1\u73b0\u5728\u6709\u754c\u5ea6\u6811\u4e0a\uff0c\u9886\u5bfc\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u4e0d\u5b58\u5728\u663e\u8457\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u5e38\u6570\u7a7a\u95f4\u534f\u8bae\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u7a33\u5b9a\u65f6\u95f4\u3002", "motivation": "\u5728\u5b8c\u5168\u4ea4\u4e92\u56fe\u4e2d\uff0c\u9886\u5bfc\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u7684\u6700\u4f18\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u73b0\u6709\u4e0b\u754c\u6280\u672f\u65e0\u6cd5\u6269\u5c55\u5230\u9ad8\u5ea6\u7a00\u758f\u56fe\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6709\u754c\u5ea6\u6811\u7b49\u7a00\u758f\u56fe\u65cf\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u7a7a\u95f4-\u65f6\u95f4\u590d\u6742\u5ea6\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u534f\u8bae\uff1a1\uff09\u9002\u7528\u4e8e\u4e00\u822c\u4ea4\u4e92\u56fe\u7684\u5feb\u901f\u81ea\u7a33\u5b9a2\u8df3\u7740\u8272\u534f\u8bae\uff0c\u4f7f\u7528\u968f\u673a\u6f02\u79fb\u8bba\u8bc1\u5206\u6790\u7a33\u5b9a\u65f6\u95f4\uff1b2\uff09\u81ea\u7a33\u5b9a\u6811\u5b9a\u5411\u7b97\u6cd5\uff0c\u5728\u4efb\u4f55\u6811\u4e0a\u4ee5\u6700\u4f18\u65f6\u95f4\u6784\u5efa\u6709\u6839\u6811\u3002\u5229\u7528\u8fd9\u4e9b\u534f\u8bae\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e3a\u6709\u5411\u6811\u8bbe\u8ba1\u7684\u7b80\u5355\u5e38\u6570\u72b6\u6001\u534f\u8bae\u5feb\u901f\u89e3\u51b3\u9886\u5bfc\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u3002", "result": "\u5728\u6709\u754c\u5ea6\u6811\u4e0a\uff0c\u9886\u5bfc\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u4e0d\u5b58\u5728\u663e\u8457\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u5e38\u6570\u7a7a\u95f4\u534f\u8bae\uff0c\u5176\u6700\u574f\u60c5\u51b5\u671f\u671b\u7a33\u5b9a\u65f6\u95f4\u63a5\u8fd1\u6700\u4f18\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u7ebf\u6027\u52a0\u901f\u3002\u4f8b\u5982\uff0c\u5728\u6709\u5411\u6811\u4e0a\u4f7f\u7528\"\u5b9a\u5411\"\u6e6e\u706d\u52a8\u529b\u5b66\u53ef\u4ee5\u5728O(n\u00b2 log n)\u6b65\u5185\u89e3\u51b3\u7cbe\u786e\u591a\u6570\u95ee\u9898\u3002", "conclusion": "\u4e0e\u5b8c\u5168\u56fe\u4e0d\u540c\uff0c\u6709\u754c\u5ea6\u6811\u4e0a\u7684\u7fa4\u4f53\u534f\u8bae\u5728\u9886\u5bfc\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u4e0a\u4e0d\u8868\u73b0\u51fa\u663e\u8457\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\u3002\u63d0\u51fa\u7684\u65b0\u534f\u8bae\u4e3a\u7a00\u758f\u4ea4\u4e92\u56fe\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5e38\u6570\u7a7a\u95f4\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u6280\u672f\u5177\u6709\u72ec\u7acb\u7684\u7814\u7a76\u4ef7\u503c\u3002"}}
{"id": "2602.16066", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16066", "abs": "https://arxiv.org/abs/2602.16066", "authors": ["Martin Klissarov", "Jonathan Cook", "Diego Antognini", "Hao Sun", "Jingling Li", "Natasha Jaques", "Claudiu Musat", "Edward Grefenstette"], "title": "Improving Interactive In-Context Learning from Natural Language Feedback", "comment": null, "summary": "Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u4ea4\u4e92\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u89c6\u4e3a\u53ef\u8bad\u7ec3\u6280\u80fd\u800c\u975e\u6d8c\u73b0\u5c5e\u6027\uff0c\u901a\u8fc7\u5c06\u5355\u8f6e\u53ef\u9a8c\u8bc1\u4efb\u52a1\u8f6c\u5316\u4e3a\u591a\u8f6e\u6559\u5b66\u4e92\u52a8\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u4ece\u8bed\u8a00\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e3b\u8981\u4f9d\u8d56\u5efa\u6a21\u9759\u6001\u8bed\u6599\u5e93\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5b66\u4e60\u4e2d\u57fa\u4e8e\u7ea0\u6b63\u53cd\u9988\u52a8\u6001\u8c03\u6574\u601d\u7ef4\u8fc7\u7a0b\u7684\u91cd\u8981\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u3002\u73b0\u6709\u6a21\u578b\u5728\u6574\u5408\u7ea0\u6b63\u53cd\u9988\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u63d0\u5347\u4ea4\u4e92\u5f0f\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5c06\u5355\u8f6e\u53ef\u9a8c\u8bc1\u4efb\u52a1\u8f6c\u5316\u4e3a\u7531\u4fe1\u606f\u4e0d\u5bf9\u79f0\u9a71\u52a8\u7684\u591a\u8f6e\u6559\u5b66\u4e92\u52a8\u3002\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u6559\u5e08\u7684\u6279\u8bc4\uff0c\u6709\u6548\u5efa\u6a21\u53cd\u9988\u73af\u5883\uff0c\u5c06\u5916\u90e8\u4fe1\u53f7\u8f6c\u5316\u4e3a\u5185\u90e8\u80fd\u529b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u6ca1\u6709\u6559\u5e08\u7684\u60c5\u51b5\u4e0b\u81ea\u6211\u7ea0\u6b63\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bad\u7ec3\u7684\u5c0f\u6a21\u578b\u5728\u591a\u8f6e\u6027\u80fd\u4e0a\u63a5\u8fd1\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u5927\u6a21\u578b\u3002\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u8fdb\u884c\u4ea4\u4e92\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u7f16\u7a0b\u3001\u8c1c\u9898\u548c\u8ff7\u5bab\u5bfc\u822a\u7b49\u4e0d\u540c\u9886\u57df\u3002\u6a21\u578b\u5c55\u73b0\u51fa\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u53ef\u5851\u6027\uff0c\u5e76\u80fd\u901a\u8fc7\u81ea\u6211\u7ea0\u6b63\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u8bad\u7ec3\u6280\u80fd\u800c\u975e\u6d8c\u73b0\u5c5e\u6027\u6765\u57f9\u517b\u3002\u901a\u8fc7\u5c06\u5916\u90e8\u53cd\u9988\u4fe1\u53f7\u8f6c\u5316\u4e3a\u5185\u90e8\u80fd\u529b\uff0c\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u81ea\u6211\u7ea0\u6b63\u548c\u81ea\u6211\u6539\u8fdb\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u4e00\u8def\u5f84\u3002"}}
{"id": "2602.16156", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.16156", "abs": "https://arxiv.org/abs/2602.16156", "authors": ["Rohit Chatterjee", "Yunqi Li", "Prashant Nalini Vasudevan"], "title": "Weak Zero-Knowledge and One-Way Functions", "comment": null, "summary": "We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $\u03b5_c$, $\u03b5_s$, and $\u03b5_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:\n  1. If all languages in NP have NIZK proofs or arguments satisfying $ \u03b5_c+\u03b5_s+ \u03b5_z < 1 $, then One-Way Functions (OWFs) exist.\n  This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $\u03b5_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ \u03b5_c+\\sqrt{\u03b5_s}+\u03b5_z < 1 $ [Chakraborty et al., CRYPTO 2025].\n  2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ \u03b5_c+\u03b5_s+(2k-1).\u03b5_z < 1 $, then OWFs exist.\n  3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ \u03b5_c+\u03b5_s+k.\u03b5_z < 1 $, then infinitely-often OWFs exist.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f31\u96f6\u77e5\u8bc6\u534f\u8bae\u5bf9\u6700\u574f\u60c5\u51b5\u56f0\u96be\u8bed\u8a00\u5b58\u5728\u6027\u7684\u5f71\u54cd\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u5173\u4e8e\u8bef\u5dee\u53c2\u6570\u7684\u6761\u4ef6\uff0c\u5efa\u7acb\u4e86\u4ece\u5f31\u96f6\u77e5\u8bc6\u534f\u8bae\u5230\u5355\u5411\u51fd\u6570\u5b58\u5728\u7684\u8054\u7cfb\u3002", "motivation": "\u7814\u7a76\u5177\u6709\u975e\u53ef\u5ffd\u7565\u8bef\u5dee\uff08\u5b8c\u6574\u6027\u3001\u53ef\u9760\u6027\u548c\u96f6\u77e5\u8bc6\u8bef\u5dee\uff09\u7684\u5f31\u96f6\u77e5\u8bc6\u534f\u8bae\u7684\u5b58\u5728\u6027\u5bf9\u5bc6\u7801\u5b66\u57fa\u7840\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u8fd9\u4e9b\u534f\u8bae\u4e0e\u5355\u5411\u51fd\u6570\u5b58\u5728\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u5728NP\u4e2d\u5b58\u5728\u6700\u574f\u60c5\u51b5\u56f0\u96be\u8bed\u8a00\u7684\u5047\u8bbe\u4e0b\uff0c\u5206\u6790\u4e0d\u540c\u8bef\u5dee\u53c2\u6570\u7ec4\u5408\u6761\u4ef6\u4e0b\uff08\u03b5_c+\u03b5_s+\u03b5_z < 1\u7b49\uff09\uff0c\u5f31\u96f6\u77e5\u8bc6\u534f\u8bae\u7684\u5b58\u5728\u6027\u5982\u4f55\u8574\u542b\u5355\u5411\u51fd\u6570\u7684\u5b58\u5728\u6027\u3002", "result": "1. \u5982\u679c\u6240\u6709NP\u8bed\u8a00\u90fd\u6709\u6ee1\u8db3\u03b5_c+\u03b5_s+\u03b5_z < 1\u7684NIZK\u8bc1\u660e/\u8bba\u8bc1\uff0c\u5219\u5355\u5411\u51fd\u6570\u5b58\u5728\uff1b2. \u5bf9\u4e8ek\u8f6e\u516c\u5f00\u63b7\u5e01ZK\u534f\u8bae\uff0c\u82e5\u6ee1\u8db3\u03b5_c+\u03b5_s+(2k-1)\u00b7\u03b5_z < 1\uff0c\u5219\u5355\u5411\u51fd\u6570\u5b58\u5728\uff1b3. \u5bf9\u4e8e\u5e38\u6570k\u8f6e\u534f\u8bae\uff0c\u82e5\u6ee1\u8db3\u03b5_c+\u03b5_s+k\u00b7\u03b5_z < 1\uff0c\u5219\u65e0\u9650\u9891\u7e41\u5355\u5411\u51fd\u6570\u5b58\u5728\u3002", "conclusion": "\u5f31\u96f6\u77e5\u8bc6\u534f\u8bae\u7684\u5b58\u5728\u6027\u4e0e\u5355\u5411\u51fd\u6570\u5b58\u5728\u6027\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u8054\u7cfb\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u66f4\u4e25\u683c\u7684\u6761\u4ef6\uff08\u03b5_c+\u221a\u03b5_s+\u03b5_z < 1\uff09\uff0c\u4e3a\u5bc6\u7801\u5b66\u57fa\u7840\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.16233", "categories": ["cs.DC", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.16233", "abs": "https://arxiv.org/abs/2602.16233", "authors": ["Prabhjot Singh", "Adel N. Toosi", "Rajkumar Buyya"], "title": "DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting", "comment": null, "summary": "Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7535\u8def\u5207\u5272\u7684\u5206\u5e03\u5f0f\u6267\u884c\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6d4b\u91cf\u5728\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u7aef\u5230\u7aef\u5f00\u9500\uff0c\u53d1\u73b0\u91cd\u6784\u9636\u6bb5\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u4f46\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5728\u5339\u914d\u8bad\u7ec3\u9884\u7b97\u4e0b\u5f97\u4ee5\u4fdd\u6301\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7535\u8def\u5207\u5272\u5728\u5b50\u7535\u8def\u6570\u91cf\u548c\u91c7\u6837\u590d\u6742\u5ea6\u65b9\u9762\u7684\u5f00\u9500\uff0c\u4f46\u7f3a\u4e4f\u4ece\u7cfb\u7edf\u89d2\u5ea6\u8861\u91cf\u5176\u5bf9\u8fed\u4ee3\u5f0f\u3001\u4f30\u8ba1\u5668\u9a71\u52a8\u7684\u8bad\u7ec3\u6d41\u7a0b\u7684\u7aef\u5230\u7aef\u5f71\u54cd\u3002\u9700\u8981\u91cf\u5316\u5207\u5272\u5728\u771f\u5b9e\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u5b9e\u9645\u5f00\u9500\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5207\u5272\u611f\u77e5\u7684\u4f30\u8ba1\u5668\u6267\u884c\u6d41\u6c34\u7ebf\uff0c\u5c06\u7535\u8def\u5207\u5272\u89c6\u4e3a\u5206\u9636\u6bb5\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5c06\u6bcf\u4e2a\u4f30\u8ba1\u5668\u67e5\u8be2\u5206\u89e3\u4e3a\u5206\u533a\u3001\u5b50\u5b9e\u9a8c\u751f\u6210\u3001\u5e76\u884c\u6267\u884c\u548c\u7ecf\u5178\u91cd\u6784\u56db\u4e2a\u9636\u6bb5\u3002\u4f7f\u7528\u4e24\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff08Iris\u548cMNIST\uff09\u7684\u8fd0\u884c\u65f6\u65e5\u5fd7\u548c\u5b66\u4e60\u7ed3\u679c\u8fdb\u884c\u6d4b\u91cf\u3002", "result": "\u6d4b\u91cf\u663e\u793a\uff1a1\uff09\u5207\u5272\u5f15\u5165\u663e\u8457\u7684\u7aef\u5230\u7aef\u5f00\u9500\uff0c\u4e14\u968f\u5207\u5272\u6570\u91cf\u589e\u52a0\u800c\u589e\u957f\uff1b2\uff09\u91cd\u6784\u9636\u6bb5\u5360\u6bcf\u4e2a\u67e5\u8be2\u65f6\u95f4\u7684\u4e3b\u5bfc\u90e8\u5206\uff0c\u9650\u5236\u4e86\u5e76\u884c\u5316\u5e26\u6765\u7684\u52a0\u901f\u6f5c\u529b\uff1b3\uff09\u5c3d\u7ba1\u5b58\u5728\u7cfb\u7edf\u5f00\u9500\uff0c\u5728\u5339\u914d\u8bad\u7ec3\u9884\u7b97\u4e0b\u6d4b\u8bd5\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5f97\u4ee5\u4fdd\u6301\uff0c\u67d0\u4e9b\u5207\u5272\u914d\u7f6e\u4e0b\u751a\u81f3\u89c2\u5bdf\u5230\u6539\u8fdb\u3002", "conclusion": "\u7535\u8def\u5207\u5272\u5728\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u5173\u952e\u5728\u4e8e\u51cf\u5c11\u548c\u91cd\u53e0\u91cd\u6784\u9636\u6bb5\uff0c\u4ee5\u53ca\u8003\u8651\u5c4f\u969c\u4e3b\u5bfc\u7684\u5173\u952e\u8def\u5f84\u7684\u8c03\u5ea6\u7b56\u7565\u3002\u867d\u7136\u7cfb\u7edf\u5f00\u9500\u663e\u8457\uff0c\u4f46\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u53ef\u4ee5\u5f97\u5230\u4fdd\u6301\u3002"}}
{"id": "2602.16105", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16105", "abs": "https://arxiv.org/abs/2602.16105", "authors": ["Thinh Hung Truong", "Jey Han Lau", "Jianzhong Qi"], "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench", "AI": {"tldr": "GPSBench\u662f\u4e00\u4e2a\u5305\u542b57,800\u4e2a\u6837\u672c\u3001\u6db5\u76d617\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728GPS\u5750\u6807\u548c\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5730\u7406\u63a8\u7406\u65b9\u9762\u6bd4\u51e0\u4f55\u8ba1\u7b97\u66f4\u53ef\u9760\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u5e94\u7528\uff08\u5982\u5bfc\u822a\u3001\u673a\u5668\u4eba\u3001\u5730\u56fe\uff09\uff0c\u5f3a\u5927\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u578b\u5728GPS\u5750\u6807\u548c\u771f\u5b9e\u4e16\u754c\u5730\u7406\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165GPSBench\u6570\u636e\u96c6\uff0c\u5305\u542b\u51e0\u4f55\u5750\u6807\u64cd\u4f5c\uff08\u8ddd\u79bb\u548c\u65b9\u4f4d\u8ba1\u7b97\uff09\u4ee5\u53ca\u5750\u6807\u4e0e\u4e16\u754c\u77e5\u8bc6\u7ed3\u5408\u63a8\u7406\u7684\u4efb\u52a1\u3002\u8bc4\u4f30\u4e8614\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u5185\u5728\u6a21\u578b\u80fd\u529b\u800c\u975e\u5de5\u5177\u4f7f\u7528\u3002", "result": "GPS\u63a8\u7406\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e0d\u540c\u4efb\u52a1\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5730\u7406\u63a8\u7406\u65b9\u9762\u901a\u5e38\u6bd4\u51e0\u4f55\u8ba1\u7b97\u66f4\u53ef\u9760\u3002\u5730\u7406\u77e5\u8bc6\u5448\u5c42\u6b21\u6027\u9000\u5316\uff0c\u56fd\u5bb6\u5c42\u9762\u8868\u73b0\u5f3a\u4f46\u57ce\u5e02\u5c42\u9762\u5b9a\u4f4d\u5f31\u3002\u5bf9\u5750\u6807\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8868\u660e\u6a21\u578b\u5177\u6709\u771f\u6b63\u7684\u5750\u6807\u7406\u89e3\u800c\u975e\u5355\u7eaf\u8bb0\u5fc6\u3002", "conclusion": "GPS\u5750\u6807\u589e\u5f3a\u53ef\u4ee5\u6539\u5584\u4e0b\u6e38\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\uff0c\u5fae\u8c03\u4f1a\u5728\u51e0\u4f55\u8ba1\u7b97\u6536\u76ca\u548c\u4e16\u754c\u77e5\u8bc6\u9000\u5316\u4e4b\u95f4\u4ea7\u751f\u6743\u8861\u3002\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.16362", "categories": ["cs.DC", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16362", "abs": "https://arxiv.org/abs/2602.16362", "authors": ["MHD Saria Allahham", "Hossam S. Hassanein"], "title": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability", "comment": null, "summary": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\uff08XEC\uff09\u7684\u8ba1\u7b97\u53ef\u9760\u6027\u5206\u6790\u6846\u67b6\uff0c\u91cf\u5316\u8bbe\u5907\u6216\u8bbe\u5907\u96c6\u5408\u6ee1\u8db3\u6d41\u5a92\u4f53\u670d\u52a1\u5904\u7406\u901f\u7387\u8981\u6c42\u7684\u6982\u7387\uff0c\u89e3\u51b3\u4e86\u6d88\u8d39\u8005\u8bbe\u5907\u8ba1\u7b97\u53ef\u7528\u6027\u6ce2\u52a8\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u6d88\u8d39\u8005\u8bbe\u5907\u5728\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u6ce2\u52a8\u7684\u8ba1\u7b97\u53ef\u7528\u6027\uff0c\u8fd9\u6e90\u4e8e\u7ade\u4e89\u5e94\u7528\u7a0b\u5e8f\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u4f7f\u7528\u6a21\u5f0f\u3002\u8fd9\u79cd\u6ce2\u52a8\u6027\u5e26\u6765\u4e86\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u5982\u4f55\u91cf\u5316\u5355\u4e2a\u8bbe\u5907\u6216\u8bbe\u5907\u96c6\u5408\u80fd\u591f\u7ef4\u6301\u6d41\u5a92\u4f53\u670d\u52a1\u6240\u9700\u5904\u7406\u901f\u7387\u7684\u6982\u7387\uff1f", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u53ef\u9760\u6027\u5206\u6790\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u5728\u7279\u5b9a\u670d\u52a1\u8d28\u91cf\u9608\u503c\u4e0b\u77ac\u65f6\u5bb9\u91cf\u6ee1\u8db3\u9700\u6c42\u7684\u6982\u7387\u3002\u5728\u4e24\u79cd\u4fe1\u606f\u673a\u5236\u4e0b\u63a8\u5bfc\u95ed\u5f0f\u53ef\u9760\u6027\u8868\u8fbe\u5f0f\uff1a\u6700\u5c0f\u4fe1\u606f\u673a\u5236\uff08\u4ec5\u9700\u58f0\u660e\u7684\u64cd\u4f5c\u8fb9\u754c\uff09\u548c\u5386\u53f2\u6570\u636e\u673a\u5236\uff08\u901a\u8fc7\u8fc7\u53bb\u89c2\u6d4b\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6765\u7cbe\u5316\u4f30\u8ba1\uff09\u3002\u6846\u67b6\u6269\u5c55\u5230\u591a\u8bbe\u5907\u90e8\u7f72\uff0c\u4e3a\u4e32\u884c\u3001\u5e76\u884c\u548c\u5206\u533a\u5de5\u4f5c\u8d1f\u8f7d\u914d\u7f6e\u63d0\u4f9b\u53ef\u9760\u6027\u8868\u8fbe\u5f0f\u3002\u63a8\u5bfc\u4e86\u6700\u4f18\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u89c4\u5219\u548c\u8bbe\u5907\u9009\u62e9\u7684\u5206\u6790\u8fb9\u754c\u3002", "result": "\u4f7f\u7528YOLO11m\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u4f5c\u4e3a\u4ee3\u8868\u6027\u5206\u5e03\u5f0f\u63a8\u7406\u6d41\u5a92\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u9a8c\u8bc1\u3002\u5728\u6a21\u62df\u7684XED\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5206\u6790\u9884\u6d4b\u3001\u8499\u7279\u5361\u6d1b\u91c7\u6837\u548c\u5b9e\u9645\u6d4b\u91cf\u5728\u4e0d\u540c\u5bb9\u91cf\u548c\u9700\u6c42\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7f16\u6392\u5668\u63d0\u4f9b\u4e86\u53ef\u5904\u7406\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u90e8\u7f72\u53ef\u884c\u6027\u548c\u914d\u7f6e\u5206\u5e03\u5f0f\u6d41\u5a92\u4f53\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u8ba1\u7b97\u53ef\u9760\u6027\u7684\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u5206\u5e03\u5f0f\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6709\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.16192", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16192", "abs": "https://arxiv.org/abs/2602.16192", "authors": ["Hiroaki Yamanaka", "Daisuke Miyashita", "Takashi Toi", "Asuka Maki", "Taiga Ikeda", "Jun Deguchi"], "title": "Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage", "comment": "13 pages, 5 figures", "summary": "Driven by our mission of \"uplifting the world with memory,\" this paper explores the design concept of \"memory\" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed \"extract then store,\" involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the \"store then on-demand extract\" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5b9e\u73b0\u4eba\u5de5\u8d85\u667a\u80fd\u6240\u9700\u7684\u5173\u952e\"\u8bb0\u5fc6\"\u8bbe\u8ba1\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\"\u5148\u5b58\u50a8\u540e\u6309\u9700\u63d0\u53d6\"\u7b49\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4fdd\u7559\u539f\u59cb\u7ecf\u9a8c\u4ee5\u907f\u514d\u4fe1\u606f\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\"\u5148\u63d0\u53d6\u540e\u5b58\u50a8\"\u8303\u5f0f\u5b58\u5728\u4fe1\u606f\u635f\u5931\u98ce\u9669\uff0c\u56e0\u4e3a\u63d0\u53d6\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4e22\u5f03\u5bf9\u4e0d\u540c\u4efb\u52a1\u6709\u4ef7\u503c\u7684\u77e5\u8bc6\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\"\u7528\u8bb0\u5fc6\u63d0\u5347\u4e16\u754c\"\u7684\u4f7f\u547d\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff1a1)\"\u5148\u5b58\u50a8\u540e\u6309\u9700\u63d0\u53d6\"\u65b9\u6cd5\uff0c\u4fdd\u7559\u539f\u59cb\u7ecf\u9a8c\u5e76\u6839\u636e\u4efb\u52a1\u9700\u6c42\u7075\u6d3b\u5e94\u7528\uff1b2)\u4ece\u5927\u91cf\u6982\u7387\u7ecf\u9a8c\u4e2d\u53d1\u73b0\u6df1\u5c42\u6d1e\u5bdf\uff1b3)\u901a\u8fc7\u5171\u4eab\u5b58\u50a8\u7ecf\u9a8c\u63d0\u9ad8\u7ecf\u9a8c\u6536\u96c6\u6548\u7387\uff1b4)\u901a\u8fc7\u7b80\u5355\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7b80\u5355\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u770b\u4f3c\u76f4\u89c2\u6709\u6548\u7684\u65b9\u6cd5\u786e\u5b9e\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002\u7279\u522b\u662f\"\u5148\u5b58\u50a8\u540e\u6309\u9700\u63d0\u53d6\"\u65b9\u6cd5\u80fd\u591f\u907f\u514d\u4fe1\u606f\u635f\u5931\uff0c\u4e3a\u4e0d\u540c\u4efb\u52a1\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u77e5\u8bc6\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u6307\u51fa\u4e86\u9650\u5236\u8fd9\u4e9b\u6709\u524d\u666f\u65b9\u5411\u7814\u7a76\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u7814\u7a76\u8bfe\u9898\u3002\u5f3a\u8c03\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e9b\u8bb0\u5fc6\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u73b0\u4eba\u5de5\u8d85\u667a\u80fd\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002"}}
{"id": "2602.16480", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16480", "abs": "https://arxiv.org/abs/2602.16480", "authors": ["Yiwen Lu"], "title": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data", "comment": "Federated learning, functional encryption, privacy-preserving machine learning, neural networks", "summary": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.", "AI": {"tldr": "SRFed\u662f\u4e00\u4e2a\u9488\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53bb\u4e2d\u5fc3\u5316\u9ad8\u6548\u529f\u80fd\u52a0\u5bc6\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u9632\u5fa1\u6027\u6a21\u578b\u805a\u5408\u673a\u5236\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u3001\u62b5\u6297\u62dc\u5360\u5ead\u653b\u51fb\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u5b89\u5168\u5a01\u80c1\uff1a\u597d\u5947\u670d\u52a1\u5668\u53ef\u80fd\u53d1\u8d77\u63a8\u7406\u653b\u51fb\u91cd\u5efa\u5ba2\u6237\u7aef\u79c1\u6709\u6570\u636e\uff0c\u4ee5\u53ca\u88ab\u653b\u9677\u7684\u5ba2\u6237\u7aef\u53ef\u80fd\u53d1\u8d77\u6295\u6bd2\u653b\u51fb\u7834\u574f\u6a21\u578b\u805a\u5408\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u9ad8\uff0c\u8981\u4e48\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1. \u8bbe\u8ba1\u53bb\u4e2d\u5fc3\u5316\u9ad8\u6548\u529f\u80fd\u52a0\u5bc6\u65b9\u6848\uff0c\u652f\u6301\u9ad8\u6548\u6a21\u578b\u52a0\u5bc6\u548c\u975e\u4ea4\u4e92\u5f0f\u89e3\u5bc6\uff0c\u6d88\u9664\u7b2c\u4e09\u65b9\u4f9d\u8d56\u5e76\u9632\u5fa1\u670d\u52a1\u5668\u7aef\u63a8\u7406\u653b\u51fb\uff1b2. \u57fa\u4e8eDEFE\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u7684\u9632\u5fa1\u6027\u6a21\u578b\u805a\u5408\u673a\u5236\uff0c\u901a\u8fc7\u5206\u5c42\u6295\u5f71\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u5206\u6790\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u8fc7\u6ee4\u6709\u6bd2\u6a21\u578b\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSRFed\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u62dc\u5360\u5ead\u9c81\u68d2\u6027\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SRFed\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u573a\u666f\uff0c\u80fd\u540c\u65f6\u9632\u5fa1\u670d\u52a1\u5668\u7aef\u63a8\u7406\u653b\u51fb\u548c\u5ba2\u6237\u7aef\u6295\u6bd2\u653b\u51fb\u3002"}}
{"id": "2602.16603", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16603", "abs": "https://arxiv.org/abs/2602.16603", "authors": ["Chia-chi Hsieh", "Zan Zong", "Xinyang Chen", "Jianjiang Li", "Jidong Zhai", "Lijie Wen"], "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving", "comment": "13 pages", "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.", "AI": {"tldr": "FlowPrefill\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684TTFT-\u541e\u5410\u91cf\u4f18\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u62a2\u5360\u7c92\u5ea6\u4e0e\u8c03\u5ea6\u9891\u7387\uff0c\u89e3\u51b3\u4e86\u9884\u586b\u5145\u9636\u6bb5\u5934\u90e8\u963b\u585e\u95ee\u9898\uff0c\u5728\u6ee1\u8db3\u5f02\u6784SLO\u7684\u540c\u65f6\u63d0\u5347\u541e\u5410\u91cf\u8fbe5.6\u500d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u9700\u6c42\u589e\u957f\uff0c\u670d\u52a1\u7cfb\u7edf\u9700\u8981\u5904\u7406\u5927\u91cf\u5e76\u53d1\u8bf7\u6c42\u548c\u591a\u6837\u5316\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u3002\u8fd9\u52a0\u5267\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578b\u9884\u586b\u5145\u9636\u6bb5\u7684\u5934\u90e8\u963b\u585e\u95ee\u9898\uff0c\u957f\u8bf7\u6c42\u72ec\u5360\u8d44\u6e90\u4f1a\u5ef6\u8fdf\u9ad8\u4f18\u5148\u7ea7\u8bf7\u6c42\uff0c\u5bfc\u81f4\u5e7f\u6cdb\u7684\u65f6\u95f4\u5230\u9996\u4e2a\u4ee4\u724cSLO\u8fdd\u89c4\u3002\u867d\u7136\u5206\u5757\u9884\u586b\u5145\u5b9e\u73b0\u4e86\u53ef\u4e2d\u65ad\u6027\uff0c\u4f46\u5728\u54cd\u5e94\u6027\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\u3002", "method": "FlowPrefill\u91c7\u7528\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u64cd\u4f5c\u7b26\u7ea7\u62a2\u5360\uff1a\u5229\u7528\u64cd\u4f5c\u7b26\u8fb9\u754c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6267\u884c\u4e2d\u65ad\uff0c\u907f\u514d\u56fa\u5b9a\u5c0f\u5206\u5757\u5e26\u6765\u7684\u6548\u7387\u635f\u5931\uff1b2) \u4e8b\u4ef6\u9a71\u52a8\u8c03\u5ea6\uff1a\u4ec5\u5728\u8bf7\u6c42\u5230\u8fbe\u6216\u5b8c\u6210\u4e8b\u4ef6\u65f6\u89e6\u53d1\u8c03\u5ea6\u51b3\u7b56\uff0c\u652f\u6301\u9ad8\u6548\u62a2\u5360\u54cd\u5e94\u6027\u540c\u65f6\u6700\u5c0f\u5316\u63a7\u5236\u5e73\u9762\u5f00\u9500\u3002", "result": "\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883trace\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFlowPrefill\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5c06\u6700\u5927\u6709\u6548\u541e\u5410\u91cf\u63d0\u5347\u4e86\u6700\u9ad85.6\u500d\uff0c\u540c\u65f6\u6ee1\u8db3\u5f02\u6784SLO\u8981\u6c42\u3002", "conclusion": "FlowPrefill\u901a\u8fc7\u89e3\u8026\u62a2\u5360\u7c92\u5ea6\u4e0e\u8c03\u5ea6\u9891\u7387\uff0c\u89e3\u51b3\u4e86\u9884\u586b\u5145\u8c03\u5ea6\u4e2d\u7684\u56fa\u6709\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u54cd\u5e94\u6027\u548c\u541e\u5410\u91cf\u7684\u5e73\u8861\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.16301", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16301", "abs": "https://arxiv.org/abs/2602.16301", "authors": ["Marissa A. Weis", "Maciej Wo\u0142czyk", "Rajai Nasser", "Rif A. Saurous", "Blaise Ag\u00fcera y Arcas", "Jo\u00e3o Sacramento", "Alexander Meulemans"], "title": "Multi-agent cooperation through in-context co-player inference", "comment": "26 pages, 4 figures", "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.", "AI": {"tldr": "\u5e8f\u5217\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5408\u4f5c\uff0c\u65e0\u9700\u786c\u7f16\u7801\u5047\u8bbe\u6216\u663e\u5f0f\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\uff0c\u5728\u591a\u6837\u5316\u5bf9\u624b\u5206\u5e03\u8bad\u7ec3\u4e0b\u81ea\u7136\u6d8c\u73b0\u5408\u4f5c\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u81ea\u5229\u667a\u80fd\u4f53\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5408\u4f5c\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u786c\u7f16\u7801\u5047\u8bbe\u6216\u4e25\u683c\u7684\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\uff0c\u9700\u8981\u66f4\u81ea\u7136\u3001\u53ef\u6269\u5c55\u7684\u5408\u4f5c\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5e8f\u5217\u6a21\u578b\u8bad\u7ec3\u667a\u80fd\u4f53\u5bf9\u6297\u591a\u6837\u5316\u5bf9\u624b\u5206\u5e03\uff0c\u5229\u7528\u5e8f\u5217\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5b9e\u73b0\u5feb\u901f\u60c5\u5883\u5185\u6700\u4f73\u54cd\u5e94\u7b56\u7565\uff0c\u5f62\u6210\u5b66\u4e60\u7b97\u6cd5\u529f\u80fd\u3002", "result": "\u4e0a\u4e0b\u6587\u9002\u5e94\u4f7f\u667a\u80fd\u4f53\u6613\u53d7\u52d2\u7d22\uff0c\u76f8\u4e92\u5851\u9020\u5bf9\u624b\u4e0a\u4e0b\u6587\u5b66\u4e60\u52a8\u6001\u7684\u538b\u529b\u6700\u7ec8\u5bfc\u81f4\u5408\u4f5c\u884c\u4e3a\u7684\u5b66\u4e60\uff0c\u5e8f\u5217\u6a21\u578b\u7ed3\u5408\u5bf9\u624b\u591a\u6837\u6027\u53ef\u6269\u5c55\u5b66\u4e60\u5408\u4f5c\u884c\u4e3a\u3002", "conclusion": "\u5e8f\u5217\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7ed3\u5408\u5bf9\u624b\u591a\u6837\u6027\u4e3a\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u63d0\u4f9b\u4e86\u81ea\u7136\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u65e0\u9700\u786c\u7f16\u7801\u5047\u8bbe\u6216\u663e\u5f0f\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u3002"}}
{"id": "2602.16708", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16708", "abs": "https://arxiv.org/abs/2602.16708", "authors": ["Nils Palumbo", "Sarthak Choudhary", "Jihye Choi", "Prasad Chalasani", "Mihai Christodorescu", "Somesh Jha"], "title": "Policy Compiler for Secure Agentic Systems", "comment": null, "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.", "AI": {"tldr": "PCAS\u662f\u4e00\u4e2a\u7528\u4e8e\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7b56\u7565\u7f16\u8bd1\u5668\uff0c\u901a\u8fc7\u4f9d\u8d56\u56fe\u5efa\u6a21\u4fe1\u606f\u6d41\uff0c\u4f7f\u7528Datalog\u8bed\u8a00\u8868\u8fbe\u7b56\u7565\uff0c\u63d0\u4f9b\u786e\u5b9a\u6027\u7b56\u7565\u6267\u884c\u4fdd\u969c\uff0c\u5c06\u7b56\u7565\u5408\u89c4\u6027\u4ece48%\u63d0\u5347\u523093%\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u9700\u8981\u590d\u6742\u6388\u6743\u7b56\u7565\u7684\u573a\u666f\u4e2d\u90e8\u7f72\u589e\u591a\uff0c\u4f46\u5c06\u7b56\u7565\u5d4c\u5165\u63d0\u793a\u4e2d\u65e0\u6cd5\u63d0\u4f9b\u6267\u884c\u4fdd\u8bc1\uff0c\u9700\u8981\u786e\u5b9a\u6027\u7b56\u7565\u6267\u884c\u673a\u5236\u3002", "method": "PCAS\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u72b6\u6001\u5efa\u6a21\u4e3a\u4f9d\u8d56\u56fe\uff0c\u6355\u83b7\u5de5\u5177\u8c03\u7528\u3001\u7ed3\u679c\u548c\u6d88\u606f\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff1b\u4f7f\u7528Datalog\u884d\u751f\u8bed\u8a00\u8868\u8fbe\u7b56\u7565\uff1b\u901a\u8fc7\u5f15\u7528\u76d1\u89c6\u5668\u62e6\u622a\u6240\u6709\u52a8\u4f5c\u5e76\u5728\u6267\u884c\u524d\u963b\u6b62\u8fdd\u89c4\u3002", "result": "\u5728\u5ba2\u6237\u670d\u52a1\u4efb\u52a1\u4e2d\uff0cPCAS\u5c06\u7b56\u7565\u5408\u89c4\u6027\u4ece48%\u63d0\u5347\u523093%\uff0c\u5728\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u5b9e\u73b0\u96f6\u7b56\u7565\u8fdd\u89c4\uff1b\u65e0\u9700\u5b89\u5168\u7279\u5b9a\u91cd\u6784\u5373\u53ef\u6784\u5efa\u7b56\u7565\u5408\u89c4\u7cfb\u7edf\u3002", "conclusion": "PCAS\u4e3aLLM\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u786e\u5b9a\u6027\u7b56\u7565\u6267\u884c\uff0c\u901a\u8fc7\u7f16\u8bd1\u65b9\u6cd5\u786e\u4fdd\u7cfb\u7edf\u6784\u5efa\u65f6\u5373\u7b26\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u5408\u89c4\u6027\u5e76\u6d88\u9664\u7b56\u7565\u8fdd\u89c4\u3002"}}
{"id": "2602.16435", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16435", "abs": "https://arxiv.org/abs/2602.16435", "authors": ["Arun Vignesh Malarkkan", "Wangyang Ying", "Yanjie Fu"], "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning", "comment": "11 Pages, References and Appendix", "summary": "Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.", "AI": {"tldr": "CAFE\u6846\u67b6\u5c06\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u91cd\u6784\u4e3a\u56e0\u679c\u5f15\u5bfc\u7684\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u56e0\u679c\u53d1\u73b0\u548c\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u663e\u8457\u63d0\u5347\u7279\u5f81\u5de5\u7a0b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u4f9d\u8d56\u7edf\u8ba1\u542f\u53d1\u5f0f\uff0c\u4ea7\u751f\u7684\u7279\u5f81\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u8106\u5f31\u3002\u9700\u8981\u5f15\u5165\u56e0\u679c\u7ed3\u6784\u4f5c\u4e3a\u8f6f\u5f52\u7eb3\u5148\u9a8c\uff0c\u63d0\u5347\u7279\u5f81\u5de5\u7a0b\u7684\u9c81\u68d2\u6027\u3002", "method": "CAFE\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u7279\u5f81\u4e0e\u76ee\u6807\u4e4b\u95f4\u7684\u7a00\u758f\u6709\u5411\u65e0\u73af\u56fe\uff0c\u83b7\u5f97\u8f6f\u56e0\u679c\u5148\u9a8c\uff0c\u5c06\u7279\u5f81\u6309\u56e0\u679c\u5f71\u54cd\u5206\u7ec4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u7ea7\u8054\u591a\u667a\u80fd\u4f53\u6df1\u5ea6Q\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u5956\u52b1\u5851\u9020\u548c\u56e0\u679c\u7ec4\u7ea7\u63a2\u7d22\u7b56\u7565\u9009\u62e9\u56e0\u679c\u7ec4\u548c\u53d8\u6362\u7b97\u5b50\u3002", "result": "\u572815\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAFE\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u8fbe7%\uff0c\u51cf\u5c11\u6536\u655b\u6240\u9700\u56de\u5408\u6570\uff0c\u5728\u53d7\u63a7\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\u51cf\u5c11\u7ea64\u500d\uff0c\u4ea7\u751f\u66f4\u7d27\u51d1\u7684\u7279\u5f81\u96c6\u548c\u66f4\u7a33\u5b9a\u7684\u540e\u9a8c\u5f52\u56e0\u3002", "conclusion": "\u56e0\u679c\u7ed3\u6784\u4f5c\u4e3a\u8f6f\u5f52\u7eb3\u5148\u9a8c\u800c\u975e\u521a\u6027\u7ea6\u675f\uff0c\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u7279\u5f81\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.16481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16481", "abs": "https://arxiv.org/abs/2602.16481", "authors": ["Zihao Li", "Fabrizio Russo"], "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach", "comment": "26 pages, including appendix", "summary": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e0d\u5b8c\u7f8e\u4e13\u5bb6\uff0c\u901a\u8fc7\u56e0\u679c\u5047\u8bbe\u8bba\u8bc1\u6846\u67b6\u5c06\u8bed\u4e49\u7ed3\u6784\u5148\u9a8c\u4e0e\u6761\u4ef6\u72ec\u7acb\u6027\u8bc1\u636e\u7ed3\u5408\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u56e0\u679c\u53d1\u73b0\u6027\u80fd", "motivation": "\u56e0\u679c\u53d1\u73b0\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\u6784\u5efa\u539f\u5219\u6027\u56e0\u679c\u56fe\uff0c\u4f46\u73b0\u6709\u7edf\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c2\u6d4b\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e0d\u5b8c\u7f8e\u4e13\u5bb6\uff0c\u7ed3\u5408\u8bed\u4e49\u4fe1\u606f\u548c\u7edf\u8ba1\u8bc1\u636e\u8fdb\u884c\u56e0\u679c\u53d1\u73b0", "method": "\u91c7\u7528\u56e0\u679c\u5047\u8bbe\u8bba\u8bc1\u6846\u67b6\uff0c\u4ece\u53d8\u91cf\u540d\u79f0\u548c\u63cf\u8ff0\u4e2d\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\u5148\u9a8c\uff0c\u4e0e\u6761\u4ef6\u72ec\u7acb\u6027\u8bc1\u636e\u76f8\u7ed3\u5408\u3002\u5f15\u5165\u8bc4\u4f30\u534f\u8bae\u51cf\u8f7b\u8bb0\u5fc6\u504f\u5dee\u5bf9LLM\u8bc4\u4f30\u7684\u5f71\u54cd", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u548c\u8bed\u4e49\u57fa\u7840\u5408\u6210\u56fe\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86LLM\u4f5c\u4e3a\u4e0d\u5b8c\u7f8e\u4e13\u5bb6\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u8bed\u4e49\u5148\u9a8c\u6765\u6e90\uff0c\u4e0e\u56e0\u679c\u5047\u8bbe\u8bba\u8bc1\u6846\u67b6\u7ed3\u5408\u80fd\u591f\u63d0\u5347\u56e0\u679c\u53d1\u73b0\u6027\u80fd\uff0c\u540c\u65f6\u9700\u8981\u9002\u5f53\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u786e\u4fdd\u7ed3\u679c\u7684\u53ef\u9760\u6027"}}
{"id": "2602.16512", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16512", "abs": "https://arxiv.org/abs/2602.16512", "authors": ["Felix Fricke", "Simon Malberg", "Georg Groh"], "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs", "comment": null, "summary": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.", "AI": {"tldr": "FoT\u662f\u4e00\u4e2a\u901a\u7528\u57fa\u7840\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u4f18\u5316\u52a8\u6001\u63a8\u7406\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u63d0\u793a\u65b9\u6848\u9759\u6001\u3001\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u4f18\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u65b9\u6848\u5982Chain of Thought\u3001Tree of Thoughts\u7b49\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u9700\u8981\u7528\u6237\u5b9a\u4e49\u9759\u6001\u7684\u3001\u7279\u5b9a\u95ee\u9898\u7684\u63a8\u7406\u7ed3\u6784\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u6216\u672a\u89c1\u95ee\u9898\u7c7b\u578b\u7684\u9002\u5e94\u6027\uff1b2) \u5728\u8d85\u53c2\u6570\u3001\u63d0\u793a\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u63d0\u793a\u6210\u672c\u65b9\u9762\u901a\u5e38\u672a\u5145\u5206\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86Framework of Thoughts (FoT)\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u57fa\u7840\u6846\u67b6\uff0c\u5177\u6709\u8d85\u53c2\u6570\u8c03\u4f18\u3001\u63d0\u793a\u4f18\u5316\u3001\u5e76\u884c\u6267\u884c\u548c\u667a\u80fd\u7f13\u5b58\u7b49\u5185\u7f6e\u529f\u80fd\uff0c\u80fd\u591f\u6784\u5efa\u548c\u4f18\u5316\u52a8\u6001\u63a8\u7406\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5c06Tree of Thoughts\u3001Graph of Thoughts\u548cProbTree\u4e09\u79cd\u6d41\u884c\u65b9\u6848\u5728FoT\u4e2d\u5b9e\u73b0\uff0c\u5b9e\u8bc1\u8868\u660eFoT\u80fd\u591f\u663e\u8457\u52a0\u5feb\u6267\u884c\u901f\u5ea6\u3001\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u83b7\u5f97\u66f4\u597d\u7684\u4efb\u52a1\u5206\u6570\u3002", "conclusion": "FoT\u6846\u67b6\u91ca\u653e\u4e86\u63a8\u7406\u65b9\u6848\u7684\u6f5c\u5728\u6027\u80fd\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u672a\u6765\u52a8\u6001\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6848\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u5df2\u5f00\u6e90\u4ee3\u7801\u5e93\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2602.16578", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16578", "abs": "https://arxiv.org/abs/2602.16578", "authors": ["Vered Tohar", "Tsahi Hayat", "Amir Leshem"], "title": "Creating a digital poet", "comment": "24 pages, 3 figures", "summary": "Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.", "AI": {"tldr": "\u901a\u8fc7\u4e03\u4e2a\u6708\u7684\u8bd7\u6b4c\u5de5\u4f5c\u574a\uff0c\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u4e13\u5bb6\u53cd\u9988\u5851\u9020\u4e86\u4e00\u4e2a\u6570\u5b57\u8bd7\u4eba\uff0c\u5176\u521b\u4f5c\u7684\u8bd7\u6b4c\u5728\u76f2\u6d4b\u4e2d\u4e0e\u4eba\u7c7b\u8bd7\u6b4c\u96be\u4ee5\u533a\u5206\uff0c\u6700\u7ec8\u7531\u5546\u4e1a\u51fa\u7248\u793e\u51fa\u7248\u4e86\u8bd7\u96c6\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u80fd\u5426\u521b\u4f5c\u51fa\u4f18\u79c0\u7684\u8bd7\u6b4c\uff0c\u8fd9\u6d89\u53ca\u5230\u827a\u672f\u672c\u8d28\u548c\u4ef7\u503c\u7684\u6839\u672c\u95ee\u9898\uff0c\u65e8\u5728\u7814\u7a76AI\u5728\u521b\u9020\u6027\u5199\u4f5c\u9886\u57df\u7684\u6f5c\u529b\u53ca\u5176\u5bf9\u827a\u672f\u521b\u4f5c\u548c\u4f5c\u8005\u8eab\u4efd\u6982\u5ff5\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e03\u4e2a\u6708\u7684\u8bd7\u6b4c\u5de5\u4f5c\u574a\u5f62\u5f0f\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u4e0a\u4e0b\u6587\u4e13\u5bb6\u53cd\u9988\uff08\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff09\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5851\u9020\u6210\u6570\u5b57\u8bd7\u4eba\uff0c\u5305\u62ec\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u5e76\u4e3a\u6a21\u578b\u521b\u5efa\u7b14\u540d\u548c\u4f5c\u8005\u5f62\u8c61\u3002", "result": "\u6a21\u578b\u5f62\u6210\u4e86\u72ec\u7279\u7684\u98ce\u683c\u548c\u8fde\u8d2f\u7684\u4f5c\u54c1\u96c6\uff1b\u572850\u540d\u4eba\u6587\u5b66\u751f\u7684\u76f2\u6d4b\u4e2d\uff0c\u4eba\u7c7b\u8bd7\u6b4c\u88ab\u8bc6\u522b\u4e3a\u4eba\u7c7b\u7684\u51c6\u786e\u7387\u4e3a54%\uff0cAI\u8bd7\u6b4c\u4e3a52%\uff0895%\u7f6e\u4fe1\u533a\u95f4\u5305\u542b50%\uff09\uff0c\u8868\u660e\u65e0\u6cd5\u53ef\u9760\u533a\u5206\uff1b\u6a21\u578b\u7684\u4f5c\u54c1\u6700\u7ec8\u7531\u5546\u4e1a\u51fa\u7248\u793e\u51fa\u7248\u3002", "conclusion": "\u5de5\u4f5c\u574a\u5f0f\u63d0\u793a\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u957f\u671f\u521b\u9020\u6027\u5851\u9020\uff0cAI\u8bd7\u6b4c\u5df2\u8fbe\u5230\u4e0e\u4eba\u7c7b\u8bd7\u6b4c\u96be\u4ee5\u533a\u5206\u7684\u6c34\u5e73\uff0c\u8fd9\u91cd\u65b0\u5f15\u53d1\u4e86\u5173\u4e8e\u521b\u9020\u6027\u548c\u4f5c\u8005\u8eab\u4efd\u7684\u8fa9\u8bba\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u827a\u672f\u521b\u4f5c\u89c2\u5ff5\u3002"}}
{"id": "2602.16653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16653", "abs": "https://arxiv.org/abs/2602.16653", "authors": ["Yangjie Xu", "Lujun Li", "Lama Sleem", "Niccolo Gentile", "Yewei Song", "Yiqun Wang", "Siming Ji", "Wenbo Wu", "Radu State"], "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments", "comment": null, "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.", "AI": {"tldr": "Agent Skill\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u4e2d\u7b49\u89c4\u6a21\u5c0f\u8bed\u8a00\u6a21\u578b\uff0812B-30B\u53c2\u6570\uff09\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u6570\u636e\u5b89\u5168\u548c\u9884\u7b97\u53d7\u9650\u7684\u5de5\u4e1a\u573a\u666f\u4e2d\u8fbe\u5230\u63a5\u8fd1\u95ed\u6e90\u6a21\u578b\u7684\u6c34\u5e73\uff0c\u800c\u8d85\u5c0f\u6a21\u578b\u5728\u6280\u80fd\u9009\u62e9\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u7814\u7a76Agent Skill\u8303\u5f0f\u662f\u5426\u80fd\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5e26\u6765\u7c7b\u4f3c\u5927\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u5de5\u4e1a\u573a\u666f\u4e2d\u56e0\u6570\u636e\u5b89\u5168\u548c\u9884\u7b97\u9650\u5236\u800c\u65e0\u6cd5\u6301\u7eed\u4f9d\u8d56\u516c\u5171API\u7684\u95ee\u9898\uff0c\u4ee5\u53caSLMs\u5728\u9ad8\u5ea6\u5b9a\u5236\u5316\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86Agent Skill\u8fc7\u7a0b\u7684\u6570\u5b66\u5b9a\u4e49\uff0c\u7136\u540e\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u7528\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u4e24\u4e2a\u5f00\u6e90\u4efb\u52a1\u548c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u4fdd\u9669\u7d22\u8d54\u6570\u636e\u96c6\u3002", "result": "\u8d85\u5c0f\u6a21\u578b\u5728\u53ef\u9760\u6280\u80fd\u9009\u62e9\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1b\u4e2d\u7b49\u89c4\u6a21SLMs\uff08\u7ea612B-30B\u53c2\u6570\uff09\u4eceAgent Skill\u65b9\u6cd5\u4e2d\u83b7\u76ca\u663e\u8457\uff1b\u7ea680B\u53c2\u6570\u7684\u4ee3\u7801\u4e13\u7528\u53d8\u4f53\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u95ed\u6e90\u57fa\u7ebf\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86GPU\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u5168\u9762\u800c\u7ec6\u81f4\u5730\u63cf\u8ff0\u4e86Agent Skill\u6846\u67b6\u7684\u80fd\u529b\u548c\u9650\u5236\uff0c\u4e3a\u5728SLM\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u6709\u6548\u90e8\u7f72Agent Skills\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5b89\u5168\u548c\u9884\u7b97\u53d7\u9650\u7684\u5de5\u4e1a\u573a\u666f\u4e2d\u3002"}}
