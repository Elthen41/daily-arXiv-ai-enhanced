<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 该论文提出了两种基于信息论和热力学的无监督指标来评估大语言模型对给定任务的忠实度，通过将LLM建模为二分信息引擎，使用KL散度衡量语义忠实度，并结合热力学熵产生指标进行联合评估。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型对给定任务的忠实度是一个复杂挑战，现有方法存在局限性，需要开发新的无监督评估指标来更准确地衡量LLM在任务执行中的忠实度。

Method: 将LLM建模为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文C通过提示Q转换为答案A的过程。将QCA三元组建模为共享主题上的概率分布，使用转移矩阵Q和A分别编码查询目标和实际结果，通过凸优化同时推断这两个矩阵，计算KL散度作为语义忠实度指标，并将其映射到[0,1]区间。同时提出基于热力学的语义熵产生指标。

Result: 提出的语义忠实度和语义熵产生指标能够有效评估LLM的忠实度，高忠实度通常对应低熵产生。在SEC 10-K文件摘要任务上的演示验证了该框架的有效性。

Conclusion: 该研究开发了基于信息论和热力学的无监督忠实度评估框架，SF和SEP指标可以单独或联合用于LLM评估和幻觉控制，为LLM忠实度评估提供了新的理论和方法基础。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [2] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 该论文提出了一种创新的AI与数据科学教学方法，系统性地将传统机器学习技术与现代大语言模型相结合，通过两部分课程设计帮助学生全面理解AI发展并掌握实用技能。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的发展历程，同时掌握传统机器学习和现代大语言模型技术，更好地适应快速发展的AI行业需求。

Method: 采用两部分课程设计：第一部分教授基础机器学习概念，第二部分专注于当代大语言模型应用。课程架构包括实施策略、评估方法和为期14周（两个7周学期）的夏季课程交付。

Result: 研究发现这种集成方法增强了学生对AI领域的理解，并更好地为他们应对快速发展的AI行业需求做好准备。

Conclusion: 这种将传统机器学习与现代大语言模型系统结合的教学方法能有效提升学生对AI全景的理解，并增强他们在AI领域的实践能力。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [3] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 该论文通过形式化证明指出：任何算法（包括AI模型）都无法展示其初始算法本身不具备的新功能能力，因此无法实现真正的创造力，只能展示现有功能能力及其组合排列。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，人们开始探讨人类距离开发出达到人类智能水平的人工通用智能（AGI）还有多远。本文旨在通过定义AGI的界限来回答这个问题，特别是要确定任何机器可计算过程（算法）的上限。

Method: 采用先前研究中关于AGI的定义，该定义最能反映AI领先开发者使用该术语时的含义：在某个研究领域中以创造性和创新性的方式解锁该领域新的、先前未知的功能能力。基于这一定义，作者进行了形式化证明，证明任何算法都无法展示其初始算法本身不具备的新功能能力。

Result: 形式化证明表明：没有算法能够展示其初始算法本身不具备的新功能能力。因此，任何算法（包括AI模型）都无法在科学、工程、艺术、体育等领域实现真正的创造力。AI模型只能展示现有功能能力，以及现有功能能力的组合和排列。

Conclusion: 这一证明对AI发展的未来具有重要意义，同时也对人类智能的起源提出了深刻的问题。它表明AI无法实现真正的创造性突破，只能基于现有能力进行组合和优化，这对AGI的实现前景提出了根本性挑战。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [4] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个基于知识图谱增强的推理框架，专门针对集成电路规格文档的长上下文问题，通过构建电路知识图谱和自适应检索机制，显著提升了LLM在硬件设计中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在集成电路开发自动化方面具有巨大潜力，但受限于有限的上下文窗口。现有的上下文扩展方法难以对复杂、冗长的电路规格进行有效的语义建模和多跳推理，这阻碍了LLM在硬件设计中的实际工业部署。

Method: 1. 电路语义感知知识图谱构建：将电路规格转换为领域特定的知识图谱ChipKG；2. ChipKG增强推理机制：结合信息论自适应检索动态追踪逻辑依赖关系，以及意图感知语义过滤去除无关噪声，平衡检索完整性和精确性。

Result: 在工业级规格推理基准测试中，ChipMind显著优于现有最先进基线方法，平均提升34.59%（最高达72.73%）。

Conclusion: ChipMind框架填补了学术研究与LLM辅助硬件设计实际工业部署之间的关键空白，为集成电路开发自动化提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [5] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 本文论证可能性理论是解决Dempster-Shafer理论悖论的根本方案，通过可能性与必要性测度的二元框架，为不确定性处理提供逻辑一致的基础。


<details>
  <summary>Details</summary>
Motivation: 现有Dempster-Shafer理论存在悖论和逻辑不一致问题，许多尝试只是修补Dempster规则而非根本解决。需要建立逻辑一致且数学严谨的不确定性处理基础框架。

Method: 采用Bychkov文章中发展的公理化方法，基于可能性与必要性测度的二元装置从头构建理论框架。通过经典医疗诊断困境案例，对比分析概率、证据和可能性三种范式。

Result: 可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑。它不是简单的替代方案，而是解决DST悖论的根本方案。

Conclusion: 可能性理论为解决DST危机提供了根本性方案，通过公理化方法建立了逻辑一致的不确定性处理基础，在矛盾数据处理方面优于传统证据理论。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [6] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张将AI研究目标从"自我改进"转向"共同改进"，即人类研究者与AI系统协作进行AI研究，实现共同超智能


<details>
  <summary>Details</summary>
Motivation: 当前AI领域追求自我改进的目标存在危险且难以完全实现，需要更可行、更安全的研究方向

Method: 提倡建立人类研究者与AI系统的协作研究循环，从构思到实验全过程合作，专门提升AI与人类协作进行AI研究的能力

Result: 通过将人类研究改进纳入循环，既能加速AI研究进展，又能通过人机共生实现更安全的超智能

Conclusion: 共同改进是比自我改进更可行、更安全的目标，通过人机协作研究可以实现共同超智能，同时加快研究速度并提高安全性

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [7] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是首个为LLM约束满足提供确定性、可靠概率边界的实用框架，相比基线方法获得6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从研究原型转向生产系统，从业者需要可靠方法来验证模型输出是否满足所需约束。基于采样的估计只能提供模型行为的直觉，无法提供可靠保证。

Method: BEAVER使用新颖的token trie和frontier数据结构，系统性地探索生成空间，为任何前缀封闭的语义约束维护可证明可靠的概率边界。形式化了验证问题并证明了方法的可靠性。

Result: 在正确性验证、隐私验证和安全代码生成任务上评估BEAVER，在相同计算预算下，相比基线方法获得6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。

Conclusion: BEAVER能够提供松散边界或经验评估无法实现的精确特征描述和风险评估，为LLM约束满足提供了首个实用的确定性概率边界计算框架。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [8] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: MIND推理框架通过"理解-再思考-纠正"的认知过程，将MLLMs从被动模仿推理提升到主动判别推理，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理任务中存在多理性语义建模有限、逻辑鲁棒性不足、易受复杂场景误导等问题，需要提升其认知能力。

Method: 提出MIND推理框架，包含：1)RAD范式自动扩展数据集生成多样化理性；2)P2CL策略分两阶段增强多理性正向学习和主动逻辑判别纠正；3)MCA优化策略解决多理性语义空间表示纠缠问题。

Result: 在涵盖科学、常识和数学场景的多个公共数据集上实现了最先进的性能，为MLLMs向更高认知智能发展提供了新视角。

Conclusion: MIND框架通过赋予MLLMs类似人类的"理解-再思考-纠正"认知能力，实现了从被动模仿推理到主动判别推理的范式演进，显著提升了推理性能。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [9] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: 本文提出Executor-Analyst框架解决临床智能体中的上下文利用失败问题，通过解耦工具执行与临床推理，采用分层集成策略，在无需昂贵微调的情况下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于小型LLM的临床智能体（如TxAgent）存在"上下文利用失败"问题：模型能够成功检索生物医学证据，但无法基于这些信息进行诊断推理。需要解决这种推理缺陷。

Method: 提出Executor-Analyst框架，将工具执行的语法精度与临床推理的语义鲁棒性解耦。通过专门的TxAgents（执行器）与长上下文基础模型（分析师）协同工作，采用分层集成策略而非全局池化，以保留证据多样性。

Result: 1) 在CURE-Bench上实现最先进性能，无需昂贵的端到端微调；2) 发现上下文-性能悖论：推理上下文超过12k token会引入噪声降低准确性；3) 发现动作空间维度诅咒：工具集扩展需要分层检索策略。

Conclusion: 通过无训练的架构工程方法，为下一代可信赖的AI驱动治疗提供了可扩展、敏捷的基础。该方法证明了模块化架构在解决临床推理缺陷方面的有效性。

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [10] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 本文提出OntoAxiom基准测试，系统评估LLMs在识别本体论公理方面的性能，发现Axiom-by-Axiom提示策略优于直接方法，但性能因公理类型和本体领域而异。


<details>
  <summary>Details</summary>
Motivation: 本体开发需要大量建模和领域专业知识，自动化这一过程的本体学习在过去十年中随着NLP技术特别是LLMs的发展而进步。本文旨在解决识别公理这一核心挑战，公理是定义类和属性间逻辑关系的基本本体组件。

Method: 引入OntoAxiom基准测试，包含9个中等规模本体（共17,118个三元组和2,771个公理），专注于子类、不相交、子属性、定义域和值域公理。评估12个LLMs在三种shot设置和两种提示策略下的性能：直接查询所有公理 vs 逐个公理查询。

Result: Axiom-by-Axiom提示策略比直接方法获得更高的F1分数。性能因公理类型而异，某些公理更难识别。领域影响显著（如FOAF本体子类公理得分0.642，音乐本体仅0.218）。大型LLMs优于小型模型，但小型模型在资源受限环境中仍可用。

Conclusion: 虽然LLMs性能尚不足以完全自动化公理识别，但可以为本体工程师提供有价值的候选公理，支持本体的开发和精炼。Axiom-by-Axiom策略更有效，但性能受公理类型和本体领域影响。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [11] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出了一种新的子句权重方案DeepDist，首次针对PMS和WPMS问题使用不同的权重更新条件，并结合新的初始化方法和去偏置技术，显著提升了SLS求解器性能。


<details>
  <summary>Details</summary>
Motivation: 现有的随机局部搜索算法主要关注子句权重方案设计，但未能充分区分PMS和WPMS问题，通常采用统一的权重更新策略，忽略了这两种问题类型之间的关键结构差异。

Method: 1) 提出新的子句权重方案，首次根据PMS和WPMS实例的不同条件更新子句权重；2) 引入新的初始化方法，更好地适应两种实例类型的特性；3) 提出去偏置方法，优先满足单元子句和硬子句；4) 基于这些方法开发了名为DeepDist的新SLS求解器。

Result: 在最近MaxSAT评估的随时跟踪基准测试中，DeepDist优于最先进的SLS求解器。与TT-Open-WBO-Inc结合的混合求解器超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。

Conclusion: 提出的方法有效地区分了PMS和WPMS问题，通过针对性的权重更新策略和互补技术显著提升了求解性能，代码已开源。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [12] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: 提出FARDA框架，使用深度强化学习快速部署抗干扰雷达，相比进化算法速度提升约7000倍，同时保持相当的覆盖性能。


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有基于进化算法的方法耗时且易陷入局部最优，需要更高效的解决方案。

Method: 将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息，并设计新的奖励格式。

Result: FARDA方法在保持与进化算法相当的覆盖性能的同时，部署速度提升约7000倍，消融实验验证了各组件必要性。

Conclusion: FARDA框架通过深度强化学习实现了快速高效的抗干扰雷达部署，显著优于传统进化算法，为现代战争中的雷达部署提供了新解决方案。

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [13] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 本文提出进化推理优化（ERO）框架，通过进化策略让LLMs获得类似人类的系统2推理能力，发现GPT-5等最新模型推理能力有限，但通过简单进化循环可显著增强较弱模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在特定任务上表现出色，但在通用智能和系统2推理（慢思考）方面仍有不足。本文旨在探索机器智能（如LLMs）能否像人类一样进化获得推理能力，而不仅仅是特定技能。

Method: 提出进化推理优化（ERO）框架：1）初始化多个LLMs作为种群；2）采用进化策略对种群进行演化，最大化最佳个体的量化推理分数；3）通过"适者生存"原则搜索具有强推理能力的个体。

Result: 1）最新LLMs（如GPT-5）仍表现出有限的系统2推理能力；2）通过简单的ERO进化循环，相对较弱的模型（Qwen-7B）可被增强并涌现出强大的推理能力。

Conclusion: ERO框架证明通过进化策略可以显著提升LLMs的推理能力，为机器智能获得类似人类的系统2推理能力提供了可行路径，挑战了当前LLMs在通用智能方面的局限性。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [14] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文反驳了"LLM只是模式匹配器，无法实现推理"的批评，提出真正的瓶颈在于缺乏系统2的协调层，而非LLM本身。作者提出了UCCT理论和MACI架构来实现语义锚定和协调。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大型语言模型(LLM)的批评——认为LLM只是"模式匹配器"，在结构上无法进行推理或规划，因此是通往AGI的死胡同。作者认为这种批评混淆了问题本质，真正的瓶颈在于缺乏系统2的协调层来选择和约束模式。

Method: 提出了UCCT(语义锚定理论)，将推理建模为受有效支持(rho_d)、表征不匹配(d_r)和自适应锚定预算(gamma log k)控制的相变过程。基于此理论设计了MACI架构，包含诱饵(行为调节辩论)、过滤(苏格拉底式评判)和持久性(事务性记忆)三个协调组件。

Result: 通过理论框架将常见的反对意见重新定义为可测试的协调失败问题，展示了如何通过协调层使LLM从无根据的生成转变为目标导向的推理。为LLM作为AGI基础提供了理论支持。

Conclusion: 通往AGI的道路应该通过LLM而非绕过它们。LLM提供了必要的系统1底层模式库，而缺失的系统2协调层可以通过UCCT理论和MACI架构来实现，从而解决LLM的推理能力问题。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [15] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 本文提出了一种多模态肿瘤智能体（MOA），整合基于TITAN基础模型的IDH1突变预测组织学工具，结合临床和基因组数据推理，在低级别胶质瘤中实现了高精度的IDH1突变预测。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变具有重要的临床意义，能够定义不同的预后和治疗亚组。当前需要更准确的预测方法来整合多模态信息，提高突变预测的准确性。

Method: 开发了多模态肿瘤智能体（MOA），整合了基于TITAN基础模型的IDH1突变预测组织学工具，通过PubMed、Google搜索和OncoKB对结构化临床和基因组输入进行推理。在TCGA-LGG队列的488名患者上进行评估。

Result: MOA（无组织学工具）F1分数为0.826，优于临床基线（0.798）。整合组织学特征后，MOA达到最高性能，F1分数为0.912，超过组织学基线（0.894）和融合的组织学-临床基线（0.897）。

Conclusion: 该智能体能够通过外部生物医学资源捕获互补的突变相关信息，实现准确的IDH1突变预测，展示了多模态整合在肿瘤突变预测中的优势。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [16] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 使用GPT-5开发的论文正确性检查器发现，顶级AI会议和期刊发表的论文中存在可观数量的客观错误，且错误数量随时间增加，人类专家验证显示AI检查器准确率达83.2%。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物是构建新研究和知识的基础，但文献中的错误会传播并影响后续研究和可重复性。研究加速和同行评审系统压力使得错误更难被发现和避免。

Method: 开发基于GPT-5的论文正确性检查器，系统识别顶级AI会议和期刊已发表论文中的客观错误（如公式、推导、计算、图表错误），排除主观考量，并由人类专家验证AI识别结果。

Result: 发现论文包含显著数量的客观错误，且平均错误数随时间增加：NeurIPS从2021年的3.8个增至2025年的5.9个（增长55.3%）；ICLR从2018年的4.1个增至2025年的5.2个；TMLR从2022/23年的5.0个增至2025年的5.5个。人类专家验证316个潜在错误中263个为真实错误，精确度83.2%。

Conclusion: 前沿大语言模型在检测和纠正已发表论文中的客观错误方面具有潜力，有助于建立更坚实的知识基础，减少文献中的混淆并增强可重复性。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [17] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE框架通过透明推理和一致性评估诊断大视觉语言模型的推理轨迹，而非仅评估最终答案，使用辅助推理集分解复杂问题并暴露标准评估忽略的失败


<details>
  <summary>Details</summary>
Motivation: 可靠数学和科学推理对大型视觉语言模型仍是开放挑战，标准最终答案评估常掩盖推理错误，导致无声失败持续存在

Method: 引入TRACE框架，利用辅助推理集（紧凑的子问题-答案对）分解复杂问题，通过基于一致性的指标评估中间步骤，暴露标准评估忽略的失败

Result: 实验表明ARS间的一致性与最终答案正确性相关，帮助定位推理失败步骤，提供模型改进的可操作信号；TRACE定义置信区域区分可靠与不可靠推理路径

Conclusion: TRACE框架通过诊断推理轨迹而非仅最终结果，为大型视觉语言模型的数学和科学推理提供了更透明、可操作的评估和改进方法

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [18] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN结合变分量子电路与Rainbow DQN，在人力资源分配问题上相比传统方法获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 资源分配问题是NP难问题，传统深度强化学习方法受限于经典函数逼近器的表达能力，需要更强大的表示能力来处理组合复杂性

Method: 提出变分量子Rainbow DQN（VQR-DQN），将环形拓扑变分量子电路与Rainbow DQN集成，利用量子叠加和纠缠特性；将人力资源分配问题建模为基于人员能力、事件调度和转移时间的马尔可夫决策过程

Result: 在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少了26.8%的归一化完工时间，相比Double DQN和经典Rainbow DQN提升了4.9-13.4%的性能

Conclusion: 量子增强的深度强化学习在大规模资源分配中具有潜力，电路表达能力、纠缠与策略质量之间的理论联系得到了验证

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [19] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的大规模合成基准测试，支持无限参数配置，包含三种题型和创新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 为了评估语言模型在科学推理方面的能力，需要创建大规模、参数化、多样化的基准测试，以全面测试模型在不同物理问题类型上的表现。

Method: 创建了包含15,045个大学物理问题的合成基准测试，每个问题完全参数化，支持无限输入配置。包含三种题型：MC-Symbolic（符号多项选择）、MC-Numerical（数值多项选择）和自由形式（开放式回答）。通过代码驱动方法生成结构化逐步推理和可执行Python代码作为真实解。

Result: 实验使用最先进的指令调优语言模型进行测试，揭示了模型在科学推理方面的优势和局限性。除了标准准确率外，还引入了三个新颖评估指标：一致性分数、失败率和混淆率，用于量化跨问题变体的可变性和不确定性。

Conclusion: SymPyBench为开发更鲁棒和可解释的推理系统奠定了基础，能够全面评估语言模型在复杂物理问题解决中的表现，并识别其推理能力的局限性。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [A Practical Honeypot-Based Threat Intelligence Framework for Cyber Defence in the Cloud](https://arxiv.org/abs/2512.05321)
*Darren Malvern Chin,Bilal Isfaq,Simon Yusuf Enoch*

Main category: cs.CR

TL;DR: 本文提出了一种基于蜜罐遥测的自动化云防火墙防御框架，利用Azure原生工具实现实时威胁检测和动态规则更新，显著缩短了阻断时间并提高了威胁分类准确性。


<details>
  <summary>Details</summary>
Motivation: 传统防火墙依赖预定义规则和手动配置，难以有效应对云环境中不断演变的零日威胁和高级持续性攻击，特别是在Microsoft Azure等云平台中，这种静态防御模型使云资产面临严重安全风险。

Method: 开发了一个自动化防御框架，整合了中高交互蜜罐（Cowrie）遥测、Azure原生自动化工具（Monitor、Sentinel、Logic Apps）以及基于MITRE ATT&CK框架的检测机制，通过闭环反馈机制实现实时防火墙规则动态更新。

Result: 实验结果显示平均阻断时间为0.86秒，显著快于基准系统；准确分类了超过12,000次SSH尝试，覆盖多个MITRE ATT&CK战术；有效减少了攻击者驻留时间，增强了安全运营中心可见性。

Conclusion: 将蜜罐遥测与Azure原生自动化相结合，为现代云基础设施提供了一个可扩展、可操作的防御模型，能够显著提升云环境的安全防护能力。

Abstract: In cloud environments, conventional firewalls rely on predefined rules and manual configurations, limiting their ability to respond effectively to evolving or zero-day threats. As organizations increasingly adopt platforms such as Microsoft Azure, this static defense model exposes cloud assets to zero-day exploits, botnets, and advanced persistent threats. In this paper, we introduce an automated defense framework that leverages medium- to high-interaction honeypot telemetry to dynamically update firewall rules in real time. The framework integrates deception sensors (Cowrie), Azure-native automation tools (Monitor, Sentinel, Logic Apps), and MITRE ATT&CK-aligned detection within a closed-loop feedback mechanism. We developed a testbed to automatically observe adversary tactics, classify them using the MITRE ATT&CK framework, and mitigate network-level threats automatically with minimal human intervention.
  To assess the framework's effectiveness, we defined and applied a set of attack- and defense-oriented security metrics. Building on existing adaptive defense strategies, our solution extends automated capabilities into cloud-native environments. The experimental results show an average Mean Time to Block of 0.86 seconds - significantly faster than benchmark systems - while accurately classifying over 12,000 SSH attempts across multiple MITRE ATT&CK tactics. These findings demonstrate that integrating deception telemetry with Azure-native automation reduces attacker dwell time, enhances SOC visibility, and provides a scalable, actionable defense model for modern cloud infrastructures.

</details>


### [21] [PrivCode: When Code Generation Meets Differential Privacy](https://arxiv.org/abs/2512.05459)
*Zheng Liu,Chen Gong,Terry Yue Zhuo,Kecen Li,Weichen Yu,Matt Fredrikson,Tianhao Wang*

Main category: cs.CR

TL;DR: PrivCode是首个专门为代码数据集设计的差分隐私合成器，采用两阶段框架平衡隐私与效用，在保护敏感代码的同时生成高质量代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成方面表现出色，但在私有数据集上微调会引发隐私和专有信息泄露问题。差分隐私代码生成虽能提供理论保护，但面临严格语法依赖和隐私-效用权衡的挑战。

Method: PrivCode采用两阶段框架：1) 隐私净化阶段：使用DP-SGD训练模型生成差分隐私合规的合成代码，同时引入语法信息保持代码结构；2) 效用提升阶段：在合成隐私无关代码上微调更大的预训练LLM，缓解DP造成的效用损失。

Result: 在四个LLM上的广泛实验表明，PrivCode在四个基准测试的各种任务中生成更高效用的代码。实验还证实了在不同隐私预算下保护敏感数据的能力。

Conclusion: PrivCode是首个专门为代码数据集设计的差分隐私合成器，通过两阶段框架有效平衡隐私保护与代码生成质量，为隐私敏感的代码生成任务提供了实用解决方案。

Abstract: Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.
  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed "privacy-sanitizing", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed "utility-boosting", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.

</details>


### [22] [TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations](https://arxiv.org/abs/2512.05485)
*Xiuyuan Chen,Jian Zhao,Yuxiang He,Yuan Xun,Xinwei Liu,Yanshu Li,Huilin Zhou,Wei Cai,Ziyan Shi,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.CR

TL;DR: TeleAI-Safety是一个模块化、可复现的LLM安全评估框架和基准测试，集成了多种攻击、防御和评估方法，用于系统评估大语言模型在越狱和提示攻击下的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估存在两个主要问题：1）核心组件（攻击、防御、评估方法）集成不平衡；2）灵活评估框架与标准化基准能力之间的隔离。这些问题阻碍了可靠的跨研究比较，并为全面风险评估带来了不必要的开销。

Method: 提出了TeleAI-Safety框架，集成了19种攻击方法（包括1种自研方法）、29种防御方法和19种评估方法（包括1种自研方法）。使用包含12个风险类别、342个样本的攻击语料库，在14个目标模型上进行广泛评估。

Result: 评估结果揭示了系统性的漏洞和模型特定的失败案例，突出了安全性与实用性之间的关键权衡，并识别了未来优化的潜在防御模式。

Conclusion: TeleAI-Safety提供了一个灵活可调整的框架，可根据特定需求定制攻击、防御和评估组合。作者发布了完整代码和评估结果，以促进可复现研究并建立统一的安全基准。

Abstract: While the deployment of large language models (LLMs) in high-value industries continues to expand, the systematic assessment of their safety against jailbreak and prompt-based attacks remains insufficient. Existing safety evaluation benchmarks and frameworks are often limited by an imbalanced integration of core components (attack, defense, and evaluation methods) and an isolation between flexible evaluation frameworks and standardized benchmarking capabilities. These limitations hinder reliable cross-study comparisons and create unnecessary overhead for comprehensive risk assessment. To address these gaps, we present TeleAI-Safety, a modular and reproducible framework coupled with a systematic benchmark for rigorous LLM safety evaluation. Our framework integrates a broad collection of 19 attack methods (including one self-developed method), 29 defense methods, and 19 evaluation methods (including one self-developed method). With a curated attack corpus of 342 samples spanning 12 distinct risk categories, the TeleAI-Safety benchmark conducts extensive evaluations across 14 target models. The results reveal systematic vulnerabilities and model-specific failure cases, highlighting critical trade-offs between safety and utility, and identifying potential defense patterns for future optimization. In practical scenarios, TeleAI-Safety can be flexibly adjusted with customized attack, defense, and evaluation combinations to meet specific demands. We release our complete code and evaluation results to facilitate reproducible research and establish unified safety baselines.

</details>


### [23] [Matching Ranks Over Probability Yields Truly Deep Safety Alignment](https://arxiv.org/abs/2512.05518)
*Jason Vega,Gagandeep Singh*

Main category: cs.CR

TL;DR: 本文提出了一种名为RAP的新攻击方法，能够绕过基于数据增强的SFT安全对齐防御，并提出了PRESTO解决方案来改进安全对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据增强的SFT防御方法虽然声称实现了"深度"安全对齐，但实际上仍然存在漏洞。研究发现这些模型在面对更复杂的攻击时仍然容易泄露有害内容，需要更有效的安全对齐方法。

Method: 1. 提出了Rank-Assisted Prefilling (RAP)攻击方法，通过从top-20预测令牌中选择低概率的有害令牌来绕过SFT防御；2. 提出了PRefill attEntion STOpping (PRESTO)解决方案，通过正则化对有害预填充令牌的关注来匹配目标分布的令牌排名而非概率。

Result: PRESTO方法在三个流行的开源LLM上，在RAP攻击下的平均StrongREJECT分数提高了4.7倍，同时对模型实用性影响较小。

Conclusion: 基于概率匹配的SFT安全对齐方法存在根本性缺陷，而基于令牌排名匹配的方法能提供更有效的深度安全对齐。PRESTO是一个简单但有效的改进方案。

Abstract: A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the "deep" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability "harmful" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability "refusal" tokens). We argue that this vulnerability is enabled due to the "gaming" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.

</details>


### [24] [ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior](https://arxiv.org/abs/2512.05745)
*Weikai Lu,Ziqian Zeng,Kehua Zhang,Haoran Li,Huiping Zhuang,Ruidong Wang,Cen Chen,Hao Peng*

Main category: cs.CR

TL;DR: ARGUS是一种针对多模态间接提示注入攻击的防御方法，通过在表示空间中寻找最优防御方向，结合自适应强度调节和轻量级检测机制，实现安全性与模型效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型面临多模态间接提示注入攻击的威胁，现有防御方法主要针对文本模型，在多模态场景下容易被绕过、依赖特定模态或泛化能力差。

Method: 基于激活导向研究，发现MLLM的指令跟随行为编码在子空间中。ARGUS在安全子空间中搜索与效用退化方向解耦的最优防御方向，结合自适应强度调节、轻量级注入检测和防御后验证。

Result: 实验结果表明，ARGUS能够实现对多模态IPI攻击的鲁棒防御，同时最大程度地保留MLLM的效用。

Conclusion: 通过表示空间中的行为导向，可以构建独立于模态的通用防御机制，ARGUS为解决多模态间接提示注入攻击提供了有效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [25] [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)
*Chengjie Ma,Seungeun Oh,Jihong Park,Seong-Lyun Kim*

Main category: cs.DC

TL;DR: FedGMR通过渐进式模型恢复解决联邦学习中带宽受限客户端因模型过小而导致的收敛慢和泛化差问题，在异步异构环境下实现更快的收敛和更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在异构环境中面临带宽受限客户端参与效率低的问题，这些客户端的小模型在初期学习快但后期参数不足，导致收敛缓慢和泛化性能下降。

Method: 提出FedGMR框架，在训练过程中渐进式增加每个客户端的子模型密度，使带宽受限客户端能持续有效参与；开发了针对异步异构联邦学习的掩码感知聚合规则。

Result: 收敛性分析证明聚合误差与客户端和轮次的平均子模型密度成比例，而GMR能有效缩小与完整模型联邦学习的差距。在FEMNIST、CIFAR-10和ImageNet-100上的实验显示FedGMR实现了更快的收敛和更高的准确率，特别是在高异构性和非IID设置下。

Conclusion: FedGMR通过渐进式模型恢复有效解决了带宽受限客户端在联邦学习中的参与问题，在异步异构环境下显著提升了学习效率和模型性能。

Abstract: Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

</details>


### [26] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: 研究基于真实轨迹验证公交车搭载边缘服务器的可行性，通过上海数据集分析公交车覆盖范围，设计贪心算法选择有限数量公交车最大化需求点覆盖，证明公交车边缘服务器在城市场景中可行且有益。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置边缘服务器（如路侧单元或基站）部署后位置和容量固定，难以处理时空动态变化的用户需求。公交车等移动服务器具有增加计算弹性的潜力，需要验证其在实际场景中的可行性。

Method: 使用上海公交车/出租车/电信数据集分析公交车和基站的覆盖范围；建立数学模型并设计贪心启发式算法，在有限预算下选择最优公交车集合以最大化需求点覆盖；进行基于轨迹的仿真验证算法性能。

Result: 公交车覆盖了大部分地理区域和需求点，显示出作为边缘服务器的巨大潜力；提出的贪心算法能有效处理动态用户需求，在服务器容量和购买数量等现实约束下表现良好。

Conclusion: 公交车搭载的边缘服务器在城市车联网场景中是可行、有益且有价值的，能够为动态变化的车辆用户提供弹性计算服务。

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>
