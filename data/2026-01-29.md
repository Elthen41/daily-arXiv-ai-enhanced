<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 14]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: Discontinuous DLS是一种数据驱动的有损压缩方法，利用数据结构的局部时空子空间提高压缩效率，在保持关键特征的同时显著减少科学模拟数据的存储需求。


<details>
  <summary>Details</summary>
Motivation: 科学模拟数据量的快速增长给存储和传输带来了重大挑战，需要有效的压缩解决方案。虽然有损压缩可以减小数据大小，但需要确保重建数据对科学分析仍然有效。

Method: Discontinuous DLS是一种数据驱动的有损压缩器，利用基于底层数据结构信息的局部时空子空间来提高压缩效率。该方法灵活适用于各种科学数据，包括流体动力学、环境模拟等。在分布式计算环境中使用MPI实现。

Result: 该方法能够显著减少存储需求而不损害关键数据保真度。与最先进的误差有界压缩方法相比，在压缩比和重建精度方面表现出优越性能。

Conclusion: Discontinuous DLS是大规模科学数据压缩的有前景方法，为管理现代科学模拟日益增长的数据需求提供了稳健解决方案。

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [2] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 提出统一竞争分类框架，通过表示转换、结构建模和任务解耦机制，在高维系统环境中准确识别多任务竞争类型


<details>
  <summary>Details</summary>
Motivation: 解决高维系统环境中准确识别多任务竞争类型的挑战，为复杂计算环境中的性能管理提供可靠技术方案

Method: 构建系统状态表示，应用非线性变换提取跨维度动态特征；引入基于图的建模机制捕捉指标间潜在依赖；设计任务特定映射结构；采用自适应多任务损失加权策略

Result: 在公开系统跟踪数据集上实验显示在准确率、召回率、精确率和F1分数方面具有优势；对批次大小、训练样本规模和指标维度的敏感性分析证实模型稳定性和适用性

Conclusion: 基于高维指标的结构化表示和多任务分类能显著改善竞争模式识别，为复杂计算环境性能管理提供可靠技术途径

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [3] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: OptiKIT是一个分布式LLM优化框架，通过自动化复杂优化流程，让非专业团队也能进行模型压缩和调优，在GPU资源受限的情况下实现2倍以上的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 企业部署大语言模型面临可扩展性挑战：需要在有限的计算预算内系统化优化模型，但手动优化所需的专业知识和技能稀缺且难以获取。特别是在异构基础设施中管理GPU利用率，同时让具有不同工作负载和有限LLM优化经验的团队能够高效部署模型。

Method: OptiKIT采用分布式LLM优化框架设计，提供动态资源分配、分阶段管道执行与自动清理、无缝企业集成等功能。通过资源分配算法、管道编排和集成模式的工程实现，实现大规模生产级的模型优化民主化。

Result: 在生产环境中，OptiKIT实现了超过2倍的GPU吞吐量提升，使应用团队无需深厚的LLM优化专业知识就能获得一致的性能改进。该系统已开源以支持外部贡献和更广泛的可复现性。

Conclusion: OptiKIT成功解决了企业LLM部署中的可扩展性挑战，通过自动化复杂优化工作流程，使非专业团队也能高效进行模型优化，显著提升了GPU资源利用率，并开源系统以促进社区发展。

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [4] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: 本文提出了用户空间调度框架USF及其默认协作策略SCHED_COOP，通过在用户空间实现进程调度，减少操作系统调度器在过载情况下的干扰，提升并行应用的性能。


<details>
  <summary>Details</summary>
Motivation: 高性能计算与人工智能的融合催生了复杂的并行应用，这些应用往往包含多个并行运行时，给传统操作系统调度器带来巨大压力。在过载情况下，操作系统调度器依赖周期性抢占来复用核心，但会引入干扰导致性能下降。

Method: 提出了用户空间调度框架USF，完全在用户空间实现进程调度，允许用户自定义调度算法而无需特殊权限。基于USF实现了默认协作策略SCHED_COOP，仅在线程阻塞时切换线程，避免不必要的抢占。通过扩展GNU C库和nOS-V运行时实现，支持跨多个运行时（如OpenMP）的无缝协调。

Result: 评估显示，在过载的多进程场景中，包括嵌套BLAS工作负载、多进程PyTorch推理（LLaMA-3）和分子动力学模拟，性能提升最高可达2.4倍。有效缓解了锁持有者抢占、锁等待者抢占和可扩展性崩溃等问题。

Conclusion: USF和SCHED_COOP提供了一种有效的用户空间调度解决方案，能够显著减少操作系统调度器在过载情况下的干扰，提升复杂并行应用的性能，且无需对应用进行侵入性修改。

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [5] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: AutoOverlap是一个编译器系统，通过在单个融合内核内实现细粒度通信与计算重叠，解决大规模GPU工作负载中的通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有分布式编译器主要通过流级别的粗粒度重叠来处理通信瓶颈，但这种方法会引入额外的内核启动、强制设备级同步，并且在最慢的瓦片或内核拉伸通信尾部时产生大量空闲时间。

Method: AutoOverlap引入了通信块抽象，将通信粒度与内核结构和后端机制解耦，允许从现有分布式编译器移植块级计划、用户直接编写或从可重用模板实例化。给定本地Triton内核和块调度，AutoOverlap执行转换以使计算与块可用性对齐，作为Triton的源到源编译器实现。

Result: 在Triton上实现的AutoOverlap在多GPU工作负载上实现了平均1.3倍、最高4.7倍的端到端加速。

Conclusion: AutoOverlap通过细粒度重叠技术有效解决了大规模GPU工作负载中的通信瓶颈问题，相比现有粗粒度方法具有显著性能优势。

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [6] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: OnePiece是一个针对多阶段AI生成内容工作流优化的分布式推理系统，通过RDMA通信和微服务架构显著提升吞吐量和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成内容系统在处理并发工作负载时面临吞吐量、资源利用率和可扩展性方面的关键效率问题，需要更高效的分布式推理解决方案。

Method: 将流水线分解为细粒度微服务，利用单边RDMA通信减少节点间延迟和CPU开销；采用新颖的双环形缓冲区设计解决RDMA感知内存访问的死锁问题；引入动态节点管理器根据实时负载弹性分配资源。

Result: 在Wan2.1图像到视频生成任务中，相比单体推理流水线，OnePiece将GPU资源消耗降低了16倍，提供了可扩展、容错且高效的AIGC生产环境解决方案。

Conclusion: OnePiece通过RDMA优化的分布式架构和创新的内存管理机制，为大规模AI生成内容工作流提供了显著提升的性能和资源效率，适合生产环境部署。

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [Flexible Bit-Truncation Memory for Approximate Applications on the Edge](https://arxiv.org/abs/2601.19900)
*William Oswald,Mario Renteria-Pinon,Md. Sajjad Hossain,Kyle Mooney,Md. Bipul Hossain,Destinie Diggs,Yiwen Xu,Mohamed Shaban,Jinhui Wang,Na Gong*

Main category: cs.AR

TL;DR: 提出一种具有完全自适应灵活性的比特截断存储器，可在运行时截断任意数量的数据位，以满足不同近似应用的质量与功耗权衡需求，应用于视频处理和深度学习，实现显著的功耗节省。


<details>
  <summary>Details</summary>
Motivation: 现有比特截断存储器需要为特定应用定制设计，缺乏灵活性。需要一种能够适应多种近似应用、在运行时灵活调整比特截断数量的存储器，以优化边缘环境中的功耗/能效。

Method: 开发了一种新型比特截断存储器，具有完全自适应灵活性，能够在运行时截断任意数量的数据位。该方法支持不同质量与功耗权衡需求，适用于多种近似应用。

Result: 实验表明：1）支持三种视频应用（亮度感知、内容感知和感兴趣区域感知），功耗效率提升，相比最先进技术节省高达47.02%功耗；2）在基准和剪枝轻量级深度学习模型中分别实现高达51.69%的功耗节省；3）实现成本低，仅增加2.89%的硅面积开销。

Conclusion: 提出的比特截断存储器具有完全自适应灵活性，能够有效支持多种近似应用，在视频处理和深度学习任务中实现显著的功耗节省，同时保持较低的实现成本，为边缘计算环境中的能效优化提供了有效解决方案。

Abstract: Bit truncation has demonstrated great potential to enable run-time quality-power adaptive data storage, thereby optimizing the power/energy efficiency of approximate applications and supporting their deployment in edge environments. However, existing bit-truncation memories require custom designs for a specific application. In this paper, we present a novel bit-truncation memory with full adaptation flexibility, which can truncate any number of data bits at run time to meet different quality and power trade-off requirements for various approximate applications. The developed bit-truncation memory has been applied to two representative data-intensive approximate applications: video processing and deep learning. Our experiments show that the proposed memory can support three different video applications (including luminance-aware, content-aware, and region-of-interest-aware) with enhanced power efficiency (up to 47.02% power savings) as compared to state-of-the-art. In addition, the proposed memory achieves significant (up to 51.69%) power savings for both baseline and pruned lightweight deep learning models, respectively, with a low implementation cost (2.89% silicon area overhead).

</details>


### [8] [A Flower-Inspired Solution for Computer Memory Wear-Leveling](https://arxiv.org/abs/2601.19902)
*Elizabeth Shen,Huiyang Zhou*

Main category: cs.AR

TL;DR: 提出双环磨损均衡方法，利用黄金比例原理优化内存磨损分布，延长内存寿命


<details>
  <summary>Details</summary>
Motivation: 延长计算机内存寿命对减少电子垃圾和可持续发展至关重要。内存磨损不均是一个主要障碍，而新兴存储器如相变存储器的寿命更短，问题更加紧迫。现有解决方案要么需要复杂的硬件扩展，要么只适用于特定程序结构如循环。

Method: 提出双环磨损均衡方法，灵感来源于黄金比例的自然法则及其帮助花瓣均匀接收阳光的原理。将内存建模为两个环，并结合现有的内存管理和垃圾回收机制，提供了一种有效减少内存磨损的方法。

Result: 该方法具有确定性，能够自动适应内存大小，无需硬件更改，且不会增加程序执行的开销。

Conclusion: 双环磨损均衡方法为解决内存磨损不均问题提供了一种有效、实用且无需硬件修改的解决方案，有助于延长内存寿命。

Abstract: Lengthening a computer memory's lifespan is important for e-waste and sustainability. Uneven wear of memory is a major barrier. The problem is becoming even more urgent as emerging memory such as phase-change memory is subject to even shorter lifespan. Various solutions have been proposed, but they either require complicated hardware extensions or apply only to certain program constructs such as loops. This research proposes a new method, dual-ring wear leveling. It takes inspiration from the natural law known as the ``golden ratio" and how it helps flower petals evenly receive sun lights. By modeling memory as two rings and combines the idea with existing memory management, garbage collection, the new solution offers an effective way to reduce memory wear and hence lengthen memory lifespan. It is deterministic, able to automatically adapt to memory size, requiring no hardware changes, and adding no slowdown to program executions.

</details>


### [9] [STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification](https://arxiv.org/abs/2601.19903)
*Saeid Rajabi,Chengmo Yang,Satwik Patnaik*

Main category: cs.AR

TL;DR: STELLAR是一个基于结构相似性指导LLM生成SystemVerilog断言的框架，通过AST结构指纹检索相关知识库中的(RTL, SVA)对，显著提高了断言生成的语法正确性、风格一致性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 手动编写SystemVerilog断言(SVAs)过程缓慢且容易出错，而现有的LLM方法要么从头生成断言，要么忽略了硬件设计中的结构模式和专家编写的断言模式，需要一种更有效的方法来生成高质量的断言。

Method: STELLAR框架将RTL块表示为AST结构指纹，从知识库中检索结构相关的(RTL, SVA)对，并将它们集成到结构引导的提示中，指导LLM生成断言。

Result: 实验表明STELLAR在语法正确性、风格对齐和功能正确性方面表现优异，证明了结构感知检索是工业形式验证的一个有前景的方向。

Conclusion: STELLAR通过结构相似性指导LLM生成SystemVerilog断言，显著提高了断言质量，为工业形式验证提供了有效的自动化解决方案。

Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.

</details>


### [10] [DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs](https://arxiv.org/abs/2601.19904)
*Ziyu Hu,Zhiqing Zhong,Weijian Zheng,Zhijing Ye,Xuwei Tan,Xueru Zhang,Zheng Xie,Rajkumar Kettimuthu,Xiaodong Yu*

Main category: cs.AR

TL;DR: DABench-LLM是首个针对数据流AI加速器的LLM训练基准测试框架，通过芯片内性能分析和芯片间可扩展性分析，全面评估资源分配、负载平衡和资源效率等关键指标。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的指数级增长，传统CPU/GPU架构受摩尔定律放缓限制，数据流AI加速器成为有前景的替代方案，但缺乏深入的性能分析和标准化基准测试方法。

Method: 开发DABench-LLM基准测试框架，结合芯片内性能分析和芯片间可扩展性分析，在Cerebras WSE-2、SambaNova RDU和Graphcore IPU三种商用数据流加速器上进行验证。

Result: 框架揭示了性能瓶颈并提供了具体的优化策略，证明了其在多样化数据流AI硬件平台上的通用性和有效性。

Conclusion: DABench-LLM为研究人员提供了深入了解底层硬件和系统行为的工具，并为性能优化提供指导，填补了数据流加速器LLM训练基准测试的空白。

Abstract: The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.

</details>


### [11] [GTAC: A Generative Transformer for Approximate Circuits](https://arxiv.org/abs/2601.19906)
*Jingxin Wang,Shitong Guo,Ruicheng Dai,Wenhui Liang,Ruogu Ding,Xin Ning,Weikang Qian*

Main category: cs.AR

TL;DR: GTAC是一种基于生成式Transformer的近似电路设计模型，通过集成误差阈值到设计流程中，在满足误差约束条件下进一步减少6.4%的面积，同时速度提升4.3倍。


<details>
  <summary>Details</summary>
Motivation: 针对容错应用，近似电路通过引入可控误差来显著改善电路的性能、功耗和面积。现有方法在平衡误差约束和PPA优化方面仍有改进空间，需要更高效的设计方法。

Method: 提出GTAC，一种基于生成式Transformer的模型，利用近似计算和AI驱动的EDA原理，创新性地将误差阈值集成到设计过程中，实现自动化近似电路生成。

Result: 与最先进方法相比，GTAC在满足误差率约束条件下进一步减少了6.4%的面积，同时设计速度提升了4.3倍。

Conclusion: GTAC展示了生成式Transformer在近似电路设计中的有效性，为AI驱动的EDA工具提供了新的方向，能够在保证误差约束的同时显著优化电路PPA指标。

Abstract: Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.

</details>


### [12] [RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs](https://arxiv.org/abs/2601.19907)
*Yanru Chen,Zheyu Li,Keming Fan,Runyang Tian,John Hsu,Weihong Xu,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: RAPID-Graph是一个通过存内计算(PIM)系统解决全对最短路径(APSP)计算瓶颈的方案，在算法、架构和设备层面进行协同设计，相比GPU集群和现有PIM加速器实现了显著的速度和能效提升。


<details>
  <summary>Details</summary>
Motivation: 全对最短路径(APSP)计算在大规模图分析中面临严重瓶颈，立方级复杂度的数据移动会淹没传统内存层次结构的带宽，需要新的解决方案来突破这一限制。

Method: 1. 算法层面：提出递归感知分区器，将图分解为顶点瓦片以减少数据依赖，使Floyd-Warshall和Min-Plus内核完全在数字PIM阵列中原地执行；2. 架构层面：设计2.5D PIM堆栈，集成两个相变内存计算芯片、一个逻辑芯片和高带宽暂存内存；3. 设备层面：使用外部非易失性存储堆栈持久存储大型APSP结果，支持瓦片级和单元级并行处理。

Result: 在2.45M节点的OGBN-Products数据集上，RAPID-Graph相比最先进的GPU集群快5.8倍、能效高1186倍，相比现有PIM加速器快8.3倍、能效高104倍，相比NVIDIA H100 GPU最高实现42.8倍加速和392倍节能。

Conclusion: RAPID-Graph通过算法、架构和设备层面的协同设计，成功解决了大规模图分析中APSP计算的数据移动瓶颈，在速度和能效方面实现了显著突破，为大规模图计算提供了高效的存内计算解决方案。

Abstract: All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.

</details>


### [13] [CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference](https://arxiv.org/abs/2601.19908)
*Yanru Chen,Runyang Tian,Yue Pan,Zheyu Li,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: CHIME是一种基于小芯片的异构近内存加速器，用于边缘设备上的多模态大语言模型推理，通过结合M3D DRAM和RRAM小芯片的优势，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在边缘设备的部署，面临着严格的延迟和能耗约束，特别是多模态LLMs需要处理高维视觉输入，产生大量token序列，导致KV缓存膨胀和数据移动开销巨大。

Method: CHIME采用基于小芯片的异构近内存加速架构，结合单片3D DRAM（提供低延迟带宽用于注意力计算）和RRAM（提供密集非易失性存储用于权重），并通过协同设计的映射框架执行融合内核，最小化跨小芯片流量。

Result: 在FastVLM和MobileVLM模型上，相比NVIDIA Jetson Orin NX边缘GPU，CHIME实现了高达54倍加速和246倍能效提升；相比最先进的PIM加速器FACIL，吞吐量提升高达69.2倍；相比纯M3D DRAM设计，能效提升7%，性能提升2.4倍。

Conclusion: CHIME通过异构内存架构和近内存计算设计，有效解决了边缘设备上多模态LLMs推理的延迟和能耗挑战，为边缘AI部署提供了高效的硬件解决方案。

Abstract: The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.

</details>


### [14] [Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading](https://arxiv.org/abs/2601.19910)
*William Meng,Benjamin Lee,Hong Wang*

Main category: cs.AR

TL;DR: KV缓存卸载技术通过将缓存存储在CPU DRAM中实现长上下文LLM推理，但PCIe带宽限制造成严重瓶颈。本文开发分析框架推导临界缓存-预填充令牌比κ_crit，发现典型工作负载远超此阈值，99%延迟用于数据传输，GPU仅消耗28%额定TDP，提出硬件互连、模型架构和调度算法优化方案。


<details>
  <summary>Details</summary>
Motivation: KV缓存卸载技术虽然能支持长上下文LLM推理，但受到PCIe带宽限制的严重瓶颈影响，导致性能下降。需要深入分析瓶颈原因并找到优化方案。

Method: 开发分析框架推导临界缓存-预填充令牌比κ_crit，通过经验表征分析实际工作负载，测量延迟分布和GPU功耗情况。

Result: 发现典型工作负载远超临界阈值，99%的延迟时间用于数据传输，GPU在服务卸载请求时仅消耗28%的额定热设计功耗，表明系统严重受限于内存带宽而非计算能力。

Conclusion: 针对KV缓存卸载的瓶颈问题，提出需要在硬件互连、模型架构和调度算法三个层面进行优化，以提升长上下文LLM推理的性能效率。

Abstract: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $κ_{\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.

</details>


### [15] [Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation](https://arxiv.org/abs/2601.19941)
*M Zafir Sadik Khan,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: Bench4HLS是一个用于评估LLM生成HLS设计的基准测试框架，包含170个手动编写和验证的案例，支持自动化评估编译成功、功能正确性和综合可行性，并集成了可插拔的PPA分析API。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码生成（包括RTL硬件设计）方面展现出强大能力，其在HLS领域的应用虽然相对不成熟但兴趣日益增长（HLS与RTL研究的比例从1:10变为2:10）。这凸显了需要一个专门用于LLM-based HLS的全面基准测试和评估框架。

Method: 提出了Bench4HLS框架，包含170个从广泛使用的公共仓库中精心挑选的手动编写和验证的案例研究，涵盖从小型内核到复杂加速器。框架支持完全自动化的编译成功评估、通过仿真的功能正确性验证以及综合可行性/优化评估。关键的是集成了可插拔API，用于跨不同HLS工具链和架构的功耗、性能和面积（PPA）分析。

Result: 该框架已在Xilinx Vitis HLS上演示，并在Catapult HLS上验证，提供了一个结构化、可扩展和即插即用的测试平台。Bench4HLS为在HLS工作流程中基准测试LLM建立了基础方法学。

Conclusion: Bench4HLS通过提供全面的基准测试框架，解决了LLM在HLS领域评估的迫切需求，为未来LLM-based HLS研究和发展奠定了重要基础。

Abstract: In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.

</details>


### [16] [A Paradigm for Generalized Multi-Level Priority Encoders](https://arxiv.org/abs/2601.20067)
*Maxwell Phillips,Firas Hassan,Ahmed Ammar*

Main category: cs.AR

TL;DR: 本文提出了一种新的优先级编码器设计范式，通过将原有的两级结构扩展到三、四级，分析不同架构在复杂度和延迟上的权衡，为硬件设计者提供优化建议。


<details>
  <summary>Details</summary>
Motivation: 传统优先级编码器在高位精度或长输入（如512位以上）时硬件复杂度高，限制了其在关键应用（如高精度整数运算和内容可寻址存储器）中的使用。降低复杂度可以使优先级编码器更可行地加速这些应用。

Method: 提出新的优先级编码器构造范式，将原有的两级结构通过级联和组合两种技术扩展到三、四级，并讨论进一步泛化。分析新设计和现有设计的复杂度与延迟，比较多级结构与传统的单级结构、树形设计、递归设计以及两级结构。

Result: 两级架构提供了平衡的性能——复杂度降低约一半，但延迟相应增加。更多级别带来的收益递减，突显了复杂度与延迟之间的权衡。树形和递归设计通常更快，但比两级和多级结构更复杂。研究在不同输入长度下的设计特性，并为不同实现技术提供架构选择建议。

Conclusion: 通过分析和比较各种优先级编码器架构，为硬件设计者提供了优先级编码器工具包，帮助他们根据具体需求（复杂度或延迟优先）选择最优设计，平衡性能与资源消耗。

Abstract: Priority encoders are typically considered expensive hardware components in terms of complexity, especially at high bit precisions or input lengths (e.g., above 512 bits). However, if the complexity can be reduced, priority encoders can feasibly accelerate a variety of key applications, such as high-precision integer arithmetic and content-addressable memory. We propose a new paradigm for constructing priority encoders by generalizing the previously proposed two-level priority encoder structure. We extend this concept to three and four levels using two techniques -- cascading and composition -- and discuss further generalization. We then analyze the complexity and delay of new and existing priority encoder designs as a function of input length, for both FPGA and ASIC implementation technologies. In particular, we compare the multi-level structure to the traditional single-level priority encoder structure, a tree-based design, a recursive design, and the two-level structure. We find that the two-level architecture provides balanced performance -- reducing complexity by around half, but at the cost of a corresponding increase in delay. Additional levels have diminishing returns, highlighting a tradeoff between complexity and delay. Meanwhile, the tree and recursive designs are generally faster, but are more complex than the two-level and multi-level structures. We explore several characteristics and patterns of the designs across a wide range of input lengths. We then provide recommendations on which architecture to use for a given input length and implementation technology, based on which design factors -- such as complexity or delay -- are most important to the hardware designer. With this overview and analysis of various priority encoder architectures, we provide a priority encoder toolkit to assist hardware designers in creating the most optimal design.

</details>


### [17] [How Much Progress Has There Been in NVIDIA Datacenter GPUs?](https://arxiv.org/abs/2601.20115)
*Emanuele Del Sozzo,Martin Fleming,Kenneth Flamm,Neil Thompson*

Main category: cs.AR

TL;DR: 该研究分析了NVIDIA数据中心GPU从2000年代中期至今的技术进步趋势，量化了各项性能指标的倍增时间，并评估了美国出口管制对AI芯片性能差距的影响。


<details>
  <summary>Details</summary>
Motivation: 随着GPU在AI等关键领域的广泛应用，分析其技术演进趋势对预测未来科学研究的限制至关重要。特别是在美国实施AI芯片出口管制的背景下，了解GPU技术进步的速度和模式具有重要现实意义。

Method: 研究收集了NVIDIA数据中心GPU的全面数据集，涵盖计算性能、价格等多种特征。通过分析主要GPU特征的趋势，估算每内存带宽、每美元和每瓦特的进步指标，并量化出口管制可能导致的性能差距。

Result: FP16和FP32运算的倍增时间为1.44-1.69年，FP64为2.06-3.79年。内存大小和带宽增长较慢（3.32-3.53年）。数据中心GPU价格每5.1年翻倍，功耗每16年翻倍。出口管制调整后，潜在性能差距从23.6倍缩小到3.54倍。

Conclusion: GPU计算性能的增长速度远超内存带宽和价格增长，揭示了技术发展的不平衡性。美国出口管制政策对AI芯片性能差距有显著影响，但近期调整将大幅缩小这一差距，对全球AI竞争格局产生重要影响。

Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.

</details>


### [18] [SATA: Sparsity-Aware Scheduling for Selective Token Attention](https://arxiv.org/abs/2601.20267)
*Zhenkun Fan,Zishen Wan,Che-Kai Liu,Ashwin Sanjay Lele,Win-San Khwa,Bo Zhang,Meng-Fan Chang,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: SATA是一种针对选择性注意力机制中稀疏访问模式的动态调度方案，通过重新排序操作数流和利用数据局部性，提升系统吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制具有二次复杂度，对硬件实现构成挑战。选择性令牌注意力通过聚焦相关令牌来减少计算，但其稀疏访问模式需要有效的调度管理。

Method: 提出SATA方案，采用以局部性为中心的动态调度策略，主动管理选择性Query-Key操作产生的稀疏访问模式，通过重新排序操作数流、利用数据局部性实现中间Query/Key向量的早期获取和释放。

Result: 在基于选择性注意力模型的运行时轨迹上进行实验，结果显示系统吞吐量最高提升1.76倍，能效提升2.94倍，调度开销最小。

Conclusion: SATA方案通过有效的动态调度管理选择性注意力中的稀疏访问模式，显著提升了系统性能和能效，为Transformer硬件实现提供了有前景的优化方案。

Abstract: Transformers have become the foundation of numerous state-of-the-art AI models across diverse domains, thanks to their powerful attention mechanism for modeling long-range dependencies. However, the quadratic scaling complexity of attention poses significant challenges for efficient hardware implementation. While techniques such as quantization and pruning help mitigate this issue, selective token attention offers a promising alternative by narrowing the attention scope to only the most relevant tokens, reducing computation and filtering out noise.
  In this work, we propose SATA, a locality-centric dynamic scheduling scheme that proactively manages sparsely distributed access patterns from selective Query-Key operations. By reordering operand flow and exploiting data locality, our approach enables early fetch and retirement of intermediate Query/Key vectors, improving system utilization. We implement and evaluate our token management strategy in a control and compute system, using runtime traces from selective-attention-based models. Experimental results show that our method improves system throughput by up to 1.76x and boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.

</details>


### [19] [VersaQ-3D: A Reconfigurable Accelerator Enabling Feed-Forward and Generalizable 3D Reconstruction via Versatile Quantization](https://arxiv.org/abs/2601.20317)
*Yipu Zhang,Jintao Cheng,Xingyu Liu,Zeyu Li,Carol Jingyi Li,Jin Wu,Lin Jiang,Yuan Xie,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: VersaQ-3D是一个算法-架构协同设计框架，通过无校准的4位量化和专用加速器，解决了VGGT模型在边缘设备部署中的内存和计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: VGGT模型虽然能够实现无需逐场景优化的前馈3D重建，但其十亿参数规模带来了高内存和计算需求，阻碍了在设备上的部署。现有LLM量化方法在VGGT上失效，且存在硬件挑战。

Method: 提出VersaQ-3D框架：算法上，首次实现无需校准、场景无关的4位量化，利用正交变换去相关特征并抑制异常值；架构上，设计支持BF16、INT8和INT4的可重构加速器，采用统一脉动数据路径处理线性和非线性算子，以及两阶段重计算分块缓解长序列注意力内存压力。

Result: 在W4A8配置下保持98-99%的准确率；在W4A4配置下，相比先前方法在多样场景中性能提升1.61x-2.39x；加速器相比边缘GPU实现5.2x-10.8x的加速比，功耗低，支持高效即时3D重建。

Conclusion: VersaQ-3D通过算法-架构协同设计，成功解决了VGGT模型在边缘设备部署的挑战，实现了高效、低功耗的即时3D重建能力。

Abstract: The Visual Geometry Grounded Transformer (VGGT) enables strong feed-forward 3D reconstruction without per-scene optimization. However, its billion-parameter scale creates high memory and compute demands, hindering on-device deployment. Existing LLM quantization methods fail on VGGT due to saturated activation channels and diverse 3D semantics, which cause unreliable calibration. Furthermore, VGGT presents hardware challenges regarding precision-sensitive nonlinear operators and memory-intensive global attention. To address this, we propose VersaQ-3D, an algorithm-architecture co-design framework. Algorithmically, we introduce the first calibration-free, scene-agnostic quantization for VGGT down to 4-bit, leveraging orthogonal transforms to decorrelate features and suppress outliers. Architecturally, we design a reconfigurable accelerator supporting BF16, INT8, and INT4. A unified systolic datapath handles both linear and nonlinear operators, reducing latency by 60%, while two-stage recomputation-based tiling alleviates memory pressure for long-sequence attention. Evaluations show VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power, enabling efficient instant 3D reconstruction.

</details>


### [20] [Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling](https://arxiv.org/abs/2601.20706)
*Binglei Lou,Haoran Wu,Yao Lai,Jiayi Nie,Can Xiao,Xuan Guo,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: 本文提出针对扩散大语言模型采样阶段的NPU架构优化方案，通过轻量级向量原语、内存重用策略和混合精度内存层次结构，实现比GPU高达2.53倍的加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的采样阶段占推理延迟的70%，主要由于词汇级logits的大内存读写、基于归约的token选择和迭代掩码更新等操作，这些操作需要大量片上SRAM且涉及不规则内存访问，传统NPU难以高效处理。

Method: 识别NPU架构必须优化的关键指令集，采用轻量级非GEMM向量原语、原地内存重用策略和解耦混合精度内存层次结构。

Result: 在等效纳米技术节点下，相比NVIDIA RTX A6000 GPU实现高达2.53倍的加速，并开源了周期精确模拟和后综合RTL验证代码。

Conclusion: 针对dLLM采样阶段的特定架构优化能显著提升推理效率，为未来大语言模型推理硬件设计提供了新方向。

Abstract: Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications](https://arxiv.org/abs/2601.19970)
*Nourin Shahin,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 该研究评估了Llama模型变体在OWASP LLM应用十大安全威胁框架下的安全性能，发现小型专用安全模型（Llama-Guard-3-1B）在威胁检测准确率（76%）和延迟（0.165秒）方面优于大型通用模型（Llama-3.1-8B检测率为0%，延迟0.754秒）。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究原型转向企业系统，其安全漏洞对数据隐私和系统完整性构成严重风险。需要评估不同模型在安全威胁检测方面的性能差异。

Method: 使用FABRIC测试平台和NVIDIA A30 GPU，测试了5个标准Llama模型和5个Llama Guard变体。使用100个对抗性提示，覆盖OWASP LLM应用十大安全威胁类别，评估威胁检测准确率、响应安全性和计算开销。

Result: 紧凑型Llama-Guard-3-1B模型达到最高检测率76%，延迟最低（0.165秒/测试）。而基础模型如Llama-3.1-8B检测率为0%，延迟更高（0.754秒）。发现模型大小与安全有效性呈反比关系，小型专用模型在安全任务上优于大型通用模型。

Conclusion: 研究揭示了模型大小与安全性能之间的反比关系，表明在安全关键应用中应优先考虑专用安全模型而非通用大模型。提供了开源基准数据集支持可重复的AI安全研究。

Abstract: As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1].

</details>


### [22] [Reference-Free Spectral Analysis of EM Side-Channels for Always-on Hardware Trojan Detection](https://arxiv.org/abs/2601.20163)
*Mahsa Tahghigh,Hassan Salmani*

Main category: cs.CR

TL;DR: 提出了一种无需参考模型的硬件木马检测方法，结合时频电磁分析和高斯混合模型，通过多窗口短时傅里叶变换识别硬件木马的持久特征


<details>
  <summary>Details</summary>
Motivation: 传统侧信道检测方法依赖难以获得的黄金参考模型，而常开硬件木马对可信微电子构成严重威胁，需要无参考的检测方案

Method: 结合时频电磁分析和高斯混合模型，应用多窗口大小的短时傅里叶变换，分析电路电磁信号的统计结构特征

Result: 在AES-128上验证了可行性，无硬件木马电路显示波动的统计结构，而常开硬件木马留下持久特征，具有更少、更一致的高斯混合分量

Conclusion: 该方法无需参考模型即可检测常开硬件木马，为可信微电子安全提供了有效的无参考检测方案

Abstract: Always-on hardware Trojans (HTs) pose a critical risk to trusted microelectronics, yet most side-channel detection methods rely on unavailable golden references. We present a reference-free approach that combines time-frequency EM analysis with Gaussian Mixture Models (GMMs). By applying Short-Time Fourier Transform (STFT) at multiple window sizes, we show that HT-free circuits exhibit fluctuating statistical structure, while always-on HTs leave persistent footprints with fewer, more consistent mixture components. Results on AES-128 demonstrate feasibility without requiring reference models.

</details>


### [23] [Eliciting Least-to-Most Reasoning for Phishing URL Detection](https://arxiv.org/abs/2601.20270)
*Holly Trikilis,Pasindu Marasinghe,Fariza Rashid,Suranga Seneviratne*

Main category: cs.CR

TL;DR: 本文提出了一种用于钓鱼URL检测的Least-to-Most提示框架，通过引入"答案敏感性"机制增强推理能力，在少量训练数据下达到与监督模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击是最普遍的威胁之一，准确分类钓鱼URL至关重要。虽然大语言模型在钓鱼URL检测中显示出潜力，但其实现高性能的推理能力尚未得到充分探索。

Method: 提出Least-to-Most提示框架，引入"答案敏感性"机制指导迭代推理过程，使用三个URL数据集和四个最先进的大语言模型进行评估。

Result: 该框架在性能上优于one-shot基线方法，且与监督模型性能相当，同时所需训练数据显著减少。迭代推理和答案敏感性机制共同驱动了性能提升。

Conclusion: 这种简单而强大的提示策略在最小化训练或少量指导的情况下，持续优于one-shot和监督方法，为钓鱼URL检测提供了有效的解决方案。

Abstract: Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an "answer sensitivity" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.

</details>


### [24] [SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks](https://arxiv.org/abs/2601.20310)
*Xin Zhang,Zijin Yang,Kejiang Chen,Linfeng Ma,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: SemBind是首个防御潜在扩散模型中黑盒伪造攻击的框架，通过将潜在水印信号与图像语义绑定来抵抗攻击，保持图像质量基本不变，并提供可调节的鲁棒性-安全性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有潜在水印虽然简化了生成图像的检测和溯源，但面临黑盒伪造攻击的风险，攻击者只需一张水印图像和黑盒访问模型，就能将提供商的水印嵌入非提供商生成的图像中，严重威胁溯源和信任体系。

Method: 提出SemBind框架，通过学习的语义掩码器将潜在信号与图像语义绑定。使用对比学习训练掩码器，使相同提示生成近不变代码，不同提示生成近正交代码；这些代码经过重塑和置换后调制目标潜在表示，然后应用标准潜在水印。框架兼容现有潜在水印方案，通过掩码比率参数提供可调节的鲁棒性-安全性权衡。

Result: 在四种主流潜在水印方法上，SemBind增强的抗伪造变体显著降低了黑盒伪造攻击下的误接受率，同时提供了可控的鲁棒性-安全性平衡，图像质量基本保持不变。

Conclusion: SemBind是首个有效防御潜在扩散模型中黑盒伪造攻击的框架，通过语义绑定机制保护水印的完整性和可信度，为生成图像溯源提供了更可靠的解决方案。

Abstract: Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.

</details>


### [25] [UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion](https://arxiv.org/abs/2601.20325)
*Lulu Xue,Shengshan Hu,Wei Lu,Ziqi Zhou,Yufei Song,Jianhong Cheng,Minghui Li,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 提出首个针对机器学习遗忘反转攻击的防御方法UnlearnShield，通过在余弦表示空间引入方向性扰动并约束，在保护隐私的同时保持模型准确性和遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习遗忘技术旨在从训练模型中移除特定数据的影响以增强隐私保护，但研究发现存在遗忘反转攻击漏洞，攻击者可以利用该漏洞重建本应被删除的数据。目前缺乏专门的防御机制来应对这一严重威胁。

Method: 提出UnlearnShield防御方法，在余弦表示空间引入方向性扰动，并通过约束模块进行调节，共同保持模型准确性和遗忘效果，从而降低反转风险同时维持实用性。

Result: 实验证明UnlearnShield在隐私保护、准确性和遗忘效果之间实现了良好的平衡，能够有效减少反转攻击风险。

Conclusion: UnlearnShield是首个专门针对机器学习遗忘反转攻击的防御方案，通过创新的方向性扰动和约束机制，为解决这一新兴隐私威胁提供了有效解决方案。

Abstract: Machine unlearning is an emerging technique that aims to remove the influence of specific data from trained models, thereby enhancing privacy protection. However, recent research has uncovered critical privacy vulnerabilities, showing that adversaries can exploit unlearning inversion to reconstruct data that was intended to be erased. Despite the severity of this threat, dedicated defenses remain lacking. To address this gap, we propose UnlearnShield, the first defense specifically tailored to counter unlearning inversion. UnlearnShield introduces directional perturbations in the cosine representation space and regulates them through a constraint module to jointly preserve model accuracy and forgetting efficacy, thereby reducing inversion risk while maintaining utility. Experiments demonstrate that it achieves a good trade-off among privacy protection, accuracy, and forgetting.

</details>


### [26] [A High-Performance Fractal Encryption Framework and Modern Innovations for Secure Image Transmission](https://arxiv.org/abs/2601.20374)
*Sura Khalid Salsal,Eman Shaker Mahmood,Farah Tawfiq Abdul Hussien,Maryam Mahdi Alhusseini,Azhar Naji Alyahya,Nikolai Safiullin*

Main category: cs.CR

TL;DR: 本文提出了一种基于傅里叶变换的分形图像加密方法，旨在解决传统加密算法在安全性、图像保真度和计算效率之间的权衡问题，显著提升了加密/解密时间和图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前数字时代对数据安全的需求日益增长，传统图像加密算法在安全性、图像保真度和计算效率之间存在权衡问题，需要开发更强大的图像加密技术。

Method: 提出基于傅里叶变换的分形加密方法作为新的图像加密技术，利用最先进的技术，通过分形结构与傅里叶变换的结合来增强加密性能。

Result: 与已知的传统加密方法相比，该方法显著改善了加密/解密时间，同时更好地保持了图像质量，填补了先前研究的空白。

Conclusion: 基于傅里叶变换的分形加密方法在图像加密领域表现出优越的性能，为未来研究提供了方向和改进空间。

Abstract: The current digital era, driven by growing threats to data security, requires a robust image encryption technique. Classical encryption algorithms suffer from a trade-off among security, image fidelity, and computational efficiency. This paper aims to enhance the performance and efficiency of image encryption. This is done by proposing Fractal encryption based on Fourier transforms as a new method of image encryption, leveraging state-of-the-art technology. The new approach considered here intends to enhance both security and efficiency in image encryption by comparing Fractal Encryption with basic methods. The suggested system also aims to optimise encryption/ decryption times and preserve image quality. This paper provides an introduction to Image Encryption using the fractal-based method, its mathematical formulation, and its comparative efficiency against publicly known traditional encryption methods. As a result, after filling the gaps identified in previous research, it has significantly improved both its encryption/decryption time and image fidelity compared to other techniques. In this paper, directions for future research and possible improvements are outlined for attention.

</details>


### [27] [Towards Quantum-Safe O-RAN -- Experimental Evaluation of ML-KEM-Based IPsec on the E2 Interface](https://arxiv.org/abs/2601.20378)
*Mario Perera,Michael Mackay,Max Hashem Eiza,Alessandro Raschellà,Nathan Shone,Mukesh Kumar Maheshwari*

Main category: cs.CR

TL;DR: 该论文通过实验评估了在O-RAN E2接口上集成后量子密码学ML-KEM对IPsec隧道建立延迟和xApp运行的影响，发现ML-KEM仅增加3-5ms开销，表明量子安全迁移具有可行性。


<details>
  <summary>Details</summary>
Motivation: 随着O-RAN部署扩展和攻击者采用"先存储后解密"策略，运营商需要关于将关键控制接口迁移到后量子密码学的实际成本数据。本文旨在为O-RAN部署的量子安全迁移策略提供实证依据。

Method: 使用开源测试平台（srsRAN、Open5GS、FlexRIC、strongSwan和liboqs）构建实验环境，比较三种配置：无IPsec、基于ECDH的传统IPsec、基于ML-KEM的IPsec。重点关注IPsec隧道建立延迟和Near-RT RIC xApps在真实信令负载下的运行时行为。

Result: 实验结果显示，ML-KEM集成相比传统IPsec仅增加约3-5ms的隧道建立开销，xApp操作和RIC控制回路在实验中保持稳定。这表明基于ML-KEM的IPsec在E2接口上具有实际可行性。

Conclusion: ML-KEM基于IPsec在O-RAN E2接口上的应用是实际可行的，为O-RAN部署的量子安全迁移策略提供了重要参考依据，后量子密码学迁移仅带来可接受的小幅性能开销。

Abstract: As Open Radio Access Network (O-RAN) deployments expand and adversaries adopt 'store-now, decrypt-later' strategies, operators need empirical data on the cost of migrating critical control interfaces to post-quantum cryptography (PQC). This paper experimentally evaluates the impact of integrating a NIST-aligned module-lattice KEM (ML-KEM, CRYSTALS-Kyber) into IKEv2/IPsec protecting the E2 interface between the 5G Node B (gNB) and the Near-Real-Time RAN Intelligent Controller (Near-RT RIC). Using an open-source testbed built from srsRAN, Open5GS, FlexRIC and strongSwan (with liboqs), we compare three configurations: no IPsec, classical ECDH-based IPsec, and ML-KEM-based IPsec. The study focuses on IPsec tunnel-setup latency and the runtime behaviour of Near-RT RIC xApps under realistic signalling workloads. Results from repeated, automated runs show that ML-KEM integration adds a small overhead to tunnel establishment, which is approximately 3~5 ms in comparison to classical IPsec, while xApp operation and RIC control loops remain stable in our experiments. These findings indicate that ML-KEM based IPsec on the E2 interface is practically feasible and inform quantum-safe migration strategies for O-RAN deployments.

</details>


### [28] [Fuzzy Private Set Union via Oblivious Key Homomorphic Encryption Retrieval](https://arxiv.org/abs/2601.20400)
*Jean-Guillaume Dumas,Aude Maignan,Luiza Soezima*

Main category: cs.CR

TL;DR: 本文提出了模糊私有集合并（FPSU）协议，通过引入新的高效子协议OKHER，在考虑近似性的情况下安全计算集合的并集。


<details>
  <summary>Details</summary>
Motivation: 传统的私有集合运算（如PSU和PSI）要求元素完全相等才能参与计算，但在实际应用中，元素可能只是近似相等。现有的模糊PSI协议可以处理近似性，但缺乏高效的模糊PSU协议来处理集合的并集运算。

Method: 引入新的子协议OKHER（Oblivious Key Homomorphic Encryption Retrieval），改进现有的OKVR技术。将接收方集合X替换为以X中每个点为中心、半径为δ的d维球的并集，发送方集合保持为d维点集。使用l∞距离和同态加密技术，根据球的不同模式设计多个协议。

Result: 提出的FPSU协议具有渐近通信量范围，从O(dm log(δn))到O(d²m log(δ²n))，具体取决于接收方数据集的结构。OKHER协议在模糊环境下比现有的OKVR技术更高效。

Conclusion: 本文首次正式定义了FPSU功能性和安全属性，并提出了基于OKHER和同态加密的高效FPSU协议，能够有效处理集合运算中的近似性问题，为实际应用提供了更灵活的隐私保护集合运算方案。

Abstract: Private Set Multi-Party Computations are protocols that allow parties to jointly and securely compute functions: apart from what is deducible from the output of the function, the input sets are kept private. Then, a Private Set Union (PSU), resp. Intersection (PSI), is a protocol that allows parties to jointly compute the union, resp. the intersection, between their private sets. Now a structured PSI, is a PSI where some structure of the sets can allow for more efficient protocols. For instance in Fuzzy PSI, elements only need to be close enough, instead of equal, to be part of the intersection. We present in this paper, Fuzzy PSU protocols (FPSU), able to efficiently take into account approximations in the union. For this, we introduce a new efficient sub-protocol, called Oblivious Key Homomorphic Encryption Retrieval (OKHER), improving on Oblivious Key-Value Retrieval (OKVR) techniques in our setting. In the fuzzy context, the receiver set $X=\{x_i\}_{1..n}$ is replaced by ${\mathcal B}_δ(X)$, the union of $n$ balls of dimension $d$ with radius $δ$, centered at the $x_i$. The sender set is just its $m$ points of dimension $d$. Then the FPSU functionality corresponds to $X \sqcup \{y \in Y, y \notin {\mathcal B}_δ(X)\}$. Thus, we formally define the FPSU functionality and security properties, and propose several protocols tuned to the patterns of the balls using the $l_\infty$ distance. Using our OKHER routine and homomorphic encryption, we are for instance able to obtain a FPSU protocols with an asymptotic communication volume bound ranging from $O(dm\log(δ{n}))$ to $O(d^2m\log(δ^2n))$, depending on the receiver data set structure.

</details>


### [29] [TÄMU: Emulating Trusted Applications at the (GlobalPlatform)-API Layer](https://arxiv.org/abs/2601.20507)
*Philipp Mao,Li Shi,Marcel Busch,Mathias Payer*

Main category: cs.CR

TL;DR: TÄMU是一个重新托管平台，通过API层拦截执行，实现对可信应用(TA)的动态分析（模糊测试和调试），解决了移动TEE生态系统中动态分析能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 移动设备依赖可信执行环境(TEE)执行安全关键代码和保护敏感资产，但TEE的闭源性和碎片化严重阻碍了对可信应用(TA)的动态分析，使得测试工作主要局限于静态分析，导致TA中的漏洞可能危及整个系统安全。

Method: TÄMU利用GlobalPlatform规范驱动的TEE API标准化，在API层拦截TA执行实现重新托管。对于不同TEE特有的API，引入"贪婪高级仿真"技术，根据模糊测试期间潜在覆盖率增益来优先安排手动重新托管工作。

Result: 实现了TÄMU平台，成功仿真了4个不同TEE中的67个可信应用。模糊测试活动发现了11个TA中的17个零日漏洞，表明即使拥有源代码的供应商也未能充分利用动态分析能力。

Conclusion: TÄMU通过为移动TEE领域带来有效且实用的动态分析，有望弥补当前生态系统中的动态分析能力缺口，提升TEE安全性。

Abstract: Mobile devices rely on Trusted Execution Environments (TEEs) to execute security-critical code and protect sensitive assets. This security-critical code is modularized in components known as Trusted Applications (TAs). Vulnerabilities in TAs can compromise the TEE and, thus, the entire system. However, the closed-source nature and fragmentation of mobile TEEs severely hinder dynamic analysis of TAs, limiting testing efforts to mostly static analyses. This paper presents TÄMU, a rehosting platform enabling dynamic analysis of TAs, specifically fuzzing and debugging, by interposing their execution at the API layer. To scale to many TAs across different TEEs, TÄMU leverages the standardization of TEE APIs, driven by the GlobalPlatform specifications. For the remaining TEE-specific APIs not shared across different TEEs, TÄMU introduces the notion of greedy high-level emulation, a technique that allows prioritizing manual rehosting efforts based on the potential coverage gain during fuzzing. We implement TÄMU and use it to emulate 67 TAs across four TEEs. Our fuzzing campaigns yielded 17 zero-day vulnerabilities across 11 TAs. These results indicate a deficit of dynamic analysis capabilities across the TEE ecosystem, where not even vendors with source code unlocked these capabilities for themselves. TÄMU promises to close this gap by bringing effective and practical dynamic analysis to the mobile TEE domain.

</details>


### [30] [IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices](https://arxiv.org/abs/2601.20548)
*Kahraman Kostas,Rabia Yasa Kostas*

Main category: cs.CR

TL;DR: 论文批判性分析物联网设备识别中的机器学习方法，指出现有文献常见陷阱，分析识别方法、数据异质性、特征提取和评估指标的权衡，提供增强模型可复现性和泛化性的指南


<details>
  <summary>Details</summary>
Motivation: 现有物联网设备识别研究存在常见缺陷，包括数据增强不当、会话标识符误导等问题，导致模型可复现性和泛化性不足，需要系统分析这些陷阱并提供改进指南

Method: 通过批判性分析现有文献，系统研究设备识别方法（唯一识别与类别识别）的权衡、数据异质性挑战、特征提取问题以及评估指标选择，识别具体错误并提供解决方案

Result: 识别了物联网设备识别中的关键陷阱，包括不恰当的数据增强、误导性会话标识符使用等，提出了增强模型可复现性和泛化性的具体指南

Conclusion: 通过系统分析物联网设备识别中的机器学习陷阱，为研究人员提供了改进模型可复现性和泛化性的实用指南，有助于推动更可靠的物联网安全研究

Abstract: This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.

</details>


### [31] [Supply Chain Insecurity: Exposing Vulnerabilities in iOS Dependency Management Systems](https://arxiv.org/abs/2601.20638)
*David Schmidt,Sebastian Schrittwieser,Edgar Weippl*

Main category: cs.CR

TL;DR: iOS依赖管理系统存在安全漏洞，攻击者可利用应用泄露的包信息进行依赖混淆攻击，通过注册未声明的依赖或劫持废弃域名实现远程代码执行，影响数百万用户。


<details>
  <summary>Details</summary>
Motivation: iOS软件供应链中依赖管理系统的安全性研究不足，尽管iOS应用广泛使用，但CocoaPods等系统的安全风险未得到充分关注。依赖管理系统中的错误配置和恶意行为可能导致供应链攻击，对开发者机器和构建服务器构成严重安全威胁。

Method: 研究聚焦于iOS最常用的依赖管理系统CocoaPods，同时考察Carthage和Swift Package Manager。通过分析9,212个应用的数据集，量化易受攻击的应用数量。研究还检查了公共GitHub仓库中易受攻击依赖的使用情况，并比较iOS依赖管理系统与Cargo、Go模块、Maven、npm和pip等系统的安全性。

Result: 研究发现流行iOS应用泄露内部依赖信息，使依赖混淆攻击成为可能。通过劫持单个废弃的CocoaPod库域名，可影响63个iOS应用，危及数百万用户。攻击者可通过注册未声明的依赖实现远程代码执行，或通过回收废弃域名和GitHub URL来破坏依赖。

Conclusion: iOS依赖管理系统存在严重安全漏洞，需要采取缓解策略来应对依赖混淆和域名劫持等威胁。研究通过与其他语言依赖管理系统的比较，为iOS软件供应链安全提供了改进建议。

Abstract: Dependency management systems are a critical component in software development, enabling projects to incorporate existing functionality efficiently. However, misconfigurations and malicious actors in these systems pose severe security risks, leading to supply chain attacks. Despite the widespread use of smartphone apps, the security of dependency management systems in the iOS software supply chain has received limited attention. In this paper, we focus on CocoaPods, one of the most widely used dependency management systems for iOS app development, but also examine the security of Carthage and Swift Package Manager (SwiftPM). We demonstrate that iOS apps expose internal package names and versions. Attackers can exploit this leakage to register previously unclaimed dependencies in CocoaPods, enabling remote code execution (RCE) on developer machines and build servers. Additionally, we show that attackers can compromise dependencies by reclaiming abandoned domains and GitHub URLs. Analyzing a dataset of 9,212 apps, we quantify how many apps are susceptible to these vulnerabilities. Further, we inspect the use of vulnerable dependencies within public GitHub repositories. Our findings reveal that popular apps disclose internal dependency information, enabling dependency confusion attacks. Furthermore, we show that hijacking a single CocoaPod library through an abandoned domain could compromise 63 iOS apps, affecting millions of users. Finally, we compare iOS dependency management systems with Cargo, Go modules, Maven, npm, and pip to discuss mitigation strategies for the identified threats.

</details>


### [32] [Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy](https://arxiv.org/abs/2601.20716)
*Abylay Satybaldy,Kamil Tylinski,Jiahua Xu*

Main category: cs.CR

TL;DR: 对以太坊、Hedera和XRP Ledger三种分布式账本DID方法的实证基准测试，测量延迟、交易成本和元数据泄露，揭示不同架构在性能与隐私间的权衡。


<details>
  <summary>Details</summary>
Motivation: 去中心化标识符(DID)在分布式账本上部署日益增多，但缺乏跨平台的系统性实证研究，需要了解不同DID方法在操作行为上的实际表现差异。

Method: 使用统一的实验设置，通过参考软件开发工具包(SDK)对以太坊、Hedera和XRP Ledger三种账本DID方法进行基准测试，测量延迟、交易成本和链上元数据暴露，并使用基于熵的元数据泄露评分(MLS)量化隐私泄露。

Result: 以太坊支持近乎即时的链下DID创建，但链上生命周期操作延迟和成本最高；XRPL提供确定性和稳定的延迟与固定低费用，但交易负载更详细导致元数据泄露更高；Hedera实现最低的链上延迟和低费用，元数据泄露最小，但偶尔因SDK端处理和确认管道产生方差。

Conclusion: 账本架构和SDK工作流程在塑造DID延迟、成本和元数据暴露方面起主要作用，补充了底层共识机制的影响。这些结果为在性能和隐私约束下选择和配置DID系统提供了基于证据的见解。

Abstract: Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.
  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.
  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: 该论文基于2025年8月研讨会，探讨神经科学与人工智能的交叉领域，提出NeuroAI概念，旨在通过神经科学启发改进AI算法，同时深化对生物神经计算的理解。


<details>
  <summary>Details</summary>
Motivation: 神经科学与人工智能在过去几年都取得了显著进展，但两者之间的连接仍然松散。作者希望通过整合这两个领域，创建神经科学启发的人工智能（NeuroAI），以提升AI算法的范围和效率，同时改变对生物神经计算的理解方式。

Method: 基于2025年8月举办的研讨会，聚焦于具身性、语言与通信、机器人学、人类与机器学习以及神经形态工程等子领域。收集了多位领先研究人员的个人观点，并附有研究人员和学员进行的SWOT分析。

Result: 识别了神经科学与AI之间当前和未来的协同领域，提出了NeuroAI的发展框架。通过SWOT分析描述了NeuroAI的益处和风险，为这一交叉学科的发展提供了多角度的评估。

Conclusion: 倡导发展NeuroAI，认为这种神经科学启发的人工智能有潜力显著提高AI算法的范围和效率，同时改变我们对生物神经计算的理解方式。通过整合两个领域的优势，可以开辟新的研究途径。

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [34] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: SQ-BCP是一种在部分可观测环境下进行推理时规划的方法，通过显式表示前提条件状态、自我查询和桥接假设来解决大语言模型在缺失关键前提时的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在部分可观测环境下进行推理时规划时经常失败：当任务关键前提条件在查询时未指定时，模型倾向于产生幻觉或生成违反硬约束的计划。

Method: 提出自我查询双向分类规划(SQ-BCP)，显式表示前提条件状态(Sat/Viol/Unk)，通过(i)针对性的自我查询和(ii)桥接假设来解析未知条件，执行双向搜索并使用基于拉回的验证器作为目标兼容性的分类证书。

Result: 在WikiHow和RecipeNLG任务中，SQ-BCP将资源违规率分别降低到14.9%和5.8%（相比最佳基线的26.0%和15.7%），同时保持有竞争力的参考质量。

Conclusion: SQ-BCP通过显式处理部分可观测性，在理论上保证找到接受计划（当存在时），并在实践中显著减少违反约束的计划生成，提高了大语言模型在真实世界规划任务中的可靠性。

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [35] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出模糊范畴论规划(FCP)，将模糊逻辑融入范畴论规划框架，处理自然语言规划中的模糊谓词，通过t-范数组合计划质量，同时保持硬约束验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有范畴论规划器将适用性视为二元判断，无法处理自然语言规划中固有的模糊谓词（如"合适替代品"、"足够稳定"），导致阈值化处理会丢失有意义的质量差异，且无法跟踪多步计划中的质量退化。

Method: FCP为每个动作（态射）标注[0,1]区间内的程度值，通过Lukasiewicz t-范数组合计划质量，同时通过拉回验证保持可执行性检查的精确性。使用LLM进行k样本中位数聚合从语言中获取分级适用性，并支持基于剩余运算的中间相遇搜索。

Result: 在PDDL3偏好/超额预订基准测试和RecipeNLG-Subs（基于RecipeNLG构建的缺失替代品食谱规划基准）上评估。FCP在RecipeNLG-Subs上相比LLM-only和ReAct风格基线提高了成功率，减少了硬约束违反，同时与经典PDDL3规划器保持竞争力。

Conclusion: FCP成功地将模糊逻辑集成到范畴论规划框架中，有效处理自然语言规划中的模糊谓词，在保持硬约束验证能力的同时，能够跟踪计划质量退化并做出有意义的区分。

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [36] [Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models](https://arxiv.org/abs/2601.20305)
*Zhenchen Tang,Songlin Yang,Zichuan Wang,Bo Peng,Yang Li,Beibei Dong,Jing Dong*

Main category: cs.AI

TL;DR: SEER框架通过内生重提示机制解决UMMs认知鸿沟问题，仅用300个样本训练即可提升模型生成质量


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型虽然具备强大的理解能力，但这种理解能力往往无法有效指导生成过程，存在"认知鸿沟"：模型缺乏如何改进自身生成过程的理解

Method: 提出内生重提示机制，将模型理解从被动编码过程转变为显式生成推理步骤；引入SEER训练框架，建立两阶段内生循环：1) RLVR通过课程学习激活模型潜在评估能力，产生高保真内生奖励信号；2) RLMT利用该信号优化生成推理策略

Result: SEER在评估准确性、重提示效率和生成质量方面持续优于最先进的基线方法，且不牺牲通用多模态能力

Conclusion: 内生重提示机制有效弥合了UMMs的认知鸿沟，SEER框架仅需少量样本即可显著提升模型生成质量，为多模态模型的自进化提供了新思路

Abstract: Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.

</details>


### [37] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: AMA框架通过多智能体协作实现自适应记忆管理，显著提升长期记忆一致性并减少80%的token消耗


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆系统存在检索粒度僵化、维护策略累积过重、更新机制粗粒度等问题，导致存储信息与任务需求不匹配，以及逻辑不一致性的持续累积

Method: 提出自适应记忆多智能体协作框架(AMA)，采用分层记忆设计，通过Constructor和Retriever实现多粒度记忆构建和自适应查询路由，Judge验证相关性和一致性，Refresher执行针对性更新

Result: 在挑战性长上下文基准测试中，AMA显著优于现有最优基线，相比全上下文方法减少约80%的token消耗，有效保持了检索精度和长期记忆一致性

Conclusion: AMA框架通过多智能体协作和分层记忆设计，成功解决了LLM智能体记忆系统中的关键问题，为构建更高效、一致的长时记忆系统提供了有效方案

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [38] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: PoT框架通过在线优化策略解决LLMs在复杂长程推理中的不稳定性问题，使小模型在代码生成任务上超越GPT-4o等大模型


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂长程推理中存在不稳定性，主要源于其固定的策略假设。现有方法仅将执行反馈作为外部信号用于筛选或重写轨迹，未能将其内化以改进底层推理策略。受波普尔"猜想与反驳"认识论启发，作者认为智能需要通过学习失败尝试来实时演化模型策略。

Method: 提出Policy of Thoughts (PoT)框架，将推理重新定义为实例内的在线优化过程。首先通过高效探索机制生成多样候选解，然后使用Group Relative Policy Optimization (GRPO)基于执行反馈更新瞬态LoRA适配器。这种闭环设计实现了模型推理先验的动态、实例特定细化。

Result: 实验表明PoT显著提升性能：一个4B参数模型在LiveCodeBench上达到49.71%准确率，超越了GPT-4o和DeepSeek-V3，尽管模型规模小了50倍以上。

Conclusion: PoT框架通过将推理重新定义为在线优化过程，实现了LLMs在复杂任务上的显著性能提升，证明了小模型通过策略优化可以超越大模型，为LLMs的推理能力改进提供了新方向。

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [39] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT是一个双粒度CoT压缩框架，通过语义抽象和令牌级剪枝的协调，在减少30.7%令牌的同时提升推理准确性7.6个百分点


<details>
  <summary>Details</summary>
Motivation: 现有的CoT提示方法虽然能提升LLM推理能力，但会产生冗长的推理轨迹，导致高延迟和高内存成本。现有的压缩方法要么在语义层面过于保守，要么在令牌层面过于激进，容易丢失关键推理线索并降低准确性。

Method: 提出CtrlCoT框架，包含三个核心组件：1) 分层推理抽象：生成多粒度语义的CoT；2) 逻辑保持蒸馏：训练逻辑感知的剪枝器，保留关键推理线索（如数字和运算符）；3) 分布对齐生成：对齐压缩轨迹与推理时风格，避免碎片化。

Result: 在MATH-500数据集上使用Qwen2.5-7B-Instruct模型，CtrlCoT相比最强基线减少了30.7%的令牌使用，同时准确率提升了7.6个百分点，实现了更高效可靠的推理。

Conclusion: CtrlCoT通过协调语义抽象和令牌级剪枝，有效解决了CoT压缩中的权衡问题，在保持推理正确性的同时显著降低了计算成本，为高效LLM推理提供了新方法。

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [40] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 该研究将迭代条件风险价值（ICVaR）动态风险度量应用于部分可观测环境下的风险敏感规划，开发了具有有限时间性能保证的策略评估算法，并将三种主流在线规划算法扩展为优化ICVaR值函数而非期望回报的风险敏感版本。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中，传统基于期望回报的规划方法可能无法充分处理尾部风险，特别是在安全关键应用中需要更严格的风险控制。现有风险敏感规划方法在动作空间较大时性能保证不足，需要开发不依赖动作空间规模的有限时间性能保证算法。

Method: 1. 开发了ICVaR策略评估算法，具有不依赖动作空间规模的有限时间性能保证；2. 将三种在线规划算法扩展为风险敏感版本：ICVaR稀疏采样、ICVaR PFT-DPW和ICVaR POMCPOW；3. 引入风险参数α（α=1恢复期望规划，α<1增加风险厌恶）；4. 为ICVaR稀疏采样建立了风险敏感目标下的有限时间性能保证，并设计了针对ICVaR的探索策略。

Result: 在基准POMDP领域实验中，提出的ICVaR规划器相比风险中性对应方法实现了更低的尾部风险。ICVaR稀疏采样算法获得了理论性能保证，且所有扩展算法都能通过调整α参数在风险中性和风险厌恶行为之间平滑过渡。

Conclusion: 该研究成功将ICVaR动态风险度量集成到部分可观测规划框架中，提供了具有理论保证的风险敏感规划算法，能够在安全关键应用中更好地控制尾部风险，同时保持与现有规划方法的兼容性。

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [41] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: 该研究开发了一个通过结构化多模型对话实证测试AI对齐策略的方法框架，基于和平研究传统，将AI对齐从控制问题重构为关系问题，并通过实验发现不同AI模型能够有意义地参与复杂对齐框架讨论并产生新见解。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个可复现的方法框架来实证测试AI对齐策略，将和平研究传统（基于利益的谈判、冲突转化、公共资源治理）应用于AI对齐问题，将对齐从控制问题重构为通过对话推理发展的关系问题。

Method: 采用实验设计，为不同AI系统分配四个角色（提议者、回应者、监督者、翻译者），在六种条件下测试当前大语言模型是否能实质性参与复杂对齐框架。使用Claude、Gemini和GPT-4o进行72轮对话，总计576,822字符的结构化交流。

Result: 结果显示AI系统能够有意义地参与和平研究概念讨论，从不同架构视角提出互补性反对意见，并产生初始框架中未出现的新见解（包括"VCW作为过渡框架"的新颖综合）。跨架构模式显示不同模型关注不同问题：Claude强调验证挑战，Gemini关注偏见和可扩展性，GPT-4o突出实施障碍。

Conclusion: 该框架为研究人员提供了在实施前压力测试对齐提案的可复现方法，研究结果为AI具备VCW所提出的对话推理能力提供了初步证据。研究讨论了局限性（对话更多关注过程元素而非AI本质的基础主张），并概述了未来研究方向，包括人-AI混合协议和扩展对话研究。

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [42] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: MathForge框架通过难度感知组策略优化算法和多方面问题重构策略，针对数学推理中的难题进行改进，显著提升大模型在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励方法在数学推理中存在系统性缺陷：算法上，广泛使用的组相对策略优化存在隐式不平衡，对难题的策略更新幅度较小；数据上，增强方法主要重述问题以增加多样性，但没有系统性地提升内在难度。这些缺陷限制了模型在更具挑战性问题上的能力发展。

Method: 提出MathForge框架，包含两个核心组件：1) 难度感知组策略优化算法，通过难度平衡的组优势估计纠正GRPO中的隐式不平衡，并通过难度感知的问题级权重优先处理难题；2) 多方面问题重构策略，从多个方面重构问题以增加难度，同时保持原始正确答案。这两个组件形成协同循环：MQR扩展数据边界，DGPO有效学习增强数据。

Result: 大量实验表明，MathForge在各种数学推理任务上显著优于现有方法。代码和增强数据已在GitHub上开源。

Conclusion: MathForge通过算法和数据两个维度系统性地针对难题进行优化，有效解决了现有方法在数学推理中对挑战性问题关注不足的问题，形成了协同增强的学习框架，显著提升了数学推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [43] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: 本文研究LLM智能体能否在协作推理任务中发展出不同于标准自然语言的任务导向通信协议，重点关注协议的效率和隐蔽性两个核心特性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM智能体是否能够发展出任务导向的通信协议，这些协议可能比自然语言更高效，同时也可能变得难以被外部观察者理解，从而引发透明度和控制方面的担忧。

Method: 采用指称游戏框架，让视觉语言模型智能体进行通信，提供一个可控、可测量的环境来评估语言变体。通过实验观察智能体如何发展通信模式。

Result: 实验表明：1）VLM能够发展出有效、适应任务的通信模式；2）能够发展出对人类和外部智能体都难以理解的隐蔽协议；3）观察到相似模型之间无需显式共享协议就能自发协调。

Conclusion: 这些发现突显了任务导向通信的潜力和风险，并将指称游戏定位为该领域未来研究的宝贵测试平台。

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [44] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: 提出了一种计算度量ASP的方法来处理定量时间约束，通过差异约束扩展ASP来避免时间粒度导致的规模化问题


<details>
  <summary>Details</summary>
Motivation: 需要处理定量时间约束（如持续时间和截止时间）的度量ASP，但细粒度时间约束会显著加剧ASP的接地瓶颈问题

Method: 利用带有差异约束的ASP扩展来处理时间相关方面，将度量ASP与时间粒度解耦

Result: 开发出不受时间精度影响的解决方案，有效解决了细粒度时间约束下的可扩展性问题

Conclusion: 通过差异约束扩展ASP的方法能够有效处理定量时间约束，同时避免了时间粒度对系统可扩展性的负面影响

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>
