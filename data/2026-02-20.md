<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.AI](#cs.AI) [Total: 49]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Read-Modify-Writable Snapshots from Read/Write operations](https://arxiv.org/abs/2602.16903)
*Armando Castañeda,Braulio Ramses Hernández Martínez*

Main category: cs.DC

TL;DR: 该论文研究了在异步并发共享内存系统中，仅使用读/写操作实现RMWable快照算法的可能性，提出了两种算法：一种用于已知有限进程数的标准模型，另一种用于具有无限并发性的变体模型。


<details>
  <summary>Details</summary>
Motivation: 现有RMWable快照算法严重依赖compare&swap或load-link/store-conditional等强大底层操作，而读/写操作已知比这些操作更弱。本文旨在探索仅使用读/写操作是否可能实现RMWable快照，延续了理解仅用读/写操作能解决什么问题的大量研究工作。

Method: 提出了两种仅使用读/写操作的RMWable快照算法：1）在标准并发共享内存模型中，进程数n有限且已知；2）在具有无限并发性的变体模型中，进程数量无限但任意时刻只有有限多个进程参与执行。

Result: 成功设计了两种仅使用读/写操作的RMWable快照算法，证明了在两种不同并发模型下仅用读/写操作实现RMWable快照是可能的。

Conclusion: RMWable快照算法可以在不依赖强大底层操作（如compare&swap或load-link/store-conditional）的情况下实现，仅使用简单的读/写操作即可，这扩展了我们对读/写操作计算能力的理解。

Abstract: In the context of asynchronous concurrent shared-memory systems, a snapshot algorithm allows failure-prone processes to concurrently and atomically write on the entries of a shared array MEM , and also atomically read the whole array. Recently, Read-Modify-Writable (RMWable) snapshot was proposed, a variant of snapshot that allows processes to perform operations more complex than just read and write, specifically, each entry MEM[k] is an arbitrary readable object. The known RMWable snapshot algorithms heavily rely on powerful low-level operations such as compare&swap or load-link/store-conditional to correctly produce snapshots of MEM. Following the large body of research devoted to understand the limits of what can be solved using the simple read/write low-level operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, we explore if RMWable snapshots are possible using only read/write operations. We present two read/write RMWable snapshot algorithms, the first one in the standard concurrent shared-memory model where the number of processes n is finite and known in advance, and the second one in a variant of the standard model with unbounded concurrency, where there are infinitely many processes, but at any moment only finitely many processes participate in an execution.

</details>


### [2] [Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation](https://arxiv.org/abs/2602.16936)
*Zikai Zhang,Rui Hu,Jiahao Xu*

Main category: cs.DC

TL;DR: Fed-PLoRA：一种轻量级异构联邦微调框架，通过并行单秩适配模块和Select-N-Fold策略解决异构客户端资源下的LoRA秩不一致问题，提升联邦学习中大语言模型微调的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）通过LoRA实现分布式客户端协作微调大语言模型（LLMs）并保护数据隐私，但实际部署中客户端资源异构导致采用不同LoRA秩，产生显著的初始化和聚合噪声，严重影响性能。

Method: 提出Fed-PLoRA框架：1）引入PLoRA（并行单秩适配），用多个并行单秩模块替代经典多秩LoRA模块；2）提出Select-N-Fold策略，在本地训练前将未训练的PLoRA模块折叠到预训练权重中，以适应异构客户端资源。

Result: 在多样化LLM微调任务上的大量实验表明，Fed-PLoRA在准确性和效率方面持续优于现有方法。提供了Fed-PLoRA初始化和聚合噪声的统一分析，展示了其如何解决最先进方法的局限性。

Conclusion: Fed-PLoRA有效解决了联邦学习中因客户端资源异构导致的LoRA秩不一致问题，通过创新的PLoRA架构和Select-N-Fold策略，显著提升了分布式大语言模型微调的性能和实用性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. The code is available at https://github.com/TNI-playground/Fed-PLoRA.

</details>


### [3] [Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads](https://arxiv.org/abs/2602.17318)
*Patrick Zojer,Jonas Posner,Taylan Özden*

Main category: cs.DC

TL;DR: 该研究评估了高性能计算集群中资源弹性的好处，通过动态调整可延展作业的资源分配，显著改善了作业周转时间、等待时间和节点利用率等关键指标。


<details>
  <summary>Details</summary>
Motivation: 传统的高性能计算集群采用刚性作业调度，导致资源利用率低下和作业等待时间增加。为了解决这些问题，研究探索了资源弹性调度的潜力，通过动态调整作业资源分配来提高系统效率和用户满意度。

Method: 使用来自Cori、Eagle和Theta超级计算机的真实工作负载轨迹，通过ElastiSim软件模拟不同比例（0-100%）的可延展作业。评估了五种作业调度策略，包括一种新颖的策略：尽可能保持可延展作业在其首选资源分配上运行。

Result: 与完全刚性工作负载相比，可延展作业在所有关键指标上都取得了显著改进。采用各超级计算机上表现最佳的调度策略时：作业周转时间减少37-67%，作业完成时间减少16-65%，作业等待时间减少73-99%，节点利用率提高5-52%。即使在20%的可延展作业比例下，改进仍然显著。

Conclusion: 研究证实了可延展性在解决当前高性能计算实践中效率低下问题的潜力，并表明即使有限采用也能带来实质性优势。研究结果鼓励将资源弹性集成到高性能计算资源管理中，并强调了工作负载特征、可延展比例和调度策略之间的重要相关性。

Abstract: Optimizing resource utilization in high-performance computing (HPC) clusters is essential for maximizing both system efficiency and user satisfaction. However, traditional rigid job scheduling often results in underutilized resources and increased job waiting times.
  This work evaluates the benefits of resource elasticity, where the job scheduler dynamically adjusts the resource allocation of malleable jobs at runtime. Using real workload traces from the Cori, Eagle, and Theta supercomputers, we simulate varying proportions (0-100%) of malleable jobs with the ElastiSim software.
  We evaluate five job scheduling strategies, including a novel one that maintains malleable jobs at their preferred resource allocation when possible. Results show that, compared to fully rigid workloads, malleable jobs yield significant improvements across all key metrics. Considering the best-performing scheduling strategy for each supercomputer, job turnaround times decrease by 37-67%, job makespan by 16-65%, job wait times by 73-99%, and node utilization improves by 5-52%. Although improvements vary, gains remain substantial even at 20% malleable jobs.
  This work highlights important correlations between workload characteristics (e.g., job runtimes and node requirements), malleability proportions, and scheduling strategies. These findings confirm the potential of malleability to address inefficiencies in current HPC practices and demonstrate that even limited adoption can provide substantial advantages, encouraging its integration into HPC resource management.

</details>


### [4] [TopoSZp: Lightweight Topology-Aware Error-controlled Compression for Scientific Data](https://arxiv.org/abs/2602.17552)
*Tripti Agarwal,Sheng Di,Xin Liang,Zhaoyuan Su,Yuxiao Li,Ganesh Gopalakrishnan,Hanqi Guo,Franck Cappello*

Main category: cs.DC

TL;DR: TopoSZp是一种轻量级、拓扑感知的误差控制有损压缩器，在保持高压缩性能的同时，能够保留科学数据中的关键拓扑结构（极值点和鞍点）。


<details>
  <summary>Details</summary>
Motivation: 现有有损压缩器（如SZ和ZFP）虽然提供数值误差保证，但无法有效保留对科学分析至关重要的拓扑结构（如极值点和鞍点）。现有的拓扑感知压缩器计算开销过大，需要一种轻量级解决方案。

Method: 基于高性能的SZp压缩器，TopoSZp集成了高效的关键点检测、局部顺序保持和针对性的鞍点优化，在宽松但严格执行的误差范围内工作。

Result: 在真实科学数据集上，TopoSZp相比现有拓扑感知压缩器：1）减少了3到100倍未保留的关键点；2）无假阳性或错误关键点类型；3）压缩速度快100到10000倍；4）解压缩速度快10到500倍；5）保持有竞争力的压缩比。

Conclusion: TopoSZp成功实现了轻量级、拓扑感知的误差控制压缩，在保持高性能的同时有效保留科学数据的关键拓扑特征，解决了现有方法的局限性。

Abstract: Error-bounded lossy compression is essential for managing the massive data volumes produced by large-scale HPC simulations. While state-of-the-art compressors such as SZ and ZFP provide strong numerical error guarantees, they often fail to preserve topological structures (example, minima, maxima, and saddle points) that are critical for scientific analysis. Existing topology-aware compressors address this limitation but incur substantial computational overhead. We present TopoSZp, a lightweight, topology-aware, error-controlled lossy compressor that preserves critical points and their relationships while maintaining high compression and decompression performance. Built on the high-throughput SZp compressor, TopoSZp integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement, all within a relaxed but strictly enforced error bound. Experimental results on real-world scientific datasets show that TopoSZp achieves 3 to 100 times fewer non-preserved critical points, introduces no false positives or incorrect critical point types, and delivers 100 to 10000 times faster compression and 10 to 500 times faster decompression compared to existing topology-aware compressors, while maintaining competitive compression ratios.

</details>


### [5] [Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction](https://arxiv.org/abs/2602.17610)
*Nicolau Manubens Gil*

Main category: cs.DC

TL;DR: 评估DAOS和Ceph对象存储系统在ECMWF数值天气预报及HPC/AI应用中的性能表现，与传统Lustre文件系统对比，DAOS展现出卓越的可扩展性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着HPC和AI应用（如数值天气预报）数据量激增，传统POSIX分布式文件系统在规模扩展时存在性能瓶颈，需要探索新型存储解决方案来满足高性能I/O需求。

Method: 开发软件适配器使ECMWF数值天气预报系统能够使用DAOS和Ceph对象存储，在多个计算机系统上进行广泛的I/O基准测试，并与相同硬件上的Lustre文件系统部署进行性能比较。

Result: DAOS和Ceph都表现出优异性能，但DAOS相对于Ceph和Lustre表现更突出，为大规模I/O应用提供了卓越的可扩展性和灵活性，展示了对象存储在HPC中心的良好应用前景。

Conclusion: 对象存储（特别是DAOS）在HPC环境中具有显著优势，虽然不意味着完全替代POSIX式I/O，但有望在未来几年在HPC中心获得更广泛采用。

Abstract: Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.
  This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community.
  DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE本体是一个专门用于法医牙科年龄评估的标准化语义框架，旨在解决当前方法异质性、数据碎片化和系统互操作性不足的问题，为法医和司法决策提供透明可追溯的基础。


<details>
  <summary>Details</summary>
Motivation: 年龄评估在法医和司法决策中至关重要，特别是对于无证件个人和无人陪伴未成年人。当前牙科年龄评估实践面临方法异质性、数据表示碎片化以及临床、法医和法律信息系统之间互操作性有限的问题，这些问题阻碍了透明度和可重复性，而AI方法的采用进一步放大了这些挑战。

Method: 开发了AIdentifyAGE本体，这是一个领域特定的标准化语义框架。它涵盖手动和AI辅助的法医牙科年龄评估工作流程，能够追踪观察、方法、参考数据和报告结果之间的关联。该本体建模完整的法医-法律工作流程，整合司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究和基于AI的估计方法。开发过程与领域专家合作，并基于上层和已建立的生物医学、牙科和机器学习本体，确保互操作性、可扩展性和符合FAIR原则。

Result: AIdentifyAGE本体提供了一个标准化的语义框架，能够增强法医牙科年龄评估的一致性、透明度和可解释性。它建立了强大的基础，支持在法医-法律和司法背景下开发本体驱动的决策支持系统。

Conclusion: AIdentifyAGE本体是提高法医牙科年龄评估一致性、透明度和可解释性的重要步骤，为法医-法律和司法环境中的本体驱动决策支持系统奠定了坚实基础。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [7] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 论文证明上下文性不是量子力学的特性，而是经典概率表示中单状态复用的必然结果，揭示了自适应智能的普遍表示约束


<details>
  <summary>Details</summary>
Motivation: 自适应系统常因内存、表示或物理资源限制而在多个上下文中复用固定内部状态空间，这种单状态复用普遍存在于自然和人工智能中，但其基本表示后果尚未被充分理解

Method: 将上下文建模为作用于共享内部状态的干预，证明任何再现上下文结果统计的经典模型都必须承担不可约的信息论代价，提供最小构造示例明确实现这一代价并澄清其操作意义

Result: 证明上下文性不是量子力学的特性，而是经典概率表示中单状态复用的必然结果，上下文依赖性不能仅通过内部状态来中介，必须承担信息论代价

Conclusion: 上下文性是自适应智能的一般表示约束，独立于物理实现，非经典概率框架通过放松单一全局联合概率空间假设来避免这种障碍，无需量子动力学或希尔伯特空间结构

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [8] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache是一个用于大规模人类移动仿真的缓存框架，通过可重构缓存和轻量级解码器显著提升效率，同时保持与最先进LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的人类移动仿真方法虽然能模拟真实移动行为，但计算成本过高，限制了可扩展性，需要一种更高效的解决方案。

Method: 设计MobCache框架，包含两个组件：1) 推理组件将推理步骤编码为潜在空间嵌入，使用潜在空间评估器实现推理步骤的重用和重组；2) 解码组件采用轻量级解码器，通过移动规律约束蒸馏训练，将潜在空间推理链转换为自然语言。

Result: 实验表明，MobCache在多个维度上显著提升了效率，同时保持了与最先进的基于LLM方法相当的性能。

Conclusion: MobCache通过可重构缓存和轻量级解码器设计，实现了高效的大规模人类移动仿真，在保持仿真保真度的同时解决了现有方法计算成本高的问题。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [9] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 研究分析了60个LLM基准测试的饱和现象，发现近半数基准已饱和，且饱和率随时间增加。专家策划的基准比众包基准更抗饱和，而隐藏测试数据无保护效果。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳性能模型，降低了其长期价值。需要了解基准饱和现象及其驱动因素。

Method: 从主要模型开发商的技术报告中选取60个大型语言模型基准测试，从任务设计、数据构建和评估格式三个维度定义14个特征属性。测试5个假设，分析每个属性对饱和率的影响。

Result: 近一半基准测试显示饱和现象，且饱和率随基准年龄增加而上升。隐藏测试数据（公开vs私有）没有保护效果，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究揭示了哪些设计选择能延长基准测试的寿命，为创建更持久的评估策略提供了信息。专家策划的基准设计是保持长期有效性的关键因素。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [10] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 简单基准方法在代码演化任务中表现优于复杂方法，搜索空间设计和领域知识比演化算法本身更重要


<details>
  <summary>Details</summary>
Motivation: 许多代码演化技术展示了令人印象深刻的表现，但往往没有与更简单的基准方法进行比较，需要验证简单方法的实际效果

Method: 在三个领域测试简单基准方法：寻找更好的数学边界、设计智能体框架、机器学习竞赛，分析代码演化方法的开发和使用中的缺陷

Result: 简单基准方法在所有三个领域都匹配或超越了更复杂的方法；数学边界问题中搜索空间设计和领域知识是性能关键；智能体框架设计中高方差导致次优选择

Conclusion: 代码演化的主要挑战在于设计好的搜索空间而非搜索算法本身，需要更好的评估方法和更严谨的研究实践

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [11] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 本文研究了n维超立方体Q_n的边被超平面切割问题，证明了S(n) ≤ ⌈4n/5⌉的新上界，改进了1971年的已知结果⌈5n/6⌉，并利用AI工具CPro1发现了切割Q_{10}的8个超平面构造。


<details>
  <summary>Details</summary>
Motivation: 研究n维超立方体Q_n的边被超平面切割的最小超平面数S(n)问题。这是一个经典的组合几何问题，自1971年Paterson给出S(n) ≤ ⌈5n/6⌉的上界后，一直没有显著改进。本文旨在利用现代计算工具改进这一上界。

Method: 1. 构造性方法：通过构造具体的超平面集合来证明上界。2. 使用AI工具CPro1：这是一个结合推理大语言模型和自动超参数调优的工具，用于搜索数学构造。3. 具体构造了8个超平面来切割Q_{10}，这是证明改进上界的关键。

Result: 证明了S(n) ≤ ⌈4n/5⌉（当n不是5的奇数倍时），当n是5的奇数倍时，S(n) ≤ 4n/5 + 1。这改进了Paterson在1971年给出的S(n) ≤ ⌈5n/6⌉的上界。同时获得了使用k<n个超平面时能切割的最大边数的新下界。

Conclusion: 本文成功改进了n维超立方体边切割问题中S(n)的上界，从⌈5n/6⌉提升到⌈4n/5⌉。这一改进得益于AI工具CPro1在构造具体超平面配置方面的帮助，展示了AI辅助数学研究在解决经典组合几何问题中的潜力。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [12] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流，用于中子衍射数据分析，将手动处理时间从435分钟减少到约90分钟（4.6-5.0倍加速），同时生成经过验证的晶体结构文件。


<details>
  <summary>Details</summary>
Motivation: 大型科学设施面临分析延迟问题，特别是对于结构复杂的样品，传统的手动分析流程耗时且效率低下，需要自动化解决方案来加速科学产出。

Method: 开发了NeuDiff Agent作为受治理的AI工作流，使用工具调用AI执行中子衍射数据分析的完整流程（数据还原、积分、精修、验证），通过白名单工具限制、故障关闭验证门和完整溯源记录确保可控性。

Result: 在基准测试中，NeuDiff Agent将处理时间从手动435分钟减少到86.5-94.4分钟（4.6-5.0倍加速），生成无A或B级警报的验证CIF文件，同时保持完整的溯源记录。

Conclusion: NeuDiff Agent为设施晶体学提供了一条实用的AI部署路径，在保持可追溯性和验证要求的同时显著加速分析流程，为科学产出提供了高效解决方案。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [13] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 提出Node Learning这一去中心化学习范式，将智能置于边缘节点，通过选择性对等交互扩展，避免集中式智能的成本和脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: AI向边缘扩展时，集中式智能面临成本高、脆弱性强的问题。数据传输、延迟、能耗以及对大型数据中心的依赖在异构、移动和资源受限环境中难以扩展。

Method: 引入Node Learning范式：节点从本地数据持续学习，维护自身模型状态，仅在有益时进行机会性知识交换。学习通过重叠和扩散传播，而非全局同步或集中聚合。

Result: 该概念论文建立了Node Learning的理论基础，对比了现有去中心化方法，并探讨了对通信、硬件、信任和治理的影响。

Conclusion: Node Learning不是取代现有范式，而是将其置于更广泛的去中心化视角中，统一自主和协作行为，适应数据、硬件、目标和连接性的异构性。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [14] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出了一种基于序理论的犹豫模糊集评分统一框架，证明了经典序不构成格结构，但对称序满足评分函数的关键规范准则，并引入了优势函数用于犹豫模糊元素排序。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论的形式化基础，需要建立更灵活、一致的评分机制框架。

Method: 提出基于给定序的统一评分框架，分析经典序在犹豫模糊元素上的性质，证明对称序满足强单调性和Gärdenfors条件等规范准则，引入包含最小可接受阈值的控制集优势函数。

Result: 经典序不诱导格结构，但对称序定义的评分满足评分函数关键规范准则；提出的离散优势函数和相对优势函数可用于构建模糊偏好关系并支持群体决策。

Conclusion: 序导向的评分框架为犹豫模糊集提供了更严谨的理论基础，优势函数方法增强了犹豫模糊元素的可比性，支持更有效的群体决策应用。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [15] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个能让LLM自动创建具有自生成拓扑结构和工具集的智能体开发套件，提供结构化内存支持，在多个基准测试中优于现有ADK


<details>
  <summary>Details</summary>
Motivation: 现有智能体开发套件要么功能支持不足，要么依赖人工手动设计拓扑、工具和内存组件，限制了智能体的泛化能力和整体性能

Method: OpenSage让LLM自动创建和管理子智能体及工具集，采用分层图结构的内存系统，并提供专门针对软件工程任务的工具包

Result: 在三个最先进的基准测试中，使用不同骨干模型的OpenSage均优于现有ADK，消融研究验证了各组件设计的有效性

Conclusion: OpenSage为下一代智能体开发铺平了道路，将焦点从以人为中心转向以AI为中心的范式

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [16] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应长时程攻击脆弱性的基准测试，包含5种新型攻击类型、28个真实环境和644个安全测试用例，发现现有智能体对此类攻击高度脆弱且单轮防御措施无效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在复杂长时程环境中的部署增加，它们面临通过多轮用户-智能体-环境交互实现单轮设置中不可行目标的长时程攻击风险，需要专门的基准来测量这种脆弱性。

Method: 开发了AgentLAB基准测试，支持五种新型攻击类型：意图劫持、工具链式攻击、任务注入、目标漂移和内存污染，涵盖28个真实智能体环境和644个安全测试用例，用于评估代表性LLM智能体的脆弱性。

Result: 评估发现代表性LLM智能体对长时程攻击仍然高度脆弱，而且为单轮交互设计的防御措施无法可靠缓解长时程威胁。

Conclusion: AgentLAB可作为跟踪实际环境中保护LLM智能体进展的宝贵基准，有助于提高智能体在长时程复杂环境中的安全性。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [17] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识的基准测试，要求模型通过维基百科超链接从源页面逐步导航到目标页面。前沿模型在简单任务上表现优异甚至超越人类，但在困难任务上成功率骤降，揭示了当前推理系统的明显局限性。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的能力，特别是在需要前瞻性规划和理解现实世界概念关联的任务中。现有基准可能无法充分测试这些能力，因此创建了一个需要多步导航和策略规划的新基准。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接从给定源页面逐步导航到目标页面。评估了包括Gemini-3、GPT-5、Claude Opus 4.5在内的广泛开源和闭源模型，分为简单和困难两个难度级别。通过轨迹级分析模型在失败后的重新规划能力。

Result: 前沿模型在简单任务上表现优异，甚至达到超人类水平，但在困难任务上性能急剧下降：表现最佳的Gemini-3在困难游戏中仅成功23%。分析表明世界知识是成功的必要条件，但超过一定阈值后，规划和长视野推理能力成为主导因素。轨迹分析显示即使最强模型在失败后也难以重新规划，经常陷入循环。

Conclusion: LLM-Wikirace是一个简单但有效的基准测试，揭示了当前推理系统在规划、长视野推理和失败恢复方面的明显局限性。虽然前沿模型在简单任务上表现出色，但在需要复杂规划和推理的困难任务上仍有很大改进空间，为规划能力强的LLMs提供了一个开放的竞技场。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [18] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 研究发现：对齐的视觉语言模型在窄域有害数据集上进行微调会导致严重的泛化性错位，这种错位在跨任务和模态中广泛存在，且多模态评估比纯文本评估显示出更高的错位程度。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在获取能力和保持安全对齐之间产生了根本性矛盾。研究者希望了解对齐的视觉语言模型在窄域有害数据集上微调后，安全对齐会如何退化。

Method: 使用Gemma3-4B模型进行实验，通过LoRA（低秩适应）在不同秩上进行微调，比较文本评估和多模态评估的错位程度。分析有害数据比例的影响，进行几何分析以理解有害行为的低维子空间特性，并评估两种缓解策略：良性窄域微调和基于激活的引导。

Result: 1. 错位程度随LoRA秩单调增加；2. 多模态评估显示出比文本评估显著更高的错位（70.71±1.22 vs 41.19±2.51）；3. 即使训练混合中只有10%有害数据也会导致实质性对齐退化；4. 有害行为占据极低维子空间，10个主成分就能捕获大部分错位信息；5. 两种缓解策略都能显著减少错位，但都无法完全消除已学习的有害行为。

Conclusion: 当前的后训练范式可能无法在部署后环境中充分保持对齐，需要开发更强大的持续学习框架来应对终身多模态智能体面临的安全对齐挑战。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [19] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 研究发现语言模型的文本安全性与工具调用安全性存在显著差距，文本拒绝有害请求时工具调用仍可能执行危险操作，需要专门的安全评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前安全评估主要关注文本层面的拒绝行为，但缺乏对工具调用安全性的系统评估。随着LLM作为智能体通过工具调用与外部系统交互，需要了解文本对齐是否也能抑制有害行动。

Method: 引入GAP基准测试框架，在6个受监管领域（制药、金融、教育、就业、法律、基础设施）测试6个前沿模型，每个领域7种越狱场景，3种系统提示条件（中性、安全强化、工具鼓励），2种提示变体，共生成17,420个分析数据点。

Result: 文本安全无法转移到工具调用安全。所有模型都存在文本拒绝但工具调用执行禁止操作的情况（GAP指标）。即使在安全强化提示下，6个模型仍有219个此类案例。系统提示对工具调用行为影响显著，TC安全率差异达21-57个百分点。运行时治理合约减少信息泄露但无法阻止禁止的工具调用尝试。

Conclusion: 仅文本安全评估不足以评估智能体行为，工具调用安全需要专门的测量和缓解措施。必须开发针对工具调用层面的安全评估框架。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [20] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 论文挑战了AI系统黑盒安全评估的基本假设，证明对于依赖未观测内部变量的模型，任何黑盒评估器都无法可靠估计部署风险，并建立了统计和计算上的根本限制。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全评估假设测试分布上的模型行为能可靠预测部署性能，但作者发现这一假设存在根本缺陷，特别是对于依赖未观测内部变量（在评估中罕见但在部署中普遍）的模型，需要形式化分析黑盒评估的局限性。

Method: 采用理论分析方法：1) 使用Le Cam方法证明被动评估的最小最大下界；2) 基于哈希触发构造和Yao最小最大原理分析自适应评估；3) 基于陷门单向函数假设分析计算分离；4) 为白盒探测提供样本复杂度分析和偏差校正。

Result: 1) 被动评估：任何估计器的期望绝对误差≥0.208δL；2) 自适应评估：最坏情况误差≥δL/16，检测需要Θ(1/ε)查询；3) 计算分离：具有特权信息的部署环境可激活不安全行为，多项式时间评估器无法区分；4) 白盒探测：估计部署风险到精度ε_R需要O(1/(γ²ε_R²))样本。

Conclusion: 黑盒测试在统计上可能无法确定安全风险，需要额外保障措施（架构约束、训练时保证、可解释性、部署监控）来确保最坏情况下的安全保证，为AI安全评估提供了明确的数学标准。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [21] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式纵向基准测试，它超越了传统的行为模仿评估，通过多视角参考区分描述性行为和规范性效用，以评估LLMs在金融咨询中的决策质量。


<details>
  <summary>Details</summary>
Motivation: 传统推荐基准主要评估模型模仿用户行为的能力，但在金融咨询领域，观察到的用户行为可能因市场波动而存在噪声或短视，可能与用户的长期目标相冲突。将用户选择作为唯一真实标签会混淆行为模仿与决策质量。

Method: 构建Conv-FinRe基准，包含入职访谈、逐步市场情境和咨询对话。模型需要在固定投资期限内生成股票排名。基准提供多视角参考，区分基于投资者特定风险偏好的描述性行为和规范性效用，从而诊断LLMs是遵循理性分析、模仿用户噪声还是受市场动量驱动。

Result: 评估结果显示，在理性决策质量与行为对齐之间存在持续紧张关系：在基于效用的排名上表现良好的模型往往无法匹配用户选择，而行为对齐的模型可能过度拟合短期噪声。

Conclusion: Conv-FinRe基准超越了传统的行为模仿评估，为金融咨询中的LLMs提供了更全面的评估框架，揭示了理性决策与行为对齐之间的权衡，相关数据集和代码已公开。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [22] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过搜索-验证流程解决时间序列数据库的自然语言查询问题，使用特征索引和Python程序来定位和验证候选窗口。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史记录，需要新的解决方案来帮助非专家用户从海量时间记录中检索有意义的事件、区间和摘要。

Method: 提出Sonar-TS神经符号框架，采用搜索-验证流程：1）使用特征索引通过SQL查询候选窗口；2）生成Python程序锁定并验证候选窗口与原始信号的匹配。

Result: 实验表明Sonar-TS能有效处理传统方法失败的复杂时间查询，并引入了NLQTSBench作为该领域首个大规模基准测试。

Conclusion: 这是NLQ4TSDB领域的首个系统性研究，提供了通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [23] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过Ruzicka相似性指数快速筛选，然后使用基于倒置正态分布技能桶的Kantorovich距离计算"制裁分数"，实现公平快速的团队匹配。


<details>
  <summary>Details</summary>
Motivation: 现代多人游戏中，公平快速的匹配系统直接影响玩家留存和满意度。然而，为技能水平异质的预组队团队创建公平匹配面临重大挑战，传统的基于平均技能指标的方法在技能分布广泛或偏斜时经常导致不平衡的对局。

Method: Cinder采用两阶段匹配系统：第一阶段使用Ruzicka相似性指数快速比较团队的"非异常值"技能范围进行初步筛选；第二阶段将玩家排名映射到基于倒置正态分布生成的技能桶中，提供平均技能水平的更高粒度，然后使用Kantorovich距离计算团队排序桶索引之间的"制裁分数"来量化匹配公平性。

Result: 通过分析1.4亿个模拟团队配对的制裁分数分布，证明了系统的可行性，为公平匹配阈值提供了稳健基础。

Conclusion: Cinder系统能够为技能异质的预组队团队提供快速且公平的匹配，解决了传统平均技能匹配方法在技能分布广泛时的局限性问题。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [24] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个实现项目级数学文本自动形式化的智能体框架，可将教科书等长文本转换为可编译的Lean代码库，大幅提升形式化效率。


<details>
  <summary>Details</summary>
Motivation: 现有数学自动形式化技术局限于孤立定理和短代码片段，无法处理教科书和研究论文等大规模项目，需要解决跨文件依赖、导入解析和端到端编译等问题。

Method: M2F采用两阶段智能体框架：1) 语句编译阶段将文档分割为原子块，通过推断依赖关系排序，修复声明骨架直到项目可编译（证明部分可留占位符）；2) 证明修复阶段在固定签名下使用目标导向的局部编辑填补证明空缺。整个过程保持验证器在循环中，只有工具链反馈确认改进时才提交编辑。

Result: 在约三周时间内，M2F将479页实分析和凸分析教科书转换为153,853行Lean代码库，实现完全形式化。在FATE-H基准测试中达到96%的证明成功率（基线为80%），展示出教科书级形式化的实际可行性。

Conclusion: M2F框架证明大规模数学文献自动形式化已具备实际可行性，能够以传统专家需要数月或数年才能完成的速度实现教科书级形式化，为数学形式化领域带来突破性进展。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [25] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: ITR方法通过检索式RAG技术，在LLM智能体运行时动态检索最小化的系统指令片段和必要工具子集，大幅减少上下文token使用，提升工具路由准确率并降低成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在运行时需要反复加载长系统指令和大量工具目录，这导致成本增加、智能体偏离概率升高、延迟增加以及工具选择错误等问题。

Method: 提出Instruction-Tool Retrieval (ITR)方法，这是一种RAG变体，在每个步骤中仅检索必要的系统提示片段和最小工具子集，动态组合运行时系统提示并暴露经过筛选的工具集，包含置信度门控回退机制。

Result: 在受控基准测试中，ITR将每步上下文token减少95%，相对提升正确工具路由32%，端到端成本降低70%，使智能体在上下文限制内能运行2-20倍更多循环。

Conclusion: ITR方法显著降低了LLM智能体的运行成本并提升了性能，特别适用于长时间运行的自主智能体，随着智能体步骤增加，节省效果会累积放大。

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [26] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA是一个多智能体计算机使用框架，通过意图对齐的计划记忆来稳定长时程执行，在桌面自动化任务中实现74.83%的成功率和0.91的步骤效率比。


<details>
  <summary>Details</summary>
Motivation: 现有方法（基于RL的规划器和轨迹检索）在长时程任务中容易偏离用户意图，重复解决常规子问题，导致错误累积和效率低下。需要一种能稳定执行、减少冗余规划的方法。

Method: 提出IntentCUA多智能体框架，包含规划器、计划优化器和批评器，通过共享记忆将原始交互轨迹抽象为多视图意图表示和可重用技能。运行时通过意图原型检索子组对齐的技能并注入部分计划。

Result: 在端到端评估中，IntentCUA实现了74.83%的任务成功率，步骤效率比为0.91，优于基于RL和轨迹中心的基线方法。消融实验显示多视图意图抽象和共享计划记忆共同提升执行稳定性。

Conclusion: 系统级意图抽象和基于记忆的协调是大型动态环境中可靠高效桌面自动化的关键。多智能体协作循环在长时程任务中提供最大收益。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [27] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 论文提出RFEval基准，通过立场一致性和因果影响两个可测试条件来评估大型推理模型的推理忠实性，发现49.7%的输出存在不忠实问题，且准确性与忠实性关联微弱。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽然表现出色，但常常产生听起来合理却无法反映其真实决策过程的推理，这削弱了可靠性和信任度。当前缺乏一个形式化框架来评估推理的忠实性。

Method: 提出一个形式化框架，定义推理忠实性的两个可测试条件：立场一致性和因果影响。开发RFEval基准，包含7,186个实例，涵盖七个任务，通过受控的输出级反事实干预来探测忠实性。评估了12个开源大型推理模型。

Result: 评估发现49.7%的输出存在不忠实问题，主要源于立场不一致。失败集中在数学和代码等脆弱、收敛的领域。忠实性与后训练机制相关性高于模型规模：在监督微调基础上添加当前RL风格目标会降低推理忠实性，即使准确性得以保持。准确性与忠实性关联微弱且统计上不显著。

Conclusion: 建立了一个严谨的方法论来审计大型推理模型的可靠性，表明可信AI不仅需要优化正确结果，还需要优化推理过程的结构完整性。准确性与忠实性脱钩，需要独立评估推理忠实性。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [28] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: S2Q是一种新的多智能体强化学习方法，通过学习多个子价值函数来保留替代的高价值动作，使用Softmax行为策略促进持续探索，在MARL基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有价值分解方法依赖单一最优动作，当训练过程中基础价值函数发生变化时难以适应，容易收敛到次优策略。

Method: 提出Successive Sub-value Q-learning (S2Q)，学习多个子价值函数来保留替代的高价值动作，结合Softmax行为策略促进持续探索，使总Q值能快速适应变化的最优解。

Result: 在具有挑战性的MARL基准测试中，S2Q一致优于各种MARL算法，展现出更好的适应性和整体性能。

Conclusion: S2Q通过保留替代高价值动作和促进持续探索，有效解决了现有价值分解方法在适应价值函数变化方面的局限性。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [29] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS是一种通过动态优先处理高损失样本来加速语言模型收敛的训练优化技术，使用轻量级线性预测器从静态标记特征估计样本难度，实现6-13%的收敛加速。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法需要预定义难度指标，而硬样本挖掘方法需要昂贵的逐样本损失跟踪。PBS旨在通过轻量级在线预测器解决这些问题，利用静态标记特征有效估计样本难度，实现高效的课程学习。

Method: PBS使用在线训练的轻量级线性预测器，仅基于四个简单的标记级特征（标记频率、序列长度、词汇多样性和稀有标记比例）来估计样本难度。预测器在训练过程中动态更新，用于在批次构建时优先选择高损失样本。

Result: 在130M参数transformer上的实验表明，PBS实现了6-13%的收敛加速（通过训练检查点的评估损失测量）。预测器与实际损失的相关性从0.14提高到0.44（经过10,000个训练步骤），最终达到0.44的相关性。

Conclusion: 标记频率统计编码了关于样本难度的有意义信息，使得能够以可忽略的计算开销实现有效的课程学习。PBS提供了一种实用且高效的训练优化方法，无需昂贵的逐样本损失跟踪或预定义难度指标。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [30] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 提出一个人类-AI协作框架来生成可信的可持续发展评级基准数据集，解决不同机构评级结果差异大的问题


<details>
  <summary>Details</summary>
Motivation: 不同可持续发展评级机构对同一公司的评级结果差异很大，限制了评级的可比性、可信度和决策相关性，需要一种方法来协调这些评级结果

Method: 提出一个通用的人类-AI协作框架，包含两个互补部分：STRIDE（提供原则性标准和评分系统，指导使用大语言模型构建公司级基准数据集）和SR-Delta（差异分析程序框架，揭示潜在调整的见解）

Result: 该框架能够实现可持续发展评级方法的可扩展和可比评估，为评估不同评级方法提供基准

Conclusion: 呼吁更广泛的AI社区采用AI驱动的方法来加强和推进可持续发展评级方法，支持并执行紧迫的可持续发展议程

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [31] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 本文提出了一种基于Owen值的新型SHAP解释方法，通过满足T属性的层次化特征分组，解决了传统SHAP在视觉任务中特征独立性假设失效的问题，提高了归因精度和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 在可解释人工智能中，基于Shapley值的方法虽然理论基础扎实，但在视觉任务中面临特征独立性假设失效的问题。像素等特征通常具有强烈的空间和语义依赖性，而现有的SHAP实现虽然引入了支持分组归因的Owen值，但其效果严重依赖于特征分组的定义方式。常用的分割方法（如轴对齐或SLIC）违反了关键的一致性属性，需要新的解决方案。

Method: 提出了一种新的特征分割方法，该方法满足T属性以确保层次结构中的语义对齐。这种层次化方法支持计算剪枝，同时提高了归因准确性和可解释性。该方法通过Owen值（Shapley值的层次化推广）来实现，特别适用于具有结构依赖性的特征。

Result: 在图像和表格数据集上的实验表明，O-Shap方法在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的情况下表现更佳。

Conclusion: 通过满足T属性的层次化特征分组，O-Shap方法有效解决了传统SHAP在视觉任务中的局限性，提供了更准确、语义更一致且计算效率更高的特征归因解释，为结构依赖性强的特征解释提供了更好的解决方案。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [32] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG框架自动从课程材料构建教师对齐的知识图谱，捕捉概念间的学习依赖关系，用于个性化学习干预


<details>
  <summary>Details</summary>
Motivation: 大规模课程中，教师难以诊断学生知识缺口并提供个性化干预。现有知识图谱方法要么过于表层，要么忽略了教学材料中丰富的教学信号

Method: 从课程讲义材料（幻灯片、笔记等）中提取重要概念作为节点，推断学习依赖关系作为有向边（如"部分-整体"或"依赖"关系），结合教育材料特有的时间语义信号与大语言模型的泛化能力

Result: 在真实世界多样化的课程材料上进行实验和人工评估，证明InstructKG能够捕捉丰富且与教师意图一致的学习进展关系

Conclusion: InstructKG框架能够自动构建教师对齐的知识图谱，有效捕捉课程预期的学习进展，为个性化学习提供支持

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [33] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出高维空间的索引认识论，将生成式AI视为学习流形上的导航者，将导航知识作为区别于符号推理和统计重组的第三种知识生产模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI对知识和知识生产的理解提出了前所未有的挑战，其工作机制的认知特性仍然模糊。缺乏这种理解，就无法在科学、教育和制度生活中负责任地整合生成式AI。需要打破传统的信息处理范式，建立新的认识论框架。

Method: 基于高维几何的四个结构特性（测度集中、近正交性、指数方向容量、流形正则性），结合皮尔士符号学和帕珀特建构主义，发展高维空间的索引认识论，将生成模型重新概念化为学习流形上的导航者。

Result: 提出了导航知识作为第三种知识生产模式，区别于符号推理和统计重组。这种认识论框架为理解生成式AI的认知特性提供了新的理论基础。

Conclusion: 生成式AI需要突破传统的图灵-香农-冯·诺依曼范式，建立基于高维空间几何特性的新认识论。导航知识作为新的知识生产模式，为负责任地整合生成式AI到科学和社会提供了原则性基础。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [34] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种新颖的并行算法，用于分解困难的CircuitSAT实例，通过专门约束将原始SAT实例划分为弱化公式族，参数化设计允许高效识别高质量分解


<details>
  <summary>Details</summary>
Motivation: 解决困难CircuitSAT实例的分解问题，这些实例包括布尔电路逻辑等价性检查和密码哈希函数原像攻击等具有挑战性的应用场景

Method: 使用专门约束将原始SAT实例划分为弱化公式族，实现为参数化并行算法，通过调整参数在并行计算的硬度估计指导下高效识别高质量分解

Result: 在具有挑战性的CircuitSAT实例上展示了算法的实际效果，包括布尔电路逻辑等价性检查和密码哈希函数原像攻击的编码实例

Conclusion: 提出的并行分解算法为处理困难CircuitSAT问题提供了有效的解决方案，在多个实际应用场景中表现出良好性能

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [35] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一个新的基因组基础模型预训练框架，它结合了联合嵌入预测架构和传统生成目标，通过预测掩码基因组片段的高层功能嵌入而非单个核苷酸，来获得更具全局生物学视角的表示。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型主要依赖掩码语言建模或下一标记预测，这些方法擅长捕捉局部基因组语法和细粒度基序模式，但往往无法捕获更广泛的功能上下文，导致表示缺乏全局生物学视角。

Method: 引入JEPA-DNA框架，将联合嵌入预测架构与传统生成目标相结合。通过将标记级恢复与潜在空间中的预测目标耦合，监督CLS标记，迫使模型预测掩码基因组片段的高层功能嵌入，而不仅仅是关注单个核苷酸。

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上始终优于仅生成式基线，提供了更稳健和生物学基础的表示。

Conclusion: JEPA-DNA为基因组基础模型提供了一条可扩展的路径，使其不仅能理解基因组字母表，还能理解序列底层的功能逻辑，实现了对基因组语言更全面的理解。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [36] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个仅含2000万参数的轻量级公式识别模型，通过精心设计、知识蒸馏和词汇表/分词器迁移，性能媲美SOTA模型，同时模型大小减少65-80%，支持消费级硬件实时推理和浏览器部署。


<details>
  <summary>Details</summary>
Motivation: 开发一个轻量级但高性能的公式识别模型，使其能够在消费级硬件上实现实时推理，并支持浏览器内部署，降低计算资源需求。

Method: 采用精心设计、知识蒸馏技术，并迁移词汇表和分词器，构建仅含2000万参数的简约模型架构。

Result: Texo在性能上可与UniMERNet-T和PPFormulaNet-S等SOTA模型相媲美，同时模型大小分别减少了80%和65%，实现了消费级硬件上的实时推理。

Conclusion: Texo证明了通过精心设计和优化，可以在大幅减小模型规模的同时保持高性能，为公式识别任务提供了轻量级解决方案，并开发了Web应用来展示模型能力。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [37] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出基于AI Agent的人文社科研究协作工作流，并以台湾Claude.ai使用数据验证其可行性，强调人类在研究判断、伦理决策等方面的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI研究主要关注软件工程和自然科学领域，人文社科领域的方法论探索有限。本研究旨在填补这一空白，为人文社科研究者提供可复制的AI协作框架。

Method: 提出基于AI Agent的协作研究工作流（Agentic Workflow），包含七个模块化阶段，基于任务模块化、人机分工和可验证性三原则。以台湾Claude.ai使用数据（N=7,729次对话）作为实证载体验证方法可行性。

Result: 成功设计和验证了方法论框架，识别出三种人机协作操作模式：直接执行、迭代优化和人类主导。通过反思性文档记录操作过程，揭示了人类在研究问题制定、理论解释、情境化推理和伦理反思方面的不可替代性。

Conclusion: 本研究为人文社科研究者提供了可复制的AI协作框架，强调人类在研究中的核心作用。同时承认了单平台数据、横断面设计和AI可靠性风险等局限性。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [38] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 提出大型行为模型（LBM），通过行为嵌入而非临时提示来预测个体战略决策，利用结构化心理特征档案提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预测人类高风险决策时存在局限，特别是难以生成一致、个体特定的行为，且提示方法易出现身份漂移，无法充分利用详细的人格描述

Method: 开发大型行为模型（LBM），基于结构化高维特征档案进行行为嵌入，使用连接稳定倾向、动机状态和情境约束与观察选择的数据集进行微调

Result: LBM在保留场景评估中优于未适应的Llama-3.1-8B-Instruct骨干模型，与前沿基线表现相当；随着特征维度增加，性能持续提升，而提示方法存在复杂性上限

Conclusion: LBM为高保真行为模拟提供了可扩展方法，在战略预见、谈判分析、认知安全和决策支持等领域具有应用潜力

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [39] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 该研究使用布卢姆分类法作为分层框架，通过分析大语言模型的高维激活向量，探究不同认知复杂度水平是否在模型的残差流中线性可分，发现线性分类器能达到约95%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的黑箱特性需要超越表面性能指标的新评估框架。本研究旨在探索认知复杂度的内部神经表征，使用布卢姆分类法作为分层视角来分析模型如何处理不同认知层次的任务。

Method: 通过分析不同大语言模型的高维激活向量，使用布卢姆分类法作为认知层次框架，探究从基本回忆到抽象综合的不同认知水平是否在模型残差流中线性可分。研究采用线性分类器来测试这些表征的可分离性。

Result: 线性分类器在所有布卢姆认知层次上达到约95%的平均准确率，表明认知水平在模型表征的线性可访问子空间中被编码。模型在正向传播早期就解析了提示的认知难度，且表征在不同层中变得越来越可分。

Conclusion: 研究提供了强有力的证据，表明认知复杂度在模型内部表征中以线性可访问的方式编码，这为理解大语言模型如何处理不同认知层次的任务提供了新的见解，并为模型评估提供了新的框架。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [40] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 该研究提出了检测和量化大语言模型中时间知识泄漏的框架，通过将模型推理分解为原子声明并应用Shapley值来衡量泄漏信息对预测的贡献，同时提出了主动过滤时间污染的预测方法。


<details>
  <summary>Details</summary>
Motivation: 为了评估LLMs能否准确预测未来事件，需要在已解决的事件上进行回溯测试，但LLMs可能在训练中无意泄漏截止日期后的知识，这会破坏回溯评估的有效性。因此需要检测和量化这种时间知识泄漏。

Method: 提出了一个声明级别的框架：1) 将模型推理分解为原子声明并按时间可验证性分类；2) 应用Shapley值衡量每个声明对预测的贡献，得到Shapley加权决策关键泄漏率(Shapley-DCLR)；3) 提出了TimeSPEC方法，通过生成、声明验证和再生成的交替过程主动过滤时间污染。

Result: 在350个实例的实验（包括美国最高法院案件预测、NBA薪资估计和股票回报排名）中，发现标准提示基线存在显著泄漏。TimeSPEC在保持任务性能的同时降低了Shapley-DCLR，表明显式的、可解释的声明级别验证优于基于提示的时间约束。

Conclusion: 该研究提出的框架能够有效检测和量化LLMs中的时间知识泄漏，TimeSPEC方法通过主动验证声明的时间有效性，为可靠的回溯测试提供了更可靠的解决方案，比简单的提示约束更有效。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [41] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 该研究详细记录了在有限计算资源下（2xA100 GPU）从arXiv LaTeX源文件训练1.36B参数科学语言模型的完整流程，分析了预处理、分词、训练稳定性等工程实践问题。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型虽然展现出强大的推理和数学能力，但训练领域专业化科学语言模型的实际过程缺乏详细文档。本研究旨在为中等计算预算的研究者提供从原始科学文献训练专业模型的工程实践指导。

Method: 构建端到端训练流程：包括元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词，以及在受限计算资源下（2xA100 GPU）的密集Transformer训练。通过24个实验运行分析训练稳定性、扩展行为和数据损失。

Result: 研究发现预处理决策显著影响可用token数量，分词策略影响符号稳定性，存储和I/O约束可能成为与计算同等重要的限制因素。在数据丰富（520亿预训练token）的情况下表现出稳定的训练行为。

Conclusion: 本研究没有提出新架构，而是提供了从零开始训练小型科学语言模型的工程实践透明记录。这些见解有助于在中等计算预算下构建领域专业化模型的研究者。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [42] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无需数据的任务向量正则化方法，通过曲率矩阵近似解决多任务组合中的表示漂移问题，在任务添加和否定任务中达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 任务算术提供了一种模块化、可扩展的方式来调整基础模型，但组合多个任务向量会导致跨任务干扰，引起表示漂移和性能下降。现有表示漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突。

Method: 提出一种无需数据的方法，将表示漂移的正则化框架化为曲率矩阵近似问题。采用Kronecker分解近似曲率技术，获得一个实用的正则化器，在任务添加和否定任务中实现最先进的结果。

Result: 该方法在任务数量上具有恒定复杂度，促进对任务向量缩放的鲁棒性，无需保留调优，在任务添加和否定任务中达到最先进性能。

Conclusion: 通过将表示漂移正则化框架化为曲率矩阵近似问题，提出了一种无需数据的实用方法，解决了任务算术中的跨任务干扰问题，同时保持了模块化和数据隐私要求。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [43] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出将形式化验证与深度学习图像检索结合的新框架，通过图验证和神经代码生成支持开放词汇自然语言查询，提供可验证的检索结果


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言搜索在处理复杂关系、对象组合、精确约束（如身份、数量、比例）等查询时仍存在不可靠问题，需要更可信和可验证的检索框架

Method: 集成形式化验证到深度学习图像检索中，结合图验证方法和神经代码生成，对用户查询中的每个原子事实进行形式化验证

Result: 框架不仅能返回匹配结果，还能识别标记哪些具体约束被满足、哪些未满足，提供更透明和可问责的检索过程，同时提升主流嵌入方法的性能

Conclusion: 通过将检索结果建立在形式化推理系统上，超越了向量表示的模糊性和近似性，为复杂自然语言查询提供了可信赖且可验证的解决方案

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [44] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特定变分编码器、融合瓶颈门控机制和多任务目标，解决NSCLC患者生存预测中多模态数据严重缺失的问题。


<details>
  <summary>Details</summary>
Motivation: NSCLC患者生存预测具有挑战性，多模态数据（全切片图像、转录组、DNA甲基化）提供互补信息但临床数据常严重缺失。现有方法在严重缺失情况下缺乏鲁棒性。

Method: 提出多模态对比变分自编码器（MCVAE）：1）模态特定变分编码器捕获各数据源不确定性；2）融合瓶颈带学习门控机制归一化各模态贡献；3）多任务目标结合生存损失和重建损失；4）跨模态对比损失强制潜在空间对齐；5）训练时应用随机模态掩码增强鲁棒性。

Result: 在TCGA-LUAD（475例）和TCGA-LUSC（446例）数据集上验证，MCVAE在疾病特异性生存预测方面优于两种SOTA模型，对严重缺失场景具有更好鲁棒性。发现并非所有模态组合都能提升任务性能。

Conclusion: MCVAE能有效处理多模态数据严重缺失问题，提升NSCLC生存预测性能。多模态整合并非总是有益，需谨慎选择模态组合。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [45] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 该论文提出了一个基于隐私设计（PbD）的框架，用于指导AI应用（特别是基于大语言模型的应用）在面向儿童时的隐私保护设计，整合了GDPR、PIPEDA、COPPA等法规原则，并通过教育辅导应用案例验证了框架的实用性。


<details>
  <summary>Details</summary>
Motivation: 儿童越来越多地使用AI技术，但存在隐私风险担忧。尽管现有隐私法规要求公司实施保护措施，但在实践中实施这些措施具有挑战性。需要为设计者和开发者提供一个主动、风险规避的框架来设计面向儿童的AI技术。

Method: 提出了一个基于隐私设计（PbD）的框架，整合了GDPR、PIPEDA、COPPA等隐私法规的原则，并将这些原则映射到大语言模型应用的各个阶段（数据收集、模型训练、操作监控、持续验证）。框架还包括基于UNCRC、AADC和学术研究的儿童设计指南，并通过一个面向13岁以下儿童的LLM教育辅导应用案例研究来展示框架的实际应用。

Result: 框架展示了如何通过技术和组织控制等数据保护策略，以及在整个LLM生命周期中做出适合年龄的设计决策，来支持开发既提供隐私保护又符合法律要求的儿童AI应用。案例研究表明该框架在实践中具有可行性和有效性。

Conclusion: 通过整合隐私法规原则和儿童设计指南，基于隐私设计的框架能够帮助AI服务提供商和开发者在开发面向儿童的AI应用时减少隐私风险，同时满足法律标准，为儿童AI应用的隐私保护提供了系统化的指导方法。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [46] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec是一个高性能推荐系统框架，通过后端无关架构解决了学术界与工业界之间的鸿沟，支持50+算法、40+指标和19种过滤分割策略，实现从本地到分布式环境的无缝过渡，同时集成了能源追踪功能以促进可持续发展。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统生态系统存在分裂问题：研究人员必须在易于使用的内存实验和需要重写代码的复杂分布式工业引擎之间做出选择，这阻碍了推荐系统的创新。

Method: 提出WarpRec框架，采用新颖的后端无关架构，包含50多种最先进算法、40个评估指标和19种过滤分割策略，支持从本地执行到分布式训练和优化的无缝过渡，并集成CodeCarbon进行实时能源追踪。

Result: WarpRec消除了学术界与工业界之间的权衡，展示了可扩展性不必以科学完整性或可持续性为代价，同时为推荐系统向生成式AI生态系统中的交互式工具演进提供了架构基础。

Conclusion: WarpRec不仅弥合了学术界与工业界的差距，还可以作为下一代可持续、支持智能体的推荐系统的架构基础，代码已在GitHub开源。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [47] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 该研究提出了一个针对ARM Cortex处理器（M0+、M4、M7）的AI模型优化基准测试框架，重点关注嵌入式系统中的能效、精度和资源利用率，通过帕累托分析平衡能耗与精度，为开发者提供设计高效能AI系统的指导。


<details>
  <summary>Details</summary>
Motivation: 随着AI在嵌入式系统中的广泛应用，如何在资源受限的ARM Cortex处理器上优化AI模型，平衡能效、精度和性能成为关键挑战。现有研究缺乏系统性的基准测试框架来指导开发者在不同处理器上选择最优的AI模型配置。

Method: 设计自动化测试平台，系统评估关键性能指标（KPI），包括能效、精度和资源利用率。通过分析浮点运算（FLOPs）与推理时间的相关性，建立计算需求估算指标。采用帕累托分析来平衡能耗与模型精度的权衡。

Result: 发现FLOPs与推理时间呈近线性相关，可作为可靠的计算需求估算指标。M7处理器适合短推理周期任务，M4处理器在长推理任务中能效更优，M0+处理器适用于简单AI任务。通过帕累托分析确定了能耗与精度的最佳平衡点。

Conclusion: 该研究为嵌入式AI开发者提供了实用的基准测试框架和设计指导，帮助在不同ARM Cortex处理器上选择最优的AI模型配置，实现高性能、高能效的嵌入式AI应用，推动可持续的AI系统设计。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [48] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架将知识图谱与检索增强生成相结合，专门用于提升大语言模型在电信领域的性能，减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电信领域应用面临挑战，包括领域复杂性、标准演进和专用术语，导致通用LLMs在该领域产生幻觉、准确性不足。

Method: 提出KG-RAG框架，将知识图谱（提供电信标准和文档的结构化知识表示）与检索增强生成（动态检索相关事实）相结合，以增强LLMs的电信任务能力。

Result: 在基准数据集上的实验表明，KG-RAG优于纯LLM和标准RAG基线，平均准确率分别提高21.6%和14.3%。

Conclusion: KG-RAG能有效提升大语言模型在复杂电信场景中的准确性、可靠性和可解释性输出。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [49] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 该研究针对多智能体IR管道中LLM智能体交换思维链的问题，提出了两个新评估指标：可重用性和可验证性，以弥补当前仅关注任务准确性的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前思维链评估过于狭隘，仅关注目标任务准确性，无法评估推理过程本身的质量或效用。需要新的评估指标来衡量思维链的质量。

Method: 采用Thinker-Executor框架将思维链生成与执行解耦。可重用性衡量执行者重用思考者思维链的容易程度，可验证性衡量执行者使用思维链匹配思考者答案的频率。评估了4个思考者模型与10个执行者模型委员会在5个基准测试上的表现。

Result: 研究发现可重用性和可验证性与标准准确性不相关，揭示了当前基于准确性的推理能力排行榜存在盲点。令人惊讶的是，专业推理模型生成的思维链并不比通用LLM（如Llama和Gemma）生成的思维链更可重用或可验证。

Conclusion: 需要超越准确性的新评估指标来全面评估思维链质量。思维链的可重用性和可验证性提供了对推理过程质量的重要洞察，这些指标与准确性不相关，表明当前评估体系存在不足。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [50] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练来解决超长时域任务，在PaperBench等基准上超越了Kimi K2 Thinking等模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在处理超长时域任务时存在困难，需要开发专门的方法来训练模型解决这类复杂、多步骤的任务。

Method: 1. 使用轨迹分割SFT冷启动模型：通过保留早期上下文、渐进截断后期上下文并保持子轨迹重叠的方式处理超长轨迹；2. 构建Research-Factory自动化管道生成高质量训练数据；3. 采用渐进式RL训练：分多个阶段进行训练，逐步延长超时时间。

Result: KLong（106B）在PaperBench上超越了Kimi K2 Thinking（1T）11.28%，在SWE-bench Verified和MLE-bench等其他编码基准上也表现出良好的泛化性能。

Conclusion: 提出的轨迹分割SFT和渐进式RL训练方法有效提升了LLM智能体解决超长时域任务的能力，KLong模型在多个基准上表现出优越性和良好的泛化性。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [51] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于常微分方程的统一理论框架ODESteer，用于大语言模型的激活导向对齐，通过屏障函数和多步自适应导向显著提升对齐效果


<details>
  <summary>Details</summary>
Motivation: 当前激活导向方法存在两个关键局限：缺乏统一理论框架指导导向方向设计，以及过度依赖单步导向无法捕捉激活分布的复杂模式

Method: 提出基于常微分方程的ODESteer方法，将传统激活加法解释为ODE的一阶近似，通过定义屏障函数（正负激活的对数密度比）构建多步自适应导向的ODE

Result: ODESteer在多个LLM对齐基准测试中取得一致改进：TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%

Conclusion: 通过ODE统一激活导向的理论基础，提出的ODESteer方法在理论和实证上均取得进展，为大语言模型对齐提供了新的原则性视角

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [52] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 本文提出了一种基于联邦学习的混合AI模型，结合SWIN Transformer和CNN，用于通过X光片诊断COVID-19和肺炎，旨在提高诊断准确性并保护医疗数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的显著提升，人工智能在医疗健康领域应用的机会大大增加。医疗专家和医院需要共享数据空间，但又要保护患者隐私和数据安全。因此需要建立一个安全、分布式的医疗数据处理系统，提高疾病诊断的效率和可靠性。

Method: 采用联邦学习框架下的混合AI模型，结合最新的CNN模型（DenseNet201、Inception V3、VGG 19）和Transformer模型（SWIN Transformer）。使用TensorFlow和Keras框架，以及微软开发的Vision Transformer技术。通过实时持续学习方法来提高疾病诊断和严重程度预测的准确性。

Result: 论文提出了一种能够检测COVID-19和肺炎的混合模型，通过联邦学习确保模型安全性和信息真实性。该模型旨在为医生提供可靠的辅助诊断工具，特别是在应对全球大流行病方面发挥作用。

Conclusion: 联邦学习支持的混合AI模型能够提高疾病诊断的准确性，同时确保医疗数据的安全性和隐私保护。这种技术整合为医疗领域提供了高效可靠的解决方案，有助于全球共同应对疫情挑战。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [53] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于分层离散扩散模型的分子图生成框架，通过引入化学先验编码和解耦原子编码，在MOSES数据集上实现了近乎完美的化学有效性，超越了现有图扩散模型和1D基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于图扩散的分子生成模型存在化学有效性低、难以满足目标属性要求的问题，相比1D建模方法表现不佳。需要开发能够克服这些性能限制的新方法。

Method: MolHIT基于分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别，并采用解耦原子编码策略，根据原子的化学角色分离原子类型。

Result: 在MOSES数据集上实现了新的最先进性能，首次在图扩散中达到近乎完美的化学有效性，在多个指标上超越了强大的1D基线方法。在下游任务如多属性引导生成和支架扩展中也表现出色。

Conclusion: MolHIT成功克服了现有分子图生成方法的长期性能限制，为AI驱动的药物发现和材料科学提供了强大的分子生成框架。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [54] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了之前的研究，增加了语义关系提取任务，要求系统识别两种类型的关系并进行时空推理。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决历史文本中人物与地点关系提取的挑战，特别是在多语言、多时间段的噪声文本中。通过建立评估框架，支持大规模历史数据处理，促进知识图谱构建、历史传记重建和数字人文空间分析等下游应用。

Method: HIPE-2026采用了三方面评估框架：1）准确性评估；2）计算效率评估；3）领域泛化能力评估。系统需要从多语言历史文本中提取两种类型的关系：$at$（人物是否曾到过该地点）和$isAt$（人物在出版时间附近是否位于该地点），这需要基于时间和地理线索进行推理。

Result: 作为评估实验室，HIPE-2026本身不提供具体实验结果，而是建立了一个评估框架和基准数据集。该框架扩展了HIPE-2020和HIPE-2022系列，将关系提取任务引入历史文本处理，为后续研究提供了标准化的评估方法。

Conclusion: HIPE-2026成功地将语义关系提取任务引入历史文本处理领域，通过建立全面的评估框架，促进了人物-地点关系提取技术的发展，为数字人文领域的大规模历史数据处理提供了重要支持，有助于知识图谱构建和历史分析应用。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [55] [Intent Laundering: AI Safety Datasets Are Not What They Seem](https://arxiv.org/abs/2602.16729)
*Shahriar Golchin,Marc Wetter*

Main category: cs.CR

TL;DR: 当前AI安全数据集过度依赖"触发线索"（具有明显负面/敏感含义的词语），无法真实反映现实世界攻击。通过"意图清洗"技术去除这些线索后，所有先前评估为"相对安全"的模型都变得不安全，且该技术作为越狱方法成功率高达90-98%。


<details>
  <summary>Details</summary>
Motivation: 评估广泛使用的AI安全数据集的质量，检验这些数据集是否能真实反映现实世界的攻击行为。研究发现现有数据集过度依赖明显的"触发线索"，这与现实攻击中攻击者会隐藏意图的实际行为存在显著差异。

Method: 从两个角度系统评估：1）孤立评估：检查数据集是否反映现实攻击的三个关键属性（别有用心驱动、精心设计、分布外）；2）实践评估：引入"意图清洗"技术，抽象去除攻击中的触发线索，同时严格保留恶意意图和所有相关细节。

Result: 1）当前AI安全数据集因过度依赖触发线索而无法忠实代表现实攻击；2）去除触发线索后，所有先前评估为"相对安全"的模型（包括Gemini 3 Pro和Claude Sonnet 3.7）都变得不安全；3）意图清洗作为越狱技术，在黑盒访问条件下攻击成功率高达90-98%。

Conclusion: 当前AI安全评估方法与现实世界攻击者行为存在显著脱节。数据集过度依赖明显的触发线索导致安全评估失真，需要开发更能反映现实攻击复杂性的评估方法。

Abstract: We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on "triggering cues": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce "intent laundering": a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues. In fact, once these cues are removed, all previously evaluated "reasonably safe" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated and how real-world adversaries behave.

</details>


### [56] [Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis](https://arxiv.org/abs/2602.16741)
*Scott Thornton*

Main category: cs.CR

TL;DR: 研究发现，在代码漏洞检测任务中，对抗性注释对大型语言模型的性能影响很小且统计上不显著，这与代码生成任务中对抗性提示能显著降低模型性能的情况形成鲜明对比。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明对抗性提示操作会降低LLM在代码生成任务中的性能，本研究旨在探究类似的基于注释的对抗性操作是否也会误导LLM在漏洞检测任务中的表现。

Method: 构建了包含100个样本的基准测试集（涵盖Python、JavaScript和Java），每个样本配有8种注释变体（从无注释到权威欺骗、技术欺骗等对抗策略）。评估了8个前沿模型（5个商业模型和3个开源模型），共进行了9,366次试验。还测试了4种自动防御机制，进行了4,646次额外试验。

Result: 对抗性注释对检测准确率的影响很小且统计上不显著（McNemar精确p值>0.21；所有95%置信区间包含零）。商业模型基线检测率为89-96%，开源模型为53-72%，对抗性注释均未显著降低性能。静态分析交叉引用防御效果最佳（96.9%检测率，恢复了47%的基线漏检）。

Conclusion: 与代码生成任务不同，在漏洞检测任务中，对抗性注释操作不会显著降低LLM性能。更复杂的对抗策略相比简单的操纵性注释没有优势。失败案例主要集中在固有的困难漏洞类别（如竞争条件、时序侧信道、复杂授权逻辑），而非对抗性注释本身。

Abstract: AI-assisted code review is widely used to detect vulnerabilities before production release. Prior work shows that adversarial prompt manipulation can degrade large language model (LLM) performance in code generation. We test whether similar comment-based manipulation misleads LLMs during vulnerability detection. We build a 100-sample benchmark across Python, JavaScript, and Java, each paired with eight comment variants ranging from no comments to adversarial strategies such as authority spoofing and technical deception. Eight frontier models, five commercial and three open-source, are evaluated in 9,366 trials. Adversarial comments produce small, statistically non-significant effects on detection accuracy (McNemar exact p > 0.21; all 95 percent confidence intervals include zero). This holds for commercial models with 89 to 96 percent baseline detection and open-source models with 53 to 72 percent, despite large absolute performance gaps. Unlike generation settings where comment manipulation achieves high attack success, detection performance does not meaningfully degrade. More complex adversarial strategies offer no advantage over simple manipulative comments. We test four automated defenses across 4,646 additional trials (14,012 total). Static analysis cross-referencing performs best at 96.9 percent detection and recovers 47 percent of baseline misses. Comment stripping reduces detection for weaker models by removing helpful context. Failures concentrate on inherently difficult vulnerability classes, including race conditions, timing side channels, and complex authorization logic, rather than on adversarial comments.

</details>


### [57] [The Vulnerability of LLM Rankers to Prompt Injection Attacks](https://arxiv.org/abs/2602.16752)
*Yu Yin,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.CR

TL;DR: 该论文对LLM排序器面临的越狱提示攻击进行了全面的实证研究，评估了不同LLM家族、架构和设置下的漏洞程度，发现编码器-解码器架构对这类攻击具有内在韧性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已成为强大的重排序器，但研究表明简单的提示注入可以显著改变LLM的排序决策。虽然这对基于LLM的排序管道构成严重安全风险，但这种漏洞在不同LLM家族、架构和设置中的持续程度尚未得到充分探索。

Method: 研究采用两个互补任务进行评估：(1) 偏好漏洞评估，通过攻击成功率测量内在易感性；(2) 排序漏洞评估，量化对排序质量的影响。系统检查了三种主流排序范式（成对、列表、集合）下的两种注入变体：决策目标劫持和决策标准劫持。扩展分析涵盖了模型家族的漏洞扩展、位置敏感性、骨干架构和跨领域鲁棒性。

Result: 结果揭示了这些漏洞的边界条件，关键发现包括编码器-解码器架构对越狱攻击表现出强大的内在韧性。研究公开了代码和额外实验结果。

Conclusion: 该研究全面表征了LLM排序器对越狱提示攻击的漏洞，为理解不同LLM架构的安全特性提供了重要见解，特别是编码器-解码器架构的韧性优势。

Abstract: Large Language Models (LLMs) have emerged as powerful re-rankers. Recent research has however showed that simple prompt injections embedded within a candidate document (i.e., jailbreak prompt attacks) can significantly alter an LLM's ranking decisions. While this poses serious security risks to LLM-based ranking pipelines, the extent to which this vulnerability persists across diverse LLM families, architectures, and settings remains largely under-explored. In this paper, we present a comprehensive empirical study of jailbreak prompt attacks against LLM rankers. We focus our evaluation on two complementary tasks: (1) Preference Vulnerability Assessment, measuring intrinsic susceptibility via attack success rate (ASR); and (2) Ranking Vulnerability Assessment, quantifying the operational impact on the ranking's quality (nDCG@10). We systematically examine three prevalent ranking paradigms (pairwise, listwise, setwise) under two injection variants: decision objective hijacking and decision criteria hijacking. Beyond reproducing prior findings, we expand the analysis to cover vulnerability scaling across model families, position sensitivity, backbone architectures, and cross-domain robustness. Our results characterize the boundary conditions of these vulnerabilities, revealing critical insights such as that encoder-decoder architectures exhibit strong inherent resilience to jailbreak attacks. We publicly release our code and additional experimental results at https://github.com/ielab/LLM-Ranker-Attack.

</details>


### [58] [Large-scale online deanonymization with LLMs](https://arxiv.org/abs/2602.16800)
*Simon Lermen,Daniel Paleka,Joshua Swanson,Michael Aerni,Nicholas Carlini,Florian Tramèr*

Main category: cs.CR

TL;DR: 大型语言模型可实现大规模去匿名化攻击，仅凭在线匿名资料和对话就能高精度识别用户身份，性能远超传统方法


<details>
  <summary>Details</summary>
Motivation: 研究在线匿名用户的隐私保护现状，探索LLM在去匿名化攻击中的能力，重新评估在线隐私的威胁模型

Method: 设计基于LLM的攻击流程：1)提取身份相关特征，2)通过语义嵌入搜索候选匹配，3)对候选结果进行推理验证以减少误报；构建三个真实数据集进行评估

Result: LLM方法在三个数据集上显著优于传统基线，在90%精度下达到68%召回率，而最佳非LLM方法接近0%；证明在线匿名用户的"实际模糊性"保护已失效

Conclusion: LLM能够实现大规模高效的去匿名化攻击，在线匿名用户的隐私保护已不再可靠，需要重新考虑在线隐私的威胁模型

Abstract: We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that individual, we implement a scalable attack pipeline that uses LLMs to: (1) extract identity-relevant features, (2) search for candidate matches via semantic embeddings, and (3) reason over top candidates to verify matches and reduce false positives. Compared to prior deanonymization work (e.g., on the Netflix prize) that required structured data or manual feature engineering, our approach works directly on raw user content across arbitrary platforms. We construct three datasets with known ground-truth data to evaluate our attacks. The first links Hacker News to LinkedIn profiles, using cross-platform references that appear in the profiles. Our second dataset matches users across Reddit movie discussion communities; and the third splits a single user's Reddit history in time to create two pseudonymous profiles to be matched. In each setting, LLM-based methods substantially outperform classical baselines, achieving up to 68% recall at 90% precision compared to near 0% for the best non-LLM method. Our results show that the practical obscurity protecting pseudonymous users online no longer holds and that threat models for online privacy need to be reconsidered.

</details>


### [59] [NeST: Neuron Selective Tuning for LLM Safety](https://arxiv.org/abs/2602.16835)
*Sasha Behrouzi,Lichao Wu,Mohamadreza Rostami,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: NeST是一个轻量级、结构感知的安全对齐框架，通过选择性调整安全相关神经元子集来增强拒绝行为，相比全微调减少17,310倍参数更新，将攻击成功率从44.5%降至4.36%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全对齐方法存在成本高、更新困难、效率与安全增益不平衡等问题。全微调计算和存储开销大，参数高效方法如LoRA在安全增益上不一致且对设计选择敏感，安全干预机制如断路器不直接塑造内部安全表示。这些限制阻碍了快速可靠的安全更新。

Method: NeST通过聚类功能一致的安全神经元并强制每个簇内共享更新，选择性地调整安全相关神经元子集，同时冻结模型其余部分，实现有针对性和稳定的安全适应，无需广泛的模型修改或推理时开销。

Result: 在10个开源LLM上的评估显示，NeST将攻击成功率从平均44.5%降至4.36%，不安全生成减少90.2%，平均仅需0.44百万可训练参数，相比全微调减少17,310倍参数更新，相比LoRA减少9.25倍，同时获得更强的安全性能。

Conclusion: NeST提供了一个轻量级、结构感知的安全对齐框架，能够实现快速可靠的安全更新，特别适用于模型频繁演进或需要适应新政策和领域的场景，为负责任的大语言模型部署提供了有效的解决方案。

Abstract: Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.
  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.

</details>


### [60] [Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs](https://arxiv.org/abs/2602.17223)
*Arka Pal,Louai Zahran,William Gvozdjak,Akilesh Potti,Micah Goldblum*

Main category: cs.CR

TL;DR: 本文提出利用隐私保护LLM推理来实现低成本验证推理的新方法，避免传统零知识证明的高计算开销


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增大，用户难以本地运行而依赖第三方托管服务，但缺乏对推理计算过程的保证，不诚实的提供商可能用廉价弱模型替代昂贵大模型

Method: 提出两种新协议，利用隐私保护LLM推理技术，通过添加少量额外token计算来提供推理过程保证，相比零知识证明方法成本更低

Result: 提出的方法成本低廉，仅需少量额外token计算，对下游影响小，且验证运行时间优于零知识证明方法

Conclusion: 该工作揭示了隐私保护与可验证性在LLM推理中的新联系，为低成本验证推理提供了新思路

Abstract: As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely on methods from cryptography such as zero-knowledge proofs (ZKPs), but these add significant computational overhead, and remain infeasible for use for large models. In this work, we develop a new insight -- that given a method for performing private LLM inference, one can obtain forms of verified inference at marginal extra cost. Specifically, we propose two new protocols which leverage privacy-preserving LLM inference in order to provide guarantees over the inference that was carried out. Our approaches are cheap, requiring the addition of a few extra tokens of computation, and have little to no downstream impact. As the fastest privacy-preserving inference methods are typically faster than ZK methods, the proposed protocols also improve verification runtime. Our work provides novel insights into the connections between privacy and verifiability in LLM inference.

</details>


### [61] [Grothendieck Topologies and Sheaf-Theoretic Foundations of Cryptographic Security: Attacker Models and $Σ$-Protocols as the First Step](https://arxiv.org/abs/2602.17301)
*Takao Inoué*

Main category: cs.CR

TL;DR: 该论文提出了一种基于格罗滕迪克拓扑和层理论的密码学安全结构重构方法，将攻击者观察建模为格罗滕迪克位点，协议转录本自然形成层，安全属性表现为几何条件。


<details>
  <summary>Details</summary>
Motivation: 传统密码学安全使用基于游戏或模拟的定义，作者希望通过结构化的数学框架重新表述密码学安全，提供更概念化的解释和几何基础。

Method: 使用格罗滕迪克拓扑和层理论，将攻击者观察建模为格罗滕迪克位点，覆盖族表示由高效模拟确定的局部信息的可容许分解。协议转录本形成层，安全属性作为几何条件出现。

Result: 证明了任何Σ-协议的转录本结构在关联的层拓扑中定义了一个挠子，局部平凡性对应零知识性，全局截面的缺失反映可靠性。以Schnorr Σ-协议为例进行了具体分析。

Conclusion: 这种层理论视角为基于模拟的安全提供了概念解释，并为进一步的密码学抽象提供了几何基础。

Abstract: Cryptographic security is traditionally formulated using game-based or simulation-based definitions. In this paper, we propose a structural reformulation of cryptographic security based on Grothendieck topologies and sheaf theory.
  Our key idea is to model attacker observations as a Grothendieck site, where covering families represent admissible decompositions of partial information determined by efficient simulation. Within this framework, protocol transcripts naturally form sheaves, and security properties arise as geometric conditions.
  As a first step, we focus on $Σ$-protocols. We show that the transcript structure of any $Σ$-protocol defines a torsor in the associated topos of sheaves. Local triviality of this torsor corresponds to zero-knowledge, while the absence of global sections reflects soundness. A concrete analysis of the Schnorr $Σ$-protocol is provided to illustrate the construction.
  This sheaf-theoretic perspective offers a conceptual explanation of simulation-based security and suggests a geometric foundation for further cryptographic abstractions.

</details>


### [62] [Security of the Fischlin Transform in Quantum Random Oracle Model](https://arxiv.org/abs/2602.17307)
*Christian Majenz,Jaya Sharma*

Main category: cs.CR

TL;DR: Fischlin变换在量子随机预言机模型中保持直线可提取性，为后量子安全NIZK提供了更小证明尺寸的替代方案。


<details>
  <summary>Details</summary>
Motivation: Fischlin变换在经典随机预言机模型中能产生直线可提取的非交互式零知识证明，但它在量子随机预言机模型中的安全性一直未得到证明，因为量子查询的复杂性使得分析困难。

Method: 使用压缩预言机方法构建提取器，结合独立随机变量和的尾界、鞅理论、对称化、查询振幅和量子并集界等技术进行证明。

Result: 证明了Fischlin变换在量子随机预言机模型中保持直线可提取性，建立了其量子后安全性，相比Pass变换具有更小的证明尺寸。

Conclusion: Fischlin变换在量子随机预言机模型中仍然是安全的，为后量子直线可提取NIZK提供了一个有效的替代方案，具有实际应用价值。

Abstract: The Fischlin transform yields non-interactive zero-knowledge proofs with straight-line extractability in the classical random oracle model. This is done by forcing a prover to generate multiple accepting transcripts through a proof-of-work mechanism. Whether the Fischlin transform is straight-line extractable against quantum adversaries has remained open due to the difficulty of reasoning about the likelihood of query transcripts in the quantum-accessible random oracle model (QROM), even when using the compressed oracle methodology. In this work, we prove that the Fischlin transform remains straight-line extractable in the QROM, via an extractor based on the compressed oracle. This establishes the post-quantum security of the Fischlin transform, providing a post-quantum straight-line extractable NIZK alternative to Pass' transform with smaller proof size. Our techniques include tail bounds for sums of independent random variables and for martingales as well as symmetrization, query amplitude and quantum union bound arguments.

</details>


### [63] [DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing](https://arxiv.org/abs/2602.17413)
*René Brinkhege,Prahlad Menon*

Main category: cs.CR

TL;DR: DAVE是一个基于LLM的发言人系统，通过自然语言接口执行使用策略，在查询时虚拟屏蔽敏感信息，避免原始文档共享


<details>
  <summary>Details</summary>
Motivation: 当前组织间数据空间的使用策略主要在资产级别执行，整个文档要么共享要么不共享。当文档只有部分内容敏感时，提供者需要手动编辑文档，成本高、粒度粗且难以维护。需要更细粒度的策略执行机制

Method: 提出DAVE系统：一个基于LLM的使用策略执行发言人，通过自然语言接口回答问题，响应受机器可读使用策略约束。采用虚拟编辑技术，在查询时屏蔽敏感信息而不修改源文档。架构集成Eclipse Dataspace Components和ODRL风格策略

Result: 目前主要是架构设计，尚未完全实现或实证评估。提出了初步的提供方集成原型，将QA请求路由通过发言人服务而非触发原始文档传输。提出了评估方法学来评估安全、效用和性能权衡

Conclusion: DAVE为多方数据空间中LLM访问的系统治理提供了架构基础，通过虚拟编辑实现细粒度的策略执行，避免敏感信息泄露。需要未来实证工作来评估安全、效用和性能的权衡

Abstract: In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over private documents on behalf of a data provider. Instead of releasing documents, the provider exposes a natural language interface whose responses are constrained by machine-readable usage policies. We formalize policy-violating information disclosure in this setting, drawing on usage control and information flow security, and introduce virtual redaction: suppressing sensitive information at query time without modifying source documents. We describe an architecture for integrating such a spokesperson with Eclipse Dataspace Components and ODRL-style policies, and outline an initial provider-side integration prototype in which QA requests are routed through a spokesperson service instead of triggering raw document transfer. Our contribution is primarily architectural: we do not yet implement or empirically evaluate the full enforcement pipeline. We therefore outline an evaluation methodology to assess security, utility, and performance trade-offs under benign and adversarial querying as a basis for future empirical work on systematically governed LLM access to multi-party data spaces.

</details>


### [64] [Privacy in Theory, Bugs in Practice: Grey-Box Auditing of Differential Privacy Libraries](https://arxiv.org/abs/2602.17454)
*Tudor Cebere,David Erb,Damien Desfontaines,Aurélien Bellet,Jack Fitzsimons*

Main category: cs.CR

TL;DR: Re:cord-play是一种灰盒审计范式，通过检查DP算法的内部状态来检测隐私违规，相比现有方法更实用有效。


<details>
  <summary>Details</summary>
Motivation: 差分隐私实现容易出错，现有验证方法不实用：形式化工具限制太多，黑盒统计审计对复杂流程不可行且无法定位bug来源。

Method: 引入Re:cord-play灰盒审计范式，通过在有相同随机性的相邻数据集上运行插桩算法，直接检查数据依赖的控制流，并通过比较声明敏感度与内部输入间经验距离来具体证伪敏感度违规。

Result: 审计12个开源库（包括SmartNoise SDK、Opacus、Diffprivlib），发现13个影响理论保证的隐私违规，证明该方法有效且必要。

Conclusion: 该方法为DP开发者提供了有效、计算成本低且无缝集成的隐私测试框架，可作为软件开发周期的一部分。

Abstract: Differential privacy (DP) implementations are notoriously prone to errors, with subtle bugs frequently invalidating theoretical guarantees. Existing verification methods are often impractical: formal tools are too restrictive, while black-box statistical auditing is intractable for complex pipelines and fails to pinpoint the source of the bug. This paper introduces Re:cord-play, a gray-box auditing paradigm that inspects the internal state of DP algorithms. By running an instrumented algorithm on neighboring datasets with identical randomness, Re:cord-play directly checks for data-dependent control flow and provides concrete falsification of sensitivity violations by comparing declared sensitivity against the empirically measured distance between internal inputs. We generalize this to Re:cord-play-sample, a full statistical audit that isolates and tests each component, including untrusted ones. We show that our novel testing approach is both effective and necessary by auditing 12 open-source libraries, including SmartNoise SDK, Opacus, and Diffprivlib, and uncovering 13 privacy violations that impact their theoretical guarantees. We release our framework as an open-source Python package, thereby making it easy for DP developers to integrate effective, computationally inexpensive, and seamless privacy testing as part of their software development lifecycle.

</details>


### [65] [Coin selection by Random Draw according to the Boltzmann distribution](https://arxiv.org/abs/2602.17490)
*Jan Lennart Bönsel,Michael Maurer,Silvio Petriconi,Andrea Tundis,Marc Winstel*

Main category: cs.CR

TL;DR: 本文提出了一种基于统计物理原理的Boltzmann Draw算法，用于改进加密货币和CBDC中的币选择问题，相比随机抽取和贪心算法，能更好地控制输入代币数量、减少粉尘生成并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 在基于代币的支付系统（如加密货币和央行数字货币）中，币选择问题涉及如何选择一组代币来为交易提供资金。现有方法存在效率、隐私和性能方面的不足，需要一种更好的算法来处理大规模高频交易。

Method: 提出Boltzmann Draw概率算法，基于统计物理中的玻尔兹曼分布原理，作为随机抽取方法的扩展和改进。算法通过玻尔兹曼分布来抽取代币，能够高效实现并满足隐私要求。

Result: 数值结果表明，该方法能有效限制所选输入代币的数量，减少粉尘生成，限制钱包中的代币池大小。算法实现高效，提升性能并尊重隐私要求，相比随机抽取和贪心算法具有优势。

Conclusion: Boltzmann Draw算法在币选择问题上优于随机抽取和贪心算法，对基于代币的技术具有重要意义，特别是对于需要处理大规模高频交易的央行数字货币作为法定货币的应用场景。

Abstract: Coin selection refers to the problem of choosing a set of tokens to fund a transaction in token-based payment systems such as, e.g., cryptocurrencies or central bank digital currencies (CBDCs). In this paper, we propose the Boltzmann Draw that is a probabilistic algorithm inspired by the principles of statistical physics. The algorithm relies on drawing tokens according to the Boltzmann distribution, serving as an extension and improvement of the Random Draw method. Numerical results demonstrate the effectiveness of our method in bounding the number of selected input tokens as well as reducing dust generation and limiting the token pool size in the wallet. Moreover, the probabilistic algorithm can be implemented efficiently, improves performance and respects privacy requirements - properties of significant relevance for current token-based technologies. We compare the Boltzmann draw to both the standard Random Draw and the Greedy algorithm. We argue that the former is superior to the latter in the sense of the above objectives. Our findings are relevant for token-based technologies, and are also of interest for CBDCs, which as a legal tender possibly needs to handle large transaction volumes at a high frequency.

</details>


### [66] [What Makes a Good LLM Agent for Real-world Penetration Testing?](https://arxiv.org/abs/2602.17622)
*Gelei Deng,Yi Liu,Yuekang Li,Ruozhao Yang,Xiaofei Xie,Jie Zhang,Han Qiu,Tianwei Zhang*

Main category: cs.CR

TL;DR: 论文分析了28个基于LLM的渗透测试系统，发现两种失败模式：A类（工具/提示不足）可通过工程解决；B类（规划/状态管理限制）源于缺乏实时任务难度估计。作者提出Excalibur系统，通过工具技能层解决A类失败，通过任务难度评估机制和证据引导攻击树搜索解决B类失败。


<details>
  <summary>Details</summary>
Motivation: LLM-based agents在自动化渗透测试方面有前景，但不同系统和基准测试的性能差异很大。需要深入分析失败原因并开发更可靠的系统。

Method: 1. 分析28个LLM-based渗透测试系统，评估5个代表性实现；2. 提出Excalibur系统：包含工具技能层（解决Type A失败）和任务难度评估机制（解决Type B失败）；3. 使用证据引导攻击树搜索框架进行难度感知规划。

Result: Excalibur在CTF基准测试中达到91%的任务完成率（比基线相对提升39-49%）；在GOAD Active Directory环境中攻陷4/5个主机（先前系统仅2个）。难度感知规划在不同模型上都能带来一致的端到端性能提升。

Conclusion: Type B失败的根源是缺乏实时任务难度估计，导致努力分配不当。难度感知规划能显著提升LLM-based渗透测试系统的性能，这是仅靠模型缩放无法解决的问题。

Abstract: LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains.
  Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate.

</details>


### [67] [Non-Trivial Zero-Knowledge Implies One-Way Functions](https://arxiv.org/abs/2602.17651)
*Suvradip Chakraborty,James Hulett,Dakshita Khurana,Kabir Tomer*

Main category: cs.CR

TL;DR: 该论文证明了在NP不包含于ioP/poly的假设下，非平凡（高错误率）的零知识论证可以推导出单向函数的存在性，填补了先前研究的空白。


<details>
  <summary>Details</summary>
Motivation: 先前研究已证明，如果NP不包含于ioP/poly，那么具有可忽略错误的NP零知识论证意味着单向函数的存在。然而，对于高错误率（非平凡）的零知识论证是否也能推导出单向函数，这个问题仍然开放。本文旨在填补这一空白，研究高错误率零知识论证与单向函数存在性之间的关系。

Method: 作者定义了"非平凡"零知识论证：完整性、可靠性和零知识错误之和远离1。在NP不包含于ioP/poly的假设下，证明了：1）非平凡的非交互式零知识（NIZK）论证意味着单向函数的存在；2）非平凡的常数轮公开掷币零知识论证也意味着单向函数的存在。利用已知的放大技术，这些结果提供了从弱NIZK到标准NIZK的无条件转换。

Result: 1. 在NP不包含于ioP/poly的假设下，非平凡NIZK论证意味着单向函数的存在
2. 同样条件下，非平凡的常数轮公开掷币零知识论证也意味着单向函数的存在
3. 填补了先前研究的空白，特别是当ε_zk + √ε_s ≥ 1时的情况
4. 提供了从弱NIZK到标准NIZK的无条件转换

Conclusion: 本文证明了在标准复杂性假设下，即使是高错误率的非平凡零知识论证也足以推导出单向函数的存在性。这一结果不仅填补了先前研究的空白，还为从最坏情况硬度构造单向函数提供了有用的技术基础。特别地，该工作扩展了零知识论证与单向函数存在性之间的关系，涵盖了更广泛的错误参数范围。

Abstract: A recent breakthrough [Hirahara and Nanashima, STOC'2024] established that if $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$, the existence of zero-knowledge with negligible errors for $\mathsf{NP}$ implies the existence of one-way functions (OWFs). In this work, we obtain a characterization of one-way functions from the worst-case complexity of zero-knowledge {\em in the high-error regime}.
  We say that a zero-knowledge argument is {\em non-trivial} if the sum of its completeness, soundness and zero-knowledge errors is bounded away from $1$. Our results are as follows, assuming $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$:
  1. {\em Non-trivial} Non-Interactive ZK (NIZK) arguments for $\mathsf{NP}$ imply the existence of OWFs. Using known amplification techniques, this result also provides an unconditional transformation from weak to standard NIZK proofs for all meaningful error parameters.
  2. We also generalize to the interactive setting: {\em Non-trivial} constant-round public-coin zero-knowledge arguments for $\mathsf{NP}$ imply the existence of OWFs, and therefore also (standard) four-message zero-knowledge arguments for $\mathsf{NP}$.
  Prior to this work, one-way functions could be obtained from NIZKs that had constant zero-knowledge error $ε_{zk}$ and soundness error $ε_{s}$ satisfying $ε_{zk} + \sqrt{ε_{s}} < 1$ [Chakraborty, Hulett and Khurana, CRYPTO'2025]. However, the regime where $ε_{zk} + \sqrt{ε_{s}} \geq 1$ remained open. This work closes the gap, and obtains new implications in the interactive setting. Our results and techniques could be useful stepping stones in the quest to construct one-way functions from worst-case hardness.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [68] [Low-Cost IoT-Enabled Tele-ECG Monitoring for Resource-Constrained Settings: System Design and Prototype](https://arxiv.org/abs/2602.17114)
*Seemron Neupane,Aashish Ghimire*

Main category: cs.AR

TL;DR: 本文提出了一种基于物联网的远程心电图监测系统，旨在解决慢性疾病患者（特别是心血管疾病）的远程医疗需求，通过连接操作员、医生和服务器三方来降低医疗成本和旅行负担。


<details>
  <summary>Details</summary>
Motivation: 随着自动化机械的普及，人们活动减少导致疾病增加，而许多地区缺乏基本医疗设施。心血管疾病如果早期发现和干预可以极大程度治愈，但患者需要定期健康检查和紧急医疗关注。物联网技术的快速发展为远程医疗提供了可能。

Method: 论文提出了一种基于物联网的远程心电图监测系统，该系统包含三个主要组成部分：操作员（负责数据采集）、医生（负责诊断）和服务器（负责数据传输和处理）。通过这种三方架构实现远程医疗支持。

Result: 该系统能够显著降低患者的旅行成本和相关费用，使社区卫生工作者能够通过远程医疗支持和赋能患者。虽然仍存在一些财务和物流负担，但整体上提高了慢性疾病患者的医疗可及性。

Conclusion: 物联网技术在远程医疗领域具有巨大潜力，特别是对于需要定期监测的慢性疾病患者。通过连接操作员、医生和服务器三方的远程心电图监测系统，可以有效支持患者管理，减少医疗成本，提高医疗服务的可及性。

Abstract: With the availability of automation machinery and its superiority, are being slothful and inviting many diseases to invade them. The world still has so many places where people lack basic health facilities. Due to early detection and intervention, CDV can be cured to an extreme extent. It heavily reduces travel and associated costs. A remote ECG monitoring system enables community health workers to support and empower patients through telemedicine. However, there remains some financial and logistical burden. Heart disease cannot be taken lightly. These patients require regular health check-ups and the attention of health personnel in a short period if their health deteriorates suddenly and rapidly. Chronic diseases are extremely variable in their symptoms and evolution of treatment. Some, if not treated early, will end the patient's life. The trend of the INTERNET OF THINGS, IoT, is spreading massively. This paper focuses on the three main: the operator, the doctor, and the server over which the data is being sent.

</details>


### [69] [When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning](https://arxiv.org/abs/2602.17520)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: LLMs在标准数字逻辑任务上表现良好，但在局部重新定义语义时会出现系统性失败，包括语义覆盖错误（忽略提示中的局部定义）和假设注入错误（做出未声明的假设）。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在形式化场景（如电路规范、考试、硬件文档）中处理局部重新定义语义的能力，这些场景要求模型暂时抑制全局学习惯例而采用提示中的局部定义。

Method: 创建了一个包含30个逻辑和数字电路推理任务的微基准测试，涵盖布尔代数、运算符重载、重新定义的门电路和电路级语义，作为验证器式的陷阱来测试三个前沿LLMs。

Result: 评估发现LLMs存在持续不遵守局部规范的问题，包括自信但不兼容的假设、即使在基础设置中也忽略约束条件，表现出表面正确性与规范忠实推理之间的差距。

Conclusion: 研究揭示了LLMs在形式化领域中局部遗忘和语义合规性方面的缺陷，需要开发专门测试这些能力的评估协议。

Abstract: Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.

</details>
