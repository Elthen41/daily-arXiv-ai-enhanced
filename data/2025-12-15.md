<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver框架通过模块化协作和双级规划，在固定预算下优化多智能体系统中的测试时计算分配，显著提升协作性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术（如重复采样、自我验证）在单智能体上表现良好，但缺乏在多智能体系统中分配计算以促进协作的机制，特别是在预算约束下。

Method: 提出FutureWeaver框架：1）通过自对弈反思抽象历史轨迹中的交互模式，形成可调用的模块化协作函数；2）采用双级规划架构，在当前任务状态推理基础上推测未来步骤，优化计算分配。

Result: 在复杂智能体基准测试中，FutureWeaver在不同预算设置下均优于基线方法，验证了其在推理时优化多智能体协作的有效性。

Conclusion: FutureWeaver为多智能体系统中的测试时计算分配提供了原则性框架，通过模块化协作和前瞻性规划，在预算约束下显著提升了协作性能。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [2] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 本文首次为大型视觉语言模型（LVLMs）引入了一个名为CAPTURE的CAPTCHA基准测试，涵盖4种主要类型和25种子类型，用于全面评估LVLMs解决验证码的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉验证码的基准测试存在局限性，无法全面覆盖所有验证码类型，且缺乏专门针对LVLMs的基准测试。现有研究在设计基准和数据集时通常根据特定研究目标定制，导致评估不够全面。

Method: 创建CAPTURE基准测试，包含来自31个供应商的4种主要验证码类型和25种子类型。该基准具有广泛的类别多样性、大规模数据以及专门为LVLMs定制的标签，填补了数据全面性和标签针对性方面的空白。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决验证码方面表现不佳，显示出LVLMs在实际验证码识别任务中的局限性。

Conclusion: CAPTURE基准测试为LVLMs提供了一个全面、多维度的验证码评估框架，填补了该领域的研究空白，并揭示了当前LVLMs在验证码解决能力方面的不足。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [3] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个任务完成框架，使基于LLM的智能体能够在强化学习形式化描述的环境中，在明确的行为指导下执行任务，提高可靠性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然展现出强大的推理和生成能力，但在多轮任务中的行为往往缺乏可靠性和可验证性，需要一种能够提供明确行为指导的框架。

Method: 框架包含三个组件：轻量级任务分析器选择推理和生成策略；推理模块学习可验证的观察-动作映射；生成模块通过验证或确定性合成确保约束合规的输出。这些组件在智能体与环境交互过程中协同进化。

Result: 随着智能体与环境的交互，这些组件协同进化，产生可信赖的行为。

Conclusion: 该框架能够使LLM智能体在强化学习形式化环境中，在明确的行为指导下执行任务，提高行为的可靠性和可验证性。

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [4] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，采用先骨干后拓扑的设计方法，相比现有方法在相同预算下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统在构建时很少考虑明确的token成本和延迟预算约束，导致在预算有限时成本效益不佳。需要一种能够在明确预算约束下优化多智能体系统设计的框架。

Method: 采用先骨干后拓扑的两阶段设计：1)骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干的智能体；2)自适应MAS拓扑生成：通过智能体表示学习、门控和延迟感知拓扑合成指导智能体间通信。

Result: 在包含14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%的性能提升，在延迟预算下实现高达22%的性能提升，并在性能-预算曲线上表现出强大的AUC。

Conclusion: AgentBalance是一个有效的框架，能够在明确预算约束下构建成本效益的多智能体系统，可作为现有MAS的插件提升性能，并能很好地泛化到未见过的LLM，适用于实际的预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [5] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 该论文指出当前XAI中基于基线函数的保真度评估指标存在问题，不同基线的选择会偏向不同的归因方法，甚至导致矛盾结果。作者研究了基线的两个理想属性，发现现有基线无法同时满足，并提出了新的模型依赖基线来改善这一权衡。


<details>
  <summary>Details</summary>
Motivation: 当前XAI中广泛使用的归因方法通常通过Insertion和Deletion等保真度指标进行评估，这些指标依赖于基线函数来修改输入图像中被归因图认为最重要的像素。然而，作者发现不同基线的选择会不可避免地偏向某些归因方法，甚至导致矛盾的结果，这引发了关于应该使用哪个基线的根本问题。

Method: 作者通过研究基线的两个理想属性来分析该问题：(1) 能够移除信息；(2) 不会产生过度分布外(OOD)的图像。首先测试了现有基线，发现它们都无法同时满足这两个标准，存在权衡关系。然后，作者利用特征可视化的最新工作，提出了一种新的模型依赖基线，该基线能够移除信息而不会产生过度OOD的图像。

Result: 研究发现现有基线都无法同时满足信息移除和避免过度OOD图像这两个标准，存在明显的权衡关系。作者提出的新基线在权衡方面优于其他现有基线，能够更好地移除信息而不产生过度OOD的图像。

Conclusion: 当前XAI评估中使用的基线选择对归因方法的评估结果有重大影响，不同基线会偏向不同的方法。作者提出的模型依赖基线在信息移除和避免过度OOD图像之间取得了更好的平衡，为更公平的归因方法评估提供了改进方案。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [6] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，旨在弥合开源系统与专有前沿模型在复杂推理和长上下文理解方面的差距，通过创新的训练方法在有限计算资源下实现与更大模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型与专有前沿模型在复杂推理和长上下文理解方面的性能差距，同时应对推理适应过程中常见的模型崩溃和训练不稳定问题。

Method: 采用综合训练方案：1) 使用混合并行和内核级优化的内存高效基础设施支持64K-token上下文；2) 两阶段监督微调课程，通过验证对齐的合成数据缓解分布不匹配；3) 强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编程和智能体基准测试中实现了与参数数量显著更大的模型相当的性能，为社区提供了具有竞争力的开源模型。

Conclusion: 该研究提供了一个实用的蓝图，展示了如何在现实计算约束下扩展推理能力，弥合了开源模型与专有前沿模型在复杂推理任务上的差距。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [7] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 本文首次系统比较了经典优化方法与AI方法在"No-Three-In-Line"问题上的表现，ILP在19×19网格内获得最优解，PatternBoost在14×14网格内匹配最优性能，PPO在10×10网格表现完美但在11×11网格失败。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line问题是组合几何中的经典问题，传统整数线性规划(ILP)方法虽然能保证最优解，但随着网格规模增大面临指数级复杂度增长。机器学习方法为模式近似提供了有前景的替代方案，但缺乏系统比较。

Method: 首次将PatternBoost变压器学习和强化学习(PPO)应用于该问题，并与传统ILP方法进行比较。ILP作为基准提供最优解，PatternBoost通过变压器学习模式，PPO通过强化学习优化点配置。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格获得完美解，但在11×11网格因约束违规而失败。

Conclusion: 经典优化方法对于精确解仍然至关重要，而AI方法在较小实例上提供有竞争力的性能。混合方法为扩展到更大问题规模提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [8] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID框架系统评估AI文本检测器在人口统计学、年龄、教育水平、方言、正式程度、政治倾向和主题等7个维度的偏见，发现检测器对少数群体文本存在系统性偏差


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中广泛应用，但缺乏对更广泛社会语言因素的系统性偏见评估。先前研究仅发现对英语学习者的孤立偏见案例，需要更全面的评估框架

Method: 提出BAID评估框架，包含7大类别（人口统计学、年龄、教育水平、方言、正式程度、政治倾向、主题）超过20万个样本，并为每个样本生成保留原始内容但反映特定群体写作风格的合成版本，评估4个开源最先进的AI文本检测器

Result: 发现检测器存在一致的性能差异，特别是对来自代表性不足群体的文本召回率较低，表明系统性偏见问题

Conclusion: BAID提供了一个可扩展、透明的AI检测器审计方法，强调在这些工具公开部署前需要进行偏见感知评估

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [9] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 研究通过模拟真实临床场景，系统评估主流大语言模型从含噪声冗余的患者主诉中提取核心医疗信息的能力，发现模型存在类似代谢功能障碍的功能缺陷，提出"AI-MASLD"新概念。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在真实临床环境中的实际表现，验证其在处理含噪声冗余医疗信息时是否会出现类似代谢功能障碍性脂肪肝病(MASLD)的功能衰退，为AI在医疗领域的应用提供安全警示。

Method: 采用基于标准化医疗探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四种主流LLM，使用包含20个医疗探针、覆盖5个核心维度的评估系统模拟真实临床沟通环境，由两位独立临床医生进行双盲逆向评分。

Result: 所有测试模型均表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差；极端噪声条件下多数模型出现功能崩溃；GPT-4o在深静脉血栓继发肺栓塞风险评估中做出严重误判。

Conclusion: 首次实证确认LLMs在处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念；强调当前LLMs必须在人类专家监督下作为辅助工具使用，其理论知识与实际临床应用仍存在显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [10] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文提出AI基准测试需要从静态评估转向动态自适应框架，以应对AI快速发展和实际部署需求，并倡导建立"AI基准测试工艺"的教育体系。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试面临多重挑战：模型架构快速演进、规模扩大、数据集更新、部署环境多样化，导致静态基准测试与真实世界性能存在差距。大型语言模型容易记忆静态基准，使得评估结果失真。需要新的基准测试方法来匹配科学评估与部署风险。

Method: 提出动态自适应基准测试框架，包含持续演进的模型、更新的数据和异构平台。强调需要技术创新的同时，建立系统化的"AI基准测试工艺"教育体系，培养基准设计和使用方面的专业能力。通过社区合作（如MLCommons、DOE万亿参数联盟等）提供支持。

Result: 识别出当前基准测试的主要障碍：高资源需求、专用硬件访问有限、基准设计专业知识缺乏、结果与应用领域关联不确定性。当前基准过于强调顶级硬件上的峰值性能，对多样化实际场景指导有限。

Conclusion: 基准测试必须向动态化、包容性方向发展，保持透明度、可重复性和可解释性。通过技术创新和系统教育相结合，建立可持续的专业知识体系，使评估能够跟上AI发展步伐，支持负责任、可重复和可访问的AI部署。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [11] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结构因果模型的能源需求预测方法，通过分析天气和日历因素之间的因果关系，构建贝叶斯模型实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受多种因素影响，包括天气条件和日历信息，这些因素之间存在复杂的因果关系，简单的基于相关性的学习方法无法充分处理这种复杂性。

Method: 1. 提出结构因果模型来解释变量间的因果关系；2. 进行完整分析验证因果信念；3. 基于学到的因果洞察作为先验知识构建贝叶斯模型；4. 在未见数据上训练和测试模型。

Result: 1. 因果模型揭示了能源需求对温度波动的响应具有季节依赖性敏感性；2. 冬季能源需求方差较低，因为温度变化与日常活动模式之间存在解耦效应；3. 贝叶斯模型在测试集上达到3.84%的MAPE（平均绝对百分比误差），表现优异；4. 跨两年数据的交叉验证平均MAPE为3.88%，显示出强大的鲁棒性。

Conclusion: 通过结合结构因果分析和贝叶斯建模，该方法能够有效捕捉能源需求预测中的复杂因果关系，实现了最先进的预测性能，并为电网运营和能源管理提供了有价值的洞察。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [12] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 本文提出了一种新的任务调度算法，通过显式优化CPU、磁盘和I/O资源利用率来降低云服务的能耗。


<details>
  <summary>Details</summary>
Motivation: 云服务能耗巨大，而现有能源有限且对环境有温室效应。现有研究要么最大化平均资源利用率，要么最小化任务完成时间，但都未考虑物理机中不同类型资源（CPU、磁盘、I/O）的差异。

Method: 提出一种任务调度算法，使用适应度函数来显式优化CPU、磁盘和I/O资源利用率。适应度值是CPU、磁盘、I/O利用率和任务处理时间的函数。

Result: 通过合成数据集对提出的算法和现有MaxUtil算法进行广泛模拟。结果显示，提出的算法在能源效率方面表现更好，比MaxUtil算法消耗更少的能量。

Conclusion: 提出的任务调度算法通过显式优化多种资源利用率，能够有效降低云服务能耗，是一种更好的能源高效算法。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [13] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX是一个AI系统，能够大规模生成功能正确的Triton PyTorch ATen内核，用于新兴加速器平台，重点是覆盖率和正确性而非性能优化。


<details>
  <summary>Details</summary>
Motivation: 为新兴加速器平台快速生成完整的PyTorch ATen后端内核，传统方法只关注少数高性能内核，而TritorX旨在实现整个算子集的全面覆盖。

Method: 集成开源大语言模型、自定义linter、JIT编译和基于PyTorch OpInfo的测试框架，兼容真实MTIA芯片和硬件仿真环境。

Result: 成功为481个独特的ATen算子生成内核和包装器，通过了所有对应的PyTorch OpInfo测试（总计超过20,000个）。

Conclusion: TritorX能够在一夜之间为新加速器平台生成完整的PyTorch ATen后端，为硬件开发提供了快速适配解决方案。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [14] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文对实时迁移技术进行了全面综述，重点关注容器和虚拟机两种迁移方式，分析技术现状、采用差异、实际挑战，并为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有综述往往忽略实时迁移技术在实际应用中的关键技术细节和实际挑战。本文旨在填补这一空白，通过多维度综合分析，为实际部署提供指导。

Method: 整合现有综述内容，从迁移技术、迁移单元、基础设施特性等多个维度对实时迁移技术进行全面分析，重点关注容器和虚拟机两种迁移方式。

Result: 发现实时迁移技术虽然广泛研究，但其对多种系统因素的依赖带来了挑战，在某些情况下复杂性和资源需求可能超过收益。容器和虚拟机迁移在采用率上存在差异。

Conclusion: 本文为实时迁移爱好者提供了宝贵资源，同时通过概述当前技术挑战和提供未来研究方向指南，有助于推动实时迁移技术在各种计算环境中的实际应用。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [15] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 本文系统评估了GPU集群利用率低的问题，提出了三种动态调度器（HPS、PBS、SBS），在64-GPU集群上显著提升了利用率、吞吐量和公平性，相比静态调度器有显著改进。


<details>
  <summary>Details</summary>
Motivation: GPU集群在现代AI系统中至关重要，但实际部署的平均利用率仅为50%左右。这种低效主要由资源碎片化、异构工作负载和静态调度策略的局限性引起，需要更有效的调度方案来提升集群效率。

Method: 提出了三种专门的动态调度器：混合优先级调度器（HPS）、预测性回填调度器（PBS）和智能批处理调度器（SBS）。在包含1000个AI作业的64-GPU、8节点集群上进行控制模拟评估，工作负载包括训练、推理和研究任务。

Result: 静态调度器（FIFO、SJF等）的GPU利用率为45-67%，吞吐量为12.5-18.3作业/小时，存在严重饥饿问题（最多156个作业等待超过30分钟）。动态调度器显著优于静态策略：HPS达到最高利用率78.2%、最高吞吐量25.8作业/小时、最低公平性方差457，饥饿作业减少到12个；PBS达到76.1%利用率；SBS达到74.6%利用率。

Conclusion: 动态多目标调度器在所有关键指标上均优于单目标启发式方法。针对性的透明调度策略能够显著提升异构AI集群的GPU效率，为未来生产调度框架提供了实用基础。

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [16] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 本文比较了集中式分层联邦学习（HFL）与两种去中心化方法（AFL和CFL），发现去中心化方法在多个指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 联邦学习领域近年来在去中心化方法上取得显著进展，但集中式HFL面临通信瓶颈和隐私问题，需要比较不同架构的性能差异。

Method: 对集中式分层联邦学习（HFL）、去中心化聚合联邦学习（AFL）和去中心化持续联邦学习（CFL）进行综合比较，使用Fashion MNIST和MNIST数据集进行评估。

Result: AFL和CFL在精确度、召回率、F1分数和平衡准确率等指标上均优于HFL，展示了去中心化方法的性能优势。

Conclusion: 去中心化聚合机制在AFL和CFL中有效支持分布式设备间的协作模型训练，为联邦学习研究者和实践者提供了向去中心化方法发展的指导。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [17] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 论文提出三种GPU原生编译方法消除CPU-GPU数据传输延迟：并行传统编译、神经编译和混合架构，可实现10-100倍代码迭代加速


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统在编译、执行和测试阶段存在显著的CPU-GPU数据传输延迟瓶颈，需要消除这些传输以提高效率

Method: 提出三种互补的GPU原生编译方法：1) 适应GPU执行的并行传统编译；2) 使用学习序列到序列翻译和概率验证的神经编译；3) 结合两者的混合架构

Result: 理论分析显示：传统GPU编译通过消除传输实现2-5倍改进，神经编译通过大规模并行实现10-100倍加速，混合方法提供具有保证正确性的实用部署路径

Conclusion: GPU原生编译能显著提升代码迭代速度，概率验证框架允许在编译准确性和并行探索之间权衡，对自改进AI系统和未来模拟计算基板有重要影响

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [18] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux是一个针对强化学习后训练中rollout-training解耦架构的集群调度框架，通过跨集群编排回收同步依赖带来的空闲时间，将成本效率提升1.84倍


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练中，rollout（内存密集型）和training（计算密集型）解耦到专用集群虽然能最大化硬件效率，但on-policy算法的严格同步要求导致严重的依赖气泡，使得一个集群运行时另一个集群必须空闲等待

Method: 提出RollMux调度框架，基于"一个作业的结构性空闲可以被另一个作业的活动阶段利用"的洞察，引入协同执行组抽象将集群划分为隔离的局部域，采用两层调度架构：组间调度器使用保守随机规划优化作业放置，组内调度器编排可证明最优的轮转调度

Result: 在包含328个H20和328个H800 GPU的生产规模测试平台上，RollMux相比标准解耦架构将成本效率提升1.84倍，相比最先进的共置基线提升1.38倍，同时实现100%的SLO达成率

Conclusion: RollMux通过跨集群编排有效回收了rollout-training解耦架构中的依赖气泡，显著提升了硬件利用率和成本效率，同时保证了服务质量目标

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [19] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax框架通过计算图分区、分支感知内存管理和自适应调度，在移动设备上加速DNN推理，无需模型重构或自定义算子实现，实现高达46%的延迟降低和30%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 移动设备上实时DNN应用需求增长，但现有框架对动态控制流算子和不支持的内核处理不佳，导致CPU核心闲置、高延迟和内存峰值问题。

Method: 1) 分区计算DAG以暴露并行性；2) 采用分支感知内存管理，使用专用内存池和缓冲区重用；3) 自适应调度器根据设备内存约束执行分支；4) 细粒度子图控制实现动态模型的异构推理。

Result: 在三种不同移动设备上评估五个代表性DNN模型，Parallax实现高达46%的延迟降低，平均内存开销控制在26.5%，能耗节省高达30%，优于现有最先进框架。

Conclusion: Parallax框架无需模型重构即可显著提升移动DNN推理性能，满足实时移动推理的响应性需求，通过并行化、内存优化和自适应调度解决了现有框架的局限性。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [20] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO是一个视频分析框架，通过识别经历相似数据漂移的摄像头并为其训练共享模型，显著降低了持续学习的计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 当前为每个摄像头单独重新训练专用轻量级DNN模型的方法存在高计算和通信成本，无法扩展。数据漂移在相邻摄像头间常具有时空相关性，这为共享模型训练提供了机会。

Method: ECCO包含三个核心组件：1）轻量级分组算法动态形成和更新摄像头组；2）GPU分配器动态分配GPU资源以提高重训练精度并确保公平性；3）每个摄像头的传输控制器配置帧采样并基于分配的GPU资源协调带宽共享。

Result: 在三个不同数据集上对两个视觉任务进行评估，与领先基线相比，ECCO在使用相同计算和通信资源的情况下将重训练精度提高了6.7%-18.1%，或在相同精度下支持3.3倍多的并发摄像头。

Conclusion: ECCO通过利用摄像头间数据漂移的时空相关性，实现了资源高效的持续学习，显著降低了视频分析系统的计算和通信开销，提高了系统的可扩展性。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [SCOUT: A Defense Against Data Poisoning Attacks in Fine-Tuned Language Models](https://arxiv.org/abs/2512.10998)
*Mohamed Afane,Abhishek Satyam,Ke Chen,Tao Li,Junaid Farooq,Juntao Chen*

Main category: cs.CR

TL;DR: 该论文提出SCOUT防御框架，通过令牌级显著性分析检测后门攻击，包括针对传统攻击和新型上下文感知攻击的防御。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法能有效应对明显的后门攻击（如上下文外触发词和安全对齐违规），但无法防御使用上下文适当触发词的复杂攻击，这些触发词能无缝融入自然语言，对医疗等敏感领域的AI系统构成严重威胁。

Method: 提出SCOUT（基于显著性的不可信令牌分类）防御框架，通过令牌级显著性分析识别后门触发词。SCOUT通过测量移除单个令牌对目标标签输出logits的影响来构建显著性图，从而检测明显和微妙的操纵尝试。

Result: 在标准基准数据集（SST-2、IMDB、AG News）上评估SCOUT，针对传统攻击（BadNet、AddSent、SynBkd、StyleBkd）和新型攻击，SCOUT成功检测这些复杂威胁，同时在干净输入上保持准确性。

Conclusion: SCOUT框架通过令牌级显著性分析有效检测传统和新型上下文感知后门攻击，为AI系统在敏感领域部署提供了更强大的安全保障。

Abstract: Backdoor attacks create significant security threats to language models by embedding hidden triggers that manipulate model behavior during inference, presenting critical risks for AI systems deployed in healthcare and other sensitive domains. While existing defenses effectively counter obvious threats such as out-of-context trigger words and safety alignment violations, they fail against sophisticated attacks using contextually-appropriate triggers that blend seamlessly into natural language. This paper introduces three novel contextually-aware attack scenarios that exploit domain-specific knowledge and semantic plausibility: the ViralApp attack targeting social media addiction classification, the Fever attack manipulating medical diagnosis toward hypertension, and the Referral attack steering clinical recommendations. These attacks represent realistic threats where malicious actors exploit domain-specific vocabulary while maintaining semantic coherence, demonstrating how adversaries can weaponize contextual appropriateness to evade conventional detection methods. To counter both traditional and these sophisticated attacks, we present \textbf{SCOUT (Saliency-based Classification Of Untrusted Tokens)}, a novel defense framework that identifies backdoor triggers through token-level saliency analysis rather than traditional context-based detection methods. SCOUT constructs a saliency map by measuring how the removal of individual tokens affects the model's output logits for the target label, enabling detection of both conspicuous and subtle manipulation attempts. We evaluate SCOUT on established benchmark datasets (SST-2, IMDB, AG News) against conventional attacks (BadNet, AddSent, SynBkd, StyleBkd) and our novel attacks, demonstrating that SCOUT successfully detects these sophisticated threats while preserving accuracy on clean inputs.

</details>


### [22] [An LLVM-Based Optimization Pipeline for SPDZ](https://arxiv.org/abs/2512.11112)
*Tianye Dai,Hammurabi Mendes,Heuichan Lim*

Main category: cs.CR

TL;DR: 本文提出基于LLVM的SPDZ协议优化框架，通过自动批处理、非阻塞调度和GPU加速，显著提升安全多方计算性能，同时保持易用性。


<details>
  <summary>Details</summary>
Motivation: 当前安全多方计算框架存在性能瓶颈：需要特定编译栈、程序员需显式表达并行性、通信开销高，限制了实际应用中的性能和可用性。

Method: 设计基于LLVM的优化流水线：前端接受带隐私注解的C语言子集，转换为LLVM IR以重用成熟优化；后端进行数据流和控制流分析，驱动非阻塞运行时调度器，支持GPU内核映射。

Result: CPU后端在中等和重度代数工作负载下实现最高5.56倍加速，线程扩展性良好；GPU后端随输入规模增长扩展性更好，在线阶段性能显著优于MP-SPDZ。

Conclusion: 结合LLVM和协议感知调度是提取并行性而不牺牲可用性的有效架构方向，为安全多方计算提供了实用的优化解决方案。

Abstract: Actively secure arithmetic MPC is now practical for real applications, but performance and usability are still limited by framework-specific compilation stacks, the need for programmers to explicitly express parallelism, and high communication overhead. We design and implement a proof-of-concept LLVM-based optimization pipeline for the SPDZ protocol that addresses these bottlenecks. Our front end accepts a subset of C with lightweight privacy annotations and lowers it to LLVM IR, allowing us to reuse mature analyses and transformations to automatically batch independent arithmetic operations. Our back end performs data-flow and control-flow analysis on the optimized IR to drive a non-blocking runtime scheduler that overlaps independent operations and aggressively overlaps communication with computation; when enabled, it can map batched operations to GPU kernels. This design preserves a low learning curve by using a mainstream language and hiding optimization and hardware-specific mechanics from programmers. We evaluate the system on controlled microbenchmarks against MP-SPDZ, focusing on online phase performance. Our CPU back end achieves up to 5.56 times speedup under intermediate and heavy algebraic workloads, shows strong scaling with thread count, and our GPU back end scales better as the input size increases. Overall, these results indicate that leveraging LLVM with protocol-aware scheduling is an effective architectural direction for extracting parallelism without sacrificing usability.

</details>


### [23] [Leveraging FPGAs for Homomorphic Matrix-Vector Multiplication in Oblivious Message Retrieval](https://arxiv.org/abs/2512.11690)
*Grant Bosworth,Keewoo Lee,Sunwoong Kim*

Main category: cs.CR

TL;DR: 本文提出了一种硬件架构来加速Oblivious Message Retrieval（OMR）中的同态矩阵向量乘法算法，通过FPGA平台实现13.86倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 端到端加密保护消息内容但不保护元数据，OMR使用同态加密来保护元数据隐私，但其核心的同态矩阵向量乘法算法计算密集，限制了OMR的实用性。

Method: 提出硬件架构加速同态矩阵向量乘法算法，使用高层次综合实现同态算子，设计不同并行度参数，在FPGA平台上采用高效的设计空间探索策略进行部署。

Result: 与软件实现相比，提出的硬件加速器实现了13.86倍的加速比。

Conclusion: 硬件加速显著提升了OMR中同态矩阵向量乘法的性能，为保护元数据的隐私解决方案提供了更实用的实现途径。

Abstract: While end-to-end encryption protects the content of messages, it does not secure metadata, which exposes sender and receiver information through traffic analysis. A plausible approach to protecting this metadata is to have senders post encrypted messages on a public bulletin board and receivers scan it for relevant messages. Oblivious message retrieval (OMR) leverages homomorphic encryption (HE) to improve user experience in this solution by delegating the scan to a resource-rich server while preserving privacy. A key process in OMR is the homomorphic detection of pertinent messages for the receiver from the bulletin board. It relies on a specialized matrix-vector multiplication algorithm, which involves extensive multiplications between ciphertext vectors and plaintext matrices, as well as homomorphic rotations. The computationally intensive nature of this process limits the practicality of OMR. To address this challenge, this paper proposes a hardware architecture to accelerate the matrix-vector multiplication algorithm. The building homomorphic operators in this algorithm are implemented using high-level synthesis, with design parameters for different parallelism levels. These operators are then deployed on a field-programmable gate array platform using an efficient design space exploration strategy to accelerate homomorphic matrix-vector multiplication. Compared to a software implementation, the proposed hardware accelerator achieves a 13.86x speedup.

</details>


### [24] [A Scalable Multi-GPU Framework for Encrypted Large-Model Inference](https://arxiv.org/abs/2512.11269)
*Siddharth Jayashankar,Joshua Kim,Michael B. Sullivan,Wenting Zheng,Dimitrios Skarlatos*

Main category: cs.CR

TL;DR: Cerium是一个多GPU框架，用于全同态加密（FHE）的大型模型推理，通过自动生成高性能GPU内核、管理TB级内存和跨多GPU并行计算，首次在GPU上实现BERT-Base和Llama3-8B的加密推理。


<details>
  <summary>Details</summary>
Motivation: FHE提供强隐私保证但性能慢，ASIC加速方案成本高且可访问性差，GPU更易获得但难以达到ASIC级性能。现有方法主要针对小模型，而支持LLM等大型模型需要优化GPU内核并管理TB级内存，这超出了单GPU容量。

Method: Cerium集成领域特定语言、优化编译器和运行时系统，自动生成高性能GPU内核，管理TB级内存足迹，并在多GPU间并行计算。引入新的IR构造、编译器传递、稀疏多项式表示、内存高效数据布局和通信感知并行化技术。

Result: 对于小模型，Cerium比专家手写优化的GPU库快2.25倍；性能与最先进FHE ASIC竞争，匹配CraterLake ASIC；首次在GPU上实现10毫秒以下的引导（7.5毫秒）；首次展示BERT-Base（8秒）和Llama3-8B（134秒）的加密推理。

Conclusion: Cerium通过创新的多GPU框架解决了FHE在大型模型上的性能瓶颈，使GPU平台能够达到ASIC级性能，为实际部署加密AI提供了可行方案，特别是在需要隐私保护的大型语言模型推理场景中。

Abstract: Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.

</details>


### [25] [Visualisation for the CIS benchmark scanning results](https://arxiv.org/abs/2512.11316)
*Zhenshuo Zhao,Maria Spichkova,Duttkumari Champavat,Juilee N. Kulkarni,Sahil Singla,Muhammad A. Zulkefli,Pradhuman Khandelwal*

Main category: cs.CR

TL;DR: GraphSecure是一个用于分析和可视化AWS安全扫描结果的Web应用，支持CIS基准验证并提供统计图表和账户状态警告


<details>
  <summary>Details</summary>
Motivation: 为了帮助用户更好地理解和监控其AWS账户的安全状况，提供直观的安全扫描结果分析和可视化工具

Method: 开发了一个Web应用程序，能够发起AWS账户扫描，根据CIS基准验证扫描结果，并以统计图表形式展示结果

Result: 创建了GraphSecure应用，实现了AWS安全扫描、CIS基准验证、结果可视化以及账户状态警告功能

Conclusion: GraphSecure为用户提供了有效的AWS安全监控和分析工具，帮助用户更好地理解和改善其云安全状况

Abstract: In this paper, we introduce GraphSecure, a web application that provides advanced analysis and visualisation of security scanning results. GraphSecure enables users to initiate scans for their AWS account, validate them against specific Center for Internet Security (CIS) Benchmarks and return results, showcase those returned results in the form of statistical charts and warn the users about their account status.

</details>


### [26] [Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution](https://arxiv.org/abs/2512.11431)
*Qifan Zhang,Zilin Shen,Imtiaz Karim,Elisa Bertino,Zhou Li*

Main category: cs.CR

TL;DR: DNSSECVerif是首个针对DNSSEC协议套件的自动化形式化安全分析框架，通过形式化验证发现了协议规范中的关键模糊性和漏洞，包括NSEC与NSEC3的不安全共存问题。


<details>
  <summary>Details</summary>
Motivation: DNSSEC对于防止DNS欺骗至关重要，但其规范存在模糊性和漏洞，传统"破坏-修复"方法难以发现，需要进行全面、基础的安全分析。

Method: 基于SAPIC+符号验证器构建DNSSECVerif框架，建立高保真模型捕获协议级交互，包括加密操作和具有细粒度并发控制的状态缓存。

Result: 形式化证明了DNSSEC的四个核心安全保证，发现了标准中的关键模糊性（特别是NSEC和NSEC3的不安全共存），自动重新发现了三类已知攻击，并通过大规模测量研究（220万开放解析器）验证了实际影响。

Conclusion: 该工作为强化DNSSEC规范和实现提供了关键、基于证据的建议，揭示了协议设计中的根本弱点。

Abstract: The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional "break-and-fix" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.

</details>


### [27] [Capacitive Touchscreens at Risk: Recovering Handwritten Trajectory on Smartphone via Electromagnetic Emanations](https://arxiv.org/abs/2512.11484)
*Yukun Cheng,Shiyu Zhu,Changhai Ou,Xingshuo Han,Yuan Li,Shihui Zheng*

Main category: cs.CR

TL;DR: TESLA攻击框架利用电容触摸屏的电磁侧信道漏洞，通过非接触方式捕获屏幕书写时的电磁信号，实时重建二维手写轨迹，在商用智能手机上实现77%的字符识别准确率。


<details>
  <summary>Details</summary>
Motivation: 揭示并利用电容触摸屏的电磁侧信道安全漏洞，该漏洞会泄露足够信息来恢复精细、连续的手写轨迹。这种非接触攻击方式对移动设备安全构成严重威胁。

Method: 提出TESLA（触摸屏电磁侧信道泄漏攻击）框架，通过捕获屏幕上书写时产生的电磁信号，并将这些信号回归为二维手写轨迹，实现实时攻击。

Result: 在多种商用智能手机上的广泛评估显示，TESLA达到77%的字符识别准确率和0.74的Jaccard指数，证明其能够恢复高度可识别的运动轨迹，与原始手写非常相似。

Conclusion: 电容触摸屏的电磁侧信道泄露了足够信息来恢复精细手写轨迹，TESLA攻击框架在现实攻击条件下能够有效重建可识别的手写内容，揭示了移动设备安全的新威胁。

Abstract: This paper reveals and exploits a critical security vulnerability: the electromagnetic (EM) side channel of capacitive touchscreens leaks sufficient information to recover fine-grained, continuous handwriting trajectories. We present Touchscreen Electromagnetic Side-channel Leakage Attack (TESLA), a non-contact attack framework that captures EM signals generated during on-screen writing and regresses them into two-dimensional (2D) handwriting trajectories in real time. Extensive evaluations across a variety of commercial off-the-shelf (COTS) smartphones show that TESLA achieves 77% character recognition accuracy and a Jaccard index of 0.74, demonstrating its capability to recover highly recognizable motion trajectories that closely resemble the original handwriting under realistic attack conditions.

</details>


### [28] [SoK: Demystifying the multiverse of MPC protocols](https://arxiv.org/abs/2512.11699)
*Roberta De Viti,Vaastav Anand,Pierfrancesco Ingo,Deepak Garg*

Main category: cs.CR

TL;DR: 本文系统化研究了多方计算（MPC）协议的性能，通过实验分析不同协议的效率权衡，为开发者提供实用指导，旨在缩小MPC理论与实践的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管MPC具有强大的隐私和正确性保证，但在实际应用中采用仍然有限，主要原因是恶意设置下的高成本和缺乏针对具体工作负载选择合适协议的指导。

Method: 识别影响MPC效率的理论和实践参数，通过多样化的基准测试进行广泛的实验研究，分析不同协议之间的权衡。

Result: 分析讨论了不同协议之间的权衡，并突出了哪些技术最适合不同的应用场景和需求。

Conclusion: 通过为开发者提供可操作的指导并为研究人员概述开放挑战，这项工作旨在缩小MPC理论与实践之间的差距。

Abstract: This paper systematizes knowledge on the performance of Multi-Party Computation (MPC) protocols. Despite strong privacy and correctness guarantees, MPC adoption in real-world applications remains limited by high costs (especially in the malicious setting) and lack of guidance on choosing suitable protocols for concrete workloads. We identify the theoretical and practical parameters that shape MPC efficiency and conduct an extensive experimental study across diverse benchmarks. Our analysis discusses the trade-offs between protocols, and highlights which techniques align best with different application scenarios and needs. By providing actionable guidance for developers and outlining open challenges for researchers, this work seeks to narrow the gap between MPC theory and practice.

</details>


### [29] [Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously](https://arxiv.org/abs/2512.11783)
*Andrew Adiletta,Kathryn Adiletta,Kemal Derya,Berk Sunar*

Main category: cs.CR

TL;DR: 本文提出Super Suffixes攻击方法，能够绕过多个模型的防护机制，并开发了DeltaGuard检测方法来防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速部署，处理不可信文本输入和执行代码生成时面临安全隐私挑战。现有防护模型（如Llama Prompt Guard 2）存在被绕过的风险，需要研究更强大的攻击方法和相应的防御措施。

Method: 1. 提出Super Suffixes攻击方法：通过联合优化技术生成能够绕过多个模型对齐目标的后缀；2. 提出DeltaGuard防御方法：通过分析模型内部状态与特定概念方向的余弦相似度变化来检测攻击。

Result: 1. Super Suffixes成功绕过了Llama Prompt Guard 2对5个不同文本生成模型的保护，实现了恶意文本和代码生成；2. DeltaGuard将非良性分类率提升至近100%，显著提高了对Super Suffixes攻击的检测能力。

Conclusion: 本研究首次揭示了Llama Prompt Guard 2可通过联合优化被绕过，同时提出的DeltaGuard检测方法能有效防御Super Suffixes攻击，为防护模型堆栈提供了有价值的增强方案。

Abstract: The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.
  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [30] [PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration](https://arxiv.org/abs/2512.11550)
*Yifan Zhang,Zhiheng Chen,Ye Qiao,Sitao Huang*

Main category: cs.AR

TL;DR: PD-Swap是一种基于FPGA的LLM加速器，通过动态部分重配置技术将预填充和解码阶段分离，针对不同计算特性优化硬件架构，显著提升长上下文场景下的推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着提示长度增长到数万个token，边缘硬件性能因二次预填充成本和快速增长的KV缓存带宽需求而急剧下降。现有静态加速器需要为计算密集的预填充阶段和内存带宽受限的解码阶段提供统一资源，导致注意力逻辑重复、硬件利用率低，限制了模型大小和可用上下文长度。

Method: 提出预填充-解码分离的LLM加速器PD-Swap，利用动态部分重配置技术在边缘FPGA上时间复用注意力模块。核心的三元矩阵乘法和权重缓冲引擎保持静态，而注意力子系统作为可重配置分区，包含两个阶段专用架构：计算密集的token并行预填充引擎和带宽优化的KV缓存中心解码引擎。

Result: PD-Swap实现了高达27 tokens/s的解码吞吐量，在相同面积成本下，比现有最先进工作提升1.3-2.1倍（上下文越长提升越明显）。

Conclusion: 通过动态部分重配置实现预填充和解码阶段的硬件架构分离，能够有效解决LLM推理中的计算-内存带宽不对称问题，显著提升边缘FPGA上长上下文LLM推理的性能。

Abstract: Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.

</details>
