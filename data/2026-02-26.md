<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [General Convex Agreement with Near-Optimal Communication](https://arxiv.org/abs/2602.21411)
*Marc Dufay,Diana Ghinea,Anton Paramonov*

Main category: cs.DC

TL;DR: 该论文研究了凸一致性协议(CA)的通信复杂度问题，针对抽象凸性空间提出了确定性同步协议，实现了接近最优的通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 凸一致性协议在拜占庭容错系统中具有实际应用价值（如鲁棒学习、传感器融合），但现有方法通信复杂度高（Θ(Ln²)），与拜占庭一致性协议的下界Ω(Ln)存在差距。虽然最近工作对足够大的L实现了最优通信复杂度O(Ln)，但将其推广到一般凸性空间仍是一个开放问题。

Method: 使用提取器图获得确定性委员会分配方案，该方案能抵抗自适应敌手攻击。针对有限凸性空间和欧几里得空间分别设计了协议，具有渐近最优的轮复杂度O(n)。

Result: 当L = Ω(n·κ)时，实现了O(L·n log n)通信复杂度（有限凸性空间）和O(L·n¹⁺ᵒ⁽¹⁾)通信复杂度（欧几里得空间）。当输入长度L有先验界时，实现了接近最优的容错性t < n/(ω+ε)；当L未知时，容错性为t < n/(ω+ε+1)。

Conclusion: 该工作填补了凸一致性协议通信复杂度的理论空白，通过提取器图技术实现了对自适应敌手具有抵抗力的确定性委员会分配，为抽象凸性空间提供了接近最优的通信效率解决方案。

Abstract: Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \emph{communication}: standard approaches for CA have a communication complexity of $Θ(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $Ω(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.
  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = Ω(n \cdot κ)$, where $κ$ is a security parameter, we achieve $O(L\cdot n\log n)$ communication for finite convexity spaces and $O(L\cdot n^{1+o(1)})$ communication for Euclidean spaces $\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(ω+\varepsilon)$ for any constant $\varepsilon>0$, where $ω$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(ω+\varepsilon+1)$ for any constant $\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.
  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.

</details>


### [2] [Multi-Layer Scheduling for MoE-Based LLM Reasoning](https://arxiv.org/abs/2602.21626)
*Yifan Sun,Gholamreza Haffar,Minxian Xu,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: 本文提出了一种针对MoE大语言模型服务的多层调度框架，在请求、引擎和专家三个层面优化调度策略，相比现有框架vLLM显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务面临计算和延迟挑战，现有调度策略如FCFS和RR无法充分利用系统资源，存在队头阻塞和负载不均衡问题。MoE模型的出现带来了专家并行和路由复杂性的新挑战。

Method: 提出三层调度框架：1) 请求层采用SJF和优先级感知老化算法；2) 引擎层设计考虑前缀token负载、KV缓存利用率和用户粘性的负载感知分发策略；3) 专家层通过缓解专家热点和策略性放置层间专家依赖来平衡负载。

Result: 在超过100个不同工作负载分布的实验中，该方法持续优于最先进的vLLM推理框架，实现了TTFT延迟降低17.8%，TPOT延迟降低13.3%。

Conclusion: 针对MoE大语言模型服务的多层调度框架能有效解决现有调度策略的局限性，显著提升服务效率和降低延迟。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.

</details>


### [3] [DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism](https://arxiv.org/abs/2602.21788)
*Yifan Niu,Han Xiao,Dongyi Liu,Wei Zhou,Jia Li*

Main category: cs.DC

TL;DR: DHP是一种动态混合并行策略，用于解决多模态大语言模型训练中的数据异构性问题，通过自适应重构通信组和并行度，在NPU集群上实现高达1.36倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多模态数据集极其异构，而现有的训练框架主要依赖静态并行策略，在数据异构情况下存在严重的负载不平衡、冗余通信和硬件利用率低下的问题。

Method: 提出动态混合并行策略，自适应地重构通信组和并行度；推广非2的幂次并行度，并开发多项式时间算法生成近似最优的并行策略，每个训练批次仅增加毫秒级开销。

Result: DHP显著优于Megatron-LM和DeepSpeed，在NPU集群上实现高达1.36倍的训练吞吐量加速，同时保持接近线性的扩展效率，即使在极端数据变异性下也能维持高硬件效率。

Conclusion: DHP是一种高效的并行策略，能够有效解决多模态大语言模型训练中的数据异构性问题，显著提升训练效率。

Abstract: Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.

</details>


### [4] [A task-based data-flow methodology for programming heterogeneous systems with multiple accelerator APIs](https://arxiv.org/abs/2602.21897)
*Aleix Boné,Alejandro Aguirre,David Álvarez,Pedro J. Martinez-Ferrer,Vicenç Beltran*

Main category: cs.DC

TL;DR: 提出任务感知API和nOS-V库，实现异构节点上多种加速器编程模型的无缝集成，解决多运行时线程争用问题。


<details>
  <summary>Details</summary>
Motivation: 异构节点（CPU+多种加速器）成为HPC和AI基础设施常态，但集成CUDA、SYCL、Triton等不同加速器API和库时面临抽象差异、执行语义不一致、同步机制复杂等问题，导致应用开发错误多、工作量大。

Method: 采用基于任务的数据流方法，结合任务感知API（TA-libs），将应用表示为有向无环图（DAG），包含主机任务和设备内核，由OpenMP/OmpSs-2运行时管理。引入Task-Aware SYCL（TASYCL）并利用Task-Aware CUDA（TACUDA），将单个加速器调用提升为一级任务。使用nOS-V任务和线程库统一线程管理，避免多运行时共存时的线程争用问题。

Result: 任务感知库与nOS-V库结合，使单个应用能够透明高效地利用多种加速器编程模型。该方法适用于当前异构节点，并可扩展至未来集成更丰富CPU、GPU、FPGA和AI加速器的系统。

Conclusion: 提出的任务感知API和统一线程管理方法有效解决了异构节点上多加速器编程模型集成的复杂性，为当前和未来异构计算系统提供了可行的解决方案。

Abstract: Heterogeneous nodes that combine multi-core CPUs with diverse accelerators are rapidly becoming the norm in both high-performance computing (HPC) and AI infrastructures. Exploiting these platforms, however, requires orchestrating several low-level accelerator APIs such as CUDA, SYCL, and Triton. In some occasions they can be combined with optimized vendor math libraries: e.g., cuBLAS and oneAPI. Each API or library introduces its own abstractions, execution semantics, and synchronization mechanisms. Combining them within a single application is therefore error-prone and labor-intensive. We propose reusing a task-based data-flow methodology together with Task-Aware APIs (TA-libs) to overcome these limitations and facilitate the seamless integration of multiple accelerator programming models, while still leveraging the best-in-class kernels offered by each API.
  Applications are expressed as a directed acyclic graph (DAG) of host tasks and device kernels managed by an OpenMP/OmpSs-2 runtime. We introduce Task-Aware SYCL (TASYCL) and leverage Task-Aware CUDA (TACUDA), which elevate individual accelerator invocations to first-class tasks. When multiple native runtimes coexist on the same multi-core CPU, they contend for threads, leading to oversubscription and performance variability. To address this, we unify their thread management under the nOS-V tasking and threading library, to which we contribute a new port of the PoCL (Portable OpenCL) runtime.
  These results demonstrate that task-aware libraries, coupled with the nOS-V library, enable a single application to harness multiple accelerator programming models transparently and efficiently. The proposed methodology is immediately applicable to current heterogeneous nodes and is readily extensible to future systems that integrate even richer combinations of CPUs, GPUs, FPGAs, and AI accelerators.

</details>


### [5] [IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs](https://arxiv.org/abs/2602.22017)
*Chris Egersdoerfer,Arnav Sareen,Jean Luca Bez,Suren Byna,Dongkuan,Xu,Dong Dai*

Main category: cs.DC

TL;DR: IOAgent：基于LLM的自动化HPC I/O性能诊断工具，通过模块化预处理、RAG领域知识集成和树状合并技术，为科学家提供准确的I/O问题诊断和交互式分析。


<details>
  <summary>Details</summary>
Motivation: HPC存储栈日益复杂，领域科学家难以有效利用存储系统达到理想I/O性能。I/O专家数量有限，无法满足数据密集型应用增长的需求，成为科学家提升生产力的主要瓶颈。

Method: 提出IOAgent系统，包含：1）模块化预处理器处理Darshan跟踪文件；2）基于RAG的领域知识集成器整合HPC I/O专业知识；3）树状合并器准确诊断I/O问题。系统提供详细诊断依据和交互式界面。

Result: 创建首个开放诊断测试套件TraceBench，包含多样化标记作业跟踪。评估显示IOAgent在准确性和实用性上匹配或优于现有最先进的I/O诊断工具，且不依赖特定LLM，在专有和开源LLM上表现一致。

Conclusion: IOAgent能够有效解决LLM在长上下文处理、领域知识缺乏和幻觉生成方面的挑战，有望成为科学家应对复杂HPC I/O子系统的强大工具。

Abstract: As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.

</details>


### [6] [PASTA: A Modular Program Analysis Tool Framework for Accelerators](https://arxiv.org/abs/2602.22103)
*Mao Lin,Hyeran Jeon,Keren Zhou*

Main category: cs.DC

TL;DR: PASTA是一个低开销、模块化的加速器程序分析工具框架，通过抽象底层API和深度学习框架提供统一接口，支持快速定制工具开发，在GPU上相比传统工具加速高达1.3万倍。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统中硬件加速器的复杂性和多样性日益增加，需要灵活、低开销的程序分析工具来满足不同加速器和深度学习框架的需求。

Method: PASTA通过抽象底层性能分析API和多种深度学习框架，提供统一的运行时事件捕获和分析接口。采用可扩展设计，支持研究人员快速原型化定制工具，并利用GPU加速后端降低开销。

Result: 在NVIDIA和AMD GPU上对主流深度学习工作负载进行单GPU和多GPU场景测试，PASTA展示了广泛适用性。在NVIDIA GPU上，PASTA提供详细性能洞察的同时，开销显著降低，相比传统分析工具加速高达1.3万倍。

Conclusion: PASTA在可用性、可扩展性和效率之间取得了实用平衡，非常适合现代基于加速器的计算环境，为加速器性能分析提供了高效解决方案。

Abstract: The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program AnalysiS Tool Framework for Accelerators. PASTA abstracts over low-level profiling APIs and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. Its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. We demonstrate the utility of PASTA by developing several analysis tools, including a deep learning workload characterization tool and a UVM optimization tool. Through extensive evaluation on mainstream deep learning workloads tested on NVIDIA and AMD GPUs under both single- and multi-GPU scenarios, we demonstrate PASTA's broad applicability. On NVIDIA GPUs, we further show that PASTA provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its GPU-accelerated backend. PASTA strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives](https://arxiv.org/abs/2602.21351)
*Dmitrii Pantiukhin,Ivan Kuznetsov,Boris Shapkin,Antonia Anna Jost,Thomas Jung,Nikolay Koldunov*

Main category: cs.AI

TL;DR: PANGAEA-GPT：一个用于地球科学数据自主发现和分析的分层多智能体框架，通过集中式监督-工作者拓扑解决数据可扩展性挑战


<details>
  <summary>Details</summary>
Motivation: 地球科学数据快速积累带来了可扩展性挑战，PANGAEA等存储库中大量数据集未被充分利用，限制了数据重用性

Method: 采用分层多智能体框架，具有集中式监督-工作者拓扑结构，包含数据类型感知路由、沙盒确定性代码执行和通过执行反馈的自我校正机制

Result: 通过物理海洋学和生态学的用例场景，展示了系统能够以最少人工干预执行复杂的多步骤工作流程

Conclusion: 该框架提供了一种通过协调智能体工作流程查询和分析异构存储库数据的方法论

Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.

</details>


### [8] [ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning](https://arxiv.org/abs/2602.21534)
*Xiaoxuan Wang,Han Zhang,Haixin Wang,Yidan Shi,Ruoyan Li,Kaiqiao Han,Chenyi Tong,Haoran Deng,Renliang Sun,Alexander Taylor,Yanqiao Zhu,Jason Cong,Yizhou Sun,Wei Wang*

Main category: cs.AI

TL;DR: 该论文提出了ARLArena框架和SAMPO方法，用于解决Agentic强化学习中的训练不稳定问题，通过标准化测试平台和策略梯度分解分析，实现了稳定的智能体训练。


<details>
  <summary>Details</summary>
Motivation: Agentic强化学习在解决复杂多步交互任务方面具有潜力，但训练过程极不稳定，经常导致训练崩溃，这限制了其在更大环境和更长交互周期中的可扩展性，也阻碍了对算法设计选择的系统探索。

Method: 1. 提出ARLArena：一个稳定的训练配方和系统分析框架，在可控和可复现的环境中检查训练稳定性；2. 构建干净标准化的测试平台；3. 将策略梯度分解为四个核心设计维度并评估每个维度的性能和稳定性；4. 提出SAMPO方法：一种稳定的Agentic策略优化方法，旨在缓解ARL中的主要不稳定来源。

Result: SAMPO在各种Agentic任务中实现了持续稳定的训练和强大的性能。通过细粒度分析，为ARL提供了统一的策略梯度视角。

Conclusion: 本研究为ARL提供了统一的策略梯度视角，并为构建稳定且可复现的基于LLM的智能体训练流程提供了实用指导。

Abstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.

</details>


### [9] [ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices](https://arxiv.org/abs/2602.21858)
*Dezhi Kong,Zhengzhao Feng,Qiliang Liang,Hao Wang,Haofei Sun,Changpeng Yang,Yang Li,Peng Zhou,Shuai Nie,Hongzhen Wang,Linfeng Zhou,Hao Jia,Jiaming Xu,Runyu Shi,Ying Huang*

Main category: cs.AI

TL;DR: ProactiveMobile是一个用于评估移动智能体主动智能能力的基准测试，包含3,660个实例、14个场景和63个API函数，通过微调的Qwen2.5-VL-7B-Instruct模型达到19.15%的成功率，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在移动智能体开发中主要局限于被动执行用户命令的范式，而主动智能（智能体自主预测需求并启动行动）代表了下一代移动智能体的前沿方向，但其发展受到缺乏能够处理现实世界复杂性并提供客观可执行评估的基准测试的限制。

Method: 提出ProactiveMobile基准测试，将主动任务形式化为：基于设备上下文信号的四个维度推断潜在用户意图，并从包含63个API的全面函数池中生成可执行函数序列。基准包含3,660个实例、14个场景，采用多答案标注以应对现实复杂性，并由30名专家团队进行最终审核，确保事实准确性、逻辑一致性和行动可行性。

Result: 实验表明，微调的Qwen2.5-VL-7B-Instruct模型在基准测试中达到19.15%的成功率，优于o1模型（15.71%）和GPT-5模型（7.39%）。这表明主动智能是当前MLLMs普遍缺乏但可学习的关键能力。

Conclusion: 主动智能是移动智能体发展的关键方向，但当前模型普遍缺乏这一能力。ProactiveMobile基准测试为评估主动智能提供了系统化工具，证明了主动智能的可学习性，强调了该基准对主动智能评估的重要性。

Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Heterogeneous Memory Design Exploration for AI Accelerators with a Gain Cell Memory Compiler](https://arxiv.org/abs/2602.21278)
*Xinxin Wang,Lixian Yan,Shuhan Liu,Luke Upton,Zhuoqi Cai,Yiming Tan,Shengman Li,Koustav Jana,Peijing Li,Jesse Cirimelli-Low,Thierry Tambe,Matthew Guthaus,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: 开发了OpenGCRAM编译器，支持SRAM和GCRAM两种内存技术，用于生成异构内存系统设计，优化AI任务的性能指标。


<details>
  <summary>Details</summary>
Motivation: 随着内存成本在系统总成本中占比越来越高，需要结合不同技术优势的异构片上内存系统。GCRAM相比传统SRAM具有更高密度、更低功耗和可调保持时间的特点，扩展了内存设计空间。

Method: 创建了OpenGCRAM编译器，支持SRAM和GCRAM两种内存技术。该工具能够为商业CMOS工艺生成宏级设计和布局，并根据用户定义的配置来表征面积、延迟和功耗特性。

Result: 该工具能够系统性地识别在特定性能指标下AI任务的最优异构内存配置，帮助设计者在不同内存技术之间做出权衡决策。

Conclusion: OpenGCRAM编译器为异构内存系统设计提供了有效的工具支持，特别适用于AI应用场景，能够在不同内存技术之间找到最优配置平衡性能、面积和功耗。

Abstract: As memory increasingly dominates system cost and energy, heterogeneous on-chip memory systems that combine technologies with complementary characteristics are becoming essential. Gain Cell RAM (GCRAM) offers higher density, lower power, and tunable retention, expanding the design space beyond conventional SRAM. To this end, we create an OpenGCRAM compiler supporting both SRAM and GCRAM. It generates macro-level designs and layouts for commercial CMOS processes and characterizes area, delay, and power across user-defined configurations. The tool enables systematic identification of optimal heterogeneous memory configurations for AI tasks under specified performance metrics.

</details>
