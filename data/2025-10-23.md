<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.CR](#cs.CR) [Total: 8]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality](https://arxiv.org/abs/2510.18982)
*Arpan Mukherjee,Marcello Bullo,Debabrota Basu,Deniz Gündüz*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架来分析验证器在测试时缩放中的作用，将可验证测试时缩放建模为传输问题，揭示了次优性-覆盖率曲线的三个机制：传输机制、策略改进机制和饱和机制。


<details>
  <summary>Details</summary>
Motivation: 虽然带验证的测试时缩放已显示出改善大语言模型性能的潜力，但验证器的作用及其缺陷仍未得到充分探索。现有研究只捕捉了这些因素的部分子集，缺乏量化它们相互作用几何结构的统一框架。

Method: 将可验证测试时缩放构建为传输问题，通过分析生成器的覆盖率、验证器的收敛区域和采样算法的次优性这三个量的相互作用来表征其几何特性。提出并分析了顺序和批量两类采样算法。

Result: 理论分析发现次优性-覆盖率曲线呈现三个机制：传输机制（次优性随覆盖率增加而增加）、策略改进机制（次优性可能随覆盖率增加而减少，取决于验证器的收敛区域）和饱和机制（次优性趋于平稳，不受覆盖率影响）。使用Qwen、Llama和Gemma模型的实证结果验证了理论发现。

Conclusion: 通过传输问题的视角，本文为理解验证器在测试时缩放中的作用提供了统一框架，揭示了覆盖率、验证器收敛区域和采样算法次优性之间的复杂相互作用，并展示了不同采样算法如何影响这些权衡关系。

Abstract: While test-time scaling with verification has shown promise in improving the
performance of large language models (LLMs), the role of the verifier and its
imperfections remain underexplored. The effect of verification manifests
through interactions of three quantities: (i) the generator's coverage, (ii)
the verifier's region of convergence (ROC), and (iii) the sampling algorithm's
sub-optimality. Though recent studies capture subsets of these factors, a
unified framework quantifying the geometry of their interplay is missing. We
frame verifiable test-time scaling as a transport problem. This characterizes
the interaction of coverage, ROC, and sub-optimality, and uncovers that the
sub-optimality--coverage curve exhibits three regimes. A transport regime --
where sub-optimality increases with coverage, a policy improvement regime --
where sub-optimality may decrease with coverage, depending on the verifier's
ROC, and a saturation regime -- where sub-optimality plateaus, unaffected by
coverage. We further propose and analyze two classes of sampling algorithms --
sequential and batched, and examine how their computational complexities shape
these trade-offs. Empirical results with Qwen, Llama, and Gemma models
corroborate our theoretical findings.

</details>


### [2] [Timely Clinical Diagnosis through Active Test Selection](https://arxiv.org/abs/2510.18988)
*Silas Ruhrberg Estévez,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: ACTMED是一个结合贝叶斯实验设计和大型语言模型的临床诊断框架，通过自适应选择能最大程度减少诊断不确定性的测试，模拟真实临床推理过程，提高诊断准确性、可解释性和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习诊断方法依赖静态完整数据集，无法模拟临床医生在实践中的顺序性、资源感知推理过程。诊断在高压或资源有限环境中仍然复杂且易出错，需要能够帮助医生做出及时且成本效益高的决策的框架。

Method: ACTMED整合贝叶斯实验设计与大型语言模型，在每个步骤选择预期能为特定患者带来最大诊断不确定性减少的测试。LLMs作为灵活模拟器，生成合理的患者状态分布并支持信念更新，无需结构化、任务特定的训练数据。

Result: 在真实世界数据集上的评估显示，ACTMED能够优化测试选择，提高诊断准确性、可解释性和资源使用效率。

Conclusion: ACTMED代表了向透明、自适应且与临床医生对齐的诊断系统迈出的一步，能够在减少对领域特定数据依赖的情况下跨设置泛化。

Abstract: There is growing interest in using machine learning (ML) to support clinical
diag- nosis, but most approaches rely on static, fully observed datasets and
fail to reflect the sequential, resource-aware reasoning clinicians use in
practice. Diagnosis remains complex and error prone, especially in
high-pressure or resource-limited settings, underscoring the need for
frameworks that help clinicians make timely and cost-effective decisions. We
propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental
Design), a diagnostic framework that integrates Bayesian Experimental Design
(BED) with large language models (LLMs) to better emulate real-world diagnostic
reasoning. At each step, ACTMED selects the test expected to yield the greatest
reduction in diagnostic uncertainty for a given patient. LLMs act as flexible
simulators, generating plausible patient state distributions and supporting
belief updates without requiring structured, task-specific training data.
Clinicians can remain in the loop; reviewing test suggestions, interpreting
intermediate outputs, and applying clinical judgment throughout. We evaluate
ACTMED on real-world datasets and show it can optimize test selection to
improve diagnostic accuracy, interpretability, and resource use. This
represents a step to- ward transparent, adaptive, and clinician-aligned
diagnostic systems that generalize across settings with reduced reliance on
domain-specific data.

</details>


### [3] [The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS](https://arxiv.org/abs/2510.19055)
*Brandon James Carone,Iran R. Roman,Pablo Ripollés*

Main category: cs.AI

TL;DR: 该论文提出了MUSE基准测试，用于评估多模态大语言模型在音乐理解方面的关系推理能力，发现当前SOTA模型存在严重缺陷，与人类专家存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在音频理解方面的评估可能掩盖了其在关系推理方面的根本弱点，需要更全面的评估工具来揭示这些缺陷。

Method: 开发了MUSE基准测试，包含10个任务来探测基础音乐感知技能，评估了4个SOTA模型（Gemini Pro和Flash、Qwen2.5-Omni、Audio-Flamingo 3）并与200人的人类基线进行比较。

Result: 发现SOTA模型能力差异很大，与人类专家存在持续差距；Gemini Pro在基础感知上表现良好，但Qwen和Audio Flamingo 3表现接近随机水平；思维链提示提供不一致且通常有害的结果。

Conclusion: MUSE基准为评估不变音乐表征和开发更鲁棒的AI系统提供了关键工具。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in
audio understanding, but current evaluations may obscure fundamental weaknesses
in relational reasoning. We introduce the Music Understanding and Structural
Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to
probe fundamental music perception skills. We evaluate four SOTA models (Gemini
Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human
baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a
persistent gap with human experts. While Gemini Pro succeeds on basic
perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing
severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)
prompting provides inconsistent, often detrimental results. Our work provides a
critical tool for evaluating invariant musical representations and driving
development of more robust AI systems.

</details>


### [4] [A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist](https://arxiv.org/abs/2510.19139)
*Sohyeon Jeon,Hyung-Chul Lee*

Main category: cs.AI

TL;DR: 本研究通过行为和元认知分析方法，比较了两种代表性LLM在三种提示条件下评估临床试验报告CONSORT标准的认知和推理策略差异。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医疗领域快速扩张，但其根据CONSORT标准评估临床试验报告的能力尚不明确，特别是在认知和推理策略方面。

Method: 采用行为和元认知分析方法，使用专家验证数据，系统比较两种代表性LLM在三种提示条件下的表现。

Result: 模型在处理不同CONSORT项目和提示类型时表现出明显差异，包括推理风格转变、明确不确定性和替代解释等响应模式变化。

Conclusion: 结果凸显了当前系统在临床合规自动化方面的局限性，强调了理解其认知适应和策略行为对于开发更可解释和可靠的医疗AI的重要性。

Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare,
the ability of these systems to assess clinical trial reporting according to
CONSORT standards remains unclear, particularly with respect to their cognitive
and reasoning strategies. This study applies a behavioral and metacognitive
analytic approach with expert-validated data, systematically comparing two
representative LLMs under three prompt conditions. Clear differences emerged in
how the models approached various CONSORT items, and prompt types, including
shifts in reasoning style, explicit uncertainty, and alternative
interpretations shaped response patterns. Our results highlight the current
limitations of these systems in clinical compliance automation and underscore
the importance of understanding their cognitive adaptations and strategic
behavior in developing more explainable and reliable medical AI.

</details>


### [5] [The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models](https://arxiv.org/abs/2510.19176)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 本文探讨了推理模型中的模式选择问题，将其视为早期退出问题的更具挑战性变体。研究发现现有方法在信息有限的情况下难以有效解决模式选择问题。


<details>
  <summary>Details</summary>
Motivation: 推理模型在数学和逻辑推理任务中表现出色，但逐步思考过程常导致过度思考，产生不必要的计算开销。模式选择和早期退出方法旨在减少这种计算负担。

Method: 通过实证研究比较了九种基线方法，包括基于提示的方法和利用模型内部信息的方法。模式选择需要在推理过程开始时做出决策，依赖预定义的虚假思考（零步思考）。

Result: 基于提示的方法由于分类能力有限而经常失败，而利用内部信息的方法在大多数场景中表现更好，但仍存在稳定性问题。现有方法在信息有限的情况下不足以有效解决模式选择问题。

Conclusion: 模式选择是早期退出问题的更具挑战性变体，现有方法在信息有限的情况下仍面临困难，突显了该任务的持续挑战。

Abstract: Reasoning models have demonstrated exceptional performance in tasks such as
mathematics and logical reasoning, primarily due to their ability to engage in
step-by-step thinking during the reasoning process. However, this often leads
to overthinking, resulting in unnecessary computational overhead. To address
this issue, Mode Selection aims to automatically decide between Long-CoT
(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking
mode. Simultaneously, Early Exit determines the optimal stopping point during
the iterative reasoning process. Both methods seek to reduce the computational
burden. In this paper, we first identify Mode Selection as a more challenging
variant of the Early Exit problem, as they share similar objectives but differ
in decision timing. While Early Exit focuses on determining the best stopping
point for concise reasoning at inference time, Mode Selection must make this
decision at the beginning of the reasoning process, relying on pre-defined fake
thoughts without engaging in an explicit reasoning process, referred to as
zero-step thinking. Through empirical studies on nine baselines, we observe
that prompt-based approaches often fail due to their limited classification
capabilities when provided with minimal hand-crafted information. In contrast,
approaches that leverage internal information generally perform better across
most scenarios but still exhibit issues with stability. Our findings indicate
that existing methods relying solely on the information provided by models are
insufficient for effectively addressing Mode Selection in scenarios with
limited information, highlighting the ongoing challenges of this task. Our code
is available at https://github.com/Trae1ounG/Zero_Step_Thinking.

</details>


### [6] [WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation](https://arxiv.org/abs/2510.19205)
*Yaoyao Qian,Yuanli Wang,Jinda Zhang,Yun Zong,Meixu Chen,Hanhan Zhou,Jindan Huang,Yifan Zeng,Xinyu Hu,Chan Hee Song,Danqing Zhang*

Main category: cs.AI

TL;DR: WebGraphEval是一个将多个智能体的轨迹抽象为统一加权动作图的框架，用于评估网页智能体，能够捕捉结构多样性并识别关键决策点。


<details>
  <summary>Details</summary>
Motivation: 当前网页智能体评估主要依赖于二元成功指标或单一参考轨迹，忽略了基准数据集中的结构多样性。

Method: 将多个智能体的轨迹抽象为统一的加权动作图，进行规范化编码、合并重复行为，并应用结构分析（如奖励传播和成功加权边统计）。

Result: 在来自六个网页智能体的数千条轨迹上的评估显示，图抽象能够捕捉跨模型规律、突出冗余和低效行为，并识别被基于结果的指标忽略的关键决策点。

Conclusion: 通过将网页交互构建为图结构数据，WebGraphEval为网页智能体的多路径、跨智能体和效率感知评估建立了通用方法。

Abstract: Current evaluation of web agents largely reduces to binary success metrics or
conformity to a single reference trajectory, ignoring the structural diversity
present in benchmark datasets. We present WebGraphEval, a framework that
abstracts trajectories from multiple agents into a unified, weighted action
graph. This representation is directly compatible with benchmarks such as
WebArena, leveraging leaderboard runs and newly collected trajectories without
modifying environments. The framework canonically encodes actions, merges
recurring behaviors, and applies structural analyses including reward
propagation and success-weighted edge statistics. Evaluations across thousands
of trajectories from six web agents show that the graph abstraction captures
cross-model regularities, highlights redundancy and inefficiency, and
identifies critical decision points overlooked by outcome-based metrics. By
framing web interaction as graph-structured data, WebGraphEval establishes a
general methodology for multi-path, cross-agent, and efficiency-aware
evaluation of web agents.

</details>


### [7] [ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate](https://arxiv.org/abs/2510.19261)
*Marianna Molinari,Ilaria Angela Amantea,Marinella Quaranta,Guido Governatori*

Main category: cs.AI

TL;DR: 本研究通过法律领域实验评估ChatGPT性能，与正则表达式基线对比，发现ChatGPT即使具备必要知识和能力，也无法整合推理得出全面结果，揭示了其在复杂问题分解和综合解决方面的重大局限。


<details>
  <summary>Details</summary>
Motivation: 评估ChatGPT在法律领域的实际表现，特别是与简单技术（正则表达式）的对比，而非仅与人类表现比较，以揭示AI在复杂推理任务中的真实局限性。

Method: 在法律领域设计实验，将ChatGPT的表现与基于正则表达式的基线方法进行对比分析，重点关注从法律判决中提取法律原则关键段落的能力。

Result: ChatGPT即使拥有必要的知识和能力，也无法有效整合和推理以产生全面结果，在复杂问题分解和多能力协调方面存在显著不足。

Conclusion: 真正的智能包括分解复杂问题并协调多种能力提供统一全面解决方案的能力，这在法律领域尤为关键。目前，真正的智能仍然是人类独有的特质，至少在该特定领域如此。

Abstract: This study examines the performance of ChatGPT with an experiment in the
legal domain. We compare the outcome with it a baseline using regular
expressions (Regex), rather than focusing solely on the assessment against
human performance. The study reveals that even if ChatGPT has access to the
necessary knowledge and competencies, it is unable to assemble them, reason
through, in a way that leads to an exhaustive result. This unveils a major
limitation of ChatGPT. Intelligence encompasses the ability to break down
complex issues and address them according to multiple required competencies,
providing a unified and comprehensive solution. In the legal domain, one of the
most crucial tasks is reading legal decisions and extracting key passages
condensed from principles of law (PoLs), which are then incorporated into
subsequent rulings by judges or defense documents by lawyers. In performing
this task, artificial intelligence lacks an all-encompassing understanding and
reasoning, which makes it inherently limited. Genuine intelligence, remains a
uniquely human trait, at least in this particular field.

</details>


### [8] [An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents](https://arxiv.org/abs/2510.19263)
*Wachara Fungwacharakorn,Gauvain Bourgne,Ken Satoh*

Main category: cs.AI

TL;DR: 本文扩展了推导状态论证框架（DSA-framework），为容纳不一致先例的广义推理模型提供论证性解释方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于一致先例的推理模型存在局限性，需要为广义推理框架（允许不一致先例）开发相应的论证性解释方法。

Method: 扩展推导状态论证框架（DSA-framework），以解释基于广义推理模型的推理过程。

Result: 提出了一个能够处理不一致先例的论证性解释框架。

Conclusion: 该扩展框架填补了广义推理模型中论证性解释方法的空白，为处理不一致先例提供了理论支持。

Abstract: Precedential constraint is one foundation of case-based reasoning in AI and
Law. It generally assumes that the underlying set of precedents must be
consistent. To relax this assumption, a generalized notion of the reason model
has been introduced. While several argumentative explanation approaches exist
for reasoning with precedents based on the traditional consistent reason model,
there has been no corresponding argumentative explanation method developed for
this generalized reasoning framework accommodating inconsistent precedents. To
address this question, this paper examines an extension of the derivation state
argumentation framework (DSA-framework) to explain the reasoning according to
the generalized notion of the reason model.

</details>


### [9] [Continual Knowledge Adaptation for Reinforcement Learning](https://arxiv.org/abs/2510.19314)
*Jinwu Hu,Zihao Lian,Zhiquan Wen,Chenghao Li,Guohao Chen,Xutao Wen,Bin Xiao,Mingkui Tan*

Main category: cs.AI

TL;DR: 本文提出CKA-RL方法，通过持续知识适应策略解决强化学习中的灾难性遗忘问题，使用任务特定知识向量池和自适应知识合并机制，在三个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境通常是非平稳的，需要智能体持续适应新任务和变化条件。现有的持续强化学习方法存在灾难性遗忘和知识利用效率低的问题。

Method: 提出持续知识适应策略，维护任务特定知识向量池，动态使用历史知识适应新任务；引入自适应知识合并机制，合并相似知识向量以减少内存需求。

Result: 在三个基准测试中，CKA-RL优于最先进方法，整体性能提升4.20%，前向迁移提升8.02%。

Conclusion: CKA-RL通过持续知识适应和自适应知识合并，有效缓解灾难性遗忘，实现跨任务的高效知识迁移，在持续强化学习任务中表现优异。

Abstract: Reinforcement Learning enables agents to learn optimal behaviors through
interactions with environments. However, real-world environments are typically
non-stationary, requiring agents to continuously adapt to new tasks and
changing conditions. Although Continual Reinforcement Learning facilitates
learning across multiple tasks, existing methods often suffer from catastrophic
forgetting and inefficient knowledge utilization. To address these challenges,
we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),
which enables the accumulation and effective utilization of historical
knowledge. Specifically, we introduce a Continual Knowledge Adaptation
strategy, which involves maintaining a task-specific knowledge vector pool and
dynamically using historical knowledge to adapt the agent to new tasks. This
process mitigates catastrophic forgetting and enables efficient knowledge
transfer across tasks by preserving and adapting critical model parameters.
Additionally, we propose an Adaptive Knowledge Merging mechanism that combines
similar knowledge vectors to address scalability challenges, reducing memory
requirements while ensuring the retention of essential knowledge. Experiments
on three benchmarks demonstrate that the proposed CKA-RL outperforms
state-of-the-art methods, achieving an improvement of 4.20% in overall
performance and 8.02% in forward transfer. The source code is available at
https://github.com/Fhujinwu/CKA-RL.

</details>


### [10] [NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning](https://arxiv.org/abs/2510.19429)
*Wonje Choi,Jooyoung Kim,Honguk Woo*

Main category: cs.AI

TL;DR: NeSyPr是一个新颖的具身推理框架，通过神经符号程序化将知识编译，为基于语言模型的智能体提供结构化、自适应和及时的推理能力，在延迟敏感和资源受限的物理系统中实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决在动态环境中采用语言模型进行具身任务时面临的挑战，特别是在延迟、连接性和资源限制下无法在线访问大规模推理引擎或符号规划器的问题。

Method: 首先利用符号工具生成任务特定计划，然后将这些计划转换为可组合的程序表示，编码计划的隐式产生规则，使组合后的程序能够无缝集成到语言模型的推理过程中。

Result: 在PDDLGym、VirtualHome和ALFWorld等具身基准测试中，NeSyPr展示了比大规模推理模型和符号规划器更高效的推理能力，同时使用更紧凑的语言模型。

Conclusion: 神经符号程序化将多步符号结构化路径查找和推理抽象并泛化为单步语言模型推理，类似于人类知识编译，支持无需外部符号指导的高效测试时推理。

Abstract: We address the challenge of adopting language models (LMs) for embodied tasks
in dynamic environments, where online access to large-scale inference engines
or symbolic planners is constrained due to latency, connectivity, and resource
limitations. To this end, we present NeSyPr, a novel embodied reasoning
framework that compiles knowledge via neurosymbolic proceduralization, thereby
equipping LM-based agents with structured, adaptive, and timely reasoning
capabilities. In NeSyPr, task-specific plans are first explicitly generated by
a symbolic tool leveraging its declarative knowledge. These plans are then
transformed into composable procedural representations that encode the plans'
implicit production rules, enabling the resulting composed procedures to be
seamlessly integrated into the LM's inference process. This neurosymbolic
proceduralization abstracts and generalizes multi-step symbolic structured
path-finding and reasoning into single-step LM inference, akin to human
knowledge compilation. It supports efficient test-time inference without
relying on external symbolic guidance, making it well suited for deployment in
latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr
on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating
its efficient reasoning capabilities over large-scale reasoning models and a
symbolic planner, while using more compact LMs.

</details>


### [11] [AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing](https://arxiv.org/abs/2510.19661)
*Xusen Guo,Mingxing Peng,Xixuan Hao,Xingchen Zou,Qiongyan Wang,Sijie Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: AgentSense是一个基于多智能体进化系统的混合训练免费框架，将大语言模型集成到参与式城市感知中，通过迭代优化解决方案来适应动态城市条件和异构工作者偏好，同时提供自然语言解释以增强透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 现有的城市感知系统在多样化城市场景中的泛化能力有限，决策过程缺乏可解释性。为了解决这些问题，需要开发能够适应动态城市条件、考虑工作者偏好并提供透明解释的感知系统。

Method: AgentSense采用混合训练免费框架，首先使用经典规划器生成基线解决方案，然后通过多智能体进化系统迭代优化这些解决方案，使其适应动态城市条件和异构工作者偏好，同时生成自然语言解释。

Result: 在两个大规模移动数据集和七种动态干扰类型上的广泛实验表明，AgentSense在适应性和可解释性方面优于传统方法。与单智能体LLM基线相比，该方法在性能和鲁棒性方面表现更优，并提供更合理和透明的解释。

Conclusion: AgentSense代表了向部署自适应和可解释城市感知系统的重要进展，为基于Web的城市管理提供了有效的解决方案。

Abstract: Web-based participatory urban sensing has emerged as a vital approach for
modern urban management by leveraging mobile individuals as distributed
sensors. However, existing urban sensing systems struggle with limited
generalization across diverse urban scenarios and poor interpretability in
decision-making. In this work, we introduce AgentSense, a hybrid, training-free
framework that integrates large language models (LLMs) into participatory urban
sensing through a multi-agent evolution system. AgentSense initially employs
classical planner to generate baseline solutions and then iteratively refines
them to adapt sensing task assignments to dynamic urban conditions and
heterogeneous worker preferences, while producing natural language explanations
that enhance transparency and trust. Extensive experiments across two
large-scale mobility datasets and seven types of dynamic disturbances
demonstrate that AgentSense offers distinct advantages in adaptivity and
explainability over traditional methods. Furthermore, compared to single-agent
LLM baselines, our approach outperforms in both performance and robustness,
while delivering more reasonable and transparent explanations. These results
position AgentSense as a significant advancement towards deploying adaptive and
explainable urban sensing systems on the web.

</details>


### [12] [SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems](https://arxiv.org/abs/2509.23130)
*Qian Cheng,Ruize Tang,Emilie Ma,Finn Hackett,Peiyang He,Yiming Su,Ivan Beschastnikh,Yu Huang,Xiaoxing Ma,Tianyin Xu*

Main category: cs.AI

TL;DR: SysMoBench是一个评估AI为大型复杂系统生成形式化模型能力的基准测试，专注于并发和分布式系统，使用TLA+作为规范语言，包含9个系统组件并自动化评估指标。


<details>
  <summary>Details</summary>
Motivation: 形式化模型对于指定大型复杂计算机系统并验证其正确性至关重要，但编写和维护成本高昂。现有AI方法主要针对小型代码而非完整系统，需要评估AI是否能处理现实系统组件并将其复杂行为属性抽象为形式化模型。

Method: 开发SysMoBench基准测试，专注于并发和分布式系统，使用TLA+作为规范语言，自动化评估语法和运行时正确性、与系统代码一致性以及不变式正确性等指标。

Result: SysMoBench目前包含9个多样化系统组件：Etcd和Redis的Raft实现、Asterinas OS中的自旋锁和互斥锁等，更多组件正在积极添加中。

Conclusion: SysMoBench使研究人员能够了解当前LLM和智能体的能力与局限性，为该领域工具提供坚实基础，并开辟有前景的新研究方向。

Abstract: Formal models are essential to specifying large, complex computer systems and
verifying their correctness, but are notoriously expensive to write and
maintain. Recent advances in generative AI show promise in generating certain
forms of specifications. However, existing work mostly targets small code, not
complete systems. It is unclear whether AI can deal with realistic system
artifacts, as this requires abstracting their complex behavioral properties
into formal models. We present SysMoBench, a benchmark that evaluates AI's
ability to formally model large, complex systems. We focus on concurrent and
distributed systems, which are keystones of today's critical computing
infrastructures, encompassing operating systems and cloud infrastructure. We
use TLA+, the de facto specification language for concurrent and distributed
systems, though the benchmark can be extended to other specification languages.
We address the primary challenge of evaluating AI-generated models by
automating metrics like syntactic and runtime correctness, conformance to
system code, and invariant correctness. SysMoBench currently includes nine
diverse system artifacts: the Raft implementation of Etcd and Redis, the
Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively
added. SysMoBench enables us to understand the capabilities and limitations of
today's LLMs and agents, putting tools in this area on a firm footing and
opening up promising new research directions.

</details>


### [13] [A Graph Engine for Guitar Chord-Tone Soloing Education](https://arxiv.org/abs/2510.19666)
*Matthew Keating,Michael Casey*

Main category: cs.AI

TL;DR: 开发了一个基于图论的吉他即兴演奏建议引擎，通过构建和弦音阶图并计算最优路径，为吉他学生提供和弦音独奏练习指导。


<details>
  <summary>Details</summary>
Motivation: 和弦音独奏是爵士吉他即兴演奏的基础练习，但学习和练习难度较大，需要系统化的工具来帮助学生掌握这一重要技能。

Method: 首先生成和弦音阶，构建加权图结构，计算相邻和弦节点间的最优过渡音权重，通过最短路径算法生成和弦音独奏线。

Result: 开发了一个用户友好的系统，能够处理输入和输出，为吉他学生提供有效的和弦音独奏练习工具。

Conclusion: 该基于图论的引擎为吉他学生提供了一个实用的和弦音独奏练习系统，有助于掌握爵士吉他即兴演奏的基础技能。

Abstract: We present a graph-based engine for computing chord tone soloing suggestions
for guitar students. Chord tone soloing is a fundamental practice for
improvising over a chord progression, where the instrumentalist uses only the
notes contained in the current chord. This practice is a building block for all
advanced jazz guitar theory but is difficult to learn and practice. First, we
discuss methods for generating chord-tone arpeggios. Next, we construct a
weighted graph where each node represents a chord tone arpeggio for a chord in
the progression. Then, we calculate the edge weight between each consecutive
chord's nodes in terms of optimal transition tones. We then find the shortest
path through this graph and reconstruct a chord-tone soloing line. Finally, we
discuss a user-friendly system to handle input and output to this engine for
guitar students to practice chord tone soloing.

</details>


### [14] [Explainable e-sports win prediction through Machine Learning classification in streaming](https://arxiv.org/abs/2510.19671)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.AI

TL;DR: 本文提出了一种可解释的电子竞技流式胜率预测分类解决方案，通过滑动窗口控制输入数据以反映游戏变化，准确率超过90%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子竞技观众和玩家数量的增长，以及通信解决方案和云计算技术的发展，推动了在线游戏行业的持续发展。虽然基于人工智能的电子竞技分析传统上被定义为从相关数据中提取有意义的模式并进行可视化以增强决策，但大多数专业胜率预测工作都集中在批量分类方面，而忽略了可视化技术。

Method: 提出了一种可解释的流式胜率预测分类解决方案，通过多个滑动窗口控制输入数据以反映相关游戏变化。

Result: 实验结果显示准确率超过90%，超越了文献中的竞争解决方案性能。

Conclusion: 该系统可被排名和推荐系统利用进行明智决策，得益于可解释性模块，增强了对结果预测的信任。

Abstract: The increasing number of spectators and players in e-sports, along with the
development of optimized communication solutions and cloud computing
technology, has motivated the constant growth of the online game industry. Even
though Artificial Intelligence-based solutions for e-sports analytics are
traditionally defined as extracting meaningful patterns from related data and
visualizing them to enhance decision-making, most of the effort in professional
winning prediction has been focused on the classification aspect from a batch
perspective, also leaving aside the visualization techniques. Consequently,
this work contributes to an explainable win prediction classification solution
in streaming in which input data is controlled over several sliding windows to
reflect relevant game changes. Experimental results attained an accuracy higher
than 90 %, surpassing the performance of competing solutions in the literature.
Ultimately, our system can be leveraged by ranking and recommender systems for
informed decision-making, thanks to the explainability module, which fosters
trust in the outcome predictions.

</details>


### [15] [RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models](https://arxiv.org/abs/2510.19698)
*Yang Yang,Hua XU,Zhangyi Hu,Yutao Yue*

Main category: cs.AI

TL;DR: RLIE是一个将大语言模型与概率建模相结合的统一框架，用于学习加权规则集，通过规则生成、逻辑回归、迭代优化和评估四个阶段实现更可靠的神经符号推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法忽视了规则间的相互作用，且将LLM与概率规则学习结合进行鲁棒推理的机会尚未充分探索。

Method: RLIE包含四个阶段：LLM生成和筛选候选规则；逻辑回归学习概率权重；基于预测误差迭代优化规则集；评估加权规则集作为直接分类器的性能。

Result: 直接使用学习到的权重应用规则可获得优越性能，而将规则、权重和逻辑模型输出提示给LLM反而会降低准确性。

Conclusion: LLM擅长语义生成和解释，但在精确概率集成方面可靠性较低。RLIE阐明了LLM在归纳推理中的潜力和局限性，并将其与经典概率规则组合方法结合以实现更可靠的神经符号推理。

Abstract: Large Language Models (LLMs) can propose rules in natural language,
sidestepping the need for a predefined predicate space in traditional rule
learning. Yet many LLM-based approaches ignore interactions among rules, and
the opportunity to couple LLMs with probabilistic rule learning for robust
inference remains underexplored. We present RLIE, a unified framework that
integrates LLMs with probabilistic modeling to learn a set of weighted rules.
RLIE has four stages: (1) Rule generation, where an LLM proposes and filters
candidates; (2) Logistic regression, which learns probabilistic weights for
global selection and calibration; (3) Iterative refinement, which updates the
rule set using prediction errors; and (4) Evaluation, which compares the
weighted rule set as a direct classifier with methods that inject rules into an
LLM. We evaluate multiple inference strategies on real-world datasets. Applying
rules directly with their learned weights yields superior performance, whereas
prompting LLMs with the rules, weights, and logistic-model outputs surprisingly
degrades accuracy. This supports the view that LLMs excel at semantic
generation and interpretation but are less reliable for precise probabilistic
integration. RLIE clarifies the potential and limitations of LLMs for inductive
reasoning and couples them with classic probabilistic rule combination methods
to enable more reliable neuro-symbolic reasoning.

</details>


### [16] [Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning](https://arxiv.org/abs/2510.19732)
*Gunshi Gupta,Karmesh Yadav,Zsolt Kira,Yarin Gal,Rahaf Aljundi*

Main category: cs.AI

TL;DR: Memo是一种基于transformer的架构和训练方法，用于解决强化学习中长时程任务的记忆问题，通过插入周期性总结标记来创建和检索记忆，在计算和存储效率上优于传统长上下文transformer方法。


<details>
  <summary>Details</summary>
Motivation: 现有transformer策略在具身决策任务中视觉输入常常超出上下文限制，而人类能够将终身经验压缩为记忆使用。现有方法要么使用固定大小记忆的循环模型，要么依赖完整上下文的transformer，缺乏有效的记忆压缩机制。

Method: 提出Memo架构，在训练期间将周期性总结标记与模型输入交错，实现记忆的创建和检索。该方法在网格世界元强化学习基准和逼真室内环境的多目标导航任务上进行验证。

Result: Memo在计算和存储效率上优于传统长上下文transformer基线，在推理时对更长上下文具有更好的泛化能力，在需要截断历史上下文的流式设置中保持鲁棒性。

Conclusion: Memo通过记忆压缩机制有效解决了长时程强化学习任务中的上下文限制问题，为具身智能体在扩展时间框架内的有效操作提供了可行方案。

Abstract: To enable embodied agents to operate effectively over extended timeframes, it
is crucial to develop models that form and access memories to stay
contextualized in their environment. In the current paradigm of training
transformer-based policies for embodied sequential decision-making tasks,
visual inputs often overwhelm the context limits of transformers, while humans
can maintain and utilize a lifetime of experience compressed as memories.
Significant compression is possible in principle, as much of the input is
irrelevant and can be abstracted. However, existing approaches predominantly
focus on either recurrent models with fixed-size memory or transformers with
full-context reliance. In this work, we propose Memo, a transformer-based
architecture and training recipe for reinforcement learning (RL) on
memory-intensive, long-horizon tasks. Memo incorporates the creation and
retrieval of memory by interleaving periodic summarization tokens with the
inputs of a model during training. We demonstrate Memo's effectiveness on a
gridworld meta-RL benchmark and a multi-object navigation task in
photo-realistic indoor settings. Memo outperforms naive long-context
transformer baselines while being more compute and storage efficient.
Additionally, Memo generalizes better to longer contexts at inference time and
remains robust in streaming settings, where historical context must be
truncated to fit inference constraints.

</details>


### [17] [Misalignment Bounty: Crowdsourcing AI Agent Misbehavior](https://arxiv.org/abs/2510.19738)
*Rustem Turtayev,Natalia Fedorova,Oleg Serikov,Sergey Koldyba,Lev Avagyan,Dmitrii Volkov*

Main category: cs.AI

TL;DR: 论文介绍了'Misalignment Bounty'项目，这是一个众包项目，旨在收集AI系统追求非预期或不安全目标的案例。项目收到295份提交，其中9份获奖。


<details>
  <summary>Details</summary>
Motivation: 收集AI系统与人类意图不一致行为的清晰、可复现案例，以更好地理解和解决AI对齐问题。

Method: 通过众包项目收集案例，使用特定评估标准对295份提交进行筛选，最终选出9个获奖案例。

Result: 成功收集了295份案例提交，其中9个案例被认定为有价值的AI对齐问题实例。

Conclusion: Misalignment Bounty项目为研究AI对齐问题提供了实际案例，展示了AI系统可能偏离人类意图的具体方式。

Abstract: Advanced AI systems sometimes act in ways that differ from human intent. To
gather clear, reproducible examples, we ran the Misalignment Bounty: a
crowdsourced project that collected cases of agents pursuing unintended or
unsafe goals. The bounty received 295 submissions, of which nine were awarded.
  This report explains the program's motivation and evaluation criteria, and
walks through the nine winning submissions step by step.

</details>


### [18] [Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents](https://arxiv.org/abs/2510.19771)
*Gil Pasternak,Dheeraj Rajagopal,Julia White,Dhruv Atreja,Matthew Thomas,George Hurn-Maloney,Ash Lewis*

Main category: cs.AI

TL;DR: 本文提出了PROBE基准来评估LLM智能体的主动性能力，发现即使是先进模型在该基准上的端到端性能也仅为40%，揭示了自主行动系统的当前局限性。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM智能体主动性的基准局限于局部上下文，无法测试跨来源和长时间跨度的推理能力，需要新的评估框架。

Method: 提出PROBE基准，将主动性分解为三个核心能力：搜索未指定问题、识别具体瓶颈、执行适当解决方案，并应用于评估领先LLM和流行智能体框架。

Result: GPT-5和Claude Opus-4.1在端到端性能上达到40%的最佳表现，但仍远未达到理想水平，揭示了各模型的相对能力和共同失败模式。

Conclusion: 研究结果突显了智能体系统中自主行动的当前局限性，并揭示了有前景的未来研究方向。

Abstract: LLM-based agents are increasingly moving towards proactivity: rather than
awaiting instruction, they exercise agency to anticipate user needs and solve
them autonomously. However, evaluating proactivity is challenging; current
benchmarks are constrained to localized context, limiting their ability to test
reasoning across sources and longer time horizons. To address this gap, we
present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes
proactivity as a pipeline of three core capabilities: (1) searching for
unspecified issues, (2) identifying specific bottlenecks, and (3) executing
appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular
agentic frameworks, showing that even state-of-the-art models struggle to solve
this benchmark. Computing our consistent measurements across frontier LLMs and
agents, we find that the best end-to-end performance of 40% is achieved by both
GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative
capabilities of each model and analyze mutual failure modes. Our results
highlight the current limitations of autonomous action in agentic systems, and
expose promising future research directions.

</details>


### [19] [Benchmarking World-Model Learning](https://arxiv.org/abs/2510.19788)
*Archana Warrier,Dat Nyugen,Michelangelo Naim,Moksh Jain,Yichao Liang,Karen Schroeder,Cambridge Yang,Joshua B. Tenenbaum,Sebastian Vollmer,Kevin Ellis,Zenna Tavares*

Main category: cs.AI

TL;DR: WorldTest是一个评估模型学习智能体的新协议，将无奖励交互与在不同但相关环境中的评分测试阶段分离，支持多种未知任务。作者开发了AutumnBench测试套件，包含43个交互式网格世界环境和129个任务，发现人类表现优于前沿模型，且计算扩展仅在某些环境中提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型学习和评估方法偏离了真正目标：训练和评估都锚定在下一帧预测上，成功标准是在同一环境中最大化奖励。作者希望开发一个能评估智能体对环境动态学习程度的协议，支持多种下游任务。

Method: 提出WorldTest协议，包含无奖励探索、衍生测试和行为评分三个要素。开发AutumnBench测试套件，包含43个网格世界环境和129个任务，涵盖三个任务家族：掩码帧预测、规划和因果动态变化预测。比较了517名人类参与者和三个前沿模型的表现。

Result: 人类在所有任务上都优于模型表现。计算扩展仅在某些环境中提高模型性能，在其他环境中无效。AutumnBench揭示了世界模型学习方面存在显著提升空间。

Conclusion: WorldTest提供了一个新颖的评估模板来评估智能体对环境动态的学习程度，AutumnBench测试套件暴露了当前世界模型学习的局限性，为未来研究指明了改进方向。

Abstract: Model-learning agents should gather information to learn world models that
support many downstream tasks and inferences, such as predicting unobserved
states, estimating near- and far-term consequences of actions, planning action
sequences, and detecting changes in dynamics. Current methods for learning and
evaluating world models diverge from this goal: training and evaluation are
anchored to next-frame prediction, and success is scored by reward maximization
in the same environment. We propose WorldTest, a protocol to evaluate
model-learning agents that separates reward-free interaction from a scored test
phase in a different but related environment. WorldTest is
open-ended$\unicode{x2014}$models should support many different tasks unknown
ahead of time$\unicode{x2014}$and agnostic to model representation, allowing
comparison across approaches. We instantiated WorldTest with AutumnBench, a
suite of 43 interactive grid-world environments and 129 tasks across three
families: masked-frame prediction, planning, and predicting changes to the
causal dynamics. We compared 517 human participants and three frontier models
on AutumnBench. We found that humans outperform the models, and scaling compute
improves performance only in some environments but not others. WorldTest
provides a novel template$\unicode{x2014}$reward-free exploration, derived
tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn
about environment dynamics, and AutumnBench exposes significant headroom in
world-model learning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: CodeCRDT提出了一种基于观察驱动的协调模式，通过共享状态监控而非显式消息传递来实现多智能体LLM系统的并行加速，使用CRDT技术实现无锁、无冲突的并发代码生成。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统由于昂贵的协调成本而无法实现并行加速，需要一种更高效的协调机制来提升性能。

Method: 采用基于冲突自由复制数据类型（CRDTs）的观察驱动协调模式，智能体通过监控共享状态的可见更新和确定性收敛来进行协调，而不是显式消息传递。

Result: 在600次试验（6个任务，每种模式50次运行）中，某些任务实现了最高21.1%的加速，其他任务最多39.4%的减速，100%收敛且零合并失败。揭示了5-10%的语义冲突率和质量-性能权衡。

Conclusion: 研究形式化了随机LLM智能体的观察驱动协调，提供了基于任务结构的并行协调成功与失败的实证特征描述。

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [21] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: 该论文提出了一种结合大型语言模型（LLMs）随机代码生成和确定性验证的AI驱动分布式系统策略设计方法，通过生成-验证循环框架实现调度器设计。


<details>
  <summary>Details</summary>
Motivation: 探索AI在分布式系统策略设计中的应用，通过结合LLMs的生成能力和领域特定模拟器的验证，在保持可解释性的同时实现大规模设计空间的定向搜索。

Method: 使用Function-as-a-Service运行时（Bauplan）及其开源模拟器（Eudoxia）作为案例研究，建立迭代的生成-验证循环：LLM提出Python策略，模拟器在标准化跟踪上评估，结构化反馈指导后续生成。

Result: 在多个模型上报告了吞吐量改进的初步结果，展示了早期收益。

Conclusion: 讨论了当前设置的局限性并概述了后续步骤，特别推测AI在通过帮助引导新模拟器来扩展该方法论方面将至关重要。

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [22] [Comparative analysis of large data processing in Apache Spark using Java, Python and Scala](https://arxiv.org/abs/2510.19012)
*Ivan Borodii,Illia Fedorovych,Halyna Osukhivska,Diana Velychko,Roman Butsii*

Main category: cs.DC

TL;DR: 该研究比较了在Apache Spark平台上使用Java、Python和Scala处理大数据的性能差异。结果显示，对于小数据集（5MB），Python表现最佳（6.71秒）；对于大数据集（1.6GB），三种语言性能相近，Python略优（46.34秒）；对于复杂操作（合并两个CSV文件），Scala表现最好（374.42秒）。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注单个处理阶段，但使用Apache Iceberg跨编程语言对完整ETL工作流程进行全面比较的研究仍然有限。本研究旨在填补这一空白，分析不同编程语言在Apache Spark平台上的数据处理性能差异。

Method: 通过执行多个操作进行对比分析，包括从CSV文件下载数据、转换数据并将其加载到Apache Iceberg分析表中。测试了不同大小的数据集（5MB和1.6GB）以及复杂操作（合并两个CSV文件）。

Result: 处理5MB CSV文件时，Python最快（6.71秒），Scala次之（9.13秒），Java最慢（9.62秒）。处理1.6GB CSV文件时，三种语言性能相近：Python（46.34秒）、Scala（47.72秒）、Java（50.56秒）。复杂操作中，Scala最快（374.42秒），Java次之（379.8秒），Python最慢（398.32秒）。

Conclusion: 编程语言显著影响Apache Spark算法的数据处理效率。Scala和Java在处理大数据量和复杂操作时更具生产力，而Python在处理小数据量时具有优势。这些结果可根据特定性能要求和处理信息量来优化数据处理过程。

Abstract: During the study, the results of a comparative analysis of the process of
handling large datasets using the Apache Spark platform in Java, Python, and
Scala programming languages were obtained. Although prior works have focused on
individual stages, comprehensive comparisons of full ETL workflows across
programming languages using Apache Iceberg remain limited. The analysis was
performed by executing several operations, including downloading data from CSV
files, transforming and loading it into an Apache Iceberg analytical table. It
was found that the performance of the Spark algorithm varies significantly
depending on the amount of data and the programming language used. When
processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71
seconds, which is superior to Scala's score of 9.13 seconds and Java's time of
9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming
languages demonstrated similar results: the fastest performance was showed in
Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56
seconds, respectively. When performing a more complex operation that involved
combining two CSV files into a single dataset for further loading into an
Apache Iceberg table, Scala demonstrated the highest performance, at 374.42
seconds. Java processing was completed in 379.8 seconds, while Python was the
least efficient, with a runtime of 398.32 seconds. It follows that the
programming language significantly affects the efficiency of data processing by
the Apache Spark algorithm, with Scala and Java being more productive for
processing large amounts of data and complex operations, while Python
demonstrates an advantage in working with small amounts of data. The results
obtained can be useful for optimizing data handling processes depending on
specific performance requirements and the amount of information being
processed.

</details>


### [23] [On the Randomized Locality of Matching Problems in Regular Graphs](https://arxiv.org/abs/2510.19151)
*Seri Khoury,Manish Purohit,Aaron Schild,Joshua Wang*

Main category: cs.DC

TL;DR: 本文研究了正则图中匹配问题的局部性，证明了(1+ε)-近似匹配是真正局部的（仅依赖于ε），而最大匹配需要依赖于节点数n或度数Δ。同时揭示了正则图中最大匹配的节点平均复杂度与最坏情况复杂度之间的强分离。


<details>
  <summary>Details</summary>
Motivation: 分布式对称性破坏的主要目标是理解问题的局部性，即节点需要探索邻域的半径才能获得全局解决方案。在正则图族中研究匹配问题的局部性，这是建立对称性破坏问题局部性下界和分类结果的主要基准。

Method: 开发随机算法，使用基于鞅的分析方法对Luby已有40年历史的算法进行分析，证明在Δ-正则图的线图上应用一轮Luby算法会产生几乎Δ/2-正则的图。

Result: 证明了(1+ε)-近似匹配在正则图中是真正局部的，仅依赖于ε且独立于其他图参数；当Δ≥poly(1/ε)时，这种依赖关系仅为1/ε的对数。同时建立了最大匹配节点平均复杂度与最坏情况复杂度的强分离，前者仅为O(1)。

Conclusion: 正则图中的近似匹配具有真正的局部性特征，而最大匹配的局部性需要依赖于图规模参数。这一结果揭示了分布式对称性破坏问题中局部性特征的重要差异，并为理解匹配问题的计算复杂性提供了新的见解。

Abstract: The main goal in distributed symmetry-breaking is to understand the locality
of problems; i.e., the radius of the neighborhood that a node needs to explore
in order to arrive at its part of a global solution. In this work, we study the
locality of matching problems in the family of regular graphs, which is one of
the main benchmarks for establishing lower bounds on the locality of
symmetry-breaking problems, as well as for obtaining classification results.
For approximate matching, we develop randomized algorithms to show that $(1 +
\epsilon)$-approximate matching in regular graphs is truly local; i.e., the
locality depends only on $\epsilon$ and is independent of all other graph
parameters. Furthermore, as long as the degree $\Delta$ is not very small
(namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is
only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal
matching in regular graphs which requires some dependence on the number of
nodes $n$ or the degree $\Delta$. We show matching lower bounds for both
results. For maximal matching, our techniques further allow us to establish a
strong separation between the node-averaged complexity and worst-case
complexity of maximal matching in regular graphs, by showing that the former is
only $O(1)$. Central to our main technical contribution is a novel
martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In
particular, our analysis shows that applying one round of Luby's algorithm on
the line graph of a $\Delta$-regular graph results in an almost
$\Delta/2$-regular graph.

</details>


### [24] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: RLBoost是一个利用可抢占GPU资源进行成本高效强化学习训练的系统，通过自适应卸载、基于拉取的权重转移和令牌级响应收集等关键技术，在提升训练吞吐量的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 强化学习工作流中，rollout阶段占用大部分执行时间但可并行扩展，而训练阶段需要紧密耦合的GPU。现有框架无法有效解决这种资源需求矛盾，同时可抢占GPU资源提供了显著的成本节约机会。

Method: 采用混合架构，包含三个关键技术：(1) 自适应rollout卸载，动态调整预留集群上的工作负载；(2) 基于拉取的权重转移，快速配置新可用实例；(3) 令牌级响应收集和迁移，用于高效处理抢占和持续负载均衡。

Result: 实验表明，RLBoost相比仅使用按需GPU资源，训练吞吐量提升1.51-1.97倍，成本效率提高28%-49%。

Conclusion: RLBoost通过有效利用可抢占GPU资源，成功解决了强化学习工作流中的资源利用效率问题，实现了训练吞吐量和成本效率的显著提升。

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [25] [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)
*Heng Xu,Zhiwei Yu,Chengze Du,Ying Zhou,Letian Li,Haojie Wang,Weiqiang Cheng,Jialong Li*

Main category: cs.DC

TL;DR: RailS是一个分布式负载均衡框架，专门优化MoE模型训练中的稀疏且高度不平衡的all-to-all通信，通过利用Rail架构的确定性拓扑和并行传输机制，显著提升带宽利用率和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统负载均衡方法无法充分利用Rail架构的多NIC带宽，导致MoE训练中的all-to-all通信成为性能瓶颈，需要新的拓扑感知负载均衡方案。

Method: RailS利用Rail拓扑的对称性，将全局协调转化为本地调度，每个节点独立执行LPT喷涂调度器进行主动流量平衡，并激活N条并行rail进行细粒度多路径传输。

Result: 在合成和真实MoE工作负载中，RailS将总线带宽提升20%-78%，完成时间减少17%-78%；在Mixtral工作负载中，迭代时间缩短18%-40%，实现接近最优的负载均衡。

Conclusion: RailS通过拓扑感知的负载均衡和并行传输机制，充分挖掘分布式训练中的架构并行性，显著提升MoE模型的训练效率。

Abstract: Training Mixture-of-Experts (MoE) models introduces sparse and highly
imbalanced all-to-all communication that dominates iteration time. Conventional
load-balancing methods fail to exploit the deterministic topology of Rail
architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a
distributed load-balancing framework that minimizes all-to-all completion time
in MoE training. RailS leverages the Rail topology's symmetry to prove that
uniform sending ensures uniform receiving, transforming global coordination
into local scheduling. Each node independently executes a Longest Processing
Time First (LPT) spraying scheduler to proactively balance traffic using local
information. RailS activates N parallel rails for fine-grained, topology-aware
multipath transmission. Across synthetic and real-world MoE workloads, RailS
improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For
Mixtral workloads, it shortens iteration time by 18%--40% and achieves
near-optimal load balance, fully exploiting architectural parallelism in
distributed training.

</details>


### [26] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: FLASH Viterbi是一种快速、轻量级、自适应且硬件友好的Viterbi解码算子，通过非递归分治策略结合剪枝和并行化技术，提升时间和内存效率，适用于资源受限的数据系统。


<details>
  <summary>Details</summary>
Motivation: 随着结构化序列推理工作负载迁移到资源受限的边缘平台，标准的Viterbi解码存在内存密集和计算不灵活的问题，现有方法通常在解码时间和空间效率之间权衡，但往往产生显著的运行时开销且缺乏对各种系统约束的适应性。

Method: 提出了FLASH Viterbi算法，结合非递归分治策略与剪枝和并行化技术；进一步提出了FLASH-BS Viterbi，基于内存高效数据结构的动态波束搜索变体，通过动态调整内部参数适应不同部署场景。

Result: 实验表明，所提算法在解码时间和内存效率方面持续优于现有基线方法，同时保持了现代数据系统所需的适应性和硬件友好特性。

Conclusion: FLASH Viterbi系列算法为资源受限的数据系统提供了高效、自适应的Viterbi解码解决方案，并开发了基于FPGA的硬件加速器，展示了高吞吐量和低资源使用。

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems.To further decouple
space complexity from the hidden state space size, we present FLASH-BS Viterbi,
a dynamic beam search variant built on a memory-efficient data structure. Both
proposed algorithms exhibit strong adaptivity to diverse deployment scenarios
by dynamically tuning internal parameters.To ensure practical deployment on
edge devices, we also develop FPGA-based hardware accelerators for both
algorithms, demonstrating high throughput and low resource usage. Extensive
experiments show that our algorithms consistently outperform existing baselines
in both decoding time and memory efficiency, while preserving adaptability and
hardware-friendly characteristics essential for modern data systems. All codes
are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [27] [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)
*Weihao Yang,Hao Huang,Donglei Wu,Ningke Li,Yanqi Pan,Qiyang Zheng,Wen Xia,Shiyi Li,Qiang Wang*

Main category: cs.DC

TL;DR: 提出HybridEP框架，通过动态调整专家空间布局来减少数据通信流量和频率，优化受限带宽下的专家并行训练性能


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型规模快速增长，单数据中心训练已无法满足需求，转向跨数据中心训练。但在低带宽场景下，现有专家并行优化效果有限，成为MoE模型持续增长的关键瓶颈

Method: 构建流式模型确定最优传输比例，结合基于域的分区构建混合模式与GPU级通信拓扑的映射，以及参数高效迁移技术减少专家传输开销并扩大域规模

Result: 在受限带宽下，HybridEP比现有最先进的MoE训练系统性能提升高达5.6倍；在1000个数据中心的大规模模拟中，在不同带宽下实现高达1.45倍的加速

Conclusion: HybridEP可被视为具有更好可扩展性的更通用专家并行方法，有效解决了跨数据中心专家并行的通信瓶颈问题

Abstract: Mixture-of-Experts (MoE) has become a popular architecture for scaling large
models. However, the rapidly growing scale outpaces model training on a single
DC, driving a shift toward a more flexible, cross-DC training paradigm. Under
this, Expert Parallelism (EP) of MoE faces significant scalability issues due
to the limited cross-DC bandwidth. Specifically, existing EP optimizations
attempt to overlap data communication and computation, which has little benefit
in low-bandwidth scenarios due to a much longer data communication time.
Therefore, the trends of cross-DC EP scaling is fast becoming a critical
roadblock to the continued growth of MoE models.
  To address this, we propose HybridEP, a modeling-guided framework to optimize
EP under constrained bandwidth. Our key idea is to dynamically transform the
spatial placement of experts to reduce data communication traffic and
frequency, thereby minimizing EP's communication overheads. However, it is
non-trivial to find the optimal solution because it complicates the original
communication pattern by mixing data and expert communication. We therefore
build a stream-based model to determine the optimal transmission ratio. Guided
by this, we incorporate two techniques: (1) domain-based partition to construct
the mapping between hybrid patterns and specific communication topology at GPU
level, and (2) parameter-efficient migration to further refine this topology by
reducing expert transmission overhead and enlarging the domain size. Combining
all these designs, HybridEP can be considered as a more general EP with better
scalability. Experimental results show that HybridEP outperforms existing
state-of-the-art MoE training systems by up to 5.6x under constrained
bandwidth. We further compare HybridEP and EP on large-scale simulations.
HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.

</details>


### [28] [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)
*Eric Ding*

Main category: cs.DC

TL;DR: 提出Propius系统，用于解决协作机器学习中的资源管理问题，通过控制平面和数据平面实现高效资源分配和模型共享，显著提升资源利用率、吞吐量和任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 当前协作机器学习缺乏可扩展的基础设施，开发者通常构建特定用途的服务器-客户端系统，无法满足大规模协作ML的需求，需要可扩展、高效的多租户资源管理系统。

Method: 设计Propius系统，包含控制平面和数据平面。控制平面支持多协作ML作业间的资源共享和多种资源分配策略，数据平面提升协作ML模型共享和结果收集的可扩展性。

Result: 评估显示Propius在资源利用率（最高1.88倍）、吞吐量（最高2.76倍）和任务完成时间（最高1.26倍）方面优于现有资源管理技术和框架。

Conclusion: Propius系统能够适应客户端机器的异构性，以可扩展方式高效管理和控制ML作业与边缘资源之间的计算流，为大规模协作ML提供有效的资源管理解决方案。

Abstract: Collaborative Machine Learning is a paradigm in the field of distributed
machine learning, designed to address the challenges of data privacy,
communication overhead, and model heterogeneity. There have been significant
advancements in optimization and communication algorithm design and ML hardware
that enables fair, efficient and secure collaborative ML training. However,
less emphasis is put on collaborative ML infrastructure development. Developers
and researchers often build server-client systems for a specific collaborative
ML use case, which is not scalable and reusable. As the scale of collaborative
ML grows, the need for a scalable, efficient, and ideally multi-tenant resource
management system becomes more pressing. We propose a novel system, Propius,
that can adapt to the heterogeneity of client machines, and efficiently manage
and control the computation flow between ML jobs and edge resources in a
scalable fashion. Propius is comprised of a control plane and a data plane. The
control plane enables efficient resource sharing among multiple collaborative
ML jobs and supports various resource sharing policies, while the data plane
improves the scalability of collaborative ML model sharing and result
collection. Evaluations show that Propius outperforms existing resource
management techniques and frameworks in terms of resource utilization (up to
$1.88\times$), throughput (up to $2.76$), and job completion time (up to
$1.26\times$).

</details>


### [29] [Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond](https://arxiv.org/abs/2510.19805)
*Carl-Johan Fauvelle Munck af Rosensch"old,Feras M. Awaysheh,Ahmad Awad*

Main category: cs.DC

TL;DR: 本文对Redis替代品Valkey、KeyDB和Garnet在Kubernetes环境下的性能进行了基准测试，评估了吞吐量、延迟、资源效率和迁移复杂性等指标。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对内存键值数据存储领域最新工具的实验评估，这些工具在现代云原生基础设施中至关重要，但其发展面临可扩展性、兼容性和可持续性限制。

Method: 在Kubernetes部署中使用现实工作负载对Valkey、KeyDB和Garnet进行基准测试，系统评估性能指标。

Result: 结果显示被测试的数据系统之间存在明确的权衡关系，不同工具在性能、兼容性和长期可行性方面各有优劣。

Conclusion: 研究提供了对新兴内存键值存储的全面性能和可行性评估，强调了性能、兼容性和长期可行性之间的权衡关系，包括项目成熟度、社区支持和持续发展等方面。

Abstract: In-memory key-value datastores have become indispensable building blocks of
modern cloud-native infrastructures, yet their evolution faces scalability,
compatibility, and sustainability constraints. The current literature lacks an
experimental evaluation of state-of-the-art tools in the domain. This study
addressed this timely gap by benchmarking Redis alternatives and systematically
evaluating Valkey, KeyDB, and Garnet under realistic workloads within
Kubernetes deployments. The results demonstrate clear trade-offs among the
benchmarked data systems. Our study presents a comprehensive performance and
viability assessment of the emerging in-memory key-value stores. Metrics
include throughput, tail latency, CPU and memory efficiency, and migration
complexity. We highlight trade-offs between performance, compatibility, and
long-term viability, including project maturity, community support, and
sustained development.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [30] [HAMLOCK: HArdware-Model LOgically Combined attacK](https://arxiv.org/abs/2510.19145)
*Sanskar Amgain,Daniel Lobo,Atri Chatterjee,Swarup Bhunia,Fnu Suya*

Main category: cs.CR

TL;DR: HAMLOCK是一种新型硬件-软件联合攻击，通过在硬件中植入木马来检测特定神经元激活，从而绕过传统模型级后门攻击检测方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型级后门攻击容易被检测，因为攻击逻辑完全嵌入在模型中，而HAMLOCK通过将攻击逻辑分布在硬件-软件边界来提高隐蔽性。

Method: 软件层面仅微调少量神经元的激活值，硬件层面通过监控神经元的最重要位或8位指数来检测特定激活，并直接操纵最终输出logits实现误分类。

Result: 在MNIST、CIFAR10、GTSRB和ImageNet等基准测试中，HAMLOCK实现了近乎完美的攻击成功率，且清洁准确率下降可忽略，成功规避了最先进的模型级防御。

Conclusion: HAMLOCK暴露了硬件-软件接口的关键漏洞，需要新的跨层防御来应对这种新兴威胁。

Abstract: The growing use of third-party hardware accelerators (e.g., FPGAs, ASICs) for
deep neural networks (DNNs) introduces new security vulnerabilities.
Conventional model-level backdoor attacks, which only poison a model's weights
to misclassify inputs with a specific trigger, are often detectable because the
entire attack logic is embedded within the model (i.e., software), creating a
traceable layer-by-layer activation path.
  This paper introduces the HArdware-Model Logically Combined Attack (HAMLOCK),
a far stealthier threat that distributes the attack logic across the
hardware-software boundary. The software (model) is now only minimally altered
by tuning the activations of few neurons to produce uniquely high activation
values when a trigger is present. A malicious hardware Trojan detects those
unique activations by monitoring the corresponding neurons' most significant
bit or the 8-bit exponents and triggers another hardware Trojan to directly
manipulate the final output logits for misclassification.
  This decoupled design is highly stealthy, as the model itself contains no
complete backdoor activation path as in conventional attacks and hence, appears
fully benign. Empirically, across benchmarks like MNIST, CIFAR10, GTSRB, and
ImageNet, HAMLOCK achieves a near-perfect attack success rate with a negligible
clean accuracy drop. More importantly, HAMLOCK circumvents the state-of-the-art
model-level defenses without any adaptive optimization. The hardware Trojan is
also undetectable, incurring area and power overheads as low as 0.01%, which is
easily masked by process and environmental noise. Our findings expose a
critical vulnerability at the hardware-software interface, demanding new
cross-layer defenses against this emerging threat.

</details>


### [31] [OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform](https://arxiv.org/abs/2510.19169)
*Thomas Wang,Haowen Li*

Main category: cs.CR

TL;DR: OpenGuardrails是一个开源项目，提供上下文感知的安全性和操作检测模型，以及可部署的AI护栏平台，保护内容安全、防止模型操作攻击和数据泄露。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的广泛集成，保护它们免受不安全、恶意或侵犯隐私的内容变得至关重要。

Method: 使用统一的大模型实现内容安全和模型操作检测，通过轻量级NER管道（如Presidio风格模型或基于正则表达式的检测器）进行数据泄露识别和编辑。系统可作为安全网关或基于API的服务部署。

Result: 在安全基准测试中达到最先进性能，在英语、中文和多语言任务中的提示和响应分类方面表现出色。

Conclusion: 所有模型均以Apache 2.0许可证发布供公众使用，提供企业级、完全私有的部署选项。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, safeguarding them against unsafe, malicious, or
privacy-violating content is critically important. We present OpenGuardrails,
the first open-source project to provide both a context-aware safety and
manipulation detection model and a deployable platform for comprehensive AI
guardrails. OpenGuardrails protects against content-safety risks,
model-manipulation attacks (e.g., prompt injection, jailbreaking,
code-interpreter abuse, and the generation/execution of malicious code), and
data leakage. Content-safety and model-manipulation detection are implemented
by a unified large model, while data-leakage identification and redaction are
performed by a separate lightweight NER pipeline (e.g., Presidio-style models
or regex-based detectors). The system can be deployed as a security gateway or
an API-based service, with enterprise-grade, fully private deployment options.
OpenGuardrails achieves state-of-the-art (SOTA) performance on safety
benchmarks, excelling in both prompt and response classification across
English, Chinese, and multilingual tasks. All models are released under the
Apache 2.0 license for public use.

</details>


### [32] [LAPRAD: LLM-Assisted PRotocol Attack Discovery](https://arxiv.org/abs/2510.19264)
*R. Can Aygun,Yehuda Afek,Anat Bremler-Barr,Leonard Kleinrock*

Main category: cs.CR

TL;DR: 本文提出了一种名为LAPRAD的LLM辅助协议攻击发现方法，通过三阶段流程（识别潜在漏洞、自动构建攻击配置、验证有效性）来发现DNS协议中的新DDoS攻击。


<details>
  <summary>Details</summary>
Motivation: 为了提高互联网协议的安全性，需要更快、半自动的方法来发现DNS、BGP等协议中的新漏洞。

Method: LAPRAD采用三阶段流程：1) 咨询经过DNS相关数据训练的LLM识别潜在漏洞；2) 使用不同LLM通过ReACT方法自动构建攻击配置；3) 验证攻击的功能性和有效性。

Result: 使用LAPRAD发现了三种新的DNS DDoS攻击：1) 诱饵切换技术欺骗解析器缓存大型伪造DNSSEC RRSIG；2) 利用大型DNSSEC加密算法绕过RRSet限制；3) 利用ANY类型响应产生类似效果。这些攻击严重降低解析器查询能力至6%。

Conclusion: LAPRAD方法能够有效发现新的DNS协议漏洞，这些名为SigCacheFlush的缓存刷新DDoS攻击变种能够绕过现有补丁，影响主要DNS解析器实现的最新版本。

Abstract: With the goal of improving the security of Internet protocols, we seek
faster, semi-automatic methods to discover new vulnerabilities in protocols
such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted
Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers
with some DNS knowledge to efficiently uncover vulnerabilities that would
otherwise be hard to detect.
  LAPRAD follows a three-stage process. In the first, we consult an LLM
(GPT-o1) that has been trained on a broad corpus of DNS-related sources and
previous DDoS attacks to identify potential exploits. In the second stage, a
different LLM automatically constructs the corresponding attack configurations
using the ReACT approach implemented via LangChain (DNS zone file generation).
Finally, in the third stage, we validate the attack's functionality and
effectiveness.
  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and
rediscovered two recently reported ones that were not included in the LLM's
training data. The first new attack employs a bait-and-switch technique to
trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving
capacity to as little as 6%. The second exploits large DNSSEC encryption
algorithms (RSA-4096) with multiple keys, thereby bypassing a recently
implemented default RRSet limit. The third leverages ANY-type responses to
produce a similar effect.
  These variations of a cache-flushing DDoS attack, called SigCacheFlush,
circumvent existing patches, severely degrade resolver query capacity, and
impact the latest versions of major DNS resolver implementations.

</details>


### [33] [A Probabilistic Computing Approach to the Closest Vector Problem for Lattice-Based Factoring](https://arxiv.org/abs/2510.19390)
*Max O. Al-Hasso,Marko von der Leyen*

Main category: cs.CR

TL;DR: 本文研究了将概率计算应用于格基分解算法中的CVP近似优化任务，展示了概率计算在解决CVP和作为格基分解子程序方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 最近的研究探讨了在格基分解算法中加入启发式CVP近似优化步骤，使用量子变分算法执行启发式优化。这与概率计算作为随机算法硬件加速器的出现相吻合。

Method: 提出了用于CVP近似优化的概率计算算法设计，讨论了'素数格'参数，并通过实验验证了概率计算在解决CVP及其作为格基分解子程序的有效性。

Result: 主要结果发现：(a)该方法能够在问题规模线性时间内找到最大可用CVP近似优化；(b)与类似量子和经典方法相比，概率计算结合所提出的格参数能够使用最多100倍少的格实例找到半素数的复合素因子。

Conclusion: 概率计算在CVP近似优化和格基分解中表现出显著优势，特别是在减少所需计算资源方面。

Abstract: The closest vector problem (CVP) is a fundamental optimization problem in
lattice-based cryptography and its conjectured hardness underpins the security
of lattice-based cryptosystems. Furthermore, Schnorr's lattice-based factoring
algorithm reduces integer factoring (the foundation of current cryptosystems,
including RSA) to the CVP. Recent work has investigated the inclusion of a
heuristic CVP approximation `refinement' step in the lattice-based factoring
algorithm, using quantum variational algorithms to perform the heuristic
optimization. This coincides with the emergence of probabilistic computing as a
hardware accelerator for randomized algorithms including tasks in combinatorial
optimization. In this work we investigate the application of probabilistic
computing to the heuristic optimization task of CVP approximation refinement in
lattice-based factoring. We present the design of a probabilistic computing
algorithm for this task, a discussion of `prime lattice' parameters, and
experimental results showing the efficacy of probabilistic computing for
solving the CVP as well as its efficacy as a subroutine for lattice-based
factoring. The main results found that (a) this approach is capable of finding
the maximal available CVP approximation refinement in time linear in problem
size and (b) probabilistic computing used in conjunction with the lattice
parameters presented can find the composite prime factors of a semiprime number
using up to 100x fewer lattice instances than similar quantum and classical
methods.

</details>


### [34] [From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data](https://arxiv.org/abs/2510.19418)
*Mete Harun Akcay,Buse Gul Atli,Siddharth Prakash Rao,Alexandros Bakas*

Main category: cs.CR

TL;DR: 提出了一种可信数据共享系统架构，通过策略驱动的访问控制实现敏感区域的选择性保护，结合自动检测、后校正、密钥管理和访问控制模块，在视觉数据集上验证了系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着存储数据量增长，在大规模存储库中识别和保护敏感信息变得日益困难，特别是在与具有不同角色和权限的多个用户共享数据时。

Method: 集成四个核心模块：自动敏感区域检测、后校正、密钥管理和访问控制。采用混合加密方案，使用对称加密提高效率，基于属性的加密实现策略执行。支持高效密钥分发并隔离密钥存储。

Result: 在视觉数据集上评估显示，系统提供有效的隐私敏感对象检测，宏观平均F1分数提高5%，平均精度均值提高10%，平均策略执行解密时间低于每张图像1秒。

Conclusion: 该系统为细粒度访问控制提供了有效、高效且可扩展的解决方案，能够在大规模数据共享环境中保护敏感信息。

Abstract: As the volume of stored data continues to grow, identifying and protecting
sensitive information within large repositories becomes increasingly
challenging, especially when shared with multiple users with different roles
and permissions. This work presents a system architecture for trusted data
sharing with policy-driven access control, enabling selective protection of
sensitive regions while maintaining scalability. The proposed architecture
integrates four core modules that combine automated detection of sensitive
regions, post-correction, key management, and access control. Sensitive regions
are secured using a hybrid scheme that employs symmetric encryption for
efficiency and Attribute-Based Encryption for policy enforcement. The system
supports efficient key distribution and isolates key storage to strengthen
overall security. To demonstrate its applicability, we evaluate the system on
visual datasets, where Privacy-Sensitive Objects in images are automatically
detected, reassessed, and selectively encrypted prior to sharing in a data
repository. Experimental results show that our system provides effective PSO
detection, increases macro-averaged F1 score (5%) and mean Average Precision
(10%), and maintains an average policy-enforced decryption time of less than 1
second per image. These results demonstrate the effectiveness, efficiency and
scalability of our proposed solution for fine-grained access control.

</details>


### [35] [Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation](https://arxiv.org/abs/2510.19420)
*Chengcan Wu,Zhixin Zhang,Mingqian Xu,Zeming Wei,Meng Sun*

Main category: cs.CR

TL;DR: 提出了一种针对基于大语言模型的多智能体系统的动态防御范式，通过持续监控通信并动态调整图拓扑结构来防御恶意攻击。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统涉及复杂的通信过程，容易受到腐败攻击，而现有的静态图防御方法无法有效应对动态攻击。

Method: 采用动态防御范式，持续监控MAS图中的通信，动态调整图拓扑结构，准确破坏恶意通信。

Result: 在日益复杂和动态的MAS环境中，该方法显著优于现有的MAS防御机制。

Conclusion: 该方法为基于LLM的多智能体系统的可信应用提供了有效的防护措施。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a
popular paradigm of AI applications. However, trustworthiness issues in MAS
remain a critical concern. Unlike challenges in single-agent systems, MAS
involve more complex communication processes, making them susceptible to
corruption attacks. To mitigate this issue, several defense mechanisms have
been developed based on the graph representation of MAS, where agents represent
nodes and communications form edges. Nevertheless, these methods predominantly
focus on static graph defense, attempting to either detect attacks in a fixed
graph structure or optimize a static topology with certain defensive
capabilities. To address this limitation, we propose a dynamic defense paradigm
for MAS graph structures, which continuously monitors communication within the
MAS graph, then dynamically adjusts the graph topology, accurately disrupts
malicious communications, and effectively defends against evolving and diverse
dynamic attacks. Experimental results in increasingly complex and dynamic MAS
environments demonstrate that our method significantly outperforms existing MAS
defense mechanisms, contributing an effective guardrail for their trustworthy
applications. Our code is available at
https://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems.

</details>


### [36] [Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains](https://arxiv.org/abs/2510.19491)
*Jonas Gebele,Timm Mutzel,Burak Oez,Florian Matthes*

Main category: cs.CR

TL;DR: 提出了一种基于可信执行环境(TEE)的密封投标拍卖协议，在机密计算区块链上执行敏感投标逻辑，同时在公共链上完成结算和执行，解决了密封投标的隐私性、可验证性和可扩展性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统密封投标拍卖在中心化基础设施上部署存在不透明操纵风险，而公共区块链的透明性与密封投标的机密性要求相冲突。现有方案难以在不依赖可信中介、多轮协议或昂贵密码学的情况下同时满足隐私性、可验证性和可扩展性。

Method: 使用TEE支持的机密计算区块链执行敏感投标逻辑，投标人将资金提交到由enclave生成的托管地址确保机密性和绑定承诺。截止后任何方都可触发解决方案：机密区块链通过可验证的链下计算确定获胜者，并签发签名结算交易在公共链上执行。

Result: 该设计在不依赖可信第三方或协议修改的情况下提供了安全性、隐私性和可扩展性。在SUAVE上实现并与以太坊结算集成，评估了其可扩展性和信任假设，展示了在现有基础设施上的最小集成部署。

Conclusion: 提出的协议成功解决了密封投标拍卖在区块链环境中的隐私与透明性矛盾，通过TEE和机密计算区块链的结合实现了无需可信中介的安全、私密且可扩展的拍卖系统。

Abstract: Sealed-bid auctions ensure fair competition and efficient allocation but are
often deployed on centralized infrastructure, enabling opaque manipulation.
Public blockchains eliminate central control, yet their inherent transparency
conflicts with the confidentiality required for sealed bidding. Prior attempts
struggle to reconcile privacy, verifiability, and scalability without relying
on trusted intermediaries, multi-round protocols, or expensive cryptography. We
present a sealed-bid auction protocol that executes sensitive bidding logic on
a Trusted Execution Environment (TEE)-backed confidential compute blockchain
while retaining settlement and enforcement on a public chain. Bidders commit
funds to enclave-generated escrow addresses, ensuring confidentiality and
binding commitments. After the deadline, any party can trigger resolution: the
confidential blockchain determines the winner through verifiable off-chain
computation and issues signed settlement transactions for execution on the
public chain. Our design provides security, privacy, and scalability without
trusted third parties or protocol modifications. We implement it on SUAVE with
Ethereum settlement, evaluate its scalability and trust assumptions, and
demonstrate deployment with minimal integration on existing infrastructure

</details>


### [37] [CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage](https://arxiv.org/abs/2510.19676)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型在RTL代码生成中的记忆问题，提出了CircuitGuard防御策略，通过RTL感知相似性度量和激活级引导方法，在保持生成质量的同时显著减少对专有设计的记忆泄露。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在RTL硬件合成中表现出色，但其记忆训练数据的倾向可能导致专有或安全敏感设计在推理过程中意外泄露。RTL代码的特殊性使得传统自然语言记忆检测方法不适用，需要专门解决方案。

Method: CircuitGuard包含两个核心组件：(1) 新颖的RTL感知相似性度量，捕捉结构和功能等价性；(2) 激活级引导方法，识别并衰减负责记忆的transformer组件。

Result: CircuitGuard在Llama 3.1-8B模型中识别出275个记忆关键特征，实现了高达80%的语义相似度降低，同时保持生成质量，并展示78-85%的跨域迁移效果。

Conclusion: CircuitGuard有效平衡了泄露减少和正确性保持，为RTL代码生成中的记忆问题提供了实用解决方案，具有跨电路类别的鲁棒性。

Abstract: Large Language Models (LLMs) have achieved remarkable success in generative
tasks, including register-transfer level (RTL) hardware synthesis. However,
their tendency to memorize training data poses critical risks when proprietary
or security-sensitive designs are unintentionally exposed during inference.
While prior work has examined memorization in natural language, RTL introduces
unique challenges: In RTL, structurally different implementations (e.g.,
behavioral vs. gate-level descriptions) can realize the same hardware, leading
to intellectual property (IP) leakage (full or partial) even without verbatim
overlap. Conversely, even small syntactic variations (e.g., operator precedence
or blocking vs. non-blocking assignments) can drastically alter circuit
behavior, making correctness preservation especially challenging. In this work,
we systematically study memorization in RTL code generation and propose
CircuitGuard, a defense strategy that balances leakage reduction with
correctness preservation. CircuitGuard (1) introduces a novel RTL-aware
similarity metric that captures both structural and functional equivalence
beyond surface-level overlap, and (2) develops an activation-level steering
method that identifies and attenuates transformer components most responsible
for memorization. Our empirical evaluation demonstrates that CircuitGuard
identifies (and isolates) 275 memorization-critical features across layers
18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic
similarity to proprietary patterns while maintaining generation quality.
CircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling
robust memorization mitigation across circuit categories without retraining.

</details>
