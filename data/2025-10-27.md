<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [FIFOAdvisor: A DSE Framework for Automated FIFO Sizing of High-Level Synthesis Designs](https://arxiv.org/abs/2510.20981)
*Stefan Abi-Karam,Rishov Sarkar,Suhail Basalama,Jason Cong,Callie Hao*

Main category: cs.AR

TL;DR: FIFOAdvisor是一个自动确定HLS设计中FIFO大小的框架，通过快速模拟和优化算法找到延迟与内存使用之间的帕累托最优解。


<details>
  <summary>Details</summary>
Motivation: 数据流硬件设计通过HLS实现高效的FPGA实现，但正确调整FIFO通道缓冲区大小具有挑战性。过小的FIFO会导致停顿和死锁，过大的FIFO会浪费内存。现有方法依赖限制性假设、保守过度分配或缓慢的RTL模拟。

Method: 利用LightningSim（99.9%周期精确模拟器）进行毫秒级增量运行，将FIFO大小确定制定为双目标黑盒优化问题，探索启发式和基于搜索的方法来表征延迟-资源权衡。与Stream-HLS框架集成，优化从C++、MLIR或PyTorch降低的仿射数据流设计。

Result: 在线性代数和深度学习工作负载的Stream-HLS设计基准测试中，FIFOAdvisor揭示了跨优化策略的帕累托最优延迟-内存前沿。与基线设计相比，以最小延迟开销实现了更低的内存使用，相比传统HLS/RTL协同模拟提供了显著的运行时加速。

Conclusion: FIFOAdvisor为快速设计空间探索提供了实用解决方案，能够处理具有数据相关控制流的复杂加速器，在保证死锁自由的同时优化FIFO大小。

Abstract: Dataflow hardware designs enable efficient FPGA implementations via
high-level synthesis (HLS), but correctly sizing first-in-first-out (FIFO)
channel buffers remains challenging. FIFO sizes are user-defined and balance
latency and area-undersized FIFOs cause stalls and potential deadlocks, while
oversized ones waste memory. Determining optimal sizes is non-trivial: existing
methods rely on restrictive assumptions, conservative over-allocation, or slow
RTL simulations. We emphasize that runtime-based analyses (i.e., simulation)
are the only reliable way to ensure deadlock-free FIFO optimization for
data-dependent designs.
  We present FIFOAdvisor, a framework that automatically determines FIFO sizes
in HLS designs. It leverages LightningSim, a 99.9\% cycle-accurate simulator
supporting millisecond-scale incremental runs with new FIFO configurations.
FIFO sizing is formulated as a dual-objective black-box optimization problem,
and we explore heuristic and search-based methods to characterize the
latency-resource trade-off. FIFOAdvisor also integrates with Stream-HLS, a
framework for optimizing affine dataflow designs lowered from C++, MLIR, or
PyTorch, enabling deeper optimization of FIFOs in these workloads.
  We evaluate FIFOAdvisor on Stream-HLS design benchmarks spanning linear
algebra and deep learning workloads. Our results reveal Pareto-optimal
latency-memory frontiers across optimization strategies. Compared to baseline
designs, FIFOAdvisor achieves much lower memory usage with minimal delay
overhead. Additionally, it delivers significant runtime speedups over
traditional HLS/RTL co-simulation, making it practical for rapid design space
exploration. We further demonstrate its capability on a complex accelerator
with data-dependent control flow.
  Code and results: https://github.com/sharc-lab/fifo-advisor

</details>


### [2] [Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs](https://arxiv.org/abs/2510.21533)
*Misaki Kida,Shimpei Sato*

Main category: cs.AR

TL;DR: 提出了一种针对AMD Xilinx 7系列FPGA的硬件高效4位乘法器设计，仅使用11个LUT和2个CARRY4模块，相比之前的12-LUT设计减少了资源使用并缩短了关键路径延迟。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和边缘推理的普及，需要在LUT乘法器中同时优化面积和延迟，以并行实现大量低比特位运算。

Method: 通过重新组织映射到LUT的逻辑函数，减少LUT数量并优化关键路径，设计仅使用11个LUT和两个CARRY4模块的4位乘法器。

Result: 评估确认该电路实现了最小资源使用和2.750 ns的关键路径延迟，相比之前的12-LUT设计减少了LUT数量。

Conclusion: 提出的4位乘法器设计在AMD Xilinx 7系列FPGA上实现了硬件效率和性能的优化，适用于物联网和边缘推理应用。

Abstract: As IoT and edge inference proliferate,there is a growing need to
simultaneously optimize area and delay in lookup-table (LUT)-based multipliers
that implement large numbers of low-bitwidth operations in parallel. This paper
proposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx
7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the
logic functions mapped to the LUTs, the proposed method reduces the LUT count
by one compared with the prior 12-LUT design while also shortening the critical
path. Evaluation confirms that the circuit attains minimal resource usage and a
critical-path delay of 2.750 ns.

</details>


### [3] [Accelerating Electrostatics-based Global Placement with Enhanced FFT Computation](https://arxiv.org/abs/2510.21547)
*Hangyu Zhang,Sachin S. Sapatnekar*

Main category: cs.AR

TL;DR: 使用AccFFT加速技术进行电场计算，显著降低了全局布局算法的运行时间，在标准基准测试中实现了5.78倍的FFT计算加速和32%的总运行时间改进。


<details>
  <summary>Details</summary>
Motivation: 现代复杂VLSI设计需要高质量和高效的电路布局，静电分析布局方法虽然提升了可扩展性和解决方案质量，但运行时间仍有优化空间。

Method: 采用加速FFT技术（AccFFT）进行电场计算，并将其整合到ePlace-MS和Pplace-MS算法中。

Result: 实验结果显示FFT计算速度提升5.78倍，总运行时间改善32%，详细布局后缩放半周线长仅减少1.0%。

Conclusion: AccFFT技术能显著提升静电分析布局算法的性能，为复杂VLSI设计提供更高效的全局布局解决方案。

Abstract: Global placement is essential for high-quality and efficient circuit
placement for complex modern VLSI designs. Recent advancements, such as
electrostatics-based analytic placement, have improved scalability and solution
quality. This work demonstrates that using an accelerated FFT technique,
AccFFT, for electric field computation significantly reduces runtime.
Experimental results on standard benchmarks show significant improvements when
incorporated into the ePlace-MS and Pplace-MS algorithms, e.g., a 5.78x speedup
in FFT computation and a 32% total runtime improvement against ePlace-MS, with
1.0% reduction of scaled half-perimeter wirelength after detailed placement.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [Fuzzy numbers revisited: operations on extensional fuzzy numbers](https://arxiv.org/abs/2510.20861)
*Krzysztof Siminski*

Main category: cs.AI

TL;DR: 本文提出了一种新的模糊数表示方法——扩展模糊数，以解决传统模糊数运算中的计算复杂性和结果特征不一致问题，并定义了相应的运算和关系运算符。


<details>
  <summary>Details</summary>
Motivation: 传统模糊数运算存在三个主要问题：(1)计算复杂度高；(2)某些运算结果不具备原始模糊集的特征；(3)模糊扩散问题。这些问题限制了模糊数的应用范围。

Method: 采用扩展模糊数作为新的表示方法，定义了基于扩展模糊数的运算操作和关系运算符（=、>、>=、<、<=），并通过应用实例进行验证。

Result: 提出的方法能够有效解决传统模糊数运算中的问题，提供了C++实现并开源在GitHub仓库中。

Conclusion: 扩展模糊数方法为模糊数运算提供了更高效和实用的解决方案，有望扩展模糊数在更多领域的应用。

Abstract: Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to
better represent imprecise data. However, operations on fuzzy numbers are not
as straightforward as maths on crisp numbers. Commonly, the Zadeh's extension
rule is applied to elaborate a result. This can produce two problems: (1) high
computational complexity and (2) for some fuzzy sets and some operations the
results is not a fuzzy set with the same features (eg. multiplication of two
triangular fuzzy sets does not produce a triangular fuzzy set). One more
problem is the fuzzy spread -- fuzziness of the result increases with the
number of operations. These facts can severely limit the application field of
fuzzy numbers. In this paper we would like to revisite this problem with a
different kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines
operations on extensional fuzzy numbers and relational operators (=, >, >=, <,
<=) for them. The proposed approach is illustrated with several applicational
examples. The C++ implementation is available from a public GitHub repository.

</details>


### [5] [Epistemic Deference to AI](https://arxiv.org/abs/2510.21043)
*Benjamin Lange*

Main category: cs.AI

TL;DR: 本文探讨何时应该优先采纳AI输出而非人类专家判断，提出了AI优先主义观点及其替代方案——全证据视角的AI遵从理论。


<details>
  <summary>Details</summary>
Motivation: 基于社会认识论研究，探讨AI系统作为人工认识权威的资格问题，以及AI输出与人类独立认识理由之间的关系。

Method: 分析AI优先主义观点及其经典反对意见，发展全证据视角的AI遵从理论，强调AI输出应作为贡献性理由而非完全替代人类认识考量。

Result: 全证据视角具有三个关键优势：防止专业知识萎缩、为有意义的人类监督提供认识论依据、解释AI可靠性条件未满足时的合理不信任。

Conclusion: 全证据视角提供了一种原则性方法来确定AI遵从何时是合理的，特别适用于需要严格可靠性的高风险情境。

Abstract: When should we defer to AI outputs over human expert judgment? Drawing on
recent work in social epistemology, I motivate the idea that some AI systems
qualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated
reliability and epistemic superiority. I then introduce AI Preemptionism, the
view that AEA outputs should replace rather than supplement a user's
independent epistemic reasons. I show that classic objections to preemptionism
- such as uncritical deference, epistemic entrenchment, and unhinging epistemic
bases - apply in amplified form to AEAs, given their opacity, self-reinforcing
authority, and lack of epistemic failure markers. Against this, I develop a
more promising alternative: a total evidence view of AI deference. According to
this view, AEA outputs should function as contributory reasons rather than
outright replacements for a user's independent epistemic considerations. This
approach has three key advantages: (i) it mitigates expertise atrophy by
keeping human users engaged, (ii) it provides an epistemic case for meaningful
human oversight and control, and (iii) it explains the justified mistrust of AI
when reliability conditions are unmet. While demanding in practice, this
account offers a principled way to determine when AI deference is justified,
particularly in high-stakes contexts requiring rigorous reliability.

</details>


### [6] [From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL](https://arxiv.org/abs/2510.21045)
*Ali Khosravi Kazazi,Zhenlong Li,M. Naser Lessani,Guido Cervone*

Main category: cs.AI

TL;DR: 提出了一种多智能体框架，将自然语言问题准确翻译为空间SQL查询，通过专业分工和验证机制显著提升空间查询的准确性和语义对齐度。


<details>
  <summary>Details</summary>
Motivation: SQL和PostGIS等地理空间工具的复杂性阻碍了非专家进行空间数据分析，现有单智能体方法在处理空间查询的语义和语法复杂性方面存在困难。

Method: 采用多智能体框架，包括知识库、上下文检索嵌入和协作管道，由专门智能体负责实体提取、元数据检索、查询逻辑制定、SQL生成和程序化语义验证。

Result: 在KaggleDBQA基准测试中达到81.2%准确率，在空间查询基准测试中达到87.7%准确率（相比无验证智能体的76.7%），且生成的查询在语义上更符合用户意图。

Conclusion: 该工作使空间分析更易用，为空间Text-to-SQL系统提供了稳健、可推广的基础，推动了自主GIS的发展。

Abstract: The complexity of Structured Query Language (SQL) and the specialized nature
of geospatial functions in tools like PostGIS present significant barriers to
non-experts seeking to analyze spatial data. While Large Language Models (LLMs)
offer promise for translating natural language into SQL (Text-to-SQL),
single-agent approaches often struggle with the semantic and syntactic
complexities of spatial queries. To address this, we propose a multi-agent
framework designed to accurately translate natural language questions into
spatial SQL queries. The framework integrates several innovative components,
including a knowledge base with programmatic schema profiling and semantic
enrichment, embeddings for context retrieval, and a collaborative multi-agent
pipeline as its core. This pipeline comprises specialized agents for entity
extraction, metadata retrieval, query logic formulation, SQL generation, and a
review agent that performs programmatic and semantic validation of the
generated SQL to ensure correctness (self-verification). We evaluate our system
using both the non-spatial KaggleDBQA benchmark and a new, comprehensive
SpatialQueryQA benchmark that includes diverse geometry types, predicates, and
three levels of query complexity. On KaggleDBQA, the system achieved an overall
accuracy of 81.2% (221 out of 272 questions) after the review agent's review
and corrections. For spatial queries, the system achieved an overall accuracy
of 87.7% (79 out of 90 questions), compared with 76.7% without the review
agent. Beyond accuracy, results also show that in some instances the system
generates queries that are more semantically aligned with user intent than
those in the benchmarks. This work makes spatial analysis more accessible, and
provides a robust, generalizable foundation for spatial Text-to-SQL systems,
advancing the development of autonomous GIS.

</details>


### [7] [MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning](https://arxiv.org/abs/2510.21093)
*Siyong Chen,Jinbo Wen,Jiawen Kang,Tenghui Huang,Xumin Huang,Yuanjia Su,Hudan Pan,Zishao Zhong,Dusit Niyato,Shengli Xie,Dong In Kim*

Main category: cs.AI

TL;DR: MedAlign是一个解决医疗视觉问答中大型视觉语言模型幻觉、固定深度推理效率低和多机构协作困难的新框架，通过多模态直接偏好优化、检索感知专家混合架构和联邦治理机制实现高性能医疗问答。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医疗领域部署面临三个关键挑战：产生无视觉依据的幻觉答案、固定深度推理效率低下、多机构协作困难。

Method: 提出多模态直接偏好优化(mDPO)目标，设计检索感知专家混合(RA-MoE)架构，采用联邦治理机制实现自适应推理和多机构协作。

Result: 在三个代表性Med-VQA数据集上达到最先进性能，比强检索增强基线F1分数提升11.85%，平均推理长度比固定深度CoT方法减少51.60%。

Conclusion: MedAlign框架有效解决了医疗视觉问答中LVLM的部署挑战，实现了高精度、高效率和多机构协作的医疗AI服务。

Abstract: Recently, large models have shown significant potential for smart healthcare.
However, the deployment of Large Vision-Language Models (LVLMs) for clinical
services is currently hindered by three critical challenges: a tendency to
hallucinate answers not grounded in visual evidence, the inefficiency of
fixed-depth reasoning, and the difficulty of multi-institutional collaboration.
To address these challenges, in this paper, we develop MedAlign, a novel
framework to ensure visually accurate LVLM responses for Medical Visual
Question Answering (Med-VQA). Specifically, we first propose a multimodal
Direct Preference Optimization (mDPO) objective to explicitly align preference
learning with visual context. We then design a Retrieval-Aware
Mixture-of-Experts (RA-MoE) architecture that utilizes image and text
similarity to route queries to a specialized and context-augmented LVLM (i.e.,
an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive
reasoning and facilitate multi-institutional collaboration, we propose a
federated governance mechanism, where the selected expert, fine-tuned on
clinical datasets based on mDPO, locally performs iterative Chain-of-Thought
(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive
experiments on three representative Med-VQA datasets demonstrate that MedAlign
achieves state-of-the-art performance, outperforming strong retrieval-augmented
baselines by up to $11.85\%$ in F1-score, and simultaneously reducing the
average reasoning length by $51.60\%$ compared with fixed-depth CoT approaches.

</details>


### [8] [NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](https://arxiv.org/abs/2510.21144)
*Hanyu Zhu,Lance Fiondella,Jiawei Yuan,Kai Zeng,Long Jiao*

Main category: cs.AI

TL;DR: 本文提出NeuroGenPoisoning攻击框架，通过LLM内部神经元归因和遗传优化生成对抗性外部知识，在RAG系统中实现高效知识投毒，解决知识冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG投毒攻击主要关注检索内容或提示结构操作，忽略了模型内部表示动态和神经元级敏感性，且未充分考虑与强参数化知识的知识冲突机制。

Method: 首先识别与上下文投毒知识强相关的毒物响应神经元，然后使用遗传算法进化对抗性段落以最大化激活这些神经元，并通过观察到的归因信号重用有潜力但初始不成功的外部知识变体。

Result: 跨模型和数据集的实验结果显示，该方法持续实现超过90%的高群体覆盖成功率，同时保持流畅性。

Conclusion: NeuroGenPoisoning框架能够大规模生成有效的RAG投毒知识，并通过毒物响应神经元引导的投毒有效解决知识冲突问题。

Abstract: Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to
dynamically integrate external knowledge during inference, improving their
factual accuracy and adaptability. However, adversaries can inject poisoned
external knowledge to override the model's internal memory. While existing
attacks iteratively manipulate retrieval content or prompt structure of RAG,
they largely ignore the model's internal representation dynamics and
neuron-level sensitivities. The underlying mechanism of RAG poisoning has not
been fully studied and the effect of knowledge conflict with strong parametric
knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,
a novel attack framework that generates adversarial external knowledge in RAG
guided by LLM internal neuron attribution and genetic optimization. Our method
first identifies a set of Poison-Responsive Neurons whose activation strongly
correlates with contextual poisoning knowledge. We then employ a genetic
algorithm to evolve adversarial passages that maximally activate these neurons.
Crucially, our framework enables massive-scale generation of effective poisoned
RAG knowledge by identifying and reusing promising but initially unsuccessful
external knowledge variants via observed attribution signals. At the same time,
Poison-Responsive Neurons guided poisoning can effectively resolves knowledge
conflict. Experimental results across models and datasets demonstrate
consistently achieving high Population Overwrite Success Rate (POSR) of over
90% while preserving fluency. Empirical evidence shows that our method
effectively resolves knowledge conflict.

</details>


### [9] [How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation](https://arxiv.org/abs/2510.21148)
*Yang Zhao,Pu Wang,Hao Frank Yang*

Main category: cs.AI

TL;DR: EGO-Prompt是一个自动化的提示优化框架，通过进化图优化方法改进LLM的提示设计和推理过程，在真实世界任务中显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 在领域特定任务中为大型语言模型设计最优提示和推理过程既必要又具挑战性，需要解决如何整合领域知识、提升推理效率以及为领域专家提供知识整合指导等关键问题。

Method: 提出EGO-Prompt框架，从专家构建的初始语义因果图开始，通过因果引导的文本梯度过程自动优化：首先生成确定性推理指导，然后让LLM适应性地利用指导与原始输入。采用迭代优化算法使用真实标签的文本梯度来精炼语义因果图和推理机制。

Result: 在公共卫生、交通和人类行为等真实世界任务中测试，EGO-Prompt比最先进方法F1分数提高7.32%-12.61%，使小模型以不到20%的成本达到大模型性能，并输出精炼的领域特定语义因果图提升可解释性。

Conclusion: EGO-Prompt成功解决了领域特定提示设计的挑战，通过自动化优化过程显著提升了LLM性能，同时降低了计算成本并增强了模型的可解释性。

Abstract: Designing optimal prompts and reasoning processes for large language models
(LLMs) on domain-specific tasks is both necessary and challenging in real-world
applications. Determining how to integrate domain knowledge, enhance reasoning
efficiency, and even provide domain experts with refined knowledge integration
hints are particularly crucial yet unresolved tasks. In this research, we
propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an
automated framework to designing better prompts, efficient reasoning processes
and providing enhanced causal-informed process. EGO-Prompt begins with a
general prompt and fault-tolerant initial Semantic Causal Graph (SCG)
descriptions, constructed by human experts, which is then automatically refined
and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may
be partial or imperfect and that their optimal integration varies across LLMs,
EGO-Prompt integrates a novel causal-guided textual gradient process in two
steps: first, generating nearly deterministic reasoning guidance from the SCG
for each instance, and second, adapting the LLM to effectively utilize the
guidance alongside the original input. The iterative optimization algorithm
further refines both the SCG and the reasoning mechanism using textual
gradients with ground-truth. We tested the framework on real-world public
health, transportation and human behavior tasks. EGO-Prompt achieves
7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to
reach the performence of larger models at under 20% of the original cost. It
also outputs a refined, domain-specific SCG that improves interpretability.

</details>


### [10] [String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation](https://arxiv.org/abs/2510.21150)
*Kou Misaki,Takuya Akiba*

Main category: cs.AI

TL;DR: SSoT是一种新颖的提示方法，通过让LLM先生成随机字符串来增加熵，然后从中提取随机性来改善概率指令跟随性能，解决LLM在需要非确定性行为任务中的偏见问题。


<details>
  <summary>Details</summary>
Motivation: LLM在需要从预定义选项中选择答案并符合特定概率分布的任务中表现不佳，存在偏见问题，影响人类行为模拟、内容多样化和多人游戏等应用，也限制了生成响应的多样性。

Method: 提出String Seed of Thought (SSoT)方法，指导LLM先生成一个随机字符串产生足够熵，然后通过操作该字符串提取随机性来推导最终答案，在保持多样性的同时遵守特定约束。

Result: SSoT显著提高了LLM的概率指令跟随性能，接近伪随机数生成器的理想表现。在NoveltyBench上的实验表明，SSoT还能增强开放任务的响应多样性。

Conclusion: SSoT是一种简单有效的提示方法，能够改善LLM在概率指令跟随任务中的表现，同时增强生成内容的多样性，适用于需要非确定性行为的各种应用场景。

Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs
that improves Probabilistic Instruction Following (PIF). We define PIF as a
task requiring an LLM to select its answer from a predefined set of options,
each associated with a specific probability, such that the empirical
distribution of the generated answers aligns with the target distribution when
prompted multiple times. While LLMs excel at tasks with single, deterministic
answers, they often fail at PIF, exhibiting biases problematic for applications
requiring non-deterministic behaviors, such as human-behavior simulation,
content diversification, and multiplayer games. It also harms the diversity of
generated responses, a crucial factor in test-time scaling, by causing the
outputs to collapse into a limited set of answers. To address this, we propose
SSoT, a simple prompting method that instructs an LLM to first output a random
string to generate sufficient entropy. SSoT also instructs the LLM to extract
randomness by manipulating this string to derive a final answer, thereby
preserving diversity while adhering to specific constraints. We demonstrate
that SSoT significantly improves the PIF performance of LLMs, approaching the
ideal performance of a pseudo-random number generator. Furthermore, our
experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks
to open-ended tasks by enhancing response diversity.

</details>


### [11] [Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models](https://arxiv.org/abs/2510.21175)
*Yujin Jo,Taesup Kim*

Main category: cs.AI

TL;DR: NuSA-CL是一个轻量级、无需内存的持续学习框架，通过低秩适应和约束任务特定权重更新到模型参数的近似零空间，来保护预训练视觉语言模型的零样本能力，同时实现持续学习。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型（如CLIP）在现实部署中面临分布变化和新任务，静态零样本能力不足，需要持续学习方法在适应新任务的同时避免灾难性遗忘。

Method: 采用低秩适应技术，将任务特定的权重更新约束在模型当前参数的近似零空间内，从而最小化对已学习知识的干扰。

Result: 实验表明，该框架不仅有效保护了零样本迁移能力，还在持续学习基准测试中取得了有竞争力的性能。

Conclusion: NuSA-CL为现实应用中持续演化的零样本视觉语言模型提供了一个实用且可扩展的解决方案。

Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
remarkable zero-shot generalization, enabling deployment in a wide range of
real-world tasks without additional task-specific training. However, in real
deployment scenarios with evolving environments or emerging classes, these
models inevitably face distributional shifts and novel tasks. In such contexts,
static zero-shot capabilities are insufficient, and there is a growing need for
continual learning methods that allow models to adapt over time while avoiding
catastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for
Continual Learning), a lightweight memory-free continual learning framework
designed to address this challenge. NuSA-CL employs low-rank adaptation and
constrains task-specific weight updates to lie within an approximate null space
of the model's current parameters. This strategy minimizes interference with
previously acquired knowledge, effectively preserving the zero-shot
capabilities of the original model. Unlike methods relying on replay buffers or
costly distillation, NuSA-CL imposes minimal computational and memory overhead,
making it practical for deployment in resource-constrained, real-world
continual learning environments. Experiments show that our framework not only
effectively preserves zero-shot transfer capabilities but also achieves highly
competitive performance on continual learning benchmarks. These results
position NuSA-CL as a practical and scalable solution for continually evolving
zero-shot VLMs in real-world applications.

</details>


### [12] [OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series](https://arxiv.org/abs/2510.21244)
*Pengyu Xu,Shijia Li,Ao Sun,Feng Zhang,Yahan Li,Bo Wu,Zhanyu Ma,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Rui Wang,Yang Liu,Xiaobo Hu,Fan Yang,Jia Zheng,Guanghua Yao*

Main category: cs.AI

TL;DR: 提出了OutboundEval基准测试，用于评估大语言模型在专家级智能外呼场景中的表现，解决了现有方法在数据集多样性、用户模拟真实性和评估指标准确性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在三个关键限制：数据集多样性和类别覆盖不足、用户模拟不真实、评估指标不准确，需要开发更全面的基准测试来评估LLM在专业外呼场景中的表现。

Method: 1) 设计涵盖6大业务领域和30个子场景的结构化基准测试；2) 开发大模型驱动的用户模拟器，生成多样化、角色丰富的虚拟用户；3) 引入动态评估方法，结合自动化和人工评估，测量任务执行准确性、专业知识应用、适应性和用户体验质量。

Result: 在12个最先进的LLM上进行的实验揭示了专家级任务完成度与交互流畅性之间的权衡，为构建可靠、类人的外呼AI系统提供了实用见解。

Conclusion: OutboundEval为专业应用中LLM的基准测试建立了实用、可扩展且面向领域的标准。

Abstract: We propose OutboundEval, a comprehensive benchmark for evaluating large
language models (LLMs) in expert-level intelligent outbound calling scenarios.
Unlike existing methods that suffer from three key limitations - insufficient
dataset diversity and category coverage, unrealistic user simulation, and
inaccurate evaluation metrics - OutboundEval addresses these issues through a
structured framework. First, we design a benchmark spanning six major business
domains and 30 representative sub-scenarios, each with scenario-specific
process decomposition, weighted scoring, and domain-adaptive metrics. Second,
we develop a large-model-driven User Simulator that generates diverse,
persona-rich virtual users with realistic behaviors, emotional variability, and
communication styles, providing a controlled yet authentic testing environment.
Third, we introduce a dynamic evaluation method that adapts to task variations,
integrating automated and human-in-the-loop assessment to measure task
execution accuracy, professional knowledge application, adaptability, and user
experience quality. Experiments on 12 state-of-the-art LLMs reveal distinct
trade-offs between expert-level task completion and interaction fluency,
offering practical insights for building reliable, human-like outbound AI
systems. OutboundEval establishes a practical, extensible, and domain-oriented
standard for benchmarking LLMs in professional applications.

</details>


### [13] [Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems](https://arxiv.org/abs/2510.21254)
*Victoria J. Hodge,Colin Paterson,Ibrahim Habli*

Main category: cs.AI

TL;DR: 本文综述了自主系统中OOD检测技术，分析了其在安全关键领域安全保证中的应用，探讨了OOD检测的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主系统能力的扩展，在安全关键领域需要严格证明其安全性，而OOD检测是应对系统生命周期中新颖和不确定情况的关键技术。

Method: 采用综合性文献综述方法，定义相关概念，分析OOD产生原因，识别ML开发生命周期中可用的OOD检测技术，并讨论其在安全保证论证中的应用。

Result: 识别了一系列可在ML开发生命周期中使用的OOD检测技术，提出了在生命周期中支持安全保证论证的应用建议，并指出了系统工程师在集成OOD检测时需要注意的注意事项。

Conclusion: OOD检测对自主系统的安全开发和运行至关重要，但仍面临挑战，需要进一步研究以支持跨领域应用的安全保证。

Abstract: The operational capabilities and application domains of AI-enabled autonomous
systems have expanded significantly in recent years due to advances in robotics
and machine learning (ML). Demonstrating the safety of autonomous systems
rigorously is critical for their responsible adoption but it is challenging as
it requires robust methodologies that can handle novel and uncertain situations
throughout the system lifecycle, including detecting out-of-distribution (OoD)
data. Thus, OOD detection is receiving increased attention from the research,
development and safety engineering communities. This comprehensive review
analyses OOD detection techniques within the context of safety assurance for
autonomous systems, in particular in safety-critical domains. We begin by
defining the relevant concepts, investigating what causes OOD and exploring the
factors which make the safety assurance of autonomous systems and OOD detection
challenging. Our review identifies a range of techniques which can be used
throughout the ML development lifecycle and we suggest areas within the
lifecycle in which they may be used to support safety assurance arguments. We
discuss a number of caveats that system and safety engineers must be aware of
when integrating OOD detection into system lifecycles. We conclude by outlining
the challenges and future work necessary for the safe development and operation
of autonomous systems across a range of domains and applications.

</details>


### [14] [Investigating Scale Independent UCT Exploration Factor Strategies](https://arxiv.org/abs/2510.21275)
*Robin Schmöcker,Christoph Schnell,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了一种自适应选择UCT探索常数λ的策略，使算法对游戏奖励尺度不敏感，推荐使用2σ方法（σ为搜索树中所有状态-动作对Q值的经验标准差），在多种任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: UCT算法对游戏奖励尺度敏感，在具有密集奖励的游戏中使用固定探索常数会导致性能问题，需要开发对奖励尺度不敏感的自适应λ选择策略。

Method: 评估了文献中提出的各种λ策略以及五种新策略，包括基于Q值经验标准差的方法。

Result: 新提出的2σ方法在广泛任务中表现最佳，无论是使用单一参数值还是优化所有可用参数时的峰值性能都优于现有方法。

Conclusion: 推荐使用2σ作为UCT探索常数，该方法对游戏奖励尺度不敏感且在多种任务中表现优异。

Abstract: The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the
reward scale of the game it is applied to. For zero-sum games with the sparse
rewards of $\{-1,0,1\}$ at the end of the game, this is not a problem, but many
games often feature dense rewards with hand-picked reward scales, causing a
node's Q-value to span different magnitudes across different games. In this
paper, we evaluate various strategies for adaptively choosing the UCT
exploration constant $\lambda$, called $\lambda$-strategies, that are agnostic
to the game's reward scale. These $\lambda$-strategies include those proposed
in the literature as well as five new strategies. Given our experimental
results, we recommend using one of our newly suggested $\lambda$-strategies,
which is to choose $\lambda$ as $2 \cdot \sigma$ where $\sigma$ is the
empirical standard deviation of all state-action pairs' Q-values of the search
tree. This method outperforms existing $\lambda$-strategies across a wide range
of tasks both in terms of a single parameter value and the peak performances
obtained by optimizing all available parameters.

</details>


### [15] [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285)
*Yingzhi Mao,Chunkang Zhang,Junxiang Wang,Xinyan Guan,Boxi Cao,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: 本文提出Chain-of-Guardrail (CoG)训练框架，通过重组或回溯不安全的推理步骤，在保持有效推理链的同时引导模型回到安全轨迹，解决了大型推理模型的安全性与推理能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但存在严重的安全风险，包括有害内容生成和越狱攻击。现有缓解策略依赖注入启发式安全信号，往往会抑制推理能力，无法解决安全-推理权衡问题。

Method: 分析了多种LRMs的推理轨迹，发现Self-Jailbreak现象，即模型会覆盖自身的风险评估并为响应不安全提示提供理由。基于此提出CoG训练框架，重组或回溯不安全推理步骤，引导模型回到安全轨迹。

Result: 在多个推理和安全基准测试上的广泛实验表明，CoG显著提高了当前LRMs的安全性，同时保持了相当的推理能力，明显优于先前存在严重安全-推理权衡的方法。

Conclusion: LRMs本身具备拒绝不安全查询的能力，但这种能力被削弱导致有害输出。CoG框架通过系统性地处理不安全推理步骤，有效解决了安全性与推理能力之间的冲突。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.

</details>


### [16] [Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles](https://arxiv.org/abs/2510.21293)
*Siddharth Mehrotra,Jin Huang,Xuelong Fu,Roel Dobbe,Clara I. Sánchez,Maarten de Rijke*

Main category: cs.AI

TL;DR: 对AIES和FAccT会议论文的综述分析显示，当前可信AI研究过度关注技术属性而忽视社会技术维度，需要结合技术严谨性与社会文化因素的跨学科方法。


<details>
  <summary>Details</summary>
Motivation: 当前可信AI研究主要采用技术中心方法，关注可靠性、鲁棒性和公平性等技术属性，而忽视了理解真实世界环境中AI可信度所必需的社会技术维度。

Method: 对AIES和FAccT会议论文集进行范围综述，系统分析可信度在不同研究领域中的定义、操作化和应用方式，重点关注概念化方法、测量方法、验证技术、应用领域和基础价值观。

Result: 在定义透明度、问责制和鲁棒性等技术属性方面取得显著进展，但存在关键差距。当前研究往往过度强调技术精度而牺牲社会和伦理考量，AI系统的社会技术性质较少被探索，可信度成为由有权定义者塑造的争议概念。

Conclusion: 结合技术严谨性与社会、文化和制度考量的跨学科方法对于推进可信AI至关重要。为AI伦理社区提出可操作措施，采用真正解决AI系统与社会复杂互动的整体框架，最终促进惠及所有利益相关者的负责任技术发展。

Abstract: Background: Trustworthy AI serves as a foundational pillar for two major AI
ethics conferences: AIES and FAccT. However, current research often adopts
techno-centric approaches, focusing primarily on technical attributes such as
reliability, robustness, and fairness, while overlooking the sociotechnical
dimensions critical to understanding AI trustworthiness in real-world contexts.
  Objectives: This scoping review aims to examine how the AIES and FAccT
communities conceptualize, measure, and validate AI trustworthiness,
identifying major gaps and opportunities for advancing a holistic understanding
of trustworthy AI systems.
  Methods: We conduct a scoping review of AIES and FAccT conference proceedings
to date, systematically analyzing how trustworthiness is defined,
operationalized, and applied across different research domains. Our analysis
focuses on conceptualization approaches, measurement methods, verification and
validation techniques, application areas, and underlying values.
  Results: While significant progress has been made in defining technical
attributes such as transparency, accountability, and robustness, our findings
reveal critical gaps. Current research often predominantly emphasizes technical
precision at the expense of social and ethical considerations. The
sociotechnical nature of AI systems remains less explored and trustworthiness
emerges as a contested concept shaped by those with the power to define it.
  Conclusions: An interdisciplinary approach combining technical rigor with
social, cultural, and institutional considerations is essential for advancing
trustworthy AI. We propose actionable measures for the AI ethics community to
adopt holistic frameworks that genuinely address the complex interplay between
AI systems and society, ultimately promoting responsible technological
development that benefits all stakeholders.

</details>


### [17] [Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning](https://arxiv.org/abs/2510.21302)
*Sanghyun Ahn,Wonje Choi,Junyong Lee,Jinwoo Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出了一种结合符号验证和交互验证的神经符号具身任务规划框架，通过在代码生成过程中主动与环境交互获取缺失观测，显著提高了动态和部分可观测环境中的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码即策略方法在动态或部分可观测环境中存在环境基础不足的问题，导致代码生成错误或不完整，影响任务成功率。

Method: 采用神经符号方法，在代码生成过程中加入显式符号验证和交互验证阶段，生成探索性代码主动与环境交互获取缺失观测，同时保持任务相关状态。

Result: 在RLBench和真实世界动态部分可观测场景中，任务成功率比Code-as-Policies基线提高46.2%，任务相关动作的可执行性达到86.8%以上。

Conclusion: 该框架通过增强生成代码的环境基础，显著提高了动态环境中任务规划的可靠性。

Abstract: Recent advances in large language models (LLMs) have enabled the automatic
generation of executable code for task planning and control in embodied agents
such as robots, demonstrating the potential of LLM-based embodied intelligence.
However, these LLM-based code-as-policies approaches often suffer from limited
environmental grounding, particularly in dynamic or partially observable
settings, leading to suboptimal task success rates due to incorrect or
incomplete code generation. In this work, we propose a neuro-symbolic embodied
task planning framework that incorporates explicit symbolic verification and
interactive validation processes during code generation. In the validation
phase, the framework generates exploratory code that actively interacts with
the environment to acquire missing observations while preserving task-relevant
states. This integrated process enhances the grounding of generated code,
resulting in improved task reliability and success rates in complex
environments. We evaluate our framework on RLBench and in real-world settings
across dynamic, partially observable scenarios. Experimental results
demonstrate that our framework improves task success rates by 46.2% over
Code-as-Policies baselines and attains over 86.8% executability of
task-relevant actions, thereby enhancing the reliability of task planning in
dynamic environments.

</details>


### [18] [CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation](https://arxiv.org/abs/2510.21324)
*Jinhui Lou,Yan Yang,Zhou Yu,Zhenqi Fu,Weidong Han,Qingming Huang,Jun Yu*

Main category: cs.AI

TL;DR: 提出CXRAgent，一个由导演协调的多阶段智能体，用于胸部X光片分析，通过工具调用验证、诊断规划和协作决策来提高诊断的适应性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的CXR分析模型难以适应新的诊断任务和复杂推理场景，现有智能体依赖单一诊断流程且缺乏工具可靠性评估机制。

Method: 采用导演协调的三阶段方法：工具调用与证据驱动验证器、基于任务需求和中间发现的诊断规划、专家团队协作决策与上下文记忆整合。

Result: 在各种CXR解释任务上的实验表明，CXRAgent表现出强大的性能，提供视觉证据，并能很好地泛化到不同复杂度的临床任务。

Conclusion: CXRAgent通过多阶段协作和证据验证机制，显著提升了CXR分析的适应性、可靠性和诊断准确性。

Abstract: Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety
of task-specific and foundation models have been developed for automatic CXR
interpretation. However, these models often struggle to adapt to new diagnostic
tasks and complex reasoning scenarios. Recently, LLM-based agent models have
emerged as a promising paradigm for CXR analysis, enhancing model's capability
through tool coordination, multi-step reasoning, and team collaboration, etc.
However, existing agents often rely on a single diagnostic pipeline and lack
mechanisms for assessing tools' reliability, limiting their adaptability and
credibility. To this end, we propose CXRAgent, a director-orchestrated,
multi-stage agent for CXR interpretation, where a central director coordinates
the following stages: (1) Tool Invocation: The agent strategically orchestrates
a set of CXR-analysis tools, with outputs normalized and verified by the
Evidence-driven Validator (EDV), which grounds diagnostic outputs with visual
evidence to support reliable downstream diagnosis; (2) Diagnostic Planning:
Guided by task requirements and intermediate findings, the agent formulates a
targeted diagnostic plan. It then assembles an expert team accordingly,
defining member roles and coordinating their interactions to enable adaptive
and collaborative reasoning; (3) Collaborative Decision-making: The agent
integrates insights from the expert team with accumulated contextual memories,
synthesizing them into an evidence-backed diagnostic conclusion. Experiments on
various CXR interpretation tasks show that CXRAgent delivers strong
performance, providing visual evidence and generalizes well to clinical tasks
of different complexity. Code and data are valuable at this
\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.

</details>


### [19] [AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving](https://arxiv.org/abs/2510.21436)
*Ankur Sinha,Shobhit Arora,Dhaval Pujara*

Main category: cs.AI

TL;DR: AutoOpt-11k是一个包含11,000多个手写和打印数学优化模型图像的数据集，配套开发了AutoOpt框架，通过深度学习模型识别数学表达式、生成PYOMO脚本并求解优化问题，实现了从图像到求解的自动化流程。


<details>
  <summary>Details</summary>
Motivation: 为了解决数学优化问题求解过程中需要人工建模和编程的繁琐过程，研究者希望开发一个能够直接从优化问题图像自动求解的端到端框架，减少人工干预。

Method: AutoOpt框架包含三个模块：M1使用深度学习模型进行数学表达式识别，将图像转换为LaTeX代码；M2使用微调的小型LLM将LaTeX代码转换为PYOMO脚本；M3使用双层优化分解方法求解PYOMO脚本描述的优化问题。

Result: 在AutoOpt-11k数据集上，M1模块在BLEU分数指标上优于ChatGPT、Gemini和Nougat；M3模块在复杂测试问题上比内点算法和遗传算法表现更好。

Conclusion: AutoOpt框架成功实现了从优化问题图像到求解的自动化流程，在数学表达式识别和复杂问题求解方面都表现出色，为优化问题的自动化求解提供了有效解决方案。

Abstract: This study presents AutoOpt-11k, a unique image dataset of over 11,000
handwritten and printed mathematical optimization models corresponding to
single-objective, multi-objective, multi-level, and stochastic optimization
problems exhibiting various types of complexities such as non-linearity,
non-convexity, non-differentiability, discontinuity, and high-dimensionality.
The labels consist of the LaTeX representation for all the images and modeling
language representation for a subset of images. The dataset is created by 25
experts following ethical data creation guidelines and verified in two-phases
to avoid errors. Further, we develop AutoOpt framework, a machine learning
based automated approach for solving optimization problems, where the user just
needs to provide an image of the formulation and AutoOpt solves it efficiently
without any further human intervention. AutoOpt framework consists of three
Modules: (i) M1 (Image_to_Text)- a deep learning model performs the
Mathematical Expression Recognition (MER) task to generate the LaTeX code
corresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-
a small-scale fine-tuned LLM generates the PYOMO script (optimization modeling
language) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization
based Decomposition (BOBD) method solves the optimization formulation described
in the PYOMO script. We use AutoOpt-11k dataset for training and testing of
deep learning models employed in AutoOpt. The deep learning model for MER task
(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method
(M3), which is a hybrid approach, yields better results on complex test
problems compared to common approaches, like interior-point algorithm and
genetic algorithm.

</details>


### [20] [Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP](https://arxiv.org/abs/2510.21453)
*Yuxin Pan,Zhiguang Cao,Chengyang Gu,Liu Liu,Peilin Zhao,Yize Chen,Fangzhen Lin*

Main category: cs.AI

TL;DR: 该论文提出了一种解决多任务车辆路径问题的新框架，通过分解状态空间和重用基础求解器来提升性能，避免了传统统一求解器忽视VRP变体组合结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经方法通常学习统一求解器同时处理多个约束，但未能充分利用VRP变体的组合结构，每个变体都可以从一组基础VRP变体推导而来。这种关键疏忽导致统一求解器错过了基础求解器的潜在优势。

Method: 提出状态可分解MDP框架，将状态空间表示为与基础VRP变体相关的基础状态空间的笛卡尔积。开发基于潜在空间的SDMDP扩展，结合最优基础策略和可学习混合函数，在潜在空间中实现策略重用。引入MoSES求解器，通过专门的LoRA专家实现基础策略，并通过自适应门控机制实现混合函数。

Result: 在多个VRP变体上进行的广泛实验表明，MoSES优于先前的方法。

Conclusion: 该框架通过感知VRP变体间的共享组件性质并主动重用基础求解器，同时缓解训练神经求解器的指数增长，为多任务车辆路径问题提供了有效的解决方案。

Abstract: Existing neural methods for multi-task vehicle routing problems (VRPs)
typically learn unified solvers to handle multiple constraints simultaneously.
However, they often underutilize the compositional structure of VRP variants,
each derivable from a common set of basis VRP variants. This critical oversight
causes unified solvers to miss out the potential benefits of basis solvers,
each specialized for a basis VRP variant. To overcome this limitation, we
propose a framework that enables unified solvers to perceive the
shared-component nature across VRP variants by proactively reusing basis
solvers, while mitigating the exponential growth of trained neural solvers.
Specifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates
VRPs by expressing the state space as the Cartesian product of basis state
spaces associated with basis VRP variants. More crucially, this formulation
inherently yields the optimal basis policy for each basis VRP variant.
Furthermore, a Latent Space-based SDMDP extension is developed by incorporating
both the optimal basis policies and a learnable mixture function to enable the
policy reuse in the latent space. Under mild assumptions, this extension
provably recovers the optimal unified policy of SDMDP through the mixture
function that computes the state embedding as a mapping from the basis state
embeddings generated by optimal basis policies. For practical implementation,
we introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes
basis policies through specialized Low-Rank Adaptation (LoRA) experts, and
implements the mixture function via an adaptive gating mechanism. Extensive
experiments conducted across VRP variants showcase the superiority of MoSES
over prior methods.

</details>


### [21] [Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts](https://arxiv.org/abs/2510.21557)
*Hongwei Zhang,Ji Lu,Shiqing Jiang,Chenxiang Zhu,Li Xie,Chen Zhong,Haoran Chen,Yurui Zhu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.AI

TL;DR: Co-Sight通过冲突感知元验证和可信推理结构化事实机制，解决了LLM智能体长程推理中的验证不足问题，在多个基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体长程推理失败的主要原因是中间推理验证不足，而非生成能力问题，需要建立可证伪和可审计的推理过程。

Method: 采用冲突感知元验证和可信推理结构化事实两个互补机制：CAMV将验证重构为冲突识别和针对性证伪，TRSF通过结构化事实模块持续组织、验证和同步证据。

Result: 在GAIA上达到84.4%准确率，Humanity's Last Exam上35.5%，Chinese-SimpleQA上93.8%，均达到最先进水平。

Conclusion: Co-Sight为LLM智能体提供了可扩展的可靠长程推理范式，结构化事实基础和冲突感知验证的协同作用是性能提升的关键。

Abstract: Long-horizon reasoning in LLM-based agents often fails not from generative
weakness but from insufficient verification of intermediate reasoning. Co-Sight
addresses this challenge by turning reasoning into a falsifiable and auditable
process through two complementary mechanisms: Conflict-Aware Meta-Verification
(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV
reformulates verification as conflict identification and targeted
falsification, allocating computation only to disagreement hotspots among
expert agents rather than to full reasoning chains. This bounds verification
cost to the number of inconsistencies and improves efficiency and reliability.
TRSF continuously organizes, validates, and synchronizes evidence across agents
through a structured facts module. By maintaining verified, traceable, and
auditable knowledge, it ensures that all reasoning is grounded in consistent,
source-verified information and supports transparent verification throughout
the reasoning process. Together, TRSF and CAMV form a closed verification loop,
where TRSF supplies structured facts and CAMV selectively falsifies or
reinforces them, yielding transparent and trustworthy reasoning. Empirically,
Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last
Exam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies
confirm that the synergy between structured factual grounding and
conflict-aware verification drives these improvements. Co-Sight thus offers a
scalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code
is available at
https://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.

</details>


### [22] [Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning](https://arxiv.org/abs/2510.21560)
*Yuxuan Yang,Hussein Sibai*

Main category: cs.AI

TL;DR: 该论文提出了一种使用模仿学习来训练神经控制屏障函数的方法，用于在难以明确定义故障状态的情况下保证自主系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 在关键领域运行的自主系统中，安全性是基本要求。传统基于优化的控制屏障函数合成计算昂贵，且故障状态集往往难以明确定义，而专家演示数据更容易获得。

Method: 使用模仿学习训练约束函数来分类系统状态为安全或不安全，然后利用该函数标注模拟轨迹数据来训练神经控制屏障函数。

Result: 在四个不同环境中进行实证评估，该方法优于现有基线方法，并且与使用真实安全标签训练的神经控制屏障函数性能相当。

Conclusion: 提出的方法能够有效处理故障状态难以明确定义的情况，通过模仿学习训练神经控制屏障函数，为自主系统提供安全保证。

Abstract: Safety is a fundamental requirement for autonomous systems operating in
critical domains. Control barrier functions (CBFs) have been used to design
safety filters that minimally alter nominal controls for such systems to
maintain their safety. Learning neural CBFs has been proposed as a data-driven
alternative for their computationally expensive optimization-based synthesis.
However, it is often the case that the failure set of states that should be
avoided is non-obvious or hard to specify formally, e.g., tailgating in
autonomous driving, while a set of expert demonstrations that achieve the task
and avoid the failure set is easier to generate. We use ICL to train a
constraint function that classifies the states of the system under
consideration to safe, i.e., belong to a controlled forward invariant set that
is disjoint from the unspecified failure set, and unsafe ones, i.e., belong to
the complement of that set. We then use that function to label a new set of
simulated trajectories to train our neural CBF. We empirically evaluate our
approach in four different environments, demonstrating that it outperforms
existing baselines and achieves comparable performance to a neural CBF trained
with the same data but annotated with ground-truth safety labels.

</details>


### [23] [CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning](https://arxiv.org/abs/2510.21656)
*Marta Contreiras Silva,Daniel Faria,Catia Pesquita*

Main category: cs.AI

TL;DR: CMOMgen是首个端到端的复杂多本体匹配策略，能够生成完整且语义合理的映射，无目标本体或实体数量限制，在生物医学任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 构建全面知识图谱需要多个本体来完整地将数据置于领域上下文中。简单成对匹配无法提供相关但不相交本体的完整语义集成，需要复杂多本体匹配来建立更细致的等价关系和来源追踪。

Method: 使用检索增强生成方法选择相关类组成映射，并过滤匹配参考映射作为示例来增强上下文学习。这是首个端到端的CMOM策略，无目标本体或实体数量限制。

Result: 在三个生物医学任务中，CMOMgen在类选择方面优于基线方法，F1分数至少达到63%，在两个任务中优于所有基线和消融版本，在第三个任务中排名第二。手动评估显示46%的非参考映射获得最高分。

Conclusion: CMOMgen能够构建语义合理的映射，在复杂多本体匹配任务中表现出色，验证了专用策略的有效性。

Abstract: Constructing comprehensive knowledge graphs requires the use of multiple
ontologies in order to fully contextualize data into a domain. Ontology
matching finds equivalences between concepts interconnecting ontologies and
creating a cohesive semantic layer. While the simple pairwise state of the art
is well established, simple equivalence mappings cannot provide full semantic
integration of related but disjoint ontologies. Complex multi-ontology matching
(CMOM) aligns one source entity to composite logical expressions of multiple
target entities, establishing more nuanced equivalences and provenance along
the ontological hierarchy.
  We present CMOMgen, the first end-to-end CMOM strategy that generates
complete and semantically sound mappings, without establishing any restrictions
on the number of target ontologies or entities. Retrieval-Augmented Generation
selects relevant classes to compose the mapping and filters matching reference
mappings to serve as examples, enhancing In-Context Learning. The strategy was
evaluated in three biomedical tasks with partial reference alignments. CMOMgen
outperforms baselines in class selection, demonstrating the impact of having a
dedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,
outperforming all baselines and ablated versions in two out of three tasks and
placing second in the third. Furthermore, a manual evaluation of non-reference
mappings showed that 46% of the mappings achieve the maximum score, further
substantiating its ability to construct semantically sound mappings.

</details>


### [24] [A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection](https://arxiv.org/abs/2510.21679)
*Gaku Morio,Harri Rowlands,Dominik Stammbach,Christopher D. Manning,Peter Henderson*

Main category: cs.AI

TL;DR: 本文介绍了一个用于评估视觉语言模型的多模态数据集，专门用于分析能源公司公关广告中的框架类型，旨在检测绿色洗白等不匹配现象。


<details>
  <summary>Details</summary>
Motivation: 企业公关活动存在言行不一的问题，特别是能源公司的绿色洗白现象。理解大规模框架及其变化有助于分析公关活动的目标和本质。

Method: 构建专家标注的视频广告数据集，包含13种框架类型，覆盖50多家公司和20个国家，专门用于视觉语言模型评估。

Result: 基线实验显示GPT-4.1在检测环境信息方面达到79% F1分数，但最佳模型在识别绿色创新框架方面仅达到46% F1分数。

Conclusion: 该数据集促进了能源领域战略沟通的多模态分析研究，同时揭示了视觉语言模型在处理隐含框架、不同长度视频和文化背景方面的挑战。

Abstract: Companies spend large amounts of money on public relations campaigns to
project a positive brand image. However, sometimes there is a mismatch between
what they say and what they do. Oil & gas companies, for example, are accused
of "greenwashing" with imagery of climate-friendly initiatives. Understanding
the framing, and changes in framing, at scale can help better understand the
goals and nature of public relations campaigns. To address this, we introduce a
benchmark dataset of expert-annotated video ads obtained from Facebook and
YouTube. The dataset provides annotations for 13 framing types for more than 50
companies or advocacy groups across 20 countries. Our dataset is especially
designed for the evaluation of vision-language models (VLMs), distinguishing it
from past text-only framing datasets. Baseline experiments show some promising
results, while leaving room for improvement for future work: GPT-4.1 can detect
environmental messages with 79% F1 score, while our best model only achieves
46% F1 score on identifying framing around green innovation. We also identify
challenges that VLMs must address, such as implicit framing, handling videos of
various lengths, or implicit cultural backgrounds. Our dataset contributes to
research in multimodal analysis of strategic communication in the energy
sector.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics](https://arxiv.org/abs/2510.20852)
*Safa Ben Atitallah,Maha Driss,Henda Ben Ghezela*

Main category: cs.CR

TL;DR: 本文提出了一种基于微服务架构的联邦学习解决方案，用于物联网边缘计算环境中的数据分析和恶意软件检测，在保护数据隐私的同时实现低延迟和高可靠性。


<details>
  <summary>Details</summary>
Motivation: 物联网数据分析和处理面临隐私安全、延迟和带宽拥堵等问题，需要一种既能保护数据隐私又能提供高效数据分析的分布式解决方案。

Method: 采用微服务架构将物联网应用构建为细粒度、松耦合的可重用实体，结合联邦学习技术提供智能微服务，实现边缘计算环境下的数据分析和恶意软件检测。

Result: 在MaleVis数据集（包含14,000多张RGB转换图像，涵盖25个恶意软件类别和1个良性类别）上的实验表明，该方法在检测和分类性能上达到99.24%的准确率，优于现有最先进方法。

Conclusion: 提出的基于微服务架构的联邦学习方法能够有效解决物联网环境下的数据隐私保护和高效数据分析问题，在恶意软件检测应用中表现出色，具有实际应用价值。

Abstract: The Internet of Things (IoT) has recently proliferated in both size and
complexity. Using multi-source and heterogeneous IoT data aids in providing
efficient data analytics for a variety of prevalent and crucial applications.
To address the privacy and security concerns raised by analyzing IoT data
locally or in the cloud, distributed data analytics techniques were proposed to
collect and analyze data in edge or fog devices. In this context, federated
learning has been recommended as an ideal distributed machine/deep
learning-based technique for edge/fog computing environments. Additionally, the
data analytics results are time-sensitive; they should be generated with
minimal latency and high reliability. As a result, reusing efficient
architectures validated through a high number of challenging test cases would
be advantageous. The work proposed here presents a solution using a
microservices-based architecture that allows an IoT application to be
structured as a collection of fine-grained, loosely coupled, and reusable
entities. The proposed solution uses the promising capabilities of federated
learning to provide intelligent microservices that ensure efficient, flexible,
and extensible data analytics. This solution aims to deliver cloud calculations
to the edge to reduce latency and bandwidth congestion while protecting the
privacy of exchanged data. The proposed approach was validated through an
IoT-malware detection and classification use case. MaleVis, a publicly
available dataset, was used in the experiments to analyze and validate the
proposed approach. This dataset included more than 14,000 RGB-converted images,
comprising 25 malware classes and one benign class. The results showed that our
proposed approach outperformed existing state-of-the-art methods in terms of
detection and classification performance, with a 99.24%.

</details>


### [26] [FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models](https://arxiv.org/abs/2510.20856)
*Jia Deng,Jin Li,Zhenhua Zhao,Shaowei Wang*

Main category: cs.CR

TL;DR: 提出FPT-Noise方法，通过动态特征调制器和特征感知阈值增强CLIP模型的对抗鲁棒性，无需昂贵微调即可在测试时防御对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型如CLIP在零样本任务中表现出色，但视觉模态极易受到对抗攻击。传统对抗训练方法需要大量重新训练且计算成本高昂。

Method: 1. 动态特征调制器生成图像特定和攻击自适应的噪声强度参数；2. 建立特征感知阈值区分干净图像和对抗图像；3. 集成场景感知调节和测试时变换集成技术。

Result: FPT-Noise显著优于现有测试时防御方法，在AutoAttack下将平均鲁棒准确率从0.07%提升至56.86%，同时在干净图像上性能损失很小（-1.1%）。

Conclusion: FPT-Noise提供了一种高效且有效的测试时防御方案，显著提升了CLIP模型的对抗鲁棒性，无需昂贵的重新训练过程。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot generalizability across diverse downstream tasks. However, recent
studies have revealed that VLMs, including CLIP, are highly vulnerable to
adversarial attacks, particularly on their visual modality. Traditional methods
for improving adversarial robustness, such as adversarial training, involve
extensive retraining and can be computationally expensive. In this paper, we
propose a new Test-Time defense: Feature Perception Threshold Counterattack
Noise (FPT-Noise), which enhances the adversarial robustness of CLIP without
costly fine-tuning. Our core contributions are threefold: First, we introduce a
Dynamic Feature Modulator that dynamically generate an image-specific and
attack-adaptive noise intensity parameter. Second, We reanalyzed the image
features of CLIP. When images are exposed to different levels of noise, clean
images and adversarial images exhibit distinct rates of feature change. We
established a feature perception threshold to distinguish clean images from
attacked ones. Finally, we integrate a Scene-Aware Regulation guided by a
stability threshold and leverage Test-Time Transformation Ensembling (TTE) to
further mitigate the impact of residual noise and enhance robustness.Extensive
experimentation has demonstrated that FPT-Noise significantly outperforms
existing Test-Time defense methods, boosting average robust accuracy from 0.07%
to 56.86% under AutoAttack while maintaining high performance on clean images
(-1.1%). The code will be made public following the publication of the study.
The code will be made public following the publication of the study.

</details>


### [27] [A new measure for dynamic leakage based on quantitative information flow](https://arxiv.org/abs/2510.20922)
*Luigi D. C. Soares,Mário S. Alvim,Natasha Fernandes*

Main category: cs.CR

TL;DR: 本文提出了动态信息泄漏的新定义，将攻击者对秘密的信念与衡量攻击成功率的基线分布解耦，并验证了该定义满足信息论公理，与静态视角兼容。


<details>
  <summary>Details</summary>
Motivation: 定量信息流(QIF)中，静态视角的理论已较成熟，但动态视角仍缺乏同等理论深度。本文旨在弥合这一差距，为系统监控和跟踪等应用提供更好的理论基础。

Method: 提出动态泄漏的新定义，将攻击者信念与基线分布分离；验证该定义满足非干扰性、单调性和数据处理不等式等公理；分析强公理版本不成立的条件；展示与静态视角的兼容性；在隐私保护数据发布攻击中应用验证。

Result: 新定义成功解耦了攻击者信念和基线分布；满足相关信息论公理；识别了强单调性和强数据处理不等式不成立的条件；与静态视角兼容；在隐私保护数据发布攻击中有效应用。

Conclusion: 本文为动态信息泄漏提供了坚实的理论基础，填补了QIF领域的重要空白，为系统监控和实时决策提供了更好的理论支持。

Abstract: Quantitative information flow (QIF) is concerned with assessing the leakage
of information in computational systems. In QIF there are two main perspectives
for the quantification of leakage. On one hand, the static perspective
considers all possible runs of the system in the computation of information
flow, and is usually employed when preemptively deciding whether or not to run
the system. On the other hand, the dynamic perspective considers only a
specific, concrete run of the system that has been realised, while ignoring all
other runs. The dynamic perspective is relevant for, e.g., system monitors and
trackers, especially when deciding whether to continue or to abort a particular
run based on how much leakage has occurred up to a certain point. Although the
static perspective of leakage is well-developed in the literature, the dynamic
perspective still lacks the same level of theoretical maturity. In this paper
we take steps towards bridging this gap with the following key contributions:
(i) we provide a novel definition of dynamic leakage that decouples the
adversary's belief about the secret value from a baseline distribution on
secrets against which the success of the attack is measured; (ii) we
demonstrate that our formalisation satisfies relevant information-theoretic
axioms, including non-interference and relaxed versions of monotonicity and the
data-processing inequality (DPI); (iii) we identify under what kind of analysis
strong versions of the axioms of monotonicity and the DPI might not hold, and
explain the implications of this (perhaps counter-intuitive) outcome; (iv) we
show that our definition of dynamic leakage is compatible with the
well-established static perspective; and (v) we exemplify the use of our
definition on the formalisation of attacks against privacy-preserving data
releases.

</details>


### [28] [An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing](https://arxiv.org/abs/2510.20932)
*Reza Ahmari,Ahmad Mohammadi,Vahid Hemmati,Mohammed Mynuddin,Mahmoud Nabil Mahmoud,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.CR

TL;DR: 本研究调查了城市空中交通（UAM）车辆中自主导航和着陆系统的漏洞，特别关注针对深度学习模型（如CNN）的特洛伊木马攻击。实验显示，在DroNet框架下，特洛伊木马攻击导致准确率从96.4%显著下降到73.3%。


<details>
  <summary>Details</summary>
Motivation: 随着城市空中交通系统的发展，确保其自主导航和着陆系统的安全性变得至关重要。特洛伊木马攻击可能在这些系统中嵌入隐蔽触发器，导致特定条件下的故障，而其他情况下表现正常，这构成了严重的安全威胁。

Method: 使用DroNet框架评估城市自主空中车辆（UAAVs）的脆弱性，收集自定义数据集并训练模型以模拟真实世界条件。开发了专门用于识别特洛伊木马感染模型的评估框架。

Result: 实验结果表明，特洛伊木马攻击显著降低了模型性能，准确率从96.4%（清洁数据）下降到73.3%（受特洛伊木马触发的数据）。

Conclusion: 这项研究揭示了特洛伊木马攻击对UAM系统构成的潜在安全风险，为未来增强UAM系统弹性的研究奠定了基础。

Abstract: This study investigates the vulnerabilities of autonomous navigation and
landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses
on Trojan attacks that target deep learning models, such as Convolutional
Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within
a model's training data. These triggers cause specific failures under certain
conditions, while the model continues to perform normally in other situations.
We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using
the DroNet framework. Our experiments showed a significant drop in accuracy,
from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To
conduct this study, we collected a custom dataset and trained models to
simulate real-world conditions. We also developed an evaluation framework
designed to identify Trojan-infected models. This work demonstrates the
potential security risks posed by Trojan attacks and lays the groundwork for
future research on enhancing the resilience of UAM systems.

</details>


### [29] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CR

TL;DR: 研究发现推理语言模型在数学或代码领域的良性推理训练后会出现自我越狱现象，模型会使用多种策略绕过自身的安全防护机制，包括引入良性假设来合理化有害请求。


<details>
  <summary>Details</summary>
Motivation: 揭示推理语言模型在良性推理训练后出现的意外安全对齐失效现象，即自我越狱行为，这对日益强大的RLMs的安全保障构成威胁。

Method: 通过分析多个开源RLMs（包括DeepSeek-R1-distilled、s1.1、Phi-4-mini-reasoning和Nemotron）的行为，研究自我越狱的机制，并探索通过在训练中加入少量安全推理数据来缓解此问题。

Result: 发现RLMs在良性推理训练后变得更加顺从，在自我越狱过程中模型在思维链中感知恶意请求的危害性降低，从而满足这些请求。加入少量安全推理数据可以有效保持RLMs的安全对齐。

Conclusion: 这是对自我越狱行为的首次系统性分析，为维护日益强大的推理语言模型的安全性提供了实用路径，强调在训练中纳入安全推理数据的重要性。

Abstract: We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [30] [Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004)
*Nguyen Linh Bao Nguyen,Alsharif Abuadbba,Kristen Moore,Tingming Wu*

Main category: cs.CR

TL;DR: 本文首次系统评估了FOICE音频深度伪造检测，发现现有检测器在标准及噪声条件下均失效，提出了针对性微调策略显著提升检测准确率，并揭示了在FOICE专门化与未见过合成管道鲁棒性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: FOICE技术能够从单张面部图像生成受害者声音，无需语音样本，可绕过行业标准认证系统，引发严重安全担忧。本文旨在研究现有音频深度伪造检测器能否可靠检测FOICE生成语音，以及微调是否能提升检测性能而不损害对其他语音生成器的鲁棒性。

Method: 系统评估FOICE检测性能，引入针对性微调策略捕捉FOICE特定伪影，评估微调后的泛化能力。

Result: 领先检测器在标准和噪声条件下均持续失效；针对性微调策略带来显著准确率提升；微调后在FOICE专门化与未见过合成管道鲁棒性之间存在权衡。

Conclusion: 研究揭示了当前防御系统的根本弱点，为下一代音频深度伪造检测的新架构和训练协议提供了动机。

Abstract: The rapid advancement of generative models has enabled the creation of
increasingly stealthy synthetic voices, commonly referred to as audio
deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly
alarming capability: generating a victim's voice from a single facial image,
without requiring any voice sample. By exploiting correlations between facial
and vocal features, FOICE produces synthetic voices realistic enough to bypass
industry-standard authentication systems, including WeChat Voiceprint and
Microsoft Azure. This raises serious security concerns, as facial images are
far easier for adversaries to obtain than voice samples, dramatically lowering
the barrier to large-scale attacks. In this work, we investigate two core
research questions: (RQ1) can state-of-the-art audio deepfake detectors
reliably detect FOICE-generated speech under clean and noisy conditions, and
(RQ2) whether fine-tuning these detectors on FOICE data improves detection
without overfitting, thereby preserving robustness to unseen voice generators
such as SpeechT5.
  Our study makes three contributions. First, we present the first systematic
evaluation of FOICE detection, showing that leading detectors consistently fail
under both standard and noisy conditions. Second, we introduce targeted
fine-tuning strategies that capture FOICE-specific artifacts, yielding
significant accuracy improvements. Third, we assess generalization after
fine-tuning, revealing trade-offs between specialization to FOICE and
robustness to unseen synthesis pipelines. These findings expose fundamental
weaknesses in today's defenses and motivate new architectures and training
protocols for next-generation audio deepfake detection.

</details>


### [31] [A Reinforcement Learning Framework for Robust and Secure LLM Watermarking](https://arxiv.org/abs/2510.21053)
*Li An,Yujian Liu,Yepeng Liu,Yuheng Bu,Yang Zhang,Shiyu Chang*

Main category: cs.CR

TL;DR: 本文提出了一种端到端的强化学习框架，用于优化大型语言模型的水印技术，通过锚定机制和正则化项解决多目标优化中的训练不稳定和奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印算法大多依赖启发式的绿/红令牌列表设计，直接使用强化学习优化面临多目标冲突导致训练不稳定，以及巨大动作空间易受奖励黑客攻击的挑战。

Method: 采用端到端强化学习框架，引入锚定机制确保训练稳定，并添加正则化项防止奖励黑客，优化绿/红令牌列表设计。

Result: 在两个骨干LLM的标准基准测试中，该方法在所有标准上实现了最先进的权衡，特别是在抵抗欺骗攻击方面有显著改进，且不降低其他标准。

Conclusion: 提出的强化学习框架能够有效优化LLM水印技术，在多目标权衡方面表现优异，特别是在安全性方面有显著提升。

Abstract: Watermarking has emerged as a promising solution for tracing and
authenticating text generated by large language models (LLMs). A common
approach to LLM watermarking is to construct a green/red token list and assign
higher or lower generation probabilities to the corresponding tokens,
respectively. However, most existing watermarking algorithms rely on heuristic
green/red token list designs, as directly optimizing the list design with
techniques such as reinforcement learning (RL) comes with several challenges.
First, desirable watermarking involves multiple criteria, i.e., detectability,
text quality, robustness against removal attacks, and security against spoofing
attacks. Directly optimizing for these criteria introduces many partially
conflicting reward terms, leading to an unstable convergence process. Second,
the vast action space of green/red token list choices is susceptible to reward
hacking. In this paper, we propose an end-to-end RL framework for robust and
secure LLM watermarking. Our approach adopts an anchoring mechanism for reward
terms to ensure stable training and introduces additional regularization terms
to prevent reward hacking. Experiments on standard benchmarks with two backbone
LLMs show that our method achieves a state-of-the-art trade-off across all
criteria, with notable improvements in resistance to spoofing attacks without
degrading other criteria. Our code is available at
https://github.com/UCSB-NLP-Chang/RL-watermark.

</details>


### [32] [QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute](https://arxiv.org/abs/2510.21124)
*Jie Zhang,Xiaohong Li,Mengke Zhang,Ruitao Feng,Shanshan Xu,Zhe Hou,Guangdong Bai*

Main category: cs.CR

TL;DR: 提出QAE-BAC框架，解决区块链属性访问控制中的隐私和效率双重挑战，通过量化匿名性模型和熵加权路径树优化，在Hyperledger Fabric上实现11倍吞吐量提升和87%延迟降低。


<details>
  <summary>Details</summary>
Motivation: 区块链属性访问控制面临两大挑战：区块链透明性导致用户隐私泄露风险，政策匹配的计算复杂度与区块链性能限制冲突。现有解决方案要么开销大，要么忽视隐私影响。

Method: 提出QAE-BAC框架，包含：(r,t)-匿名性模型动态量化重识别风险，熵加权路径树基于实时匿名性指标优化策略结构，降低策略匹配复杂度。

Result: 在Hyperledger Fabric上实现，实验结果显示能有效缓解重识别风险，性能优于现有基准方法，吞吐量提升11倍，延迟降低87%。

Conclusion: QAE-BAC在隐私敏感的去中心化应用中实现了隐私和性能的优越平衡，具有实际应用价值。

Abstract: Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a
decentralized paradigm for secure data governance but faces two inherent
challenges: the transparency of blockchain ledgers threatens user privacy by
enabling reidentification attacks through attribute analysis, while the
computational complexity of policy matching clashes with blockchain's
performance constraints. Existing solutions, such as those employing
Zero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable
anonymity guarantees, while efficiency optimizations frequently ignore privacy
implications. To address these dual challenges, this paper proposes QAEBAC
(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with
Attribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically
quantify the re-identification risk of users based on their access attributes
and history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that
optimizes policy structure based on realtime anonymity metrics, drastically
reducing policy matching complexity. Implemented and evaluated on Hyperledger
Fabric, QAE-BAC demonstrates a superior balance between privacy and
performance. Experimental results show that it effectively mitigates
re-identification risks and outperforms state-of-the-art baselines, achieving
up to an 11x improvement in throughput and an 87% reduction in latency, proving
its practicality for privacy-sensitive decentralized applications.

</details>


### [33] [Quantifying CBRN Risk in Frontier Models](https://arxiv.org/abs/2510.21133)
*Divyanshu Kumar,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 该论文对10个领先商业大语言模型在化学、生物、放射性和核武器知识方面的安全风险进行了首次全面评估，发现现有安全机制存在严重脆弱性，简单的提示工程技术就能绕过防护措施。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型存在前所未有的双重使用风险，可能传播CBRN武器知识，需要评估其安全防护机制的有效性。

Method: 使用包含200个提示的新CBRN数据集和FORTRESS基准的180个提示子集，采用严格的三层攻击方法学进行评估。

Result: 深度诱导攻击成功率高达86.0%，而直接请求仅为33.8%；模型安全性能差异巨大，攻击成功率从2%到96%不等；8个模型在增强危险材料属性方面的脆弱性超过70%。

Conclusion: 当前安全对齐机制存在根本性脆弱性，需要标准化评估框架、透明安全指标和更强大的对齐技术来减轻灾难性误用风险。

Abstract: Frontier Large Language Models (LLMs) pose unprecedented dual-use risks
through the potential proliferation of chemical, biological, radiological, and
nuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation
of 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and
a 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier
attack methodology. Our findings expose critical safety vulnerabilities: Deep
Inception attacks achieve 86.0\% success versus 33.8\% for direct requests,
demonstrating superficial filtering mechanisms; Model safety performance varies
dramatically from 2\% (claude-opus-4) to 96\% (mistral-small-latest) attack
success rates; and eight models exceed 70\% vulnerability when asked to enhance
dangerous material properties. We identify fundamental brittleness in current
safety alignment, where simple prompt engineering techniques bypass safeguards
for dangerous CBRN information. These results challenge industry safety claims
and highlight urgent needs for standardized evaluation frameworks, transparent
safety metrics, and more robust alignment techniques to mitigate catastrophic
misuse risks while preserving beneficial capabilities.

</details>


### [34] [Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](https://arxiv.org/abs/2510.21189)
*Yukun Jiang,Mingjie Li,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种基于任务并发的LLM越狱攻击方法JAIL-CON，通过在相邻单词中编码不同意图来实现并发任务执行，显著降低了有害内容被防护机制检测的概率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要遵循顺序逻辑，而并发作为顺序场景的自然扩展被忽视。研究发现将有害任务与良性任务结合能显著降低被防护机制过滤的概率，揭示了LLM中任务并发的潜在风险。

Method: 提出词级方法实现LLM中的任务并发，其中相邻单词编码不同意图。开发了JAIL-CON迭代攻击框架，通过任务并发进行越狱攻击。

Result: 实验表明JAIL-CON相比现有攻击具有更强的越狱能力。在应用防护机制作为防御时，JAIL-CON生成的并发答案比顺序答案更具隐蔽性，更难被防护机制检测。

Conclusion: 任务并发在LLM越狱中具有独特优势，JAIL-CON框架展示了并发攻击的强效性和隐蔽性，凸显了LLM中并发任务执行的安全风险。

Abstract: Despite their superior performance on a wide range of domains, large language
models (LLMs) remain vulnerable to misuse for generating harmful content, a
risk that has been further amplified by various jailbreak attacks. Existing
jailbreak attacks mainly follow sequential logic, where LLMs understand and
answer each given task one by one. However, concurrency, a natural extension of
the sequential scenario, has been largely overlooked. In this work, we first
propose a word-level method to enable task concurrency in LLMs, where adjacent
words encode divergent intents. Although LLMs maintain strong utility in
answering concurrent tasks, which is demonstrated by our evaluations on
mathematical and general question-answering benchmarks, we notably observe that
combining a harmful task with a benign one significantly reduces the
probability of it being filtered by the guardrail, showing the potential risks
associated with concurrency in LLMs. Based on these findings, we introduce
$\texttt{JAIL-CON}$, an iterative attack framework that
$\underline{\text{JAIL}}$breaks LLMs via task $\underline{\text{CON}}$currency.
Experiments on widely-used LLMs demonstrate the strong jailbreak capabilities
of $\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the
guardrail is applied as a defense, compared to the sequential answers generated
by previous attacks, the concurrent answers in our $\texttt{JAIL-CON}$ exhibit
greater stealthiness and are less detectable by the guardrail, highlighting the
unique feature of task concurrency in jailbreaking LLMs.

</details>


### [35] [The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning](https://arxiv.org/abs/2510.21190)
*Mingrui Liu,Sixiao Zhang,Cheng Long,Kwok Yan Lam*

Main category: cs.CR

TL;DR: TrojFill是一种黑盒越狱方法，通过将有害指令嵌入多部分模板中，让模型推理指令的不安全性并生成详细示例，从而绕过安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有越狱技术存在局限性：白盒方法需要模型内部信息不适用于闭源API，黑盒方法生成的模板缺乏可解释性和可迁移性。

Method: 将不安全指令通过占位符替换或编码方式嵌入模板，让模型进行不安全性推理并生成详细示例，利用示例组件作为特洛伊木马。

Result: 在主流LLMs上表现优异，Gemini-flash-2.5和DeepSeek-3.1攻击成功率100%，GPT-4o达97%，生成的提示具有更好的可解释性和可迁移性。

Conclusion: TrojFill提供了一种有效的黑盒越狱方法，在保持高攻击成功率的同时提高了提示的可解释性和可迁移性。

Abstract: Large Language Models (LLMs) have advanced rapidly and now encode extensive
world knowledge. Despite safety fine-tuning, however, they remain susceptible
to adversarial prompts that elicit harmful content. Existing jailbreak
techniques fall into two categories: white-box methods (e.g., gradient-based
approaches such as GCG), which require model internals and are infeasible for
closed-source APIs, and black-box methods that rely on attacker LLMs to search
or mutate prompts but often produce templates that lack explainability and
transferability. We introduce TrojFill, a black-box jailbreak that reframes
unsafe instruction as a template-filling task. TrojFill embeds obfuscated
harmful instructions (e.g., via placeholder substitution or Caesar/Base64
encoding) inside a multi-part template that asks the model to (1) reason why
the original instruction is unsafe (unsafety reasoning) and (2) generate a
detailed example of the requested text, followed by a sentence-by-sentence
analysis. The crucial "example" component acts as a Trojan Horse that contains
the target jailbreak content while the surrounding task framing reduces refusal
rates. We evaluate TrojFill on standard jailbreak benchmarks across leading
LLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical
performance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,
and 97% on GPT-4o). Moreover, the generated prompts exhibit improved
interpretability and transferability compared with prior black-box optimization
approaches. We release our code, sample prompts, and generated outputs to
support future red-teaming research.

</details>


### [36] [Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses](https://arxiv.org/abs/2510.21214)
*Xingwei Zhong,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: 本文提出了一种针对多模态大语言模型的黑盒越狱攻击方法，通过文本和图像提示来评估模型安全性，并设计了重新攻击策略和防御方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型因结合视觉和文本模态而面临新的安全威胁，特别是越狱攻击可能导致未经授权或有害的响应，需要评估和改进其安全性。

Method: 设计了包含挑衅指令的文本提示和具有突变、多图像能力的图像提示，采用黑盒越狱方法，并提出了重新攻击策略来加强评估。

Result: 实验结果表明，该方法能够有效评估开源和闭源多模态大语言模型的安全性，识别现有防御方法的不足，并验证了重新设计的防御方法对越狱攻击的防护效果。

Conclusion: 通过提出的越狱攻击和防御方法，能够更全面地评估和提升多模态大语言模型的安全性，为模型安全防护提供了新的策略和验证手段。

Abstract: Multimodal large language models (MLLMs) comprise of both visual and textual
modalities to process vision language tasks. However, MLLMs are vulnerable to
security-related issues, such as jailbreak attacks that alter the model's input
to induce unauthorized or harmful responses. The incorporation of the
additional visual modality introduces new dimensions to security threats. In
this paper, we proposed a black-box jailbreak method via both text and image
prompts to evaluate MLLMs. In particular, we designed text prompts with
provocative instructions, along with image prompts that introduced mutation and
multi-image capabilities. To strengthen the evaluation, we also designed a
Re-attack strategy. Empirical results show that our proposed work can improve
capabilities to assess the security of both open-source and closed-source
MLLMs. With that, we identified gaps in existing defense methods to propose new
strategies for both training-time and inference-time defense methods, and
evaluated them across the new jailbreak methods. The experiment results showed
that the re-designed defense methods improved protections against the jailbreak
attacks.

</details>


### [37] [What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions](https://arxiv.org/abs/2510.21246)
*Michael Külper,Jan-Niclas Hilgert,Frank Breitinger,Martin Lambertz*

Main category: cs.CR

TL;DR: 本文针对自托管云存储平台（如Nextcloud）的数字取证挑战，提出了一个扩展取证框架，通过设备监控和云API实现结构化、可重复的证据获取，并开发了开源采集工具。


<details>
  <summary>Details</summary>
Motivation: 自托管云存储平台日益流行，但给数字取证调查带来了新挑战，特别是在系统分析客户端和服务器组件方面。Nextcloud作为广泛使用的平台，在取证研究中关注有限，现有云存储取证框架存在局限性。

Method: 提出扩展取证框架，整合设备监控并利用云API进行结构化证据获取。以Nextcloud为案例研究，展示如何利用其原生API可靠访问取证工件，并开发开源采集工具实现该方法。

Result: 演示了Nextcloud原生API可用于可靠访问取证工件，开发的开源采集工具成功实现了所提出的取证方法。

Conclusion: 该框架为调查人员提供了更灵活的分析自托管云存储系统的方法，并为这一不断发展的数字取证领域提供了进一步发展的基础。

Abstract: Self-hosted cloud storage platforms like Nextcloud are gaining popularity
among individuals and organizations seeking greater control over their data.
However, this shift introduces new challenges for digital forensic
investigations, particularly in systematically analyzing both client and server
components. Despite Nextcloud's widespread use, it has received limited
attention in forensic research. In this work, we critically examine existing
cloud storage forensic frameworks and highlight their limitations. To address
the gaps, we propose an extended forensic framework that incorporates device
monitoring and leverages cloud APIs for structured, repeatable evidence
acquisition. Using Nextcloud as a case study, we demonstrate how its native
APIs can be used to reliably access forensic artifacts, and we introduce an
open-source acquisition tool that implements this approach. Our framework
equips investigators with a more flexible method for analyzing self-hosted
cloud storage systems, and offers a foundation for further development in this
evolving area of digital forensics.

</details>


### [38] [LLM-Powered Detection of Price Manipulation in DeFi](https://arxiv.org/abs/2510.21272)
*Lu Liu,Wuqi Zhang,Lili Wei,Hao Guan,Yongqiang Tian,Yepang Liu*

Main category: cs.CR

TL;DR: PMDetector是一个结合静态分析和LLM推理的混合框架，用于主动检测DeFi智能合约中的价格操纵漏洞，在真实数据集上达到88%精确率和90%召回率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DeFi智能合约管理数十亿美元资金，价格操纵漏洞（通常通过闪电贷）造成重大财务损失。现有检测方法有限：反应性方法仅在攻击发生后分析，而主动静态分析工具依赖僵化的预定义启发式规则，无法识别新型变体或理解复杂经济逻辑。

Method: 采用混合框架，结合静态分析和LLM推理。使用正式攻击模型和三阶段流水线：1）静态污点分析识别潜在漏洞代码路径；2）两阶段LLM过程通过分析防御措施过滤路径，然后模拟攻击评估可利用性；3）静态分析检查器验证LLM结果，仅保留高风险路径并生成详细漏洞报告。

Result: 在包含73个真实漏洞和288个良性DeFi协议的数据集上，PMDetector使用Gemini 2.5-flash达到88%精确率和90%召回率，显著优于最先进的静态分析和基于LLM的方法。使用GPT-4.1审计一个漏洞仅需0.03美元和4.0秒。

Conclusion: PMDetector提供了一种高效且经济实惠的替代手动审计的方法，能够主动检测价格操纵漏洞，在检测精度和成本效益方面优于现有技术。

Abstract: Decentralized Finance (DeFi) smart contracts manage billions of dollars,
making them a prime target for exploits. Price manipulation vulnerabilities,
often via flash loans, are a devastating class of attacks causing significant
financial losses. Existing detection methods are limited. Reactive approaches
analyze attacks only after they occur, while proactive static analysis tools
rely on rigid, predefined heuristics, limiting adaptability. Both depend on
known attack patterns, failing to identify novel variants or comprehend complex
economic logic. We propose PMDetector, a hybrid framework combining static
analysis with Large Language Model (LLM)-based reasoning to proactively detect
price manipulation vulnerabilities. Our approach uses a formal attack model and
a three-stage pipeline. First, static taint analysis identifies potentially
vulnerable code paths. Second, a two-stage LLM process filters paths by
analyzing defenses and then simulates attacks to evaluate exploitability.
Finally, a static analysis checker validates LLM results, retaining only
high-risk paths and generating comprehensive vulnerability reports. To evaluate
its effectiveness, we built a dataset of 73 real-world vulnerable and 288
benign DeFi protocols. Results show PMDetector achieves 88% precision and 90%
recall with Gemini 2.5-flash, significantly outperforming state-of-the-art
static analysis and LLM-based approaches. Auditing a vulnerability with
PMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an
efficient and cost-effective alternative to manual audits.

</details>


### [39] [The Qey: Implementation and performance study of post quantum cryptography in FIDO2](https://arxiv.org/abs/2510.21353)
*Aditya Mitra,Sibi Chakkaravarthy Sethuraman*

Main category: cs.CR

TL;DR: 本文探讨了将基于模块格的后量子密码签名算法ML-DSA（基于Crystals Dilithium）用于FIDO2认证标准的可行性，以应对量子计算机对现有经典密码算法的威胁。


<details>
  <summary>Details</summary>
Motivation: FIDO2标准目前使用ECDSA、RSA等经典密码签名算法，这些算法在面对大规模量子计算机时存在安全风险。随着量子计算的发展，需要为FIDO2认证系统提供抗量子攻击的密码方案。

Method: 研究采用基于模块格的数字签名算法ML-DSA（基于Crystals Dilithium），将其作为后量子密码签名标准应用于FIDO2认证协议中。

Result: 论文比较了ML-DSA与经典算法密钥在性能和安全性方面的表现，评估了后量子密码算法在FIDO2标准中的适用性。

Conclusion: 研究表明基于模块格的ML-DSA算法有望成为FIDO2标准的后量子密码签名方案，能够提供对抗量子计算机攻击的安全性保障。

Abstract: Authentication systems have evolved a lot since the 1960s when Fernando
Corbato first proposed the password-based authentication. In 2013, the FIDO
Alliance proposed using secure hardware for authentication, thus marking a
milestone in the passwordless authentication era [1]. Passwordless
authentication with a possession-based factor often relied on hardware-backed
cryptographic methods. FIDO2 being one an amalgamation of the W3C Web
Authentication and FIDO Alliance Client to Authenticator Protocol is an
industry standard for secure passwordless authentication with rising adoption
for the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256
(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature
algorithms. This makes it insecure against attacks involving large-scale
quantum computers [3]. This study aims at exploring the usability of Module
Lattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium
as a post quantum cryptographic signature standard for FIDO2. The paper
highlights the performance and security in comparison to keys with classical
algorithms.

</details>


### [40] [FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security](https://arxiv.org/abs/2510.21401)
*Mojtaba Eshghie,Gabriele Morello,Matteo Lauretano,Alexandre Bartel,Martin Monperrus*

Main category: cs.CR

TL;DR: FLAMES是一种自动化方法，使用领域适应的大语言模型生成Solidity智能合约的可执行运行时防护，无需漏洞标签或形式化规范，能有效防止真实攻击。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞每年造成数十亿美元损失，现有自动化分析工具无法生成可部署的防御措施。

Method: 使用领域适应的大语言模型，通过填空式监督微调在514,506个已验证合约的真实世界不变量上进行训练，生成Solidity 'require'语句作为运行时防护。

Result: 编译成功率96.7%；在5000个挑战性不变量测试集上，44.5%生成精确或语义等价匹配；防止108个真实攻击中的22个（20.4%）；成功阻止APEMAGA真实事件攻击。

Conclusion: 领域适应的LLM能够自动为智能合约生成生产就绪的安全防御，无需漏洞检测、形式化规范或人工干预。

Abstract: Smart contract vulnerabilities cost billions of dollars annually, yet
existing automated analysis tools fail to generate deployable defenses. We
present FLAMES, a novel automated approach that synthesizes executable runtime
guards as Solidity "require" statements to harden smart contracts against
exploits. Unlike prior work that relies on vulnerability labels, symbolic
analysis, or natural language specifications, FLAMES employs domain-adapted
large language models trained through fill-in-the-middle supervised fine-tuning
on real-world invariants extracted from 514,506 verified contracts. Our
extensive evaluation across three dimensions demonstrates FLAMES's
effectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for
synthesized invariant (2) Semantic Quality: on a curated test set of 5,000
challenging invariants, FLAMES produces exact or semantically equivalent
matches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES
prevents 22 out of 108 real exploits (20.4%) while preserving contract
functionality, and (4) FLAMES successfully blocks the real-world APEMAGA
incident by synthesizing a pre-condition that mitigates the attack. FLAMES
establishes that domain-adapted LLMs can automatically generate
production-ready security defenses for smart contracts without requiring
vulnerability detection, formal specifications, or human intervention. We
release our code, model weights, datasets, and evaluation infrastructure to
enable reproducible research in this critical domain.

</details>


### [41] [SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](https://arxiv.org/abs/2510.21459)
*Adetayo Adebimpe,Helmut Neukirchen,Thomas Welsh*

Main category: cs.CR

TL;DR: 提出了SBASH框架，使用本地轻量级LLM解决蜜罐的数据保护问题，评估了RAG和非RAG LLM在Linux shell命令处理中的性能，发现RAG能提高未调优模型的准确性，而通过系统提示调优的模型无需RAG也能达到相似准确性且延迟更低。


<details>
  <summary>Details</summary>
Motivation: 蜜罐需要最大化攻击者参与度，但现有基于LLM的方法存在准确性、响应时间、运营成本高和云部署数据保护问题。

Method: 提出SBASH框架，使用本地轻量级LLM管理数据保护问题，研究RAG支持和非RAG的LLM处理Linux shell命令，通过响应时间差异、人类测试者评估的现实性、以及使用Levenshtein距离、SBert和BertScore计算的与真实系统相似度等指标进行评估。

Result: RAG提高了未调优模型的准确性，而通过系统提示调优的模型无需RAG也能达到与未调优+RAG相似的准确性，且延迟略低。

Conclusion: SBASH框架通过本地轻量级LLM有效解决了数据保护问题，RAG和非RAG方法各有优势，系统提示调优可在不依赖RAG的情况下获得良好性能。

Abstract: Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.

</details>


### [42] [Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise](https://arxiv.org/abs/2510.21483)
*Pierre Guillot,Auguste Hoang Duc,Michel Koskas,Florian Méhats*

Main category: cs.CR

TL;DR: GRAFHEN是一种无需自举（无噪声）的全同态加密方案，基于群编码实现，通过重写系统表示群，使攻击者需要解决的子群成员问题达到最大难度，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有全同态加密方案通常需要自举操作来处理噪声，这限制了性能。GRAFHEN旨在开发一种无需自举的全同态加密方案，通过群编码方法消除噪声问题。

Method: 基于Nuida等人的工作，使用群编码实现全同态加密，通过重写系统表示群结构，使子群成员问题变得极其困难，同时优化性能。

Result: 实现了一个运行速度比现有标准快几个数量级的加密方案，并分析了多种可能的攻击方式及其防护措施。

Conclusion: GRAFHEN提供了一种高效的无自举全同态加密方案，通过群编码和重写系统实现了安全性和性能的平衡，为实际应用提供了有前景的解决方案。

Abstract: We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic
Encryption without the need for bootstrapping (or in other words, without
noise). Building on the work of Nuida and others, we achieve this using
encodings in groups.
  The groups are represented on a machine using rewriting systems. In this way
the subgroup membership problem, which an attacker would have to solve in order
to break the scheme, becomes maximally hard, while performance is preserved. In
fact we include a simple benchmark demonstrating that our implementation runs
several orders of magnitude faster than existing standards.
  We review many possible attacks against our protocol and explain how to
protect the scheme in each case.

</details>


### [43] [PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis](https://arxiv.org/abs/2510.21601)
*Emmanuel Dare Alalade,Ashraf Matrawy*

Main category: cs.CR

TL;DR: 提出了一种新颖的隐私威胁模型框架PTMF，通过分析威胁参与者的行为、意图和行动阶段来深入理解隐私威胁，并在物联网系统中进行了用户研究验证。


<details>
  <summary>Details</summary>
Motivation: 现有隐私威胁分析主要关注威胁发生的可能性和潜在区域，缺乏对威胁参与者及其行为意图的深入理解。需要开发一个以隐私为中心的框架来分析威胁参与者的活动和意图。

Method: 基于MITRE ATT&CK框架和LINDDUN隐私威胁模型，开发了PTMF框架。通过基于PTMF开发的问卷对12个物联网隐私威胁进行用户研究，收集了来自工业界和学术界安全隐私专家的意见。

Result: 识别了物联网用户识别威胁中的前三大威胁参与者及其关键路径，以及其余11个隐私威胁的威胁参与者。分析了威胁参与者的活动和意图模式。

Conclusion: PTMF框架为理解如何在物联网系统中主动有效地部署隐私措施提供了坚实基础，能够基于威胁参与者的活动和意图来缓解隐私威胁。

Abstract: Previous studies on PTA have focused on analyzing privacy threats based on
the potential areas of occurrence and their likelihood of occurrence. However,
an in-depth understanding of the threat actors involved, their actions, and the
intentions that result in privacy threats is essential. In this paper, we
present a novel Privacy Threat Model Framework (PTMF) that analyzes privacy
threats through different phases.
  The PTMF development is motivated through the selected tactics from the MITRE
ATT\&CK framework and techniques from the LINDDUN privacy threat model, making
PTMF a privacy-centered framework. The proposed PTMF can be employed in various
ways, including analyzing the activities of threat actors during privacy
threats and assessing privacy risks in IoT systems, among others. In this
paper, we conducted a user study on 12 privacy threats associated with IoT by
developing a questionnaire based on PTMF and recruited experts from both
industry and academia in the fields of security and privacy to gather their
opinions. The collected data were analyzed and mapped to identify the threat
actors involved in the identification of IoT users (IU) and the remaining 11
privacy threats. Our observation revealed the top three threat actors and the
critical paths they used during the IU privacy threat, as well as the remaining
11 privacy threats. This study could provide a solid foundation for
understanding how and where privacy measures can be proactively and effectively
deployed in IoT systems to mitigate privacy threats based on the activities and
intentions of threat actors within these systems.

</details>


### [44] [Toward provably private analytics and insights into GenAI use](https://arxiv.org/abs/2510.21684)
*Albert Cheu,Artem Lagzdin,Brett McLarnon,Daniel Ramage,Katharine Daly,Marco Gruteser,Peter Kairouz,Rakshita Tandon,Stanislav Chiknavaryan,Timon Van Overveldt,Zoe Gong*

Main category: cs.CR

TL;DR: 提出基于可信执行环境(TEE)的新一代联邦分析系统，为服务器端处理提供可验证的隐私保证，支持灵活工作负载包括LLM处理非结构化数据和差分隐私聚合。


<details>
  <summary>Details</summary>
Motivation: 大规模设备分析系统需要在保证高隐私安全标准的同时，满足数据质量、可用性和资源效率要求。

Method: 使用AMD SEV-SNP和Intel TDX等TEE技术，设备加密上传数据并标记允许的服务器处理步骤，开源TEE托管密钥管理服务确保数据仅能被授权步骤访问。

Result: 系统已成功部署到生产环境，为真实世界GenAI体验提供有价值的洞察。

Conclusion: 该系统通过TEE提供可验证的隐私保护，确保所有原始和衍生数据都在TEE中处理，防止系统操作员检查，并对所有发布结果应用差分隐私。

Abstract: Large-scale systems that compute analytics over a fleet of devices must
achieve high privacy and security standards while also meeting data quality,
usability, and resource efficiency expectations. We present a next-generation
federated analytics system that uses Trusted Execution Environments (TEEs)
based on technologies like AMD SEV-SNP and Intel TDX to provide verifiable
privacy guarantees for all server-side processing. In our system, devices
encrypt and upload data, tagging it with a limited set of allowable server-side
processing steps. An open source, TEE-hosted key management service guarantees
that the data is accessible only to those steps, which are themselves protected
by TEE confidentiality and integrity assurance guarantees. The system is
designed for flexible workloads, including processing unstructured data with
LLMs (for structured summarization) before aggregation into differentially
private insights (with automatic parameter tuning). The transparency properties
of our system allow any external party to verify that all raw and derived data
is processed in TEEs, protecting it from inspection by the system operator, and
that differential privacy is applied to all released results. This system has
been successfully deployed in production, providing helpful insights into
real-world GenAI experiences.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [45] [Lincoln AI Computing Survey (LAICS) and Trends](https://arxiv.org/abs/2510.20931)
*Albert Reuther,Peter Michaleas,Michael Jones,Vijay Gadepally,Jeremy Kepner*

Main category: cs.DC

TL;DR: 本文是对过去七年AI加速器和处理器调查的更新，重点关注生成式AI模型训练和推理的计算系统，收集并总结了当前商业加速器的峰值性能和功耗数据。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型在过去一年受到广泛关注，对训练和推理计算系统的需求增加，因此需要更新这项调查。

Method: 采用林肯AI计算调查(LAICS)的传统方法，收集公开宣布的商业加速器数据，绘制性能和功耗散点图，分析趋势和维度，并按市场细分进行详细分析。

Result: 调查提供了当前AI加速器的性能功耗对比图，展示了不同市场细分的特点，并新增了对今年新加入加速器的简要描述和计算架构分类。

Conclusion: 这项年度更新调查继续为AI计算系统领域提供有价值的性能功耗基准和趋势分析，特别关注生成式AI时代的新需求。

Abstract: In the past year, generative AI (GenAI) models have received a tremendous
amount of attention, which in turn has increased attention to computing systems
for training and inference for GenAI. Hence, an update to this survey is due.
This paper is an update of the survey of AI accelerators and processors from
past seven years, which is called the Lincoln AI Computing Survey -- LAICS
(pronounced "lace"). This multi-year survey collects and summarizes the current
commercial accelerators that have been publicly announced with peak performance
and peak power consumption numbers. In the same tradition of past papers of
this survey, the performance and power values are plotted on a scatter graph,
and a number of dimensions and observations from the trends on this plot are
again discussed and analyzed. Market segments are highlighted on the scatter
plot, and zoomed plots of each segment are also included. A brief description
of each of the new accelerators that have been added in the survey this year is
included, and this update features a new categorization of computing
architectures that implement each of the accelerators.

</details>


### [46] [Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach](https://arxiv.org/abs/2510.21155)
*Dandan Liang,Jianing Zhang,Evan Chen,Zhe Li,Rui Li,Haibo Yang*

Main category: cs.DC

TL;DR: MU-SplitFed是一种针对分裂联邦学习(SFL)中掉队者问题的零阶优化算法，通过不平衡更新机制解耦训练进度与掉队者延迟，实现线性加速。


<details>
  <summary>Details</summary>
Motivation: 分裂联邦学习结合了联邦学习的并行性和分裂学习的计算卸载优势，但受到掉队者问题的严重影响。由于分裂服务器与客户端之间的依赖关系，服务器端模型更新需要等待客户端激活，这种同步要求导致显著的时间延迟，使掉队者成为系统可扩展性和效率的关键瓶颈。

Method: 提出MU-SplitFed算法，通过允许服务器在每个客户端轮次中执行τ次本地更新，实现不平衡更新机制。该算法在零阶优化框架下运行，能够自适应调整τ值来缓解掉队者影响。

Result: 对于非凸目标函数，MU-SplitFed实现了O(√(d/(τT)))的收敛速率，在通信轮次中表现出τ倍的线性加速。实验表明，在存在掉队者的情况下，MU-SplitFed始终优于基线方法，并通过自适应调整τ有效减轻了掉队者的影响。

Conclusion: MU-SplitFed通过简单而有效的不平衡更新机制成功解决了分裂联邦学习中的掉队者问题，显著提升了系统的可扩展性和训练效率，为分布式学习系统提供了实用的解决方案。

Abstract: Split Federated Learning (SFL) enables scalable training on edge devices by
combining the parallelism of Federated Learning (FL) with the computational
offloading of Split Learning (SL). Despite its great success, SFL suffers
significantly from the well-known straggler issue in distributed learning
systems. This problem is exacerbated by the dependency between Split Server and
clients: the Split Server side model update relies on receiving activations
from clients. Such synchronization requirement introduces significant time
latency, making straggler a critical bottleneck to the scalability and
efficiency of the system. To mitigate this problem, we propose MU-SplitFed, a
straggler-resilient SFL algorithm in zeroth-order optimization that decouples
training progress from straggler delays via a simple yet effective unbalanced
update mechanism.
  By enabling the server to perform $\tau$ local updates per client round,
MU-SplitFed achieves a convergence rate of $O(\sqrt{d/(\tau T)})$ for
non-convex objectives, demonstrating a linear speedup of $\tau$ in
communication rounds. Experiments demonstrate that MU-SplitFed consistently
outperforms baseline methods with the presence of stragglers and effectively
mitigates their impact through adaptive tuning of $\tau$. The code for this
project is available at https://github.com/Johnny-Zip/MU-SplitFed.

</details>


### [47] [From SLA to vendor-neutral metrics: An intelligent knowledge-based approach for multi-cloud SLA-based broker](https://arxiv.org/abs/2510.21173)
*Víctor Rampérez,Javier Soriano,David Lizcano,Shadi Aljawarneh,Juan A. Lara*

Main category: cs.DC

TL;DR: 提出一个智能知识系统，用于在多云环境中自动将高层次SLA转换为供应商中立指标，解决云提供商锁定问题。


<details>
  <summary>Details</summary>
Motivation: 当前云提供商将确保服务级别协议合规的责任推给消费者，但消费者缺乏专业知识，且不同云提供商的低层指标不兼容，导致提供商锁定，阻碍多云环境的优势发挥。

Method: 1. 提出智能知识系统，自动将高层次SLA转换为供应商中立指标条件；2. 定义供应商中立指标集，并说明如何在不同云提供商中测量；3. 通过IaaS和PaaS两个用例在多云环境中进行验证。

Result: 验证表明，通过两种解决方案的互补性，云消费者可以在多云环境中自动透明地利用多云优势，得到云专家认可。

Conclusion: 该解决方案成功解决了云提供商锁定问题，使消费者能够在多云环境中自动实现SLA合规，促进多云环境的有效利用。

Abstract: Cloud computing has been consolidated as a support for the vast majority of
current and emerging technologies. However, there are some barriers that
prevent the exploitation of the full potential of this technology. First, the
major cloud providers currently put the onus of implementing the mechanisms
that ensure compliance with the desired service levels on cloud consumers.
However, consumers do not have the required expertise. Since each cloud
provider exports a different set of low-level metrics, the strategies defined
to ensure compliance with the established service-level agreement (SLA) are
bound to a particular cloud provider. This fosters provider lock-in and
prevents consumers from benefiting from the advantages of multi-cloud
environments. This paper presents a solution to the problem of automatically
translating SLAs into objectives expressed as metrics that can be measured
across multiple cloud providers. First, we propose an intelligent
knowledge-based system capable of automatically translating high-level SLAs
defined by cloud consumers into a set of conditions expressed as vendor-neutral
metrics, providing feedback to cloud consumers (intelligent tutoring system).
Secondly, we present the set of vendor-neutral metrics and explain how they can
be measured for the different cloud providers. Finally, we report a validation
based on two use cases (IaaS and PaaS) in a multi-cloud environment formed by
leading cloud providers. This evaluation has demonstrated that, thanks to the
complementarity of the two solutions, cloud consumers can automatically and
transparently exploit the multi-cloud in many application domains, as endorsed
by the cloud experts consulted in the course of this study.

</details>


### [48] [Arbitration-Free Consistency is Available (and Vice Versa)](https://arxiv.org/abs/2510.21304)
*Hagit Attiya,Constantin Enea,Enrique Román-Calvo*

Main category: cs.DC

TL;DR: 该论文提出了仲裁自由一致性定理(AFC)，揭示了分布式存储系统中可用性和一致性权衡的根本属性——仲裁自由性，即不需要总仲裁顺序来解决可见性或读取依赖关系。


<details>
  <summary>Details</summary>
Motivation: 解决分布式存储系统中可用性和一致性之间的基本矛盾，为各种对象语义和一致性模型的组合提供精确的理论解释，填补经典CAP定理在复杂接口应用中的空白。

Method: 开发了一个通用的语义框架，将操作语义和一致性模型结合在存储规范中，涵盖键值存储、计数器、集合、CRDT和事务数据库等多种对象，以及从因果一致性到快照隔离等各种一致性模型。

Result: 证明了仲裁自由一致性定理：当且仅当对象规范在一致性模型中是仲裁自由的（不需要总仲裁顺序来解决可见性或读取依赖），才允许可用的实现。

Conclusion: AFC定理统一并推广了先前的结果，揭示了仲裁自由性是区分无协调一致性和固有同步行为的根本属性，为分布式系统设计提供了理论基础。

Abstract: The fundamental tension between \emph{availability} and \emph{consistency}
shapes the design of distributed storage systems. Classical results capture
extreme points of this trade-off: the CAP theorem shows that strong models like
linearizability preclude availability under partitions, while weak models like
causal consistency remain implementable without coordination. These theorems
apply to simple read-write interfaces, leaving open a precise explanation of
the combinations of object semantics and consistency models that admit
available implementations.
  This paper develops a general semantic framework in which storage
specifications combine operation semantics and consistency models. The
framework encompasses a broad range of objects (key-value stores, counters,
sets, CRDTs, and transactional databases) and consistency models (from causal
consistency and sequential consistency to snapshot isolation and transactional
and non-transactional SQL).
  Within this framework, we prove the \emph{Arbitration-Free Consistency} (AFC)
theorem, showing that an object specification within a consistency model admits
an available implementation if and only if it is \emph{arbitration-free}, that
is, it does not require a total arbitration order to resolve visibility or read
dependencies.
  The AFC theorem unifies and generalizes previous results, revealing
arbitration-freedom as the fundamental property that delineates
coordination-free consistency from inherently synchronized behavior.

</details>


### [49] [LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science](https://arxiv.org/abs/2510.21373)
*Sankalpa Timilsina,Susmit Shannigrahi*

Main category: cs.DC

TL;DR: 提出了一种使用语义名称的分布式控制平面，用于在地理分散的计算集群上部署计算任务，解决了集中式控制器在多组织协作中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的计算部署方法主要使用集中式控制器（如Kubernetes），这在多组织协作中不适用，且工作流通常需要针对单一平台的手动配置，无法适应基础设施的动态变化。

Method: 通过为计算任务分配语义名称，将其与命名的Kubernetes服务端点进行匹配，实现去中心化的控制平面。

Result: 该方法使计算作业的部署与位置无关，任何具有足够资源的集群都可以执行计算；同时支持动态计算部署，无需预先了解集群位置或预定义配置。

Conclusion: 基于语义名称的去中心化控制平面为地理分布式计算平台提供了更灵活、适应性更强的计算部署解决方案。

Abstract: Scientific communities are increasingly using geographically distributed
computing platforms. The current methods of compute placement predominantly use
logically centralized controllers such as Kubernetes (K8s) to match tasks to
available resources. However, this centralized approach is unsuitable in
multi-organizational collaborations. Furthermore, workflows often need to use
manual configurations tailored for a single platform and cannot adapt to
dynamic changes across infrastructure. Our work introduces a decentralized
control plane for placing computations on geographically dispersed compute
clusters using semantic names. We assign semantic names to computations to
match requests with named Kubernetes (K8s) service endpoints. We show that this
approach provides multiple benefits. First, it allows placement of
computational jobs to be independent of location, enabling any cluster with
sufficient resources to execute the computation. Second, it facilitates dynamic
compute placement without requiring prior knowledge of cluster locations or
predefined configurations.

</details>


### [50] [On Reduction and Synthesis of Petri's Cycloids](https://arxiv.org/abs/2510.21493)
*Rüdiger Valk,Daniel Moldt*

Main category: cs.DC

TL;DR: 本文研究循环体（cycloids）这种特殊Petri网的结构特性，定义了循环体的归约系统，证明了不可约循环体的性质，并提出了从Petri网结构合成循环体参数的方法，用于循环体同构判定。


<details>
  <summary>Details</summary>
Motivation: 循环体是Petri网中用于建模动作和事件过程的特殊类型，属于Petri一般系统理论的基础。为了深入理解其结构特性，需要研究循环体的归约系统和参数合成方法。

Method: 定义了类似重写系统的循环体归约系统，研究了不可约循环体的性质，并推导了从Petri网结构合成循环体参数的方法。

Result: 提出了循环体同构判定的高效决策程序，建立了循环体参数与其Petri网结构之间的对应关系。

Conclusion: 通过归约系统和参数合成方法，为循环体的结构分析和同构判定提供了有效的理论工具，深化了对这种特殊Petri网模型的理解。

Abstract: Cycloids are particular Petri nets for modelling processes of actions and
events, belonging to the fundaments of Petri's general systems theory. Defined
by four parameters they provide an algebraic formalism to describe strongly
synchronized sequential processes. To further investigate their structure,
reduction systems of cycloids are defined in the style of rewriting systems and
properties of irreducible cycloids are proved. In particular the synthesis of
cycloid parameters from their Petri net structure is derived, leading to an
efficient method for a decision procedure for cycloid isomorphism.

</details>


### [51] [Distributed $(Δ+1)$-Coloring in Graphs of Bounded Neighborhood Independence](https://arxiv.org/abs/2510.21549)
*Marc Fuchs,Fabian Kuhn*

Main category: cs.DC

TL;DR: 本文显著改进了具有邻域独立性θ的图中(Δ+1)-着色问题的确定性分布式算法复杂度，将运行时间从2^O(√logΔ)改进到(θ·logΔ)^O(loglogΔ/logloglogΔ)，在θ为polylog(Δ)时达到准多项式对数时间。


<details>
  <summary>Details</summary>
Motivation: 分布式着色问题是分布式图算法领域的核心问题之一，其中(Δ+1)-顶点着色的确定性复杂度是该领域最重要的开放问题之一。本文旨在通过研究邻域独立性θ受限的特殊图族，来改进对(Δ+1)-着色确定性复杂度的理解。

Method: 研究具有邻域独立性θ的图族，其中θ是图中某个节点的最大两两不相邻邻居数。通过分析这类图的结构特性，设计改进的分布式着色算法。

Result: 证明了在邻域独立性为θ的图中，(Δ+1)-着色可以在(θ·logΔ)^O(loglogΔ/logloglogΔ)+O(log* n)轮内计算完成，当θ最多为polylog(Δ)时达到准多项式对数时间。

Conclusion: 本文显著改进了邻域独立性受限图中(Δ+1)-着色的确定性分布式算法复杂度，并指出已知的(2Δ-1)-边着色方法无法推广到秩至少为3的超图边着色问题。

Abstract: The distributed coloring problem is arguably one of the key problems studied
in the area of distributed graph algorithms. The most standard variant of the
problem asks for a proper vertex coloring of a graph with $\Delta+1$ colors,
where $\Delta$ is the maximum degree of the graph. Despite an immense amount of
work on distributed coloring problems in the distributed setting, determining
the deterministic complexity of $(\Delta+1)$-coloring in the standard message
passing model remains one of the most important open questions of the area. In
this paper, we aim to improve our understanding of the deterministic complexity
of $(\Delta+1)$-coloring as a function of $\Delta$ in a special family of
graphs for which significantly faster algorithms are already known. The
neighborhood independence $\theta$ of a graph is the maximum number of pairwise
non-adjacent neighbors of some node of the graph. In general, in graphs of
neighborhood independence $\theta=O(1)$ (e.g., line graphs), it is known that
$(\Delta+1)$-coloring can be solved in $2^{O(\sqrt{\log\Delta})}+O(\log^* n)$
rounds. In the present paper, we significantly improve this result, and we show
that in graphs of neighborhood independence $\theta$, a $(\Delta+1)$-coloring
can be computed in $(\theta\cdot\log\Delta)^{O(\log\log\Delta /
\log\log\log\Delta)}+O(\log^* n)$ rounds and thus in quasipolylogarithmic time
in $\Delta$ as long as $\theta$ is at most polylogarithmic in $\Delta$. We also
show that the known approach that leads to a polylogarithmic in $\Delta$
algorithm for $(2\Delta-1)$-edge coloring already fails for edge colorings of
hypergraphs of rank at least $3$.

</details>
