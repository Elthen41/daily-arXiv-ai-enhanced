<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [An Event-Driven Spiking Compute-In-Memory Macro based on SOT-MRAM](https://arxiv.org/abs/2511.03203)
*Deyang Yu,Chenchen Liu,Chuanjie Zhang,Xiao Fang,Weisheng Zhao*

Main category: cs.AR

TL;DR: 本文提出了一种基于自旋轨道矩磁阻随机存取存储器(SOT-MRAM)的计算内存宏，采用事件驱动脉冲处理实现高能效，通过混合串并联单元结构支持矩阵向量乘法，峰值能效达243.6 TOPS/W。


<details>
  <summary>Details</summary>
Motivation: 现有MRAM计算内存设计因依赖复杂模拟电路而导致高能耗问题，需要开发更高效的解决方案。

Method: 采用SOT-MRAM交叉阵列，设计混合串并联单元结构支持矩阵向量乘法，使用轻量级电路进行脉冲编码解码，避免传统模拟电路。

Result: 在28nm工艺下实现，峰值能效达到243.6 TOPS/W，显著优于现有设计。

Conclusion: 所提出的SOT-MRAM计算内存宏通过事件驱动脉冲处理和混合结构设计，实现了高能效的计算内存解决方案。

Abstract: The application of Magnetic Random-Access Memory (MRAM) in
computing-in-memory (CIM) has gained significant attention. However, existing
designs often suffer from high energy consumption due to their reliance on
complex analog circuits for computation. In this work, we present a Spin-Orbit-
Torque MRAM(SOT-MRAM)-based CIM macro that employs an event-driven spiking
processing for high energy efficiency. The SOT-MRAM crossbar adopts a hybrid
series-parallel cell structure to efficiently support matrix-vector
multiplication (MVM). Signal information is (en) decoded as spikes using
lightweight circuits, eliminating the need for conventional area- and
powerintensive analog circuits. The SOT-MRAM macro is designed and evaluated in
28nm technology, and experimental results show that it achieves a peak energy
efficiency of 243.6 TOPS/W, significantly outperforming existing designs.

</details>


### [2] [Design and Optimization of Mixed-Kernel Mixed-Signal SVMs for Flexible Electronics](https://arxiv.org/abs/2511.03427)
*Florentia Afentaki,Maha Shatta,Konstantinos Balaskas,Georgios Panagopoulos,Georgios Zervakis,Mehdi B. Tahoori*

Main category: cs.AR

TL;DR: 本文提出了柔性电子中首个混合核函数和混合信号的SVM设计，通过联合优化方法在数字/模拟域中平衡线性核和RBF核的使用，在保持高精度的同时显著降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 柔性电子技术虽然具有低成本、可弯曲和可持续性等优势，但其大特征尺寸限制了集成密度，难以实现机器学习电路。现有SVM设计在线性核和RBF核之间存在硬件成本与精度的权衡问题。

Method: 提出混合核函数和混合信号的SVM设计，采用联合优化方法训练混合核SVM，并将二元SVM分类器映射到适当的核函数（线性/RBF）和域（数字/模拟），以减少昂贵的RBF分类器数量。

Result: 与最先进的单核线性SVM相比，精度提高7.7%；与数字RBF实现相比，面积和功耗分别平均降低108倍和17倍。

Conclusion: 该混合核函数和混合信号SVM设计成功平衡了柔性电子中硬件成本与精度的权衡，为近传感器应用提供了高效的机器学习解决方案。

Abstract: Flexible Electronics (FE) have emerged as a promising alternative to
silicon-based technologies, offering on-demand low-cost fabrication,
conformality, and sustainability. However, their large feature sizes severely
limit integration density, imposing strict area and power constraints, thus
prohibiting the realization of Machine Learning (ML) circuits, which can
significantly enhance the capabilities of relevant near-sensor applications.
Support Vector Machines (SVMs) offer high accuracy in such applications at
relatively low computational complexity, satisfying FE technologies'
constraints. Existing SVM designs rely solely on linear or Radial Basis
Function (RBF) kernels, forcing a trade-off between hardware costs and
accuracy. Linear kernels, implemented digitally, minimize overhead but
sacrifice performance, while the more accurate RBF kernels are prohibitively
large in digital, and their analog realization contains inherent functional
approximation. In this work, we propose the first mixed-kernel and mixed-signal
SVM design in FE, which unifies the advantages of both implementations and
balances the cost/accuracy trade-off. To that end, we introduce a
co-optimization approach that trains our mixed-kernel SVMs and maps binary SVM
classifiers to the appropriate kernel (linear/RBF) and domain (digital/analog),
aiming to maximize accuracy whilst reducing the number of costly RBF
classifiers. Our designs deliver 7.7% higher accuracy than state-of-the-art
single-kernel linear SVMs, and reduce area and power by 108x and 17x on average
compared to digital RBF implementations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: 欧洲高性能计算卓越中心在CFD领域的能源效率研究，通过案例研究评估了多种CFD应用在不同架构上的能耗表现，强调加速器和混合精度技术对降低能耗的优势。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算系统面临能源效率挑战，计算需求增长和架构复杂性导致显著能耗足迹，需要测量、分析和优化能源消耗。

Method: 使用代表性CFD应用（waLBerla、FLEXI/GALÆXI、Neko、NekRS）进行案例研究，评估不同架构（CPU和GPU）上的能源解决方案和时间解决方案指标。

Result: 结果显示加速器和混合精度技术在保持计算精度的同时显著降低能耗，在LUMI、MareNostrum5、MeluXina和JUWELS Booster等系统上验证了效果。

Conclusion: 需要促进HPC系统上的能源测量，提高意识、教育社区，并采取行动实现更可持续的百亿亿次计算。

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [4] [Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots](https://arxiv.org/abs/2511.03286)
*Ehud Shapiro*

Main category: cs.DC

TL;DR: 本文提出了基于原子事务的多智能体转换系统和协议作为研究全球数字平台的正式框架，引入了基本智能体的概念，并根据基本智能体的基数将全球平台分为四类：中心化、去中心化、联邦化和草根化。


<details>
  <summary>Details</summary>
Motivation: 为全球数字平台提供一个统一的数学框架，用于分类和分析不同类型的平台架构，包括现有和想象中的平台。

Method: 使用基于原子事务的多智能体转换系统和协议作为形式化框架，引入基本智能体的概念，并通过分析最小基本智能体集合的基数来分类平台。

Result: 成功将全球平台分为四类：中心化（1个基本智能体）、去中心化（有限个>1基本智能体）、联邦化（无限但不通用基本智能体）、草根化（所有智能体都是基本的）。

Conclusion: 这项工作为任何全球平台提供了第一个数学分类框架，通过多智能体原子事务规范来确定平台类型，并将草根化平台置于更广泛的正式背景中。

Abstract: Global digital platforms are software systems designed to serve entire
populations, with some already serving billions of people. We propose atomic
transactions-based multiagent transition systems and protocols as a formal
framework to study them; introduce essential agents -- minimal sets of agents
the removal of which makes communication impossible; and show that the
cardinality of essential agents partitions all global platforms into four
classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we
provide centralised, decentralised, federated, and grassroots specifications
via multiagent atomic transactions, and prove they satisfy basic correctness
properties. We discuss informally additional global platforms -- currencies,
``sharing economy'' apps, AI, and more. While this may be the first
characterisation of centralised, decentralised, and federated global platforms,
grassroots platforms have been formally defined previously, but using different
notions. Here, we prove that their original definition implies that all agents
are essential, placing grassroots platforms in a distinct class within the
broader formal context that includes all global platforms. This work provides
the first mathematical framework for classifying any global platform --
existing or imagined -- by providing a multiagent atomic-transactions
specification of it and determining the cardinality of the minimal set of
essential agents in the ensuing multiagent protocol. It thus

</details>


### [5] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: UMDAM是一种针对NPU-PIM协同执行的统一内存亲和性数据布局和DRAM地址映射方案，通过列主序、基于分块的布局和可配置的DRAM映射策略，在不引入额外内存开销或带宽损失的情况下，显著提升边缘设备上LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备NPU上部署时，解码阶段仍然内存密集，性能受限。NPU-PIM协同执行面临数据布局不匹配、带宽损失和冗余存储等挑战。

Method: 提出UMDAM方案，采用列主序、基于分块的布局和可配置的DRAM映射策略，确保与NPU计算的兼容性，同时最大化PIM效率。

Result: 在OPT模型上的综合评估显示，UMDAM将首token时间(TTFT)降低高达3.0倍，末token时间(TTLT)降低2.18倍。

Conclusion: UMDAM显著提升了边缘设备上端到端LLM推理效率，解决了NPU-PIM协同执行的关键挑战。

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [6] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: 本文评估了三种隔离策略（cgroups和CPU固定、Docker容器、Firecracker MicroVMs）在Duet基准测试中的效果，发现进程隔离通常能降低误报，但Docker容器对噪声影响更敏感。


<details>
  <summary>Details</summary>
Motivation: 云环境中多租户资源争用导致基准测试存在性能变异性，Duet基准测试通过在相同VM上并发运行两个工作负载版本来缓解此问题，但需要额外的隔离机制来处理同步工作负载间的VM内争用。

Method: 通过运行带有噪声生成器的duet设置基准测试，比较三种隔离策略（cgroups和CPU固定、Docker容器、Firecracker MicroVMs）与无隔离基线实验的效果。

Result: 所有实验在噪声生成影响下显示出不同的延迟分布，但进程隔离通常降低了误报，除了Docker容器实验。Docker容器虽然内部依赖cgroups和CPU固定，但对噪声导致的性能下降更敏感。

Conclusion: 建议对同步工作负载使用进程隔离，但应避免使用Docker容器。

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [7] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文提出了一种基于谱空间的拓扑编码方法，用于分析分布式计算中基于轮次的全信息对手模型，给出了无色任务可解性的特征化，并通过Stone对偶性建立了通用的分布式可计算性定理。


<details>
  <summary>Details</summary>
Motivation: 统一分布式计算中的拓扑方法，为消息对手模型下的无色任务可解性提供更一般的特征化，并解释彩色与无色模型计算能力等价性的拓扑原因。

Method: 使用谱空间对分布式协议执行进行拓扑编码，考虑面偏序集上的Alexandrov拓扑，通过谱空间范畴中的投影极限定义极限对象，应用Stone对偶性建立可计算性定理。

Result: 得到了无色任务可解性的充分必要条件：存在与Δ兼容的谱映射f:Π∞_M(I)→O。从这一通用特征化可以推导出许多已知的无色可计算性定理，并解释了彩色与无色模型计算能力等价性的拓扑原因。

Conclusion: 该工作为分布式计算中的拓扑方法提供了统一框架，通过谱空间和Stone对偶性建立了通用的可计算性理论，揭示了不同模型间计算能力等价性的深层拓扑机制。

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [8] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文扩展了分布式计算中无色任务的可行性理论，将消息对手模型推广到输入依赖型对手，证明了核心弹性对手在IIS模型中的计算能力等价性，并给出了k-集合协议在条件基础、核心依赖对手下的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定消息对手模型，而实际分布式系统中对手行为可能依赖于输入配置。本文旨在扩展计算可行性理论，研究输入依赖型对手的计算能力，并深入分析k-集合协议在不同对手模型下的可解性条件。

Method: 采用拓扑框架和几何构造方法，将CG-24的几何方法推广到输入依赖型对手，分析核心弹性对手在IIS模型中的等价性，并建立条件基础对手下k-集合协议的充要条件。

Result: 证明了输入依赖型对手的可行性特征化，核心弹性对手在IIS模型中的计算能力等价性，以及k-集合协议在条件基础、核心依赖对手下的充要可解条件。

Conclusion: 本文成功扩展了分布式计算可行性理论，为输入依赖型对手提供了完整的特征化，深化了对k-集合协议在不同对手模型下可解性的理解，为分布式系统设计提供了理论依据。

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)
*Jon Kutasov,Chloe Loughridge,Yuqi Sun,Henry Sleight,Buck Shlegeris,Tyler Tracy,Joe Benton*

Main category: cs.AI

TL;DR: 本文系统评估了AI控制协议在SHADE-Arena环境中的安全性，发现关键行动延迟策略对红队攻击具有高度鲁棒性，而重采样策略在红队了解协议内部信息时安全性会大幅下降。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统作为智能体被广泛部署，确保其安全运行变得至关重要。AI控制通过监控智能体行为并在必要时干预或审计来降低风险，需要评估这些协议的有效性和对自适应攻击的鲁棒性。

Method: 在SHADE-Arena多样化智能体环境中系统评估控制协议，包括蓝队协议（可信模型延迟、重采样、关键行动延迟）对抗默认攻击策略，然后迭代红队策略对抗这些协议。

Result: 重采样和关键行动延迟协议将安全性从50%提升到96%；但当红队策略具有额外能力（如知道重采样发生时间或能模拟监控器）时，重采样策略的安全性降至17%；关键行动延迟策略对最强红队攻击仍保持高度鲁棒性。

Conclusion: 关键行动延迟策略对自适应攻击具有高度鲁棒性，证明拒绝攻击策略访问协议内部信息的重要性，为AI控制协议设计提供了重要见解。

Abstract: As AI systems become more capable and widely deployed as agents, ensuring
their safe operation becomes critical. AI control offers one approach to
mitigating the risk from untrusted AI agents by monitoring their actions and
intervening or auditing when necessary. Evaluating the safety of these
protocols requires understanding both their effectiveness against current
attacks and their robustness to adaptive adversaries. In this work, we
systematically evaluate a range of control protocols in SHADE-Arena, a dataset
of diverse agentic environments. First, we evaluate blue team protocols,
including deferral to trusted models, resampling, and deferring on critical
actions, against a default attack policy. We find that resampling for
incrimination and deferring on critical actions perform best, increasing safety
from 50% to 96%. We then iterate on red team strategies against these protocols
and find that attack policies with additional affordances, such as knowledge of
when resampling occurs or the ability to simulate monitors, can substantially
improve attack success rates against our resampling strategy, decreasing safety
to 17%. However, deferring on critical actions is highly robust to even our
strongest red team strategies, demonstrating the importance of denying attack
policies access to protocol internals.

</details>


### [10] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: PublicAgent是一个多智能体框架，通过分解为专门化的智能体（意图澄清、数据集发现、分析和报告）来解决LLM在端到端分析工作流中的局限性，保持专注注意力并实现阶段验证。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库对非专家难以访问，需要数据集发现、模式映射和统计分析的专业知识。LLM在单个任务中表现良好，但在端到端分析工作流中存在注意力稀释、专业推理模式干扰和错误传播等问题。

Method: 采用多智能体框架，将分析工作流分解为专门的智能体：意图澄清、数据集发现、分析和报告。这种架构在智能体上下文中保持专注注意力，并在每个阶段实现验证。

Result: 评估显示：1）专业化独立于模型强度提供价值；2）智能体分为通用型（发现、分析）和条件型（报告、意图）；3）智能体缓解不同故障模式；4）架构优势在不同任务复杂度下持续存在；5）智能体有效性在不同模型间差异显著。

Conclusion: 多智能体专业化对于复杂分析工作流是必要的，通过自然语言接口实现更广泛的公共数据访问，并提供了多智能体LLM系统的设计原则。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [11] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文构建了首个基准测试来评估LLMs是否内化了真实世界的概率分布知识，发现LLMs在理解现实世界统计分布方面表现不佳，未能自然内化观测分布知识。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能够内化描述真实世界概率分布的知识，验证LLMs作为通用分布近似器的能力，并探讨高维分布学习中的根本挑战。

Method: 开发首个基准测试，直接测试LLMs是否能够访问描述真实世界人口的经验分布，涵盖经济、健康、教育和社会行为等领域。

Result: LLMs整体表现不佳，似乎无法自然内化真实世界统计信息，缺乏对观测分布（Pearl因果层次第一层）的知识。

Conclusion: 语言模型在理解真实世界概率分布方面存在局限，根据因果层次定理，这意味着它们在干预和反事实知识方面也受到限制。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [12] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: SnapStream是一种KV缓存压缩方法，可在保持模型精度的同时显著减少内存使用，已在生产环境中实现4倍内存改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型支持长上下文导致KV缓存内存需求激增，现有压缩技术难以在工业部署框架中应用，且对现代指令遵循模型的影响不明确。

Method: 开发SnapStream KV缓存压缩方法，在静态图和连续批处理的生产环境中实现稀疏KV注意力技术。

Result: 在DeepSeek-671B的16路张量并行部署中，实现4倍内存使用改进，128k上下文长度下达到1832 tokens/秒，在多个基准测试中精度损失最小。

Conclusion: SnapStream是首个在生产推理系统中成功部署的稀疏KV注意力技术，解决了工业部署中的KV缓存内存瓶颈问题。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [13] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 论文提出了一种基于能力的大语言模型监控新方法，替代传统基于任务的机器学习监控，以解决LLM在医疗领域应用中的系统性弱点检测问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于任务的机器学习监控方法不适用于大语言模型，因为LLM不是针对特定任务或特定人群训练的，其性能退化模式与传统ML不同，需要新的监控组织原则。

Method: 提出能力基础监控方法，围绕LLM共享的内部能力（如总结、推理、翻译、安全防护等）组织监控，而非独立评估每个下游任务，实现跨任务的系统性弱点检测。

Result: 该方法能够检测到基于任务监控可能遗漏的系统性弱点、长尾错误和涌现行为，为LLM在医疗领域的监控提供了可扩展的基础。

Conclusion: 能力基础监控为医疗领域LLM和未来通用人工智能模型的安全、自适应和协作监控提供了可扩展的基础框架。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [14] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 本文对miniF2F基准测试中的形式化与非形式化陈述进行了深入分析，发现原始基准中存在大量不一致问题，导致AI系统在数学奥林匹克竞赛任务中的准确率仅为36%。通过修正所有错误和差异，创建了miniF2F-v2版本，将准确率提升至70%。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在数学奥林匹克竞赛任务中的表现，包括自然语言理解、形式化转换和定理证明的完整流程，发现现有基准测试存在质量问题影响评估准确性。

Method: 对miniF2F基准进行系统性分析，识别形式化与非形式化陈述之间的差异，修正所有错误和不一致之处，创建改进的miniF2F-v2数据集，并在新数据集上重新评估完整的定理证明流程。

Result: 原始miniF2F上最佳准确率为36%，显著低于文献中单独报告的形式化(97%)和定理证明(69%)的SOTA准确率。修正后的miniF2F-v2将准确率提升至70%，但仍显示形式化模型与定理证明器之间存在显著不对齐。

Conclusion: 高质量的基准测试对于评估形式推理领域的进展至关重要，能够更好地诊断形式化模型和定理证明模型的失败与成功模式。miniF2F-v2为社区提供了更可靠的评估工具。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [15] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 本文提出了一种利用多模态大语言模型辅助设计烟花算法的新框架，通过引入关键部分概念扩展烟花算法到复杂高维任务，并在旅行商问题和电子设计自动化问题上取得了优于或达到当前最优的结果。


<details>
  <summary>Details</summary>
Motivation: 随着优化问题日益复杂多样，传统零阶或一阶方法在处理非凸性、高维性、黑盒特性等问题时效率低下、梯度信息不准确且优化信息利用不足。大语言模型在语言理解和代码生成方面的进步为优化算法设计提供了新机遇。

Method: 以烟花算法为基础优化器，结合多模态大语言模型辅助设计，提出关键部分概念来扩展烟花算法处理复杂高维任务，并利用大语言模型的多模态特性充分挖掘优化过程中的信息。

Result: 在旅行商问题和电子设计自动化问题上的实验结果表明，基于新框架生成的烟花算法在许多问题实例上达到或超越了当前最优结果。

Conclusion: 将多模态大语言模型与烟花算法结合的新框架能够有效提升优化算法性能，为解决复杂高维优化问题提供了有前景的方向。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [16] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的大语言模型安全响应框架，通过输入级安全分类和输出级知识增强，系统性地保护LLM的安全部署。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其安全问题日益突出，严重制约了在关键领域的可信部署。

Method: 在输入级采用基于监督微调的四级安全分类模型（安全、不安全、条件安全、重点关注）；在输出级结合检索增强生成和专门微调的解释模型，确保响应基于实时可信知识库。

Result: 风险召回率达到99.3%，在公共安全评估基准上的安全得分显著高于基线模型TinyR1-Safety-8B，在专有高风险测试集上组件获得100%安全分。

Conclusion: 该研究为构建高安全性、高可信度的LLM应用提供了有效的工程路径。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [17] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出了一种验证形式化可解释AI（XAI）方法的新方法，并评估了公开可用的形式化解释器PyXAI，发现其在大多数数据集上计算出的解释存在错误。


<details>
  <summary>Details</summary>
Motivation: 形式化XAI相比非形式化方法具有理论严谨性保证，但对其实际实现的验证关注不足。

Method: 开发了一种新颖的验证形式化解释器的方法论，并对PyXAI进行了评估。

Result: 实验发现PyXAI在大多数分析的数据集上计算出的解释存在错误。

Conclusion: 证实了所提出的验证方法对形式化解释器验证的重要性。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [18] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 本文提出了一个多智能体AI框架来形式化工程设计过程，通过专业智能体协作生成和优化设计候选方案，并以NACA翼型气动优化为例验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的工程设计方法需要多领域专业知识，导致复杂的协作和迭代优化过程，资源密集且效率低下。本文旨在通过AI多智能体框架解决这些问题。

Method: 构建包含三个关键AI智能体的框架：图本体学家（使用LLM构建领域知识图谱）、系统工程师（制定技术需求）和设计工程师（生成候选设计）。系统通过迭代反馈循环进行设计验证和优化。

Result: 该框架成功应用于4位数NACA翼型的气动优化，能够有效生成满足技术需求的设计方案，并通过迭代优化提升性能指标如升阻比。

Conclusion: 研究表明，配备结构化知识表示的协作AI智能体能够显著提高工程设计过程的效率、一致性和质量。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [19] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 本文介绍了为Adobe Summit开发的领域特定AI助手Summit Concierge，它通过人机协同开发流程解决了数据稀疏、质量保证和快速部署等现实约束，展示了在冷启动场景下构建可扩展可靠AI助手的经验。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI助手提升企业环境中的生产力、简化信息访问并改善用户体验，特别是在Adobe Summit这样的企业活动中。

Method: 采用人机协同开发工作流程，结合提示工程、检索基础和轻量级人工验证，构建能够处理各种活动相关查询的领域特定AI助手。

Result: 成功开发并部署了Summit Concierge系统，该系统在真实约束条件下运行良好，证明了敏捷、反馈驱动的开发方法在冷启动场景下的有效性。

Conclusion: 敏捷、反馈驱动的开发方法能够构建可扩展且可靠的AI助手，即使在数据稀疏的冷启动场景下也能成功部署。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [20] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）能够仅基于大五人格量表的最小定量输入，准确模拟人类心理特质的相关结构，其生成的跨量表相关模式与人类数据高度一致（R² > 0.89），表现优于基于语义相似度的预测，并接近直接在数据集上训练的机器学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够从最小定量输入中建模人类心理特质的关联结构，探索其作为心理学模拟工具和揭示其推理能力的潜力。

Method: 使用816名人类个体的大五人格量表回答提示各种LLMs，让它们扮演这些个体在其他九个心理量表上的回答。分析推理过程发现LLMs采用两阶段系统过程：首先将原始大五回答转化为自然语言人格摘要，然后基于这些摘要推理生成目标量表回答。

Result: LLMs在捕捉人类心理结构方面表现出显著准确性，跨量表相关模式与人类数据高度一致（R² > 0.89）。零样本性能显著超过基于语义相似度的预测，接近直接在数据集上训练的机器学习算法。压缩摘要不仅捕获冗余信息，还编码了特质的协同交互模式。

Conclusion: LLMs能够通过抽象和推理过程从最小数据精确预测个体参与者的心理特质，既为心理学模拟提供了强大工具，也为其新兴推理能力提供了宝贵见解。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [21] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 本文提出了一个名为AAA的网页可访问性审计框架，通过人机协作模式实现WCAG-EM标准的可扩展执行，包含GRASP采样方法和MaC智能助手两个关键创新。


<details>
  <summary>Details</summary>
Motivation: 当前网页可访问性审计存在资源密集和难以扩展的问题，大多数网站界面不符合可访问性标准，阻碍了数字空间的社会福利、正义和平等发展。

Method: AAA框架通过GRASP（基于图的多模态采样方法）确保代表性页面覆盖，以及MaC（基于多模态大语言模型的智能助手）提供跨模态推理和智能辅助，实现端到端的可访问性审计。

Result: 实验证明该方法有效，提供了小规模语言模型经过微调后能够胜任专业任务的见解，并贡献了四个用于审计流程基准测试的新数据集。

Conclusion: 该框架通过人机协作实现了可扩展的网页可访问性审计，为实际应用提供了AI增强的辅助工具，具有现实影响意义。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [22] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 本文对多种机器学习模型中的解释问题进行了参数化复杂性理论分析，重点关注具有透明内部机制的模型，包括决策树、决策集、决策列表、布尔电路及其集成模型。


<details>
  <summary>Details</summary>
Motivation: 填补可解释AI领域在理解生成解释复杂性方面的理论空白，为AI系统的透明度和问责性提供理论支持。

Method: 采用参数化复杂性理论框架，分析两种主要解释问题（溯因解释和对比解释）在局部和全局变体中的计算复杂性。

Result: 为不同ML模型中的解释问题提供了复杂性理论分析，揭示了这些问题的计算特征。

Conclusion: 这项工作为XAI领域的进一步研究提供了重要理论基础，强调了AI系统透明度和问责性的必要性。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>


### [23] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: Solly是首个在简化版Liar's Poker中达到精英人类水平的AI智能体，通过无模型、演员-评论家深度强化学习算法进行自博弈训练，在单挑和多玩家游戏中均表现出色，胜率超过50%，并优于大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 虽然AI在德州扑克等游戏中已取得突破，但多玩家动态较弱，大多数手牌快速收敛，只有两名玩家参与多轮竞标。研究者希望开发能在多玩家参与度高的游戏中达到精英水平的AI。

Method: 使用无模型、演员-评论家深度强化学习算法进行自博弈训练。

Result: Solly在单挑和多玩家Liar's Poker中达到精英人类水平，胜率超过50%，在资金收益上表现出色，优于大型语言模型，开发了新颖的竞标策略，能有效随机化游戏，不易被世界级人类玩家利用。

Conclusion: Solly成功展示了在多玩家参与度高的不完全信息游戏中达到精英人类水平的可行性，为AI在多玩家动态环境中的发展提供了重要进展。

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [AI Agents with Decentralized Identifiers and Verifiable Credentials](https://arxiv.org/abs/2511.02841)
*Sandro Rodriguez Garzon,Awid Vaziry,Enis Mert Kuzu,Dennis Enrique Gehrmann,Buse Varkan,Alexander Gaballa,Axel Küpper*

Main category: cs.CR

TL;DR: 本文提出了一个基于自管理数字身份的多智能体信任建立框架，通过去中心化标识符和可验证凭证实现跨域信任关系。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI智能体在对话开始时缺乏自动建立细粒度信任的技术手段，而跨组织边界的自主互操作信任建立是智能体在非隔离环境中运行的基本前提。

Method: 为每个智能体配备自管理数字身份，结合账本锚定的去中心化标识符和第三方颁发的可验证凭证，使智能体能够在对话开始时证明身份所有权并通过交换凭证建立跨域信任关系。

Result: 原型系统评估证明了技术可行性，但也揭示了当智能体的LLM单独控制安全程序时的局限性。

Conclusion: 自管理数字身份与可验证凭证的结合为AI智能体提供了有效的跨域信任建立机制，但需要进一步解决LLM单独控制安全程序的问题。

Abstract: LLM-based AI agents still lack the technical means to automatically build
nuanced and differentiated trust in other agents at the beginning of an
agent-to-agent dialogue. But autonomous and interoperable trust establishing
becomes a fundamental prerequisite once agents start to operate beyond isolated
environments and engage in dialogues across individual or organizational
boundaries. A promising way to fill this gap in Agentic AI is to equip agents
with long-lived digital identities and introduce tamper-proof and flexible
identity-bound attestations of agents, provisioned by commonly trusted third
parties and designed for cross-domain verifiability. This article presents a
conceptual framework and a prototypical multi-agent system, where each agent is
endowed with a self-sovereign digital identity. It combines a unique and
ledger-anchored Decentralized Identifier (DID) of an agent with a set of
third-party issued Verifiable Credentials (VCs). This enables agents at the
start of a dialog to prove ownership of their self-controlled DIDs for
authentication purposes and to establish various cross-domain trust
relationships through the spontaneous exchange of their self-hosted DID-bound
VCs. A comprehensive evaluation of the prototypical implementation demonstrates
technical feasibility but also reveals limitations once an agent's LLM is in
sole charge to control the respective security procedures.

</details>


### [25] [PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat](https://arxiv.org/abs/2511.02993)
*Yixuan Gao,Tanvir Ahmed,Zekun Chang,Thijs Roumen,Rajalakshmi Nandakumar*

Main category: cs.CR

TL;DR: PrivyWave是一个基于密钥的物理混淆系统，通过生成密码学控制的伪心跳信号来保护无线感知隐私，允许授权设备准确监测而阻止未授权设备。


<details>
  <summary>Details</summary>
Motivation: 现有隐私解决方案要么完全阻止所有感知系统（牺牲实用性），要么在数据收集后操作（无法实现选择性访问），需要一种能在物理层实现选择性隐私保护的方法。

Method: 使用密码学确定的频率生成受控的伪心跳信号，未授权传感器接收真实和伪信号的混合信号，授权传感器使用密钥过滤伪信号以恢复准确测量。

Result: 评估显示：毫米波雷达中，未授权传感器平均绝对误差21.3 BPM，授权传感器仅5.8 BPM；声学感知中，未授权误差42.0 BPM，授权传感器9.7 BPM。系统在不同距离、方向和环境下均表现稳健。

Conclusion: 物理层混淆是实现普适健康监测中选择性隐私保护的可行方法，无需针对不同感知模式进行定制，并提供密码学混淆保证。

Abstract: Wireless sensing technologies can now detect heartbeats using radio frequency
and acoustic signals, raising significant privacy concerns. Existing privacy
solutions either protect from all sensing systems indiscriminately preventing
any utility or operate post-data collection, failing to enable selective access
where authorized devices can monitor while unauthorized ones cannot. We present
a key-based physical obfuscation system, PrivyWave, that addresses this
challenge by generating controlled decoy heartbeat signals at
cryptographically-determined frequencies. Unauthorized sensors receive a
mixture of real and decoy signals that are indistinguishable without the secret
key, while authorized sensors use the key to filter out decoys and recover
accurate measurements. Our evaluation with 13 participants demonstrates
effective protection across both sensing modalities: for mmWave radar,
unauthorized sensors show 21.3 BPM mean absolute error while authorized sensors
maintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error
increases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system
operates across multiple sensing modalities without per-modality customization
and provides cryptographic obfuscation guarantees. Performance benchmarks show
robust protection across different distances (30-150 cm), orientations
(120{\deg} field of view), and diverse indoor environments, establishing
physical-layer obfuscation as a viable approach for selective privacy in
pervasive health monitoring.

</details>


### [26] [Bayesian Advantage of Re-Identification Attack in the Shuffle Model](https://arxiv.org/abs/2511.03213)
*Pengcheng Su,Haibo Cheng,Ping Wang*

Main category: cs.CR

TL;DR: 本文首次系统研究了混洗模型下的贝叶斯优势，分析了在混洗数据中重新识别用户消息的成功概率，并建立了与总变差距离的紧密联系，最后给出了差分隐私混洗模型中贝叶斯攻击成功概率的上界。


<details>
  <summary>Details</summary>
Motivation: 混洗模型在密码学和差分隐私中广泛应用，但对其在贝叶斯攻击下的安全性缺乏系统研究。本文旨在填补这一空白，分析混洗模型中重新识别用户消息的贝叶斯优势。

Method: 定义了基本设置：从分布P抽取1个样本，从分布Q抽取n-1个样本，然后混洗所有样本。定义了贝叶斯最优攻击者的成功概率β_n(P,Q)，以及加性和乘性贝叶斯优势。推导了精确解析表达式和渐近特征，并在多个代表性场景中进行了评估。

Result: 建立了加性贝叶斯优势与总变差距离之间的（近乎）紧致互界。在差分隐私混洗模型中，当n个用户通过ε-差分隐私本地随机化器处理后混洗输出时，攻击者成功重新识别任何目标用户消息的概率最多为e^ε/n。

Conclusion: 本文为混洗模型的安全性提供了首个系统性的贝叶斯分析框架，建立了贝叶斯优势与统计距离的定量关系，并为差分隐私混洗模型的安全性提供了理论保证。

Abstract: The shuffle model, which anonymizes data by randomly permuting user messages,
has been widely adopted in both cryptography and differential privacy. In this
work, we present the first systematic study of the Bayesian advantage in
re-identifying a user's message under the shuffle model. We begin with a basic
setting: one sample is drawn from a distribution $P$, and $n - 1$ samples are
drawn from a distribution $Q$, after which all $n$ samples are randomly
shuffled. We define $\beta_n(P, Q)$ as the success probability of a
Bayes-optimal adversary in identifying the sample from $P$, and define the
additive and multiplicative Bayesian advantages as $\mathsf{Adv}_n^{+}(P, Q) =
\beta_n(P,Q) - \frac{1}{n}$ and $\mathsf{Adv}_n^{\times}(P, Q) = n \cdot
\beta_n(P,Q)$, respectively.
  We derive exact analytical expressions and asymptotic characterizations of
$\beta_n(P, Q)$, along with evaluations in several representative scenarios.
Furthermore, we establish (nearly) tight mutual bounds between the additive
Bayesian advantage and the total variation distance.
  Finally, we extend our analysis beyond the basic setting and present, for the
first time, an upper bound on the success probability of Bayesian attacks in
shuffle differential privacy. Specifically, when the outputs of $n$ users--each
processed through an $\varepsilon$-differentially private local randomizer--are
shuffled, the probability that an attacker successfully re-identifies any
target user's message is at most $e^{\varepsilon}/n$.

</details>


### [27] [LaMoS: Enabling Efficient Large Number Modular Multiplication through SRAM-based CiM Acceleration](https://arxiv.org/abs/2511.03341)
*Haomin Li,Fangxin Liu,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.CR

TL;DR: LaMoS是一种高效的SRAM基内存计算设计，用于大数模乘运算，解决了现有方案在主流密码算法中高比特位宽需求不足和扩展成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有内存计算方案主要关注低比特位宽模乘，无法满足ECC和RSA等主流密码算法的高比特位宽需求；同时现有大数模乘方案依赖低效的内存逻辑操作，导致高扩展成本和延迟。

Method: 分析Barrett模乘方法，将高比特位宽工作负载映射到SRAM内存计算宏；开发高效的内存计算架构和数据流优化大数模乘；通过工作负载分组改进映射方案以提高可扩展性。

Result: 实验结果显示，LaMoS相比现有SRAM基内存计算设计实现了7.02倍的加速，并降低了高比特位宽的扩展成本。

Conclusion: LaMoS提供了一种高可扩展性和面积效率的大数模乘内存计算解决方案，有效解决了现有方案在性能和扩展性方面的局限性。

Abstract: Barrett's algorithm is one of the most widely used methods for performing
modular multiplication, a critical nonlinear operation in modern privacy
computing techniques such as homomorphic encryption (HE) and zero-knowledge
proofs (ZKP). Since modular multiplication dominates the processing time in
these applications, computational complexity and memory limitations
significantly impact performance. Computing-in-Memory (CiM) is a promising
approach to tackle this problem. However, existing schemes currently suffer
from two main problems: 1) Most works focus on low bit-width modular
multiplication, which is inadequate for mainstream cryptographic algorithms
such as elliptic curve cryptography (ECC) and the RSA algorithm, both of which
require high bit-width operations; 2) Recent efforts targeting large number
modular multiplication rely on inefficient in-memory logic operations,
resulting in high scaling costs for larger bit-widths and increased latency. To
address these issues, we propose LaMoS, an efficient SRAM-based CiM design for
large-number modular multiplication, offering high scalability and area
efficiency. First, we analyze the Barrett's modular multiplication method and
map the workload onto SRAM CiM macros for high bit-width cases. Additionally,
we develop an efficient CiM architecture and dataflow to optimize large-number
modular multiplication. Finally, we refine the mapping scheme for better
scalability in high bit-width scenarios using workload grouping. Experimental
results show that LaMoS achieves a $7.02\times$ speedup and reduces high
bit-width scaling costs compared to existing SRAM-based CiM designs.

</details>


### [28] [Death by a Thousand Prompts: Open Model Vulnerability Analysis](https://arxiv.org/abs/2511.03247)
*Amy Chang,Nicholas Conley,Harish Santhanalakshmi Ganesan,Adam Swanda*

Main category: cs.CR

TL;DR: 测试8个开源大语言模型的安全性和防御能力，发现多轮攻击成功率高达25.86%-92.78%，比单轮攻击高2-10倍，显示当前开源模型在持续对话中难以维持安全防护。


<details>
  <summary>Details</summary>
Motivation: 评估开源大语言模型在下游应用中的安全风险，识别可能影响微调和部署的漏洞，为开发者和研究人员提供安全参考。

Method: 使用自动化对抗测试，测量模型对单轮和多轮提示注入及越狱攻击的抵抗能力。

Result: 所有测试模型都存在普遍漏洞，多轮攻击成功率显著高于单轮攻击；能力导向模型（如Llama 3.3、Qwen 3）多轮易感性更高，安全导向模型（如Google Gemma 3）表现更均衡。

Conclusion: 开源模型虽促进创新，但部署时存在操作和道德风险，需要采用安全优先设计和分层保护措施，确保在企业和公共领域的可靠部署。

Abstract: Open-weight models provide researchers and developers with accessible
foundations for diverse downstream applications. We tested the safety and
security postures of eight open-weight large language models (LLMs) to identify
vulnerabilities that may impact subsequent fine-tuning and deployment. Using
automated adversarial testing, we measured each model's resilience against
single-turn and multi-turn prompt injection and jailbreak attacks. Our findings
reveal pervasive vulnerabilities across all tested models, with multi-turn
attacks achieving success rates between 25.86\% and 92.78\% -- representing a
$2\times$ to $10\times$ increase over single-turn baselines. These results
underscore a systemic inability of current open-weight models to maintain
safety guardrails across extended interactions. We assess that alignment
strategies and lab priorities significantly influence resilience:
capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher
multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma
3 exhibit more balanced performance.
  The analysis concludes that open-weight models, while crucial for innovation,
pose tangible operational and ethical risks when deployed without layered
security controls. These findings are intended to inform practitioners and
developers of the potential risks and the value of professional AI security
solutions to mitigate exposure. Addressing multi-turn vulnerabilities is
essential to ensure the safe, reliable, and responsible deployment of
open-weight LLMs in enterprise and public domains. We recommend adopting a
security-first design philosophy and layered protections to ensure resilient
deployments of open-weight models.

</details>


### [29] [Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs](https://arxiv.org/abs/2511.03271)
*Yize Liu,Yunyun Hou,Aina Sui*

Main category: cs.CR

TL;DR: 本文提出了一种基于动态加权图拓扑的多轮越狱攻击模型，并设计了ABC算法来高效搜索最优攻击路径，显著降低了攻击所需的查询次数。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型红队评估方法缺乏对攻击空间中成功对话轨迹的探索，且忽视了攻击过程中的巨大开销。

Method: 提出基于动态加权图拓扑的理论模型，将多轮攻击过程抽象为路径规划问题，并设计了增强的人工蜂群算法ABC，包含雇佣蜂、观察蜂和侦察蜂的协作搜索机制。

Result: 在三个开源和两个专有语言模型上的实证评估显示，攻击成功率均超过90%，在GPT-3.5-Turbo上达到98%，平均仅需26次查询，显著优于现有基线方法。

Conclusion: ABC算法在保持高攻击成功率的同时，大幅降低了红队评估的开销，展现了卓越的效率优势。

Abstract: Large Language Models (LLMs) have been widely deployed across various
applications, yet their potential security and ethical risks have raised
increasing concerns. Existing research employs red teaming evaluations,
utilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.
However, these approaches often lack exploration of successful dialogue
trajectories within the attack space, and they tend to overlook the
considerable overhead associated with the attack process. To address these
limitations, this paper first introduces a theoretical model based on
dynamically weighted graph topology, abstracting the multi-turn attack process
as a path planning problem. Based on this framework, we propose ABC, an
enhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a
collaborative search mechanism with employed, onlooker, and scout bees. This
algorithm significantly improves the efficiency of optimal attack path search
while substantially reducing the average number of queries required. Empirical
evaluations on three open-source and two proprietary language models
demonstrate the effectiveness of our approach, achieving attack success rates
above 90\% across the board, with a peak of 98\% on GPT-3.5-Turbo, and
outperforming existing baselines. Furthermore, it achieves comparable success
with only 26 queries on average, significantly reducing red teaming overhead
and highlighting its superior efficiency.

</details>


### [30] [Two thousand years of the oracle problem. Insights from Ancient Delphi on the future of blockchain oracles](https://arxiv.org/abs/2511.03319)
*Giulio Caldarelli,Massimiliano Ornaghi*

Main category: cs.CR

TL;DR: 本文通过比较古代德尔斐神谕与现代区块链预言机，建立了一个分析框架，探讨如何提高预言机信息的可靠性和真实性。


<details>
  <summary>Details</summary>
Motivation: 解决预言机问题——即如何确保从预言机获取的信息是真实且无偏见的，这一问题在古典时代和现代计算领域都存在。作者希望通过连接德尔斐神谕和区块链预言机，揭示两者之间的共性，为改进区块链预言机的可靠性提供新思路。

Method: 采用概念比较框架，结合区块链预言机分类法，对167个德尔斐查询进行词汇分析，研究问题类型与预言机答案质量之间的关系。

Result: 建立了古典预言机与计算预言机之间的比较框架，揭示了它们在信息可靠性方面的共同挑战，并分析了不同类型问题对预言机答案质量的影响。

Conclusion: 该研究为计算机科学文献提供了基于德尔斐神谕经验改进区块链预言机可靠性的策略，同时为古典文献提供了一个可用于解释和分类其他古代预言机制的分析框架。

Abstract: The oracle problem refers to the inability of an agent to know if the
information coming from an oracle is authentic and unbiased. In ancient times,
philosophers and historians debated on how to evaluate, increase, and secure
the reliability of oracle predictions, particularly those from Delphi, which
pertained to matters of state. Today, we refer to data carriers for automatic
machines as oracles, but establishing a secure channel between these oracles
and the real world still represents a challenge. Despite numerous efforts, this
problem remains mostly unsolved, and the recent advent of blockchain oracles
has added a layer of complexity because of the decentralization of blockchains.
This paper conceptually connects Delphic and modern blockchain oracles,
developing a comparative framework. Leveraging blockchain oracle taxonomy,
lexical analysis is also performed on 167 Delphic queries to shed light on the
relationship between oracle answer quality and question type. The presented
framework aims first at revealing commonalities between classical and
computational oracles and then at enriching the oracle analysis within each
field. This study contributes to the computer science literature by proposing
strategies to improve the reliability of blockchain oracles based on insights
from Delphi and to classical literature by introducing a framework that can
also be applied to interpret and classify other ancient oracular mechanisms.

</details>


### [31] [Federated Anonymous Blocklisting across Service Providers and its Application to Group Messaging](https://arxiv.org/abs/2511.03486)
*David Soler,Carlos Dafonte,Manuel Fernández-Veiga,Ana Fernández Vilas,Francisco J. Nóvoa*

Main category: cs.CR

TL;DR: 本文提出了一种联邦匿名封禁方案，用分布式领域替代集中式服务提供商，每个领域有自己的封禁列表，用户认证时需要证明自己不在任何信任领域的封禁列表中。


<details>
  <summary>Details</summary>
Motivation: 即时通讯中需要防止不良用户通过创建新假名来规避封禁，现有匿名封禁方案存在性能依赖封禁列表大小的问题。

Method: 设计联邦匿名封禁方案，将集中式服务提供商替换为小型分布式领域，领域间建立信任关系，用户认证时需证明不在任何信任领域的封禁列表中。

Result: 实现了该方案，性能不依赖当前封禁列表大小，也不需要处理封禁列表的新增条目，并成功集成到消息层安全协议中。

Conclusion: 联邦匿名封禁方案解决了现有匿名封禁方案的性能问题，适用于现实世界的消息群组场景。

Abstract: Instant messaging has become one of the most used methods of communication
online, which has attracted significant attention to its underlying
cryptographic protocols and security guarantees. Techniques to increase privacy
such as End-to-End Encryption and pseudonyms have been introduced. However,
online spaces such as messaging groups still require moderation to prevent
misbehaving users from participating in them, particularly in anonymous
contexts.. In Anonymous Blocklisting (AB) schemes, users must prove during
authentication that none of their previous pseudonyms has been blocked,
preventing misbehaving users from creating new pseudonyms. In this work we
propose an alternative \textit{Federated Anonymous Blocklisting} (FAB) in which
the centralised Service Provider is replaced by small distributed Realms, each
with its own blocklist. Realms can establish trust relationships between each
other, such that when users authenticate to a realm, they must prove that they
are not banned in any of its trusted realms. We provide an implementation of
our proposed scheme; unlike existing AB constructions, the performance of ours
does not depend on the current size of the blocklist nor requires processing
new additions to the blocklist. We also demonstrate its applicability to
real-world messaging groups by integrating our FAB scheme into the Messaging
Layer Security protocol.

</details>


### [32] [Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology](https://arxiv.org/abs/2511.03641)
*Thomas Souverain*

Main category: cs.CR

TL;DR: 该论文为响应欧盟AI法案对通用AI模型输出标记的要求，提出了一个评估水印技术的框架，包括分类、评估和比较现有方法，发现目前尚无方法完全满足所有四个标准。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案要求通用AI模型提供商标记和检测其输出，但现有水印技术评估标准不明确。论文旨在将法案的四个标准（可靠性、互操作性、有效性和鲁棒性）转化为具体的可测量评估方法。

Method: 提出三方面贡献：(1) 根据LLM生命周期阶段对水印方法进行分类；(2) 将欧盟标准映射到现有的水印评估指标，并为互操作性提出三个规范性维度；(3) 比较现有水印方法与欧盟标准的符合程度。

Result: 比较显示目前尚无水印方法能同时满足所有四个欧盟标准。研究发现直接嵌入LLM底层架构的水印方法具有潜力。

Conclusion: 建议进一步研究直接嵌入LLM底层架构的水印技术，以更好地满足欧盟AI法案的要求。

Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union,
the AI Act requires providers to mark and detect the outputs of their
general-purpose models. The Article 50 and Recital 133 call for marking methods
that are ''sufficiently reliable, interoperable, effective and robust''. Yet,
the rapidly evolving and heterogeneous landscape of watermarks for Large
Language Models (LLMs) makes it difficult to determine how these four standards
can be translated into concrete and measurable evaluations. Our paper addresses
this challenge, anchoring the normativity of European requirements in the
multiplicity of watermarking techniques. Introducing clear and distinct
concepts on LLM watermarking, our contribution is threefold. (1) Watermarking
Categorisation: We propose an accessible taxonomy of watermarking methods
according to the stage of the LLM lifecycle at which they are applied - before,
during, or after training, and during next-token distribution or sampling. (2)
Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping
each criterion with state-of-the-art evaluations on robustness and
detectability of the watermark, and of quality of the LLM. Since
interoperability remains largely untheorised in LLM watermarking research, we
propose three normative dimensions to frame its assessment. (3) Watermarking
Comparison: We compare current watermarking methods for LLMs against the
operationalised European criteria and show that no approach yet satisfies all
four standards. Encouraged by emerging empirical tests, we recommend further
research into watermarking directly embedded within the low-level architecture
of LLMs.

</details>
