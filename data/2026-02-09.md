<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.DC](#cs.DC) [Total: 13]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs](https://arxiv.org/abs/2602.06252)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: D-Legion是一种新型可扩展多核架构，专为加速量化大语言模型中的矩阵乘法而设计，通过自适应精度脉动阵列核心实现高性能计算。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要大量计算和内存资源，量化模型虽能减少需求但需要专门架构来加速。现有架构在处理量化LLM时效率不足，需要新的硬件设计来优化性能。

Method: 提出D-Legion架构，包含多个Legion单元，每个Legion包含一组自适应精度脉动阵列。支持量化稀疏和稠密矩阵乘法，利用块结构化稀疏性，通过并行累加器减少部分和的内存访问，通过多播优化数据重用。

Result: 在BitNet模型的注意力工作负载上，D-Legion相比最先进工作实现了8.2倍延迟降低、3.8倍内存节省和3倍部分和内存节省。8个Legion版本达到135.68 TOPS峰值吞吐量，32个Legion版本相比Google TPUv4i实现2.5倍延迟降低、2.3倍吞吐量提升和2.7倍内存节省。

Conclusion: D-Legion架构通过自适应精度脉动阵列和优化的调度技术，为量化大语言模型提供了高效的可扩展硬件加速方案，在性能、内存效率和可扩展性方面都表现出显著优势。

Abstract: The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\times$ lower latency, up to 3.8$\times$ higher memory savings, and up to 3$\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\times$ lower total latency, up to 2.3$\times$ higher total throughput, and up to 2.7$\times$ higher total memory savings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架通过最优预算拒绝采样减少rollout模型与策略分布不匹配，实现LLM强化学习中rollout生成与策略优化的解耦，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习成本高昂，主要因为rollout过程开销大。将rollout生成与策略优化解耦（如使用更高效的模型进行rollout）可以显著提升效率，但这会引入严重的分布不匹配问题，导致学习不稳定。

Method: 提出Jackpot框架，采用最优预算拒绝采样直接减少rollout模型与演化策略之间的分布差异。框架包含：1）原则性的OBRS过程；2）联合更新策略和rollout模型的统一训练目标；3）通过top-k概率估计和批次级偏差校正实现的高效系统实现。

Result: 理论分析表明OBRS在可控接受预算下持续将rollout分布向目标分布移动。实证显示，相比重要性采样基线，Jackpot显著提升训练稳定性，在Qwen3-8B-Base模型上训练300个更新步骤（批次大小64）时，性能与on-policy RL相当。

Conclusion: 基于OBRS的对齐方法使LLM强化学习中rollout生成与策略优化的解耦更接近实用和有效，为解决RL训练效率问题提供了有前景的方向。

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [3] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 该论文首次对LLM推理失败进行全面调查，提出了新的分类框架，将推理分为具身与非具身类型，并将推理失败分为基础架构失败、应用特定限制和鲁棒性问题三类，为理解LLM系统性弱点提供了结构化视角。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM展现出卓越的推理能力，但在看似简单的场景中仍存在显著的推理失败。为了系统性地理解和解决这些缺陷，需要对这些失败进行全面的调查和分类。

Method: 提出了一个新颖的分类框架：将推理分为具身推理和非具身推理，后者进一步细分为非正式（直觉）推理和正式（逻辑）推理。同时，将推理失败分为三类：基础架构失败、应用特定限制和鲁棒性问题。对每种失败提供明确定义、分析现有研究、探索根本原因并提出缓解策略。

Result: 创建了首个全面的LLM推理失败调查，提供了结构化视角来理解LLM的系统性弱点。同时发布了GitHub资源库（Awesome-LLM-Reasoning-Failures），收集了相关研究工作，为该领域提供了便捷的入门点。

Conclusion: 该调查统一了分散的研究工作，为理解LLM推理的系统性弱点提供了有价值的见解，并指导未来研究朝着构建更强大、更可靠和更鲁棒的推理能力方向发展。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [4] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT逻辑框架的非马尔可夫奖励规范方法，用于大规模状态空间的MDP，通过奖励机器和HER解决奖励稀疏性问题


<details>
  <summary>Details</summary>
Motivation: 传统逻辑规范在处理大规模状态空间和非结构化数据时表达能力有限，需要手动编码谓词，缺乏统一可重用框架

Method: 使用LTLfMT（线性时序逻辑模理论）扩展经典时序逻辑，提出可处理片段，结合奖励机器和Hindsight Experience Replay（HER）方法

Result: 在连续控制环境中使用非线性算术理论验证了方法的有效性，实验表明定制的HER实现对于解决复杂目标任务至关重要

Conclusion: LTLfMT框架为大规模状态空间MDP中的复杂任务提供了自然规范能力，结合HER方法能有效处理奖励稀疏性问题

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [5] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个用于评估大型推理模型的图算法基准测试，通过9个任务揭示LRMs在长上下文推理中的两大弱点：准确率随节点数增加而急剧下降，以及过度思考现象导致的无效自我验证。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准存在局限性：缺乏长上下文评估、挑战性不足、答案难以程序化验证。需要一个新的基准来更全面地评估大型推理模型的推理能力。

Method: 提出GrAlgoBench基准，使用图算法问题评估LRMs。图算法问题特别适合测试推理能力：需要长上下文推理、可精细控制难度、支持标准化程序化评估。包含9个任务进行系统实验。

Result: 实验揭示了当前LRMs的两大主要弱点：1) 准确率随上下文长度增加而急剧下降，当图超过120个节点时准确率低于50%，主要由频繁执行错误、弱记忆和冗余推理导致；2) 存在过度思考现象，主要由广泛但低效的自我验证引起，增加了推理痕迹但没有提高正确性。

Conclusion: GrAlgoBench通过暴露这些限制，确立了图算法问题作为推进LRMs推理研究的严谨、多维且实际相关的测试平台。代码已开源。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [6] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: 本文提出DEPO框架，通过动态难度估计筛选训练数据，减少低效样本的rollout计算开销，在保持模型性能的同时将rollout成本降低2倍。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在遇到过于简单或复杂的问题时，梯度信号会衰减甚至消失，导致收敛不稳定。虽然DAPO等变体尝试解决梯度消失问题，但无法缓解在低效用样本上进行大量rollout带来的巨大计算开销。

Method: 提出Difficulty-Estimated Policy Optimization (DEPO)框架，集成了在线难度估计器，在rollout阶段前动态评估和筛选训练数据，优先将计算资源分配给具有高学习潜力的样本。

Result: 实验结果表明，DEPO在不影响模型性能的情况下，将rollout成本降低了最多2倍，显著降低了训练高性能推理模型的计算门槛。

Conclusion: DEPO为推理对齐提供了更高效和鲁棒的优化框架，通过智能资源分配降低了计算成本，为推理模型的规模化训练提供了更可持续的路径。

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [7] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: QA-Token是一种质量感知的分词方法，通过双层优化、强化学习和自适应参数学习，将数据可靠性直接纳入词汇表构建，在基因组学和金融领域显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法处理序列数据时未考虑信号质量，限制了其在噪声现实世界语料库中的有效性。需要一种能够直接纳入数据可靠性的分词方法

Method: 提出了QA-Token（质量感知分词），包含三个关键贡献：1）双层优化公式，联合优化词汇表构建和下游性能；2）强化学习方法，通过质量感知奖励学习合并策略，具有收敛保证；3）通过Gumbel-Softmax松弛的自适应参数学习机制，实现端到端优化

Result: 实验评估显示一致改进：基因组学（变异检测F1分数比BPE提高6.7个百分点）、金融（夏普比率提高30%）。在基础模型规模上，对1.7万亿碱基对的预训练语料进行分词，实现最先进的病原体检测（94.53 MCC），同时减少15%的token数量

Conclusion: QA-Token解锁了噪声现实世界语料库（包括petabases级别的基因组序列和terabytes级别的金融时间序列），用于基础模型训练，且推理开销为零

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [8] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: 论文提出自回归生成存在内在稳定性极限，导致大语言模型在长时程任务中性能急剧下降，这源于过程级不稳定性而非单纯的任务复杂度，需要离散分段和图状执行结构来维持稳定推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长时程任务中性能急剧下降，传统解释主要归因于任务复杂度，但作者认为这些解释不完整，即使在无分支的线性任务中，自回归执行也存在内在稳定性极限。

Method: 提出自回归生成的过程级不稳定性是长时程推理的根本约束，推导定理A显示单路径自回归推理中的决策优势随执行长度呈指数衰减，并通过合成环境和真实TextWorld任务进行实证研究。

Result: 实证研究显示可观察到的性能悬崖与理论预测一致，揭示了长时程推理失败的动力系统视角，并表明短时程评估协议可能掩盖结构不稳定性。

Conclusion: 长时程推理需要离散分段，自然诱导出有向无环图等图状执行结构，未来推理系统可能需要从单纯扩展转向结构化治理。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [9] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文系统研究了4B参数规模的边缘模型训练，提出了AgentCPM-Explore模型，通过参数融合、奖励去噪和上下文优化解决了边缘模型的三个瓶颈，在多项基准测试中达到或超越了更大规模模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体系统过度依赖大规模模型，而边缘规模模型（4B参数级别）的能力尚未得到充分探索。本文旨在系统研究边缘规模智能体模型的训练，解决其性能瓶颈。

Method: 提出AgentCPM-Explore模型，采用整体训练框架：1）参数空间模型融合解决SFT中的灾难性遗忘；2）奖励信号去噪解决RL中的噪声敏感问题；3）上下文信息精炼解决长上下文场景中的推理退化问题。

Result: AgentCPM-Explore在4B级别模型中达到SOTA性能，在四个基准测试中匹配或超越了8B级别SOTA模型，在五个基准测试中甚至超越了Claude-4.5-Sonnet或DeepSeek-v3.2等更大规模模型。在GAIA文本任务中达到97.09%准确率（pass@64）。

Conclusion: 边缘规模模型的瓶颈并非其内在能力上限，而是推理稳定性问题。通过建立的训练框架，AgentCPM-Explore有效释放了边缘规模模型被低估的潜力，证明了小规模模型在智能体任务中的强大能力。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [10] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER是一种用于专家混合模型多路径解码的训练免费在线控制策略，通过动态扩展-缩减控制重新分配计算资源，在固定预算下实现更好的探索-利用平衡，显著提升推理准确性并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索-利用权衡上存在局限：树结构搜索通过脆弱的扩展规则硬编码探索，干扰后训练推理；并行推理则过度探索冗余假设路径且依赖弱答案选择。研究发现最优平衡是阶段依赖的，正确和错误推理路径往往在后期才分叉。

Method: 将测试时扩展重新表述为假设池上的动态扩展-缩减控制问题。提出HyPER：包含在线控制器（随假设池演化从探索转向利用）、令牌级精炼机制（无需完整路径重采样即可实现高效生成时利用）、长度和置信度感知聚合策略（可靠答案时利用）。

Result: 在四个专家混合语言模型和多样化推理基准上的实验表明，HyPER始终实现更优的准确性-计算权衡，准确性提高8-10%，同时令牌使用量减少25-40%。

Conclusion: HyPER通过动态控制策略有效解决了多路径推理中的探索-利用权衡问题，在固定计算预算下显著提升了推理性能，为测试时计算扩展提供了更灵活高效的解决方案。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [11] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 本文分析了强化学习在大型语言模型智能体训练中的收敛性问题，发现现有主流RL算法在多轮交互场景中缺乏收敛保证，提出了具有收敛保证的无评论家方法SeeUPO。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM智能体训练方法在多轮交互场景中缺乏验证的收敛保证，导致训练不稳定和无法收敛到最优策略，需要设计具有收敛保证的RL算法。

Method: 提出SeeUPO（序列级顺序更新策略优化），将多轮交互建模为顺序执行的多臂赌博机问题，通过反向执行顺序的逐轮顺序策略更新，确保单调改进并通过反向归纳收敛到全局最优解。

Result: 在AppWorld和BFCL v4基准测试中，SeeUPO相比现有骨干算法取得显著提升：在Qwen3-14B上相对增益43.3%-54.6%，在Qwen2.5-14B上相对增益24.1%-41.9%（跨基准平均），同时表现出更优的训练稳定性。

Conclusion: SeeUPO解决了多轮交互场景中RL算法的收敛性问题，提供了一种具有收敛保证的无评论家方法，显著提升了LLM智能体的训练效果和稳定性。

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [12] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型（VLMs）的鲁棒性评估存在缺陷，仅关注输出稳定性不足。通过提出表示感知和频率感知的评估框架，揭示了三种失效模式：内部表示漂移、规模不改善鲁棒性、以及扰动对不同任务的差异化影响。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的鲁棒性评估主要依赖输出层面的不变性，隐含假设稳定预测反映稳定的多模态处理过程。然而这种假设存在不足，需要更深入评估模型内部表示的变化。

Method: 提出表示感知和频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉token的空间一致性），并结合标准基于标签的指标。在SEEDBench、MMMU和POPE数据集上对现代VLMs进行评估。

Result: 揭示了三种失效模式：1）模型经常保持预测答案不变，但内部表示发生显著漂移，某些扰动下漂移接近图像间变异的幅度；2）鲁棒性不随模型规模提升，更大模型准确率更高但敏感性相同或更大；3）扰动对不同任务影响不同：破坏推理任务中粗细视觉线索的组合，但在幻觉基准测试中可能减少误报。

Conclusion: 仅依赖输出不变性评估VLMs鲁棒性存在严重缺陷，需要更全面的表示层面评估。模型内部表示可能在扰动下发生显著变化，而更大模型虽然准确率更高但鲁棒性并未改善，表明需要新的鲁棒性评估方法。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [13] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK是一种自回归知识图谱生成模型，将图谱视为三元组序列进行生成，无需显式规则监督即可学习语义约束，在IntelliGraphs基准上达到89.2%-100%的语义有效性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组间的复杂语义依赖关系，同时保持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕捉整个子图的相互依赖关系以产生语义一致的结构。

Method: 提出ARK（自回归知识图谱生成）模型家族，将图谱视为(head, relation, tail)三元组序列进行自回归生成。模型直接从数据中学习隐式语义约束（类型一致性、时间有效性、关系模式），无需显式规则监督。还提出SAIL，ARK的变分扩展，通过学习的潜在表示支持受控生成。

Result: 在IntelliGraphs基准上，模型在多样化数据集上达到89.2%到100.0%的语义有效性，同时生成训练中未见的新图谱。分析显示模型容量（隐藏维度≥64）比架构深度对KG生成更重要，循环架构在达到与基于Transformer的替代方案相当有效性的同时提供显著计算效率。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答方面具有实际应用价值。模型容量比架构深度更关键，循环架构在保持有效性的同时提供计算优势。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [14] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 该论文提出了一种基于语义LTL到自动机转换的多任务强化学习方法，通过语义标记的自动机提取任务嵌入来调节策略，实现了对复杂LTL规范的高效处理。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中，现有方法在处理复杂的线性时序逻辑（LTL）规范时存在困难，需要一种能够高效处理完整LTL规范并提取丰富任务表示的方法。

Method: 提出了一种新颖的任务嵌入技术，利用新一代语义LTL到自动机的转换方法，生成语义标记的自动机，从中提取表达性强的任务嵌入来调节策略，支持完整的LTL规范。

Result: 实验结果表明，该方法在多个领域中实现了最先进的性能，能够扩展到复杂规范，而现有方法在这些复杂规范下会失败。

Conclusion: 基于语义LTL到自动机转换的任务嵌入技术为多任务强化学习提供了一种有效的解决方案，能够处理复杂的LTL规范并实现良好的泛化性能。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [15] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 研究对比了理性主动学习器（最大化期望信息增益）和人类式积极测试策略在概念学习中的表现，发现EIG在复杂规则学习中有效，但在简单概念上表现不佳，而PTS虽信息次优但能保持假设有效性，更快收敛于简单规则。


<details>
  <summary>Details</summary>
Motivation: 研究人类概念学习中的主动学习平衡问题：如何在查询的信息量和学习者生成假设的稳定性之间取得平衡。探讨"确认偏误"是否可能是一种认知错误，还是维持可处理推理的理性适应策略。

Method: 采用神经符号贝叶斯学习器，假设由大型语言模型生成的可执行程序组成，通过贝叶斯更新重新加权。比较两种策略：理性主动学习器（最大化近似期望信息增益EIG）和人类式积极测试策略（查询当前最佳假设预测为正的实例）。在经典数字游戏中进行概念学习任务测试。

Result: EIG在需要证伪的复杂规则（如复合规则或包含例外的规则）中有效，但在简单概念上表现不佳。失败原因是EIG策略与LLM提议分布之间的支持不匹配：高度诊断性的边界查询将后验推向生成器产生无效或过于具体程序的区域，导致粒子近似中的支持不匹配陷阱。PTS虽信息次优但倾向于通过选择"安全"查询来保持提议有效性，在简单规则上收敛更快。

Conclusion: "确认偏误"可能不是认知错误，而是在人类思维特有的稀疏、开放式假设空间中维持可处理推理的理性适应策略。积极测试策略通过选择安全查询来保持假设有效性，在简单概念学习中具有优势。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [16] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见多轮工具使用基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通用智能体需要交互式环境进行自我探索，但当前交互环境严重稀缺，现有合成方法在环境多样性和可扩展性方面存在显著限制。

Method: 提出ScaleEnv框架，从零开始构建完全交互式环境和可验证任务；通过程序化测试确保环境可靠性；通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准（如τ²-Bench和VitaBench）上表现出显著性能提升，展示了强大的泛化能力；实证研究表明增加领域数量对模型泛化性能有重要影响。

Conclusion: 扩展环境多样性对于鲁棒的智能体学习至关重要，ScaleEnv为解决交互环境稀缺问题提供了有效框架，能够显著提升智能体在复杂任务中的泛化能力。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [17] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: POP是一种高效的在线结构化剪枝框架，能够在自回归token生成过程中实现上下文条件化的动态剪枝，最小化计算开销


<details>
  <summary>Details</summary>
Motivation: 大型基础模型通过扩展获得强大性能，但当前的结构化剪枝方法在推理时采用固定的剪枝决策，忽略了自回归token生成过程中出现的稀疏模式

Method: POP将模型通道划分为保留区、候选区和剪枝区，预填充阶段定义粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重新评估

Result: 在多种大型基础模型（包括LLMs、MoEs和VLMs）上的广泛评估表明，POP比现有剪枝方法提供更高准确性，同时产生更小的计算开销并最小化推理延迟

Conclusion: POP是一种轻量级、即插即用的方法，无需预处理（包括离线校准、重新训练或学习预测器），能够实现上下文条件化的动态剪枝

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [18] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 提出自适应差分隐私联邦学习框架，通过客户端轻量压缩模块、服务器自适应梯度裁剪和约束感知聚合机制，解决异构数据和隐私约束下的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在实际部署中面临设备异构性、非独立同分布数据导致的梯度更新不稳定和偏差问题，差分隐私的固定梯度裁剪和高斯噪声注入会进一步放大梯度扰动，导致训练震荡和性能下降。

Method: 1. 客户端引入轻量级本地压缩模块，正则化中间表示并约束梯度变异性；2. 服务器采用自适应梯度裁剪策略，基于历史更新统计动态调整裁剪阈值；3. 设计约束感知聚合机制，抑制不可靠或噪声主导的客户端更新。

Result: 在CIFAR-10和SVHN数据集上的大量实验表明，该框架提高了收敛稳定性和分类准确率。

Conclusion: 提出的自适应差分隐私联邦学习框架能有效解决异构和隐私约束环境下的训练不稳定问题，提升模型效率和性能。

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [19] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该研究比较了静态预测解释方法与智能体系统轨迹诊断方法，发现传统特征归因方法在静态分类中有效，但无法可靠诊断智能体执行失败，而基于轨迹的诊断方法能更有效地定位行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，AI系统从静态预测转向多步骤决策的智能体系统。传统可解释AI方法主要针对静态模型预测，而智能体系统的成功与失败由决策序列决定，需要研究传统解释方法如何适应智能体环境。

Method: 通过实证比较静态分类任务中的归因解释方法与智能体基准测试（TAU-bench Airline和AssistantBench）中的轨迹诊断方法，区分两种解释方法的适用性。

Result: 归因方法在静态设置中特征排序稳定（Spearman ρ=0.86），但无法可靠诊断智能体轨迹中的执行级故障。基于轨迹的评估方法能一致定位行为故障，发现状态跟踪不一致性在失败运行中高2.7倍，并使成功概率降低49%。

Conclusion: 研究结果表明需要从静态解释转向轨迹级可解释性，以更好地评估和诊断自主AI系统的行为，特别是在智能体系统中。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [20] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AIRS-Bench是一个用于评估LLM智能体在科学研究全生命周期能力的基准测试套件，包含20个来自前沿机器学习论文的任务，涵盖多个领域，旨在加速自主科学研究的发展。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在科学研究中具有巨大潜力，但缺乏全面评估其在完整研究生命周期中能力的基准测试。为了加速这一领域的发展，需要建立一个能够评估智能体从想法生成到实验分析和迭代改进全过程的基准测试。

Method: 从最先进的机器学习论文中选取20个任务，涵盖语言建模、数学、生物信息学和时间序列预测等多个领域。任务格式灵活，便于集成新任务和比较不同智能体框架。使用前沿模型配合顺序和并行架构建立基线。

Result: 智能体在4个任务中超过了人类SOTA水平，但在16个任务中未能达到人类水平。即使智能体超越了人类基准，也未能达到底层任务的理论性能上限。这表明AIRS-Bench远未饱和，仍有很大的改进空间。

Conclusion: AIRS-Bench是一个远未饱和的基准测试，为自主科学研究提供了重要的评估框架。开源任务定义和评估代码将促进该领域的进一步发展，智能体在科学研究方面仍有巨大的提升潜力。

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [The Avatar Cache: Enabling On-Demand Security with Morphable Cache Architecture](https://arxiv.org/abs/2602.06433)
*Anubhav Bhatla,Navneet Navneet,Moinuddin Qureshi,Biswabandan Panda*

Main category: cs.CR

TL;DR: 提出Avatar缓存设计，支持三种模式动态切换：非安全模式、随机化安全模式和分区安全模式，在保持传统缓存结构的同时实现强安全性


<details>
  <summary>Details</summary>
Motivation: 现代处理器仍使用不安全的组相联LLC，现有安全设计要么存储开销大（随机化方案），要么性能损失严重（分区方案），阻碍工业采用。需要一种既能提供强安全性又对传统缓存改动最小的方案。

Method: 提出Avatar可变形LLC，支持三种模式动态切换：Avatar-N（非安全）、Avatar-R（随机化安全）和Avatar-P（分区安全）。Avatar-R通过增加无效条目和高相联度提供安全性；Avatar-P采用分区设计防御两种攻击。

Result: Avatar-R仅需1.5%存储开销，静态功耗增加2.7%，性能仅降低0.2%，安全性极高（每10^30年才发生一次组相联驱逐）。Avatar-P性能开销仅3%，显著优于现有分区方案。整体设计接近传统缓存，便于工业采用。

Conclusion: Avatar在保持传统缓存结构的同时实现了强LLC安全性，支持按需切换安全模式，平衡了安全、性能和工业可行性，为安全缓存设计提供了实用解决方案。

Abstract: The sharing of the last-level cache (LLC) among multiple cores makes it vulnerable to cross-core conflict- and occupancy-based attacks. Despite extensive prior work, modern processors still employ non-secure set-associative LLCs. Existing secure LLC designs broadly fall into two categories: (i) randomized and (ii) partitioned. The state-of-the-art randomized design, Mirage, mitigates conflict-based attacks but incurs significant area overhead (20% additional storage) and design complexity. Partitioned LLCs mitigate both conflict- and occupancy-based attacks, but often suffer from large performance overheads (on average over 5% and up to 49%), require OS support in set-based schemes, or face scalability issues in way-based schemes. These factors pose major obstacles to the industrial adoption of secure LLCs. This paper asks whether strong LLC security can be achieved with minimal changes to a conventional set-associative LLC, enabling security only when needed while preserving low performance, power, and area overheads. We propose Avatar, a secure and morphable LLC that supports three modes: non-secure (Avatar-N), randomized secure (Avatar-R), and partitioned secure (Avatar-P), and can switch dynamically between them. Avatar closely resembles a conventional set-associative LLC, facilitating industrial adoption. Avatar-R introduces extra invalid entries and leverages high associativity to provide a strong security guarantee with little capacity loss, achieving only one set-associative eviction per $10^{30}$ years, while incurring 1.5% storage overhead, a 2.7% increase in static power, and a 0.2% slowdown over a 16~MB baseline. Avatar-P mitigates both conflict- and occupancy-based attacks with only a 3% performance overhead, substantially outperforming prior way-based partitioned LLCs. When security is unnecessary, Avatar switches to Avatar-N to maximize performance and energy efficiency.

</details>


### [22] [Know Your Scientist: KYC as Biosecurity Infrastructure](https://arxiv.org/abs/2602.06172)
*Jonathan Feldman,Tal Feldman,Annie I Anton*

Main category: cs.CR

TL;DR: 该论文提出基于金融反洗钱KYC原则的三层生物AI安全框架，从内容审查转向用户验证与监控，以应对蛋白质设计AI的双重用途风险。


<details>
  <summary>Details</summary>
Motivation: 当前基于关键词过滤、输出筛查和内容访问限制的生物AI安全措施存在根本缺陷，无法可靠预测蛋白质功能，且新型威胁会刻意规避检测，需要更有效的治理框架。

Method: 提出三层KYC框架：第一层利用研究机构作为信任锚点，验证研究人员身份并承担责任；第二层通过序列同源性搜索和功能注释进行输出筛查；第三层监控行为模式以检测与声明研究目的不符的异常活动。

Result: 该分层方法在保持合法研究人员访问权限的同时，通过机构问责和可追溯性提高了滥用成本，可立即利用现有机构基础设施实施，无需新立法或监管授权。

Conclusion: 基于KYC原则的生物AI治理框架比当前内容审查方法更有效，能够应对蛋白质设计AI的双重用途风险，实现安全与创新的平衡。

Abstract: Biological AI tools for protein design and structure prediction are advancing rapidly, creating dual-use risks that existing safeguards cannot adequately address. Current model-level restrictions, including keyword filtering, output screening, and content-based access denials, are fundamentally ill-suited to biology, where reliable function prediction remains beyond reach and novel threats evade detection by design. We propose a three-tier Know Your Customer (KYC) framework, inspired by anti-money laundering (AML) practices in the financial sector, that shifts governance from content inspection to user verification and monitoring. Tier I leverages research institutions as trust anchors to vouch for affiliated researchers and assume responsibility for vetting. Tier II applies output screening through sequence homology searches and functional annotation. Tier III monitors behavioral patterns to detect anomalies inconsistent with declared research purposes. This layered approach preserves access for legitimate researchers while raising the cost of misuse through institutional accountability and traceability. The framework can be implemented immediately using existing institutional infrastructure, requiring no new legislation or regulatory mandates.

</details>


### [23] [AdFL: In-Browser Federated Learning for Online Advertisement](https://arxiv.org/abs/2602.06336)
*Ahmad Alemari,Pritam Sen,Cristian Borcea*

Main category: cs.CR

TL;DR: AdFL是一个在浏览器中运行的联邦学习框架，用于学习用户广告偏好，平衡广告收入与用户隐私，支持差分隐私保护，广告可见性预测AUC达92.59%


<details>
  <summary>Details</summary>
Motivation: 随着GDPR等在线隐私法规的出台，在线出版商需要在定向广告收入和用户隐私之间找到平衡。联邦学习可以在不共享用户原始数据的情况下实现分布式学习，保护用户隐私。

Method: 提出AdFL联邦学习框架，在浏览器中运行，利用广告可见性、点击率、页面停留时间、页面内容等特征学习用户广告偏好。本地模型参数在服务器聚合形成全局模型，支持差分隐私保护本地参数。

Result: AdFL原型在浏览器中几毫秒内捕获训练信息，广告可见性预测模型AUC达到92.59%。使用差分隐私保护本地模型参数时性能适度下降，但仍保持足够性能。

Conclusion: AdFL证明了在浏览器中实施联邦学习进行广告偏好学习的可行性，能在保护用户隐私的同时实现有效的定向广告，为在线出版商提供了GDPR合规的解决方案。

Abstract: Since most countries are coming up with online privacy regulations, such as GDPR in the EU, online publishers need to find a balance between revenue from targeted advertisement and user privacy. One way to be able to still show targeted ads, based on user personal and behavioral information, is to employ Federated Learning (FL), which performs distributed learning across users without sharing user raw data with other stakeholders in the publishing ecosystem. This paper presents AdFL, an FL framework that works in the browsers to learn user ad preferences. These preferences are aggregated in a global FL model, which is then used in the browsers to show more relevant ads to users. AdFL can work with any model that uses features available in the browser such as ad viewability, ad click-through, user dwell time on pages, and page content. The AdFL server runs at the publisher and coordinates the learning process for the users who browse pages on the publisher's website. The AdFL prototype does not require the client to install any software, as it is built utilizing standard APIs available on most modern browsers. We built a proof-of-concept model for ad viewability prediction that runs on top of AdFL. We tested AdFL and the model with two non-overlapping datasets from a website with 40K visitors per day. The experiments demonstrate AdFL's feasibility to capture the training information in the browser in a few milliseconds, show that the ad viewability prediction achieves up to 92.59% AUC, and indicate that utilizing differential privacy (DP) to safeguard local model parameters yields adequate performance, with only modest declines in comparison to the non-DP variant.

</details>


### [24] [VENOMREC: Cross-Modal Interactive Poisoning for Targeted Promotion in Multimodal LLM Recommender Systems](https://arxiv.org/abs/2602.06409)
*Guowei Guan,Yurong Hao,Jiaming Zhang,Tiantong Wu,Fuyao Zhang,Tianxiang Chen,Longtao Huang,Cyril Leung,Wei Yang Bryan Lim*

Main category: cs.CR

TL;DR: 论文提出了一种针对多模态推荐系统的新型攻击方法VENOMREC，通过跨模态协同投毒来操纵融合表示，相比传统单模态攻击更有效


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型推动推荐系统向基于内容的检索和排序发展，跨模态共识虽然能缓解传统投毒攻击，但也引入了新的攻击面——同步多模态投毒可以稳定地操纵融合表示

Method: 提出VENOMREC攻击框架：1) 曝光对齐识别联合嵌入空间中的高曝光区域；2) 跨模态交互扰动通过注意力引导的耦合token-patch编辑来制作协同攻击

Result: 在三个真实世界多模态数据集上的实验表明，VENOMREC始终优于强基线，平均ER@20达到0.73，比最强基线平均提高+0.52绝对ER点，同时保持可比的推荐效用

Conclusion: 跨模态交互投毒是多模态推荐系统的新安全威胁，需要开发更鲁棒的防御机制来应对这种协同攻击

Abstract: Multimodal large language models (MLLMs) are pushing recommender systems (RecSys) toward content-grounded retrieval and ranking via cross-modal fusion. We find that while cross-modal consensus often mitigates conventional poisoning that manipulates interaction logs or perturbs a single modality, it also introduces a new attack surface where synchronised multimodal poisoning can reliably steer fused representations along stable semantic directions during fine-tuning. To characterise this threat, we formalise cross-modal interactive poisoning and propose VENOMREC, which performs Exposure Alignment to identify high-exposure regions in the joint embedding space and Cross-modal Interactive Perturbation to craft attention-guided coupled token-patch edits. Experiments on three real-world multimodal datasets demonstrate that VENOMREC consistently outperforms strong baselines, achieving 0.73 mean ER@20 and improving over the strongest baseline by +0.52 absolute ER points on average, while maintaining comparable recommendation utility.

</details>


### [25] [Subgraph Reconstruction Attacks on Graph RAG Deployments with Practical Defenses](https://arxiv.org/abs/2602.06495)
*Minkyoo Song,Jaehan Kim,Myungchul Kang,Hanna Kim,Seungwon Shin,Sooel Son*

Main category: cs.CR

TL;DR: 论文提出GRASP攻击方法，能够有效从Graph RAG系统中提取知识图谱子图，克服现有攻击方法的不足，并在真实场景下达到82.9 F1分数的高效重建。


<details>
  <summary>Details</summary>
Motivation: Graph RAG系统虽然增强了LLM的关系推理能力，但带来了新的隐私威胁：攻击者可能从目标RAG系统的知识图谱中重建子图，从而窃取隐私信息和复现精心构建的知识资产。现有攻击方法在Graph RAG环境下效果不佳，需要新的攻击技术。

Method: 提出GRASP攻击方法，包含三个关键技术：(1) 将提取任务重构为上下文处理任务；(2) 通过每记录标识符强制格式合规、实例接地的输出，减少幻觉并保留关系细节；(3) 使用动量感知调度器多样化目标驱动的攻击查询，在严格查询预算内操作。

Result: 在两个真实世界知识图谱、四个安全对齐的LLM和多个Graph RAG框架上，GRASP实现了最强的类型忠实重建，达到最高82.9 F1分数，而先前方法完全失败。同时评估了防御措施并提出了两种轻量级缓解方案。

Conclusion: Graph RAG系统存在严重的隐私泄露风险，GRASP攻击能够有效提取知识图谱子图。需要加强Graph RAG系统的安全防护，论文提出的轻量级缓解方案可以在不损失实用性的情况下显著降低重建保真度。

Abstract: Graph-based retrieval-augmented generation (Graph RAG) is increasingly deployed to support LLM applications by augmenting user queries with structured knowledge retrieved from a knowledge graph. While Graph RAG improves relational reasoning, it introduces a largely understudied threat: adversaries can reconstruct subgraphs from a target RAG system's knowledge graph, enabling privacy inference and replication of curated knowledge assets. We show that existing attacks are largely ineffective against Graph RAG even with simple prompt-based safeguards, because these attacks expose explicit exfiltration intent and are therefore easily suppressed by lightweight safe prompts. We identify three technical challenges for practical Graph RAG extraction under realistic safeguards and introduce GRASP, a closed-box, multi-turn subgraph reconstruction attack. GRASP (i) reframes extraction as a context-processing task, (ii) enforces format-compliant, instance-grounded outputs via per-record identifiers to reduce hallucinations and preserve relational details, and (iii) diversifies goal-driven attack queries using a momentum-aware scheduler to operate within strict query budgets. Across two real-world knowledge graphs, four safety-aligned LLMs, and multiple Graph RAG frameworks, GRASP attains the strongest type-faithful reconstruction where prior methods fail, reaching up to 82.9 F1. We further evaluate defenses and propose two lightweight mitigations that substantially reduce reconstruction fidelity without utility loss.

</details>


### [26] [Sequential Auditing for f-Differential Privacy](https://arxiv.org/abs/2602.06518)
*Tim Kutta,Martin Dunsche,Yu Wei,Vassilis Zikas*

Main category: cs.CR

TL;DR: 提出基于输出样本评估差分隐私算法的新审计方法，专注于f-DP隐私概念，自适应确定最优样本量，支持白盒和黑盒设置


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私审计方法多为批量处理或针对传统(ε,δ)-DP，且需要用户指定样本量，导致样本量过大，特别是对于昂贵的训练过程如DP-SGD

Method: 开发基于f-DP概念的新审计方法，能够检测完整隐私谱系中的违规，具有统计显著性保证，自适应确定最优样本量，无需用户指定样本大小

Result: 审计方法通过理论和模拟验证，能够显著减少所需样本量，特别适用于昂贵的训练过程，支持白盒、黑盒设置和单次运行框架

Conclusion: 提出的f-DP审计方法相比现有方法更高效，自适应确定样本量，减少采样成本，为差分隐私算法的验证提供了更实用的工具

Abstract: We present new auditors to assess Differential Privacy (DP) of an algorithm based on output samples. Such empirical auditors are common to check for algorithmic correctness and implementation bugs. Most existing auditors are batch-based or targeted toward the traditional notion of $(\varepsilon,δ)$-DP; typically both. In this work, we shift the focus to the highly expressive privacy concept of $f$-DP, in which the entire privacy behavior is captured by a single tradeoff curve. Our auditors detect violations across the full privacy spectrum with statistical significance guarantees, which are supported by theory and simulations. Most importantly, and in contrast to prior work, our auditors do not require a user-specified sample size as an input. Rather, they adaptively determine a near-optimal number of samples needed to reach a decision, thereby avoiding the excessively large sample sizes common in many auditing studies. This reduction in sampling cost becomes especially beneficial for expensive training procedures such as DP-SGD. Our method supports both whitebox and blackbox settings and can also be executed in single-run frameworks.

</details>


### [27] [Dependable Artificial Intelligence with Reliability and Security (DAIReS): A Unified Syndrome Decoding Approach for Hallucination and Backdoor Trigger Detection](https://arxiv.org/abs/2602.06532)
*Hema Karnam Surendrababu,Nithin Nagaraj*

Main category: cs.CR

TL;DR: 提出基于Syndrome Decoding的统一方法DAIReS，用于检测机器学习系统中的安全性和可靠性违规，包括检测数据投毒攻击和LLM幻觉内容


<details>
  <summary>Details</summary>
Motivation: 机器学习模型（包括大语言模型）存在多种安全性和可靠性问题：安全方面易受后门数据投毒攻击，导致未经授权的模型行为和系统性误分类；可靠性方面存在幻觉问题，产生不可预测的输出，给终端用户带来重大风险

Method: 提出DAIReS方法，基于Syndrome Decoding的统一方法，将综合征解码方法适配到NLP句子嵌入空间，能够检测ML训练数据集中的投毒和非投毒样本，同时也能有效检测LLM中由于自引用元解释任务产生的幻觉内容

Result: 该方法能够统一检测安全性和可靠性违规，具体包括：1）在NLP句子嵌入空间中区分投毒和非投毒样本；2）检测大语言模型中的幻觉内容

Conclusion: Syndrome Decoding方法为机器学习系统的可靠性和安全性提供了一种统一的检测框架，能够同时应对数据投毒攻击和LLM幻觉问题，提高了AI系统的可信赖性

Abstract: Machine Learning (ML) models, including Large Language Models (LLMs), are characterized by a range of system-level attributes such as security and reliability. Recent studies have demonstrated that ML models are vulnerable to multiple forms of security violations, among which backdoor data-poisoning attacks represent a particularly insidious threat, enabling unauthorized model behavior and systematic misclassification. In parallel, deficiencies in model reliability can manifest as hallucinations in LLMs, leading to unpredictable outputs and substantial risks for end users. In this work on Dependable Artificial Intelligence with Reliability and Security (DAIReS), we propose a novel unified approach based on Syndrome Decoding for the detection of both security and reliability violations in learning-based systems. Specifically, we adapt the syndrome decoding approach to the NLP sentence-embedding space, enabling the discrimination of poisoned and non-poisoned samples within ML training datasets. Additionally, the same methodology can effectively detect hallucinated content due to self referential meta explanation tasks in LLMs.

</details>


### [28] [Confundo: Learning to Generate Robust Poison for Practical RAG Systems](https://arxiv.org/abs/2602.06616)
*Haoyang Hu,Zhejun Jiang,Yueming Lyu,Junyuan Zhang,Yi Liu,Ka-Ho Chow*

Main category: cs.CR

TL;DR: Confundo是一个针对实际RAG系统的学习型投毒攻击框架，通过微调大语言模型作为投毒生成器，在多种攻击目标下实现高有效性、鲁棒性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG投毒攻击在实际系统中效果严重下降，原因在于两个被忽视的现实：1) 内容在使用前常被处理，可能破坏投毒内容；2) 用户查询往往与攻击设计时预期的不同。这导致实践者低估风险并产生虚假的安全感。

Method: Confundo是一个学习型投毒框架，通过微调大语言模型作为投毒生成器，支持多种攻击目标（操纵事实正确性、诱导偏见观点、触发幻觉），并考虑实际RAG系统的处理流程和用户查询模式。

Result: Confundo在多种数据集和RAG配置下，大幅优于各种专门设计的攻击方法，即使在存在防御措施的情况下也保持高有效性。同时展示了防御性应用，保护网页内容免遭未经授权的RAG系统抓取。

Conclusion: 该研究通过Confundo框架更好地描述了实际RAG系统面临的威胁，暴露了现有攻击评估的局限性，并提供了既能攻击又能防御的统一框架，有助于更准确地评估RAG系统的安全风险。

Abstract: Retrieval-augmented generation (RAG) is increasingly deployed in real-world applications, where its reference-grounded design makes outputs appear trustworthy. This trust has spurred research on poisoning attacks that craft malicious content, inject it into knowledge sources, and manipulate RAG responses. However, when evaluated in practical RAG systems, existing attacks suffer from severely degraded effectiveness. This gap stems from two overlooked realities: (i) content is often processed before use, which can fragment the poison and weaken its effect, and (ii) users often do not issue the exact queries anticipated during attack design. These factors can lead practitioners to underestimate risks and develop a false sense of security. To better characterize the threat to practical systems, we present Confundo, a learning-to-poison framework that fine-tunes a large language model as a poison generator to achieve high effectiveness, robustness, and stealthiness. Confundo provides a unified framework supporting multiple attack objectives, demonstrated by manipulating factual correctness, inducing biased opinions, and triggering hallucinations. By addressing these overlooked challenges, Confundo consistently outperforms a wide range of purpose-built attacks across datasets and RAG configurations by large margins, even in the presence of defenses. Beyond exposing vulnerabilities, we also present a defensive use case that protects web content from unauthorized incorporation into RAG systems via scraping, with no impact on user experience.

</details>


### [29] [Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus](https://arxiv.org/abs/2602.06655)
*Zeta Avarikioti,Ray Neiheiser,Krzysztof Pietrzak,Michelle X. Yeo*

Main category: cs.CR

TL;DR: Wonderboom是一个百万级规模的签名聚合协议，能够在单个以太坊时隙内高效聚合数百万验证者的签名，速度比现有协议快32倍，同时提供更高的安全保障。


<details>
  <summary>Details</summary>
Motivation: 以太坊作为全球重要金融平台，拥有近百万验证者，但其现有协议需要约15分钟才能最终确定区块，这对于许多实际应用来说过慢。现有签名聚合协议存在缺陷，可能被攻击者利用来转移权益比例。

Method: 提出了Wonderboom协议，这是首个百万级规模的聚合协议。为了评估该协议，作者实现了首个能够模拟百万级规模协议的仿真工具。

Result: 即使在最坏情况下，Wonderboom也能在单个以太坊时隙内聚合和验证超过200万个签名，比现有协议快32倍，同时提供更高的安全保证。

Conclusion: Wonderboom协议解决了以太坊大规模验证者集合带来的性能瓶颈问题，显著提高了区块最终确定速度，同时增强了系统的安全性，对以太坊的实际应用具有重要意义。

Abstract: Over the last years, Ethereum has evolved into a public platform that safeguards the savings of hundreds of millions of people and secures more than $650 billion in assets, placing it among the top 25 stock exchanges worldwide in market capitalization, ahead of Singapore, Mexico, and Thailand. As such, the performance and security of the Ethereum blockchain are not only of theoretical interest, but also carry significant global economic implications. At the time of writing, the Ethereum platform is collectively secured by almost one million validators highlighting its decentralized nature and underlining its economic security guarantees. However, due to this large validator set, the protocol takes around 15 minutes to finalize a block which is prohibitively slow for many real world applications. This delay is largely driven by the cost of aggregating and disseminating signatures across a validator set of this scale. Furthermore, as we show in this paper, the existing protocol that is used to aggregate and disseminate the signatures has several shortcomings that can be exploited by adversaries to shift stake proportion from honest to adversarial nodes. In this paper, we introduce Wonderboom, the first million scale aggregation protocol that can efficiently aggregate the signatures of millions of validators in a single Ethereum slot (x32 faster) while offering higher security guarantees than the state of the art protocol used in Ethereum. Furthermore, to evaluate Wonderboom, we implement the first simulation tool that can simulate such a protocol on the million scale and show that even in the worst case Wonderboom can aggregate and verify more than 2 million signatures within a single Ethereum slot.

</details>


### [30] [TrapSuffix: Proactive Defense Against Adversarial Suffixes in Jailbreaking](https://arxiv.org/abs/2602.06630)
*Mengyao Du,Han Fang,Haokai Ma,Gang Yang,Quanjun Yin,Shouling Ji,Ee-Chien Chang*

Main category: cs.CR

TL;DR: TrapSuffix是一种主动防御方法，通过轻量级微调在LLM中植入陷阱行为，使攻击者要么陷入优化陷阱无法生成有效对抗后缀，要么生成带有可追踪指纹的后缀，实现强防御和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有基于后缀的越狱攻击防御大多是被动检测可疑后缀，没有充分利用防御者可以主动植入秘密和隐藏漏洞的不对称优势。后缀攻击有无限多的表面形式，使得防御变得困难。

Method: 提出TrapSuffix方法，采用轻量级微调方法将陷阱对齐行为注入基础模型，不改变推理流程。通过重塑模型对对抗后缀的响应景观，引导越狱尝试进入两种结果：要么陷入防御者设计的优化陷阱失败，要么生成带有独特可追踪指纹的对抗后缀。

Result: 在各种基于后缀的越狱攻击设置中，TrapSuffix将平均攻击成功率降低到0.01%以下，平均追踪成功率达到87.9%。不引入推理时开销，平均仅需15.87MB额外内存，而现有基于LLM的检测防御通常需要1e4MB级别的内存开销。

Conclusion: TrapSuffix提供了一种主动防御范式，通过可控性导向的方法有效应对后缀越狱攻击，实现了强防御和可靠的可追溯性，与现有基于过滤的防御自然互补。

Abstract: Suffix-based jailbreak attacks append an adversarial suffix, i.e., a short token sequence, to steer aligned LLMs into unsafe outputs. Since suffixes are free-form text, they admit endlessly many surface forms, making jailbreak mitigation difficult. Most existing defenses depend on passive detection of suspicious suffixes, without leveraging the defender's inherent asymmetric ability to inject secrets and proactively conceal gaps. Motivated by this, we take a controllability-oriented perspective and develop a proactive defense that nudges attackers into a no-win dilemma: either they fall into defender-designed optimization traps and fail to produce an effective adversarial suffix, or they can succeed only by generating adversarial suffixes that carry distinctive, traceable fingerprints. We propose TrapSuffix, a lightweight fine-tuning approach that injects trap-aligned behaviors into the base model without changing the inference pipeline. TrapSuffix channels jailbreak attempts into these two outcomes by reshaping the model's response landscape to adversarial suffixes. Across diverse suffix-based jailbreak settings, TrapSuffix reduces the average attack success rate to below 0.01 percent and achieves an average tracing success rate of 87.9 percent, providing both strong defense and reliable traceability. It introduces no inference-time overhead and incurs negligible memory cost, requiring only 15.87 MB of additional memory on average, whereas state-of-the-art LLM-based detection defenses typically incur memory overheads at the 1e4 MB level, while composing naturally with existing filtering-based defenses for complementary protection.

</details>


### [31] [Evaluating and Enhancing the Vulnerability Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2602.06687)
*Li Lu,Yanjie Zhao,Hongzhou Rao,Kechi Zhang,Haoyu Wang*

Main category: cs.CR

TL;DR: 论文提出DAGVul框架，将漏洞推理建模为有向无环图生成任务，通过强化学习与可验证奖励机制提升大语言模型在漏洞检测中的逻辑一致性，在8B参数规模下达到与Claude-Sonnet-4.5竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在漏洞检测中存在可靠性问题：模型经常基于幻觉逻辑或表面模式给出正确检测结果，但缺乏对根本原因的准确推理。现有基准主要关注粗粒度分类指标，缺乏评估底层推理过程的细粒度真实标签。

Method: 1) 构建包含真实漏洞专家标注因果推理和语义等价代码扰动的基准数据集；2) 提出DAGVul框架，将漏洞推理建模为有向无环图生成任务，显式映射因果依赖关系；3) 引入强化学习与可验证奖励机制，对齐模型推理轨迹与程序内在逻辑。

Result: 实验显示：1) 现有最先进模型在语义代码理解中难以保持逻辑一致性，表现出12种系统性失败模式；2) DAGVul框架相比所有基线平均提升推理F1分数18.9%；3) 8B参数实现不仅超越同规模模型，还超越专门的大规模推理模型，与Claude-Sonnet-4.5竞争（75.47% vs. 76.11%）。

Conclusion: DAGVul框架通过将漏洞推理建模为有向无环图生成任务，并结合强化学习与可验证奖励机制，显著提升了大语言模型在漏洞检测中的逻辑一致性和推理可靠性，在不同模型规模上建立了新的效率标准。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in vulnerability detection. However, a critical reliability gap persists: models frequently yield correct detection verdicts based on hallucinated logic or superficial patterns that deviate from the actual root cause. This misalignment remains largely obscured because contemporary benchmarks predominantly prioritize coarse-grained classification metrics, lacking the granular ground truth required to evaluate the underlying reasoning process. To bridge this gap, we first construct a benchmark consisting of two datasets: (1) real-world vulnerabilities with expert-curated causal reasoning as ground truth, and (2) semantically equivalent code perturbations for assessing reasoning robustness. Our large-scale empirical study reveals that even state-of-the-art models struggle to maintain logical consistency during semantic code comprehension, exhibiting 12 systematic failure patterns. Addressing these limitations, we propose DAGVul, a novel framework that models vulnerability reasoning as a Directed Acyclic Graph (DAG) generation task. Unlike linear chain-of-thought (CoT), our approach explicitly maps causal dependencies to enforce structural consistency. By further introducing Reinforcement Learning with Verifiable Rewards (RLVR), we align model reasoning trace with program-intrinsic logic. Experimental results demonstrate that our framework improves the reasoning F1-score by an average of 18.9% over all the baselines. Remarkably, our 8B-parameter implementation not only outperforms existing models of comparable scale but also surpasses specialized large-scale reasoning models, including Qwen3-30B-Reasoning and GPT-OSS-20B-High. It is even competitive with state-of-the-art models like Claude-Sonnet-4.5 (75.47% vs. 76.11%), establishing new efficiency in vulnerability reasoning across model scales.

</details>


### [32] [Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs](https://arxiv.org/abs/2602.06700)
*Ying Song,Balaji Palanisamy*

Main category: cs.CR

TL;DR: 提出了Taipan，首个无需查询、基于迁移学习的图多敏感属性推断攻击框架，揭示了仅从公开图数据中存在的固有敏感信息泄露风险。


<details>
  <summary>Details</summary>
Motivation: 现有属性推断攻击主要依赖重复模型查询，这在现实场景中不切实际。更重要的是，现有方法忽略了仅从公开发布的图数据中就存在的固有多敏感信息泄露漏洞。

Method: 提出Taipan框架，包含分层攻击知识路由以捕捉属性间复杂关联，以及提示引导的攻击原型精炼以缓解负迁移和性能下降。

Result: 在多种真实图数据集上的实验表明，Taipan在相同分布、异构相似分布和异分布设置下均能保持强攻击性能，即使在严格差分隐私保证下仍然有效。

Conclusion: 研究结果强调了开发更鲁棒的多属性隐私保护图发布方法和数据共享实践的紧迫需求。

Abstract: Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.

</details>


### [33] [GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models](https://arxiv.org/abs/2602.06718)
*Zuyao Xu,Yuqi Qiu,Lu Sun,FaSheng Miao,Fubin Wu,Xinyi Wang,Xiang Li,Haozhe Lu,ZhengZe Zhang,Yuxin Hu,Jialu Li,Jin Luo,Feng Zhang,Rui Luo,Xinran Liu,Yingxian Li,Jiaji Liu*

Main category: cs.CR

TL;DR: 大型语言模型生成虚假引用的问题日益严重，CiteVerifier框架首次系统研究LLM时代的引用有效性，发现所有模型都存在引用幻觉，且学术论文中虚假引用比例上升，研究人员和审稿人存在验证缺口。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在学术写作中的广泛应用，其生成虚假引用（"幽灵引用"）的趋势对引用有效性构成系统性威胁，需要量化这种威胁并制定缓解措施。

Method: 开发CiteVerifier开源框架进行大规模引用验证，通过三个实验：1) 在40个研究领域基准测试13个最先进的LLM；2) 分析2020-2025年顶级AI/ML和安全会议56,381篇论文中的220万条引用；3) 调查97名研究人员并分析94份有效回复。

Result: 所有LLM都存在引用幻觉，比例从14.23%到94.93%；1.07%的论文包含无效或伪造引用（604篇），2025年单年增长80.9%；41.5%研究人员复制粘贴BibTeX而不检查，44.4%遇到可疑引用时不采取行动；76.7%审稿人不彻底检查引用，80%从未怀疑虚假引用。

Conclusion: 不可靠的AI工具与研究人员不足的人工验证以及同行评审审查不足相结合，导致虚假引用污染科学记录，形成加速危机。需要为研究人员、会议和工具开发者提出干预措施以保护引用完整性。

Abstract: Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.
  To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\% to 94.93\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\% of researchers copy-paste BibTeX without checking and 44.4\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\% of reviewers do not thoroughly check references and 80.0\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.

</details>


### [34] [Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection](https://arxiv.org/abs/2602.06751)
*Yikun Li,Ting Zhang,Jieke Shi,Chengran Yang,Junda He,Xin Zhou,Jinfeng Jiang,Huihui Huang,Wen Bin Leow,Yide Yin,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: CPRVul是一个上下文感知的漏洞检测框架，通过上下文分析和结构化推理，在三个高质量漏洞数据集上显著优于仅基于函数的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法大多在函数级别操作，缺乏过程间上下文信息。实际漏洞的存在和根本原因通常依赖于上下文信息，但简单地附加上下文会导致性能下降，因为真实世界的上下文冗长、冗余且嘈杂。

Method: CPRVul采用两阶段框架：1) 上下文分析和选择：构建代码属性图提取候选上下文，使用LLM生成安全导向的配置文件并分配相关性分数，选择高影响力的上下文元素；2) 结构化推理：整合目标函数、选定上下文和漏洞元数据生成推理轨迹，用于微调LLM进行基于推理的漏洞检测。

Result: 在PrimeVul、TitanVul和CleanVul三个数据集上，CPRVul准确率从64.94%到73.76%，显著优于UniXcoder的56.65%到63.68%。在PrimeVul基准测试中达到67.78%准确率，比先前最佳方法提升22.9%。消融实验表明，只有处理后的上下文与结构化推理结合才能带来性能提升。

Conclusion: CPRVul通过上下文分析和结构化推理的有效结合，显著提升了漏洞检测性能，证明了处理后的上下文与推理机制协同工作的重要性。

Abstract: Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.
  We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.
  We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.

</details>


### [35] [$f$-Differential Privacy Filters: Validity and Approximate Solutions](https://arxiv.org/abs/2602.06756)
*Long Tran,Antti Koskela,Ossi Räisä,Antti Honkela*

Main category: cs.CR

TL;DR: 该论文研究了完全自适应组合下的隐私损失计算问题，发现基于f-DP的自然隐私过滤器方法存在根本性缺陷，并建立了其有效性的充要条件，同时证明了完全自适应中心极限定理并构建了近似高斯DP过滤器。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私中，完全自适应组合下的隐私损失计算是一个核心挑战，其中机制选择和隐私参数都可能依赖于先前输出的完整历史。隐私过滤器作为组合的停止规则，确保规定的全局隐私预算不被超过。目前尚不清楚最优的基于权衡函数的f-DP概念是否在完全自适应交互下允许有效的隐私过滤器。

Method: 研究分析了基于f-DP的自然隐私过滤器方法（即组合个体权衡曲线并在规定的f-DP曲线被跨越时停止）的有效性。通过理论分析，刻画了该方法失败的情况和原因，并建立了自然过滤器有效的充要条件。同时证明了完全自适应中心极限定理，并为子采样高斯机制在特定采样率下构建了近似高斯DP过滤器。

Result: 研究发现基于f-DP的自然隐私过滤器方法存在根本性缺陷，并非总是有效。建立了该过滤器有效性的充要条件。证明了完全自适应中心极限定理，并为子采样高斯机制在采样率q<0.2和q>0.8时构建了近似高斯DP过滤器，该过滤器在相同设置下比基于Rényi DP的过滤器提供更严格的隐私保证。

Conclusion: 该论文揭示了基于f-DP的自然隐私过滤器在完全自适应组合下的局限性，提供了理论分析框架来理解其有效性条件，并提出了改进的近似高斯DP过滤器，为完全自适应差分隐私组合提供了更精确的隐私分析工具。

Abstract: Accounting for privacy loss under fully adaptive composition -- where both the choice of mechanisms and their privacy parameters may depend on the entire history of prior outputs -- is a central challenge in differential privacy (DP). In this setting, privacy filters are stopping rules for compositions that ensure a prescribed global privacy budget is not exceeded. It remains unclear whether optimal trade-off-function-based notions, such as $f$-DP, admit valid privacy filters under fully adaptive interaction. We show that the natural approach to defining an $f$-DP filter -- composing individual trade-off curves and stopping when the prescribed $f$-DP curve is crossed -- is fundamentally invalid. We characterise when and why this failure occurs, and establish necessary and sufficient conditions under which the natural filter is valid. Furthermore, we prove a fully adaptive central limit theorem for $f$-DP and construct an approximate Gaussian DP filter for subsampled Gaussian mechanisms at small sampling rates $q<0.2$ and large sampling rates $q>0.8$, yielding tighter privacy guarantees than filters based on Rényi DP in the same setting.

</details>


### [36] ["Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs](https://arxiv.org/abs/2602.06759)
*Yunlong Lyu,Yixuan Tang,Peng Chen,Tian Dong,Xinyu Wang,Zhiqiang Dong,Hao Chen*

Main category: cs.CR

TL;DR: 本文首次系统研究了AI集成IDE中Next Edit Suggestions（NES）系统的安全风险，揭示了NES因扩展上下文检索机制而引入的新攻击面，包括上下文投毒、事务编辑和人类-IDE交互敏感性等漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代AI集成IDE正从被动代码补全转向主动的Next Edit Suggestions（NES），这种演变引入了更复杂的上下文检索机制和交互模式。然而，现有研究几乎完全专注于独立LLM代码生成的安全影响，忽视了NES在现代AI集成IDE中可能带来的攻击向量。NES的底层机制尚未充分探索，其安全影响尚未完全理解。

Method: 1. 深入剖析NES机制以理解新引入的威胁向量；2. 进行全面的实验室研究评估NES的安全影响；3. 开展涉及200多名专业开发者的大规模在线调查，评估实际开发工作流中NES安全风险的认知情况。

Result: 研究发现NES检索了显著扩展的上下文（包括不可察觉的用户操作输入和全局代码库检索），增加了攻击面。评估显示NES易受上下文投毒攻击，并对事务编辑和人类-IDE交互敏感。调查结果表明开发者普遍缺乏对NES相关安全风险的认知，突显了在AI集成IDE中加强教育和改进安全对策的必要性。

Conclusion: NES系统在提高开发者生产力的同时引入了新的安全风险，需要系统性的安全研究和相应的防护措施。研究揭示了NES机制的漏洞和开发者认知差距，为AI集成IDE的安全设计提供了重要见解。

Abstract: Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood.
  In this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.

</details>


### [37] [Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs](https://arxiv.org/abs/2602.06777)
*Yassine Chagna,Antal Goldschmidt*

Main category: cs.CR

TL;DR: 该研究探索使用大语言模型进行异构日志源的异常检测，解决了传统入侵检测系统的高误报率、语义盲点和数据稀缺问题，提出了新的数据集、评估指标和两阶段训练框架。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统存在高误报率、语义盲点和数据稀缺问题，因为日志数据具有敏感性，干净的标注数据集很少。需要一种能够理解日志语义并有效检测异常的新方法。

Method: 提出了三个主要贡献：1) LogAtlas-Foundation-Sessions和LogAtlas-Defense-Set两个平衡且异构的日志数据集，包含明确的攻击标注和隐私保护；2) 实证基准测试，揭示了为什么F1和准确率等标准指标在安全应用中具有误导性；3) 两阶段训练框架，结合日志理解(Base-AMAN, 3B参数)和实时检测(AMAN, 0.5B参数，通过知识蒸馏)。

Result: 结果证明了实际可行性，每个会话的推理时间为0.3-0.5秒，每日运营成本低于50美元。该方法在异构日志源异常检测方面表现出色。

Conclusion: 该研究为大语言模型在安全日志分析中的应用提供了实用框架，通过创新的数据集、评估指标和模型架构，有效解决了传统入侵检测系统的局限性。

Abstract: This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.

</details>


### [38] [Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations](https://arxiv.org/abs/2602.06887)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Yanwen Jia,Xueluan Gong,Qian Wang,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: PROTOPURIFY是一个针对大语言模型后门攻击的防御框架，通过参数编辑实现后门净化，在最小假设下工作，支持后门防御即服务


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法难以实现后门防御即服务，因为它们需要不现实的辅助信息（如下游干净数据、已知触发器/目标或任务领域特定信息），并且缺乏跨不同后门模型的可重用、可扩展的净化方法

Method: PROTOPURIFY首先从干净模型和后门模型对构建后门向量池，将向量聚合成候选原型，通过相似性匹配选择与目标模型最对齐的候选原型；然后通过层间原型对齐识别边界层，在受影响层中抑制原型对齐组件，实现细粒度缓解

Result: 在各种大语言模型上的分类和生成任务实验中，PROTOPURIFY在6种代表性防御方法中表现最佳，对抗6种多样化攻击（包括单触发器、多触发器和无触发器后门设置）；将攻击成功率降至10%以下，某些情况下低至1.6%，同时干净效用下降不到3%

Conclusion: PROTOPURIFY是一个BDaaS就绪的原始框架，支持可重用性、可定制性、可解释性和运行时效率，对自适应后门变体具有鲁棒性，在非后门模型上保持稳定

Abstract: Large language models (LLMs) are increasingly deployed in security-sensitive applications, yet remain vulnerable to backdoor attacks. However, existing backdoor defenses are difficult to operationalize for Backdoor Defense-as-a-Service (BDaaS), as they require unrealistic side information (e.g., downstream clean data, known triggers/targets, or task domain specifics), and lack reusable, scalable purification across diverse backdoored models. In this paper, we present PROTOPURIFY, a backdoor purification framework via parameter edits under minimal assumptions. PROTOPURIFY first builds a backdoor vector pool from clean and backdoored model pairs, aggregates vectors into candidate prototypes, and selects the most aligned candidate for the target model via similarity matching. PROTOPURIFY then identifies a boundary layer through layer-wise prototype alignment and performs targeted purification by suppressing prototype-aligned components in the affected layers, achieving fine-grained mitigation with minimal impact on benign utility. Designed as a BDaaS-ready primitive, PROTOPURIFY supports reusability, customizability, interpretability, and runtime efficiency. Experiments across various LLMs on both classification and generation tasks show that PROTOPURIFY consistently outperforms 6 representative defenses against 6 diverse attacks, including single-trigger, multi-trigger, and triggerless backdoor settings. PROTOPURIFY reduces ASR to below 10%, and even as low as 1.6% in some cases, while incurring less than a 3% drop in clean utility. PROTOPURIFY further demonstrates robustness against adaptive backdoor variants and stability on non-backdoored models.

</details>


### [39] [TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering](https://arxiv.org/abs/2602.06911)
*Saad Hossain,Tom Tseng,Punya Syon Pandey,Samanvay Vajpayee,Matthew Kowal,Nayeema Nonta,Samuel Simko,Stephen Casper,Zhijing Jin,Kellin Pelrine,Sirisha Rambhatla*

Main category: cs.CR

TL;DR: TamperBench是首个系统评估大语言模型防篡改能力的统一框架，包含攻击方法库、超参数扫描、安全与实用性评估，用于评估21个开源LLM的防篡改能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型部署增多，提高其防篡改能力变得至关重要，但目前缺乏标准评估方法，不同数据集、指标和篡改配置使得模型间的安全性、实用性和鲁棒性难以比较。

Method: TamperBench框架包含：(1) 收集最先进的权重空间微调攻击和潜在空间表示攻击方法库；(2) 通过系统化超参数扫描实现真实对抗评估；(3) 提供安全性和实用性评估。框架只需最少额外代码即可指定任何微调配置、对齐阶段防御方法和指标套件，确保端到端可复现性。

Result: 使用TamperBench评估了21个开源LLM（包括防御增强变体），在9种篡改威胁下使用标准化安全和能力指标进行评估，获得新发现：训练后处理对防篡改能力的影响、越狱调优通常是最严重的攻击、Triplet成为领先的对齐阶段防御方法。

Conclusion: TamperBench为系统评估LLM防篡改能力提供了首个统一框架，通过标准化评估揭示了重要发现，有助于提高开源LLM的安全性和鲁棒性。

Abstract: As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing](https://arxiv.org/abs/2602.06057)
*Satyam Kumar,Saurabh Jha*

Main category: cs.DC

TL;DR: QEIL框架通过推理时间缩放定律和异构硬件编排，在边缘设备上实现高效的大语言模型推理，显著提升能效和覆盖率，同时保持零精度损失。


<details>
  <summary>Details</summary>
Motivation: 资源受限的边缘设备上运行大语言模型面临延迟挑战，现有解决方案过度依赖云或数据中心基础设施，需要一种能在本地高效执行LLM推理的框架。

Method: 提出QEIL框架：1) 推导五个架构无关的定理，描述推理效率如何随模型大小、样本预算和设备约束缩放；2) 集成三个优化维度：异构工作负载分布、硬件感知路由、性能-能耗权衡量化；3) 通过渐进样本复用统一编排器组合这些组件。

Result: 在125M到2.6B参数的五个模型系列上评估显示：k覆盖率提升7-10.5个百分点，能耗降低35.6-78.2%，平均功耗降低68%满足边缘热预算，延迟改善15.8%，零精度损失。

Conclusion: 推理时间缩放定律具有普适性和架构无关性，异构边缘编排是能源受限智能系统的最优策略，为边缘设备上的高效LLM推理提供了理论基础和实用框架。

Abstract: Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.

</details>


### [41] [Mapping Gemma3 onto an Edge Dataflow Architecture](https://arxiv.org/abs/2602.06063)
*Shouyu Du,Miaoxiang Yu,Zhiheng Ni,Jillian Cai,Qing Yang,Tao Wei,Zhenyu Xu*

Main category: cs.DC

TL;DR: 该论文提出了首个在AMD Ryzen AI NPU上端到端部署Gemma3系列大语言和视觉模型的方法，通过硬件感知优化技术实现了显著的性能提升和能效改进。


<details>
  <summary>Details</summary>
Motivation: 现代NPU在边缘设备上运行大语言模型和视觉模型时面临性能瓶颈和能效问题，需要针对tiled数据流架构进行专门的优化部署。

Method: 提出了一系列硬件感知优化技术：预填充阶段包括高效反量化引擎、优化的tiled矩阵乘法内核、以及FlowQKV（分块流水线注意力机制）；解码阶段包括FusedDQP（融合反量化和投影的单一内核）和FlowKV（重构注意力以维持高内存带宽利用率），配合Q4NX 4位量化格式。

Result: 相比iGPU，预填充速度提升5.2倍，解码速度提升4.8倍；相比CPU，分别提升33.5倍和2.2倍。能效相比iGPU提升67.2倍，相比CPU提升222.9倍。

Conclusion: 现代NPU能够在边缘设备上实现实用、低功耗的LLM和VLM推理，为将基于Transformer的模型映射到tiled数据流加速器提供了可推广的蓝图。

Abstract: We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\times$ faster prefill and $4.8\times$ faster decoding versus the iGPU, and $33.5\times$ and $2.2\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\times$ and $222.9\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.

</details>


### [42] [iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems](https://arxiv.org/abs/2602.06064)
*Yi-Xiang Hu,Yuke Wang,Feng Wu,Zirui Huang,Shuli Zeng,Xiang-Yang Li*

Main category: cs.DC

TL;DR: iScheduler是一个基于强化学习的迭代调度框架，用于解决资源投资问题(RIP)，通过将问题分解为子问题并顺序选择进程来构建调度方案，显著加速优化并支持动态重配置。


<details>
  <summary>Details</summary>
Motivation: 在共享可再生资源下调度具有优先约束的任务是现代计算平台的核心问题。资源投资问题(RIP)旨在最小化资源配置成本，但传统混合整数规划和约束规划方法在大规模实例上速度过慢，且动态更新需要在严格延迟预算下重新调度。

Method: iScheduler将RIP求解建模为马尔可夫决策过程，通过分解子问题并顺序选择进程来构建调度方案。该框架支持通过重用未更改的进程调度和仅重新调度受影响进程来实现快速重配置。

Result: iScheduler在保持竞争力的资源成本的同时，将可行解时间减少了高达43倍（相比强大的商业基线）。作者还发布了L-RIPLIB工业级基准测试集，包含1000个实例，每个实例有2500-10000个任务。

Conclusion: iScheduler通过强化学习驱动的迭代调度框架，有效解决了大规模资源投资问题的优化效率问题，支持快速重配置，为工业级任务调度提供了高效解决方案。

Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\times$ against strong commercial baselines.

</details>


### [43] [HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference](https://arxiv.org/abs/2602.06069)
*Dinesh Gopalan,Ratul Ali*

Main category: cs.DC

TL;DR: HQP框架通过结合结构化剪枝和量化，在边缘设备上实现高性能AI推理，在保持精度损失小于1.5%的同时，达到3.12倍加速和55%模型压缩。


<details>
  <summary>Details</summary>
Motivation: 边缘-云分布式环境中对高保真实时推理的需求日益增长，但面临严重的延迟和能耗限制，需要激进的模型优化技术来应对这些挑战。

Method: 提出混合量化与剪枝框架，采用基于Fisher信息矩阵近似计算的动态权重敏感度指标指导结构化剪枝，在满足最大允许精度下降约束后，再进行8位后训练量化。

Result: 在NVIDIA Jetson边缘平台上，使用MobileNetV3和ResNet-18架构，HQP框架实现了3.12倍推理加速和55%模型大小减少，同时精度损失严格控制在1.5%以内。

Conclusion: HQP框架相比传统单目标压缩技术具有显著优势，是资源受限边缘基础设施中部署超低延迟AI的硬件无关解决方案。

Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.

</details>


### [44] [FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs](https://arxiv.org/abs/2602.06071)
*Rajat Vadiraj Dwaraknath,Sungyoon Kim,Mert Pilanci*

Main category: cs.DC

TL;DR: 论文提出BlockPerm-SJLT稀疏草图方法及其高效GPU实现FlashSketch，通过块排列结构平衡GPU效率与草图质量，在RandNLA基准测试中实现1.7倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏草图虽然能降低计算成本，但其随机稀疏性导致GPU内存访问模式不规则，严重影响内存带宽利用率。为解决GPU效率与草图鲁棒性之间的张力，需要设计新的稀疏草图结构。

Method: 采用草图-内核协同设计方法：1) 设计BlockPerm-SJLT稀疏草图族，引入可调参数平衡GPU效率与草图鲁棒性；2) 开发FlashSketch优化CUDA内核，专门实现这些草图的高效GPU计算。

Result: 理论分析证明BlockPerm-SJLT满足无意识子空间嵌入(OSE)保证；实验评估显示FlashSketch在RandNLA基准测试和GraSS数据归因管道中均表现优异，相比现有GPU草图实现约1.7倍几何平均加速。

Conclusion: 通过草图-内核协同设计，BlockPerm-SJLT和FlashSketch成功解决了稀疏草图在GPU上的效率瓶颈，在保持理论保证的同时显著提升了计算性能，推动了草图质量与速度的帕累托前沿。

Abstract: Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.

</details>


### [45] [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072)
*Rui Ning,Wei Zhang,Fan Lai*

Main category: cs.DC

TL;DR: PackInfer是一个针对大语言模型推理的注意力框架，通过将异构批处理请求打包成负载均衡的执行组，优化计算和I/O效率，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 生产环境中LLM推理需要批处理不同长度的请求以实现高吞吐量，但现有注意力优化技术（如FlashAttention）主要针对单个请求，导致计算和I/O不均衡、拖尾效应严重、GPU资源利用率低。

Method: PackInfer通过将批处理请求组织成负载均衡的执行组，将多个请求打包到统一的内核启动中；构建直接作用于打包查询键区域的注意力内核，消除冗余计算并平衡线程块执行；采用I/O感知分组，将共享前缀请求共置，并将KV缓存重组为组连续布局。

Result: 在实际工作负载评估中，PackInfer相比最先进的FlashAttention，推理延迟降低13.0-20.1%，吞吐量提升20%。

Conclusion: PackInfer通过计算和I/O感知的异构批处理推理执行，有效解决了生产环境中LLM推理的注意力效率问题，显著提升了GPU利用率和整体性能。

Abstract: Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.

</details>


### [46] [Experimental Analysis of Server-Side Caching for Web Performance](https://arxiv.org/abs/2602.06074)
*Mohammad Umar,Bharat Tripathi*

Main category: cs.DC

TL;DR: 本文通过实验比较了无缓存与使用内存缓存（固定TTL）的两种服务器端Web应用配置，发现缓存能显著降低响应时间，为小型Web应用提供了简单有效的性能优化方案。


<details>
  <summary>Details</summary>
Motivation: 虽然缓存技术已被广泛研究用于Web性能优化，但缺乏针对小型Web应用中简单内存缓存效果的实验研究。本文旨在填补这一研究空白，探索简单服务器端缓存在小型Web应用中的实际效果。

Method: 采用实验方法，比较两种服务器端Web应用配置：一种无缓存，另一种使用内存缓存并设置固定生存时间（TTL）。使用轻量级Web服务器框架，在相同环境条件下通过重复HTTP请求测量响应时间。

Result: 结果显示缓存请求的响应时间显著降低，证明了简单服务器端缓存在提高Web应用性能方面的有效性。

Conclusion: 简单服务器端缓存能有效提高Web应用性能，特别适用于教育环境和需要简单性、可重现性的小型Web应用场景。

Abstract: Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.

</details>


### [47] [Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)
*Liangyu Wang,Siqi Zhang,Junjie Wang,Yiming Dong,Bo Zheng,Zihan Qiu,Shengkun Tang,Di Wang,Rui Men,Dayiheng Liu*

Main category: cs.DC

TL;DR: Canzona框架解决了矩阵优化器在分布式训练中的冲突问题，通过解耦逻辑优化器分配与物理参数分布，在256 GPU上实现1.57倍加速和5.8倍优化器延迟降低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模扩大推动了对矩阵优化器的需求，但这些优化器需要整体更新，与分布式框架中的张量分片存在冲突。现有解决方案存在计算冗余或违反高效通信原语几何约束的问题。

Method: 提出Canzona统一异步负载均衡框架：1）数据并行采用alpha平衡静态分区策略，保持原子性同时平衡负载；2）张量并行采用异步计算流水线，利用微组调度批量处理分片更新并隐藏重构开销。

Result: 在Qwen3模型家族（最多320亿参数）和256个GPU上的评估显示，该方法保持了现有并行架构的效率，端到端迭代时间加速1.57倍，优化器步骤延迟降低5.8倍。

Conclusion: Canzona框架成功解决了矩阵优化器在分布式训练中的冲突问题，通过创新的解耦设计和异步流水线实现了显著的性能提升，为大语言模型的高效训练提供了有效解决方案。

Abstract: The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.

</details>


### [48] [LAAFD: LLM-based Agents for Accelerated FPGA Design](https://arxiv.org/abs/2602.06085)
*Maxim Moraru,Kamalavasan Kamalakkannan,Jered Dominguez-Trujillo,Patrick Diehl,Atanu Barai,Julien Loiseau,Zachary Kent Baker,Howard Pritchard,Galen M Shipman*

Main category: cs.DC

TL;DR: LAAFD是一个基于大语言模型的自动化工作流，能够将通用C++代码转换为优化的Vitis HLS内核，实现接近手工调优的性能（99.9%几何平均性能），显著降低FPGA加速的专业门槛。


<details>
  <summary>Details</summary>
Motivation: FPGA在科学计算和边缘计算中具有高性能、低延迟和能效优势，但由于需要专门的硬件专业知识，其应用受到限制。虽然高级综合（HLS）提高了生产力，但竞争性设计仍需要硬件感知优化和精细的数据流设计。

Method: LAAFD是一个基于大语言模型的智能工作流，能够将通用C++代码自动转换为优化的Vitis HLS内核。该方法自动化关键转换：深度流水线化、向量化和数据流分区，并通过HLS协同仿真和综合反馈形成闭环，验证正确性的同时迭代改进周期执行时间。

Result: 在代表HPC常见计算模式的15个内核测试套件中，LAAFD相比手工调优的Vitis HLS基准实现了99.9%的几何平均性能。对于模板计算工作负载，LAAFD与最先进的基于DSL的HLS代码生成器SODA性能相当，同时生成更易读的内核。

Conclusion: LAAFD显著降低了FPGA加速的专业门槛，同时不牺牲效率，为FPGA在科学和边缘计算中的更广泛采用提供了有前景的解决方案。

Abstract: FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.

</details>


### [49] [BouquetFL: Emulating diverse participant hardware in Federated Learning](https://arxiv.org/abs/2602.06498)
*Arno Geimer*

Main category: cs.DC

TL;DR: BouquetFL是一个联邦学习框架，通过在单台物理机上模拟异构客户端硬件来解决现有研究忽略硬件异构性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数联邦学习研究在中央机器上进行模拟，忽略了实际部署中参与方之间潜在的硬件异构性，这导致了研究方法与实际部署条件之间的差距。

Method: 通过资源限制编程模拟不同的硬件配置，在单台物理机上模拟异构客户端硬件，提供基于真实世界硬件流行度的自定义硬件采样器，以及从常见消费级和小型实验室设备派生的多种硬件配置文件。

Result: 开发了BouquetFL框架，使研究人员能够在不需要多台物理设备的情况下研究联邦学习中的系统异构性，将实验实践更贴近实际部署条件。

Conclusion: BouquetFL为研究高度异构联邦学习的研究人员提供了一个可访问的工具，填补了现有研究方法与实际部署条件之间的差距，支持更真实的联邦学习实验。

Abstract: In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.

</details>


### [50] [FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training](https://arxiv.org/abs/2602.06499)
*Gyeongseo Park,Eungyeong Lee,Song-woo Sok,Myung-Hoon Cha,Kwangwon Koh,Baik-Song An,Hongyeon Kim,Ki-Dong Kang*

Main category: cs.DC

TL;DR: FCDP是一种针对带宽受限集群的分布式训练优化方法，通过将前向传播参数缓存在主机内存中，在反向传播时复用，减少50%的节点间通信，在参数高效微调场景下可减少99%以上的节点间流量。


<details>
  <summary>Details</summary>
Motivation: 现有ZeRO-3优化方案在商用硬件上存在瓶颈：GPU内存缓存方案（如MiCS、ZeRO++）用内存容量换取通信减少，但会导致大模型内存溢出；主机内存卸载方案（如ZeRO-Offload、ZeRO-Infinity）扩展了容量但PCIe开销降低了吞吐量。作者观察到在带宽受限集群中，主机内存可以作为快速缓存层，性能优于节点间通信。

Method: FCDP将前向传播参数缓存在主机内存中，在反向传播时通过快速的节点内all-gather复用这些参数，从而消除冗余的节点间通信。对于参数高效微调，FCDP选择性地只通信可训练参数以最大化缓存效果。该方法保持了ZeRO-3的最小GPU内存占用。

Result: 在商用集群设置中，FCDP相比ZeRO-3实现了高达100倍的吞吐量提升，相比ZeRO++实现了51倍的吞吐量提升，同时保持了ZeRO-3的最大批处理大小。

Conclusion: FCDP通过将主机内存作为快速缓存层而非溢出层，有效解决了带宽受限集群中的分布式训练瓶颈，在保持内存效率的同时显著提升了训练吞吐量，特别适用于参数高效微调场景。

Abstract: Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.

</details>


### [51] [DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving](https://arxiv.org/abs/2602.06502)
*Ying Yuan,Pengfei Zuo,Bo Wang,Zhangyu Chen,Zhipeng Tan,Zhou Yu*

Main category: cs.DC

TL;DR: DualMap是一种用于分布式LLM服务的双映射调度策略，通过两个独立的哈希函数将请求映射到候选实例，智能选择最佳实例，同时实现缓存亲和性和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 在LLM服务中，跨请求重用提示的KV缓存对降低TTFT和服务成本至关重要。现有的调度器无法协调缓存亲和性调度（将相同提示前缀的请求放在一起）和负载均衡调度（将请求均匀分布到计算实例）之间的冲突，因为它们都在单一映射空间中操作。

Method: 提出DualMap双映射调度策略：1）通过两个独立的基于请求提示的哈希函数将每个请求映射到两个候选实例；2）基于当前系统状态智能选择更好的候选实例；3）引入三种技术：SLO感知请求路由、热点感知再平衡、轻量级双哈希环扩展。

Result: 在实际工作负载上的实验表明，在相同的TTFT SLO约束下，DualMap相比最先进的工作将有效请求容量提高了最多2.25倍。

Conclusion: DualMap通过双映射调度策略成功解决了缓存亲和性与负载均衡之间的冲突，提高了分布式LLM服务的效率和性能，特别是在动态和倾斜的实际工作负载下表现出色。

Abstract: In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\times$ under the same TTFT SLO constraints compared with SOTA work.

</details>


### [52] [Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms](https://arxiv.org/abs/2602.06555)
*Lanpei Li,Massimo Coppola,Malio Li,Valerio Besozzi,Jack Bell,Vincenzo Lomonaco*

Main category: cs.DC

TL;DR: 提出了一个在无服务器平台上动态管理结构化并行处理骨架的框架，专注于Farm模式，通过AI驱动的动态扩展来管理并行度，相比基于模型的性能导向方法能更好地适应平台特定限制。


<details>
  <summary>Details</summary>
Motivation: 将类似HPC的性能和弹性引入无服务器和连续计算环境，同时保持骨架编程的可编程性优势。解决无服务器平台上结构化并行处理骨架的动态管理问题。

Method: 基于OpenFaaS平台实现Farm模式，将工作池自动扩展视为QoS感知的资源管理问题。框架将可复用的farm模板与基于Gymnasium的监控控制层结合，暴露队列、时序和QoS指标给反应式和基于学习的控制器。研究AI驱动的动态扩展来管理farm的并行度。

Result: AI驱动的管理相比纯基于模型的性能导向方法能更好地适应平台特定限制，在保持高效资源使用和稳定扩展行为的同时改善QoS。评估了两种强化学习策略与基于简单farm性能模型的反应式管理基线。

Conclusion: 基于AI的管理框架在无服务器平台上对结构化并行处理骨架的动态管理是有效的，能够平衡性能、资源效率和稳定性，为HPC-like性能在无服务器环境中的实现提供了可行方案。

Abstract: We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.

</details>
