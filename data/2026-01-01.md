<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.CR](#cs.CR) [Total: 19]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models](https://arxiv.org/abs/2512.23850)
*Rahul Baxi*

Main category: cs.AI

TL;DR: 该研究提出DDFT评估框架，测量语言模型在语义压缩和对抗性攻击下的认知稳健性，发现模型稳健性与参数规模无关，而主要取决于错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估（如MMLU、TruthfulQA）只测量理想条件下的知识掌握，无法评估模型在信息质量下降或对抗性攻击下的认知稳健性。需要开发能测量模型在现实压力下保持事实准确性的评估方法。

Method: 提出DDFT（Drill-Down and Fabricate Test）评估协议，测量认知稳健性。采用两系统认知模型：语义系统生成流畅文本，认知验证器验证事实准确性。评估9个前沿模型在8个知识领域、5个压缩级别下的表现（共1800次轮级评估）。

Result: 认知稳健性与传统设计范式正交：参数数量（r=0.083）和架构类型（r=0.153）均不显著预测稳健性。错误检测能力强烈预测整体稳健性（rho=-0.817）。旗舰模型尽管规模大但表现出脆弱性，而较小模型可能实现稳健性能。

Conclusion: DDFT框架为评估认知稳健性提供了理论基础和实用工具，挑战了模型规模与可靠性关系的传统假设，表明稳健性主要取决于训练方法和验证机制，而非参数规模。

Abstract: Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

</details>


### [2] [CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution](https://arxiv.org/abs/2512.23880)
*Xu Huang,Junwu Chen,Yuxing Fei,Zhuohan Li,Philippe Schwaller,Gerbrand Ceder*

Main category: cs.AI

TL;DR: CASCADE是一个自我演化的智能体框架，通过持续学习和自我反思两大元技能，使LLM智能体能够掌握复杂外部工具并积累可执行技能，在材料科学和化学研究任务中取得93.3%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体依赖预定义工具或脆弱的工具生成，限制了其在复杂科学任务中的能力和适应性。需要从"LLM+工具使用"向"LLM+技能获取"转变。

Method: CASCADE框架通过两大元技能实现自我演化：1）持续学习（通过网页搜索和代码提取）；2）自我反思（通过内省和知识图谱探索）。框架还包括人机协作和记忆巩固机制。

Result: 在SciSkillBench基准测试（116个材料科学和化学研究任务）中，CASCADE使用GPT-5实现了93.3%的成功率，相比没有演化机制的35.4%有显著提升。在计算分析、自主实验室实验和论文选择性复现等实际应用中表现出色。

Conclusion: CASCADE能够积累可跨智能体和科学家共享的可执行技能，推动可扩展的AI辅助科学研究，代表了从工具使用向技能获取的重要转变。

Abstract: Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from "LLM + tool use" to "LLM + skill acquisition". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.

</details>


### [3] [A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming](https://arxiv.org/abs/2512.23932)
*Ioanna Gemou,Evangelos Lamprou*

Main category: cs.AI

TL;DR: McCoy框架结合大语言模型和答案集编程，通过LLM将医学文献转化为ASP代码，结合患者数据进行疾病诊断，提供可解释的预测系统。


<details>
  <summary>Details</summary>
Motivation: 准确的疾病预测对于及时干预、有效治疗和减少医疗并发症至关重要。虽然符号AI已在医疗保健领域应用，但由于构建高质量知识库需要大量努力，其采用仍然有限。

Method: McCoy框架结合大型语言模型和答案集编程：1）使用LLM将医学文献翻译成ASP代码；2）将生成的ASP代码与患者数据结合；3）使用ASP求解器处理以获得最终诊断结果。

Result: 初步结果显示，McCoy在小规模疾病诊断任务上表现出色，提供了一个强大且可解释的预测框架。

Conclusion: McCoy框架通过整合LLM和ASP，克服了传统符号AI在医疗领域应用的知识库构建障碍，创建了一个既强大又可解释的疾病预测系统。

Abstract: Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.

</details>


### [4] [SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing](https://arxiv.org/abs/2512.24008)
*Gaurab Chhetri,Subasish Das,Tausif Islam Chowdhury*

Main category: cs.AI

TL;DR: SPARK是一个基于多智能体LLM的个性化搜索框架，通过角色化智能体协作实现动态检索和个性化，模拟人类信息寻求行为的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统搜索系统受限于静态用户画像和单一检索流程，无法有效建模用户不断演化的多维信息需求，需要能够捕捉人类信息寻求行为的复杂性、流动性和上下文敏感性的新一代搜索系统。

Method: SPARK框架定义了一个由角色、专业知识、任务上下文和领域构成的角色空间，引入角色协调器动态解释查询并激活最相关的专业智能体。每个智能体执行独立的检索增强生成过程，配备长短期记忆存储和上下文感知推理模块。智能体间通过共享内存库、迭代辩论和中继式知识传递等结构化通信协议进行协作。

Result: 该框架产生了关于协调效率、个性化质量和认知负载分布的可测试预测，同时整合了自适应学习机制以实现持续的角色优化。通过将细粒度智能体专业化与协作检索相结合，SPARK为能够捕捉人类信息寻求行为复杂性的下一代搜索系统提供了理论见解。

Conclusion: SPARK展示了如何通过分布式智能体行为和最小协调规则产生涌现的个性化特性，为构建能够建模用户演化、多维信息需求的个性化搜索系统提供了新的框架和方法论基础。

Abstract: Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.

</details>


### [5] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自进化智能体框架，通过将LLM集成到"计划-执行-总结"认知范式中，解决了传统进化方法在代码空间中的结构推理不足问题，显著提高了进化效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法在从静态LLM向自进化智能体过渡时存在结构推理不足的问题，现有方法在高维代码空间中容易过早收敛且探索效率低下，需要新的框架来解决这些挑战。

Method: LoongFlow采用"计划-执行-总结"认知范式，将LLM集成到进化搜索中，使其成为推理密集型过程。同时引入混合进化记忆系统，结合多岛屿模型、MAP-Elites和自适应玻尔兹曼选择，平衡探索与利用的权衡。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow相比领先基线（如OpenEvolve、ShinkaEvolve）进化效率提升高达60%，并能发现更优的解决方案。

Conclusion: LoongFlow代表了自主科学发现的重要进展，能够在减少计算开销的同时生成专家级解决方案，为自进化智能体框架的发展开辟了新方向。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


### [6] [Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks](https://arxiv.org/abs/2512.24156)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.AI

TL;DR: 提出一种无需训练、基于图结构的交互推理方法，在ARC-AGI-3基准测试中显著优于当前最先进的LLM模型


<details>
  <summary>Details</summary>
Motivation: ARC-AGI-3基准测试包含类似游戏的任务，需要智能体通过有限交互推断任务机制并适应递增的复杂性。当前最先进的LLM无法可靠解决这些任务，因此需要探索新的方法。

Method: 结合视觉帧处理与基于图结构的系统状态空间探索。方法包括：将视觉帧分割为有意义的组件，基于视觉显著性优先选择动作，维护探索状态和转换的有向图，通过跟踪已访问状态和已测试动作来优先选择到达未测试状态-动作对的最短路径。

Result: 在ARC-AGI-3预览挑战中，该方法在6个游戏的52个关卡中解决了中位数30个关卡，在私有排行榜上排名第3，显著优于前沿的基于LLM的智能体。

Conclusion: 即使无需学习，显式的图结构探索也可以作为交互推理的强大基线，并强调了在稀疏反馈环境中系统状态跟踪和动作优先级的重要性，这些是当前LLM无法捕捉任务动态的关键因素。

Abstract: We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.

</details>


### [7] [Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment](https://arxiv.org/abs/2512.24263)
*Lijun Zhang,Lin Li,Wei Wei,Yajie Qi,Huizhong Song,Jun Wang,Yaodong Yang,Jiye Liang*

Main category: cs.AI

TL;DR: 本文提出了一种风险感知的逐步对齐方法（RSA），通过引入嵌套风险度量来增强语言模型安全对齐过程中的风险控制能力，有效抑制低概率高危害行为。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法（如Safe RLHF和SACPO）通常采用风险中性范式，无法充分应对参考策略偏差带来的风险，且对罕见但可能灾难性的有害行为鲁棒性有限。需要一种能明确纳入风险意识的方法来增强安全性和可信度。

Method: 提出风险感知逐步对齐（RSA）方法，将安全对齐建模为token级别的风险感知约束策略优化问题，利用嵌套风险度量设计逐步对齐过程，生成基于嵌套风险度量的token级别策略更新。

Result: 实验结果表明，该方法在保持高帮助性的同时确保了强安全性，显著抑制了尾部风险（低概率高危害的不安全响应）。理论分析在温和假设下证明了策略最优性。

Conclusion: RSA方法通过明确的风险感知设计，有效解决了现有安全对齐方法的风险控制不足问题，为语言模型的安全对齐提供了更鲁棒的解决方案。

Abstract: When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we propose Risk-aware Stepwise Alignment (RSA), a novel alignment method that explicitly incorporates risk awareness into the policy optimization process by leveraging a class of nested risk measures. Specifically, RSA formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it through a stepwise alignment procedure that yields token-level policy updates derived from the nested risk measures. This design offers two key benefits: (1) it mitigates risks induced by excessive model shift away from a reference policy, and (2) it explicitly suppresses low-probability yet high-impact harmful behaviors. Moreover, we provide theoretical analysis on policy optimality under mild assumptions. Experimental results demonstrate that our method achieves high levels of helpfulness while ensuring strong safety and significantly suppresses tail risks, namely low-probability yet high-impact unsafe responses.

</details>


### [8] [What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?](https://arxiv.org/abs/2512.24497)
*Basile Terver,Tsung-Yen Yang,Jean Ponce,Adrien Bardes,Yann LeCun*

Main category: cs.AI

TL;DR: 本文提出JEPA-WM模型框架，研究在表示空间进行规划的方法，通过系统实验确定最优技术组合，在导航和操作任务上超越现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决AI中长期存在的挑战：开发能够解决广泛物理任务并能泛化到未见任务和环境的智能体。当前基于世界模型和规划的方法通常在输入空间进行规划，而本文关注在表示空间进行规划的方法，旨在通过抽象无关细节实现更高效的规划。

Method: 将这类方法定义为JEPA-WMs，系统研究模型架构、训练目标和规划算法等关键组件。在模拟环境和真实机器人数据上进行实验，分析各组件对规划成功率的影响，最终提出优化的模型组合。

Result: 提出的模型在导航和操作任务上超越了DINO-WM和V-JEPA-2-AC两个基准模型。代码、数据和检查点已公开。

Conclusion: 通过系统研究JEPA-WMs家族中的技术选择，确定了实现高效表示空间规划的最优方法组合，为物理任务智能体的发展提供了重要技术进展。

Abstract: A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.

</details>


### [9] [Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems](https://arxiv.org/abs/2512.24505)
*Samuel Golladay,Majid Bani-Yaghoub*

Main category: cs.AI

TL;DR: 该研究分析了三个主流大语言模型（GPT-4o-mini、Gemini-2.0-Flash、DeepSeek-V3）在代表性不足的数学竞赛问题（密苏里大学数学竞赛）上的表现，发现DeepSeek-V3在所有三个数学领域（微积分、解析几何、离散数学）表现最佳，但所有模型在几何问题上表现都明显较弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多使用相同的数据集评估大语言模型的数学推理能力，这限制了研究结果的普适性，可能无法充分捕捉数学任务中的多样化挑战。本研究旨在通过分析LLMs在代表性不足的数学竞赛问题上的表现，获得更全面的评估。

Method: 使用密苏里大学数学竞赛中的微积分、解析几何和离散数学问题，对三个主流LLMs（GPT-4o-mini、Gemini-2.0-Flash、DeepSeek-V3）进行测试。将模型回答与已知正确答案进行比较以确定准确率，并分析模型的推理过程以探索不同问题类型和模型间的错误模式。

Result: DeepSeek-V3在微积分、解析几何和离散数学三个领域均表现最佳，包括推理过程和最终答案的正确性。所有三个LLMs在几何问题上都表现出明显薄弱。DeepSeek-V3的主要错误是计算和逻辑错误，GPT-4o-mini经常出现逻辑和方法相关错误，而Gemini则倾向于推理不完整和匆忙得出结论。

Conclusion: 在代表性不足的数学竞赛数据集上评估LLMs可以提供对其独特错误模式的更深入洞察，并突显结构化推理方面的持续挑战，特别是在几何领域。这种评估方法有助于更全面地理解LLMs的数学推理能力。

Abstract: Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.

</details>


### [10] [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)
*Amir Tahmasbi,Sadegh Majidi,Kazem Taram,Aniket Bera*

Main category: cs.AI

TL;DR: 论文提出两阶段方法提升大语言模型空间推理能力：先通过监督微调学习基础空间变换，再使用GRPO框架训练LoRA适配器进行多步规划，在ASCII艺术环境中验证效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用语言能力上表现出色，但在空间变换和结构化环境中的多步规划方面仍然存在困难。需要提升LLMs在空间推理任务上的能力，特别是在导航和规划等应用场景中。

Method: 采用两阶段方法：1）通过监督微调学习基础空间变换（旋转、平移、缩放），使模型具备基本空间物理知识；2）冻结物理感知模型，在GRPO框架内训练轻量级LoRA适配器，以闭环方式学习组合这些基础模块进行多步规划。为此合成了ASCII艺术数据集并构建了相应的强化学习环境。

Result: 该方法在动态环境（有显式状态更新）和静态环境（依赖内部状态）中均优于基线方法，包括通用主干模型、物理感知模型和端到端强化学习模型。此外，该方法收敛更快，训练更稳定。注意力模式分析表明微调确实带来了空间理解能力的提升。

Conclusion: 提出的两阶段方法能有效提升大语言模型的空间推理能力，通过分解空间推理为原子构建块及其组合，在结构化环境中实现了更好的多步规划性能，为LLMs在导航和规划等应用中的实际部署提供了有效解决方案。

Abstract: Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.

</details>


### [11] [MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use](https://arxiv.org/abs/2512.24565)
*Wenrui Liu,Zixiang Liu,Elsie Dai,Wenhan Yu,Lei Yu,Tong Yang*

Main category: cs.AI

TL;DR: MCPAgentBench：基于真实MCP定义构建的基准测试，用于评估智能体工具使用能力，包含真实任务和模拟工具，采用动态沙盒环境测试工具选择与辨别能力。


<details>
  <summary>Details</summary>
Motivation: 当前MCP评估集存在依赖外部MCP服务和缺乏难度感知的问题，需要更有效的基准来评估LLM作为自主智能体的工具使用能力。

Method: 构建包含真实任务和模拟MCP工具的数据集，采用动态沙盒环境，向智能体提供包含干扰项的工具候选列表，测试其工具选择和辨别能力，并引入综合指标衡量任务完成率和执行效率。

Result: 在多种最新主流大语言模型上的实验显示，在处理复杂多步骤工具调用时存在显著的性能差异。

Conclusion: MCPAgentBench为解决现有MCP评估局限性提供了有效方案，能够全面评估智能体的工具使用能力，代码已开源。

Abstract: Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.

</details>


### [12] [Recursive Language Models](https://arxiv.org/abs/2512.24601)
*Alex L. Zhang,Tim Kraska,Omar Khattab*

Main category: cs.AI

TL;DR: 提出递归语言模型（RLMs），通过让LLMs以编程方式检查、分解并递归调用自身来处理超长提示，实现超出模型上下文窗口两个数量级的输入处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型受限于固定的上下文窗口长度，无法处理任意长度的提示。需要一种方法让LLMs能够处理超出其原始设计限制的长文本输入。

Method: 提出递归语言模型（RLMs）推理策略，将长提示视为外部环境，允许LLM以编程方式检查、分解提示，并对提示片段进行递归调用，实现逐步处理超长内容。

Result: RLMs成功处理超出模型上下文窗口两个数量级的输入，在四个不同的长上下文任务中，即使对于较短的提示，也显著优于基础LLMs和常见的长上下文框架，同时查询成本相当或更低。

Conclusion: 递归语言模型提供了一种有效的方法来扩展LLMs处理长提示的能力，通过推理时扩展策略实现了对任意长度输入的可靠处理，具有实际应用价值。

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.

</details>


### [13] [Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization](https://arxiv.org/abs/2512.24609)
*Dong Qiu,Duo Xu,Limengxi Yue*

Main category: cs.AI

TL;DR: 提出一个强化学习增强的LLM多智能体协作框架，通过Dec-POMDP建模和CTDE训练，在协作写作和编程任务中显著提升效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单智能体任务中表现良好，但在多智能体协作场景中缺乏全局优化能力，难以协调多个智能体以实现整体最优性能。

Method: 1) 将多智能体协作建模为去中心化部分可观察马尔可夫决策过程(Dec-POMDP)；2) 采用集中训练分散执行(CTDE)框架；3) 提出组相对策略优化(GRPO)算法，在训练时利用全局信号联合优化智能体策略；4) 设计简化的联合奖励函数，平衡任务质量、速度和协调成本。

Result: 在协作写作和编程基准测试中：1) 任务处理速度比单智能体基线提高3倍；2) 写作任务中达到98.7%的结构/风格一致性；3) 编程任务中达到74.6%的测试通过率；4) 持续优于其他多智能体LLM基线方法。

Conclusion: 该框架为复杂工作流中的可靠协作提供了实用路径，通过强化学习增强的LLM多智能体系统能够有效解决协作优化问题，在保持高质量输出的同时显著提升效率。

Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.

</details>


### [14] [Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions](https://arxiv.org/abs/2512.24679)
*Pengcheng Xia,Yixiang Huang,Chengjin Qin,Chengliang Liu*

Main category: cs.AI

TL;DR: 提出多模态跨域混合融合模型，通过双重解耦框架分离模态不变/特定特征和域不变/特定表示，结合跨域混合融合策略和三模态融合机制，提升电机故障诊断在未见工况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有智能故障诊断方法在真实场景中面对未见工况时性能显著下降，域适应方法依赖目标域样本，且多数研究仅使用单模态信号，忽略了多模态信息的互补性对提升模型泛化能力的作用。

Method: 提出多模态跨域混合融合模型，包含：1）双重解耦框架，分离模态不变特征和模态特定特征，以及域不变表示和域特定表示；2）跨域混合融合策略，随机混合跨域的模态信息以增强模态和域多样性；3）三模态融合机制，自适应整合多模态异构信息。

Result: 在感应电机故障诊断任务中，针对未见恒定工况和时变工况进行实验，结果表明该方法始终优于先进方法，综合消融研究进一步验证了每个提出组件和多模态融合的有效性。

Conclusion: 该方法通过双重解耦、跨域混合融合和多模态自适应融合，有效提升了故障诊断模型在未见工况下的泛化性能，充分利用了多模态信息的互补优势。

Abstract: Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.

</details>


### [15] [Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences](https://arxiv.org/abs/2512.24829)
*Emmanuel Fashae,Michael Burke,Leimin Tian,Lingheng Meng,Pamela Carreno-Medrano*

Main category: cs.AI

TL;DR: 该研究提出了一个包含四个可解释构念的显式物体排列偏好模型：空间实用性、习惯便利性、语义连贯性和常识适当性，并通过问卷验证了这些构念，最终将其集成到MCTS规划器中用于机器人规划。


<details>
  <summary>Details</summary>
Motivation: 当前基于人类演示的机器人家庭物体重排系统虽然预测有效，但提供的可解释性有限，无法深入理解指导人类决策的因素。需要开发一个能够明确解释物体排列偏好的模型。

Method: 1. 提出了包含四个可解释构念的物体排列偏好模型；2. 设计并验证了自报告问卷，通过63名参与者的在线研究确认构念的心理区分度；3. 将验证后的偏好模型集成到蒙特卡洛树搜索规划器中；4. 在厨房和客厅两种场景下测试了规划器的性能。

Result: 1. 问卷研究证实了四个构念的心理区分性和解释力；2. 基于参与者偏好指导的MCTS规划器能够生成合理的物体排列方案；3. 规划器生成的排列与参与者生成的排列高度一致。

Conclusion: 该研究贡献了一个紧凑、可解释的物体排列偏好模型，并展示了如何将其操作化用于机器人规划，为机器人系统提供了更透明、可解释的决策框架。

Abstract: Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.

</details>


### [16] [GenZ: Foundational models as latent variable generators within traditional statistical models](https://arxiv.org/abs/2512.24834)
*Marko Jojic,Nebojsa Jojic*

Main category: cs.AI

TL;DR: GenZ是一个混合模型，通过可解释的语义特征桥接基础模型和统计建模。它通过迭代过程发现数据集特定的语义特征，而不是依赖基础模型的通用知识，在房价预测和电影推荐任务上显著优于GPT-5基线。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型拥有广泛的领域知识，但往往无法捕捉对预测任务至关重要的数据集特定模式。现有方法过度依赖基础模型的领域理解，而忽略了数据集本身的统计特性。

Method: 提出一种广义EM算法，通过迭代对比统计建模误差识别的项目组来发现语义特征描述。该方法使用冻结的基础模型基于发现的特征对项目进行分类，将这些判断视为潜在二元特征的噪声观测，这些特征通过学习的统计关系预测实值目标。

Result: 在房价预测任务中，使用多模态列表数据发现的语义特征实现了12%的中位数相对误差，显著优于依赖LLM通用知识的GPT-5基线（38%误差）。在Netflix电影嵌入预测中，仅从语义描述就能达到0.59的余弦相似度，相当于传统协同过滤需要约4000个用户评分才能达到的性能。

Conclusion: GenZ成功地将基础模型的语义理解与统计建模相结合，发现了数据集特定的模式（如预测本地房地产市场的建筑细节、预测用户偏好的系列电影成员资格），这些模式与模型单独依赖的领域知识有所不同，证明了混合方法的有效性。

Abstract: We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.

</details>


### [17] [A study on constraint extraction and exception exclusion in care worker scheduling](https://arxiv.org/abs/2512.24853)
*Koki Suenaga,Tomohiro Furuta,Satoshi Ono*

Main category: cs.AI

TL;DR: 提出一种基于约束模板的方法，用于从养老院管理者访谈中提取设施特定的排班约束条件，避免提取异常约束，并通过约束规划求解器生成护理人员排班表。


<details>
  <summary>Details</summary>
Motivation: 养老机构的排班条件因设施而异，需要通过与制定排班表的管理者访谈来设计设施特定的约束条件，但现有约束提取技术缺乏排除异常约束的机制。

Method: 使用约束模板提取各种组件的组合（如连续工作日的班次模式或员工组合），通过改变关注的天数和员工数量，以及将提取重点调整为模式或频率，来提取多样化的约束条件，并包含排除异常约束的机制。

Result: 实验表明，该方法成功创建了满足所有硬约束的排班表，并通过避免提取异常约束，减少了软约束的违反次数。

Conclusion: 提出的约束模板方法能够有效提取养老院特定的排班约束条件，排除异常约束，从而生成更符合实际需求的护理人员排班表。

Abstract: Technologies for automatically generating work schedules have been extensively studied; however, in long-term care facilities, the conditions vary between facilities, making it essential to interview the managers who create shift schedules to design facility-specific constraint conditions. The proposed method utilizes constraint templates to extract combinations of various components, such as shift patterns for consecutive days or staff combinations. The templates can extract a variety of constraints by changing the number of days and the number of staff members to focus on and changing the extraction focus to patterns or frequency. In addition, unlike existing constraint extraction techniques, this study incorporates mechanisms to exclude exceptional constraints. The extracted constraints can be employed by a constraint programming solver to create care worker schedules. Experiments demonstrated that our proposed method successfully created schedules that satisfied all hard constraints and reduced the number of violations for soft constraints by circumventing the extraction of exceptional constraints.

</details>


### [18] [Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing](https://arxiv.org/abs/2512.24896)
*Andrii Gamalii,Daniel Górniak,Robert Nowak,Bartłomiej Olber,Krystian Radlak,Jakub Winter*

Main category: cs.AI

TL;DR: DARTS项目开发了一个半自动数据标注流水线，用于创建波兰驾驶场景的大规模多模态数据集，通过人机协同方法结合AI与人工标注，显著降低了标注成本和时间。


<details>
  <summary>Details</summary>
Motivation: 手动标注异质驾驶数据成本高、耗时长，需要开发更高效的标注方法来支持波兰自动驾驶研究。

Method: 采用人在回路方法，结合AI自动生成初始标注、迭代模型重训练，并包含数据匿名化和领域适应技术，核心使用3D目标检测算法生成初步标注。

Result: 开发出的工具和方法显著节省了时间，同时确保跨不同传感器模态的一致高质量标注。

Conclusion: 该解决方案直接支持DARTS项目，加速了符合项目标准化格式的大规模标注数据集准备，增强了波兰自动驾驶研究的技术基础。

Abstract: This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.

</details>


### [19] [Iterative Deployment Improves Planning Skills in LLMs](https://arxiv.org/abs/2512.24940)
*Augusto B. Corrêa,Yoav Gelberg,Luckeciano C. Melo,Ilia Shumailov,André G. Pereira,Yarin Gal*

Main category: cs.AI

TL;DR: 迭代部署LLM并通过用户筛选数据进行微调，能显著改变模型特性，在规划任务中实现能力提升和泛化，这本质上是一种隐式强化学习机制。


<details>
  <summary>Details</summary>
Motivation: 研究迭代部署大型语言模型时，用户从先前模型部署中精心筛选数据来微调后续模型，这种机制如何影响模型特性变化，特别是在规划能力方面。

Method: 在多个规划领域测试迭代部署机制：每个新模型都基于用户从前一模型部署中精心筛选的数据进行微调，观察模型特性的演变。

Result: 观察到规划能力显著提升，后续模型展现出涌现的泛化能力，能够发现比初始模型长得多的规划方案。

Conclusion: 迭代部署本质上实现了外层循环的强化学习训练，具有隐式奖励函数。这对AI安全有重要启示（奖励函数未明确定义），也为显式强化学习提供了一种基于数据筛选的替代训练机制。

Abstract: We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.

</details>


### [20] [AMAP Agentic Planning Technical Report](https://arxiv.org/abs/2512.24957)
*Yulan Hu,Xiangwen Zhang,Sheng Ouyang,Hao Yi,Lu Xu,Qinglin Lang,Lide Tan,Xiang Cheng,Tianchen Ye,Zhicong Li,Ge Chen,Wenjin Yang,Zheng Pan,Shaopan Xiong,Siran Yang,Ju Huang,Yan Zhang,Jiamang Wang,Yong Liu,Yinfeng Huang,Tucheng Lin,Xin Li,Ning Guo*

Main category: cs.AI

TL;DR: STAgent是一个专门用于时空理解的智能体大语言模型，能够处理受限兴趣点发现和行程规划等复杂任务，通过工具交互实现探索、验证和推理，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门针对时空理解场景的智能体模型，能够处理复杂的时空任务如兴趣点发现和行程规划，同时保持模型的通用能力。

Method: 采用三个关键技术：1）包含10多个领域特定工具的稳定工具环境；2）分层数据筛选框架，以1:10000的比例筛选高质量查询；3）级联训练方法，包括种子SFT阶段、高确定性查询的SFT阶段和低确定性数据的RL阶段。

Result: STAgent在TravelBench基准测试中表现出色，同时在广泛的通用基准测试中保持了良好的性能，证明了所提出的智能体模型的有效性。

Conclusion: STAgent成功展示了专门针对时空理解任务的智能体大语言模型的可行性，通过精心设计的工具环境、数据筛选和训练方法，实现了在特定领域任务上的优异表现同时不损害通用能力。

Abstract: We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [21] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: HERO Sign：一种GPU加速的SPHINCS+签名方案，通过分层调优和编译器优化，在RTX 4090等GPU上实现1.28-3.13倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: SPHINCS+作为后量子安全的无状态哈希签名方案，其签名生成因密集哈希计算而缓慢。现有GPU优化未能充分利用Merkle树结构的并行性，也缺乏跨不同计算内核的细粒度编译器级定制。

Method: 提出分层调优和高效编译时优化：1) 重新审视SPHINCS+组件（FORS、MSS、WOTS+）的数据独立性并行机会；2) 为FORS引入Tree Fusion策略，由自动Tree Tuning搜索算法指导；3) 采用自适应编译策略，根据内核特性在PTX和本地代码路径间选择；4) 批处理签名生成时使用基于任务图的构造优化内核级重叠。

Result: 在RTX 4090上，相比现有GPU实现，HERO Sign在SPHINCS+ 128f、192f、256f参数集下分别实现1.28-3.13、1.28-2.92、1.24-2.60倍的吞吐量提升。在A100、H100、GTX 2080上也观察到类似增益，内核启动延迟降低两个数量级。

Conclusion: HERO Sign通过分层调优和编译器优化，有效解决了SPHINCS+ GPU加速中的并行性利用不足和编译器级定制缺失问题，显著提升了签名生成性能。

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [22] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 提出基于测量的容器资源管理框架CRMS，通过非线性拟合模型和两阶段优化，在边缘服务器上联合最小化延迟和功耗，相比基线方法降低14%以上延迟并提高能效。


<details>
  <summary>Details</summary>
Motivation: 边缘计算需要处理延迟敏感应用，但任务异构性和资源有限性给高效编排带来挑战。现有方法难以在单边缘服务器上同时优化多个异构应用的延迟和能耗。

Method: 1) 通过大量性能分析实验建立CPU/内存分配与处理延迟的非线性拟合模型；2) 基于排队延迟公式构建混合整数非线性规划问题；3) 将NP难问题分解为可处理的凸子问题；4) 设计两阶段容器资源管理方案，结合凸优化和贪心细化。

Result: CRMS相比启发式和搜索基线方法降低14%以上延迟，同时提高能效。方案具有多项式时间复杂度，支持在全局资源约束下的准动态执行。

Conclusion: 提出的测量驱动容器资源管理框架为异构边缘环境提供实用可扩展的解决方案，能够有效处理动态工作负载特征，在延迟和能耗方面取得显著优化效果。

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [23] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV是一个针对长上下文生成的KV缓存管理框架，通过专门设计的压缩算法和系统架构协同设计，在保持高计算效率的同时显著减少内存使用并提高执行吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型在实际应用中表现出巨大潜力，但长上下文推理面临KV缓存内存需求巨大的挑战，随着序列长度和批量大小增加，KV缓存可达数GB级别，需要高效的压缩管理方案。

Method: PackKV引入专门针对KV缓存数据特性设计的有损压缩技术，结合压缩算法和系统架构的协同设计，支持KV缓存动态增长特性，同时保持高计算效率。

Result: 在相同最小精度损失下，PackKV相比现有量化方法平均实现K缓存153.2%、V缓存179.6%更高的内存减少率；在A100和RTX Pro 6000 GPU上，相比cuBLAS矩阵向量乘法内核，平均实现K缓存75.7%、V缓存171.7%的吞吐量提升，同时需要更少的GPU内存带宽。

Conclusion: PackKV提供了一个通用高效的KV缓存管理框架，通过专门设计的压缩技术和系统架构协同优化，有效解决了长上下文生成中的内存瓶颈问题，在保持精度的同时显著提升了内存效率和执行性能。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [24] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 本文研究了LLM大规模训练中检查点/恢复的I/O性能问题，开发了基于liburing的微基准测试，发现通过文件系统感知的聚合和合并策略可显著提升检查点写入吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和基础模型规模扩大，检查点/恢复成为训练和推理的关键模式。在3D并行（张量、流水线、数据）架构下，检查点涉及大量进程管理不同形状和大小的张量，需要频繁持久化到稳定存储，这使检查点/恢复成为具有数据量大、种类多、速度快的I/O问题。现有方案在并发条件下存在性能瓶颈，需要探索更有效的I/O优化策略。

Method: 开发微基准测试来量化使用liburing时的权衡，评估聚合、对齐和I/O合并在缓冲I/O和直接I/O下的交互效果。研究文件系统感知的聚合策略，分析不同I/O模式对性能的影响。

Result: 发现未合并的小缓冲区操作使吞吐量相比合成工作负载减半，而文件系统感知的聚合恢复了带宽并减少了元数据开销。相比最先进的LLM检查点引擎，该方法实现了比DataStates-LLM高3.9倍的写入吞吐量，比TorchSnapshot高7.6倍。

Conclusion: 研究结果表明需要与现代文件系统和I/O后端对齐的聚合和合并策略。liburing等内核加速I/O库在LLM检查点场景中具有显著性能优势，但需要针对特定存储栈特性进行优化设计。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [25] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出首个资源自适应的分布式双层优化框架RABO和RAFBO，采用二阶自由超梯度估计器，允许客户端根据可用资源优化子模型，达到渐进最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，传统分布式双层优化算法无法直接应用于低资源客户端，主要原因在于优化内外层函数的计算量过大。需要设计资源自适应的方法来解决这一问题。

Method: 提出资源自适应分布式双层优化框架，包含二阶自由超梯度估计器，允许客户端根据可用资源优化子模型。由于外层参数x和内层参数y的耦合影响，需要重新理论分析全局平均超梯度的上界，并重新表述局部部分训练的内层参数误差界。

Result: 理论证明显示RABO和RAFBO都能达到渐进最优收敛速率$O(1/\sqrt{C_x^{\ast}Q})$，该速率由外层参数的最小覆盖度$C_x^{\ast}$主导。在两个不同任务上的大量实验证明了所提方法的有效性和计算效率。

Conclusion: 提出的资源自适应分布式双层优化框架成功解决了传统方法在低资源环境下的应用限制，通过资源自适应设计和二阶自由超梯度估计器，实现了高效的双层优化，为大规模模型在分布式环境中的部署提供了有效解决方案。

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [26] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 提出AI驱动的多集群云系统自适应资源优化框架，通过预测学习和策略感知决策实现跨集群主动协调管理


<details>
  <summary>Details</summary>
Motivation: 现代云原生系统依赖多集群部署实现可扩展性、弹性和地理分布，但现有资源管理方法主要是反应式和集群中心的，无法在动态工作负载下优化系统范围行为，导致资源利用效率低、适应延迟和运维开销增加

Method: 提出AI驱动的自适应资源优化框架，整合预测学习、策略感知决策和持续反馈机制，通过分析跨集群遥测数据和历史执行模式，动态调整资源分配以平衡性能、成本和可靠性目标

Result: 原型实现展示了相比传统反应式方法，在资源效率、工作负载波动时的快速稳定性和性能变异性减少方面的改进

Conclusion: 智能自适应基础设施管理是构建可扩展和弹性云平台的关键推动因素

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [27] [Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory](https://arxiv.org/abs/2512.23760)
*Ken Huang,Jerry Huang*

Main category: cs.CR

TL;DR: ASG-SI框架将智能体自我改进视为技能图的迭代编译过程，通过可验证的技能提取、审计和合成，解决强化学习智能体在部署中的安全与治理挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大型语言模型智能体在自我改进过程中存在安全与治理挑战：优化压力可能导致奖励黑客行为、行为漂移难以审计或复现、改进通常隐藏在不可复用的参数更新中。需要一种可审计、可复现的自我改进框架。

Method: 提出ASG-SI框架，将自我改进视为技能图的迭代编译。从成功轨迹中提取候选改进，将其规范化为具有明确接口的技能，通过验证器支持的复现和合约检查后才提升为正式技能。奖励被分解为可重构的组件，支持独立审计。集成经验合成进行压力测试，以及持续记忆控制以保持长视野性能。

Result: 提供了完整的系统架构、威胁模型和安全分析，以及可运行的参考实现，展示了验证器支持的奖励构建、技能编译、审计日志记录，以及在持续任务流下的可测量改进。

Conclusion: ASG-SI将智能体自我改进重新定义为可验证、可重用能力的积累过程，为自我改进AI智能体的可复现评估和操作治理提供了实用路径。

Abstract: Reinforcement learning is increasingly used to transform large language models into agentic systems that act over long horizons, invoke tools, and manage memory under partial observability. While recent work has demonstrated performance gains through tool learning, verifiable rewards, and continual training, deployed self-improving agents raise unresolved security and governance challenges: optimization pressure can incentivize reward hacking, behavioral drift is difficult to audit or reproduce, and improvements are often entangled in opaque parameter updates rather than reusable, verifiable artifacts.
  This paper proposes Audited Skill-Graph Self-Improvement (ASG-SI), a framework that treats self-improvement as iterative compilation of an agent into a growing, auditable skill graph. Each candidate improvement is extracted from successful trajectories, normalized into a skill with an explicit interface, and promoted only after passing verifier-backed replay and contract checks. Rewards are decomposed into reconstructible components derived from replayable evidence, enabling independent audit of promotion decisions and learning signals. ASG-SI further integrates experience synthesis for scalable stress testing and continual memory control to preserve long-horizon performance under bounded context.
  We present a complete system architecture, threat model, and security analysis, and provide a fully runnable reference implementation that demonstrates verifier-backed reward construction, skill compilation, audit logging, and measurable improvement under continual task streams. ASG-SI reframes agentic self-improvement as accumulation of verifiable, reusable capabilities, offering a practical path toward reproducible evaluation and operational governance of self-improving AI agents.

</details>


### [28] [Secure and Governed API Gateway Architectures for Multi-Cluster Cloud Environments](https://arxiv.org/abs/2512.23774)
*Vinoth Punniyamoorthy,Kabilan Kannan,Akshay Deshpande,Lokesh Butra,Akash Kumar Agarwal,Adithya Parthasarathy,Suhas Malempati,Bikesh Kumar*

Main category: cs.CR

TL;DR: 提出一种面向多集群云环境的治理感知、意图驱动的API网关协调管理架构，通过高层声明式意图表达安全、治理和性能目标，实现配置自动转换和持续验证，显著减少策略漂移并提升配置传播效率。


<details>
  <summary>Details</summary>
Motivation: 随着多云和混合云部署的普及，在异构网关环境中保持一致的策略执行、可预测的性能和操作稳定性变得日益困难。现有方法通常将安全、治理和性能作为松散耦合的关注点处理，导致配置漂移、策略传播延迟以及动态工作负载下的运行时行为不稳定。

Method: 提出治理感知、意图驱动的API网关管理架构，将安全、治理和性能目标表达为高层声明式意图，系统性地转换为可执行的网关配置，并通过策略验证和遥测驱动的反馈进行持续验证。该架构将意图规范与执行解耦，同时支持有界、策略合规的适配，兼容异构网关实现。

Result: 在多个Kubernetes集群上的原型实现显示：与手动和声明式基线方法相比，策略漂移减少高达42%，配置传播时间提升31%，在可变工作负载下p95延迟开销持续低于6%。

Conclusion: 治理感知、意图驱动的网关编排为安全、一致且性能可预测的云原生平台提供了可扩展且可靠的基础，能够有效解决多集群环境中API网关管理的挑战。

Abstract: API gateways serve as critical enforcement points for security, governance, and traffic management in cloud-native systems. As organizations increasingly adopt multi-cluster and hybrid cloud deployments, maintaining consistent policy enforcement, predictable performance, and operational stability across heterogeneous gateway environments becomes challenging. Existing approaches typically manage security, governance, and performance as loosely coupled concerns, leading to configuration drift, delayed policy propagation, and unstable runtime behavior under dynamic workloads. This paper presents a governance-aware, intent-driven architecture for coordinated API gateway management in multi-cluster cloud environments. The proposed approach expresses security, governance, and performance objectives as high-level declarative intents, which are systematically translated into enforceable gateway configurations and continuously validated through policy verification and telemetry-driven feedback. By decoupling intent specification from enforcement while enabling bounded, policy-compliant adaptation, the architecture supports heterogeneous gateway implementations without compromising governance guarantees or service-level objectives. A prototype implementation across multiple Kubernetes clusters demonstrates the effectiveness of the proposed design. Experimental results show up to a 42% reduction in policy drift, a 31% improvement in configuration propagation time, and sustained p95 latency overhead below 6% under variable workloads, compared to manual and declarative baseline approaches. These results indicate that governance-aware, intent-driven gateway orchestration provides a scalable and reliable foundation for secure, consistent, and performance-predictable cloud-native platforms.

</details>


### [29] [SyncGait: Robust Long-Distance Authentication for Drone Delivery via Implicit Gait Behaviors](https://arxiv.org/abs/2512.23778)
*Zijian Ling,Man Zhou,Hongda Zhai,Yating Huang,Lingchen Zhao,Qi Li,Chao Shen,Qian Wang*

Main category: cs.CR

TL;DR: SyncGait是一种基于步态的无人机配送隐式双向认证系统，利用用户行走时的独特手臂摆动模式，在远距离（>18米）实现高精度认证，无需额外硬件或特定认证动作。


<details>
  <summary>Details</summary>
Motivation: 无人机配送中需要确保无人机与用户之间的安全距离，直到完成双向认证。现有认证方案存在距离限制和抗攻击能力不足的问题，需要一种既安全又用户友好的远距离认证方案。

Method: SyncGait利用用户走向无人机时的独特手臂摆动步态特征进行隐式双向认证。系统通过分析用户的步态模式实现认证，无需额外硬件或特定认证动作。

Result: 在31名受试者的14个数据集上进行广泛实验，SyncGait在远距离（>18米）实现了99.84%的平均准确率，并对各种欺骗攻击表现出强大的抗攻击能力。

Conclusion: SyncGait为无人机配送提供了一个鲁棒、安全且用户友好的双向认证解决方案，能够在真实场景中有效工作，解决了现有方案的认证距离限制和安全漏洞问题。

Abstract: In recent years, drone delivery, which utilizes unmanned aerial vehicles (UAVs) for package delivery and pickup, has gradually emerged as a crucial method in logistics. Since delivery drones are expensive and may carry valuable packages, they must maintain a safe distance from individuals until user-drone mutual authentication is confirmed. Despite numerous authentication schemes being developed, existing solutions are limited in authentication distance and lack resilience against sophisticated attacks. To this end, we introduce SyncGait, an implicit gait-based mutual authentication system for drone delivery. SyncGait leverages the user's unique arm swing as he walks toward the drone to achieve mutual authentication without requiring additional hardware or specific authentication actions. We conducted extensive experiments on 14 datasets collected from 31 subjects. The results demonstrate that SyncGait achieves an average accuracy of 99.84\% at a long distance ($>18m$) and exhibits strong resilience against various spoofing attacks, making it a robust, secure, and user-friendly solution in real-world scenarios.

</details>


### [30] [Application-Specific Power Side-Channel Attacks and Countermeasures: A Survey](https://arxiv.org/abs/2512.23785)
*Sahan Sanjaya,Aruna Jayasena,Prabhat Mishra*

Main category: cs.CR

TL;DR: 这是一篇关于电力侧信道攻击及其防御措施的综述论文，涵盖了不同应用领域而不仅仅是密码学实现。


<details>
  <summary>Details</summary>
Motivation: 现有的综述主要关注密码学实现中的电力侧信道攻击，但近年来电力侧信道攻击已扩展到机器学习模型逆向工程、用户行为数据利用、指令级反汇编等多个应用领域，需要对这些新兴应用进行全面综述。

Method: 通过文献调研和分类分析的方法，对近年来的电力侧信道攻击研究进行系统性综述，基于应用特定考虑提供全面比较。

Result: 论文提供了电力侧信道攻击在不同应用领域的分类框架和全面比较，包括密码实现密钥提取、机器学习模型逆向工程、用户行为数据利用、指令级反汇编等多个方面。

Conclusion: 电力侧信道攻击已成为跨多个应用领域的重要安全威胁，需要针对不同应用场景开发相应的防御措施，该综述为研究人员提供了系统性的分类和比较框架。

Abstract: Side-channel attacks try to extract secret information from a system by analyzing different side-channel signatures, such as power consumption, electromagnetic emanation, thermal dissipation, acoustics, time, etc. Power-based side-channel attack is one of the most prominent side-channel attacks in cybersecurity, which rely on data-dependent power variations in a system to extract sensitive information. While there are related surveys, they primarily focus on power side-channel attacks on cryptographic implementations. In recent years, power-side channel attacks have been explored in diverse application domains, including key extraction from cryptographic implementations, reverse engineering of machine learning models, user behavior data exploitation, and instruction-level disassembly. In this paper, we provide a comprehensive survey of power side-channel attacks and their countermeasures in different application domains. Specifically, this survey aims to classify recent power side-channel attacks and provide a comprehensive comparison based on application-specific considerations.

</details>


### [31] [Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense](https://arxiv.org/abs/2512.23849)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.CR

TL;DR: EDS是一种检测无关的安全框架，通过经济手段使攻击在经济上不可行，利用防御者控制环境的优势，为资源受限的IoT/边缘环境提供保护。


<details>
  <summary>Details</summary>
Motivation: 传统基于检测的安全方法在面对使用加密、隐身和低速率技术的复杂攻击者时失效，特别是在资源受限的IoT/边缘环境中无法部署基于机器学习的入侵检测系统。

Method: EDS框架包含四种机制：自适应计算谜题、诱饵驱动的交互熵、时间拉伸和带宽征税，通过机制组合实现超线性成本放大。将EDS形式化为Stackelberg博弈，推导出最优参数选择的闭式均衡解。

Result: EDS仅需<12KB内存，可在ESP32类微控制器上部署。在20设备异构IoT测试床上评估显示：攻击减速32-560倍，成本不对称比85-520:1，攻击成功率降低8-62%，延迟开销<20ms，接近0%误报。对抗IoT-23恶意软件实现88%独立缓解，与ML-IDS结合达到94%缓解率。

Conclusion: EDS提供检测无关的保护，适用于传统方法失效的资源受限环境，通过将经济平衡转向防御者，为IoT和边缘系统提供可行的保护方法。

Abstract: Detection-based security fails against sophisticated attackers using encryption, stealth, and low-rate techniques, particularly in IoT/edge environments where resource constraints preclude ML-based intrusion detection. We present Economic Denial Security (EDS), a detection-independent framework that makes attacks economically infeasible by exploiting a fundamental asymmetry: defenders control their environment while attackers cannot. EDS composes four mechanisms adaptive computational puzzles, decoy-driven interaction entropy, temporal stretching, and bandwidth taxation achieving provably superlinear cost amplification. We formalize EDS as a Stackelberg game, deriving closed-form equilibria for optimal parameter selection (Theorem 1) and proving that mechanism composition yields 2.1x greater costs than the sum of individual mechanisms (Theorem 2). EDS requires < 12KB memory, enabling deployment on ESP32 class microcontrollers. Evaluation on a 20-device heterogeneous IoT testbed across four attack scenarios (n = 30 trials, p < 0.001) demonstrates: 32-560x attack slowdown, 85-520:1 cost asymmetry, 8-62% attack success reduction, < 20ms latency overhead, and close to 0% false positives. Validation against IoT-23 malware (Mirai, Torii, Hajime) shows 88% standalone mitigation; combined with ML-IDS, EDS achieves 94% mitigation versus 67% for IDS alone a 27% improvement. EDS provides detection-independent protection suitable for resource-constrained environments where traditional approaches fail. The ability to detect and mitigate the malware samples tested was enhanced; however, the benefits provided by EDS were realized even without the inclusion of an IDS. Overall, the implementation of EDS serves to shift the economic balance in favor of the defender and provides a viable method to protect IoT and edge systems methodologies.

</details>


### [32] [RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress](https://arxiv.org/abs/2512.23995)
*Ruixuan Huang,Qingyue Wang,Hantao Huang,Yudong Gao,Dong Chen,Shuai Wang,Wei Wang*

Main category: cs.CR

TL;DR: MoE架构在推理时存在路由集中漏洞，攻击者可通过重复token模式构造对抗性提示，使所有token路由到同一组专家，造成设备计算瓶颈和服务延迟


<details>
  <summary>Details</summary>
Motivation: MoE架构通过专家并行提高参数效率，但推理时缺乏负载均衡约束，攻击者可以利用路由集中漏洞将效率机制转化为拒绝服务攻击向量

Method: 提出RepetitionCurse攻击策略，利用MoE路由器的普遍缺陷，通过简单的重复token模式以模型无关的方式构造对抗性提示

Result: 在Mixtral-8x7B等广泛部署的MoE模型上，该方法使端到端推理延迟增加3.063倍，显著降低服务可用性

Conclusion: MoE架构存在严重的安全漏洞，需要设计具有负载均衡约束的路由机制来防御此类对抗性攻击

Abstract: Mixture-of-Experts architectures have become the standard for scaling large language models due to their superior parameter efficiency. To accommodate the growing number of experts in practice, modern inference systems commonly adopt expert parallelism to distribute experts across devices. However, the absence of explicit load balancing constraints during inference allows adversarial inputs to trigger severe routing concentration. We demonstrate that out-of-distribution prompts can manipulate the routing strategy such that all tokens are consistently routed to the same set of top-$k$ experts, which creates computational bottlenecks on certain devices while forcing others to idle. This converts an efficiency mechanism into a denial-of-service attack vector, leading to violations of service-level agreements for time to first token. We propose RepetitionCurse, a low-cost black-box strategy to exploit this vulnerability. By identifying a universal flaw in MoE router behavior, RepetitionCurse constructs adversarial prompts using simple repetitive token patterns in a model-agnostic manner. On widely deployed MoE models like Mixtral-8x7B, our method increases end-to-end inference latency by 3.063x, degrading service availability significantly.

</details>


### [33] [Document Data Matching for Blockchain-Supported Real Estate](https://arxiv.org/abs/2512.24457)
*Henrique Lin,Tiago Dias,Miguel Correia*

Main category: cs.CR

TL;DR: 本文提出一个结合OCR、NLP和可验证凭证的房地产文档自动化处理系统，通过区块链提供去中心化信任层，显著提高文档处理效率和防欺诈能力。


<details>
  <summary>Details</summary>
Motivation: 房地产行业高度依赖人工文档处理和验证，导致流程效率低下且容易发生欺诈。需要自动化解决方案来提高效率、增强透明度和防止欺诈。

Method: 开发了一个集成系统：1) 基于合成数据集训练的OCR-NLP提取管道；2) 凭证发行和管理的后端系统；3) 支持发行者、持有者和验证者交互的前端界面。系统将异构文档格式标准化为可验证凭证，应用自动数据匹配检测不一致性，并利用区块链提供去中心化信任层。

Result: 实验结果显示，模型在多种文档类型上达到有竞争力的准确率，端到端管道显著减少了验证时间，同时保持了可靠性。系统能够有效检测数据不一致性。

Conclusion: 该框架展示了简化房地产交易、增强利益相关者信任、实现可扩展安全数字流程的潜力，为房地产文档管理提供了创新的自动化解决方案。

Abstract: The real estate sector remains highly dependent on manual document handling and verification, making processes inefficient and prone to fraud. This work presents a system that integrates optical character recognition (OCR), natural language processing (NLP), and verifiable credentials (VCs) to automate document extraction, verification, and management. The approach standardizes heterogeneous document formats into VCs and applies automated data matching to detect inconsistencies, while the blockchain provides a decentralized trust layer that reinforces transparency and integrity. A prototype was developed that comprises (i) an OCR-NLP extraction pipeline trained on synthetic datasets, (ii) a backend for credential issuance and management, and (iii) a frontend supporting issuer, holder, and verifier interactions. Experimental results show that the models achieve competitive accuracy across multiple document types and that the end-to-end pipeline reduces verification time while preserving reliability. The proposed framework demonstrates the potential to streamline real estate transactions, strengthen stakeholder trust, and enable scalable, secure digital processes.

</details>


### [34] [Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?](https://arxiv.org/abs/2512.24044)
*Yuan Xin,Dingfan Chen,Linyi Yang,Michael Backes,Xiao Zhang*

Main category: cs.CR

TL;DR: 该论文首次系统评估了针对LLM安全对齐的越狱攻击在整个推理流程中的实际成功率，发现现有安全过滤器能有效检测大多数越狱攻击，但需要在召回率和精确度之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击评估仅关注模型本身，忽略了实际部署中通常包含的内容审核过滤器等安全机制，导致可能高估了越狱攻击的实际成功率。

Method: 首次对越狱攻击进行系统评估，不仅针对LLM安全对齐，还评估整个推理流程，包括输入和输出过滤阶段的安全机制。

Result: 1. 几乎所有评估的越狱技术都能被至少一个安全过滤器检测到，表明先前评估可能高估了攻击的实际成功率；2. 安全过滤器在检测方面有效，但在召回率和精确度平衡方面仍有改进空间。

Conclusion: 需要进一步优化LLM安全系统的检测准确性和可用性，在保护效果和用户体验之间取得更好平衡。

Abstract: As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.

</details>


### [35] [Spatial Discretization for Fine-Grain Zone Checks with STARKs](https://arxiv.org/abs/2512.24238)
*Sungmin Lee,Kichang Lee,Gyeongmin Han,JeongGil Ko*

Main category: cs.CR

TL;DR: 该论文研究了在零知识证明中高效执行点面测试（PiP）的方法，通过比较不同区域编码方式对精度和证明成本的影响，提出了基于距离感知的网格编码方法，在粗粒度网格上显著提高了精度。


<details>
  <summary>Details</summary>
Motivation: 位置服务中的点面测试在零知识证明环境下成本高昂，需要找到既能保护隐私又能高效执行几何运算的方法。研究不同区域编码方式对精度和证明成本的影响是关键问题。

Method: 在固定的STARK执行模型下，利用基于网格的查找表，比较了布尔网格基线（标记单元格内外状态）和距离感知编码方法（存储每个单元格到区域边界的距离，并使用插值进行推理）。

Result: 在真实世界数据上的实验表明，距离感知方法在粗粒度网格上实现了更高的精度（最大提升60%），验证开销仅增加约1.4倍，区域编码成为高效零知识空间检查的关键杠杆。

Conclusion: 距离感知的网格编码方法在零知识证明中实现了精度和效率的良好平衡，为隐私保护的空间查询提供了有效的解决方案，区域编码策略是优化零知识空间检查性能的关键因素。

Abstract: Many location-based services rely on a point-in-polygon test (PiP), checking whether a point or a trajectory lies inside a geographic zone. Since geometric operations are expensive in zero-knowledge proofs, privately performing the PiP test is challenging. In this paper, we answer the research questions of how different ways of encoding zones affect accuracy and proof cost by exploiting gridbased lookup tables under a fixed STARK execution model. Beyond a Boolean grid-based baseline that marks cells as in- or outside, we explore a distance-aware encoding approach that stores how far each cell is from a zone boundary and uses interpolation to reason within a cell. Our experiments on real-world data demonstrate that the proposed distance-aware approach achieves higher accuracy on coarse grids (max. 60%p accuracy gain) with only a moderate verification overhead (approximately 1.4x), making zone encoding the key lever for efficient zero-knowledge spatial checks.

</details>


### [36] [How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?](https://arxiv.org/abs/2512.24255)
*Jiping Yu,Xiaowei Zhu,Kun Chen,Guanyu Feng,Yunyi Chen,Xiaoyu Fan,Wenguang Chen*

Main category: cs.CR

TL;DR: 论文探索了在可信处理器中集成不可观测内存（OM）来提升图分析性能，通过存储结构与算法协同设计，实现了比基线快100倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 可信处理器能够保护数据隐私，但传统数据不可知算法会导致性能下降。为了解决这个问题，研究者探索在处理器中集成不可观测内存（OM），使内存访问对攻击者不可见，特别关注易受访问模式攻击的图分析应用。

Method: 采用存储结构与算法的协同设计方法，在处理器中集成不可观测内存（OM），其大小与每核缓存相当。通过这种设计，可以在现有处理器上以可忽略的开销实现OM功能。

Result: 原型系统在OM大小与每核缓存相当的情况下，比基线方法快100倍，证明了在可信处理器中集成OM的有效性和可行性。

Conclusion: 在可信处理器中集成不可观测内存（OM）是可行的，通过适当的协同设计可以显著提升图分析等应用的性能，为可信处理器设计提供了重要见解。

Abstract: Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM.

</details>


### [37] [FAST-IDS: A Fast Two-Stage Intrusion Detection System with Hybrid Compression for Real-Time Threat Detection in Connected and Autonomous Vehicles](https://arxiv.org/abs/2512.24391)
*Devika S,Vishnu Hari,Pratik Narang,Tejasvi Alladi,Vinay Chamola*

Main category: cs.CR

TL;DR: 为资源受限环境实现了一个经过混合模型压缩的多阶段车载入侵检测系统


<details>
  <summary>Details</summary>
Motivation: 车载系统面临安全威胁，但资源受限环境难以部署复杂的入侵检测系统，需要轻量化解决方案

Method: 采用多阶段入侵检测架构，结合混合模型压缩技术（可能包括剪枝、量化、知识蒸馏等）来减少模型大小和计算需求

Result: 成功实现了适用于资源受限环境的多阶段IDS，通过压缩技术降低了部署要求

Conclusion: 混合模型压缩技术使复杂的安全系统能够在资源受限的车载环境中有效部署

Abstract: We have implemented a multi-stage IDS for CAVs that can be deployed to resourec-constrained environments after hybrid model compression.

</details>


### [38] [SourceBroken: A large-scale analysis on the (un)reliability of SourceRank in the PyPI ecosystem](https://arxiv.org/abs/2512.24400)
*Biagio Montaruli,Serena Elisa Ponta,Luca Compagna,Davide Balzarotti*

Main category: cs.CR

TL;DR: SourceRank评分系统存在被恶意软件包规避攻击的风险，特别是URL混淆技术，导致其无法可靠区分PyPI上的良性和恶意软件包。


<details>
  <summary>Details</summary>
Motivation: 尽管SourceRank已被多项研究使用，但尚未有研究深入分析其在面对旨在提高恶意软件包评分的规避攻击时的可靠性。需要填补这一空白，评估SourceRank在真实场景中的有效性。

Method: 1) 提出威胁模型，识别每个指标的潜在规避方法，包括URL混淆技术；2) 在PyPI生态系统中分析SourceRank的可靠性，使用MalwareBench数据集和包含122,398个软件包的真实世界数据集；3) 分析URL混淆技术的流行程度和影响。

Result: 1) 历史数据显示良性和恶意软件包有明显区分，但真实世界分布显著重叠，主要因为SourceRank未能及时反映软件包移除；2) SourceRank无法可靠区分真实场景中的良性和恶意软件包；3) URL混淆技术流行度从MalwareBench的4.2%增加到真实数据集的7.0%，常与其他规避技术结合使用，能显著提高恶意软件包的SourceRank指标。

Conclusion: SourceRank评分系统存在严重漏洞，无法作为PyPI生态系统中区分良性和恶意软件包的可靠工具，URL混淆技术是一个新兴的攻击向量，需要更健壮的检测机制。

Abstract: SourceRank is a scoring system made of 18 metrics that assess the popularity and quality of open-source packages. Despite being used in several recent studies, none has thoroughly analyzed its reliability against evasion attacks aimed at inflating the score of malicious packages, thereby masquerading them as trustworthy. To fill this gap, we first propose a threat model that identifies potential evasion approaches for each metric, including the URL confusion technique, which can affect 5 out of the 18 metrics by leveraging a URL pointing to a legitimate repository potentially unrelated to the malicious package.
  Furthermore, we study the reliability of SourceRank in the PyPI ecosystem by analyzing the SourceRank distributions of benign and malicious packages in the state-of-the-art MalwareBench dataset, as well as in a real-world dataset of 122,398 packages. Our analysis reveals that, while historical data suggests a clear distinction between benign and malicious packages, the real-world distributions overlap significantly, mainly due to SourceRank's failure to timely reflect package removals. As a result, SourceRank cannot be reliably used to discriminate between benign and malicious packages in real-world scenarios, nor to select benign packages among those available on PyPI.
  Finally, our analysis reveals that URL confusion represents an emerging attack vector, with its prevalence increasing from 4.2% in MalwareBench to 7.0% in our real-world dataset. Moreover, this technique is often used alongside other evasion techniques and can significantly inflate the SourceRank metrics of malicious packages.

</details>


### [39] [GateChain: A Blockchain Based Application for Country Entry Exit Registry Management](https://arxiv.org/abs/2512.24416)
*Mohamad Akkad,Hüseyin Bodur*

Main category: cs.CR

TL;DR: GateChain是一个基于区块链的出入境记录系统，旨在解决传统集中式边境控制系统的数据完整性和互操作性缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着国际流动性增加和安全需求提升，需要具有保密性、完整性和可审计性的出入境记录系统。传统集中式数据库系统易受数据篡改，且机构间互操作性有限。

Method: 开发GateChain应用，利用分布式、不可变、可加密验证的区块链账本记录出入境事件，提供实时访问控制和验证机制。

Result: GateChain增强了数据完整性、可靠性和透明度，为授权机构提供实时访问控制和验证功能。

Conclusion: 区块链技术能够有效解决传统边境控制系统的安全漏洞，GateChain展示了在出入境管理中的实际应用潜力。

Abstract: Recording entry and exit records for a country, with properties such as confidentiality, integrity, and auditability, is increasingly important due to rising international mobility and security requirements. Traditional border control systems, which rely on centralised databases, are vulnerable to data manipulation and have limited interoperability between institutions. This study presents GateChain, a blockchain-based application that addresses these vulnerabilities. GateChain aims to enhance data integrity, reliability, and transparency by recording entry and exit events on a distributed, immutable, and cryptographically verifiable ledger. The application provides real-time access control and verification for authorised institutions. This paper describes the architecture and security components of GateChain and evaluates its performance and security features.

</details>


### [40] [Correctness of Extended RSA Public Key Cryptosystem](https://arxiv.org/abs/2512.24531)
*Dar-jen Chang,Suranjan Gautam*

Main category: cs.CR

TL;DR: 本文提出了一种验证RSA公钥密码系统正确性的替代方法，探讨了正整数N的选择条件可以超出标准标准的可能性，并推导了N值有效的明确条件。


<details>
  <summary>Details</summary>
Motivation: 现有文献中RSA正确性的传统证明方法存在局限性，本文旨在提供一种略有不同的方法论，探索RSA基本参数N的选择条件是否可以扩展，从而更全面地理解RSA加密方案的有效性条件。

Method: 采用数学推导的方法，分析RSA-like方案中正整数N的选择条件，推导出确定N值有效性的明确条件，并解释某些N值为何无法满足正确性要求。该方法偏离了传统证明路径。

Result: 成功推导出了决定RSA加密方案中N值有效性的具体条件，明确了哪些N值选择可以保证加密方案的正确性，哪些会导致失败。为RSA-like方案提供了更广泛的参数选择理论依据。

Conclusion: RSA加密方案的正确性可以通过扩展N的选择条件来验证，本文提出的方法为理解RSA-like方案的数学基础提供了新的视角，但仅限于正确性证明，不涉及密码安全性问题。

Abstract: This paper proposes an alternative approach to formally establishing the correctness of the RSA public key cryptosystem. The methodology presented herein deviates slightly from conventional proofs found in existing literature. Specifically, this study explores the conditions under which the choice of the positive integer N, a fundamental component of RSA, can be extended beyond the standard selection criteria. We derive explicit conditions that determine when certain values of N are valid for the encryption scheme and explain why others may fail to satisfy the correctness requirements. The scope of this paper is limited to the mathematical proof of correctness for RSA-like schemes, deliberately omitting issues related to the cryptographic security of RSA.

</details>


### [41] [SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System](https://arxiv.org/abs/2512.24571)
*Md Hasan Saju,Austin Page,Akramul Azim,Jeff Gardiner,Farzaneh Abazari,Frank Eargle*

Main category: cs.CR

TL;DR: SynRAG是一个统一框架，能够从平台无关的规范自动生成针对多个SIEM平台的威胁检测和事件调查查询，解决了不同SIEM系统查询语言差异带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 不同SIEM平台（如Qradar、Google SecOps、Splunk、Microsoft Sentinel、Elastic Stack）在属性、架构和查询语言上存在显著差异，导致安全分析师需要为每个平台单独编写查询，增加了培训成本和人力需求。

Method: SynRAG框架从分析师编写的高级平台无关规范自动生成特定平台的查询，无需分析师手动为每个SIEM系统编写单独的查询，实现了跨异构SIEM环境的无缝威胁检测和事件调查。

Result: 与GPT、Llama、DeepSeek、Gemma和Claude等最先进语言模型相比，SynRAG在Qradar和SecOps作为代表性SIEM系统的评估中，生成了显著更好的跨SIEM威胁检测和事件调查查询。

Conclusion: SynRAG通过统一框架解决了SIEM平台多样性带来的挑战，减少了专业培训需求和手动查询翻译工作，提高了安全分析师在异构SIEM环境中的工作效率。

Abstract: Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.

</details>


### [42] [Secure Digital Semantic Communications: Fundamentals, Challenges, and Opportunities](https://arxiv.org/abs/2512.24602)
*Weixuan Chen,Qianqian Yang,Yuanyuan Jia,Junyu Pan,Shuo Shao,Jincheng Dai,Meixia Tao,Ping Zhang*

Main category: cs.CR

TL;DR: 本文系统分析了数字语义通信的安全威胁与防御策略，填补了现有研究主要关注模拟语义通信的空白，为构建安全可部署的数字语义通信系统提供了理论框架。


<details>
  <summary>Details</summary>
Motivation: 语义通信从比特精确传输转向任务导向传输，虽然提高了效率，但引入了新的安全和隐私风险。现有安全研究主要关注模拟语义通信，而数字语义通信因其离散比特/符号传输机制和实际收发器管道，暴露出独特且未被充分探索的攻击面，需要系统性分析。

Method: 本文通过回顾语义通信基础，澄清模拟与数字语义通信的架构差异及其安全影响，组织数字语义通信的威胁格局，并讨论潜在防御措施。具体分析包括概率调制和确定性调制两种主要数字调制路径，以及比特/符号级语义信息、调制阶段、基于分组的传输和协议操作等层面的漏洞。

Result: 系统识别了数字语义通信特有的安全威胁，包括语义泄露、语义操纵、知识库漏洞、模型相关攻击以及对真实性和可用性的威胁。这些威胁源于离散机制和实际传输过程，与模拟语义通信的攻击面有显著差异。

Conclusion: 数字语义通信在实际系统中具有更强的兼容性，但其离散传输机制引入了独特的安全挑战。本文为构建安全可部署的数字语义通信系统提供了系统性分析框架，并指出了未来的研究方向。

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for future wireless networks by prioritizing task-relevant meaning over raw data delivery, thereby reducing communication overhead and improving efficiency. However, shifting from bit-accurate transmission to task-oriented delivery introduces new security and privacy risks. These include semantic leakage, semantic manipulation, knowledge base vulnerabilities, model-related attacks, and threats to authenticity and availability. Most existing secure SemCom studies focus on analog SemCom, where semantic features are mapped to continuous channel inputs. In contrast, digital SemCom transmits semantic information through discrete bits or symbols within practical transceiver pipelines, offering stronger compatibility with realworld systems while exposing a distinct and underexplored attack surface. In particular, digital SemCom typically represents semantic information over a finite alphabet through explicit digital modulation, following two main routes: probabilistic modulation and deterministic modulation. These discrete mechanisms and practical transmission procedures introduce additional vulnerabilities affecting bit- or symbol-level semantic information, the modulation stage, and packet-based delivery and protocol operations. Motivated by these challenges and the lack of a systematic analysis of secure digital SemCom, this paper reviews SemCom fundamentals, clarifies the architectural differences between analog and digital SemCom and their security implications, organizes the threat landscape for digital SemCom, and discusses potential defenses. Finally, we outline open research directions toward secure and deployable digital SemCom systems.

</details>


### [43] [Practical Traceable Over-Threshold Multi-Party Private Set Intersection](https://arxiv.org/abs/2512.24652)
*Le Yang,Weijing You,Huiyang He,Kailiang Ji,Jingqiang Lin*

Main category: cs.CR

TL;DR: 本文提出两种可追踪的超阈值多方私有集合交集协议，显著提升了效率和安全性，相比现有方案实现了数千倍的加速。


<details>
  <summary>Details</summary>
Motivation: 在数字取证等场景中，需要支持阈值条件的多方私有集合交集，并且要求能够追踪交集元素的来源持有者，以确保可追溯性和可靠性。现有研究对此类协议支持有限，且效率低下、安全性不足。

Method: 提出了两种协议：1) ET-OT-MP-PSI：结合Shamir秘密共享和可编程伪随机函数，抵抗最多t-2个半诚实参与者；2) ST-OT-MP-PSI：进一步利用不经意线性求值协议，可抵抗最多n-1个半诚实参与者。

Result: 实验结果显示显著改进：当n=5、t=3、集合大小为2^14时，ET-OT-MP-PSI相比Mahdavi等人的协议实现了15056倍加速，ST-OT-MP-PSI实现了505倍加速。

Conclusion: 本文提出的两种可追踪超阈值多方私有集合交集协议在效率和安全性方面都有显著提升，消除了特殊方不共谋的假设，为实际应用提供了更实用的解决方案。

Abstract: Multi-Party Private Set Intersection (MP-PSI) with threshold enhances the flexibility of MP-PSI by disclosing elements present in at least $t$ participants' sets, rather than requiring elements to appear in all $n$ sets. In scenarios where each participant is responsible for its dataset, e.g., digital forensics, MP-PSI with threshold should disclose both intersection elements and corresponding holders such that elements are traceable and the reliability of intersection is guaranteed. We refer to MP-PSI with threshold supporting traceability as Traceable Over-Threshold MP-PSI (T-OT-MP-PSI). However, research on such protocols remains limited, and existing work tolerates at most $t-2$ semi-honest participants at considerable computational cost. We propose two novel Traceable OT-MP-PSI protocols. The first, Efficient Traceable OT-MP-PSI (ET-OT-MP-PSI), combines Shamir's secret sharing with an oblivious programmable pseudorandom function, achieving significantly improved efficiency with resistance to at most $t-2$ semi-honest participants. The second, Security-enhanced Traceable OT-MP-PSI (ST-OT-MP-PSI), achieves security against up to $n-1$ semi-honest participants by further leveraging the oblivious linear evaluation protocol. Compared to Mahdavi et al.'s protocol, ours eliminate the assumption that certain special parties do not collude. Experimental results demonstrate significant improvements: for $n=5$, $t=3$, and sets of size $2^{14}$, ET-OT-MP-PSI achieves $15056\times$ speedup and ST-OT-MP-PSI achieves $505\times$ speedup over Mahdavi et al.'s protocol.

</details>


### [44] [MTSP-LDP: A Framework for Multi-Task Streaming Data Publication under Local Differential Privacy](https://arxiv.org/abs/2512.24899)
*Chang Liu,Junzhou Zhao*

Main category: cs.CR

TL;DR: MTSP-LDP是一个用于多任务流数据发布的w-event本地差分隐私框架，通过动态隐私预算分配、自适应私有二叉树结构和无预算多任务处理机制，解决了现有方法在处理复杂查询和时序相关性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有w-event本地差分隐私机制存在两个关键限制：1）主要设计用于发布简单统计量，不适合复杂查询；2）独立处理每个时间戳的数据，未能捕捉时序相关性，导致整体效用降低。

Method: MTSP-LDP采用三种核心技术：1）最优隐私预算分配算法，通过分析窗口内时序相关性动态分配隐私预算；2）数据自适应私有二叉树结构，支持复杂查询，并通过跨时间戳分组和平滑操作提升估计精度；3）无预算多任务处理机制，支持多种流查询而不消耗额外隐私预算。

Result: 在真实世界数据集上的大量实验表明，MTSP-LDP在各种流任务中始终实现高效用，显著优于现有方法。

Conclusion: MTSP-LDP成功解决了现有w-event LDP机制在处理复杂查询和时序相关性方面的局限性，为流数据隐私保护提供了更实用和高效的解决方案。

Abstract: The proliferation of streaming data analytics in data-driven applications raises critical privacy concerns, as directly collecting user data may compromise personal privacy. Although existing $w$-event local differential privacy (LDP) mechanisms provide formal guarantees without relying on trusted third parties, their practical deployment is hindered by two key limitations. First, these methods are designed primarily for publishing simple statistics at each timestamp, making them inherently unsuitable for complex queries. Second, they handle data at each timestamp independently, failing to capture temporal correlations and consequently degrading the overall utility. To address these issues, we propose MTSP-LDP, a novel framework for \textbf{M}ulti-\textbf{T}ask \textbf{S}treaming data \textbf{P}ublication under $w$-event LDP. MTSP-LDP adopts an \emph{Optimal Privacy Budget Allocation} algorithm to dynamically allocate privacy budgets by analyzing temporal correlations within each window. It then constructs a \emph{data-adaptive private binary tree structure} to support complex queries, which is further refined by cross-timestamp grouping and smoothing operations to enhance estimation accuracy. Furthermore, a unified \emph{Budget-Free Multi-Task Processing} mechanism is introduced to support a variety of streaming queries without consuming additional privacy budget. Extensive experiments on real-world datasets demonstrate that MTSP-LDP consistently achieves high utility across various streaming tasks, significantly outperforming existing methods.

</details>


### [45] [Towards Provably Secure Generative AI: Reliable Consensus Sampling](https://arxiv.org/abs/2512.24925)
*Yu Cui,Hang Fu,Sicheng Pan,Zhuoyu Sun,Yifei Liu,Yuhong Nie,Bo Ran,Baohan Huang,Xufeng Zhang,Haibin Zhang,Cong Zuo,Licheng Wang*

Main category: cs.CR

TL;DR: 提出可靠共识采样（RCS）算法，解决现有共识采样（CS）在生成式AI安全中的缺陷，通过追踪接受概率提高对抗鲁棒性，完全消除弃权需求，同时保持可控风险阈值。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI安全研究主要基于经验性的攻防方法，容易产生未知攻击绕过现有检测机制。共识采样（CS）作为可证明安全算法存在缺陷：需要频繁弃权降低实用性，且在恶意操纵下易受攻击。

Method: 提出可靠共识采样（RCS）新原语，通过追踪接受概率来容忍极端对抗行为，提高鲁棒性；完全消除弃权需求；开发反馈算法持续动态增强RCS安全性；提供理论保证RCS维持可控风险阈值。

Result: 大量实验表明，RCS在保持与CS相当的延迟的同时，显著提高了鲁棒性和实用性，同时维持可控的风险阈值。

Conclusion: RCS解决了CS算法的关键缺陷，为开发可证明安全的生成式AI做出了贡献，通过理论保证和实验验证展示了其在安全性和实用性方面的优势。

Abstract: Existing research on generative AI security is primarily driven by mutually reinforcing attack and defense methodologies grounded in empirical experience. This dynamic frequently gives rise to previously unknown attacks that can circumvent current detection and prevention. This necessitates the continual updating of security mechanisms. Constructing generative AI with provable security and theoretically controllable risk is therefore necessary. Consensus Sampling (CS) is a promising algorithm toward provably secure AI. It controls risk by leveraging overlap in model output probabilities. However, we find that CS relies on frequent abstention to avoid unsafe outputs, which reduces utility. Moreover, CS becomes highly vulnerable when unsafe models are maliciously manipulated. To address these issues, we propose a new primitive called Reliable Consensus Sampling (RCS), that traces acceptance probability to tolerate extreme adversarial behaviors, improving robustness. RCS also eliminates the need for abstention entirely. We further develop a feedback algorithm to continuously and dynamically enhance the safety of RCS. We provide theoretical guarantees that RCS maintains a controllable risk threshold. Extensive experiments show that RCS significantly improves robustness and utility while maintaining latency comparable to CS. We hope this work contributes to the development of provably secure generative AI.

</details>
