{"id": "2512.16855", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.16855", "abs": "https://arxiv.org/abs/2512.16855", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge", "comment": "Published in the IEEE ICCAD 2025 conference", "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.", "AI": {"tldr": "TOGGLE\u662f\u4e00\u4e2a\u65b0\u9896\u7684LLM\u538b\u7f29\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5f62\u5f0f\u5316\u65b9\u6cd5\uff08\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff09\u5f15\u5165\u538b\u7f29\u8fc7\u7a0b\uff0c\u5728\u4fdd\u8bc1\u8bed\u8a00\u5c5e\u6027\u6ee1\u8db3\u5f62\u5f0f\u5316\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u6700\u9ad83.3\u500d\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u548c68.8%\u6a21\u578b\u5927\u5c0f\u538b\u7f29\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u73b0\u6709\u7684\u538b\u7f29\u6280\u672f\uff08\u5982\u91cf\u5316\u548c\u526a\u679d\uff09\u901a\u5e38\u4f1a\u635f\u5bb3\u5173\u952e\u7684\u8bed\u8a00\u5c5e\u6027\uff0c\u4e14\u7f3a\u4e4f\u4fdd\u8bc1\u6a21\u578b\u884c\u4e3a\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faTOGGLE\u6846\u67b6\uff0c\u5229\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u5f62\u5f0f\u5316\u6307\u5b9a\u548c\u5f3a\u5236\u6267\u884c\u8bed\u8a00\u5c5e\u6027\u3002\u91c7\u7528STL\u9c81\u68d2\u6027\u5f15\u5bfc\u7684\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u7cfb\u7edf\u63a2\u7d22\u5c42\u7ea7\u7684\u91cf\u5316\u548c\u526a\u679d\u914d\u7f6e\uff0c\u751f\u6210\u6ee1\u8db3\u5f62\u5f0f\u5316\u8bed\u8a00\u7ea6\u675f\u7684\u538b\u7f29\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "result": "\u5728\u56db\u79cdLLM\u67b6\u6784\uff08GPT-2\u3001DeepSeek-V2 7B\u3001LLaMA 3 8B\u3001Mistral 7B\uff09\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad83.3\u500d\u8ba1\u7b97\u6210\u672c\uff08FLOPs\uff09\u964d\u4f4e\u548c\u6700\u9ad868.8%\u6a21\u578b\u5927\u5c0f\u538b\u7f29\uff0c\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u8bed\u8a00\u5c5e\u6027\u7ea6\u675f\u3002", "conclusion": "TOGGLE\u9996\u6b21\u5c06\u5f62\u5f0f\u5316\u65b9\u6cd5\u96c6\u6210\u5230LLM\u538b\u7f29\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u7684LLM\u8fb9\u7f18\u90e8\u7f72\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.16856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16856", "abs": "https://arxiv.org/abs/2512.16856", "authors": ["Nenad Toma\u0161ev", "Matija Franklin", "Julian Jacobs", "S\u00e9bastien Krier", "Simon Osindero"], "title": "Distributional AGI Safety", "comment": null, "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u62fc\u51d1\u5f0fAGI\"\u5047\u8bf4\uff0c\u8ba4\u4e3a\u901a\u7528\u667a\u80fd\u53ef\u80fd\u9996\u5148\u901a\u8fc7\u591a\u4e2a\u5b50AGI\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5b9e\u73b0\uff0c\u800c\u975e\u5355\u4e00AGI\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u5206\u5e03\u5f0f\u7684AGI\u5b89\u5168\u6846\u67b6\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u548c\u5bf9\u9f50\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e00AI\u7cfb\u7edf\u7684\u4fdd\u62a4\uff0c\u5047\u8bbe\u6700\u7ec8\u4f1a\u51fa\u73b0\u5355\u4e00\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u3002\u4f46\u53e6\u4e00\u79cd\u53ef\u80fd\u6027\u662f\u901a\u7528\u80fd\u529b\u9996\u5148\u901a\u8fc7\u591a\u4e2a\u5b50AGI\u667a\u80fd\u4f53\u7684\u534f\u8c03\u534f\u4f5c\u5b9e\u73b0\uff0c\u8fd9\u79cd\"\u62fc\u51d1\u5f0fAGI\"\u5047\u8bf4\u9700\u8981\u88ab\u8ba4\u771f\u8003\u8651\u5e76\u5236\u5b9a\u76f8\u5e94\u7684\u5b89\u5168\u4fdd\u969c\u63aa\u65bd\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0fAGI\u5b89\u5168\u6846\u67b6\uff0c\u8d85\u8d8a\u5bf9\u5355\u4e2a\u667a\u80fd\u4f53\u7684\u8bc4\u4f30\u548c\u5bf9\u9f50\u3002\u8be5\u6846\u67b6\u6838\u5fc3\u662f\u8bbe\u8ba1\u548c\u5b9e\u65bd\u865a\u62df\u667a\u80fd\u4f53\u6c99\u76d2\u7ecf\u6d4e\uff08\u4e0d\u53ef\u6e17\u900f\u6216\u534a\u6e17\u900f\uff09\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u95f4\u7684\u4ea4\u6613\u7531\u5f3a\u5927\u7684\u5e02\u573a\u673a\u5236\u7ba1\u7406\uff0c\u5e76\u914d\u5408\u9002\u5f53\u7684\u53ef\u5ba1\u8ba1\u6027\u3001\u58f0\u8a89\u7ba1\u7406\u548c\u76d1\u7763\uff0c\u4ee5\u51cf\u8f7b\u96c6\u4f53\u98ce\u9669\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u5e94\u5bf9\u62fc\u51d1\u5f0fAGI\u53ef\u80fd\u5e26\u6765\u7684\u5b89\u5168\u6311\u6218\uff0c\u5f3a\u8c03\u9700\u8981\u901a\u8fc7\u7ecf\u6d4e\u673a\u5236\u3001\u5ba1\u8ba1\u548c\u76d1\u7ba1\u6765\u7ba1\u7406\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u4ea7\u751f\u7684\u96c6\u4f53\u98ce\u9669\u3002", "conclusion": "\u968f\u7740\u5177\u6709\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u534f\u8c03\u80fd\u529b\u7684\u5148\u8fdbAI\u667a\u80fd\u4f53\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u8003\u8651\u62fc\u51d1\u5f0fAGI\u5047\u8bf4\u5e76\u5efa\u7acb\u76f8\u5e94\u7684\u5206\u5e03\u5f0f\u5b89\u5168\u6846\u67b6\u5df2\u6210\u4e3a\u7d27\u8feb\u7684\u5b89\u5168\u9700\u6c42\uff0c\u8fd9\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u7684\u5355\u4e00\u667a\u80fd\u4f53\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2512.16873", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16873", "abs": "https://arxiv.org/abs/2512.16873", "authors": ["Otman A. Basir"], "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI", "comment": null, "summary": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\"\u793e\u4f1a\u8d23\u4efb\u5806\u6808\"\u6846\u67b6\uff0c\u5c06\u793e\u4f1a\u4ef7\u503c\u89c2\u5d4c\u5165AI\u7cfb\u7edf\u4f5c\u4e3a\u663e\u5f0f\u7ea6\u675f\uff0c\u901a\u8fc7\u516d\u5c42\u67b6\u6784\u5b9e\u73b0AI\u7cfb\u7edf\u7684\u8d23\u4efb\u6cbb\u7406\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u5f71\u54cd\u4eba\u7c7b\u884c\u4e3a\u3001\u673a\u6784\u51b3\u7b56\u548c\u793e\u4f1a\u7ed3\u679c\u65b9\u9762\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u8d1f\u8d23\u4efbAI\u548c\u6cbb\u7406\u52aa\u529b\u7f3a\u4e4f\u53ef\u6267\u884c\u7684\u5de5\u7a0b\u673a\u5236\uff0c\u65e0\u6cd5\u5728\u6574\u4e2a\u7cfb\u7edf\u751f\u547d\u5468\u671f\u4e2d\u6709\u6548\u8fd0\u4f5c\u3002", "method": "\u5f15\u5165\u793e\u4f1a\u8d23\u4efb\u5806\u6808\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u516d\u5c42\u67b6\u6784\u6846\u67b6\uff0c\u5c06\u793e\u4f1a\u4ef7\u503c\u89c2\u4f5c\u4e3a\u663e\u5f0f\u7ea6\u675f\u3001\u4fdd\u969c\u63aa\u65bd\u3001\u884c\u4e3a\u63a5\u53e3\u3001\u5ba1\u8ba1\u673a\u5236\u548c\u6cbb\u7406\u6d41\u7a0b\u5d4c\u5165AI\u7cfb\u7edf\u3002\u5c06\u8d23\u4efb\u5efa\u6a21\u4e3a\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u7684\u95ed\u73af\u76d1\u7763\u63a7\u5236\u95ee\u9898\uff0c\u6574\u5408\u8bbe\u8ba1\u65f6\u4fdd\u969c\u4e0e\u8fd0\u884c\u65f6\u76d1\u63a7\u548c\u673a\u6784\u76d1\u7763\u3002", "result": "\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u57fa\u4e8e\u7ea6\u675f\u7684\u8868\u8ff0\uff0c\u5f15\u5165\u4e86\u5b89\u5168\u5305\u7edc\u548c\u53cd\u9988\u89e3\u91ca\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u6301\u7eed\u76d1\u63a7\u548c\u6267\u884c\u516c\u5e73\u6027\u3001\u81ea\u4e3b\u6027\u3001\u8ba4\u77e5\u8d1f\u62c5\u548c\u89e3\u91ca\u8d28\u91cf\u3002\u901a\u8fc7\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u534f\u4f5c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u516c\u5171\u90e8\u95e8\u7cfb\u7edf\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86SRS\u5982\u4f55\u5c06\u89c4\u8303\u6027\u76ee\u6807\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5de5\u7a0b\u548c\u8fd0\u8425\u63a7\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u8fde\u63a5\u4e86\u4f26\u7406\u5b66\u3001\u63a7\u5236\u7406\u8bba\u548cAI\u6cbb\u7406\uff0c\u4e3a\u53ef\u95ee\u8d23\u3001\u81ea\u9002\u5e94\u548c\u53ef\u5ba1\u8ba1\u7684\u793e\u4f1a\u6280\u672fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2512.16917", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16917", "abs": "https://arxiv.org/abs/2512.16917", "authors": ["Qihao Liu", "Luoxin Ye", "Wufei Ma", "Yu-Cheng Chou", "Alan Yuille"], "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.", "AI": {"tldr": "\u63d0\u51faGenerative Adversarial Reasoner\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u63a8\u7406\u5668\u548c\u5224\u522b\u5668\uff0c\u63d0\u5347LLM\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u4ecd\u5b58\u5728\u8ba1\u7b97\u9519\u8bef\u3001\u903b\u8f91\u8106\u5f31\u548c\u8868\u9762\u5408\u7406\u4f46\u65e0\u6548\u6b65\u9aa4\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf", "method": "\u91c7\u7528\u751f\u6210\u5bf9\u6297\u63a8\u7406\u5668\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u94fe\u5212\u5206\u4e3a\u903b\u8f91\u5b8c\u6574\u7684\u5207\u7247\uff0c\u901a\u8fc7\u5224\u522b\u5668\u8bc4\u4f30\u6bcf\u4e2a\u5207\u7247\u7684\u5408\u7406\u6027\uff0c\u7ed3\u5408\u63a8\u7406\u5668\u751f\u6210\u6b63\u786e\u6b65\u9aa4\u548c\u5224\u522b\u5668\u68c0\u6d4b\u9519\u8bef\u7684\u4e92\u8865\u4fe1\u53f7\u8fdb\u884c\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728AIME24\u4e0a\uff0cDeepSeek-R1-Distill-Qwen-7B\u4ece54.0\u63d0\u5347\u523061.3\uff0cDeepSeek-R1-Distill-Llama-8B\u4ece43.7\u63d0\u5347\u523053.7", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u5bc6\u96c6\u3001\u6821\u51c6\u826f\u597d\u7684\u6b65\u9aa4\u7ea7\u5956\u52b1\u4fe1\u53f7\uff0c\u6539\u5584\u4e86\u4fe1\u7528\u5206\u914d\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u589e\u5f3a\u4e86LLM\u7684\u6574\u4f53\u63a8\u7406\u8d28\u91cf\uff0c\u4e14\u6a21\u5757\u5316\u5224\u522b\u5668\u652f\u6301\u7075\u6d3b\u5956\u52b1\u5851\u9020"}}
