<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: 本文介绍了SpatiaLite基准测试，用于评估视觉语言模型的空间推理能力，发现当前先进VLMs主要依赖语言表示，在视觉中心任务上存在缺陷，并提出Imagery Driven Framework来改进空间推理。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型和视觉语言模型在逻辑推理、问题解决等方面表现出色，但空间推理（如心理旋转、导航、空间关系理解）仍然是重大挑战。作者假设想象力是空间世界模型中的主导推理机制。

Method: 引入SpatiaLite合成基准测试，联合测量空间推理准确性和效率；通过全面实验分析VLMs的空间推理机制；提出Imagery Driven Framework用于数据合成和训练。

Result: 发现三个关键结果：1）先进VLMs主要依赖语言表示，在需要感知空间关系和3D几何变换的视觉中心任务上存在显著缺陷；2）VLMs空间推理效率严重不足，token使用量随变换复杂度快速增加；3）IDF框架可以隐式构建内部世界模型，对VLMs空间推理至关重要。

Conclusion: 本研究界定了先进VLMs的空间推理限制和模式，识别了关键缺陷，并为未来进展提供了信息。SpatiaLite基准测试为评估和改进VLMs空间推理能力提供了重要工具。

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [2] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjøru,Rafael Cabañas,Helge Langseth,Antonio Salmerón*

Main category: cs.AI

TL;DR: 本文研究了将Bjøru等人提出的用于马尔可夫模型的因果概率边界计算方法扩展到半马尔可夫结构因果模型的方法，解决了外生变量可能影响多个内生变量的挑战。


<details>
  <summary>Details</summary>
Motivation: Bjøru等人的方法仅适用于马尔可夫模型，而半马尔可夫模型能够表示马尔可夫模型无法表示的混杂关系，因此需要扩展该方法以处理更一般的因果模型。

Method: 通过最小示例说明扩展挑战，提出替代解决方案策略，并进行理论和计算评估。

Result: 开发了适用于半马尔可夫结构因果模型的因果概率边界计算方法，能够处理外生变量影响多个内生变量的情况。

Conclusion: 成功将因果概率边界计算方法从马尔可夫模型扩展到半马尔可夫模型，为处理更复杂的因果推断问题提供了有效工具。

Abstract: Recently, Bjøru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [3] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: 本文系统分析了智能交通系统中大型视觉语言模型在精心设计的越狱攻击下的脆弱性，提出了基于图像排版操纵和多轮提示的新型越狱攻击方法，并开发了多层响应过滤防御技术。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态推理和实际应用中表现出强大能力，但极易受到越狱攻击。特别是在智能交通系统等关键应用中，这种脆弱性可能带来严重安全风险。

Method: 构建了与交通相关的有害查询数据集；开发了利用图像排版操纵和多轮提示的新型越狱攻击；提出了多层响应过滤防御技术。

Result: 在开源和闭源的最先进LVLMs上进行了广泛实验，使用GPT-4判断和人工验证评估攻击和防御效果。与现有越狱技术相比，该方法揭示了更严重的安全风险。

Conclusion: 图像排版操纵和多轮提示的越狱攻击对智能交通系统中的LVLMs构成严重安全威胁，需要有效的防御措施来保护系统安全。

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [4] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: 本文提出了一种新的匹配算法CORGI，用于解决规则系统中指数级时间和空间复杂度的匹配问题，特别适用于实时AI系统和数据库查询。


<details>
  <summary>Details</summary>
Motivation: 规则系统在实时应用中面临指数级时间和空间复杂度的匹配问题，特别是在AI系统自动生成规则时容易产生最坏情况匹配模式，导致程序执行缓慢或内存溢出。

Method: CORGI采用两步法：前向传递构建/维护接地关系图，后向迭代器按需生成匹配，避免了传统RETE算法中β-内存收集部分匹配的问题。

Result: 性能评估显示，CORGI在简单组合匹配任务上显著优于SOAR和OPS5的RETE实现。

Conclusion: CORGI算法通过消除高延迟延迟和内存溢出问题，为规则系统提供了实用的解决方案，无需手动工程约束即可避免不可预测的故障模式。

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [5] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 本文提出了一种基于场景图引导的生成AI框架，通过分析OSHA事故报告生成逼真的工作场所危险场景图像，并引入VQA框架评估生成数据的真实性和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 训练视觉模型准确检测工作场所危险需要真实的不安全状况图像，但由于难以捕捉实际发生的事故触发场景，获取此类数据集具有挑战性。

Method: 使用GPT-4o分析OSHA叙述提取结构化危险推理，转换为对象级场景图捕捉空间和上下文关系，然后指导文本到图像扩散模型生成构图准确的危险场景。

Result: 提出的VQA图分数在四个最先进的生成模型中，基于熵验证优于CLIP和BLIP指标，证实了其更高的判别敏感性。

Conclusion: 该框架能够有效生成逼真的工作场所危险场景图像，并通过VQA评估验证了生成数据的质量。

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [6] [Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases](https://arxiv.org/abs/2511.13987)
*Antonio Manuel Martínez-Heredia,Dolores Godrid Rodríguez,Andrés Ortiz García*

Main category: cs.AI

TL;DR: 本文综述了AI智能体在音乐分析和教育中的应用，通过实验验证了从规则模型到深度学习、多智能体架构和RAG框架的演进，展示了AI在提升音乐模式识别、创作参数化和教育反馈方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 整合AI智能体在音乐分析和教育中的历史发展，评估其在教学环境中的实际应用效果，解决传统自动化方法在可解释性和适应性方面的不足。

Method: 采用双案例研究方法：(1)在中学教育中使用生成式AI平台培养分析和创造技能；(2)设计用于符号音乐分析的多智能体系统，实现模块化、可扩展和可解释的工作流程。

Result: 实验结果表明AI智能体在音乐模式识别、创作参数化和教育反馈方面优于传统自动化方法，具有更好的可解释性和适应性。

Conclusion: 研究提出了一个统一框架，连接技术、教学和伦理考量，为计算音乐学和音乐教育中智能体的设计和应用提供基于证据的指导，同时强调了透明性、文化偏见和混合评估指标等关键挑战。

Abstract: This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.

</details>


### [7] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: ALEX是一个轻量级知识编辑框架，通过分层内存架构将知识更新组织为语义簇，将检索复杂度从O(N)降低到O(K+N/C)，并集成推理查询合成和动态证据裁决模块，显著提升多跳问答准确性和推理路径可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的知识是静态的，难以适应不断变化的信息，现有方法在处理需要多步推理的复杂多跳问题时面临可扩展性和检索效率的挑战。

Method: 提出ALEX框架，核心创新是分层内存架构，将知识更新组织为语义簇；集成推理查询合成模块来弥合查询与事实之间的语义差距；动态证据裁决引擎执行高效的两阶段检索过程。

Result: 在MQUAKE基准测试中，ALEX显著提高了多跳答案准确性和推理路径可靠性，同时将所需搜索空间减少了80%以上。

Conclusion: ALEX为构建可扩展、高效和准确的知识编辑系统提供了一条有前景的路径。

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [8] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: Syn-STARTS是一个使用LLM生成分诊案例的框架，用于解决大规模伤亡事件中真实数据稀缺的问题，验证了合成数据在开发高性能医疗AI模型中的潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件中的分诊决策至关重要，但真实数据难以获取，限制了AI模型的发展。需要高质量的基准数据集来开发和评估AI分诊系统。

Method: 开发Syn-STARTS框架，利用LLM生成分诊案例，并与人工整理的TRIAGE开放数据集进行质量对比验证。

Result: Syn-STARTS生成的分诊案例在质量上与人工整理的数据集难以区分，且在标准分诊方法START的四个类别（绿、黄、红、黑）评估中表现出高度稳定性。

Conclusion: 合成数据在开发用于严重和关键医疗情况的高性能AI模型中具有巨大潜力，为解决真实数据稀缺问题提供了可行方案。

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [9] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 本文提出了一种由教师主导的反馈循环系统，将概念级评估证据转化为经过验证的微干预措施，通过二进制整数规划方法实现个性化学习干预分配，确保技能覆盖和公平性。


<details>
  <summary>Details</summary>
Motivation: 自适应学习系统通常诊断准确但干预效果弱，导致帮助时机不当或内容不匹配。需要开发能够有效将诊断转化为针对性教学干预的系统。

Method: 采用二进制整数规划方法，包含覆盖度、时间、难度窗口、先决条件概念矩阵和反冗余多样性约束。使用贪心选择、基于梯度的松弛方法和混合方法在不同场景下优化干预分配。

Result: 在模拟和1204名学生的物理课程部署中，两种求解器都实现了几乎所有学习者在有限观看时间内的完整技能覆盖。基于梯度的方法比贪心方法减少约12%的冗余覆盖，并协调了难度分布。

Conclusion: 该系统提供了一个可追踪和可审计的控制器，能够闭合诊断-教学循环，在课堂规模上实现公平且负载感知的个性化学习。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [10] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: APD-agents是一个基于大语言模型的多智能体框架，用于自动化移动应用页面设计，通过多个智能体协作将用户描述转换为结构化布局设计。


<details>
  <summary>Details</summary>
Motivation: 移动应用页面布局设计耗时且需要专业技能，现有设计软件需要大量培训，跨页面协作设计需要额外时间确保样式一致性。

Method: 采用多智能体框架，包含编排智能体、语义解析智能体、主布局智能体、模板检索智能体和递归组件智能体，通过动态协作完成设计任务。

Result: 在RICO数据集上的实验结果表明，APD-agents达到了最先进的性能水平。

Conclusion: 该工作充分利用了大模型驱动的多智能体系统的自动协作能力，实现了高效的自动化页面设计。

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [11] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为R3的双过程思维框架，用于解决视觉语言导航任务中LLM存在的空间理解不足和计算成本高的问题。该框架通过Runner、Ruminator和Regulator三个模块的协同工作，在REVERIE基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的视觉语言导航方法存在两个主要问题：一是LLM难以精确理解真实世界的空间关联，导致任务完成性能与领域专家存在显著差距；二是引入LLM会带来巨大的计算成本和推理延迟。

Method: 提出R3双过程思维框架，包含三个核心模块：Runner（轻量级transformer专家模型，负责常规情况下的高效导航）、Ruminator（基于强大多模态LLM，采用思维链提示进行结构化推理）、Regulator（监控导航进度并根据三个标准控制适当的思考模式）。

Result: 在REVERIE基准测试中，R3显著优于其他最先进方法，SPL和RGSPL分别超过3.28%和3.30%。

Conclusion: R3框架通过整合LLM的泛化能力和VLN特定专业知识，在零样本设置下有效处理具有挑战性的VLN任务，证明了该方法的有效性。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [12] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: 该论文测试大型语言模型对时间顺序的理解能力，发现在金融应用中模型存在前瞻性偏差问题。通过三种时序任务测试发现，模型在局部顺序保持良好但全局时间线一致性不足，而增加推理预算能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 测试大型语言模型是否真正理解时间顺序，特别是在金融和经济学应用中，避免前瞻性偏差对模型应用的潜在影响。

Method: 设计三种时序任务：时间顺序排序、条件排序（先过滤后排序）和时代错误检测，评估GPT-4.1、Claude-3.7 Sonnet和GPT-5在不同推理强度设置下的表现。

Result: 随着序列长度增加，精确匹配率显著下降但排序相关性保持较高；条件排序中过滤步骤是主要失败原因；时代错误检测是最简单的任务。GPT-5和带扩展思维的Claude-3.7表现最佳。

Conclusion: 当前LLMs在时序任务上存在局限性，但分配明确推理预算能显著改善性能，这对金融领域的实时应用具有重要意义。

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [13] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 提出一种两阶段架构来减少Whisper语音识别模型在噪声环境下的幻觉错误，包括自适应层注意力机制和多目标知识蒸馏框架。


<details>
  <summary>Details</summary>
Motivation: Whisper模型在噪声条件下经常出现幻觉错误，而现有方法主要关注音频预处理或转录后处理，对模型本身的修改研究较少。

Method: 第一阶段使用自适应层注意力将编码器层分组为语义连贯块，通过多头注意力融合块表示；第二阶段使用多目标知识蒸馏框架，在噪声音频上训练学生模型，使其语义和注意力分布与处理干净输入的教师模型对齐。

Result: 在噪声语音基准测试中显著减少了幻觉错误和词错误率，同时在干净语音上保持了性能。

Conclusion: 自适应层注意力和知识蒸馏提供了改进Whisper在真实世界噪声条件下可靠性的原则性策略。

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [14] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: DevPiolt是一个基于大语言模型的物联网设备操作推荐系统，通过持续预训练、多任务微调、直接偏好优化和置信度控制机制，显著提升了推荐性能，并在小米家庭应用中成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型在处理物联网设备操作时面临复杂操作逻辑、多样化用户偏好和对次优建议敏感等问题，限制了其适用性。

Method: 1) 通过持续预训练和多任务微调为LLM提供物联网操作领域知识；2) 使用直接偏好优化使微调后的LLM与特定用户偏好对齐；3) 设计基于置信度的曝光控制机制避免低质量推荐带来的负面体验。

Result: 在所有数据集上显著优于基线模型，各项指标平均提升69.5%。在线部署后，独立访客设备覆盖率增加21.6%，页面浏览接受率增加29.1%。

Conclusion: DevPiolt通过结合领域知识、用户偏好对齐和置信度控制，有效解决了物联网设备操作推荐中的关键挑战，在实际应用中取得了显著效果。

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [15] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: 提出一个新颖的时间序列预测框架，用于预测区域级Airbnb市场的三个关键指标：收入、预订天数和预订数量，使用滑动窗口方法预测1-3个月的趋势。


<details>
  <summary>Details</summary>
Motivation: 短期租赁平台（如Airbnb）的扩张严重扰乱了当地住房市场，导致租金上涨和住房负担能力问题。准确预测区域Airbnb市场趋势可以为政策制定者和城市规划者提供关键见解。

Method: 构建区域表示，整合房源特征与外部情境因素（如城市可达性和人员流动性），将结构化表格数据转换为基于提示的LLM输入，生成全面的区域嵌入，然后输入先进的时间序列模型（RNN、LSTM、Transformer）。

Result: 在首尔Airbnb数据集上的实验表明，该方法相比传统统计和机器学习基线，平均RMSE和MAE降低了约48%。

Conclusion: 该框架不仅提高了预测准确性，还为检测供应过剩区域和支持数据驱动的城市政策决策提供了实用见解。

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [16] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: PathMind是一个新颖的知识图谱推理框架，通过选择性引导LLMs使用重要推理路径来增强忠实和可解释的推理。它采用"检索-优先化-推理"范式，包含路径优先化机制和双阶段训练策略，在复杂推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的KGR方法存在两个关键限制：1）无差别提取推理路径，可能引入无关噪声误导LLMs；2）动态探索推理路径需要高检索需求和频繁LLM调用。

Method: PathMind遵循"检索-优先化-推理"范式：1）检索模块从KG中检索查询子图；2）路径优先化机制使用语义感知路径优先级函数识别重要推理路径；3）通过双阶段训练策略（任务特定指令调优和路径偏好对齐）生成准确响应。

Result: 在基准数据集上的广泛实验表明，PathMind始终优于竞争基线，特别是在复杂推理任务上，通过识别基本推理路径，使用更少的输入token实现更好性能。

Conclusion: PathMind通过选择性引导LLMs使用重要推理路径，有效解决了现有LLM-based KGR方法的局限性，实现了更忠实、可解释和高效的推理。

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [17] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: 本文通过重新表述和扰动经典CSPLib问题来测试LLMs在自动生成优化模型时的真实推理能力，发现LLMs的表现对上下文和语言变化高度敏感，表明其理解较浅层。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs自动生成优化模型的真实能力，区分数据污染与真实推理，验证LLMs是否真正理解问题结构而非仅依赖训练数据中的模式。

Method: 系统性地重新表述和扰动一组知名CSPLib问题，保持结构不变但修改上下文并引入误导元素，比较三种代表性LLMs在原始和修改描述下的模型生成表现。

Result: LLMs能生成语法有效且语义合理的模型，但在上下文和语言变化下性能急剧下降，显示出浅层理解和对措辞的敏感性。

Conclusion: LLMs在自动生成优化模型方面的成功可能更多源于数据污染而非真实推理能力，其表现对问题表述的变化高度脆弱。

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [18] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 本研究探讨了在LLM对齐过程中融入多元社会价值观的影响，通过系统评估人口统计差异和设计参数，发现不同社会群体对模型响应的评价存在显著差异，技术设计选择（如分歧处理方法和评分量表）对模型行为有重要影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐决策往往忽视人类社会的多样性，本研究旨在探索如何通过纳入多元价值观来影响LLM行为，以平衡安全性和公平代表性。

Method: 收集来自美国和德国参与者的27,375个评分数据，在毒性、情感意识、敏感性、刻板印象偏见和帮助性五个维度上评估LLM响应。使用不同社会群体的偏好对多个大型语言模型和大型推理模型进行微调，同时改变评分量表、分歧处理方法和优化技术。

Result: 发现系统性人口统计效应：男性参与者对毒性评分比女性低18%；保守派和黑人参与者对情感意识评分分别比自由派和白人参与者高27.9%和44%。技术设计选择显示强烈影响：保留评分者分歧比多数投票实现约53%更大的毒性减少；5点量表比二元格式产生约22%更多减少；DPO在多值优化中始终优于GRPO。

Conclusion: 这些发现为回答关键问题提供了初步步骤：对齐过程应如何平衡专家驱动和用户驱动的信号，以确保安全性和公平代表性。

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [19] [A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning](https://arxiv.org/abs/2511.14533)
*Jiahao Wu,Shengwen Yu*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号框架，将感知不确定性显式建模并传播到规划中，通过Transformer感知前端和GNN关系推理提取概率符号状态，结合不确定性感知符号规划器在置信度低时主动收集信息。在桌面机器人操作任务中验证了有效性，在多个基准测试中达到90.7%平均成功率，优于POMDP基线10-14个百分点。


<details>
  <summary>Details</summary>
Motivation: 弥合连续感知信号和离散符号推理之间的鸿沟是AI系统在不确定性下运行的基本挑战，需要建立感知和规划这两个抽象层次之间的原则性连接。

Method: 结合Transformer感知前端和GNN关系推理从视觉观察中提取概率符号状态，使用不确定性感知符号规划器在置信度低时主动收集信息。

Result: 在10,047个PyBullet生成场景中处理3-10个物体，输出校准置信度的概率谓词（总体F1=0.68）。在规划器中嵌入后，在Simple Stack、Deep Stack和Clear+Stack基准测试中分别达到94%/90%/88%成功率（平均90.7%），比最强POMDP基线提高10-14个百分点，规划时间在15ms内。

Conclusion: 该框架建立了校准不确定性与规划收敛之间的定量联系，提供了理论保证并经验验证。该通用框架可应用于任何需要从感知输入到符号规划的不确定性感知推理领域。

Abstract: Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\%/90\%/88\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.

</details>


### [20] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: 该论文提出了一个基于率失真理论和最优传输几何的知识图谱构建与优化框架，用于从非结构化教育材料自动生成高质量选择题。


<details>
  <summary>Details</summary>
Motivation: 将非结构化教育材料（如讲义和幻灯片）转换为能够捕捉关键教学内容的知识图谱仍然很困难，需要一种原则性的方法来构建和优化任务导向的知识图谱。

Method: 使用率失真理论和最优传输几何，将讲座内容建模为度量-测度空间，通过融合Gromov-Wasserstein耦合量化语义失真，并应用精炼操作符（添加、合并、拆分、移除、重连）来最小化率失真拉格朗日函数。

Result: 在数据科学讲座上的原型应用显示，从优化后的知识图谱生成的选择题在15个质量标准上持续优于从原始笔记生成的选择题。

Conclusion: 这项研究为个性化教育和AI辅助教育中的信息论知识图谱优化建立了原则性基础。

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [21] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 该论文将GCS算法的形式模型扩展到实现近似的假设下，通过将单向测量范式改为双向测量范式，移除了许多先前为证明性能而施加的限制，显著降低了算法估计误差。


<details>
  <summary>Details</summary>
Motivation: 扩展GCS算法的形式模型，使其在实现近似的假设下运行，通过改变测量范式来移除先前工作中的限制，创造更现实的部署模型。

Method: 将单向测量范式替换为双向测量范式，取消统一链路长度要求，提供频率源的形式模型，对算法估计误差的不同组成部分进行细粒度区分。

Result: 显著降低了不确定性对算法估计误差的贡献，从每个链路延迟的数量级降低到延迟的10%到0.1%，并展示了GCS局部和全局偏差的匹配上界。

Conclusion: 通过采用双向测量范式，在保持GCS核心行为的同时，显著提高了算法的实际适用性和性能，误差降低了多个数量级。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [22] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: TT-Edge是一个软硬件协同设计框架，通过在边缘AI处理器上专门设计TTD引擎来加速张量训练分解，实现1.7倍速度提升和40.2%能耗降低。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上张量训练分解(TTD)因重复SVD和矩阵乘法带来的延迟和能耗问题，满足资源受限边缘设备的高效模型压缩需求。

Method: 将SVD分解为双对角化和对角化两个阶段，将计算密集型任务卸载到专门的TTD引擎，该引擎与现有GEMM加速器紧密集成，减少矩阵向量传输。

Result: 在RISC-V边缘AI处理器上压缩ResNet-32模型时，相比仅使用GEMM的基线，速度提升1.7倍，总能耗降低40.2%，总功耗仅增加4%。

Conclusion: TT-Edge通过轻量级设计有效解决了边缘环境中基于TTD压缩的延迟和能耗瓶颈，硬件开销最小且可重用GEMM资源。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [23] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Gaia是一个GPU即服务模型和架构，通过动态执行模式识别和运行时评估，为异构环境中的无服务器AI工作负载提供SLO感知、成本高效的硬件加速。


<details>
  <summary>Details</summary>
Motivation: 当前平台在管理硬件加速方面存在困难，静态用户设备分配无法在变化负载或位置下确保SLO合规，一次性动态选择往往导致次优或成本低效的配置。

Method: Gaia结合了轻量级执行模式标识器（在部署时检查函数代码并发出四种执行模式之一）和动态函数运行时（持续重新评估用户定义的SLO以在CPU和GPU后端之间进行升级或降级）。

Result: 评估显示Gaia能够无缝选择最适合工作负载的硬件加速，将端到端延迟降低高达95%。

Conclusion: Gaia为异构环境中的无服务器AI实现了SLO感知、成本高效的加速。

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [24] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: VOLT是一个针对开源GPU的轻量级编译器工具链，旨在解决现有开源GPU架构中SIMT功能执行和性能优化的技术挑战。


<details>
  <summary>Details</summary>
Motivation: 开源GPU研究正在兴起，但执行现有GPU程序和在开源ISA上优化性能依赖于技术复杂且开发成本常被低估的编译器框架。

Method: VOLT采用分层设计，通过中间端集中SIMT相关分析和优化，支持多种前端语言和开源GPU硬件，实现跨抽象级别的SIMT代码生成和优化。

Result: VOLT能够支持SIMT执行，并通过ISA扩展和主机运行时API的案例研究展示了其可扩展性。

Conclusion: VOLT为开源GPU开发提供了可扩展的编译器框架，能够适应不断发展的GPU架构，促进开源GPU生态的发展。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [25] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: ADP是一个完全GPU驻留的框架，通过自动动态精度和指数跨度容量估计器，使用低精度单元模拟双精度精度，在保持FP64精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现代GPU硬件转向低精度格式（如FP16、FP8、FP4），但科学计算需要双精度精度。研究目标是利用低精度单元通过分解方案模拟双精度计算。

Method: 提出自动动态精度（ADP）框架，核心是指数跨度容量（ESC）估计器，用于确定分解参数。集成异常处理、运行时启发式算法，并采用无符号整数切片方案提高表示效率。

Result: 在55位尾数设置下，相比原生FP64 GEMM，在NVIDIA Blackwell GB200上实现2.3倍加速，在RTX Pro 6000 Blackwell Server Edition上实现13.2倍加速，运行时开销小于10%。

Conclusion: 低精度加速器可以作为高保真、高性能科学计算工作负载的实用、生产就绪基础。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [26] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 本文提出语义多路复用概念，将多任务压缩表示合并为单一语义表示，可在不增加天线或带宽的情况下复用更多任务，显著降低延迟、能耗和通信负载。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统仅支持比特级并行传输，限制了可并发处理的任务数量。为解决这一瓶颈，需要从比特到任务的流多路复用转变。

Method: 引入语义多路复用概念，将多个任务相关的压缩表示合并为单一语义表示，在语义层扩展有效自由度。在Jetson Orin Nano和毫米波软件定义无线电实验平台上进行原型实现。

Result: 语义多路复用可在保持足够任务准确性的同时联合处理多个任务。例如，在4×4信道上复用任务数从2增加到8时，图像分类准确率下降不到4%。相比基线方法，延迟降低8倍，能耗降低25倍，通信负载降低54倍。

Conclusion: 语义多路复用能够有效突破传统通信系统的并行处理限制，显著提升多任务处理效率，同时保持性能可比性。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [27] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: MPI派生数据类型(DDT)的性能评估显示结果复杂：DDT在某些MPI实现和应用中表现最佳，但在其他情况下可能最慢，没有统一的性能优势。


<details>
  <summary>Details</summary>
Motivation: 评估MPI派生数据类型在实际应用中的性能表现，澄清关于DDT性能的争议，并提供跨MPI实现的性能对比。

Method: 使用三个2D应用（Jacobi CFD求解器、康威生命游戏、基于格点的图像重建），每个应用分别采用手动打包(BASIC)和DDT两种实现方式，在四种MPI实现(MPICH、Open MPI、Intel MPI、MVAPICH2)上测试强扩展和弱扩展。

Result: 结果复杂：DDT在Intel MPI和MPICH的图像重建代码中表现最快，但在Open MPI和MVAPICH2的相同代码中表现最慢。CFD求解器中BASIC版本普遍优于DDT，而生命游戏中性能排名因MPI库而异。

Conclusion: 没有单一策略在所有程序、通信语义和MPI实现中占优，DDT的性能可移植性无法保证。建议在目标MPI实现和通信模式下同时分析DDT和手动打包设计的性能。

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [28] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: ParallelKittens (PK) 是一个简化的 CUDA 框架，通过八个核心原语和统一编程模板，系统化地指导多 GPU 内核设计，在 Hopper 和 Blackwell 架构上显著提升了各种并行工作负载的性能。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 模型规模扩大和硬件计算吞吐量提升超过互连带宽改进，GPU 间通信已成为现代 AI 工作负载的主要瓶颈。现有系统通过计算-通信重叠来缓解，但往往无法在异构工作负载和新加速器上达到理论峰值性能。

Method: PK 扩展了 ThunderKittens 框架，通过八个核心原语和统一编程模板体现多 GPU 内核设计原则，这些原则基于对多 GPU 性能影响因素的全面分析，包括数据传输机制、资源调度和设计开销。

Result: 在 Hopper 和 Blackwell 架构上验证，仅用少于 50 行设备代码，PK 实现了：数据并行和 tensor 并行工作负载最高 2.33 倍加速，序列并行工作负载最高 4.08 倍加速，专家并行工作负载最高 1.22 倍加速。

Conclusion: PK 框架证明通过少量简单、可重用的原则可以系统化指导最优多 GPU 内核设计，显著简化了重叠多 GPU 内核的开发过程，并在多种并行工作负载中实现了显著的性能提升。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [29] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: FailSafe是一个容错的张量并行服务系统，通过循环KVCache放置、混合注意力机制和细粒度负载感知路由等技术，在GPU故障时维持高性能LLM推理，相比标准方法提升2倍吞吐量并降低两个数量级的恢复延迟。


<details>
  <summary>Details</summary>
Motivation: 现有张量并行(TP)系统存在脆弱性问题：单个GPU故障就会导致执行中断，触发昂贵的KVCache重新计算，并引入长期的计算和内存不平衡。

Method: FailSafe采用三种关键技术：1) 循环KVCache放置实现均匀内存利用；2) 结合张量和数据并行的混合注意力消除落后节点；3) 细粒度负载感知路由动态平衡请求。同时使用主动KVCache备份和按需权重恢复来避免重新计算和冗余数据传输。

Result: 在8xH100 DGX系统上测试，使用真实故障轨迹和代表性工作负载，FailSafe相比标准故障处理方法实现了高达2倍的吞吐量提升和两个数量级的恢复延迟降低。即使出现最多三个GPU故障，仍能维持高吞吐量和平衡利用。

Conclusion: FailSafe证明了在动态和不可靠硬件条件下实现稳健高效LLM服务的可行性，为大规模语言模型推理提供了有效的容错解决方案。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [30] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 10Cache是一个资源感知的张量缓存和迁移系统，通过智能协调GPU、CPU和NVMe之间的内存使用来加速大语言模型训练，解决了现有方法中张量迁移延迟高和设备内存利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 云端训练大语言模型面临GPU内存瓶颈，现有GPU内存卸载到CPU和NVMe的方法存在高张量迁移延迟和次优设备内存利用率，导致训练时间增加和云成本上升。

Method: 10Cache通过分析张量执行顺序构建预取策略，基于张量大小分布在固定内存中分配内存缓冲区，并重用内存缓冲区以最小化分配开销。

Result: 在多样化LLM工作负载中，10Cache实现了高达2倍的训练加速，GPU缓存命中率提升高达86.6倍，CPU/GPU内存利用率分别提高2.15倍和1.33倍。

Conclusion: 10Cache是优化云端LLM训练吞吐量和资源效率的实用且可扩展解决方案。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [31] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 本文分析了跨组织联邦学习中参与者故障对模型质量的影响，重点关注故障时机、数据分布和评估方法等关键因素。研究表明在高数据倾斜情况下评估结果过于乐观，故障时机也会显著影响模型质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在跨组织协作场景中需要保证系统可靠性，但参与者可能因各种原因发生故障。目前针对跨设备联邦学习的研究较多，但对跨组织联邦学习中参与者故障影响的研究相对较少。

Method: 通过广泛的实验研究，分析跨组织联邦学习中参与者故障对模型质量的影响，重点关注故障时机、数据分布和评估方法等关键因素。

Result: 研究表明，在高数据倾斜情况下，评估结果过于乐观，掩盖了故障的真实影响。同时，故障发生的时机显著影响最终训练模型的质量。

Conclusion: 研究结果为构建鲁棒的联邦学习系统提供了重要见解，帮助研究者和软件架构师更好地理解和应对参与者故障问题。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [32] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: Hapax Locks是一种新颖的锁定算法，具有简单性、恒定时间到达和释放路径、FIFO准入顺序、空间效率高、在争用情况下产生较少一致性流量等特点。


<details>
  <summary>Details</summary>
Motivation: 开发一种性能与最先进锁相当但约束更少、更容易集成到现有系统中的锁定算法，特别是不需要在线程间传递指针所有权。

Method: 提出Hapax Locks锁定算法，该算法采用恒定时间到达和释放路径，提供FIFO准入顺序，空间效率高，争用情况下产生较少一致性流量。

Result: Hapax Locks在延迟和可扩展性方面与最先进锁性能相当，同时对运行时环境约束更少，更容易集成到现有系统或API中。

Conclusion: Hapax Locks是一种简单高效的锁定算法，性能优异且易于集成，特别适合现有系统的改造和集成需求。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [33] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: Seer是一个创新的在线上下文学习系统，通过利用共享相同提示的请求在输出长度和生成模式上的相似性，解决了强化学习中同步RL系统的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有同步强化学习系统在rollout阶段面临严重的性能瓶颈，包括长尾延迟和资源利用率低的问题，这是由于工作负载不平衡导致的。

Method: Seer引入了三种关键技术：动态负载均衡的分割rollout、上下文感知调度和自适应分组推测解码，这些机制共同优化了rollout阶段的性能。

Result: 在生产级RL工作负载上的评估显示，与最先进的同步RL系统相比，Seer将端到端rollout吞吐量提高了74%到97%，并将长尾延迟降低了75%到93%。

Conclusion: Seer系统显著加速了RL训练迭代，通过利用请求间的相似性有效解决了rollout阶段的性能瓶颈问题。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [34] [ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.13771)
*Shaowei Guan,Yu Zhai,Zhengyu Zhang,Yanze Wang,Hin Chi Kwok*

Main category: cs.CR

TL;DR: 本文提出了ExplainableGuard框架，利用DeepSeek-Reasoner的思维链推理能力，提供可解释的对抗性防御，不仅能检测和中和文本中的对抗性扰动，还能为每个防御动作提供逐步解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型日益面临对抗性攻击威胁，现有防御机制多为黑盒操作，缺乏决策透明度。需要开发既能有效防御又能提供解释的透明防御框架。

Method: 利用定制的思维链提示引导LLM进行多维度分析（字符、词汇、结构和语义），生成净化输出和人类可读的防御理由。

Result: 在GLUE基准和IMDB电影评论数据集上的初步结果显示良好的防御效果。人工评估表明其解释在清晰度、特异性和可操作性方面优于消融变体，部署可信度评分为72.5%。

Conclusion: ExplainableGuard框架展示了为更可信的LLM部署提供透明对抗性防御的潜力，通过可解释的防御机制增强用户信任。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.

</details>


### [35] [Hashpower allocation in Pay-per-Share blockchain mining pools](https://arxiv.org/abs/2511.13777)
*Pierre-Olivier Goffard,Hansjoerg Albrecher,Jean-Pierre Fouque*

Main category: cs.CR

TL;DR: 该论文研究了PoW区块链中矿工通过加入PPS挖矿池来降低风险的行为，分析了矿工如何在风险转移和管理费之间权衡资源分配。


<details>
  <summary>Details</summary>
Motivation: PoW挖矿存在显著风险，矿工面临持续运营成本但只能获得不频繁的资本收益。加入挖矿池是常见的风险缓解策略，但需要研究矿工如何在风险转移和管理费之间进行权衡。

Method: 使用简化的矿工财富模型，研究PPS奖励系统中矿工如何在不同挖矿池之间分配计算资源，考虑池管理器调整份额难度和管理费的影响。

Result: 分析了矿工在风险转移和管理费之间的权衡决策，为矿工资源分配提供了理论框架。

Conclusion: 矿工需要综合考虑风险转移收益和管理费成本，在不同挖矿池之间优化分配计算资源以实现风险收益平衡。

Abstract: Mining blocks in a blockchain using the \textit{Proof-of-Work} consensus protocol involves significant risk, as network participants face continuous operational costs while earning infrequent capital gains upon successfully mining a block. A common risk mitigation strategy is to join a mining pool, which combines the computing resources of multiple miners to provide a more stable income. This article examines a Pay-per-Share (PPS) reward system, where the pool manager can adjust both the share difficulty and the management fee. Using a simplified wealth model for miners, we explore how miners should allocate their computing resources among different mining pools, considering the trade-off between risk transfer to the manager and management fees.

</details>


### [36] [Human-Centered Threat Modeling in Practice: Lessons, Challenges, and Paths Forward](https://arxiv.org/abs/2511.13781)
*Warda Usman,Yixin Zou,Daniel Zappala*

Main category: cs.CR

TL;DR: 本文通过对23位研究人员的半结构化访谈，探讨了以人为中心的威胁建模(HCTM)的实践现状，包括研究设计、威胁识别以及处理价值观、约束和长期目标的方法。研究发现HCTM不是规定性过程，而是由参与者关系、学科背景和制度结构塑造的演化实践。


<details>
  <summary>Details</summary>
Motivation: 虽然研究人员越来越多地从以人为中心的视角进行威胁建模，但对于他们如何在实践中准备和参与HCTM知之甚少。本研究旨在填补这一空白，了解HCTM的实际执行情况。

Method: 采用23个半结构化访谈的研究方法，考察研究人员如何进行HCTM研究设计、威胁识别以及应对价值观、约束和长期目标。

Result: 研究发现HCTM是一套不断演化的实践，受参与者关系、学科背景和制度结构影响。研究人员通过持续的基础工作和以参与者为中心的调查来进行威胁建模，遵循关怀、正义和自主等价值观。同时面临情感压力、伦理困境和结构性障碍等挑战。

Conclusion: 提出了通过共享基础设施、更广泛认可多样化贡献以及加强将研究成果转化为政策、设计和社会变革的机制来推进HCTM发展的机会。

Abstract: Human-centered threat modeling (HCTM) is an emerging area within security and privacy research that focuses on how people define and navigate threats in various social, cultural, and technological contexts. While researchers increasingly approach threat modeling from a human-centered perspective, little is known about how they prepare for and engage with HCTM in practice. In this work, we conduct 23 semi-structured interviews with researchers to examine the state of HCTM, including how researchers design studies, elicit threats, and navigate values, constraints, and long-term goals. We find that HCTM is not a prescriptive process but a set of evolving practices shaped by relationships with participants, disciplinary backgrounds, and institutional structures. Researchers approach threat modeling through sustained groundwork and participant-centered inquiry, guided by values such as care, justice, and autonomy. They also face challenges including emotional strain, ethical dilemmas, and structural barriers that complicate efforts to translate findings into real-world impact. We conclude by identifying opportunities to advance HCTM through shared infrastructure, broader recognition of diverse contributions, and stronger mechanisms for translating findings into policy, design, and societal change.

</details>


### [37] [Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks](https://arxiv.org/abs/2511.13789)
*Haotian Jin,Yang Li,Haihui Fan,Lin Shen,Xiangfang Li,Bo Li*

Main category: cs.CR

TL;DR: 提出基于注意力相似性的后门检测方法，无需先验知识即可检测动态或隐式触发器，并通过注意力安全对齐和逐头微调来缓解后门攻击影响。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对大型语言模型安全构成严重威胁，现有防御方法局限于特定触发器类型或需要额外干净模型支持，难以应对动态或隐式触发器的挑战。

Method: 基于注意力相似性检测后门，发现受攻击模型在触发条件下注意力头间相似度异常高，采用注意力安全对齐和逐头微调来修正受污染的注意力头。

Result: 实验结果表明该方法显著降低后门攻击成功率，同时保持模型在下游任务上的性能。

Conclusion: 提出的基于注意力相似性的方法能够有效检测和缓解后门攻击，无需先验知识，具有良好的实用性和泛化能力。

Abstract: Backdoor attacks pose a serious threat to the security of large language models (LLMs), causing them to exhibit anomalous behavior under specific trigger conditions. The design of backdoor triggers has evolved from fixed triggers to dynamic or implicit triggers. This increased flexibility in trigger design makes it challenging for defenders to identify their specific forms accurately. Most existing backdoor defense methods are limited to specific types of triggers or rely on an additional clean model for support. To address this issue, we propose a backdoor detection method based on attention similarity, enabling backdoor detection without prior knowledge of the trigger. Our study reveals that models subjected to backdoor attacks exhibit unusually high similarity among attention heads when exposed to triggers. Based on this observation, we propose an attention safety alignment approach combined with head-wise fine-tuning to rectify potentially contaminated attention heads, thereby effectively mitigating the impact of backdoor attacks. Extensive experimental results demonstrate that our method significantly reduces the success rate of backdoor attacks while preserving the model's performance on downstream tasks.

</details>


### [38] [Zipf-Gramming: Scaling Byte N-Grams Up to Production Sized Malware Corpora](https://arxiv.org/abs/2511.13808)
*Edward Raff,Ryan R. Curtin,Derek Everett,Robert J. Joyce,James Holt*

Main category: cs.CR

TL;DR: 提出了一种名为Zipf-Gramming的新算法，用于高效提取字节n-gram特征，在恶意软件检测中实现了35倍的速度提升和30%的AUC改进。


<details>
  <summary>Details</summary>
Motivation: 现有的字节n-gram分类器在恶意软件检测中虽然速度快、体积小，但更新模型成本高昂，无法频繁部署更新的模型。

Method: 利用Zipf分布特性开发新的top-k n-gram提取器，通过理论分析和工程实现相结合的方法。

Result: 新算法比之前最佳方案快35倍，能够扩展生产训练集，在检测新恶意软件时AUC提升高达30%。

Conclusion: Zipf-Gramming算法能够以较小误差选择top-k项，实现了理论与工程的有机结合，为恶意软件检测提供了高效解决方案。

Abstract: A classifier using byte n-grams as features is the only approach we have found fast enough to meet requirements in size (sub 2 MB), speed (multiple GB/s), and latency (sub 10 ms) for deployment in numerous malware detection scenarios. However, we've consistently found that 6-8 grams achieve the best accuracy on our production deployments but have been unable to deploy regularly updated models due to the high cost of finding the top-k most frequent n-grams over terabytes of executable programs. Because the Zipfian distribution well models the distribution of n-grams, we exploit its properties to develop a new top-k n-gram extractor that is up to $35\times$ faster than the previous best alternative. Using our new Zipf-Gramming algorithm, we are able to scale up our production training set and obtain up to 30\% improvement in AUC at detecting new malware. We show theoretically and empirically that our approach will select the top-k items with little error and the interplay between theory and engineering required to achieve these results.

</details>


### [39] [The Battle of Metasurfaces: Understanding Security in Smart Radio Environments](https://arxiv.org/abs/2511.13939)
*Paul Staat,Christof Paar,Swarun Kumar*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Metasurfaces, or Reconfigurable Intelligent Surfaces (RISs), have emerged as a transformative technology for next-generation wireless systems, enabling digitally controlled manipulation of electromagnetic wave propagation. By turning the traditionally passive radio environment into a smart, programmable medium, metasurfaces promise advances in communication and sensing. However, metasurfaces also present a new security frontier: both attackers and defenders can exploit them to alter wireless propagation for their own advantage. While prior security research has primarily explored unilateral metasurface applications - empowering either attackers or defenders - this work investigates symmetric scenarios, where both sides possess comparable metasurface capabilities. Using both theoretical modeling and real-world experiments, we analyze how competing metasurfaces interact for diverse objectives, including signal power and sensing perception. Thereby, we present the first systematic study of context-agnostic metasurface-to-metasurface interactions and their implications for wireless security. Our results reveal that the outcome of metasurface "battles" depends on an interplay of timing, placement, algorithmic strategy, and hardware scale. Across multiple case studies in Wi-Fi environments, including wireless jamming, channel obfuscation for sensing and communication, and sensing spoofing, we demonstrate that opposing metasurfaces can substantially or fully negate each other's effects. By undermining previously proposed security and privacy schemes, our findings open new opportunities for designing resilient and high-assurance physical-layer systems in smart radio environments.

</details>


### [40] [Privis: Towards Content-Aware Secure Volumetric Video Delivery](https://arxiv.org/abs/2511.14005)
*Kaiyuan Hu,Hong Kang,Yili Jin,Junhua Liu,Chengming Hu,Haolun Wu,Xue Liu*

Main category: cs.CR

TL;DR: Privis是一个针对体积视频传输的安全框架，通过显著性引导的分区加密和自适应密钥轮换，在保证机密性的同时满足XR应用的严格延迟要求。


<details>
  <summary>Details</summary>
Motivation: 当前体积视频传输沿用2D视频的统一加密方案，忽视了不同几何数据的隐私敏感性差异和XR应用严格的运动到光子延迟约束。

Method: 将体积资源分区为独立单元，应用轻量级认证加密和自适应密钥轮换，采用选择性流量整形来平衡机密性和低延迟。

Result: 提出了通用的传输层安全架构，定义了核心抽象和自适应保护机制，并通过原型实现和延迟测量验证了可行性。

Conclusion: Privis为实时、显著性条件的安全传输提供了早期实证指导，是内容感知安全体积视频传输的初步探索。

Abstract: Volumetric video has emerged as a key paradigm in eXtended Reality (XR) and immersive multimedia because it enables highly interactive, spatially consistent 3D experiences. However, the transport-layer security for such 3D content remains largely unaddressed. Existing volumetric streaming pipelines inherit uniform encryption schemes from 2D video, overlooking the heterogeneous privacy sensitivity of different geometry and the strict motion-to-photon latency constraints of real-time XR.
  We take an initial step toward content-aware secure volumetric video delivery by introducing Privis, a saliency-guided transport framework that (i) partitions volumetric assets into independent units, (ii) applies lightweight authenticated encryption with adaptive key rotation, and (iii) employs selective traffic shaping to balance confidentiality and low latency. Privis specifies a generalized transport-layer security architecture for volumetric media, defining core abstractions and adaptive protection mechanisms. We further explore a prototype implementation and present initial latency measurements to illustrate feasibility and design tradeoffs, providing early empirical guidance toward future work on real-time, saliency-conditioned secure delivery.

</details>


### [41] [Location-Dependent Cryptosystem](https://arxiv.org/abs/2511.14032)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: 提出了一种基于超宽带(UWB)数据传输包飞行时间差异的位置依赖加密系统，解密密钥通过精确的时间戳模式隐式编码，只有位于授权区域内的接收器才能正确解密。


<details>
  <summary>Details</summary>
Motivation: 解决传统加密方案无法防止解密密钥泄露后被滥用的缺陷，确保只有在特定物理位置的接收器才能解密受保护内容。

Method: 利用UWB精确计时硬件和自定义JMTK协议，将SHA-256哈希的AES密钥映射到预定的传输时间戳模式上，通过时间飞行差异实现位置依赖解密。

Result: 实现了完整的原型系统，能够加密传输音频数据，只有位于授权区域内的接收器才能正确解密，系统无需电子或物理共享解密密码，且能防止窃听者恢复密钥。

Conclusion: 该位置依赖加密系统有效解决了密钥泄露后的滥用问题，为数字内容分发和专有研究提供了基于物理位置的安全保障。

Abstract: Digital content distribution and proprietary research-driven industries face persistent risks from intellectual property theft and unauthorized redistribution. Conventional encryption schemes such as AES, TDES, ECC, and ElGamal provide strong cryptographic guarantees, but they remain fundamentally agnostic to where decryption takes place.In practice, this means that once a decryption key is leaked or intercepted, any adversary can misuse the key to decrypt the protected content from any location. We present a location-dependent cryptosystem in which the decryption key is not transmitted as human- or machine-readable data, but implicitly encoded in precise time-of-flight differences of ultra-wideband (UWB) data transmission packets. The system leverages precise timing hardware and a custom JMTK protocol to map a SHA-256 hashed AES key onto scheduled transmission timestamps. Only receivers located within a predefined spatial region can observe the packet timings that align with the intended "time slot" pattern, enabling them to reconstruct the key and decrypt the secret. Receivers outside the authorized region observe incorrect keys. We implement a complete prototype that encrypts and transmits audio data using our cryptosystem, and only when the receiver is within the authorized data, they are able to decrypt the data. Our evaluation demonstrates that the system (i) removes the need to share decryption passwords electronically or physically, (ii) ensures the decryption key cannot be recovered by the eavesdropper, and (iii) provides a non-trivial spatial tolerance for legitimate users.

</details>


### [42] [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045)
*Yule Liu,Heyi Zhang,Jinyi Zheng,Zhen Sun,Zifan Peng,Tianshuo Cong,Yilong Yang,Xinlei He,Zhuo Ma*

Main category: cs.CR

TL;DR: 提出了DIBA攻击框架，专门针对RLVR训练的大语言模型进行成员推理攻击，通过行为变化而非记忆来推断训练数据，在多个场景下显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: RLVR训练范式引入独特的隐私泄露模式：由于训练依赖自生成响应且无固定真实输出，成员推理需要确定给定提示是否用于微调，这种威胁源于行为改变而非答案记忆。

Method: DIBA攻击框架将焦点从记忆转向行为变化，利用模型行为在两个轴上的可测量变化：优势侧改进（如正确性提升）和对数侧分歧（如策略漂移）。

Result: DIBA显著优于现有基线，达到约0.8的AUC和数量级更高的TPR@0.1%FPR，在多种设置下验证了其优越性，包括分布内、跨数据集、跨算法、黑盒场景以及扩展到视觉语言模型。

Conclusion: 这是首个系统分析RLVR隐私漏洞的工作，揭示了即使在缺乏显式监督的情况下，训练数据暴露仍可通过行为痕迹可靠推断。

Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

</details>


### [43] [Dynamic Black-box Backdoor Attacks on IoT Sensory Data](https://arxiv.org/abs/2511.14074)
*Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.CR

TL;DR: 提出了一种针对基于传感器数据的物联网系统的黑盒对抗攻击方法，使用动态触发器生成技术，在多种数据集和分类器模型上成功实施攻击，且对输入数据的扰动最小。


<details>
  <summary>Details</summary>
Motivation: 基于传感器数据的人体活动识别系统广泛应用于身份认证和活动识别，但深度学习模型存在安全风险，需要研究对抗攻击以评估系统安全性。

Method: 开发了一种新颖的动态触发器生成技术，用于对基于传感器数据的物联网系统进行黑盒对抗攻击，通过最小化输入数据扰动来实现攻击。

Result: 实证分析表明，该攻击方法在多种数据集和分类器模型上均能成功实施，同时与后门攻击中的其他投毒技术相比，在性能和隐蔽性方面具有优势。

Conclusion: 该研究不仅展示了对抗攻击的有效性，还讨论了对抗防御机制及其对触发器生成技术效果的影响，为物联网系统安全评估提供了重要参考。

Abstract: Sensor data-based recognition systems are widely used in various applications, such as gait-based authentication and human activity recognition (HAR). Modern wearable and smart devices feature various built-in Inertial Measurement Unit (IMU) sensors, and such sensor-based measurements can be fed to a machine learning-based model to train and classify human activities. While deep learning-based models have proven successful in classifying human activity and gestures, they pose various security risks. In our paper, we discuss a novel dynamic trigger-generation technique for performing black-box adversarial attacks on sensor data-based IoT systems. Our empirical analysis shows that the attack is successful on various datasets and classifier models with minimal perturbation on the input data. We also provide a detailed comparative analysis of performance and stealthiness to various other poisoning techniques found in backdoor attacks. We also discuss some adversarial defense mechanisms and their impact on the effectiveness of our trigger-generation technique.

</details>


### [44] [Resolving Availability and Run-time Integrity Conflicts in Real-Time Embedded Systems](https://arxiv.org/abs/2511.14088)
*Adam Caulfield,Muhammad Wasif Kamran,N. Asokan*

Main category: cs.CR

TL;DR: PAIR提出了一种在实时系统中平衡安全性和可用性的运行时完整性执行方法，通过硬件监控实时任务并在检测到违规时仅终止违规任务，同时允许安全任务继续执行。


<details>
  <summary>Details</summary>
Motivation: 现有实时系统在运行时完整性执行上面临安全性与可用性的根本冲突：要么优先可用性允许受感染系统继续运行，要么优先安全性中止所有执行。需要找到这两种极端之间的折中方案。

Method: PAIR通过硬件方法监控实时任务的运行时完整性违规，维护一个安全任务可用区域(AR)。当任务引发违规时，触发不可屏蔽中断杀死违规任务，同时继续执行AR内的非违规任务。

Result: PAIR方法对执行任务不产生任何运行时开销，可与实时操作系统集成，在内存和硬件使用上仅产生+2.3%的开销，适合低端微控制器单元。

Conclusion: PAIR在实时系统的安全性和可用性之间提供了中间地带，确保只有违规任务被阻止执行，同时为剩余任务提供可用性保障。

Abstract: Run-time integrity enforcement in real-time systems presents a fundamental conflict with availability. Existing approaches in real- time systems primarily focus on minimizing the execution-time overhead of monitoring. After a violation is detected, prior works face a trade-off: (1) prioritize availability and allow a compromised system to continue to ensure applications meet their deadlines, or (2) prioritize security by generating a fault to abort all execution. In this work, we propose PAIR, an approach that offers a middle ground between the stark extremes of this trade-off. PAIR monitors real-time tasks for run-time integrity violations and maintains an Availability Region (AR) of all tasks that are safe to continue. When a task causes a violation, PAIR triggers a non-maskable interrupt to kill the task and continue executing a non-violating task within AR. Thus, PAIR ensures only violating tasks are prevented from execution, while granting availability to remaining tasks. With its hardware approach, PAIR does not cause any run-time overhead to the executing tasks, integrates with real-time operating systems (RTOSs), and is affordable to low-end microcontroller units (MCUs) by incurring +2.3% overhead in memory and hardware usage.

</details>


### [45] [Beyond Fixed and Dynamic Prompts: Embedded Jailbreak Templates for Advancing LLM Security](https://arxiv.org/abs/2511.14140)
*Hajun Kim,Hyunsik Na,Daeseon Choi*

Main category: cs.CR

TL;DR: 本文提出嵌入式越狱模板，在保持现有模板结构的同时自然嵌入有害查询，通过渐进式提示工程确保模板质量和一致性，为红队测试和政策回归测试提供更准确的基准。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，确保其安全性和鲁棒性成为关键挑战。当前的越狱攻击防御方法主要依赖两种有限策略：将有害查询替换到固定模板中，或让LLM生成整个模板，但这往往损害意图清晰度和可复现性。

Method: 引入嵌入式越狱模板，在现有模板结构中自然嵌入有害查询；提出渐进式提示工程方法确保模板质量和一致性；制定标准化的生成和评估协议。

Result: 提出的方法提供了一个更准确反映现实使用场景和有害意图的基准，便于在红队测试和政策回归测试中应用。

Conclusion: 嵌入式越狱模板和渐进式提示工程方法有效解决了当前越狱攻击防御方法的局限性，为提升LLM安全性提供了更实用的工具和评估框架。

Abstract: As the use of large language models (LLMs) continues to expand, ensuring their safety and robustness has become a critical challenge. In particular, jailbreak attacks that bypass built-in safety mechanisms are increasingly recognized as a tangible threat across industries, driving the need for diverse templates to support red-teaming efforts and strengthen defensive techniques. However, current approaches predominantly rely on two limited strategies: (i) substituting harmful queries into fixed templates, and (ii) having the LLM generate entire templates, which often compromises intent clarity and reproductibility. To address this gap, this paper introduces the Embedded Jailbreak Template, which preserves the structure of existing templates while naturally embedding harmful queries within their context. We further propose a progressive prompt-engineering methodology to ensure template quality and consistency, alongside standardized protocols for generation and evaluation. Together, these contributions provide a benchmark that more accurately reflects real-world usage scenarios and harmful intent, facilitating its application in red-teaming and policy regression testing.

</details>


### [46] [Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion](https://arxiv.org/abs/2511.14301)
*Eric Xue,Ruiyi Zhang,Zijun Zhang,Pengtao Xie*

Main category: cs.CR

TL;DR: SteganoBackdoor是一种新型的语义后门攻击方法，利用自然语言隐写术将语义触发器转化为隐写载体，在极低的数据投毒率下实现高攻击成功率，并能有效规避现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 当前后门攻击研究过于关注风格化或令牌级扰动触发器，忽视了更现实和危险的语义触发器攻击，这种攻击可能针对真实人物或事件操纵模型输出。

Method: 利用自然语言隐写术的无害特性，通过梯度引导的数据优化过程将语义触发种子转化为隐写载体，这些载体嵌入高后门载荷、保持流畅性且与触发器无表征相似性。

Result: 在多样化实验设置中，SteganoBackdoor以比先前方法低一个数量级的数据投毒率实现了超过99%的攻击成功率，同时在一系列数据级防御中保持无与伦比的规避能力。

Conclusion: SteganoBackdoor揭示了当前防御机制中的严重盲点，迫切需要关注对抗性数据防御和现实世界威胁建模。

Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

</details>


### [47] [Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection](https://arxiv.org/abs/2511.14422)
*Zhengchunmin Dai,Jiaxiong Tang,Peng Sun,Honglong Chen,Liantao Wu*

Main category: cs.CR

TL;DR: Sigil是一个专为能力受限服务器设计的强制水印框架，通过在服务器可见的激活空间施加统计约束来嵌入水印，使用梯度注入技术而无需了解数据，有效保护服务器模型不被恶意客户端窃取。


<details>
  <summary>Details</summary>
Motivation: 在Split Federated Learning等去中心化机器学习范式中，服务器能力受限虽然增强了客户端隐私，但也使服务器容易受到恶意客户端的模型窃取攻击。传统水印方案在对抗性环境中不可靠或技术上不可行。

Method: Sigil将水印定义为服务器可见激活空间的统计约束，通过梯度注入将水印嵌入客户端模型，无需数据知识。设计了自适应梯度裁剪机制确保水印过程的强制性和隐蔽性。

Result: 在多个数据集和模型上的广泛实验表明，Sigil具有良好的保真度、鲁棒性和隐蔽性，能有效对抗现有的梯度异常检测方法和专门设计的自适应子空间移除攻击。

Conclusion: Sigil为能力受限服务器提供了一种有效的知识产权保护方案，解决了在对抗性环境中服务器模型易被窃取的问题，同时保持水印的强制性和隐蔽性。

Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.

</details>


### [48] [SecureSign: Bridging Security and UX in Mobile Web3 through Emulated EIP-6963 Sandboxing](https://arxiv.org/abs/2511.14611)
*Charles Cheng Ji,Brandon Kong*

Main category: cs.CR

TL;DR: SecureSign是一个基于PWA的移动Web3安全架构，通过EIP-6963提供程序沙箱化技术，解决了移动Web3钱包在安全性和用户体验之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 移动Web3面临灾难性的用户留存率（<5%），导致每个留存用户的获客成本高达500-1000美元。现有解决方案存在安全性和用户体验的不可调和矛盾：嵌入式钱包有适度可用性但存在点击劫持漏洞；应用钱包保持安全性但下载摩擦和上下文切换导致留存率仅2-3%。

Method: SecureSign采用PWA架构，通过EIP-6963提供程序沙箱化技术，将桌面浏览器扩展安全机制适配到移动端。它在可信父应用中通过iframe隔离dApp执行，实现点击劫持免疫和交易完整性，同时支持原生移动功能（推送通知、主屏幕安装、零上下文切换）。

Result: 威胁模型分析证明SecureSign对点击劫持、覆盖攻击和窃取攻击具有免疫力，同时保持钱包在dApp间的互操作性。该SDK无需对现有Web3应用进行代码库更改即可直接使用。

Conclusion: SecureSign成功解决了移动Web3在安全性和用户体验之间的根本矛盾，提供了一种既安全又用户友好的移动Web3钱包解决方案。

Abstract: Mobile Web3 faces catastrophic retention (< 5%) yielding effective acquisition costs of \$500 - \$1,000 per retained user. Existing solutions force an impossible tradeoff: embedded wallets achieve moderate usability but suffer inherent click-jacking vulnerabilities; app wallets maintain security at the cost of 2 - 3% retention due to download friction and context-switching penalties. We present SecureSign, a PWA-based architecture that adapts desktop browser extension security to mobile via EIP-6963 provider sandboxing. SecureSign isolates dApp execution in iframes within a trusted parent application, achieving click-jacking immunity and transaction integrity while enabling native mobile capabilities (push notifications, home screen installation, zero context-switching). Our drop-in SDK requires no codebase changes for existing Web3 applications. Threat model analysis demonstrates immunity to click-jacking, overlay, and skimming attacks while maintaining wallet interoperability across dApps.

</details>
