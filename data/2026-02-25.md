<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The TCF doesn't really A(A)ID -- Automatic Privacy Analysis and Legal Compliance of TCF-based Android Applications](https://arxiv.org/abs/2602.20222)
*Victor Morel,Cristiana Santos,Pontus Carlsson,Joel Ahlinder,Romaric Duvignau*

Main category: cs.CR

TL;DR: 研究发现Android应用中TCF框架实施存在隐私违规：12.85%应用使用TCF，其中2.6%仅在用户同意时存储选择，拒绝后每次启动都重新显示横幅；66.2%在无合法依据情况下分享AAID，55.3%在用户与横幅交互前就分享AAID。


<details>
  <summary>Details</summary>
Motivation: TCF作为欧洲用户同意的实际标准，先前研究仅关注网页环境，缺乏对Android应用实施情况的系统研究，需要填补这一空白以评估移动应用中的隐私影响。

Method: 从Google Play Store下载4482个热门应用，在模拟Android设备上自动识别使用TCF的应用，自动与同意横幅交互，并在两个阶段分析应用流量：被动阶段（选择后）和主动阶段（横幅交互期间和选择后）。

Result: 576个应用（12.85%）实施了TCF，其中15个应用（2.6%）仅在用户同意时存储选择；被动阶段分析显示66.2%的TCF应用在无合法依据情况下分享AAID；主动阶段分析显示55.3%的应用在用户与横幅交互前就分享AAID。

Conclusion: TCF在Android应用中的实施存在系统性隐私违规，包括缺乏合法依据的数据分享和违反事先同意要求，表明当前框架在移动环境中的有效性有限，需要监管干预和技术改进。

Abstract: The Transparency and Consent Framework (TCF), developed by the Interactive Advertising Bureau (IAB) Europe, provides a de facto standard for requesting, recording, and managing user consent from European end-users. This framework has previously been found to infringe European data protection law and has subsequently been regularly updated. Previous research on the TCF focused exclusively on web contexts, with no attention given to its implementation in mobile applications. No work has systematically studied the privacy implications of the TCF on Android apps. To address this gap, we investigate the prevalence of the TCF in popular Android apps from the Google Play Store, and assess whether these apps respect users' consent banner choices. By scraping and downloading 4482 of the most popular Google Play Store apps on an emulated Android device, we automatically determine which apps use the TCF, automatically interact with consent banners, and analyze the apps' traffic in two different stages, passive (post choices) and active (during banner interaction and post choices).
  We found that 576 (12.85%) of the 4482 downloadable apps in our dataset implemented the TCF, and we identified potential privacy violations within this subset. In 15 (2.6%) of these apps, users' choices are stored only when consent is granted. Users who refuse consent are shown the consent banner again each time they launch the app. Network traffic analysis conducted during the passive stage reveals that 66.2% of the analyzed TCF-based apps share personal data, through the Android Advertising ID (AAID), in the absence of a lawful basis for processing. 55.3% of apps analyzed during the active stage share AAID before users interact with the apps' consent banners, violating the prior consent requirement.

</details>


### [2] [OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services](https://arxiv.org/abs/2602.20595)
*Longxiang Wang,Xiang Zheng,Xuhao Zhang,Yao Zhang,Ye Wu,Cong Wang*

Main category: cs.CR

TL;DR: OptiLeak：基于强化学习的提示泄漏攻击框架，通过两阶段微调最大化提示重建效率，相比基线方法减少12.48倍的平均请求数，揭示缓存侧信道攻击的实际威胁比之前认知更严重。


<details>
  <summary>Details</summary>
Motivation: 多租户LLM服务框架广泛采用共享键值缓存提升效率，但这带来了侧信道漏洞导致提示泄漏攻击。先前研究虽然识别了攻击面，但主要关注扩展攻击向量而非优化攻击性能，报告的攻击成本过高，低估了实际隐私风险。

Method: 提出OptiLeak强化学习增强框架，通过两阶段微调最大化提示重建效率。核心洞见是通过似然排序自动识别领域特定的"硬令牌"（难以预测但携带敏感信息的术语），并构建偏好对用于直接偏好优化，无需人工标注。这实现了有效的偏好对齐，同时避免了扩展监督微调的过拟合问题。

Result: 在医疗和金融领域的三个基准测试中，OptiLeak相比基线方法实现了高达12.48倍的平均每令牌请求数减少，在3B到14B参数规模的模型上均表现一致改进。

Conclusion: 基于缓存的提示泄漏攻击威胁比先前报告更为严重，突显了在生产部署中需要强大的缓存隔离机制的必要性。

Abstract: Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.

</details>


### [3] [Post-Quantum Sanitizable Signatures from McEliece-Based Chameleon Hashing](https://arxiv.org/abs/2602.20657)
*Shahzad Ahmad,Stefan Rass,Zahra Seyedi*

Main category: cs.CR

TL;DR: 基于McEliece密码系统变色龙哈希函数构建的后量子可净化签名方案，首次实现了透明的基于编码的后量子可净化签名


<details>
  <summary>Details</summary>
Motivation: 设计一个后量子安全的可净化签名方案，解决量子计算时代传统签名方案的安全性问题，同时提供可控的消息修改功能

Method: 基于McEliece密码系统的变色龙哈希函数构建，指定净化者拥有Goppa码的陷门，通过Patterson解码实现受控碰撞查找，采用随机预言机模型和特定权重约束实现完美透明性

Result: 提出了首个透明的基于编码的后量子可净化签名方案，提供了形式化的安全定义和严格的存在不可伪造性和不可变性证明

Conclusion: 该工作建立了首个透明的基于编码的后量子可净化签名方案，为长期安全应用提供了强大的理论保证和实际部署路径

Abstract: We introduce a novel post-quantum sanitizable signature scheme constructed upon a chameleon hash function derived from the McEliece cryptosystem. In this design, the designated sanitizer possesses the inherent trapdoor of a Goppa code, which facilitates controlled collision-finding via Patterson decoding. This mechanism enables authorized modification of specific message blocks while ensuring all other content remains immutably bound. We provide formal security definitions and rigorous proofs of existential unforgeability and immutability, grounded in the hardness of syndrome decoding in the random-oracle model, where a robust random oracle thwarts trivial linear hash collisions. A key innovation lies in our precise characterization of the transparency property: by imposing a specific weight constraint on the randomizers generated by the signer, we achieve perfect transparency, rendering sanitized signatures indistinguishable from freshly signed ones. This work establishes the first transparent, code-based, post-quantum sanitizable signature scheme, offering strong theoretical guarantees and a pathway for practical deployment in long-term secure applications.

</details>


### [4] [Vanishing Watermarks: Diffusion-Based Image Editing Undermines Robust Invisible Watermarking](https://arxiv.org/abs/2602.20680)
*Fan Guo,Jiyu Kang,Qi Ming,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: 扩散模型能够有效擦除传统鲁棒水印，即使这些水印原本设计用于抵抗常规图像处理操作


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型等强大图像生成和编辑技术的出现，传统鲁棒水印面临新的威胁，需要评估这些水印在扩散模型攻击下的脆弱性

Method: 提出扩散驱动的图像再生过程和引导扩散攻击，理论上证明扩散变换会使水印图像与隐藏载荷之间的互信息趋近于零，实验评估多种先进水印方法

Result: 扩散编辑攻击导致水印恢复率接近零，同时保持再生图像的高视觉保真度，揭示了当前鲁棒水印技术对基于生成模型的编辑存在根本性脆弱性

Conclusion: 当前鲁棒水印技术在强大扩散模型时代存在根本性漏洞，需要新的策略来确保水印的弹性

Abstract: Robust invisible watermarking schemes aim to embed hidden information into images such that the watermark survives common manipulations. However, powerful diffusion-based image generation and editing techniques now pose a new threat to these watermarks. In this paper, we present a comprehensive theoretical and empirical analysis demonstrating that diffusion models can effectively erase robust watermarks even when those watermarks were designed to withstand conventional distortions. We show that a diffusion-driven image regeneration process, which leverages generative models to recreate an image, can remove embedded watermarks while preserving the image's perceptual content. Furthermore, we introduce a guided diffusion-based attack that explicitly targets the embedded watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion transformations, the mutual information between the watermarked image and the hidden payload approaches zero, leading to inevitable decoding failure. Experimentally, we evaluate multiple state-of-the-art watermarking methods (including deep learning-based schemes like StegaStamp, TrustMark, and VINE) and demonstrate that diffusion edits yield near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings reveal a fundamental vulnerability in current robust watermarking techniques against generative model-based edits, underscoring the need for new strategies to ensure watermark resilience in the era of powerful diffusion models.

</details>


### [5] [A Secure and Interoperable Architecture for Electronic Health Record Access Control and Sharing](https://arxiv.org/abs/2602.20830)
*Tayeb Kenaza,Islam Debicha,Youcef Fares,Mehdi Sehaki,Sami Messai*

Main category: cs.CR

TL;DR: 提出基于区块链和IPFS的电子健康记录安全访问控制与共享架构，通过私有区块链和智能合约赋予患者对自身医疗数据的完全控制权，确保符合GDPR等隐私法规。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含敏感患者信息，需要严格的访问控制和共享机制来保障数据安全并符合GDPR等隐私法规。现有系统在患者数据控制权、安全性和互操作性方面存在不足。

Method: 提出综合架构，结合区块链和IPFS技术，使用私有区块链部署智能合约，让患者拥有对EHR的专属控制权。架构能与现有医疗信息系统无缝集成，解决安全性和数据异构性挑战。

Result: 基于Hyperledger平台开发原型系统，模拟多种医疗数据访问控制和共享场景。实验结果表明该解决方案具有良好的可扩展性，在实际医疗环境中具有效性和鲁棒性。

Conclusion: 该架构通过区块链和IPFS技术的协同作用，实现了安全的EHR访问控制和共享，赋予患者数据控制权，符合隐私法规要求，并能与现有医疗系统集成，具有实际应用价值。

Abstract: Electronic Health Records (EHRs) store sensitive patient information, necessitating stringent access control and sharing mechanisms to uphold data security and comply with privacy regulations such as the General Data Protection Regulation (GDPR). In this paper, we propose a comprehensive architecture with a suite of efficient protocols that leverage the synergistic capabilities of the Blockchain and Interplanetary File System (IPFS) technologies to enable secure access control and sharing of EHRs. Our approach is based on a private blockchain, wherein smart contracts are deployed to enforce control exclusively by patients. By granting patients exclusive control over their EHRs, our solution ensures compliance with personal data protection laws and empowers individuals to manage their health information autonomously. Notably, our proposed architecture seamlessly integrates with existing health provider information systems, facilitating interoperability and effectively addressing security and data heterogeneity challenges. To demonstrate the effectiveness of our approach, we developed a prototype based on a private implementation of the Hyperledger platform, enabling the simulation of diverse scenarios involving access control and health data sharing among healthcare practitioners. Our experimental results demonstrate the scalability of our solution, thereby substantiating its efficacy and robustness in real-world healthcare settings.

</details>


### [6] [SoK: Agentic Skills -- Beyond Tool Use in LLM Agents](https://arxiv.org/abs/2602.20867)
*Yanna Jiang,Delong Li,Haiyu Deng,Baihe Ma,Xu Wang,Qin Wang,Guangsheng Yu*

Main category: cs.CR

TL;DR: 论文提出了智能体技能层的完整生命周期框架，包括发现、实践、提炼、存储、组合、评估和更新，并引入了两种互补的分类法：七种系统级设计模式和表示×范围分类法，分析了基于技能的智能体的安全治理问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能体系统越来越依赖可重用的程序化能力（即智能体技能）来可靠执行长时程工作流，需要系统地理解技能层的完整生命周期、设计模式、表示形式以及相关的安全治理挑战。

Method: 论文通过映射技能层的完整生命周期，引入了两种互补的分类法：1）七种系统级设计模式，描述技能如何在实际中被封装和执行；2）表示×范围分类法，描述技能的本质（自然语言、代码、策略、混合）及其操作环境（Web、操作系统、软件工程、机器人）。同时分析了基于技能的智能体的安全治理影响。

Result: 研究发现恶意技能可能大规模渗透智能体市场（如ClawHavoc活动中近1200个恶意技能案例），窃取API密钥、加密货币钱包和浏览器凭证。基准测试证据表明，精心设计的技能能显著提高智能体成功率，而自生成的技能可能降低成功率。

Conclusion: 论文提出了面向现实世界自主智能体的鲁棒、可验证和可认证技能的开源挑战，强调了技能层在智能体系统中的重要性及其安全治理的必要性。

Abstract: Agentic systems increasingly rely on reusable procedural capabilities, \textit{a.k.a., agentic skills}, to execute long-horizon workflows reliably. These capabilities are callable modules that package procedural knowledge with explicit applicability conditions, execution policies, termination criteria, and reusable interfaces. Unlike one-off plans or atomic tool calls, skills operate (and often do well) across tasks.
  This paper maps the skill layer across the full lifecycle (discovery, practice, distillation, storage, composition, evaluation, and update) and introduces two complementary taxonomies. The first is a system-level set of \textbf{seven design patterns} capturing how skills are packaged and executed in practice, from metadata-driven progressive disclosure and executable code skills to self-evolving libraries and marketplace distribution. The second is an orthogonal \textbf{representation $\times$ scope} taxonomy describing what skills \emph{are} (natural language, code, policy, hybrid) and what environments they operate over (web, OS, software engineering, robotics).
  We analyze the security and governance implications of skill-based agents, covering supply-chain risks, prompt injection via skill payloads, and trust-tiered execution, grounded by a case study of the ClawHavoc campaign in which nearly 1{,}200 malicious skills infiltrated a major agent marketplace, exfiltrating API keys, cryptocurrency wallets, and browser credentials at scale. We further survey deterministic evaluation approaches, anchored by recent benchmark evidence that curated skills can substantially improve agent success rates while self-generated skills may degrade them. We conclude with open challenges toward robust, verifiable, and certifiable skills for real-world autonomous agents.

</details>


### [7] [When Backdoors Go Beyond Triggers: Semantic Drift in Diffusion Models Under Encoder Attacks](https://arxiv.org/abs/2602.20193)
*Shenyang Chen,Liuwan Zhu*

Main category: cs.CR

TL;DR: 该研究挑战了文本到图像模型后门攻击的现有评估范式，发现编码器侧投毒会导致持久的、无需触发的语义损坏，从根本上重塑表示流形，揭示了后门攻击的深层结构风险。


<details>
  <summary>Details</summary>
Motivation: 现有对文本到图像模型后门攻击的评估主要关注触发激活和视觉保真度，但作者认为这种范式存在局限，需要揭示编码器投毒导致的更深层次语义损坏和结构风险。

Method: 通过雅可比矩阵分析揭示后门作为低秩、目标中心形变的几何机制；引入SEMAD（语义对齐和漂移）诊断框架，测量内部嵌入漂移和下游功能错配；在扩散和对比学习范式上进行验证。

Result: 发现后门攻击会放大局部敏感性，导致失真在语义邻域内连贯传播；编码器投毒会导致持久的、无需触发的语义损坏；SEMAD框架能有效量化这种结构退化。

Conclusion: 编码器投毒存在深层结构风险，需要超越简单攻击成功率的几何审计；SEMAD框架为评估后门攻击的结构影响提供了新工具，强调了全面安全评估的重要性。

Abstract: Standard evaluations of backdoor attacks on text-to-image (T2I) models primarily measure trigger activation and visual fidelity. We challenge this paradigm, demonstrating that encoder-side poisoning induces persistent, trigger-free semantic corruption that fundamentally reshapes the representation manifold. We trace this vulnerability to a geometric mechanism: a Jacobian-based analysis reveals that backdoors act as low-rank, target-centered deformations that amplify local sensitivity, causing distortion to propagate coherently across semantic neighborhoods. To rigorously quantify this structural degradation, we introduce SEMAD (Semantic Alignment and Drift), a diagnostic framework that measures both internal embedding drift and downstream functional misalignment. Our findings, validated across diffusion and contrastive paradigms, expose the deep structural risks of encoder poisoning and highlight the necessity of geometric audits beyond simple attack success rates.

</details>


### [8] [Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study](https://arxiv.org/abs/2602.20202)
*Jeel Piyushkumar Khatiwala,Daniel Kwaku Ntiamoah Addai,Weifeng Xu*

Main category: cs.CR

TL;DR: 提出一个结合大语言模型与数字取证知识图谱的结构化框架，用于自动化提取、精炼和验证AI识别的数字证据，确保证据可靠性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 随着AI识别的数字证据在取证调查中日益重要，其可靠性问题引发关注，特别是大语言模型在取证中的集成应用需要确保证据的可信度和法证完整性。

Method: 开发结构化框架：1) 自动化取证工件提取；2) 通过LLM驱动分析精炼数据；3) 使用数字取证知识图谱(DFKG)验证结果；4) 采用确定性唯一标识符(UIDs)和取证交叉引用确保可追溯性。

Result: 在包含13GB取证镜像、61个应用、2,864个数据库和5,870个表的数据集上评估，实现超过95%的工件提取准确率，强有力地支持证据链维护，并在取证关系上保持稳健的上下文一致性。

Conclusion: 该框架能有效提升AI辅助数字取证的可靠性，减少分类错误，建立法律上可靠的方法论，为AI识别证据的可信度和法证完整性提供了可扩展、可审计的解决方案。

Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, refines data through LLM-driven analysis, and validates results using a Digital Forensic Knowledge Graph (DFKG). Evaluated on a 13 GB forensic image dataset containing 61 applications, 2,864 databases, and 5,870 tables, the framework ensures artifact traceability and evidentiary consistency through deterministic Unique Identifiers (UIDs) and forensic cross-referencing. We propose this methodology to address challenges in ensuring the credibility and forensic integrity of AI-identified evidence, reducing classification errors, and advancing scalable, auditable methodologies. A comprehensive case study on this dataset demonstrates the framework's effectiveness, achieving over 95 percent accuracy in artifact extraction, strong support of chain-of-custody adherence, and robust contextual consistency in forensic relationships. Key results validate the framework's ability to enhance reliability, reduce errors, and establish a legally sound paradigm for AI-assisted digital forensics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health](https://arxiv.org/abs/2602.20303)
*Joyanta Jyoti Mondal*

Main category: cs.AI

TL;DR: 该研究比较了统计、机器学习和深度学习模型在预测美国青少年超重肥胖方面的表现，发现逻辑回归、梯度提升和多层感知器表现最稳定，模型复杂度增加带来的改进有限，且所有模型在不同种族和贫困群体间都存在性能差异。


<details>
  <summary>Details</summary>
Motivation: 儿童和青少年超重肥胖是美国主要的公共卫生问题，受行为、家庭和社区因素共同影响。目前对这些多层次预测因素在人群层面的联合预测结构了解不足，需要比较不同预测模型的性能、校准和亚组公平性。

Method: 研究分析了2021年全国儿童健康调查中18,792名10-17岁儿童的数据。使用BMI分类定义超重/肥胖。预测因素包括饮食、体育活动、睡眠、父母压力、社会经济条件、不良经历和社区特征。比较了逻辑回归、随机森林、梯度提升、XGBoost、LightGBM、多层感知器和TabNet等模型，使用AUC、准确率、精确率、召回率、F1分数和Brier分数进行评估。

Result: 模型的区分度范围为0.66-0.79。逻辑回归、梯度提升和多层感知器在区分度和校准方面表现最稳定。提升方法和深度学习略微提高了召回率和F1分数。没有模型在所有方面都表现最优。所有算法在不同种族和贫困群体间都存在性能差异。

Conclusion: 模型复杂度增加相对于逻辑回归带来的改进有限。预测因素始终涵盖行为、家庭和社区领域。持续的亚组差异表明需要改进数据质量和关注公平性的监测，而不是追求更高的算法复杂度。

Abstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.

</details>


### [10] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: DMCD是一个两阶段因果发现框架，结合了基于LLM的语义草图和统计验证，在真实世界基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的因果发现方法通常要么依赖纯统计方法，要么依赖语义信息。作者希望结合两者的优势，利用变量元数据的语义信息来指导因果结构学习，同时通过统计验证确保结果的可靠性。

Method: DMCD采用两阶段方法：第一阶段使用大语言模型基于变量元数据生成稀疏的因果图草案，作为语义先验；第二阶段通过条件独立性测试对草案进行审计和精炼，检测到的差异指导有针对性的边修正。

Result: 在工业工程、环境监测和IT系统分析三个元数据丰富的真实世界基准测试中，DMCD相比多种因果发现基线方法取得了竞争性或领先的性能，特别是在召回率和F1分数上有显著提升。消融实验表明这些改进源于对元数据的语义推理而非对基准图的记忆。

Conclusion: 将语义先验与原则性统计验证相结合，可以产生高性能且实际有效的因果结构学习方法，DMCD框架展示了这种结合的优势。

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [11] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: DMEMM是一种新的基于扩散模型的规划方法，通过建模环境机制（转移动态和奖励函数）来调制扩散模型训练，解决了传统扩散规划方法在RL中轨迹生成的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散模型的规划方法在离线强化学习中生成轨迹时，往往忽略了RL环境对状态转移一致性的特殊要求，导致生成的轨迹与真实环境机制存在显著差异。

Method: 提出DMEMM方法，在扩散模型训练中融入关键RL环境机制，特别是转移动态和奖励函数，通过环境机制建模来调制扩散过程。

Result: 实验结果表明，DMEMM在离线强化学习的规划任务中达到了最先进的性能。

Conclusion: 通过建模环境机制来调制扩散模型训练，可以有效提升基于扩散模型的离线RL规划方法的性能，确保生成的轨迹与真实环境机制的一致性。

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [12] [PreScience: A Benchmark for Forecasting Scientific Contributions](https://arxiv.org/abs/2602.20459)
*Anirudh Ajith,Amanpreet Singh,Jay DeYoung,Nadav Kunievsky,Austin C. Kozlowski,Oyvind Tafjord,James Evans,Daniel S. Weld,Tom Hope,Doug Downey*

Main category: cs.AI

TL;DR: 论文提出了PreScience基准测试，用于评估AI系统基于历史科学记录预测未来科学进展的能力，包含合作者预测、先前工作选择、贡献生成和影响力预测四个任务，发现前沿LLM在这些任务上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统能否基于特定时间点前的科学记录预测后续科学进展，这种能力可以帮助研究人员识别合作者、发现高影响力研究方向，并预测未来将成为核心的问题和方法。

Method: 构建了PreScience基准测试，包含98K篇近期AI相关研究论文的数据集，具有消歧的作者身份、时间对齐的学术元数据，以及包含502K篇论文的作者发表历史和引用关系的结构化图。开发了四个任务的基准和评估方法，包括新的LLM-based贡献相似度度量LACERScore。

Result: 前沿LLM在各项任务上仍有很大提升空间：在贡献生成任务中，GPT-5平均得分仅为5.6分（1-10分制）。当组合成12个月的端到端科学生产模拟时，生成的合成语料库在多样性和新颖性方面系统性低于同期人类撰写的研究。

Conclusion: PreScience基准测试为评估AI系统预测科学进展的能力提供了系统框架，当前AI系统在科学预测方面仍有显著局限性，特别是在生成多样化和新颖的科学贡献方面。

Abstract: Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.

</details>


### [13] [KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning](https://arxiv.org/abs/2602.20494)
*Haotian Si,Changhua Pei,Xiao He,Zeyan Li,Zhe Xie,Zexin Wang,Jiyao Hu,Zhaoyang Yu,Tieying Zhang,Dan Pei,Jianhui Li,Gaogang Xie*

Main category: cs.AI

TL;DR: 提出语义条件时间序列推理任务和两轮强化学习框架KairosVL，将语义理解与时间序列分析结合，提升复杂决策能力


<details>
  <summary>Details</summary>
Motivation: 针对时间序列分析日益复杂和决策导向的需求，需要超越纯数值建模，融入上下文和语义理解，以应对现实世界对时间序列智能的迫切需求

Method: 提出语义条件时间序列推理任务，采用两轮强化学习框架：第一轮增强模型对基本时间原语的感知能力，第二轮专注于语义条件推理，最终构建KairosVL模型

Result: KairosVL在合成和现实任务中均取得有竞争力的性能，实验表明该框架不仅提升性能，还能保持内在推理能力，并显著提高对未见场景的泛化能力

Conclusion: 该工作展示了语义推理与时间建模结合的潜力，为现实世界时间序列智能提供了实用框架，满足了当前对复杂时间序列分析的迫切需求

Abstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.

</details>


### [14] [ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory](https://arxiv.org/abs/2602.20502)
*Hongbin Zhong,Fazle Faisal,Luis França,Tanakorn Leesatapornwongsa,Adriana Szekeres,Kexin Rong,Suman Nath*

Main category: cs.AI

TL;DR: ActionEngine提出了一种免训练框架，通过双智能体架构将GUI交互从逐步反应式执行转变为程序化规划，显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体通过逐步调用视觉语言模型进行操作，存在成本高、延迟大、准确性有限的问题，且缺乏对已访问页面的持久记忆。

Method: 采用双智能体架构：爬虫智能体通过离线探索构建可更新的状态机记忆；执行智能体利用该记忆合成完整的可执行Python程序。执行失败时触发基于视觉的重定位回退机制来修复失败动作并更新记忆。

Result: 在WebArena基准的Reddit任务上，智能体达到95%的任务成功率，平均仅需一次LLM调用，相比最强视觉基线（66%）成本降低11.8倍，端到端延迟减少2倍。

Conclusion: 通过结合全局程序化规划、爬虫验证的动作模板以及节点级执行与局部验证修复，实现了可扩展且可靠的GUI交互。

Abstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.
  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.
  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.
  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.
  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.

</details>


### [15] [Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination](https://arxiv.org/abs/2602.20517)
*Rakshit Trivedi,Kartik Sharma,David C Parkes*

Main category: cs.AI

TL;DR: MIMIC框架利用语言作为行为意图的内部表征，通过视觉语言模型训练条件变分自编码器生成"内心独白"，再结合扩散行为克隆策略实现可操控的模仿学习，显著提升行为多样性和人类演示保真度。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法难以捕捉人类行为的多样性和非马尔可夫特性，且缺乏在推理时对行为进行精细操控的能力。受人类认知过程中内心独白引导行动选择的理论启发，需要开发能够生成和利用语言表征进行行为控制的框架。

Method: 提出MIMIC框架：1) 使用视觉语言模型作为语言支架训练条件变分自编码器，从观察中生成"内心独白"；2) 采用基于扩散的行为克隆策略，根据当前观察和生成的内心独白选择行动；3) 在推理时通过条件化特定行为语言实现对行为的精细操控。

Result: 在机器人操作任务和人机协作游戏中的实验表明，MIMIC显著提升了行为多样性和对人类演示的保真度，同时能够在无需额外演示训练的情况下实现细致的行为操控。

Conclusion: MIMIC框架通过将语言作为行为意图的内部表征，成功解决了传统模仿学习在行为多样性和可控性方面的局限性，为人机协调提供了更有效的解决方案。

Abstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>


### [16] [CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation](https://arxiv.org/abs/2602.20571)
*Ayush Sawarni,Jiyuan Tan,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: CausalReasoningBenchmark是一个包含173个查询、138个真实数据集的因果推理基准，要求系统同时输出结构化识别规范和点估计值，以区分因果推理错误和数值执行错误。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理基准通常只基于单一数值输出（如平均处理效应）评估系统性能，这混淆了因果分析中的两个关键步骤：识别（在给定假设下制定有效研究设计）和估计（在有限数据上数值实现该设计）。需要区分这两种不同类型的错误。

Method: 从85篇同行评审研究论文和4本广泛使用的因果推理教科书中收集数据，构建包含173个查询、138个真实数据集的基准。每个查询要求系统输出：(1)结构化识别规范，包括策略名称、处理变量、结果变量、控制变量及所有设计特定元素；(2)点估计值及其标准误。通过分别评分这两个组件实现细粒度诊断。

Result: 使用最先进的LLM进行基线测试显示，模型在84%的情况下能正确识别高级策略，但完整识别规范的正确率仅为30%。这表明瓶颈在于研究设计的细微细节而非计算能力。

Conclusion: CausalReasoningBenchmark公开可用，旨在促进更稳健的自动化因果推理系统的发展，通过区分因果推理错误和数值执行错误，提供更精细的系统评估。

Abstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.

</details>


### [17] [When can we trust untrusted monitoring? A safety case sketch across collusion strategies](https://arxiv.org/abs/2602.20628)
*Nelson Gardner-Challis,Jonathan Bostock,Georgiy Kozhevnikov,Morgan Sinclaire,Joan Velja,Alessandro Abate,Charlie Griffin*

Main category: cs.AI

TL;DR: 论文研究了不可信监控的安全性，通过放宽对AI共谋策略的假设，开发了更全面的共谋策略分类，并识别了被动自我识别可能比先前研究的策略更有效的情况。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主性和能力的增强，错位AI可能造成灾难性伤害的风险增加。不可信监控（用一个不可信模型监督另一个）是降低风险的方法之一，但证明其安全性具有挑战性，因为开发者无法安全部署错位模型直接测试协议。

Method: 基于已有的预部署测试方法，放宽先前AI控制研究中对错位AI可能用于破坏不可信监控的共谋策略的假设。开发了涵盖被动自我识别、因果共谋（隐藏预共享信号）、非因果共谋（通过谢林点隐藏信号）和组合策略的分类法。创建安全案例草图来清晰呈现论证，明确陈述假设，并突出未解决的挑战。

Result: 识别了被动自我识别可能比先前研究的共谋策略更有效的情况，为不可信监控的更稳健评估奠定了基础。

Conclusion: 该研究通过放宽对AI共谋策略的假设，开发了更全面的分类法，并识别了新的安全挑战，为不可信监控的安全性评估提供了更严谨的方法框架，有助于构建更强大的AI控制系统。

Abstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.

</details>


### [18] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: 提出一种同时向两位决策者提问以获取其偏好模型的方法，在不知道哪个回答对应哪个决策者的情况下，通过分段线性边际价值函数识别两个偏好模型


<details>
  <summary>Details</summary>
Motivation: 在同时向两位决策者提问时，虽然能获得两个回答，但不知道哪个回答对应哪个决策者，需要解决这种匿名回答情况下的偏好模型识别问题

Method: 提出一种启发程序，假设偏好可以用加性价值函数表示，边际价值函数为分段线性且断点已知，通过同时向两位决策者提问并处理匿名回答来识别各自的偏好模型

Result: 该方法能够在不知道回答对应关系的情况下，成功识别出两位决策者各自的偏好模型

Conclusion: 提出的启发程序有效解决了同时向多位决策者提问时的匿名回答问题，为多决策者偏好建模提供了新方法

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [19] [Grounding LLMs in Scientific Discovery via Embodied Actions](https://arxiv.org/abs/2602.20639)
*Bo Zhang,Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Minlie Huang,Hongning Wang*

Main category: cs.AI

TL;DR: EmbodiedAct框架将科学软件转化为具身智能体，通过紧密的感知-执行循环让LLMs在物理仿真中具备运行时感知能力，解决现有方法无法处理瞬态异常的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学发现中潜力显著，但难以弥合理论推理与可验证物理仿真之间的鸿沟。现有解决方案采用被动的"执行-响应"循环，缺乏运行时感知能力，导致智能体无法察觉数值不稳定或发散振荡等瞬态异常。

Method: 提出EmbodiedAct框架，将成熟的科学软件转化为主动的具身智能体，通过将LLMs与具身行动相结合，建立紧密的感知-执行循环。在MATLAB环境中实例化该框架。

Result: 在复杂的工程设计和科学建模任务上的广泛实验表明，EmbodiedAct显著优于现有基线方法，在长时程仿真中确保满意的可靠性和稳定性，在科学建模中提高准确性，达到最先进的性能水平。

Conclusion: EmbodiedAct通过将科学软件转化为具身智能体，成功解决了LLMs在物理仿真中缺乏运行时感知的问题，为科学发现中的智能体系统提供了更可靠和稳定的解决方案。

Abstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>


### [20] [Recursive Belief Vision Language Model](https://arxiv.org/abs/2602.20659)
*Vaidehi Bagaria,Bijo Sebastian,Nirav Patel*

Main category: cs.AI

TL;DR: RB-VLA：一种基于信念的视觉语言动作模型，通过自监督世界模型目标训练，维护紧凑的潜在状态表示，在部分可观测环境下实现长时程操作，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言动作模型在部分可观测环境下的长时程操作中存在困难，主要问题包括：依赖短上下文窗口或重复查询导致任务进度丢失、感知混淆下的动作重复、高推理延迟；缺乏持久且动作条件化的状态表示，时空和物理推理能力有限。

Method: 提出RB-VLA架构，采用信念中心设计，通过自监督世界模型目标训练，维护紧凑的潜在状态编码任务相关历史、动态和物体交互。VLM仅用于高层意图查询，信念模块跟踪任务进度，在部分可观测环境下实现阶段感知、因果基础的控制。信念和意图共同调节扩散策略进行闭环执行。

Result: 在长时程基准测试中显著优于现有VLA方法：多阶段拾取放置任务成功率提高52.5%，堆叠任务提高37.5%（相比π0）。推理延迟降低5倍，消除时间步内存增长。消融实验显示信念模块是性能主要驱动力，成功率从32.5%提升至77.5%。

Conclusion: 基于信念的状态表示对长时程VLA策略具有显著有效性，解决了现有方法在部分可观测环境下的核心限制，为长时程操作任务提供了更鲁棒和高效的解决方案。

Abstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>


### [21] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied是一个用于视觉语言模型驱动的具身智能体的挑战性基准测试，采用统一的原生低级动作空间，在复杂场景中评估高级任务和低级技能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM驱动的具身智能体基准测试通常依赖高级命令或离散化动作空间，这些非原生设置与现实世界控制差异显著。同时，当前基准主要关注高级任务，缺乏在低级别和高级别上的联合评估分析。

Method: 构建NativeEmbodied基准，基于多样化的模拟场景，包含三个代表性的高级任务来评估整体性能。同时解耦复杂任务所需的技能，构建四类低级任务，每类针对一个基础具身技能，实现任务和技能粒度的联合评估。

Result: 实验显示最先进的VLM在多个基础具身技能上存在明显缺陷，进一步分析表明这些瓶颈显著限制了高级任务的性能表现。

Conclusion: NativeEmbodied揭示了当前VLM驱动具身智能体的关键挑战，为未来研究提供了指导性见解。

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [22] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出PromptCD方法，通过构建正负引导提示对比模型响应，在测试时控制AI行为，无需额外训练即可提升LLMs和VLMs的性能


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法大多在训练时进行，需要高质量数据和大量计算成本。对比解码方法适用范围有限，需要一种更通用、成本更低的测试时行为控制方法

Method: 提出Polarity-Prompt Contrastive Decoding (PromptCD)：为目标行为构建正负引导提示对，对比模型响应（LLMs中的token级概率分布和VLMs中的视觉注意力模式），增强期望结果

Result: 在LLMs的"3H"对齐目标上取得一致且显著的改进；在VLMs中显著提升VQA性能，通过强化行为一致的视觉定位；证明后训练模型可在测试时实现有意义的自我增强

Conclusion: PromptCD是一种简单、通用且成本高效的可靠行为控制策略，适用于多模态场景，无需额外训练即可提升模型性能

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [23] [Online Algorithms with Unreliable Guidance](https://arxiv.org/abs/2602.20706)
*Julien Dallot,Yuval Emek,Yuval Gil,Maciej Pacut,Stefan Schmid*

Main category: cs.AI

TL;DR: 本文提出了一种名为"具有不可靠指导的在线算法"（OAG）的新模型，将预测组件与算法组件完全分离，通过请求-应答游戏框架分析在线决策问题，其中指导可能以概率β被对抗性破坏。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习增强的在线决策方法通常将预测和算法紧密耦合，缺乏统一的分析框架。OAG模型旨在提供一个独立于具体问题的通用框架，将预测作为来自问题答案空间的指导输入，允许系统性地分析算法在不同指导可靠性下的性能。

Method: 提出OAG模型框架，其中算法接收来自答案空间的指导（可能以概率β被对抗性破坏）。开发了"盲目信任或丢弃"（DTB）编译器，可将任何预测无关的在线算法转换为学习增强版本。DTB编译器基于偏置硬币投掷结果决定是盲目跟随指导还是忽略指导并执行原算法。

Result: 通过DTB编译器生成的算法在三个经典在线问题上表现出色：对于缓存和均匀度量任务系统，算法达到最优；对于二分图匹配（对抗性到达顺序），算法超越了现有最佳结果。证明了简单DTB编译器能产生具有吸引人的一致性-鲁棒性保证的算法。

Conclusion: OAG模型提供了一个统一框架来分离预测和算法组件，DTB编译器提供了一种系统方法将传统在线算法转换为学习增强版本。该框架自然地产生了平滑性概念，为机器学习增强的在线决策提供了新的理论分析工具。

Abstract: This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$.
  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.

</details>


### [24] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: 本文提出Counterfactual Simulation Training (CST)方法，通过奖励能在反事实输入下准确预测模型输出的思维链，来提升思维链的忠实性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是理解LLM输出的常用方法，但思维链的忠实性问题严重限制了从这种实践中获得的洞察。需要一种方法来改善思维链的忠实性。

Method: 引入Counterfactual Simulation Training (CST)训练方法，通过奖励那些能使模拟器在反事实输入下准确预测模型输出的思维链来提升忠实性。应用在两个场景：1) 基于线索的反事实进行思维链监控；2) 基于通用模型的反事实进行模拟。

Result: 在高达235B参数的模型上实验显示，CST能显著提升基于线索反事实的监控准确率（提升35个准确点）和通用反事实的可模拟性（提升2个点）。CST优于提示基线，用LLM重写不忠实思维链比单独使用RL效率高5倍。

Conclusion: CST能普遍提升思维链的忠实性，在思维链监控方面有前景应用。更大模型不会天生具有更忠实的思维链，但能从CST中获益更多。

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [25] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: BAPO是一种用于大语言模型后训练的离策略强化学习框架，通过动态选择训练批次、重新评估历史困难样本和重用高质量样本来提高数据效率，相比GRPO在多个任务上平均提升12.5%。


<details>
  <summary>Details</summary>
Motivation: 传统基于策略的RLVR框架存在经验浪费和奖励同质化问题，这直接影响了在大语言模型后训练中对困难样本的学习效率。

Method: 提出Batch Adaptation Policy Optimization (BAPO)，这是一个离策略RLVR框架，通过动态选择训练批次、重新评估历史困难样本和重用高质量样本，同时保持策略改进的下界保证。

Result: BAPO在数学、规划和视觉推理任务上相比GRPO平均提升12.5%，成功解决了40.7%的基础模型一直无法解决的问题。

Conclusion: BAPO通过提高数据效率和有效处理困难样本，显著改善了大语言模型后训练中的强化学习效果。

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [26] [Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2602.20728)
*Chenyang Zhao,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: 该论文探索将基于AI反馈的强化学习（RLAIF）扩展到多目标自适应系统，证明多目标RLAIF能够产生反映不同用户优先级的平衡策略，无需繁琐的奖励工程。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习部署中的奖励设计是一个核心挑战，特别是在多目标场景下。现有的RLAIF工作通常只关注单目标任务，而多目标系统中冲突目标之间的权衡难以明确指定，策略容易陷入优化主导目标的问题。

Method: 将RLAIF范式扩展到多目标自适应系统，利用大型语言模型生成偏好标签，避免依赖人工标注，在多目标系统中学习反映不同用户优先级的平衡策略。

Result: 多目标RLAIF能够产生平衡的权衡策略，反映不同用户优先级，无需进行繁琐的奖励工程。

Conclusion: 将RLAIF集成到多目标强化学习中，为在具有固有冲突目标的领域中实现用户对齐的策略学习提供了一条可扩展的路径。

Abstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.

</details>


### [27] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: CHESS是一个算法-系统协同设计的KV缓存管理系统，通过上下文感知的分层选择策略，仅使用1%的KV缓存就能超越完整KV缓存的质量，实现高达4.56倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM需要低延迟的准确推理，但随着上下文增长，解码主要受KV缓存约束。现有的剪枝方法大多是上下文无关的：它们的token选择忽略了逐步相关性和局部语义，这损害了质量。此外，它们的不规则访问和选择开销只带来了有限的加速效果。

Method: CHESS采用算法-系统协同设计。算法上，引入上下文感知的分层选择策略，动态重建当前解码的连贯上下文。系统层面，粗粒度选择消除了昂贵的数据移动，充分实现理论稀疏性的实际加速。

Result: 广泛评估表明，CHESS仅使用1%的KV缓存就能超越完整KV缓存的质量，提供低延迟稳定推理，吞吐量提升高达4.56倍，并持续优于其他强基线。

Conclusion: CHESS通过算法-系统协同设计有效解决了长上下文LLM中KV缓存管理的挑战，实现了高质量、低延迟的推理，为实际应用提供了实用的加速方案。

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [28] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: PyVision-RL是一个强化学习框架，用于防止多模态智能体在训练中出现交互崩溃，通过过采样-过滤-排序策略和累积工具奖励来维持多轮工具使用，并开发了图像和视频理解模型，其中视频模型采用按需上下文构建来减少视觉标记使用。


<details>
  <summary>Details</summary>
Motivation: 多模态智能体在强化学习中经常出现交互崩溃问题，即模型倾向于减少工具使用和多轮推理，从而限制了智能体行为的优势。需要一种能够稳定训练并维持交互的框架。

Method: 提出PyVision-RL框架，结合过采样-过滤-排序的rollout策略和累积工具奖励来防止交互崩溃并鼓励多轮工具使用。采用统一训练流程开发PyVision-Image和PyVision-Video模型，其中视频模型使用按需上下文构建，在推理过程中选择性采样任务相关帧以减少视觉标记使用。

Result: 实验显示PyVision模型表现出强大的性能和改进的效率，证明了持续交互和按需视觉处理对于可扩展多模态智能体的重要性。

Conclusion: PyVision-RL框架能够有效防止多模态智能体训练中的交互崩溃，维持多轮工具使用，通过按需上下文构建显著提高效率，为可扩展的多模态智能体提供了关键解决方案。

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [29] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: POMDPPlanners是一个开源的Python包，用于POMDP规划算法的实证评估，集成了先进算法、基准环境、自动超参数优化和并行仿真功能。


<details>
  <summary>Details</summary>
Motivation: 现有工具包在风险敏感设置下存在不足，需要进行大规模、可重复的不确定性决策研究，特别是安全关键场景下的评估。

Method: 开发了一个集成框架，包含：1) 最先进的规划算法；2) 带有安全关键变体的基准环境套件；3) 通过Optuna进行自动超参数优化；4) 具有故障恢复的持久缓存；5) 可配置的并行仿真。

Result: 该软件包显著减少了大规模仿真研究的开销，支持可扩展、可重复的研究，特别是在标准工具包无法满足的风险敏感设置下。

Conclusion: POMDPPlanners为不确定性决策研究提供了一个全面的评估平台，特别适用于安全关键应用，促进了该领域的实证研究进展。

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [30] [Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset](https://arxiv.org/abs/2602.20812)
*Jia-Rui Lin,Yun-Hong Cai,Xiang-Rui Ni,Shaojie Zhou,Peng Pan*

Main category: cs.AI

TL;DR: 该研究针对BIM设计领域缺乏LLM评估基准和数据集的问题，提出了首个BIM设计专用LLM评估基准、数据集生成方法和微调策略，开发了Qwen-BIM模型，在仅14B参数下性能媲美671B参数的通用LLM。


<details>
  <summary>Details</summary>
Motivation: 建筑行业数字化转型中，BIM设计成为智能建造的关键驱动力。尽管大型语言模型在促进BIM设计方面显示出潜力，但缺乏特定数据集和评估基准严重限制了LLM在BIM领域的性能表现。

Method: 1) 提出BIM设计评估基准及量化指标；2) 开发从BIM生成文本数据的方法，构建BIM衍生数据集用于LLM评估和微调；3) 设计适应BIM设计的LLM微调策略。

Result: 提出的领域专用基准能全面评估LLM能力，显示通用LLM在领域特定任务上仍不足。基于该基准和数据集开发的Qwen-BIM相比基础LLM模型G-Eval得分平均提升21.0%，仅14B参数性能即可媲美671B参数的通用LLM在BIM设计任务上的表现。

Conclusion: 本研究通过引入全面的评估基准和高质量数据集，开发了首个BIM设计专用LLM，为各领域开发BIM相关LLM奠定了坚实基础。

Abstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.

</details>


### [31] [Pressure Reveals Character: Behavioural Alignment Evaluation at Depth](https://arxiv.org/abs/2602.20813)
*Nora Petrova,John Burden*

Main category: cs.AI

TL;DR: 研究人员开发了一个包含904个场景的AI对齐基准测试，涵盖诚实性、安全性、非操纵性、鲁棒性、可纠正性和阴谋性六个类别，通过多轮对话和压力测试评估语言模型在真实场景下的行为表现。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型对齐评估主要关注单轮对话和理论声明，缺乏真实压力下的多轮场景测试。随着对齐失败导致的实际危害增加，需要更全面的评估框架来测试模型在真实冲突情境下的行为。

Method: 创建了包含904个场景的基准测试，涵盖六个对齐类别，通过人类评估验证场景的真实性。使用模拟工具访问、冲突指令和多轮升级等压力测试方法。评估了24个前沿模型，使用经过人类标注验证的LLM法官进行评分。

Result: 即使表现最好的模型在特定类别中也存在差距，大多数模型在所有类别中都表现出一致的弱点。因子分析显示对齐行为是一个统一结构（类似于认知研究中的g因子），模型在一个类别得分高时在其他类别也倾向于得分高。

Conclusion: 该研究提供了一个全面的对齐评估基准，揭示了当前语言模型在真实压力场景下的系统性弱点。基准测试和交互式排行榜已公开，计划在观察到持续弱点的领域扩展场景，并随着新模型的发布不断更新评估。

Abstract: Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.

</details>


### [32] [Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs](https://arxiv.org/abs/2602.20878)
*Dhita Putri Pratama,Soyeon Caren Han,Yihao Ding*

Main category: cs.AI

TL;DR: 该论文提出了Vision-Language Causal Graphs (VLCGs)结构化表示方法和ViLCaR诊断基准，用于评估大型视觉语言模型的因果推理能力，发现现有模型主要缺乏结构化指导而非推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视觉问答基准上表现良好，但往往依赖虚假相关性而非真正的因果推理。现有评估主要关注答案正确性，难以区分失败是由于推理能力有限还是未能识别因果相关信息。

Method: 提出了Vision-Language Causal Graphs (VLCGs) - 一种结构化、查询条件化的表示方法，明确编码因果相关的对象、属性、关系和场景基础假设。基于此构建了ViLCaR诊断基准，包含因果归因、因果推理和问答任务，以及图对齐的评估指标。

Result: 实验表明，在现有大型视觉语言模型中注入结构化相关信息，相比零样本和标准上下文学习，能显著提高归因和推理的一致性。这表明当前模型在因果推理方面的限制主要源于结构化指导不足而非推理能力缺乏。

Conclusion: 当前大型视觉语言模型在因果推理方面的局限性主要源于缺乏结构化指导，而非推理能力不足。VLCGs和ViLCaR为评估和改进模型的因果推理能力提供了有效工具。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.

</details>


### [33] [Predicting Sentence Acceptability Judgments in Multimodal Contexts](https://arxiv.org/abs/2602.20918)
*Hyewon Jang,Nikolai Ilinykh,Sharid Loáiciga,Jey Han Lau,Shalom Lappin*

Main category: cs.AI

TL;DR: 研究比较了人类和大型语言模型在视觉上下文影响下的句子可接受性判断，发现视觉图像对人类判断影响很小，但LLMs表现出压缩效应，且不同模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 探索视觉上下文（图像）对人类和大型语言模型句子可接受性判断的影响，比较两者在多媒体语境下的处理差异，以理解LLMs与人类语言处理的相似性和差异性。

Method: 通过实验设计，让人类和多种大型语言模型在有无视觉图像的条件下对句子可接受性进行判断，分析LLMs的预测准确度、判断分布模式以及与内部概率表示的相关性。

Result: 视觉图像对人类可接受性评分几乎没有影响；LLMs表现出文档上下文中的压缩效应；不同LLMs预测人类判断的准确度很高，但去除视觉上下文时表现稍好；Qwen模型最接近人类模式；LLMs生成预测与其归一化对数概率高度相关，但在视觉上下文存在时相关性降低。

Conclusion: 人类和LLMs在多媒体语境下的句子处理存在有趣异同：视觉上下文对人类影响有限，但对LLMs内部表示与生成预测的对应关系产生影响，揭示了LLMs与人类语言认知机制的差异。

Abstract: Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.

</details>


### [34] [HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG](https://arxiv.org/abs/2602.20926)
*Yuqi Huang,Ning Liao,Kai Yang,Anning Hu,Shengchao Hu,Xiaoxing Wang,Junchi Yan*

Main category: cs.AI

TL;DR: HELP框架通过超节点扩展和逻辑路径引导的证据定位策略，在保持检索准确性的同时显著提升图增强检索生成效率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在知识边界和幻觉问题，传统检索增强生成方法忽视结构依赖关系，图增强方法在准确性和效率之间存在权衡

Method: 提出HELP框架，包含两个核心策略：1) 超节点扩展：将知识三元组迭代链接为连贯推理路径并抽象为超节点；2) 逻辑路径引导的证据定位：利用预计算的图-文本相关性将路径直接映射到语料库

Result: HELP在多个简单和多跳问答基准测试中取得竞争性性能，相比领先的图增强RAG基线实现高达28.8倍的加速

Conclusion: HELP框架通过创新的超节点扩展和逻辑路径引导策略，有效平衡了图增强检索生成的准确性和效率，避免了昂贵的随机游走和语义失真

Abstract: Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\times$ speedup over leading Graph-based RAG baselines.

</details>


### [35] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: 本文提出AgentOS框架，将大语言模型重新定义为受结构化操作系统逻辑支配的"推理内核"，通过深度上下文管理和经典操作系统抽象映射，构建可扩展、自演化的认知环境。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究主要关注扩展上下文窗口或优化提示工程，但微观token处理与宏观系统智能之间的理论桥梁仍然碎片化。需要从静态推理引擎向动态自主认知系统进行根本性转变。

Method: 提出AgentOS概念框架，将LLM重新定义为"推理内核"，采用深度上下文管理，将上下文窗口概念化为可寻址语义空间而非被动缓冲区。引入语义切片和时间对齐机制来减轻多智能体编排中的认知漂移，并将经典操作系统抽象（如内存分页、中断处理和进程调度）映射到LLM原生构造上。

Result: 该框架为构建弹性、可扩展和自演化的认知环境提供了严谨的路线图，将上下文窗口重新概念化为可寻址语义空间，并建立了从离散序列到连贯认知状态的系统化解构。

Conclusion: AGI发展的下一个前沿在于系统级协调的架构效率，AgentOS框架为实现从静态推理引擎到动态自主认知系统的根本性转变提供了理论基础和实现路径。

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>


### [36] [LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification](https://arxiv.org/abs/2602.21044)
*Yanrui Wu,Lingling Zhang,Xinyu Zhang,Jiayu Chang,Pengyu Li,Xu Jiang,Jingtao Hu,Jun Liu*

Main category: cs.AI

TL;DR: LogicGraph是首个评估多路径逻辑推理的基准，通过神经符号框架构建包含多路径推理和逻辑干扰的问题，揭示LLMs倾向于过早固定单一推理路径而无法探索替代方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估主要关注收敛性逻辑推理（产生单一正确证明），但现实世界许多推理问题存在多种有效推导路径，需要模型探索多样化的逻辑路径而非固守单一路线。

Method: 提出LogicGraph基准，采用神经符号框架，结合向后逻辑生成和语义实例化，构建经过求解器验证的推理问题，包含高深度多路径推理和内在逻辑干扰，每个实例关联一组最小证明的穷举集。

Result: 实验显示最先进语言模型的共同局限：模型倾向于过早固定单一推理路径而无法探索替代方案，且随着推理深度增加，覆盖差距显著扩大。LogicGraph揭示了这种发散性差距。

Conclusion: LogicGraph暴露了LLMs在多路径逻辑推理方面的局限性，为未来改进提供了可操作的见解，代码和数据将开源发布。

Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.

</details>


### [37] [Tool Building as a Path to "Superintelligence"](https://arxiv.org/abs/2602.21061)
*David Koplow,Tomer Galanti,Tomaso Poggio*

Main category: cs.AI

TL;DR: 论文研究了LLMs在逻辑推理任务中的步进成功率γ，发现前沿模型在GF(2)电路重构任务中表现部分鲁棒性，精确工具调用是实现通用超智能的关键。


<details>
  <summary>Details</summary>
Motivation: Diligent Learner框架认为LLMs可以通过测试时搜索实现超智能，前提是有足够的步进成功率γ。本研究旨在测量LLMs在逻辑分布外推理任务中的γ值，以评估其实现超智能的潜力。

Method: 设计了一个基准测试来测量γ值，构建了GF(2)电路重构任务类别，这些任务随着推理步骤增加而变得更难，从信息论角度看，除非LLMs仔细整合所有提供的信息，否则无法可靠解决。

Result: 分析表明，小型LLMs的γ值随深度增加呈超线性下降，而前沿模型在该任务上表现出部分鲁棒性。成功的规模化推理依赖于精确的工具调用，工具设计被确定为LLMs通过Diligent Learner框架实现通用超智能的关键能力。

Conclusion: 工具设计是LLMs通过Diligent Learner框架实现通用超智能的关键瓶颈，前沿模型在逻辑推理任务中表现出部分鲁棒性，但精确的工具调用能力对于规模化推理至关重要。

Abstract: The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.

</details>


### [38] [The Initial Exploration Problem in Knowledge Graph Exploration](https://arxiv.org/abs/2602.21066)
*Claire McNamara,Lucy Hederman,Declan O'Sullivan*

Main category: cs.AI

TL;DR: 论文提出了知识图谱探索中的"初始探索问题"(IEP)，即非专业用户在初次接触陌生知识图谱时面临的理解障碍，并构建了包含范围不确定性、本体不透明性和查询能力不足三个维度的理论框架。


<details>
  <summary>Details</summary>
Motivation: 知识图谱虽然能整合跨领域的复杂信息，但其语义丰富性和结构复杂性为非专业用户设置了重大障碍。当用户初次接触陌生的知识图谱时，他们不知道可以提出什么问题、知识如何组织、或如何开始探索，这种"初始探索问题"尚未得到充分理论化研究。

Method: 基于信息行为和人机交互理论（包括ASK理论、探索性搜索、信息觅食理论和认知负荷理论），构建了IEP的概念框架，分析知识图谱探索界面的交互原语，识别设计空间中的结构性缺口。

Result: 提出了IEP理论框架，包含三个相互依存的障碍：范围不确定性（不知道KG包含什么）、本体不透明性（不理解知识组织结构）、查询能力不足（无法有效查询）。发现现有系统依赖的认知假设在初次接触时不成立，设计空间中缺乏"范围揭示"交互原语。

Conclusion: 通过理论化IEP，本文为评估知识图谱界面和设计支持初始探索的入口支架提供了理论视角，强调了需要开发能够在不要求用户制定查询或解释本体结构的情况下传达KG内容的交互机制。

Abstract: Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the Initial Exploration Problem (IEP). Drawing on theories from information behaviour and human-computer interaction, including ASK, exploratory search, information foraging, and cognitive load theory, we develop a conceptual framing of the IEP characterised by three interdependent barriers: scope uncertainty, ontology opacity, and query incapacity. We argue that these barriers converge at the moment of first contact, distinguishing the IEP from related concepts that presuppose an existing starting point or information goal. Analysing KG exploration interfaces at the level of interaction primitives, we suggest that many systems rely on epistemic assumptions that do not hold at first contact. This reveals a structural gap in the design space: the absence of interaction primitives for scope revelation, mechanisms that communicate what a KG contains without requiring users to formulate queries or interpret ontological structures. In articulating the IEP, this paper provides a theoretical lens for evaluating KG interfaces and for designing entry-point scaffolding that supports initial exploration.

</details>


### [39] [CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning](https://arxiv.org/abs/2602.21154)
*Ziwei Niu,Hao Sun,Shujun Bian,Xihong Yang,Lanfen Lin,Yuxin Liu,Yueming Jin*

Main category: cs.AI

TL;DR: CG-DMER是一个对比生成框架，用于解耦多模态心电图表示学习，通过空间-时间掩码建模和表示解耦对齐策略，解决现有方法中忽略导联间时空依赖性和模态特定偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态心电图分析方法存在两个主要问题：1）在模态内，现有模型以导联无关的方式处理心电图，忽略了导联间的时空依赖关系，限制了其对细粒度诊断模式的建模能力；2）在模态间，现有方法直接将心电图信号与临床报告对齐，由于报告的自由文本特性引入了模态特定偏差。

Method: 提出CG-DMER框架，包含两个关键设计：1）空间-时间掩码建模，通过在空间和时间维度上应用掩码并重建缺失信息，更好地捕捉细粒度时间动态和导联间空间依赖；2）表示解耦和对齐策略，通过引入模态特定和模态共享编码器，确保模态不变和模态特定表示之间的清晰分离，减少不必要的噪声和模态特定偏差。

Result: 在三个公共数据集上的实验表明，CG-DMER在多种下游任务中实现了最先进的性能。

Conclusion: CG-DMER通过解决现有多模态心电图分析中的模态内和模态间问题，提供了一种有效的解耦表示学习方法，显著提升了心电图诊断的准确性和鲁棒性。

Abstract: Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.

</details>


### [40] [NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning](https://arxiv.org/abs/2602.21172)
*Ishaan Rawal,Shubh Gupta,Yihan Hu,Wei Zhan*

Main category: cs.AI

TL;DR: NoRD模型通过消除推理标注需求，在少于60%的数据上实现与现有VLA模型相当的自动驾驶性能，训练token减少3倍


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action模型面临两大昂贵需求：大规模数据集收集和密集推理标注，这限制了自动驾驶系统的效率

Method: 提出NoRD模型，使用小于60%的训练数据且无需推理标注；采用Dr. GRPO算法解决标准GRPO在小规模无推理数据集上的局限性，克服难度偏差问题

Result: 在Waymo和NAVSIM基准测试中达到竞争性性能，训练token减少3倍，显著降低了数据收集和标注成本

Conclusion: NoRD模型通过消除推理标注需求并减少训练数据量，实现了更高效的自动驾驶系统，为VLA模型的实际部署提供了可行方案

Abstract: Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $<$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [41] [The Tragedy of Chain Commons](https://arxiv.org/abs/2602.20341)
*Ignacio Amores-Sesar,Mirza Ahad Baig,Seth Gilbert,Ray Neiheiser,Michelle X. Yeo*

Main category: cs.DC

TL;DR: 本文系统研究了拜占庭容错共识中解耦设计（排序与执行分离）的安全隐患，发现了一种名为"gaslighting"的新型攻击，并证明了在这种解耦模型中无法同时实现攻击抵抗性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现代区块链追求高吞吐量和低延迟，解耦设计将交易排序与执行分离以提高性能，但这种设计可能导致无效交易留在账本中，增加存储成本并引发新的战略行为风险。

Method: 建立了一个形式化框架来分析共识与执行的交互关系，使用该框架识别并证明了"gaslighting"攻击的存在，并分析了攻击抵抗性与资源利用率之间的根本性权衡。

Result: 发现了解耦设计存在先前未识别的gaslighting攻击，证明了在这种模型中无法同时确定性地实现攻击抵抗性和高资源利用率，提出了针对领导者协议的中间模型解决方案。

Conclusion: 解耦共识设计存在安全与效率的根本权衡，需要新的中间模型来平衡gaslighting攻击抵抗性与高性能需求，为区块链架构设计提供了重要理论指导。

Abstract: Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.

</details>


### [42] [Circumventing the FLP Impossibility Result with Open Atomic Ethernet](https://arxiv.org/abs/2602.20444)
*Paul Borrill*

Main category: cs.DC

TL;DR: FLP不可能性定理是分布式计算的基础性负面结果，但本文认为它只是特定系统模型的定理，而非物理定律。Open Atomic Ethernet (OAE)通过拒绝异步模型基础，引入"双同步"概念，在Layer 2构建基于交换的协议，实现了确定性原子协调。


<details>
  <summary>Details</summary>
Motivation: FLP不可能性定理长期以来被视为分布式计算中不可逾越的约束，迫使研究者采用随机协议、故障检测器或弱一致性模型等变通方案。本文旨在挑战这一传统观念，证明FLP只是特定系统模型的定理，而非绝对限制，从而探索绕过FLP约束的新途径。

Method: 提出Open Atomic Ethernet (OAE)系统，引入"双同步"概念——在每轮边界上双方都能达成结果共识的有界时间双边解析。在Layer 2构建基于交换的协议，从根本上拒绝FLP的异步模型假设，实现确定性原子协调。

Result: OAE成功规避了FLP不可能性定理，通过双同步模型和Layer 2交换协议，实现了确定性共识，而不违反任何不可能性结果。这证明了FLP约束并非绝对，而是特定模型下的产物。

Conclusion: FLP不可能性定理不是物理定律，而是特定异步系统模型的定理。通过采用双同步模型和适当的系统设计（如OAE），可以绕过FLP约束，实现确定性原子协调，为分布式系统设计开辟了新方向。

Abstract: The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result.

</details>


### [43] [Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning](https://arxiv.org/abs/2602.20450)
*Nihal Balivada,Shrey Gupta,Shashank Shreedhar Bhatt,Suyash Gupta*

Main category: cs.DC

TL;DR: Terraform是一种新颖的联邦学习客户端选择方法，通过梯度更新和确定性选择算法选择异构客户端进行重训练，相比先前工作可提升高达47%的准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习由于客户端间的统计异构性导致准确率低于传统机器学习算法。现有方法使用模型更新（如损失和偏差）来选择能提升全局模型准确率的客户端，但这些更新不能准确表示客户端的异构性，且选择方法缺乏确定性。

Method: 提出Terraform方法，采用双管齐下的策略：1）使用梯度更新而非传统模型更新；2）采用确定性选择算法来选择异构客户端进行重训练。

Result: Terraform相比先前工作可提升高达47%的准确率。通过全面的消融研究和训练时间分析证明了其效率，为Terraform的鲁棒性提供了有力证据。

Conclusion: Terraform通过创新的梯度更新和确定性选择算法，有效解决了联邦学习中因统计异构性导致的准确率下降问题，显著提升了模型性能。

Abstract: Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.

</details>


### [44] [A Granularity Characterization of Task Scheduling Effectiveness](https://arxiv.org/abs/2602.20561)
*Sana Taghipour Anvar,David Kaeli*

Main category: cs.DC

TL;DR: 论文提出了一个任务粒度表征框架，将调度开销增长与任务图依赖拓扑直接关联，解释了动态调度在强扩展中的性能变化


<details>
  <summary>Details</summary>
Motivation: 基于任务的运行时系统为并行科学应用提供了灵活的负载平衡和可移植性，但其强扩展性对任务粒度高度敏感。随着并行度增加，调度开销可能从可忽略变为主导，导致某些算法性能急剧下降。虽然这种现象在经验上广泛观察到，但缺乏对算法结构如何影响动态调度效益的理解。

Method: 引入了一个粒度表征框架，将调度开销增长与任务图依赖拓扑直接关联。基于依赖结构而非问题规模单独决定开销随并行度扩展的观察，使用简单的粒度度量来表征执行行为，指示调度开销何时能被并行计算分摊，何时会主导性能。

Result: 通过对具有不同依赖模式的代表性并行工作负载进行实验评估，证明所提出的表征能够解释实践中观察到的渐进和突变的强扩展崩溃。进一步表明，从依赖拓扑导出的开销模型能够准确预测强扩展限制，并实现实用的运行时决策规则，用于选择动态或静态执行，无需进行详尽的强扩展研究或广泛的离线调优。

Conclusion: 任务依赖结构而非问题规模单独决定了调度开销随并行度的扩展行为。提出的粒度表征框架能够解释强扩展性能变化，并提供实用的运行时决策机制来优化任务调度策略选择。

Abstract: Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.

</details>


### [45] [Lagom: Unleashing the Power of Communication and Computation Overlapping for Distributed LLM Training](https://arxiv.org/abs/2602.20656)
*Guanbin Xu,ZhenGuo Xu,Yuzhe Li,Youhui Bai,Ping Gong,Chaoyi Ruan,Cheng Li*

Main category: cs.DC

TL;DR: Lagom系统通过协同调优通信参数来平衡计算与通信资源使用，引入统一成本模型和基于优先级的搜索算法，将优化复杂度从指数级降至线性级，在高低带宽GPU集群上相比NCCL和AutoCCL获得显著加速


<details>
  <summary>Details</summary>
Motivation: 在分布式大模型训练中，重叠通信与计算至关重要，但当计算成为瓶颈时，优化这种重叠仍然具有挑战性。现有方法难以在计算成为瓶颈时有效平衡计算与通信资源的使用。

Method: 提出Lagom系统，通过协同调优通信参数来平衡计算与通信之间的资源使用。引入统一成本模型来评估整体性能，并设计基于优先级的搜索算法，将优化复杂度从指数级降低到线性级。

Result: 在高带宽和低带宽GPU集群上的评估显示，Lagom相比NCCL和AutoCCL，在不同模型和并行化策略下分别实现了1.07-1.33倍和1.03-1.27倍的加速。

Conclusion: Lagom系统通过协同调优通信参数和引入高效的搜索算法，有效解决了分布式大模型训练中计算与通信资源平衡的优化问题，显著提升了训练效率。

Abstract: Overlapping communication with computation is crucial for distributed large-model training, yet optimizing it - especially when computation becomes the bottleneck-remains challenging. We present Lagom, a system that co-tunes communication parameters to balance resource usage between computation and communication. By introducing a unified cost model and a priority-based search algorithm, Lagom reduces optimization complexity from exponential to linear. Evaluations on high- and low-bandwidth GPU clusters show that Lagom achieves 1.07-1.33x and 1.03-1.27x speedup over NCCL and AutoCCL across diverse models and parallelizations.

</details>


### [46] [A Morton-Type Space-Filling Curve for Pyramid Subdivision and Hybrid Adaptive Mesh Refinement](https://arxiv.org/abs/2602.20887)
*David Knapp,Johannes Albrecht Holke,Thomas Spenke,Carsten Burstedde*

Main category: cs.DC

TL;DR: 本文提出将金字塔单元作为新的功能元素类型，用于连接四面体和六面体元素，实现了三维混合元素网格的完整树型自适应网格细化框架。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥基于树的自适应网格细化在三维混合元素网格中的潜力，需要解决四面体和六面体元素之间的连接问题，避免悬挂边。

Method: 引入金字塔作为新的功能元素类型，为其定义明确的空间填充曲线，解决元素和森林层面的金字塔细化挑战，提出必要的功能设计，并推广全局并行算法。

Result: 实现了完整的混合元素动态自适应网格细化框架，验证了其效率和可扩展性。

Conclusion: 金字塔元素的引入使森林细化树方法能够完全支持三维混合元素网格，实现了高效且可扩展的动态自适应网格细化。

Abstract: The forest-of-refinement-trees approach allows for dynamic adaptive mesh refinement (AMR) at negligible cost. While originally developed for quadrilateral and hexahedral elements, previous work established the theory and algorithms for unstructured meshes of simplicial and prismatic elements. To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes, this paper introduces the pyramid as a new functional element type; its primary purpose is to connect tetrahedral and hexahedral elements without hanging edges.We present a well-defined space-filling curve (SFC) for the pyramid and detail how the unique challenges on the element and forest level associated with the pyramidal refinement are resolved. We propose the necessary functional design and generalize the fundamental global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange to fully support this new element. Our demonstrations confirm the efficiency and scalability of this complete, hybrid-element dynamic AMR framework.

</details>


### [47] [Is a LOCAL algorithm computable?](https://arxiv.org/abs/2602.21022)
*Antonio Cruciani,Avinandan Das,Massimo Equi,Henrik Lievonen,Diep Luong-Le,Augusto Modanese,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文探讨了LOCAL模型中计算性假设与图规模知识之间的关系，发现对于局部可检查标记问题，计算性假设会影响算法复杂度，特别是当不知道图大小时。


<details>
  <summary>Details</summary>
Motivation: LOCAL模型中关于节点状态更新函数是任意函数还是可计算函数的定义存在模糊性，虽然之前认为这种区别不重要，但本文要证明这种区别对局部可检查标记问题确实有影响，并且与是否知道图规模n的知识密切相关。

Method: 通过构造一个具体的LCL问题Π来展示计算性假设的影响：1）在不可计算模型中可在O(log n)轮解决；2）在可计算模型中如果知道n的上界也可在O(log n)轮解决；3）在可计算模型中如果不知道n则需要Ω(√n)轮。同时证明对于任意LCL问题，如果有n的界，则在可计算和不可计算模型中具有相同的轮复杂度。

Result: 证明了计算性假设确实影响LCL问题的复杂度，特别是当不知道图规模时。构造的LCL问题Π展示了三种不同情况下的复杂度差异，并建立了计算性假设与图规模知识之间的一般性联系。

Conclusion: LOCAL模型中计算性假设与图规模知识是相互关联的重要因素，对于LCL问题的复杂度分析必须明确考虑这两个因素。这澄清了之前模型定义中的模糊性，为分布式计算理论提供了更精确的基础。

Abstract: Common definitions of the "standard" LOCAL model tend to be sloppy and even self-contradictory on one point: do the nodes update their state using an arbitrary function or a computable function? So far, this distinction has been safe to neglect, since problems where it matters seem contrived and quite different from e.g. typical local graph problems studied in this context.
  We show that this question matters even for locally checkable labeling problems (LCLs), perhaps the most widely studied family of problems in the context of the LOCAL model. Furthermore, we show that assumptions about computability are directly connected to another aspect already recognized as highly relevant: whether we have any knowledge of $n$, the size of the graph. Concretely, we show that there is an LCL problem $Π$ with the following properties:
  1. $Π$ can be solved in $O(\log n)$ rounds if the \textsf{LOCAL} model is uncomputable.
  2. $Π$ can be solved in $O(\log n)$ rounds in the computable model if we know any upper bound on $n$.
  3. $Π$ requires $Ω(\sqrt{n})$ rounds in the computable model if we do not know anything about $n$.
  We also show that the connection between computability and knowledge of $n$ holds in general: for any LCL problem $Π$, if you have any bound on $n$, then $Π$ has the same round complexity in the computable and uncomputable models.

</details>


### [48] [ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments](https://arxiv.org/abs/2602.21140)
*Haley Li,Xinglu Wang,Cong Feng,Chunxu Zuo,Yanan Wang,Hei Lo,Yufei Cui,Bingji Wang,Duo Cui,Shuming Jing,Yizhou Shan,Ying Xiong,Jiannan Wang,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ReviveMoE是一种用于大规模LLM部署中快速故障恢复的方法，无需重启服务实例，支持传统LLM架构和分离架构。


<details>
  <summary>Details</summary>
Motivation: 随着LLM部署在更多硬件上扩展，系统单点故障概率显著增加，云运营商需要稳健的应对措施。传统的重启恢复方法在模型即服务推理场景中成本高昂，因为重新加载模型权重和重新编译计算图会引入显著延迟。

Method: ReviveMoE是一种快速故障恢复方法，支持传统LLM架构（MoE和注意力机制在同一硬件上）和分离架构（MoE与注意力机制分离）。该方法集成在华为云的MaaS中，基于华为的xDeepServe服务平台和XCCL通信库构建。

Result: 该方法已集成到华为云的模型即服务中，基于华为的xDeepServe服务平台和XCCL通信库实现。

Conclusion: ReviveMoE为大规模LLM部署提供了一种无需重启服务实例的快速故障恢复解决方案，能够显著减少故障恢复延迟，提高服务可靠性。

Abstract: As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library.

</details>
