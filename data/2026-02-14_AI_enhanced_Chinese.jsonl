{"id": "2602.12128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12128", "abs": "https://arxiv.org/abs/2602.12128", "authors": ["Hanno Ackermann", "Hong Cai", "Mohsen Ghafoorian", "Amirhossein Habibian"], "title": "HLA: Hadamard Linear Attention", "comment": null, "summary": "The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.\n  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.", "AI": {"tldr": "\u63d0\u51faHadamard\u7ebf\u6027\u6ce8\u610f\u529b(HLA)\uff0c\u901a\u8fc7\u54c8\u8fbe\u739b\u79ef\u8ba1\u7b97\u540e\u5e94\u7528\u975e\u7ebf\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u7ebf\u6027\u6ce8\u610f\u529b\u80fd\u66f4\u597d\u5730\u8fd1\u4f3csoftmax\uff0c\u5728\u89c6\u9891\u751f\u6210\u7b49\u5927token\u91cf\u4efb\u52a1\u4e2d\u8868\u73b0\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46\u4f7f\u7528\u72ec\u7acb\u5e94\u7528\u4e8e\u8f93\u5165\u7684\u4f4e\u9636\u6709\u7406\u51fd\u6570\u8fd1\u4f3csoftmax\uff0c\u8fd1\u4f3c\u6548\u679c\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7ebf\u6027\u6ce8\u610f\u529b\u8ba1\u7b97\u6548\u7387\uff0c\u53c8\u80fd\u66f4\u597d\u8fd1\u4f3c\u6807\u51c6softmax\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHadamard\u7ebf\u6027\u6ce8\u610f\u529b(HLA)\uff1a1) \u4e0d\u50cf\u4f20\u7edf\u7ebf\u6027\u6ce8\u610f\u529b\u90a3\u6837\u5c06\u975e\u7ebf\u6027\u5206\u522b\u5e94\u7528\u4e8equery\u548ckey\uff0c\u800c\u662f\u5728\u8ba1\u7b97\u5b8cpairwise\u76f8\u4f3c\u5ea6\u540e\u5e94\u7528\u975e\u7ebf\u6027\uff1b2) \u4f7f\u7528\u66f4\u9ad8\u9636\u7684\u6709\u7406\u51fd\u6570\u6765\u8fd1\u4f3csoftmax\uff1b3) \u63a8\u5bfc\u51fa\u7c7b\u4f3c\u6807\u51c6\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u9ad8\u6548\u8ba1\u7b97\u65b9\u6848\uff0c\u65e0\u9700\u8017\u65f6\u7684\u5f20\u91cf\u91cd\u5851\u64cd\u4f5c\u3002", "result": "HLA\u5728\u8fd1\u4f3csoftmax\u65b9\u9762\u6bd4\u4f20\u7edf\u7ebf\u6027\u6ce8\u610f\u529b\u66f4\u51c6\u786e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u7684\u6269\u6563transformer\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u5927\u91cftoken\u3002", "conclusion": "HLA\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7ebf\u6027\u6ce8\u610f\u529b\u8ba1\u7b97\u6548\u7387\uff0c\u53c8\u80fd\u66f4\u597d\u8fd1\u4f3c\u6807\u51c6softmax\u6ce8\u610f\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5904\u7406\u5927\u91cftoken\u7684\u5e94\u7528\u573a\u666f\u5982\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2602.12164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12164", "abs": "https://arxiv.org/abs/2602.12164", "authors": ["Xiaohan He", "Shiyang Feng", "Songtao Huang", "Lei Bai", "Bin Wang", "Bo Zhang"], "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.", "AI": {"tldr": "Sci-CoE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u79d1\u5b66\u534f\u540c\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u7a00\u758f\u76d1\u7763\u5230\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u8f6c\u53d8\uff0c\u8ba9\u6a21\u578b\u4f5c\u4e3a\u6c42\u89e3\u5668\u548c\u9a8c\u8bc1\u5668\u81ea\u6211\u8fdb\u5316\uff0c\u63d0\u5347\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u7136\u8106\u5f31\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4e0d\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u8bc4\u4f30\u548c\u9a8c\u8bc1\u7b56\u7565\u591a\u6837\u6027\u6709\u9650\u3002\u73b0\u6709\u7684\u534f\u540c\u8fdb\u5316\u8303\u5f0f\u5728\u4ee3\u7801\u548c\u6570\u5b66\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faSci-CoE\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e3a\u9a8c\u8bc1\u5668\u5efa\u7acb\u57fa\u672c\u6b63\u786e\u6027\u5224\u65ad\u951a\u70b9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u51e0\u4f55\u5956\u52b1\u673a\u5236\uff0c\u7efc\u5408\u8003\u8651\u5171\u8bc6\u3001\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\uff0c\u5728\u672a\u6807\u6ce8\u6570\u636e\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u6211\u8fed\u4ee3\u3002", "result": "\u5728\u591a\u4e2a\u901a\u7528\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSci-CoE\u589e\u5f3a\u4e86\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u7a33\u5065\u548c\u591a\u6837\u5316\u7684\u8bc4\u4f30\u7cfb\u7edf\u3002", "conclusion": "Sci-CoE\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u8bc4\u4f30\u4e0d\u53ef\u9760\u548c\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u79d1\u5b66\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
