<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NotSoTiny: A Large, Living Benchmark for RTL Code Generation](https://arxiv.org/abs/2512.20823)
*Razine Moundir Ghorab,Emanuele Parisi,Cristian Gutierrez,Miquel Alberti-Binimelis,Miquel Moreto,Dario Garcia-Gasulla,Gokcen Kestor*

Main category: cs.AR

TL;DR: NotSoTiny是一个评估LLM生成RTL代码能力的新基准测试，基于Tiny Tapeout社区的真实硬件设计，解决了现有基准测试规模小、设计简单、验证不严格和数据污染等问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成RTL代码方面显示出潜力，但现有评估基准存在多个问题：规模有限、偏向简单设计、验证不严格、容易受到数据污染。需要更真实、更具挑战性的基准来推动该领域发展。

Method: 从Tiny Tapeout社区的数百个真实硬件设计中构建自动化流水线，包括去重、验证正确性，并定期纳入新设计以匹配Tiny Tapeout发布计划，从而减轻数据污染问题。

Result: 评估结果显示NotSoTiny任务比现有基准更具挑战性，证明了其在克服LLM应用于硬件设计当前局限性方面的有效性，并为改进这一有前景的技术提供了指导。

Conclusion: NotSoTiny基准测试通过提供结构丰富、上下文感知的RTL生成任务，有效解决了现有基准的局限性，为评估和改进LLM在硬件设计中的应用提供了更真实、更具挑战性的平台。

Abstract: LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 本文提出了一种多模态知识图谱增强的检索生成方法，通过整合视觉线索来提升对长文档内容的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成方法在处理长文档（如整本书）时存在局限性，主要受限于上下文窗口大小，难以进行深度推理。现有的知识图谱增强方法虽然提供结构化支持，但仅限于文本输入，未能利用视觉等多模态信息的互补优势。

Method: 提出多模态知识图谱增强的检索生成框架，将视觉线索整合到知识图谱构建、检索阶段和答案生成过程中，实现跨模态推理。

Result: 实验结果表明，该方法在全局和细粒度问答任务上均优于现有的检索增强生成方法，在文本和多模态语料库上均表现出色。

Conclusion: 通过整合视觉信息到知识图谱增强的检索生成框架中，能够显著提升对复杂文档内容的理解和推理能力，为多模态文档分析提供了有效解决方案。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [3] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: KICSS 2025会议论文集，涵盖人工智能、知识工程、人机交互和创造力支持系统等跨学科领域


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程、人机交互和创造力支持系统领域的研究人员提供一个多学科交流平台，促进相关领域的学术交流与合作

Method: 通过双盲评审流程接受同行评审论文，部分优秀论文经过额外评审后推荐至IEICE Transactions on Information and Systems期刊发表

Result: 成功举办了第20届国际会议，出版了包含多学科研究成果的会议论文集，建立了与IEICE Proceedings Series的合作关系

Conclusion: KICSS 2025会议为相关领域的研究人员提供了重要的学术交流平台，通过严格的评审流程确保了论文质量，促进了跨学科研究的合作与发展

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [4] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: Microprobe是一种新颖的大模型可靠性评估方法，仅需100个精心选择的探测样本即可实现全面评估，相比传统方法大幅降低计算成本和时间消耗。


<details>
  <summary>Details</summary>
Motivation: 传统大模型可靠性评估需要数千个评估样本，计算成本高、耗时久，难以满足实际部署需求，需要开发更高效的评估方法。

Method: 结合五个关键可靠性维度的策略性提示多样性、先进的不确定性量化以及自适应加权，通过精心选择的100个探测样本高效检测潜在故障模式。

Result: 在多个语言模型和跨领域验证中，microprobe相比随机采样基线获得23.5%更高的综合可靠性分数，统计显著性极强，评估成本降低90%，同时保持95%的传统方法覆盖率。

Conclusion: Microprobe为负责任AI部署中的高效模型评估填补了关键空白，能够以极低成本实现全面可靠性评估。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [5] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1是基于阿里通义千问模型开发的AI医疗咨询助手，整合500GB高质量医学知识，采用增强预训练和检索增强生成混合方法，能在3-5轮交互中提供准确诊断建议和健康指导。


<details>
  <summary>Details</summary>
Motivation: 开发安全、可靠、专业的AI健康顾问，赋能基层医疗和健康管理，成为用户的智能健康伴侣。

Method: 基于阿里通义千问模型，整合约500GB高质量结构化医学知识，采用增强预训练和检索增强生成的混合方法。

Result: Erkang-Diagnosis-1.1在综合医学考试评估中表现优于GPT-4。

Conclusion: Erkang-Diagnosis-1.1是一个有效的AI医疗咨询助手，能够通过高效交互提供准确的诊断建议，在医学知识评估方面超越现有先进模型。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [6] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 该研究探索不同大语言模型之间推理链的互换性，发现部分完成的推理链可以被其他模型可靠地继续，有时甚至能提升最终准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT提示显著提升了LLMs的推理能力，但先前研究主要关注通过内部推理策略提升模型性能，对于不同模型间推理的互换性知之甚少。本研究旨在探索一个模型部分完成的推理链是否可以被另一个模型可靠地继续，无论是同一模型家族内还是跨家族。

Method: 使用token级对数概率阈值在早期、中期和晚期阶段截断基线模型（Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct）的推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行继续实验，测试家族内和跨家族行为。评估流程结合截断阈值和过程奖励模型（PRM），为通过模型互换评估推理稳定性提供可复现框架。

Result: 使用PRM的评估显示，混合推理链通常能够保持甚至在某些情况下提升最终准确率和逻辑结构。这表明推理模型具有互换性这一新兴行为特性。

Conclusion: 推理的互换性是推理模型的一个新兴行为特性，为协作AI系统中可靠的模块化推理提供了新的范式见解。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [7] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AiAuditTrack (AAT) 是一个基于区块链的AI使用流量记录与治理框架，利用去中心化身份和可验证凭证建立可信AI实体，记录交互轨迹实现跨系统监管，并提出风险扩散算法追踪风险行为源头。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的AI应用快速扩展，AI交互数据激增，带来了安全、问责和风险可追溯性方面的紧迫挑战，需要建立可信的AI使用记录和治理机制。

Method: 1. 利用去中心化身份(DID)和可验证凭证(VC)建立可信可识别的AI实体；2. 将AI实体建模为动态交互图中的节点，边表示时间特定的行为轨迹；3. 提出风险扩散算法追踪风险行为源头并在相关实体间传播早期预警；4. 在链上记录实体间交互轨迹以实现跨系统监管和审计。

Result: 通过区块链交易每秒(TPS)指标评估系统性能，证明AAT在大规模交互记录下的可行性和稳定性，为复杂多智能体环境中的AI审计、风险管理和责任归属提供了可扩展且可验证的解决方案。

Conclusion: AAT框架为AI审计、风险管理和责任归属提供了可扩展且可验证的解决方案，能够应对复杂多智能体环境中的安全、问责和风险可追溯性挑战。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [8] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: 提出MoAS架构，通过学习的路由器为每个token动态选择最优注意力方案（MHA、GQA或MQA），在保持MHA性能的同时减少KV缓存内存需求


<details>
  <summary>Details</summary>
Motivation: Transformer模型中注意力机制的选择需要在建模质量和推理效率之间权衡。MHA提供最佳质量但推理时KV缓存内存需求大，MQA和GQA减少内存但性能下降

Method: 提出混合注意力方案（MoAS），通过学习的路由器为每个token动态选择最优注意力方案（MHA、GQA或MQA），而不是静态平均方案

Result: 在WikiText-2上的实验显示，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线竞争，同时提供条件计算效率潜力

Conclusion: MoAS通过动态路由机制有效平衡了注意力机制的质量与效率权衡，为Transformer模型提供了更灵活的架构选择

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [9] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统通过构建类人记忆架构，解决LLM在记忆方面的固有局限，包括受限上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成等问题，显著提升长期对话中的知识保真度和检索效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临固有的记忆限制，包括受限的上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成等问题，这些严重制约了持续对话和个性化服务的实现。

Method: 基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现对LLM记忆机制的全链重构。

Result: 在医疗、企业运营和教育等多个领域展示出显著的工程创新和性能突破，相比现有解决方案（如Mem0、MemGPT、Graphiti），在准确性、token效率和响应延迟等关键指标上表现更优。

Conclusion: Memory Bear系统显著改善了长期对话中的知识保真度和检索效率，降低了幻觉率，并通过记忆-认知集成增强了上下文适应性和推理能力，标志着AI从"记忆"向"认知"迈进的关键一步。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [10] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 提出AFA对抗性反馈注意力训练机制，通过动态掩码策略和政策梯度优化，使Transformer模型能自动将注意力重新分配到任务相关但较少出现的词汇上，提升情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在情感分析任务中，注意力主要分配给常见词汇，而忽略了不常见但高度任务相关的词汇，这导致在某些场景下性能不佳。

Method: 提出对抗性反馈注意力(AFA)训练机制：1) 动态掩码策略，尝试掩码不同词汇来欺骗判别器；2) 判别器检测掩码引起的显著差异；3) 利用Transformer对token级扰动的敏感性，采用政策梯度方法优化注意力分布。

Result: 在三个公开数据集上实现了最先进的结果。将该训练机制应用于增强大型语言模型的注意力，获得了12.6%的进一步性能提升。

Conclusion: AFA机制能有效解决Transformer模型注意力分配偏差问题，通过对抗训练自动优化注意力分布，显著提升情感分析性能，并可扩展到大型语言模型中。

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [11] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 该研究通过三个实验量化了LLMs的行为缺陷：懒惰性（复杂指令执行不完整）、解码次优性（短视解码）和上下文退化（长对话中遗忘指令）。发现LLMs普遍存在懒惰问题，但在简单推理任务中解码次优性有限，且在长对话中表现出意外的上下文保持能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常表现出行为缺陷，如懒惰（提前截断响应或不完全遵守多部分请求）、解码次优性（由于短视解码而未能选择更高质量的序列）和上下文退化（在长对话中遗忘或忽略核心指令）。本研究旨在通过受控实验量化这些现象，了解现代LLMs的实际表现。

Method: 研究设计了三个受控实验（A、B、C），在多个先进LLMs（OpenAI GPT-4变体、DeepSeek）上进行测试。实验A量化懒惰性，评估模型对复杂多部分指令的遵守程度；实验B测试解码次优性，使用简单推理任务检查模型是否因短视解码而选择次优答案；实验C评估上下文退化，通过200轮混乱对话测试模型保持关键事实和指令的能力。

Result: 1. 懒惰性普遍存在：模型经常省略必需部分或未能满足长度要求；2. 解码次优性有限：在简单推理任务中，模型的贪婪答案似乎与其最高置信度解决方案一致；3. 上下文退化意外稳健：在200轮混乱对话测试中，模型保持关键事实和指令的能力远超预期。

Conclusion: 虽然遵守详细指令仍是开放挑战，但现代LLMs可能在内部缓解了一些假设的失败模式（如上下文遗忘）。研究讨论了可靠性影响，将发现与先前关于指令遵循和长上下文处理的工作联系起来，并推荐了减少懒惰性和增强多指令遵守的策略（如自我完善和动态提示）。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [12] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 该立场论文指出可信人工智能（TAI）存在功能性TAI（FTAI）和规范性TAI（NTAI）之间的鸿沟，提出需要建立语义语言作为桥梁来评估AI系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着法规和功能需求的发展，可信人工智能（TAI）日益重要。然而，功能性TAI（关注如何实现可信系统）和规范性TAI（关注需要执行的法规）之间存在脱节，这使得评估AI系统的可信度变得困难。

Method: 作者提出引入概念性语义语言作为桥梁，匹配FTAI和NTAI。这种语义语言可以作为框架帮助开发者评估AI系统的可信度，同时帮助利益相关者将规范和法规转化为具体的实施步骤。

Result: 论文描述了当前最新进展，识别了FTAI和NTAI之间的鸿沟，讨论了开发语义语言的起点及其预期效果，并提供了关键考虑因素。

Conclusion: 需要建立语义语言作为桥梁来连接功能性TAI和规范性TAI，以促进可信人工智能的评估。论文提供了未来行动的关键考虑因素和方向。

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [13] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 当前大语言模型安全机制存在重大漏洞：无法理解上下文和识别用户意图，导致恶意用户可通过情感框架、渐进揭示和学术论证等系统化方法绕过安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法主要关注显性有害内容，但忽视了关键漏洞：缺乏上下文理解和用户意图识别能力。这为恶意用户提供了可系统利用的漏洞来绕过安全机制。

Method: 对多个最先进的LLM（包括ChatGPT、Claude、Gemini和DeepSeek）进行实证评估，分析通过情感框架、渐进揭示和学术论证等技术绕过安全机制的效果。

Result: 研究发现推理增强配置反而放大了利用效果，提高了事实精确性但未能质疑底层意图。唯一例外是Claude Opus 4.1，在某些用例中优先考虑意图检测而非信息提供。

Conclusion: 当前架构设计存在系统性漏洞，需要范式转变：将上下文理解和意图识别作为核心安全能力，而非事后保护机制。

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [14] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 这篇综述分析了2023-2025年间32项研究，探讨生成式AI在高等教育计算机科学教育中的个性化应用效果，识别了五种应用领域和成功设计模式，提出了探索优先的采用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够大规模实现个性化计算机科学教育，但需要研究这种个性化是否真正支持学习，以及如何设计有效的个性化机制来优化学习效果。

Method: 采用范围综述方法，从259条记录中有目的地抽样32项研究（2023-2025年），分析高等教育计算机科学环境中的个性化机制和有效性信号。

Result: 识别了五个应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码审查。发现采用解释优先指导、解决方案保留、分级提示阶梯和基于学生工件的设计比无约束聊天界面效果更好。成功实施共享四种模式：基于学生工件的上下文感知辅导、需要反思的多级提示结构、与传统CS基础设施（自动评分器和评分标准）的结合、以及人在循环的质量保证。

Conclusion: 生成式AI可以作为精确支架机制，但需要嵌入可审计的工作流程中，保持生产性挣扎的同时扩展个性化支持。提出了探索优先的采用框架，强调试点、工具化、学习保护默认设置和基于证据的扩展，并识别了学术诚信、隐私、偏见和过度依赖等风险及缓解措施。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [15] [From artificial to organic: Rethinking the roots of intelligence for digital health](https://arxiv.org/abs/2512.20723)
*Prajwal Ghimire,Keyoumars Ashkan*

Main category: cs.AI

TL;DR: 论文探讨了人工智能与有机智能之间的界限模糊性，指出AI本质上是人类有机智慧的产物，其原理源于人类神经生物学和进化过程。


<details>
  <summary>Details</summary>
Motivation: 作者旨在挑战"人工"与"有机"之间的传统二分法，强调人工智能实际上是人类有机智慧的延伸，而非完全独立于自然的存在。

Method: 通过哲学和概念分析的方法，从神经生物学、进化过程和认知科学的角度探讨人工智能的本质，强调其与有机智能的内在联系。

Result: 揭示了人工智能与有机智能之间的界限远比术语所暗示的要模糊，AI的发展本质上是有机智慧的组织和适应过程的延伸。

Conclusion: 在数字健康领域，从有机智能到人工智能的转变并非神秘过程，而是关于组织和适应的根本性问题，人工与有机之间的区分需要重新审视。

Abstract: The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.

</details>


### [16] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath是一个将语言模型推理能力与代码解释器计算精度结合的智能体框架，通过自动化生成监督微调数据、新型强化学习范式以及高效训练系统，显著提升复杂数学问题的解决效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型（如o3和DeepSeek-R1）在自然语言推理方面取得进展，但在处理需要复杂数学运算的问题时仍存在计算效率低下和准确性不足的问题。需要一种能够结合语言模型推理能力和代码解释器计算精度的解决方案。

Method: 1. 自动化方法：将自然语言思维链转换为结构化工具增强轨迹，生成高质量监督微调数据以缓解数据稀缺问题。
2. 新型智能体强化学习范式：动态交错自然语言生成与实时代码执行，让模型通过多轮交互反馈自主学习最优工具使用策略，同时培养代码优化和错误修正的涌现能力。
3. 高效训练系统：采用请求级异步rollout调度、智能体部分rollout和前缀感知加权负载平衡等创新技术，实现4-5倍加速，使在超长序列和大量工具调用场景下的高效强化学习训练成为可能。

Result: 在AIME24、AIME25和HMMT25等具有挑战性的数学竞赛基准测试中，AgentMath实现了最先进的性能。具体来说，AgentMath-30B-A3B分别达到90.6%、86.4%和73.8%的准确率，展现了先进的能力。

Conclusion: AgentMath框架有效解决了大型推理模型在复杂数学问题上的计算效率和准确性限制，验证了语言模型推理与代码解释器计算相结合的方法的有效性，为构建更复杂、可扩展的数学推理智能体铺平了道路。

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [17] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文提出了一种在参数化动作空间中学习状态和动作抽象的方法，通过在线渐进细化抽象来提高稀疏奖励、长视野任务的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界的顺序决策通常涉及参数化动作空间，需要同时处理离散动作选择和连续动作参数决策。现有方法存在严重限制：规划方法需要手工制作的动作模型，标准强化学习算法要么针对离散动作要么针对连续动作，而少数处理参数化动作的RL方法依赖领域特定工程且未能利用这些空间的潜在结构。

Method: 本文扩展了RL算法到长视野、稀疏奖励的参数化动作设置，通过使智能体能够在线自主学习状态和动作抽象。引入的算法在学习过程中逐步细化这些抽象，在状态-动作空间的关键区域增加细粒度细节，这些区域更高的分辨率能提高性能。

Result: 在多个连续状态、参数化动作领域中，这种抽象驱动的方法使TD(λ)实现了比最先进基线方法显著更高的样本效率。

Conclusion: 通过在线学习状态和动作抽象并渐进细化，可以有效解决参数化动作空间中的长视野、稀疏奖励强化学习问题，显著提高样本效率。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [18] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 本文提出多智能体多角色辩论方法，通过引入多样化的反思视角来避免单一LLM在反思过程中出现的思维退化问题，在推理任务上取得了比单一LLM反思更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在通过反思提升推理能力时存在思维退化问题，即LLM会不断重复相同的错误，即使知道自己是错的。这限制了反思方法的效果，需要新的方法来生成更有效的反思。

Method: 提出多智能体多角色辩论方法，使用多个具有不同角色的LLM智能体进行辩论来生成反思。这种方法通过引入多样化的视角和辩论过程，避免了单一LLM反思时的思维退化问题。

Result: 在HotPot QA（问答）任务上达到47%的精确匹配率，在HumanEval（编程）任务上达到82.7%的准确率，这两个性能都超过了使用单一LLM进行反思的方法。

Conclusion: 多智能体多角色辩论方法能够生成更多样化的反思，有效避免了单一LLM反思时的思维退化问题，在推理任务上取得了显著更好的性能表现。

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [19] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 本文提出了一种基于概率框架的双向知识交换模型，通过Beta-Bernoulli分布和遗忘因子量化智能体信念不确定性，将公共贡献重新定义为最优主动学习策略，显著提升了异构环境中的适应性和集体智能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和RAG的自主智能体存在认知不对称问题，只能单向消费数字内容而缺乏双向知识交换。现有的自我反思框架大多是启发式和私有的，缺乏量化确定性和证明外部交互合理性的概率基础。这种隔离导致冗余推理和集体智能停滞。

Method: 提出一个正式的概率框架：1) 使用带遗忘因子γ的Beta-Bernoulli分布建模智能体对命题的信念；2) 将认知不确定性量化为信念方差；3) 建立双重交互驱动：稳态动机（维持确定性对抗时间衰减）和最优学习策略（针对最大模糊点以最大化信息增益）；4) 引入认知缓存机制，利用遗忘因子动态优先处理非平稳知识分布的活跃头部资源。

Result: 模拟结果表明，这种不确定性驱动策略在异构（Zipfian）环境中显著优于随机基线，保持对概念漂移的高适应性。积累的信念状态可作为RLHF的可验证奖励信号和SFT的高质量数据过滤器。

Conclusion: 该框架将公共贡献重新定义为最优主动学习：分享解决方案以获取反馈是智能体减少自身不确定性的最有效方法。通过量化认知不确定性和建立非利他主义的双向知识交换动机，解决了自主智能体的认知不对称问题，促进了集体智能的发展。

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [20] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 本文提出了一种结合LangChain多智能体系统和许可区块链的架构，用于确保自主AI系统的监控、策略执行和不可篡改审计，在智能库存管理、交通信号控制和医疗监控等场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管自主AI系统在医疗、智慧城市、数字取证和供应链管理等领域应用日益广泛，具有灵活性和实时推理能力，但也引发了信任、监督以及信息与活动完整性方面的担忧。

Method: 提出单一架构模型，包含基于LangChain的多智能体系统和许可区块链，将感知-概念化-行动循环与区块链治理层关联，验证输入、评估建议行动并记录执行结果。具体实现包括Hyperledger Fabric系统、MCP集成动作执行器和LangChain智能体。

Result: 实验涵盖智能库存管理、交通信号控制和医疗监控场景，结果显示区块链安全验证能有效防止未经授权的操作，提供全决策过程的可追溯性，并将操作延迟保持在合理范围内。

Conclusion: 该框架为实施高影响力自主AI应用提供了通用系统，既能保持自主性又能确保责任性，实现了自主与负责的平衡。

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [21] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 该论文提出了一个价格感知的智能AI系统，结合个人财务管理与饮食优化，为中等收入家庭提供营养充足且价格合理的膳食计划，能自动适应市场价格波动。


<details>
  <summary>Details</summary>
Motivation: 中等收入环境中家庭预算有限与营养需求之间的矛盾，特别是食品价格波动带来的挑战，需要一种能同时考虑经济可负担性和营养充足性的解决方案。

Method: 采用模块化多智能体架构，包含预算、营养、价格监控和健康个性化等专门智能体，共享知识库并使用替代图来确保以最低成本维持营养质量。

Result: 在沙特家庭案例研究中，系统相比静态周菜单实现12-18%的成本降低，营养充足率超过95%，在20-30%价格波动下仍保持高性能。

Conclusion: 该框架能有效结合经济可负担性与营养充足性，为实现可持续和公平的饮食规划提供了可行途径，符合可持续发展目标中零饥饿和良好健康的目标。

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [22] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 论文证明：在聚合观察和算法盲评估条件下，用LLM角色模拟替代人类进行A/B测试是有效的基准替代方案，且区分不同方法所需角色评估数量可通过信息论界限确定。


<details>
  <summary>Details</summary>
Motivation: A/B测试作为社会系统方法评估的黄金标准，但成本高、延迟长，阻碍了迭代方法开发。LLM角色模拟提供了廉价替代方案，但需要验证其是否保持基准接口的有效性。

Method: 提出充要条件特征：当(1)方法仅观察聚合结果（聚合观察），(2)评估仅依赖提交产物而非算法身份（算法盲评估）时，角色替换人类只是面板变更。定义聚合信道的信息论可区分性，推导可靠区分不同方法所需独立角色评估数量的明确界限。

Result: 证明在特定条件下，LLM角色模拟可替代人类进行基准测试，且角色基准与现场实验同等决策相关性本质上是样本量问题，给出了所需评估数量的理论界限。

Conclusion: LLM角色模拟在聚合观察和算法盲评估条件下是有效的A/B测试替代方案，为迭代方法开发提供了廉价、快速的基准测试框架，并提供了确定所需评估规模的理论指导。

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [23] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: 该研究首次在真实NHS初级医疗数据上评估LLM药物安全审查系统，发现虽然系统能高灵敏度识别临床问题，但在复杂情境推理方面存在显著缺陷，仅有46.9%的患者能正确识别所有问题和干预措施。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学基准测试中常达到或超过临床医生水平，但很少有研究在真实临床数据上评估LLM，或深入分析超越表面指标的表现。本研究旨在填补这一空白，评估LLM在真实医疗环境中的实际表现。

Method: 在NHS Cheshire和Merseyside地区2,125,549名成人的回顾性研究中，战略性地抽样选取患者以覆盖广泛的临床复杂性和药物安全风险，最终纳入277名患者。由专家临床医生审查这些患者，并对系统识别的问题和提出的干预措施进行分级评估。

Result: 主要LLM系统在识别临床问题存在方面表现强劲（灵敏度100%，特异性83.1%），但仅在46.9%的患者中正确识别所有问题和干预措施。失败分析揭示了五种主要失败模式：对不确定性的过度自信、未根据患者情境调整标准指南、误解医疗实践方式、事实错误和流程盲点。

Conclusion: 该研究揭示了LLM在临床情境推理方面的显著缺陷，这些缺陷在所有患者复杂性和人口统计学层次以及各种最先进模型和配置中持续存在。在LLM临床AI安全部署前必须解决这些短板，需要进行更大规模的前瞻性评估和更深入的LLM行为研究。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [24] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe：一种用于具身智能体的混合推理运行时安全防护系统，通过可执行的基于谓词的安全逻辑来减少危险行为，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的具身智能体在执行复杂现实任务时容易受到危险指令的影响，现有防御方法（如静态规则过滤器或提示级控制）难以处理动态、时间依赖和上下文丰富的环境中的隐含风险。

Method: 提出RoboSafe，一种混合推理运行时安全防护系统，包含：1）后向反思推理模块，持续回顾短期记忆中的轨迹以推断时间安全谓词；2）前向预测推理模块，从长期安全记忆和多模态观察中生成上下文感知的安全谓词。两者结合形成可适应、可验证的安全逻辑。

Result: 在多个智能体上的实验表明，RoboSafe显著减少了危险行为（风险发生率降低36.8%），同时保持了接近原始的任务性能。物理机械臂的真实世界评估进一步证实了其实用性。

Conclusion: RoboSafe通过可执行的基于谓词的安全逻辑，为具身智能体提供了有效、可解释的运行时安全防护，在减少危险行为的同时保持了任务性能，具有实际应用价值。

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System](https://arxiv.org/abs/2512.20677)
*Zhang Wei,Peilu Hu,Shengning Lang,Hao Yan,Li Mei,Yichao Zhang,Chen Yang,Junfeng Hao,Zhimo Han*

Main category: cs.CR

TL;DR: 本文提出了一种自动化红队测试框架，用于系统性地发现大型语言模型的安全漏洞，相比人工测试在漏洞发现率上提升了3.9倍。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在关键领域的部署增加，确保其安全性和对齐性变得至关重要。现有的红队测试主要依赖人工方法，这限制了可扩展性，无法全面覆盖潜在的对抗行为空间。

Method: 开发了一个自动化红队测试框架，集成了基于元提示的攻击合成、多模态漏洞检测和标准化评估协议，涵盖六大威胁类别：奖励黑客攻击、欺骗性对齐、数据泄露、故意表现不佳、不当工具使用和思维链操纵。

Result: 在GPT-OSS-20B模型上发现了47个不同的漏洞，包括21个高严重性漏洞和12个新颖攻击模式，漏洞发现率比人工专家测试提高了3.9倍，同时保持89%的检测准确率。

Conclusion: 该框架在实现可扩展、系统化和可复现的AI安全评估方面表现出有效性，为改进对齐鲁棒性提供了可操作的见解，推动了自动化LLM红队测试的发展，有助于构建安全可信的AI系统。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes domains, ensuring their security and alignment has become a critical challenge. Existing red-teaming practices depend heavily on manual testing, which limits scalability and fails to comprehensively cover the vast space of potential adversarial behaviors. This paper introduces an automated red-teaming framework that systematically generates, executes, and evaluates adversarial prompts to uncover security vulnerabilities in LLMs. Our framework integrates meta-prompting-based attack synthesis, multi-modal vulnerability detection, and standardized evaluation protocols spanning six major threat categories -- reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation. Experiments on the GPT-OSS-20B model reveal 47 distinct vulnerabilities, including 21 high-severity and 12 novel attack patterns, achieving a $3.9\times$ improvement in vulnerability discovery rate over manual expert testing while maintaining 89\% detection accuracy. These results demonstrate the framework's effectiveness in enabling scalable, systematic, and reproducible AI safety evaluations. By providing actionable insights for improving alignment robustness, this work advances the state of automated LLM red-teaming and contributes to the broader goal of building secure and trustworthy AI systems.

</details>


### [26] [Anota: Identifying Business Logic Vulnerabilities via Annotation-Based Sanitization](https://arxiv.org/abs/2512.20705)
*Meng Wang,Philipp Görz,Joschua Schilling,Keno Hassler,Liwei Guo,Thorsten Holz,Ali Abbasi*

Main category: cs.CR

TL;DR: ANOTA是一个人类参与循环的sanitizer框架，通过轻量级注释系统让用户编码领域知识，运行时监控程序行为与注释定义策略的偏差，从而检测传统模糊测试难以发现的业务逻辑漏洞。


<details>
  <summary>Details</summary>
Motivation: 业务逻辑漏洞是软件安全中的关键挑战，传统模糊测试主要关注内存安全漏洞，无法有效检测需要理解应用特定语义上下文的业务逻辑漏洞。这些漏洞占最危险软件弱点的大多数（27/40），是现有工具的盲区。

Method: ANOTA引入轻量级、用户友好的注释系统，让用户直接编码领域特定知识作为轻量级注释来定义应用的预期行为。运行时执行监控器观察程序行为，将其与注释定义的政策进行比较，识别指示漏洞的偏差。

Result: ANOTA与最先进的模糊测试器结合后，比其他兼容相同目标的错误发现方法更有效。成功复现了43个已知漏洞，发现了22个先前未知的漏洞（其中17个获得CVE编号）。

Conclusion: ANOTA提供了一种实用有效的方法来发现传统安全测试技术经常遗漏的复杂业务逻辑缺陷，填补了现有工具的盲区。

Abstract: Detecting business logic vulnerabilities is a critical challenge in software security. These flaws come from mistakes in an application's design or implementation and allow attackers to trigger unintended application behavior. Traditional fuzzing sanitizers for dynamic analysis excel at finding vulnerabilities related to memory safety violations but largely fail to detect business logic vulnerabilities, as these flaws require understanding application-specific semantic context. Recent attempts to infer this context, due to their reliance on heuristics and non-portable language features, are inherently brittle and incomplete. As business logic vulnerabilities constitute a majority (27/40) of the most dangerous software weaknesses in practice, this is a worrying blind spot of existing tools. In this paper, we tackle this challenge with ANOTA, a novel human-in-the-loop sanitizer framework. ANOTA introduces a lightweight, user-friendly annotation system that enables users to directly encode their domain-specific knowledge as lightweight annotations that define an application's intended behavior. A runtime execution monitor then observes program behavior, comparing it against the policies defined by the annotations, thereby identifying deviations that indicate vulnerabilities. To evaluate the effectiveness of ANOTA, we combine ANOTA with a state-of-the-art fuzzer and compare it against other popular bug finding methods compatible with the same targets. The results show that ANOTA+FUZZER outperforms them in terms of effectiveness. More specifically, ANOTA+FUZZER can successfully reproduce 43 known vulnerabilities, and discovered 22 previously unknown vulnerabilities (17 CVEs assigned) during the evaluation. These results demonstrate that ANOTA provides a practical and effective approach for uncovering complex business logic flaws often missed by traditional security testing techniques.

</details>


### [27] [Real-World Adversarial Attacks on RF-Based Drone Detectors](https://arxiv.org/abs/2512.20712)
*Omer Gazit,Yael Itzhakev,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 首次针对基于RF图像的无人机检测系统提出物理攻击方法，通过优化特定类别的通用复数基带扰动波形，在合法通信信号旁传输，有效降低目标无人机检测率同时保持合法无人机检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对图像模型的RF攻击主要修改数字特征，难以在无线传输中实现，因为将数字扰动转换为可传输波形会引入同步误差、干扰和硬件限制问题。

Method: 提出首个针对RF图像无人机检测器的物理攻击，优化特定类别的通用复数基带(I/Q)扰动波形，这些波形与合法通信信号一起传输，兼容标准RF链。

Result: 使用RF记录和OTA实验评估了四种无人机类型，结果表明适度、结构化的I/Q扰动与标准RF链兼容，能可靠降低目标无人机检测率，同时保持对合法无人机的检测能力。

Conclusion: 成功实现了首个针对RF图像无人机检测系统的物理攻击，证明了通过优化特定类别的I/Q扰动波形可以在实际无线环境中有效干扰目标检测，同时保持系统对其他合法无人机的检测功能。

Abstract: Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.

</details>


### [28] [Sark: Oblivious Integrity Without Global State](https://arxiv.org/abs/2512.20775)
*Alex Lynham,David Alesch,Ziyi Li,Geoff Goodell*

Main category: cs.CR

TL;DR: 本文介绍了Sark架构，这是一个实现了USO资产系统的参考架构，包含Sloop区块链和Porters子系统，分析了系统的CIA三要素，提出了完整性轨迹概念来解决去中心化设计权衡。


<details>
  <summary>Details</summary>
Motivation: 实现Goodell、Toliver和Nakib描述的不可伪造、有状态且不可见的USO资产系统，提供一个参考架构来支持这类系统的设计和实现。

Method: 设计了Sark参考架构，包含Sloop（一个许可制的崩溃容错区块链）和Porters（负责从客户端累积和汇总承诺的子系统），使用CIA三要素（机密性、可用性、完整性）分析系统运行，并引入完整性轨迹概念来处理去中心化设计权衡。

Result: 成功实现了Sark架构，包含Sloop区块链和Porters子系统，通过CIA三要素分析验证了系统设计，使用完整性轨迹概念解决了去中心化设计中的权衡问题。

Conclusion: Sark架构为USO资产系统提供了一个可行的实现框架，完整性轨迹概念有助于解决去中心化设计中的关键权衡，未来工作将集中在实现拜占庭容错和缓解Porters的本地中心化问题上。

Abstract: In this paper, we introduce Sark, a reference architecture implementing the Unforgeable, Stateful, and Oblivious (USO) asset system as described by Goodell, Toliver, and Nakib. We describe the motivation, design, and implementation of Sloop, a permissioned, crash fault-tolerant (CFT) blockchain that forms a subsystem of Sark, and the other core subsystems, Porters, which accumulate and roll-up commitments from Clients. We analyse the operation of the system using the 'CIA Triad': Confidentiality, Availability, and Integrity. We then introduce the concept of Integrity Locus and use it to address design trade-offs related to decentralization. Finally, we point to future work on Byzantine fault-tolerance (BFT), and mitigating the local centrality of Porters.

</details>


### [29] [pokiSEC: A Multi-Architecture, Containerized Ephemeral Malware Detonation Sandbox](https://arxiv.org/abs/2512.20860)
*Alejandro Avina,Yashas Hariprasad,Naveen Kumar Chaudhary*

Main category: cs.CR

TL;DR: pokiSEC是一个轻量级、临时性的恶意软件引爆沙箱，将完整的虚拟化和访问堆栈打包在Docker容器中，支持在ARM64和x86_64主机上运行Windows虚拟机。


<details>
  <summary>Details</summary>
Motivation: 动态恶意软件分析需要在强隔离、快速重置的环境中执行不可信二进制文件。现有解决方案通常依赖于重量级虚拟机管理程序或专用裸机实验室，限制了可移植性和自动化。随着ARM64开发硬件（如Apple Silicon）的采用，许多开源沙箱方案假设x86_64主机，无法在不同架构间良好移植。

Method: pokiSEC将QEMU与硬件加速（KVM）集成到Docker容器中，提供基于浏览器的工作流，支持自带Windows磁盘镜像。关键创新是"通用入口点"，执行运行时主机架构检测并选择经过验证的虚拟机管理程序配置（机器类型、加速模式和设备配置文件）。

Result: 在Apple Silicon（ARM64）和Ubuntu（AMD64）上验证了pokiSEC，展示了适合分析师工作流程的交互性能，并通过临时容器生命周期实现一致的拆除语义。

Conclusion: pokiSEC提供了一个轻量级、跨架构的恶意软件引爆沙箱解决方案，解决了当前动态恶意软件分析环境在可移植性和自动化方面的限制，特别支持ARM64和x86_64架构的异构环境。

Abstract: Dynamic malware analysis requires executing untrusted binaries inside strongly isolated, rapidly resettable environments. In practice, many detonation workflows remain tied to heavyweight hypervisors or dedicated bare-metal labs, limiting portability and automation. This challenge has intensified with the adoption of ARM64 developer hardware (e.g., Apple Silicon), where common open-source sandbox recipes and pre-built environments frequently assume x86_64 hosts and do not translate cleanly across architectures. This paper presents pokiSEC, a lightweight, ephemeral malware detonation sandbox that packages the full virtualization and access stack inside a Docker container. pokiSEC integrates QEMU with hardware acceleration (KVM when available) and exposes a browser-based workflow that supports bring-your-own Windows disk images. The key contribution is a Universal Entrypoint that performs runtime host-architecture detection and selects validated hypervisor configurations (machine types, acceleration modes, and device profiles), enabling a single container image and codebase to launch Windows guests on both ARM64 and x86_64 hosts. We validate pokiSEC on Apple Silicon (ARM64) and Ubuntu (AMD64), demonstrating interactive performance suitable for analyst workflows and consistent teardown semantics via ephemeral container lifecycles.

</details>


### [30] [Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification](https://arxiv.org/abs/2512.20872)
*Jakir Hossain,Gurvinder Singh,Lukasz Ziarek,Ahmet Erdem Sarıyüce*

Main category: cs.CR

TL;DR: 该论文提出了Better Call Graphs (BCG)数据集，这是一个针对Android恶意软件检测的大规模、高质量函数调用图数据集，解决了现有数据集过时、冗余和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 函数调用图在恶意软件检测中具有重要作用，但在Android领域缺乏大规模、高质量的数据集。现有数据集通常过时、包含大量重复或冗余的图（由于应用重打包），无法反映真实世界恶意软件的多样性，导致基于图的分类方法容易过拟合且评估不可靠。

Method: 作者构建了BCG数据集，从最近的Android应用包中提取大规模且独特的函数调用图。数据集包含良性和恶意样本，涵盖多种家族和类型，并为每个APK提供图级特征。

Result: 通过使用基线分类器进行广泛实验，证明了BCG相比现有数据集的必要性和价值。BCG数据集已公开可用，地址为https://erdemub.github.io/BCG-dataset。

Conclusion: BCG数据集填补了Android恶意软件检测领域高质量函数调用图数据集的空白，为基于图的分类方法提供了更可靠、多样化的评估基准，有助于推动该领域的研究进展。

Abstract: Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.

</details>


### [31] [Neutralization of IMU-Based GPS Spoofing Detection using external IMU sensor and feedback methodology](https://arxiv.org/abs/2512.20964)
*Ji Hyuk Jung,Ji Won Yoon*

Main category: cs.CR

TL;DR: 本文提出了一种针对自动驾驶车辆GPS欺骗攻击的IMU传感器检测绕过系统，通过窃取内部动态状态信息进行GPS欺骗，实验证明攻击值可以在不被检测的情况下注入。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的位置感知对其自主决策至关重要，GPS欺骗攻击会破坏位置识别。虽然基于内部IMU传感器的检测方法被认为是最有效的防御机制之一，但本文旨在开发能够绕过此类检测的欺骗攻击系统。

Method: 提出了一种攻击建模方法，通过使用外部IMU传感器窃取内部动态状态信息来执行GPS欺骗。基于EKF（扩展卡尔曼滤波）传感器融合，实验分析了GPS欺骗值对目标系统的影响以及所提方法如何减少目标系统中的异常检测。

Result: 实验结果表明，所提出的攻击模型能够在不被检测的情况下注入攻击值，成功绕过了基于IMU传感器的GPS欺骗检测机制。

Conclusion: 本文展示了一种针对自动驾驶车辆GPS欺骗检测的有效绕过技术，揭示了现有基于IMU传感器的防御机制存在的安全漏洞，为改进自动驾驶系统的安全防护提供了重要参考。

Abstract: Autonomous Vehicles (AVs) refer to systems capable of perceiving their states and moving without human intervention. Among the factors required for autonomous decision-making in mobility, positional awareness of the vehicle itself is the most critical. Accordingly, extensive research has been conducted on defense mechanisms against GPS spoofing attacks, which threaten AVs by disrupting position recognition. Among these, detection methods based on internal IMU sensors are regarded as some of the most effective. In this paper, we propose a spoofing attack system designed to neutralize IMU sensor-based detection. First, we present an attack modeling approach for bypassing such detection. Then, based on EKF sensor fusion, we experimentally analyze both the impact of GPS spoofing values on the internal target system and how our proposed methodology reduces anomaly detection within the target system. To this end, this paper proposes an attack model that performs GPS spoofing by stealing internal dynamic state information using an external IMU sensor, and the experimental results demonstrate that attack values can be injected without being detected.

</details>


### [32] [GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs](https://arxiv.org/abs/2512.21008)
*Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GateBreaker是一种无需训练、轻量级、架构无关的攻击框架，专门针对MoE LLMs的安全对齐机制，通过识别和禁用安全专家中的关键神经元来破坏模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着MoE架构在关键领域的部署增加，理解其安全机制变得至关重要。现有安全研究主要关注密集架构，对MoE独特的安全特性研究不足，其模块化、稀疏激活的设计可能导致安全机制与密集模型不同，存在潜在脆弱性。

Method: GateBreaker采用三阶段攻击框架：1) 门级分析，识别在有害输入上被过度路由的安全专家；2) 专家级定位，在安全专家内部定位安全结构；3) 针对性安全移除，禁用已识别的安全结构以破坏安全对齐。

Result: 攻击成功率从7.4%提升至64.9%，仅需禁用目标专家层中约3%的神经元。安全神经元在同一模型家族内可迁移，单次迁移攻击使成功率从17.9%提升至67.7%。框架可泛化到MoE视觉语言模型，在危险图像输入上达到60.9%的成功率。

Conclusion: MoE的安全机制集中在由稀疏路由协调的小部分神经元中，这些神经元具有可迁移性。GateBreaker揭示了MoE架构的安全脆弱性，为未来安全研究提供了重要方向。

Abstract: Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.
  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.

</details>


### [33] [zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy](https://arxiv.org/abs/2512.21048)
*Savvy Sharma,George Petrovic,Sarthak Kaushik*

Main category: cs.CR

TL;DR: zkFL-Health结合联邦学习、零知识证明和可信执行环境，为医疗AI提供隐私保护、可验证正确的协作训练框架，解决数据共享中的隐私泄露和聚合器信任问题。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要大规模多样化数据集，但严格的隐私和治理限制阻碍了机构间的原始数据共享。联邦学习虽然能在数据本地训练并仅交换模型更新，但仍面临两个核心风险：1) 通过梯度或更新的隐私泄露（成员推理、梯度反转）；2) 对聚合器的信任问题，聚合器作为单点故障可能丢弃、篡改或注入贡献而不被检测。

Method: 提出zkFL-Health架构，结合联邦学习、零知识证明和可信执行环境。客户端在本地训练并提交更新承诺；聚合器在TEE内运行以计算全局更新，并通过Halo2/Nova生成简洁的ZK证明，证明其使用了确切的承诺输入和正确的聚合规则，而不向主机泄露任何客户端更新。验证节点验证证明并在链上记录加密承诺，提供不可变的审计追踪。

Result: 该框架为多机构医疗AI提供强大的机密性、完整性和可审计性，这些是临床采用和监管合规的关键属性。通过性能评估计划涵盖准确性、隐私风险、延迟和成本等方面。

Conclusion: zkFL-Health通过零知识证明和可信执行环境的结合，解决了联邦学习在医疗领域的隐私泄露和信任问题，为实现安全、可验证的多机构医疗AI协作训练提供了可行方案。

Abstract: Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.

</details>


### [34] [AutoBaxBuilder: Bootstrapping Code Security Benchmarking](https://arxiv.org/abs/2512.21132)
*Tobias von Arx,Niels Mündler,Mark Vero,Maximilian Baader,Martin Vechev*

Main category: cs.CR

TL;DR: AutoBaxBuilder是一个自动生成代码安全基准测试的框架，能够从零开始创建任务和测试，解决人工基准测试的局限性，成本低且效率高。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件工程中的广泛应用，可靠评估LLM生成代码的正确性和安全性变得至关重要。现有研究表明LLM容易生成存在安全漏洞的代码，但当前依赖安全专家手工制作的基准测试存在三个主要问题：1）基准测试会污染训练数据；2）需要扩展到新任务以提供更全面的评估；3）需要增加难度以挑战更强大的LLM。

Method: 提出AutoBaxBuilder框架，通过细粒度的合理性检查构建稳健的流水线，利用LLM的代码理解能力构建功能测试和端到端的安全探测利用。框架能够从零开始生成任务和测试，用于代码安全基准测试。

Result: 使用AutoBaxBuilder构建了全新的任务集AutoBaxBench并公开发布。实验表明，生成一个新任务仅需不到2小时，成本低于10美元。通过定性分析和定量实验验证了生成基准的质量，并与人类专家构建的任务进行了比较。

Conclusion: AutoBaxBuilder成功解决了人工基准测试的局限性，能够高效、低成本地生成代码安全基准测试任务，为评估LLM的安全能力提供了新的工具和方法。

Abstract: As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.

</details>


### [35] [Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking](https://arxiv.org/abs/2512.21236)
*Yifan Huang,Xiaojun Jia,Wenbo Guo,Yuqiang Sun,Yihao Huang,Chong Wang,Yang Liu*

Main category: cs.CR

TL;DR: SPELL框架专门测试LLM在恶意代码生成方面的安全对齐漏洞，通过时间分配选择策略构建越狱提示，在GPT-4.1、Claude-3.5和Qwen2.5-Coder上分别达到83.75%、19.38%和68.12%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: LLM辅助编程工具使非专业开发者也能创建复杂应用，但这也可能被恶意行为者利用来生成有害软件。现有越狱研究主要针对通用攻击场景，对恶意代码生成作为越狱目标的探索有限。

Method: 提出SPELL测试框架，采用时间分配选择策略，通过智能组合先验知识数据集中的句子来系统构建越狱提示，平衡新颖攻击模式的探索与成功技术的利用。

Result: 在三个先进代码模型上评估：GPT-4.1攻击成功率83.75%，Claude-3.5为19.38%，Qwen2.5-Coder为68.12%。生成的提示在Cursor等实际AI开发工具中成功产生恶意代码，超过73%的输出被先进检测系统确认为恶意。

Conclusion: 当前LLM实现存在重大安全漏洞，SPELL框架为改进代码生成应用中的AI安全对齐提供了有价值的见解。

Abstract: Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [36] [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)
*Yuxiao Wang,Yuedong Xu,Qingyang Duan,Yuxuan Liu,Lei Jiao,Yinghao Yu,Jun Wu*

Main category: cs.DC

TL;DR: AutoHet系统自动为异构GPU环境寻找最优的3D并行训练方案，支持非对称并行结构，实现细粒度工作负载分配，并在Spot实例中断时提供高效恢复策略，相比现有系统显著提升训练吞吐量和恢复速度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速增长和新GPU产品的不断发布，异构GPU环境下的分布式训练需求显著增加。现有系统在异构环境中实现3D并行面临诸多挑战，包括需要对称张量并行、非对称流水线并行中的梯度同步效率问题，以及内存利用与计算效率之间的权衡。

Method: 提出AutoHet系统，支持非对称3D并行结构，实现细粒度工作负载分配。建立理论模型，将设备分组和负载平衡问题形式化为优化问题，以最小化每次迭代的训练时间，从而有效平衡不同能力GPU之间的计算能力和内存使用。针对Spot实例抢占，提出高效恢复策略，优先从本地节点检索训练状态，仅从云存储下载缺失的检查点。

Result: 在三个大规模模型和三种不同GPU类型的组合上进行广泛评估，结果显示AutoHet优于现有DNN训练系统：与Megatron-LM和Whale相比，训练吞吐量最高提升1.79倍；与Spot实例基线相比，恢复速度提升4.38倍。

Conclusion: AutoHet系统成功解决了异构GPU环境中3D并行训练的挑战，通过自动寻找最优并行方案和高效恢复策略，显著提升了分布式训练的性能和弹性。

Abstract: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\times$ speedup of recovery speed compared to a spot instance baseline.

</details>


### [37] [Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions](https://arxiv.org/abs/2512.20967)
*Linggao Kong,Yuedong Xu,Lei Jiao,Chuan Xu*

Main category: cs.DC

TL;DR: 该论文提出了一种基于预测的在线调度框架，用于在GPU现货实例市场中进行经济高效的大模型微调，通过混合使用现货和按需实例来降低成本，同时满足截止时间要求。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模增大，微调成本急剧上升。GPU现货实例虽然成本较低，但其价格和可用性的波动性使得满足截止时间的调度变得特别困难。需要一种能够利用现货实例低成本优势，同时应对其不确定性的调度方案。

Method: 1) 展示现货市场价格和可用性的可预测性；2) 构建整数规划问题来建模混合实例使用下的价格和可用性动态；3) 提出基于预测的在线分配算法，采用承诺水平控制方法；4) 当预测不准确时，提出无预测的补充算法；5) 开发在线策略选择算法，从参数化策略池中学习最佳策略。

Result: 1) 证明基于预测的算法在预测误差减小时能获得更紧的性能界限；2) 策略选择算法具有O(√T)的遗憾界限；3) 实验结果显示，该在线框架能自适应选择最佳策略，在不同现货市场动态和预测质量下始终优于基线方法，将效用提升高达54.8%。

Conclusion: 该研究证明了现货市场价格和可用性的可预测性，并开发了一个自适应在线调度框架，能够有效利用混合实例资源，在大模型微调中实现成本效益和截止时间保证的平衡。

Abstract: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.

</details>


### [38] [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)
*Sirui Chen,Jingji Chen,Siqi Zhu,Ziheng Jiang,Yanghua Peng,Xuehai Qian*

Main category: cs.DC

TL;DR: Mesh-Attention：一种新的分布式注意力算法，通过二维计算块分配降低通信计算比，相比Ring-Attention在256 GPU上实现平均2.9倍加速和79%通信量减少


<details>
  <summary>Details</summary>
Motivation: 现有最先进的Ring-Attention方法在扩展大语言模型上下文窗口时存在可扩展性限制，主要问题是通信流量过大。需要设计更高效的分布式注意力算法来降低通信开销。

Method: 提出Mesh-Attention算法，采用新的基于矩阵的模型重新思考分布式注意力设计空间。核心创新是将二维计算块（而非一维行或列）分配给每个GPU，通过降低通信计算比提高效率。算法包含贪婪搜索算法来高效调度计算块，确保GPU间通信效率。

Result: 在256个GPU上，Mesh-Attention相比现有方法实现最高3.4倍（平均2.9倍）加速，通信量减少最高85.4%（平均79%）。理论分析显示通信复杂度显著降低，可扩展性优于当前算法。

Conclusion: Mesh-Attention通过二维计算块分配和优化的通信调度，显著降低了分布式注意力的通信开销，在大规模部署中展现出优越的可扩展性和性能优势。

Abstract: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.
  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.

</details>
