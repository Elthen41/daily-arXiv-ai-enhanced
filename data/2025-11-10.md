<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: 本文介绍了Team Twente在2024年综合医疗排班竞赛中获得第三名的算法、实现和结果，采用混合整数规划、约束规划和模拟退火的3阶段分解方法。


<details>
  <summary>Details</summary>
Motivation: 解决医疗排班优化问题，通过组合多种优化技术来提高解决方案的质量和效率。

Method: 采用基于子问题分解的3阶段方法，结合混合整数规划、约束规划和模拟退火技术。

Result: 在竞赛中获得第三名，并首次提供了基准实例的最优解下界。

Conclusion: 该方法在医疗排班问题上表现良好，但仍有改进空间，文中指出了进一步优化的开放性问题。

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [2] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: 本文提出了一种基于认知不确定性的拒绝选项预测器，在数据不足导致高认知不确定性时允许模型拒绝预测，以最小化期望遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统拒绝选项方法仅关注偶然不确定性，假设训练数据充足使得认知不确定性可忽略。但在实际应用中数据有限时，这一假设不成立，需要同时考虑认知不确定性。

Method: 基于贝叶斯学习，重新定义最优预测器为最小化期望遗憾的模型，当输入导致的遗憾超过指定拒绝成本时拒绝预测。

Result: 提出了第一个原则性框架，能够学习识别训练数据不足以做出可靠决策的输入。

Conclusion: 该框架为高风险应用中处理数据不足导致的认知不确定性提供了理论基础，使模型能够在不确定性高时明智地拒绝预测。

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [3] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: DMA是一个在线学习框架，通过整合多粒度人类反馈来动态对齐RAG系统中的检索排名，解决了静态检索无法适应意图演变和内容漂移的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统依赖静态检索，无法适应不断变化的用户意图和内容漂移，需要动态调整机制来提升系统性能。

Method: DMA将文档级、列表级和响应级信号组织成连贯的学习流程：监督训练点对和列表排序器，基于响应级偏好的策略优化，以及将知识蒸馏到轻量级评分器中实现低延迟服务。

Result: 在线部署显示人类参与度显著提升，离线测试在TriviaQA和HotpotQA等对话QA任务上取得明显增益，同时保持了基础检索的竞争力。

Conclusion: DMA为RAG系统提供了一种原则性的反馈驱动实时适应方法，在不牺牲基线能力的前提下实现了性能提升。

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [4] [Real-Time Reasoning Agents in Evolving Environments](https://arxiv.org/abs/2511.04898)
*Yule Wen,Yixin Ye,Yanzhe Zhang,Diyi Yang,Hao Zhu*

Main category: cs.AI

TL;DR: 本文提出实时推理作为动态环境中智能体的新问题框架，构建了Real-Time Reasoning Gym进行验证。研究发现现有语言模型在反应式和规划式两种推理范式下都难以同时做出逻辑和及时的判断，因此提出了同时采用两种范式的AgileThinker方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的智能体需要在动态环境中同时做出逻辑和及时的判断，而现有语言模型推理方法未能考虑环境的动态特性。

Method: 研究两种语言模型部署范式：反应式智能体（有限推理计算快速响应）和规划式智能体（扩展推理计算处理复杂问题）。提出AgileThinker方法同时采用两种推理范式。

Result: 实验表明即使最先进的模型在单一推理范式下也难以同时满足逻辑性和及时性要求。AgileThinker在任务难度和时间压力增加时持续优于单一范式智能体，有效平衡推理深度和响应延迟。

Conclusion: 实时推理是开发实用智能体的关键测试平台，为时间约束AI系统研究奠定基础，指明了实现实时能力智能体的路径。

Abstract: Agents in the real world must make not only logical but also timely
judgments. This requires continuous awareness of the dynamic environment:
hazards emerge, opportunities arise, and other agents act, while the agent's
reasoning is still unfolding. Despite advances in language model reasoning,
existing approaches fail to account for this dynamic nature. We introduce
real-time reasoning as a new problem formulation for agents in evolving
environments and build Real-Time Reasoning Gym to demonstrate it. We study two
paradigms for deploying language models in agents: (1) reactive agents, which
employ language models with bounded reasoning computation for rapid responses,
and (2) planning agents, which allow extended reasoning computation for complex
problems. Our experiments show that even state-of-the-art models struggle with
making logical and timely judgments in either paradigm. To address this
limitation, we propose AgileThinker, which simultaneously engages both
reasoning paradigms. AgileThinker consistently outperforms agents engaging only
one reasoning paradigm as the task difficulty and time pressure rise,
effectively balancing reasoning depth and response latency. Our work
establishes real-time reasoning as a critical testbed for developing practical
agents and provides a foundation for research in temporally constrained AI
systems, highlighting a path toward real-time capable agents.

</details>


### [5] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: 提出一种支持军事地面作战决策的方法论，为机械化营生成和评估行动方案，通过系统生成数千个行动替代方案并评估其预期结果，在动态作战环境中持续优化决策。


<details>
  <summary>Details</summary>
Motivation: 在军事地面作战执行阶段需要有效的决策支持系统，特别是在动态变化的战斗环境中，需要能够快速生成和评估多种行动方案以优化作战效果。

Method: 采用并发生成和评估的方法，从初始行动方案集开始，系统生成数千个个体行动替代方案，基于敌方状态和行动进行评估，考虑单位组成、兵力比、攻防类型和预期推进率等因素。

Result: 该方法能够产生多种具有更优结果的替代行动方案，并在战斗进程中持续更新方案，为决策者提供动态优化的决策支持。

Conclusion: 所提出的方法论能够有效支持军事地面作战中的决策过程，通过系统化的方案生成和评估机制，在动态作战环境中提供持续优化的行动建议。

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>


### [6] [Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance](https://arxiv.org/abs/2511.05311)
*Valeriu Dimidov,Faisal Hawlader,Sasan Jafarnejad,Raphaël Frank*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的智能体在汽车行业预测性维护（PdM）数据清洗管道中的应用潜力，特别是在处理维护日志中的六种噪声类型方面表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 经济约束、数据集可用性有限以及专业人才短缺是汽车行业预测性维护面临的主要挑战，LLM的进展为克服这些障碍提供了机会。

Method: 使用基于LLM的智能体来支持PdM数据清洗管道，重点处理维护日志中的错误，包括拼写错误、缺失字段、近似重复条目和错误日期等六种噪声类型。

Result: 研究发现LLM在处理通用清洗任务方面表现有效，为未来工业应用提供了有前景的基础。

Conclusion: 虽然领域特定错误仍然具有挑战性，但这些结果突显了通过专门训练和增强智能体能力实现进一步改进的潜力。

Abstract: Economic constraints, limited availability of datasets for reproducibility
and shortages of specialized expertise have long been recognized as key
challenges to the adoption and advancement of predictive maintenance (PdM) in
the automotive sector. Recent progress in large language models (LLMs) presents
an opportunity to overcome these barriers and speed up the transition of PdM
from research to industrial practice. Under these conditions, we explore the
potential of LLM-based agents to support PdM cleaning pipelines. Specifically,
we focus on maintenance logs, a critical data source for training
well-performing machine learning (ML) models, but one often affected by errors
such as typos, missing fields, near-duplicate entries, and incorrect dates. We
evaluate LLM agents on cleaning tasks involving six distinct types of noise.
Our findings show that LLMs are effective at handling generic cleaning tasks
and offer a promising foundation for future industrial applications. While
domain-specific errors remain challenging, these results highlight the
potential for further improvements through specialized training and enhanced
agentic capabilities.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [Jailbreaking in the Haystack](https://arxiv.org/abs/2511.04707)
*Rishi Rajesh Shah,Chen Henry Wu,Shashwat Saxena,Ziqian Zhong,Alexander Robey,Aditi Raghunathan*

Main category: cs.CR

TL;DR: NINJA是一种通过在有害用户目标后附加良性模型生成内容来攻击对齐语言模型的越狱方法，发现目标位置对安全性至关重要，能显著提高攻击成功率且难以检测。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文语言模型支持百万token输入，其在复杂任务中的应用能力增强，但这些扩展上下文的安全影响尚不明确，需要研究其潜在漏洞。

Method: 提出NINJA方法，通过将有害用户目标放置在特定位置并附加良性模型生成内容来实施越狱攻击，利用目标位置对安全性的重要影响。

Result: 在标准安全基准测试HarmBench上，NINJA显著提高了包括LLaMA、Qwen、Mistral和Gemini在内的先进开源和专有模型的攻击成功率，且具有低资源、可迁移和难以检测的特点。

Conclusion: 即使是由良性内容组成的长上下文，当精心设计目标位置时，也会在现代语言模型中引入根本性漏洞，这揭示了长上下文模型的安全风险。

Abstract: Recent advances in long-context language models (LMs) have enabled
million-token inputs, expanding their capabilities across complex tasks like
computer-use agents. Yet, the safety implications of these extended contexts
remain unclear. To bridge this gap, we introduce NINJA (short for
Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by
appending benign, model-generated content to harmful user goals. Critical to
our method is the observation that the position of harmful goals play an
important role in safety. Experiments on standard safety benchmark, HarmBench,
show that NINJA significantly increases attack success rates across
state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,
and Gemini. Unlike prior jailbreaking methods, our approach is low-resource,
transferable, and less detectable. Moreover, we show that NINJA is
compute-optimal -- under a fixed compute budget, increasing context length can
outperform increasing the number of trials in best-of-N jailbreak. These
findings reveal that even benign long contexts -- when crafted with careful
goal positioning -- introduce fundamental vulnerabilities in modern LMs.

</details>


### [8] [SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking](https://arxiv.org/abs/2511.04711)
*Wenyuan Yang,Yichen Sun,Changzheng Chen,Zhixuan Chu,Jiaheng Zhang,Yiming Li,Dacheng Tao*

Main category: cs.CR

TL;DR: 本文提出SWAP方法，通过在不同且更复杂的空间中植入水印来保护软提示的版权，解决了现有技术在软提示版权保护中的失效问题。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（如CLIP）中的软提示需要有效的版权保护，但现有技术由于软提示学习的独特特性而失效。非侵入式审计容易产生误报，而侵入式方法无法嵌入功能性触发器或面临有害性和模糊性挑战。

Method: 提出顺序水印方法SWAP，通过指定分布外类别的特定顺序来编码水印，利用CLIP的零样本预测能力，将水印植入不同且更复杂的空间中。

Result: 在11个数据集上的广泛实验证明了SWAP的有效性、无害性和对潜在自适应攻击的鲁棒性。

Conclusion: SWAP通过在不同决策空间中植入水印，成功解决了软提示版权保护问题，同时保持原始预测标签不变，减少与主要任务的冲突。

Abstract: Large-scale vision-language models, especially CLIP, have demonstrated
remarkable performance across diverse downstream tasks. Soft prompts, as
carefully crafted modules that efficiently adapt vision-language models to
specific tasks, necessitate effective copyright protection. In this paper, we
investigate model copyright protection by auditing whether suspicious
third-party models incorporate protected soft prompts. While this can be viewed
as a special case of model ownership auditing, our analysis shows that existing
techniques are ineffective due to prompt learning's unique characteristics.
Non-intrusive auditing is inherently prone to false positives when independent
models share similar data distributions with victim models. Intrusive
approaches also fail: backdoor methods designed for CLIP cannot embed
functional triggers, while extending traditional DNN backdoor techniques to
prompt learning suffers from harmfulness and ambiguity challenges. We find that
these failures in intrusive auditing stem from the same fundamental reason:
watermarking operates within the same decision space as the primary task yet
pursues opposing objectives. Motivated by these findings, we propose sequential
watermarking for soft prompts (SWAP), which implants watermarks into a
different and more complex space. SWAP encodes watermarks through a specific
order of defender-specified out-of-distribution classes, inspired by the
zero-shot prediction capability of CLIP. This watermark, which is embedded in a
more complex space, keeps the original prediction label unchanged, making it
less opposed to the primary task. We further design a hypothesis-test-guided
verification protocol for SWAP and provide theoretical analyses of success
conditions. Extensive experiments on 11 datasets demonstrate SWAP's
effectiveness, harmlessness, and robustness against potential adaptive attacks.

</details>


### [9] [P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models](https://arxiv.org/abs/2511.04716)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.CR

TL;DR: 本文首次系统研究针对认知诊断模型(CDM)的成员推理攻击(MIA)，提出利用平台可解释性特征的灰盒威胁模型，并开发基于配置文件的MIA框架，显著优于传统黑盒攻击。


<details>
  <summary>Details</summary>
Motivation: 认知诊断模型在现代智能教育平台中广泛使用，但训练数据涉及敏感学生信息，存在隐私风险。目前成员推理攻击在其他领域已有研究，但在CDM领域的应用仍为空白，其隐私风险尚未量化。

Method: 提出新颖的灰盒威胁模型，利用平台的可解释性特征（如通过雷达图暴露的内部知识状态向量），开发基于配置文件的MIA(P-MIA)框架，结合模型最终预测概率和暴露的内部知识状态向量作为特征。

Result: 在三个真实数据集上对主流CDM进行广泛实验，结果显示灰盒攻击显著优于标准黑盒基线。同时成功将P-MIA作为审计工具评估机器遗忘技术的有效性并揭示其局限性。

Conclusion: CDM存在严重的隐私风险，暴露的内部知识状态向量可被准确逆向工程，形成有效的攻击面。P-MIA框架不仅攻击效果好，还能作为有效的隐私审计工具。

Abstract: Cognitive diagnosis models (CDMs) are pivotal for creating fine-grained
learner profiles in modern intelligent education platforms. However, these
models are trained on sensitive student data, raising significant privacy
concerns. While membership inference attacks (MIA) have been studied in various
domains, their application to CDMs remains a critical research gap, leaving
their privacy risks unquantified. This paper is the first to systematically
investigate MIA against CDMs. We introduce a novel and realistic grey box
threat model that exploits the explainability features of these platforms,
where a model's internal knowledge state vectors are exposed to users through
visualizations such as radar charts. We demonstrate that these vectors can be
accurately reverse-engineered from such visualizations, creating a potent
attack surface. Based on this threat model, we propose a profile-based MIA
(P-MIA) framework that leverages both the model's final prediction
probabilities and the exposed internal knowledge state vectors as features.
Extensive experiments on three real-world datasets against mainstream CDMs show
that our grey-box attack significantly outperforms standard black-box
baselines. Furthermore, we showcase the utility of P-MIA as an auditing tool by
successfully evaluating the efficacy of machine unlearning techniques and
revealing their limitations.

</details>


### [10] [Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models](https://arxiv.org/abs/2511.04728)
*Daniyal Ganiuly,Assel Smaiyl*

Main category: cs.CR

TL;DR: 本研究提出了可信度校准框架(TCF)，用于评估钓鱼邮件检测器的可靠性，包括校准性、一致性和鲁棒性三个维度，并通过实验证明GPT-4具有最强的整体可信度。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件持续威胁在线通信，虽然大语言模型在文本分类中表现出色，但在安全系统中部署时需要评估其可靠性，而不仅仅是基准性能。

Method: 引入可信度校准框架(TCF)，包含校准性、一致性和鲁棒性三个评估维度，集成到可信度校准指数(TCI)中，并辅以跨数据集稳定性(CDS)指标。在五个语料库上使用DeBERTa-v3-base、LLaMA-3-8B和GPT-4进行实验。

Result: 实验结果表明GPT-4具有最强的整体可信度，其次是LLaMA-3-8B和DeBERTa-v3-base。统计分析证实可靠性与原始准确率独立变化。

Conclusion: 该框架为基于LLM的钓鱼检测中评估模型可靠性建立了透明且可复现的基础，强调了在真实世界部署中信任感知评估的重要性。

Abstract: Phishing emails continue to pose a persistent challenge to online
communication, exploiting human trust and evading automated filters through
realistic language and adaptive tactics. While large language models (LLMs)
such as GPT-4 and LLaMA-3-8B achieve strong accuracy in text classification,
their deployment in security systems requires assessing reliability beyond
benchmark performance. To address this, this study introduces the
Trustworthiness Calibration Framework (TCF), a reproducible methodology for
evaluating phishing detectors across three dimensions: calibration,
consistency, and robustness. These components are integrated into a bounded
index, the Trustworthiness Calibration Index (TCI), and complemented by the
Cross-Dataset Stability (CDS) metric that quantifies stability of
trustworthiness across datasets. Experiments conducted on five corpora, such as
SecureMail 2025, Phishing Validation 2024, CSDMC2010, Enron-Spam, and Nazario,
using DeBERTa-v3-base, LLaMA-3-8B, and GPT-4 demonstrate that GPT-4 achieves
the strongest overall trust profile, followed by LLaMA-3-8B and
DeBERTa-v3-base. Statistical analysis confirms that reliability varies
independently of raw accuracy, underscoring the importance of trust-aware
evaluation for real-world deployment. The proposed framework establishes a
transparent and reproducible foundation for assessing model dependability in
LLM-based phishing detection.

</details>


### [11] [Zero Trust Security Model Implementation in Microservices Architectures Using Identity Federation](https://arxiv.org/abs/2511.04925)
*Rethish Nair Rajendran,Sathish Krishna Anumula,Dileep Kumar Rai,Sachin Agrawal*

Main category: cs.CR

TL;DR: 本文提出基于零信任安全模型的微服务生态系统解决方案，采用行业标准身份认证和授权技术，通过实验验证了其在缩小攻击面、统一策略执行和多域环境互操作性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 传统基于边界的防护策略无法有效保护分布式微服务架构中的动态交互，需要新的安全模型来应对微服务扩展带来的安全挑战。

Method: 构建基于行业标准认证授权技术的零信任安全框架，包括OpenID Connect、OAuth 2.0令牌交换和SPIFFE/SPIRE工作负载身份等技术。

Result: 实验评估表明该方案具有更小的攻击面、统一的策略执行能力以及跨多域环境的互操作性，能够满足DevSecOps标准要求。

Conclusion: 该研究为组织在云原生技术中实施零信任提供了严格路线图，确保合规性和互操作性，同时保障微服务部署的自动化、可扩展性和弹性。

Abstract: The microservice bombshells that have been linked with the microservice
expansion have altered the application architectures, offered agility and
scalability in terms of complexity in security trade-offs. Feeble legacy-based
perimeter-based policies are unable to offer safeguard to distributed workloads
and temporary interaction among and in between the services. The article itself
is a case on the need of the Zero Trust Security Model of micro services
ecosystem, particularly, the fact that human and workloads require identity
federation. It is proposed that the solution framework will be based on
industry-standard authentication and authorization and end-to-end trust
identity technologies, including Authorization and OpenID connect (OIDC),
Authorization and OAuth 2.0 token exchange, and Authorization and SPIFFE/ SPIRE
workload identities. Experimental evaluation is a unique demonstration of a
superior security position of making use of a smaller attack surface, harmony
policy enforcement, as well as interoperability across multi- domain
environments. The research results overlay that the federated identity combined
with the Zero Trust basics not only guarantee the rules relating to
authentication and authorization but also fully complies with the latest
DevSecOps standards of microservice deployment, which is automated, scaled, and
resilient. The current project offers a stringent roadmap to the organizations
that desire to apply Zero Trust in cloud-native technologies but will as well
guarantee adherence and interoperability.

</details>


### [12] [The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective](https://arxiv.org/abs/2511.04946)
*Lei Chen,Erci Xu,Yiming Sun,Shengyu Fan,Xianglong Deng,Guiming Shi,Guang Fan,Liang Kong,Yilan Zhu,Shoumeng Yan,Mingzhe Zhang*

Main category: cs.CR

TL;DR: 本文分析了存储I/O对全同态加密(FHE)应用性能的影响，发现存储I/O会显著降低ASIC和GPU的性能表现。


<details>
  <summary>Details</summary>
Motivation: 全同态加密能够对加密数据进行计算，极大增强了用户隐私保护，但FHE应用部署中的I/O挑战研究不足，需要分析存储I/O对FHE性能的影响。

Method: 分析存储I/O对FHE应用性能的影响，并总结现状中的关键经验教训。

Result: 存储I/O可使ASIC性能下降高达357倍，GPU性能下降高达22倍。

Conclusion: 存储I/O是FHE应用部署中的重要瓶颈，对硬件性能产生显著负面影响，需要进一步优化I/O性能以充分发挥FHE的潜力。

Abstract: Fully Homomorphic Encryption (FHE) allows computations to be performed on
encrypted data, significantly enhancing user privacy. However, the I/O
challenges associated with deploying FHE applications remains understudied. We
analyze the impact of storage I/O on the performance of FHE applications and
summarize key lessons from the status quo. Key results include that storage I/O
can degrade the performance of ASICs by as much as 357$\times$ and reduce GPUs
performance by up to 22$\times$.

</details>


### [13] [Chasing One-day Vulnerabilities Across Open Source Forks](https://arxiv.org/abs/2511.05097)
*Romain Lefeuvre,Charly Reux,Stefano Zacchiroli,Olivier Barais,Benoit Combemale*

Main category: cs.CR

TL;DR: 本文提出了一种新方法，通过跟踪代码提交级别的漏洞信息，在分叉仓库中检测一日漏洞。该方法利用Software Heritage档案中的公共代码全局图，自动识别未合并修复补丁的分叉项目中的潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前漏洞分析方法缺乏提交级别的粒度来跟踪分叉仓库中的漏洞引入和修复，导致一日漏洞可能被遗漏。分叉仓库在漏洞引入后但在修复前分叉，可能在原项目修复后仍保持易受攻击状态。

Method: 利用Software Heritage档案中的公共代码全局图，在提交级别传播漏洞信息并执行自动化影响分析。从7162个包含易受攻击提交的仓库开始，识别220万分叉，然后进行严格过滤以找到影响活跃GitHub分叉的漏洞-分叉对。

Result: 识别了356个漏洞-分叉对，手动评估了65对，发现了3个高严重性漏洞，证明了该方法的有效性和实用性。

Conclusion: 该方法能够有效检测分叉仓库中的一日漏洞，为开发者提供识别潜在安全风险的自动化工具，展示了在大型代码库中跟踪漏洞传播的实际价值。

Abstract: Tracking vulnerabilities inherited from third-party open-source components is
a well-known challenge, often addressed by tracing the threads of dependency
information. However, vulnerabilities can also propagate through forking: a
repository forked after the introduction of a vulnerability, but before it is
patched, may remain vulnerable in the fork well after being fixed in the
original project. Current approaches for vulnerability analysis lack the
commit-level granularity needed to track vulnerability introductions and fixes
across forks, potentially leaving one-day vulnerabilities undetected. This
paper presents a novel approach to help developers identify one-day
vulnerabilities in forked repositories. Leveraging the global graph of public
code, as captured by the Software Heritage archive, the approach propagates
vulnerability information at the commit level and performs automated impact
analysis. This enables automatic detection of forked projects that have not
incorporated fixes, leaving them potentially vulnerable. Starting from 7162
repositories that, according to OSV, include vulnerable commits in their
development histories, we identify 2.2 M forks, containing at least one
vulnerable commit. Then we perform a strict filtering, allowing us to find 356
___vulnerability, fork___ pairs impacting active and popular GitHub forks, we
manually evaluate 65 pairs, finding 3 high-severity vulnerabilities,
demonstrating the impact and applicability of this approach.

</details>


### [14] [TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS](https://arxiv.org/abs/2511.05100)
*Arslan Mumtaz,Mridula Singh*

Main category: cs.CR

TL;DR: TRICK是一种安全定位原语，通过结合LEO卫星的双向测距和GNSS广播信号的单向测距，提供与可验证多点定位相同的安全保障，但仅需与单个参考节点进行双向通信。


<details>
  <summary>Details</summary>
Motivation: 尽管GNSS在安全关键和高精度应用中广泛使用，但仍易受欺骗攻击。现有的加密增强措施（如伽利略系统的TESLA协议）无法缓解到达时间操纵问题。

Method: 提出TRICK方法，仅需与单个LEO参考卫星建立双向通信，结合多个广播信号（如GNSS）的单向测距，通过椭圆约束恢复可验证多点定位的保障。

Result: 通过详细分析证明，该方法能可靠检测欺骗攻击，同时仅增加可忽略的计算开销。

Conclusion: TRICK提供了一种基础设施需求最小、消息交换最少的安全定位解决方案，有效弥补了现有GNSS安全机制的不足。

Abstract: Global Navigation Satellite Systems (GNSS) provide Positioning, Navigation,
and Timing (PNT) information to over 4 billion devices worldwide. Despite its
pervasive use in safety critical and high precision applications, GNSS remains
vulnerable to spoofing attacks. Cryptographic enhancements, such as the use of
TESLA protocol in Galileo, to provide navigation message authentication do not
mitigate time of arrival manipulations. In this paper, we propose TRICK, a
primitive for secure positioning that closes this gap by introducing a
fundamentally new approach that only requires two way communications with a
single reference node along with multiple broadcast signals. Unlike classical
Verifiable Multilateration (VM), which requires establishing two way
communication with each reference nodes, our solution relies on only two
measurements with a trusted Low Earth Orbiting (LEO) satellite and combines
broadcast navigation signals. We rigorously prove that combining the LEO
satellite based two way range measurements and multiple one way ranges such as
from broadcast signals of GNSS into ellipsoidal constraint restores the same
guarantees as offered by VM whilst using minimal infrastructure and message
exchanges. Through detailed analysis, we show that our approach reliably
detects spoofing attempts while adding negligible computation overhead.

</details>


### [15] [PhantomFetch: Obfuscating Loads against Prefetcher Side-Channel Attacks](https://arxiv.org/abs/2511.05110)
*Xingzhi Zhang,Buyi Lv,Yimin Lu,Kai Bu*

Main category: cs.CR

TL;DR: PhantomFetch是首个保留预取功能且硬件无关的防御方案，通过混淆敏感加载效果来保护IP-stride预取器免受侧信道攻击，无需硬件修改。


<details>
  <summary>Details</summary>
Motivation: IP-stride预取器被用于侧信道攻击泄露秘密，但直接禁用会牺牲预取加速效果，现有防御方案需要硬件修改。

Method: 通过混淆受害者秘密相关加载的敏感效果，直接破坏训练预取器条目与受害者秘密相关加载之间的可利用耦合关系。

Result: 实验结果显示PhantomFetch能够以可忽略的开销保护IP-stride预取器。

Conclusion: PhantomFetch提供了一种硬件无关的防御方案，既能保护安全又能保留预取性能优势，适用于现成设备。

Abstract: The IP-stride prefetcher has recently been exploited to leak secrets through
side-channel attacks. It, however, cannot be simply disabled for security with
prefetching speedup as a sacrifice. The state-of-the-art defense tries to
retain the prefetching effect by hardware modification. In this paper, we
present PhantomFetch as the first prefetching-retentive and hardware-agnostic
defense. It avoids potential remanufacturing cost and enriches applicability to
off-the-shelf devices. The key idea is to directly break the exploitable
coupling between trained prefetcher entries and the victim's secret-dependent
loads by obfuscating the sensitive load effects of the victim. The experiment
results show that PhantomFetch can secure the IP-stride prefetcher with only
negligible overhead.

</details>


### [16] [Confidentiality in a Card-Based Protocol Under Repeated Biased Shuffles](https://arxiv.org/abs/2511.05111)
*Do Hyun Kim,Ahmet Cetinkaya*

Main category: cs.CR

TL;DR: 本文分析了Bert den Boer的五卡戏法协议中的保密性问题，发现由于玩家无意识的行为导致随机切牌存在偏差，从而造成信息泄露。


<details>
  <summary>Details</summary>
Motivation: 五卡戏法协议作为安全两方计算方法，理论上应保证玩家选择的比特不被对方知晓，但实际执行中玩家的切牌行为存在偏差，破坏了协议的保密性。

Method: 通过概率分析计算条件概率来量化信息泄露，并利用马尔可夫链的特征结构推导出需要重复执行偏差随机切牌的次数界限。

Result: 发现非均匀的洗牌分布为恶意玩家提供了猜测对方选择的机会，通过分析得出了减少信息泄露所需的重复切牌次数。

Conclusion: 协议的实际执行中存在保密性风险，需要通过多次重复切牌来降低信息泄露，并可将该方法推广到恶意玩家执行洗牌的场景。

Abstract: In this paper, we provide a probabilistic analysis of the confidentiality in
a card-based protocol. We focus on Bert den Boer's original Five Card Trick to
develop our approach. Five Card Trick was formulated as a secure two-party
computation method, where two players use colored cards with identical backs to
calculate the logical AND operation on the bits that they choose. In this
method, the players first arrange the cards privately, and then shuffle them
through a random cut. Finally, they reveal the shuffled arrangement to
determine the result of the operation. An unbiased random cut is essential to
prevent players from exposing their chosen bits to each other. However, players
typically choose to move cards within the deck even though not moving any cards
should be equally likely. This unconscious behavior results in a biased,
nonuniform shuffling-distribution in the sense that some arrangements of cards
are slightly more probable after the cut. Such a nonuniform distribution
creates an opportunity for a malicious player to gain advantage in guessing the
other player's choice. We provide the conditional probabilities of such guesses
as a way to quantify the information leakage. Furthermore, we utilize the
eigenstructure of a Markov chain to derive tight bounds on the number of times
the biased random cuts must be repeated to reduce the leakage to an acceptable
level. We also discuss the generalization of our approach to the setting where
shuffling is conducted by a malicious player.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression](https://arxiv.org/abs/2511.04684)
*Yuchao Qin,Anjunyi Fan,Bonan Yan*

Main category: cs.AR

TL;DR: RAS是一个硬件架构系统，将rANS算法集成到无损压缩流水线中，通过消除关键瓶颈实现高效压缩。该系统采用多通道组织、预测引导解码和两阶段rANS更新等技术，显著提升了压缩速度。


<details>
  <summary>Details</summary>
Motivation: 数据中心处理海量数据需要高效的无损压缩，但基于概率模型的新方法通常计算速度较慢。为了解决这个问题，需要开发能够加速rANS算法的硬件架构。

Method: RAS将rANS核心与概率生成器耦合，使用BF16格式存储分布并转换为固定点域。采用两阶段rANS更新和字节级重归一化减少逻辑成本和内存流量，预测引导解码路径推测性地缩小CDF搜索窗口。多通道组织可扩展吞吐量并实现细粒度时钟门控。

Result: 在图像工作负载上，RTL模拟原型相比Python rANS基线实现了121.2倍编码和70.9倍解码加速，平均解码器二分搜索步骤从7.00减少到3.15（约减少55%）。与神经概率模型配对时，RAS保持比传统编解码器更高的压缩比，并优于CPU/GPU rANS实现。

Conclusion: RAS为快速神经无损压缩提供了一种实用方法，通过硬件加速显著提升了rANS算法的性能，同时保持了高压缩比。

Abstract: Data centers handle vast volumes of data that require efficient lossless
compression, yet emerging probabilistic models based methods are often
computationally slow. To address this, we introduce RAS, the Range Asymmetric
Numeral System Acceleration System, a hardware architecture that integrates the
rANS algorithm into a lossless compression pipeline and eliminates key
bottlenecks. RAS couples an rANS core with a probabilistic generator, storing
distributions in BF16 format and converting them once into a fixed-point domain
shared by a unified division/modulo datapath. A two-stage rANS update with
byte-level re-normalization reduces logic cost and memory traffic, while a
prediction-guided decoding path speculatively narrows the cumulative
distribution function (CDF) search window and safely falls back to maintain
bit-exactness. A multi-lane organization scales throughput and enables
fine-grained clock gating for efficient scheduling. On image workloads, our
RTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a
Python rANS baseline, reducing average decoder binary-search steps from 7.00 to
3.15 (approximately 55% fewer). When paired with neural probability models, RAS
sustains higher compression ratios than classical codecs and outperforms
CPU/GPU rANS implementations, offering a practical approach to fast neural
lossless compression.

</details>


### [18] [Eliminating the Hidden Cost of Zone Management in ZNS SSDs](https://arxiv.org/abs/2511.04687)
*Teona Bagashvili,Tarikul Islam Papon,Subhadeep Sarkar,Manos Athanassoulis*

Main category: cs.AR

TL;DR: SilentZNS是一种新的ZNS SSD区域映射和管理方法，通过动态分配资源解决传统ZNS实现中的设备级写入放大、磨损增加和主机I/O干扰问题。


<details>
  <summary>Details</summary>
Motivation: 当前ZNS实现存在设备级写入放大、增加磨损和与主机I/O干扰的问题，主要原因是固定物理区域和全区域操作导致过多物理写入。

Method: 提出SilentZNS，采用灵活的区块分配方案，允许任意区块集合分配给区域，添加必要约束确保磨损均衡和读取性能，避免区域重置时的虚拟写入。

Result: SilentZNS消除了虚拟写入负担达20倍，降低设备级写入放大86%（在10%区域占用时），减少总体磨损达76.9%，工作负载执行速度提升3.7倍。

Conclusion: SilentZNS通过动态区域分配和优化写入策略，有效解决了ZNS SSD的关键性能问题，显著提升了设备性能和寿命。

Abstract: Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput
and low-latency storage by eliminating device-side garbage collection. They
expose storage as append-only zones that give the host applications direct
control over data placement. However, current ZNS implementations suffer from
(a) device-level write amplification (DLWA), (b) increased wear, and (c)
interference with host I/O due to zone mapping and management. We identify two
primary design decisions as the main cause: (i) fixed physical zones and (ii)
full-zone operations that lead to excessive physical writes. We propose
SilentZNS, a new zone mapping and management approach that addresses the
aforementioned limitations by on-the-fly allocating available resources to
zones, while minimizing wear, maintaining parallelism, and avoiding unnecessary
writes at the device-level. SilentZNS is a flexible zone allocation scheme that
departs from the traditional logical-to-physical zone mapping and allows for
arbitrary collections of blocks to be assigned to a zone. We add the necessary
constraints to ensure wear-leveling and state-of-the-art read performance, and
use only the required blocks to avoid dummy writes during zone reset. We
implement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that
it eliminates the undue burden of dummy writes by up to 20x, leading to lower
DLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up
to 3.7x faster workload execution.

</details>


### [19] [MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars](https://arxiv.org/abs/2511.04798)
*Matheus Farias,Wanghley Martins,H. T. Kung*

Main category: cs.AR

TL;DR: MDM是一种后训练DNN权重映射技术，通过优化有源忆阻器布局来减少忆阻计算内存交叉阵列中的寄生电阻非理想性，提高模拟计算精度。


<details>
  <summary>Details</summary>
Motivation: 寄生电阻限制了交叉阵列效率，迫使将DNN矩阵映射到小型交叉阵列瓦片中，这会降低CIM加速效果，增加数字同步、ADC转换、延迟和芯片面积开销。

Method: 利用位级结构化稀疏性，从密度较高的低阶侧输入激活，并根据曼哈顿距离重新排序行，将有源单元重新定位到受寄生电阻影响较小的区域。

Result: 在ImageNet-1k上的DNN模型中，MDM将非理想性因子降低高达46%，在ResNet中模拟失真下的准确率平均提高3.6%。

Conclusion: MDM提供了一种轻量级、空间感知的方法来扩展CIM DNN加速器。

Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)
weight mapping technique for memristive bit-sliced compute-in-memory (CIM)
crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar
tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring
digital synchronization before the next layer. At this granularity, designers
either deploy many small crossbars in parallel or reuse a few sequentially-both
increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement.
Exploiting bit-level structured sparsity, it feeds activations from the denser
low-order side and reorders rows according to the Manhattan distance,
relocating active cells toward regions less affected by PR and thus lowering
the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and
improves accuracy under analog distortion by an average of 3.6% in ResNets.
Overall, it provides a lightweight, spatially informed method for scaling CIM
DNN accelerators.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Marionette: Data Structure Description and Management for Heterogeneous Computing](https://arxiv.org/abs/2511.04853)
*Nuno dos Santos Fernandes,Pedro Tomás,Nuno Roma,Frank Winklmeier,Patricia Conde-Muíño*

Main category: cs.DC

TL;DR: Marionette是一个C++17库，旨在解决大型面向对象C++代码库在异构平台（如GPU）上硬件加速的挑战，通过解耦数据布局与接口描述，提供高效、可移植的数据结构定义。


<details>
  <summary>Details</summary>
Motivation: 适应大型面向对象C++代码库进行硬件加速极具挑战性，特别是在针对GPU等异构平台时。现有方法难以平衡效率、可移植性和现有代码兼容性。

Method: 设计Marionette库，在编译时解耦数据布局与接口描述，支持多种内存管理策略，提供跨设备的高效数据传输和转换，同时通过接口增强功能保持与现有代码的兼容性。

Result: Marionette实现了灵活、高效且可移植的数据结构定义，具有最小的运行时开销，并通过CUDA案例研究证明了其效率和灵活性。

Conclusion: Marionette为大型C++代码库在异构平台上的硬件加速提供了一种有效的解决方案，平衡了性能、可移植性和代码兼容性需求。

Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might
be extremely challenging, particularly when targeting heterogeneous platforms
such as GPUs. Marionette is a C++17 library designed to address this by
enabling flexible, efficient, and portable data structure definitions. It
decouples data layout from the description of the interface, supports multiple
memory management strategies, and provides efficient data transfers and
conversions across devices, all of this with minimal runtime overhead due to
the compile-time nature of its abstractions. By allowing interfaces to be
augmented with arbitrary functions, Marionette maintains compatibility with
existing code and offers a streamlined interface that supports both
straightforward and advanced use cases. This paper outlines its design, usage,
and performance, including a CUDA-based case study demonstrating its efficiency
and flexibility.

</details>


### [21] [GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters](https://arxiv.org/abs/2511.05067)
*Giuseppe Esposito,Juan-David Guerrero-Balaguera,Josie Esteban Rodriguez Condia,Matteo Sonza Reorda,Marco Barbiero,Rossella Fortuna*

Main category: cs.DC

TL;DR: 本研究结合在线遥测参数和硬件性能计数器来评估不同应用程序在GPU上引起的压力，用于预测可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 持续工作负载会给GPU组件带来显著压力，导致潜在故障并影响计算结果的正确性，因此需要评估应用程序引起的压力以预测可靠性问题。

Method: 结合在线遥测参数和硬件性能计数器，重点关注测量吞吐量、已发出指令数量和停滞事件的性能计数器。

Result: 实验结果表明，通过结合遥测数据和性能计数器可以估计并行工作负载引起的压力，这些计数器揭示了目标工作负载在资源使用方面的效率。

Conclusion: 通过结合遥测数据和性能计数器能够有效评估GPU工作负载引起的压力，为可靠性预测提供重要依据。

Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers
and high-performance computing (HPC) systems, enabling the fast execution of
compute-intensive applications, such as Convolutional Neural Networks (CNNs).
However, sustained workloads can impose significant stress on GPU components,
raising reliability concerns due to potential faults that corrupt the
intermediate application computations, leading to incorrect results. Estimating
the stress induced by an application is thus crucial to predict reliability
(with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online
telemetry parameters and hardware performance counters to assess GPU stress
induced by different applications. The experimental results indicate the stress
induced by a parallel workload can be estimated by combining telemetry data and
Performance Counters that reveal the efficiency in the resource usage of the
target workload. For this purpose the selected performance counters focus on
measuring the i) throughput, ii) amount of issued instructions and iii) stall
events.

</details>
