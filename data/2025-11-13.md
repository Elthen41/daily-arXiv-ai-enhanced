<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Automated Hardware Trojan Insertion in Industrial-Scale Designs](https://arxiv.org/abs/2511.08703)
*Yaroslav Popryho,Debjit Pal,Inna Partin-Vaisband*

Main category: cs.CR

TL;DR: 本文提出了一种自动化、可扩展的方法，用于在工业规模网表中生成硬件木马(HT)类似模式，以压力测试检测工具而不改变用户可见功能。该方法通过解析门级设计、探索罕见区域和应用函数保持图变换来合成模仿隐蔽HT统计特征的触发-负载对。


<details>
  <summary>Details</summary>
Motivation: 工业SoC包含数百万个网表和连接边，使得在真实设计上进行HT检测器评估既必要又困难。公开基准测试仍然显著较小且手工制作，而发布真正恶意RTL会带来道德和操作风险。

Method: 该方法包括三个步骤：(i)将大型门级设计解析为连接图，(ii)使用SCOAP可测试性指标探索罕见区域，(iii)应用参数化、函数保持的图变换来合成模仿隐蔽HT统计足迹的触发-负载对。

Result: 在本文生成的基准测试上评估时，代表性的最先进图学习模型未能检测到木马。

Conclusion: 该框架通过提供可重复的挑战实例，弥合了学术电路与现代SoC之间的评估差距，推进了安全研究而无需分享逐步攻击指令。

Abstract: Industrial Systems-on-Chips (SoCs) often comprise hundreds of thousands to millions of nets and millions to tens of millions of connectivity edges, making empirical evaluation of hardware-Trojan (HT) detectors on realistic designs both necessary and difficult. Public benchmarks remain significantly smaller and hand-crafted, while releasing truly malicious RTL raises ethical and operational risks. This work presents an automated and scalable methodology for generating HT-like patterns in industry-scale netlists whose purpose is to stress-test detection tools without altering user-visible functionality. The pipeline (i) parses large gate-level designs into connectivity graphs, (ii) explores rare regions using SCOAP testability metrics, and (iii) applies parameterized, function-preserving graph transformations to synthesize trigger-payload pairs that mimic the statistical footprint of stealthy HTs. When evaluated on the benchmarks generated in this work, representative state-of-the-art graph-learning models fail to detect Trojans. The framework closes the evaluation gap between academic circuits and modern SoCs by providing reproducible challenge instances that advance security research without sharing step-by-step attack instructions.

</details>


### [2] [Channel-Robust RFF for Low-Latency 5G Device Identification in SIMO Scenarios](https://arxiv.org/abs/2511.08902)
*Yingjie Sun,Guyue Li,Hongfu Chou,Aiqun Hu*

Main category: cs.CR

TL;DR: 提出了一种基于多天线信号的射频指纹提取技术LLDR，通过计算共时信道频率响应的对数线性差值比来解决多径问题，无需多时间点采样，显著降低了识别延迟。


<details>
  <summary>Details</summary>
Motivation: 5G超低延迟通信对设备识别提出了严格的时序要求，现有加密方案增加计算开销导致延迟增加，而传统射频指纹识别在多径信道下准确性受损，现有抗多径方法需要反馈或多时间点处理，引入额外信令延迟。

Method: 使用多接收天线的共时信道频率响应，计算对数线性差值比(LLDR)来保留区分性射频指纹特征；将频带分割为子带，在每个子带内单独计算LLDR以克服对最小信道变化的依赖。

Result: 在20路径信道、信噪比20dB条件下，对30个用户设备的识别准确率达到96.13%；使用Roofline模型评估理论延迟，空中接口延迟为0.491ms，满足URLLC延迟要求。

Conclusion: 所提出的LLDR方案能够有效解决多径信道下的射频指纹识别问题，在保持高识别准确率的同时显著降低延迟，满足5G超可靠低延迟通信的要求。

Abstract: Ultra-low latency, the hallmark of fifth-generation mobile communications (5G), imposes exacting timing demands on identification as well. Current cryptographic solutions introduce additional computational overhead, which results in heightened identification delays. Radio frequency fingerprint (RFF) identifies devices at the physical layer, blocking impersonation attacks while significantly reducing latency. Unfortunately, multipath channels compromise RFF accuracy, and existing channel-resilient methods demand feedback or processing across multiple time points, incurring extra signaling latency. To address this problem, the paper introduces a new RFF extraction technique that employs signals from multiple receiving antennas to address multipath issues without adding latency. Unlike single-domain methods, the Log-Linear Delta Ratio (LLDR) of co-temporal channel frequency responses (CFRs) from multiple antennas is employed to preserve discriminative RFF features, eliminating multi-time sampling and reducing acquisition time. To overcome the challenge of the reliance on minimal channel variation, the frequency band is segmented into sub-bands, and the LLDR is computed within each sub-band individually. Simulation results indicate that the proposed scheme attains a 96.13% identification accuracy for 30 user equipments (UEs) within a 20-path channel under a signal-to-noise ratio (SNR) of 20 dB. Furthermore, we evaluate the theoretical latency using the Roofline model, resulting in the air interface latency of 0.491 ms, which satisfies ultra-reliable and low-latency communications (URLLC) latency requirements.

</details>


### [3] [iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification](https://arxiv.org/abs/2511.08905)
*Zixun Xiong,Gaoyi Wu,Qingyang Yu,Mingyu Derek Ma,Lingfeng Yao,Miao Pan,Xiaojiang Du,Hao Wang*

Main category: cs.CR

TL;DR: iSeal是一种针对LLM知识产权保护的指纹方法，能够在模型窃贼完全控制推理过程的情况下进行可靠验证，通过将独特特征注入模型和外部模块，结合纠错机制和相似性验证策略，抵抗验证时攻击。


<details>
  <summary>Details</summary>
Motivation: 由于LLM训练成本高昂，保护其知识产权变得至关重要。现有指纹方法在验证过程中容易受到攻击，当模型窃贼完全控制LLM推理过程时，这些方法会失效。

Method: 提出iSeal方法，将独特特征注入模型和外部模块，采用纠错机制和基于相似性的验证策略，抵抗基于合谋的指纹遗忘和响应操纵等验证时攻击。

Result: 在12个LLM上对抗10多种攻击，iSeal实现了100%的指纹成功率，而基线方法在遗忘和响应操纵攻击下均失败。

Conclusion: iSeal是首个在模型窃贼端到端控制可疑LLM时仍能可靠验证的指纹方法，通过理论分析和实证结果证明了其有效性。

Abstract: Given the high cost of large language model (LLM) training from scratch, safeguarding LLM intellectual property (IP) has become increasingly crucial. As the standard paradigm for IP ownership verification, LLM fingerprinting thus plays a vital role in addressing this challenge. Existing LLM fingerprinting methods verify ownership by extracting or injecting model-specific features. However, they overlook potential attacks during the verification process, leaving them ineffective when the model thief fully controls the LLM's inference process. In such settings, attackers may share prompt-response pairs to enable fingerprint unlearning or manipulate outputs to evade exact-match verification. We propose iSeal, the first fingerprinting method designed for reliable verification when the model thief controls the suspected LLM in an end-to-end manner. It injects unique features into both the model and an external module, reinforced by an error-correction mechanism and a similarity-based verification strategy. These components are resistant to verification-time attacks, including collusion-based fingerprint unlearning and response manipulation, backed by both theoretical analysis and empirical results. iSeal achieves 100 percent Fingerprint Success Rate (FSR) on 12 LLMs against more than 10 attacks, while baselines fail under unlearning and response manipulations.

</details>


### [4] [DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks](https://arxiv.org/abs/2511.08985)
*Yunfei Yang,Xiaojun Chen,Yuexin Xuan,Zhendong Zhao,Xin Zhao,He Li*

Main category: cs.CR

TL;DR: 本文提出了DeepTracer，一种针对模型窃取攻击的鲁棒水印框架，通过新颖的水印样本构建方法和同类耦合损失约束，使攻击者在窃取主要功能时不可避免地学习隐藏的水印任务。


<details>
  <summary>Details</summary>
Motivation: 现有模型水印技术在面对模型窃取攻击时容易被移除，导致模型所有者难以有效验证被盗模型的版权。本文分析了当前水印方法在模型窃取场景下失败的根本原因，并探索潜在解决方案。

Method: DeepTracer框架采用新颖的水印样本构建方法和同类耦合损失约束，创建水印任务与主要任务之间的高度耦合模型。同时提出有效的水印样本过滤机制，精心选择用于模型所有权验证的水印关键样本。

Result: 在多个数据集和模型上的广泛实验表明，该方法在防御各种模型窃取攻击和水印攻击方面超越了现有方法，实现了最新的有效性和鲁棒性。

Conclusion: DeepTracer通过建立水印任务与主要任务的高度耦合关系，有效提升了模型水印在对抗模型窃取攻击时的鲁棒性，为模型版权保护提供了新的解决方案。

Abstract: Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.

</details>


### [5] [MedHE: Communication-Efficient Privacy-Preserving Federated Learning with Adaptive Gradient Sparsification for Healthcare](https://arxiv.org/abs/2511.09043)
*Farjana Yesmin*

Main category: cs.CR

TL;DR: MedHE框架结合自适应梯度稀疏化和CKKS同态加密，在医疗联邦学习中实现隐私保护的协作学习，显著减少通信开销同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 医疗联邦学习需要在资源受限的医疗机构之间提供强大的隐私保护，同时保持计算效率。

Method: 采用自适应梯度稀疏化与CKKS同态加密相结合的方法，引入带误差补偿的动态阈值机制进行top-k梯度选择。

Result: 实现97.5%的通信减少，在5次独立试验中达到89.5%±0.8%的准确率，通信量从每轮1277MB减少到32MB，性能与标准联邦学习相当(p=0.32)。

Conclusion: MedHE框架具有实际可行性，符合HIPAA合规要求，可扩展到100多个机构，为现实世界医疗部署提供了实用的隐私保护解决方案。

Abstract: Healthcare federated learning requires strong privacy guarantees while maintaining computational efficiency across resource-constrained medical institutions. This paper presents MedHE, a novel framework combining adaptive gradient sparsification with CKKS homomorphic encryption to enable privacy-preserving collaborative learning on sensitive medical data. Our approach introduces a dynamic threshold mechanism with error compensation for top-k gradient selection, achieving 97.5 percent communication reduction while preserving model utility. We provide formal security analysis under Ring Learning with Errors assumptions and demonstrate differential privacy guarantees with epsilon less than or equal to 1.0. Statistical testing across 5 independent trials shows MedHE achieves 89.5 percent plus or minus 0.8 percent accuracy, maintaining comparable performance to standard federated learning (p=0.32) while reducing communication from 1277 MB to 32 MB per training round. Comprehensive evaluation demonstrates practical feasibility for real-world medical deployments with HIPAA compliance and scalability to 100 plus institutions.

</details>


### [6] [Attack-Centric by Design: A Program-Structure Taxonomy of Smart Contract Vulnerabilities](https://arxiv.org/abs/2511.09051)
*Parsa Hedayatnia,Tina Tavakkoli,Hadi Amini,Mohammad Allahbakhsh,Haleh Amintoosi*

Main category: cs.CR

TL;DR: 本文提出了一个基于攻击的、程序结构导向的漏洞分类法，将Solidity漏洞统一为8个根本原因家族，为智能合约安全提供一致的词汇表和实用检查清单。


<details>
  <summary>Details</summary>
Motivation: 智能合约将高价值资产和复杂逻辑集中在小型不可变程序中，即使微小错误也可能导致重大损失。现有分类法和工具仍然分散，围绕症状而非结构原因组织。

Method: 引入基于攻击的、程序结构导向的分类法，将Solidity漏洞统一为8个根本原因家族：控制流、外部调用、状态完整性、算术安全、环境依赖、访问控制、输入验证和跨域协议假设。

Result: 每个家族通过简洁的Solidity示例、利用机制和缓解措施进行说明，并与静态、动态和基于学习的工具可观察的检测信号相关联。进一步将传统数据集映射到此分类法以揭示标签漂移和覆盖差距。

Conclusion: 该分类法提供了一致的词汇表和实用检查清单，使研究人员和从业者能够进行更可解释的检测、可重现的审计和结构化的安全教育。

Abstract: Smart contracts concentrate high value assets and complex logic in small, immutable programs, where even minor bugs can cause major losses. Existing taxonomies and tools remain fragmented, organized around symptoms such as reentrancy rather than structural causes. This paper introduces an attack-centric, program-structure taxonomy that unifies Solidity vulnerabilities into eight root-cause families covering control flow, external calls, state integrity, arithmetic safety, environmental dependencies, access control, input validation, and cross-domain protocol assumptions. Each family is illustrated through concise Solidity examples, exploit mechanics, and mitigations, and linked to the detection signals observable by static, dynamic, and learning-based tools. We further cross-map legacy datasets (SmartBugs, SolidiFI) to this taxonomy to reveal label drift and coverage gaps. The taxonomy provides a consistent vocabulary and practical checklist that enable more interpretable detection, reproducible audits, and structured security education for both researchers and practitioners.

</details>


### [7] [Improving Sustainability of Adversarial Examples in Class-Incremental Learning](https://arxiv.org/abs/2511.09088)
*Taifeng Liu,Xinjing Liu,Liangqiu Dong,Yang Liu,Yilong Yang,Zhuo Ma*

Main category: cs.CR

TL;DR: 本文提出SAE方法，旨在增强对抗样本在类增量学习(CIL)环境下的可持续性，解决现有对抗样本在模型更新后失效的问题。


<details>
  <summary>Details</summary>
Motivation: 当前对抗样本主要针对静态模型设计，但在类增量学习场景中，模型会不断更新，导致原有对抗样本因领域漂移而失效。

Method: 提出SAE方法，包含语义校正模块和过滤增强模块。语义校正模块利用视觉语言模型生成通用语义，并结合CIL模型校正AE语义优化方向；过滤增强模块识别具有目标类语义的非目标样本并进行增强，以稳定语义。

Result: 综合实验表明，SAE在类别数量增加9倍的情况下，平均性能优于基线方法31.28%。

Conclusion: SAE能有效提升对抗样本在类增量学习环境下的可持续性，显著优于现有方法。

Abstract: Current adversarial examples (AEs) are typically designed for static models. However, with the wide application of Class-Incremental Learning (CIL), models are no longer static and need to be updated with new data distributed and labeled differently from the old ones. As a result, existing AEs often fail after CIL updates due to significant domain drift. In this paper, we propose SAE to enhance the sustainability of AEs against CIL. The core idea of SAE is to enhance the robustness of AE semantics against domain drift by making them more similar to the target class while distinguishing them from all other classes. Achieving this is challenging, as relying solely on the initial CIL model to optimize AE semantics often leads to overfitting. To resolve the problem, we propose a Semantic Correction Module. This module encourages the AE semantics to be generalized, based on a visual-language model capable of producing universal semantics. Additionally, it incorporates the CIL model to correct the optimization direction of the AE semantics, guiding them closer to the target class. To further reduce fluctuations in AE semantics, we propose a Filtering-and-Augmentation Module, which first identifies non-target examples with target-class semantics in the latent space and then augments them to foster more stable semantics. Comprehensive experiments demonstrate that SAE outperforms baselines by an average of 31.28% when updated with a 9-fold increase in the number of classes.

</details>


### [8] [Differentially Private Rankings via Outranking Methods and Performance Data Aggregation](https://arxiv.org/abs/2511.09120)
*Luis Del Vasto-Terrientes*

Main category: cs.CR

TL;DR: 本文提出了一种将多准则决策排序方法与差分隐私相结合的方法，在排名问题中保护个体贡献的隐私，同时保持与真实排名的高度统计相关性。


<details>
  <summary>Details</summary>
Motivation: 随着多准则决策方法在动态和数据驱动领域的应用扩展，特别是在推荐系统中，敏感数据的处理和隐私保护变得至关重要。然而，隐私机制与多准则决策方法的集成仍然不足。

Method: 采用集成方法，将多准则决策排序方法与差分隐私相结合，通过预处理步骤将多个用户评估聚合为综合性能矩阵。

Result: 评估结果显示，真实排名与其匿名对应物之间存在强到非常强的统计相关性，同时确保了稳健的隐私参数保证。

Conclusion: 该方法成功地将隐私保护机制集成到多准则决策过程中，在保护个体隐私的同时保持了决策排名的有效性。

Abstract: Multiple-Criteria Decision Making (MCDM) is a sub-discipline of Operations Research that helps decision-makers in choosing, ranking, or sorting alternatives based on conflicting criteria. Over time, its application has been expanded into dynamic and data-driven domains, such as recommender systems. In these contexts, the availability and handling of personal and sensitive data can play a critical role in the decision-making process. Despite this increased reliance on sensitive data, the integration of privacy mechanisms with MCDM methods is underdeveloped. This paper introduces an integrated approach that combines MCDM outranking methods with Differential Privacy (DP), safeguarding individual contributions' privacy in ranking problems. This approach relies on a pre-processing step to aggregate multiple user evaluations into a comprehensive performance matrix. The evaluation results show a strong to very strong statistical correlation between the true rankings and their anonymized counterparts, ensuring robust privacy parameter guarantees.

</details>


### [9] [One Signature, Multiple Payments: Demystifying and Detecting Signature Replay Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2511.09134)
*Zexu Wang,Jiachi Chen,Zewei Lin,Wenqing Chen,Kaiwen Ning,Jianxing Yu,Yuming Feng,Yu Zhang,Weizhe Zhang,Zibin Zheng*

Main category: cs.CR

TL;DR: 本文首次对智能合约中的签名重放漏洞(SRV)进行实证研究，提出了基于大语言模型的自动检测工具LASiR，在15,383个涉及签名验证的合约中发现SRV广泛存在，受影响合约持有476万美元活跃资产。


<details>
  <summary>Details</summary>
Motivation: 智能合约中缺乏对签名使用条件的检查会导致重复验证，增加权限滥用风险，威胁合约资产安全。目前缺乏对签名重放漏洞的系统性研究。

Method: 设计了LASiR工具，利用大语言模型的语义理解能力辅助静态污点分析，识别签名重用行为，并通过符号执行进行路径可达性验证。

Result: 从37家区块链安全公司的1,419份审计报告中识别出108个详细SRV案例，分类为5种类型。在15,383个涉及签名验证的合约中，以太坊上使用签名的合约中有19.63%包含SRV，受影响合约持有476万美元资产。LASiR的F1分数达到87.90%。

Conclusion: 签名重放漏洞在智能合约中广泛存在且危害严重，LASiR结合大语言模型语义理解的方法能有效检测此类漏洞，显著提升检测性能。

Abstract: Smart contracts have significantly advanced blockchain technology, and digital signatures are crucial for reliable verification of contract authority. Through signature verification, smart contracts can ensure that signers possess the required permissions, thus enhancing security and scalability. However, lacking checks on signature usage conditions can lead to repeated verifications, increasing the risk of permission abuse and threatening contract assets. We define this issue as the Signature Replay Vulnerability (SRV). In this paper, we conducted the first empirical study to investigate the causes and characteristics of the SRVs. From 1,419 audit reports across 37 blockchain security companies, we identified 108 with detailed SRV descriptions and classified five types of SRVs. To detect these vulnerabilities automatically, we designed LASiR, which utilizes the general semantic understanding ability of Large Language Models (LLMs) to assist in the static taint analysis of the signature state and identify the signature reuse behavior. It also employs path reachability verification via symbolic execution to ensure effective and reliable detection. To evaluate the performance of LASiR, we conducted large-scale experiments on 15,383 contracts involving signature verification, selected from the initial dataset of 918,964 contracts across four blockchains: Ethereum, Binance Smart Chain, Polygon, and Arbitrum. The results indicate that SRVs are widespread, with affected contracts holding $4.76 million in active assets. Among these, 19.63% of contracts that use signatures on Ethereum contain SRVs. Furthermore, manual verification demonstrates that LASiR achieves an F1-score of 87.90% for detection. Ablation studies and comparative experiments reveal that the semantic information provided by LLMs aids static taint analysis, significantly enhancing LASiR's detection performance.

</details>


### [10] [Unveiling Hidden Threats: Using Fractal Triggers to Boost Stealthiness of Distributed Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2511.09252)
*Jian Wang,Hong Shen,Chan-Tong Lam*

Main category: cs.CR

TL;DR: 本文提出了一种基于分形特征的分布式后门攻击方法FTDBA，利用分形的自相似性增强子触发器的特征强度，显著减少了达到相同攻击强度所需的投毒数据量，并通过动态角度扰动机制提高隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式后门攻击通过将全局触发器分解为子触发器来提高隐蔽性，但这需要更多投毒数据来维持攻击强度，增加了暴露风险。为了解决这一缺陷，需要开发一种能够在减少投毒数据量的同时保持攻击强度的新方法。

Method: 提出FTDBA方法：1）利用分形自相似性增强子触发器特征强度；2）引入动态角度扰动机制，在训练阶段自适应调整扰动强度以平衡效率和隐蔽性。

Result: 实验结果显示：FTDBA仅需传统DBA方法62.4%的投毒数据量就能达到92.3%的攻击成功率，同时检测率降低22.8%，KL散度降低41.2%。

Conclusion: 本研究提出了一种低暴露、高效率的联邦后门攻击范式，并拓展了分形特征在对抗样本生成中的应用。

Abstract: Traditional distributed backdoor attacks (DBA) in federated learning improve stealthiness by decomposing global triggers into sub-triggers, which however requires more poisoned data to maintian the attck strength and hence increases the exposure risk. To overcome this defect, This paper proposes a novel method, namely Fractal-Triggerred Distributed Backdoor Attack (FTDBA), which leverages the self-similarity of fractals to enhance the feature strength of sub-triggers and hence significantly reduce the required poisoning volume for the same attack strength. To address the detectability of fractal structures in the frequency and gradient domains, we introduce a dynamic angular perturbation mechanism that adaptively adjusts perturbation intensity across the training phases to balance efficiency and stealthiness. Experiments show that FTDBA achieves a 92.3\% attack success rate with only 62.4\% of the poisoning volume required by traditional DBA methods, while reducing the detection rate by 22.8\% and KL divergence by 41.2\%. This study presents a low-exposure, high-efficiency paradigm for federated backdoor attacks and expands the application of fractal features in adversarial sample generation.

</details>


### [11] [Quantum Meet-in-the-Middle Attacks on Key-Length Extension Constructions](https://arxiv.org/abs/2511.09351)
*Min Liang,Ruihao Gao,Jiali Wu*

Main category: cs.CR

TL;DR: 本文提出了针对两种密钥长度扩展构造的量子中间相遇攻击：对2kTE的两种Q2模型攻击（基于量子爪查找和Grover算法），以及对3XCE的Q1模型攻击。还扩展了量子筛子中间攻击框架。


<details>
  <summary>Details</summary>
Motivation: 研究密钥长度扩展技术在量子计算模型下的安全性，评估现有KLE构造在量子攻击下的脆弱性。

Method: 使用量子中间相遇攻击和量子筛子中间攻击技术，结合量子爪查找算法和Grover算法，分析2kTE和3XCE构造的安全性。

Result: 2kTE在Q2模型下无法增强安全性，3XCE在Q1模型下可实现二次加速攻击，量子SITM攻击可扩展到更广泛的构造。

Conclusion: 密钥长度扩展技术在量子计算环境下存在安全风险，需要重新评估其设计以抵御量子攻击。

Abstract: Key-length extension (KLE) techniques provide a general approach to enhancing the security of block ciphers by using longer keys. There are mainly two classes of KLE techniques, cascade encryption and XOR-cascade encryption. This paper presents several quantum meet-in-the-middle (MITM) attacks against two specific KLE constructions.
  For the two-key triple encryption (2kTE), we propose two quantum MITM attacks under the Q2 model. The first attack, leveraging the quantum claw-finding (QCF) algorithm, achieves a time complexity of $O(2^{2κ/3})$ with $O(2^{2κ/3})$ quantum random access memory (QRAM). The second attack, based on Grover's algorithm, achieves a time complexity of $O(2^{κ/2})$ with $O(2^κ)$ QRAM. The latter complexity is nearly identical to Grover-based brute-force attack on the underlying block cipher, indicating that 2kTE does not enhance security under the Q2 model when sufficient QRAM resources are available.
  For the 3XOR-cascade encryption (3XCE), we propose a quantum MITM attack applicable to the Q1 model. This attack requires no QRAM and has a time complexity of $O(2^{(κ+n)/2})$ ($κ$ and $n$ are the key length and block length of the underlying block cipher, respectively.), achieving a quadratic speedup over classical MITM attack.
  Furthermore, we extend the quantum MITM attack to quantum sieve-in-the-middle (SITM) attack, which is applicable for more constructions. We present a general quantum SITM framework for the construction $ELE=E^2\circ L\circ E^1$ and provide specific attack schemes for three different forms of the middle layer $L$. The quantum SITM attack technique can be further applied to a broader range of quantum cryptanalysis scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: 本文提出了一种将无约束英语翻译成ASP程序的新方法，使用LLM和AMR图来生成完整的ASP程序以解决逻辑谜题。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多不熟悉编程语言的人需要与代码交互，需要一种方法让非专业人士能够使用自然语言描述逻辑问题并自动生成ASP程序。

Method: 使用LLM简化自然语言句子、识别关键词和生成简单事实，然后通过AMR图解析简化语言并系统生成ASP约束，最小化LLM的作用。

Result: 系统成功创建了完整的ASP程序来解决组合逻辑问题，展示了在逻辑谜题上的能力。

Conclusion: 这是创建轻量级、可解释的自然语言到复杂逻辑问题解决系统的重大第一步。

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


### [13] [Vector Symbolic Algebras for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.08747)
*Isaac Joffe,Chris Eliasmith*

Main category: cs.AI

TL;DR: 本文提出了一种基于向量符号代数(VSA)的认知合理ARC-AGI求解器，整合了系统1直觉和系统2推理，通过面向对象的程序合成方法解决ARC-AGI基准测试。


<details>
  <summary>Details</summary>
Motivation: 虽然人类能轻松解决ARC-AGI基准测试，但即使最先进的AI系统也难以应对。受神经科学和心理学中人类智能建模方法的启发，作者希望开发一个认知合理的求解器。

Method: 使用神经符号方法，基于向量符号代数(VSA)整合系统1直觉和系统2推理。采用面向对象的程序合成方法，利用VSA表示抽象对象、指导解决方案搜索，并实现样本高效的神经学习。

Result: 在ARC-AGI-1-Train上得分为10.8%，在ARC-AGI-1-Eval上得分为3.0%。在较简单的基准测试上表现优异：Sort-of-ARC得分为94.5%，1D-ARC得分为83.1%，后者以极低计算成本超越GPT-4。

Conclusion: 这是首个将VSA应用于ARC-AGI的研究，开发了迄今为止最认知合理的ARC-AGI求解器，为人工通用智能的发展提供了新思路。

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.

</details>


### [14] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 本文提出单向认知优化(UCO)方法，通过多轮交互式强化学习解决LLM作为智能导师时的动态适应问题，使用进度奖励和支架奖励来评估学生认知进步和识别最近发展区。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育场景中从答案提供者转向智能导师，但现有监督微调方法只能学习表面教学模式，缺乏动态适应能力。强化学习方法面临两个关键挑战：无法区分学生是否真正理解还是简单重复答案，以及无法实时感知学生认知状态变化。

Method: 提出UCO方法，采用多轮交互式强化学习范式，包含两个协同奖励函数：进度奖励捕捉学生认知进步，评估学生是否从困惑转向理解；支架奖励动态识别每个学生的最近发展区，鼓励教师在该区域内保持有效教学。

Result: 在BigMath和MathTutorBench基准测试中与11个基线模型比较，UCO模型在同等规模模型中表现最优，性能接近先进的闭源模型。

Conclusion: UCO方法有效解决了LLM作为智能导师时的动态适应问题，通过认知状态感知和最近发展区识别，显著提升了教学效果。

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [15] [Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds](https://arxiv.org/abs/2511.08892)
*Weihao Tan,Xiangyang Li,Yunhao Fang,Heyuan Yao,Shi Yan,Hao Luo,Tenglong Ao,Huihui Li,Hongbin Ren,Bairen Yi,Yujia Qin,Bo An,Libin Liu,Guang Shi*

Main category: cs.AI

TL;DR: Lumine是首个能够在3D开放世界中实时完成数小时复杂任务的通用智能体，采用端到端的人机交互范式，在Genshin Impact中训练后能完成5小时主线剧情，并具备零样本跨游戏泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂3D开放世界中完成长时间任务的通用智能体，解决现有方法在处理实时、长时程任务方面的局限性。

Method: 采用基于视觉语言模型的端到端架构，以5Hz处理原始像素并生成30Hz的键盘鼠标动作，仅在必要时进行推理调用，在Genshin Impact中进行训练。

Result: 成功完成Genshin Impact中5小时Mondstadt主线剧情，效率达到人类水平；在Wuthering Waves和Honkai: Star Rail中零样本完成100分钟和5小时任务，展示了强大的跨游戏泛化能力。

Conclusion: Lumine在多个不同世界和交互动态中表现出有效性，标志着在开放环境中开发通用智能体的重要进展。

Abstract: We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.

</details>


### [16] [A Research on Business Process Optimisation Model Integrating AI and Big Data Analytics](https://arxiv.org/abs/2511.08934)
*Di Liao,Ruijia Liang,Ziyi Ye*

Main category: cs.AI

TL;DR: 该研究构建了一个融合人工智能和大数据的业务流程优化模型，采用三层架构实现流程全生命周期的智能管理，通过实验验证显著提升了企业运营效率。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型的深入，业务流程优化已成为提升企业竞争力的关键，需要构建智能化的流程管理解决方案。

Method: 采用包含数据处理、AI算法和业务逻辑的三层架构模型，结合分布式计算和深度学习技术，实现实时流程监控和优化。

Result: 实验验证显示，模型缩短流程处理时间42%，提高资源利用率28%，降低运营成本35%，并在高并发负载下保持99.9%的可用性。

Conclusion: 研究成果对企业数字化转型具有重要理论和实践价值，为提升企业运营效率提供了新思路。

Abstract: With the deepening of digital transformation, business process optimisation has become the key to improve the competitiveness of enterprises. This study constructs a business process optimisation model integrating artificial intelligence and big data to achieve intelligent management of the whole life cycle of processes. The model adopts a three-layer architecture incorporating data processing, AI algorithms, and business logic to enable real-time process monitoring and optimization. Through distributed computing and deep learning techniques, the system can handle complex business scenarios while maintaining high performance and reliability. Experimental validation across multiple enterprise scenarios shows that the model shortens process processing time by 42%, improves resource utilisation by 28%, and reduces operating costs by 35%. The system maintained 99.9% availability under high concurrent loads. The research results have important theoretical and practical value for promoting the digital transformation of enterprises, and provide new ideas for improving the operational efficiency of enterprises.

</details>


### [17] [AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting](https://arxiv.org/abs/2511.08947)
*Xiaohan Zhang,Tian Gao,Mingyue Cheng,Bokai Pan,Ze Guo,Yaguo Liu,Xiaoyu Tao*

Main category: cs.AI

TL;DR: AlphaCast是一个人类智慧与大型语言模型智能协同推理的框架，将时间序列预测重新定义为交互过程，通过多源认知基础和生成推理优化两阶段实现更准确的预测。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测方法缺乏人类专家的交互、推理和适应性，限制了在复杂现实环境中的实用性。

Method: 采用两阶段框架：自动预测准备（构建多源认知基础）和生成推理与反思优化（集成统计特征、先验知识、上下文信息和预测策略，触发元推理循环）。

Result: 在短期和长期数据集上的广泛实验表明，AlphaCast在预测准确性方面持续优于最先进的基线方法。

Conclusion: AlphaCast通过人类智慧与LLM智能的协同推理，显著提升了时间序列预测的准确性和实用性。

Abstract: Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .

</details>


### [18] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: 本文提出通过结构化多智能体管道实现渐进、增量、顺序的搜索空间遍历来增强LLM推理能力，并通过递归精炼方法验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型表现出优秀的流畅性，但研究人员仍在努力提取更强的推理能力。本文基于搜索的LLM计算解释，旨在系统化理解LLM推理和优化。

Method: 采用递归精炼方法，包括自我批评、对抗性压力测试和整合关键反馈的迭代过程。设计了简单线性管道与复杂结构化管道的对比实验，使用基于RAG的美国开国元勋人物模型来回应当代政治问题。

Result: 复杂模型在所有九个测试案例中始终优于简单模型，平均仲裁者评分为88.3对71.7。复杂模型的论证在分析深度、结构细微差别和战略框架方面更优越。

Conclusion: 递归精炼是通过GIS搜索增强LLM推理的稳健架构特征，高质量推理是受控的增量搜索过程。

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [19] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: 提出Argus框架，通过持续监控和自适应响应来增强端到端自动驾驶系统的韧性，防止安全违规并提升驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在公共道路上部署时面临各种驾驶危险，需要具备持续监控危险和自适应响应安全违规的能力，以在复杂驾驶场景中保持稳健驾驶行为。

Method: 提出运行时韧性导向框架Argus，持续监控自动驾驶系统生成的轨迹，当EGO车辆被认为不安全时，通过危险缓解器无缝接管控制。

Result: 与三个先进端到端自动驾驶系统集成测试，Argus有效提升系统韧性，平均提高驾驶评分达150.30%，防止64.38%的违规，额外时间开销很小。

Conclusion: Argus框架能够有效增强自动驾驶系统的韧性，显著改善驾驶性能并预防安全违规，具有实际应用价值。

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [20] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: 本文综述了下一代自动驾驶车辆在紧急服务中的优化策略，重点分析了从传统强化学习向扩散模型增强强化学习和大型语言模型辅助上下文学习的转变，以解决样本效率低和动态适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆有望通过更快、更安全、更高效的响应来彻底改变紧急服务，但传统强化学习方法在动态紧急场景中存在样本效率低和适应性不足的局限性。

Method: 分析了从传统强化学习到扩散模型增强强化学习的转变（通过合成数据生成增强策略鲁棒性），以及新兴的大型语言模型辅助上下文学习范式（提供轻量级、可解释的替代方案）。

Result: 通过综述自动驾驶智能、扩散模型增强强化学习和LLM辅助上下文学习的最新技术，为理解下一代自主应急响应系统提供了关键框架。

Conclusion: 从生成式AI的角度，本文为下一代自主应急响应系统的发展提供了重要见解，展示了不同AI方法在提升自动驾驶车辆紧急响应能力方面的潜力和权衡。

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [21] [OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.09092)
*Zezhen Ding,Zhen Tan,Jiheng Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: OR-R1是一个数据高效的训练框架，用于自动化优化建模和求解，通过监督微调和测试时组相对策略优化两阶段设计，仅需1/10的数据量就能达到67.7%的平均求解准确率，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的优化建模方法需要大量标注或合成数据，成本高且可扩展性差，需要开发数据效率更高的自动化解决方案。

Method: 采用两阶段训练框架：首先使用监督微调从有限标注数据中学习问题建模和代码生成的推理模式，然后通过测试时组相对策略优化提升能力和一致性。

Result: OR-R1仅需1/10的合成数据就能达到67.7%的平均求解准确率，比ORLM高出4.2%，在仅100个合成样本情况下仍能超越ORLM 2.4%。TGRPO带来3.1%-6.4%的额外准确率提升。

Conclusion: OR-R1提供了一个稳健、可扩展且成本效益高的自动化OR优化问题建模和求解方案，显著降低了工业应用的专家知识和数据门槛。

Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.

</details>


### [22] [Efficient Reasoning via Reward Model](https://arxiv.org/abs/2511.09158)
*Yuhao Wang,Xiaopeng Li,Cheng Gong,Ziru Liu,Suiyun Zhang,Rui Liu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种训练简洁性奖励模型（CRM）的管道，通过新颖的简洁性奖励函数（CRF）来解决大型推理模型中的过度思考问题，在提高推理效率的同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如DeepSeek-R1和OpenAI o1）经常生成冗长的推理步骤，这种现象被称为过度思考，显著增加了计算成本。现有的长度惩罚方法存在长度崩溃和训练崩溃的问题。

Method: 提出了训练简洁性奖励模型（CRM）的管道来评估推理路径的简洁性，并引入了简洁性奖励函数（CRF），该函数明确将结果奖励与简洁性评分相关联。

Result: 在五个数学基准数据集上的实验表明，该方法在Qwen2.5-7B上实现了8.1%的准确率提升和19.9%的响应token长度减少，并且在Llama和Mistral等其他LLM上也有良好的泛化能力。

Conclusion: 该方法通过理论分析和实验验证，证明了新奖励函数在方差减少和改进收敛特性方面的优越性，能够同时促进更有效和更高效的推理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.

</details>


### [23] [Perspectives on a Reliability Monitoring Framework for Agentic AI Systems](https://arxiv.org/abs/2511.09178)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.AI

TL;DR: 提出了一个双层可靠性监控框架来解决智能AI系统在操作中的可靠性挑战，包括用于检测新输入的分布外检测层和揭示内部操作的AI透明度层。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统虽然具有潜力，但在高风险领域（如医疗保健或流程工业）中可靠性不足，存在意外行为的风险，需要缓解技术。

Method: 基于智能AI系统的特性推导其主要可靠性挑战，提出双层可靠性监控框架：分布外检测层用于识别新输入，AI透明度层用于揭示内部操作。

Result: 该框架为人类操作员提供了决策支持，以判断输出是否可能不可靠并进行干预。

Conclusion: 该框架为开发缓解技术提供了基础，以减少操作中不确定性可靠性带来的风险。

Abstract: The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.

</details>


### [24] [MedFuse: Multiplicative Embedding Fusion For Irregular Clinical Time Series](https://arxiv.org/abs/2511.09247)
*Yi-Hsien Hsieh,Ta-Jung Chien,Chun-Kai Huang,Shao-Hua Sun,Che Lin*

Main category: cs.AI

TL;DR: MedFuse是一个用于不规则临床时间序列的框架，通过乘法融合模块解决传统加性嵌入的局限性，在多个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的临床时间序列具有不规则性、异步采样、缺失值和异质特征动态等特性。现有的嵌入策略通常通过加法操作结合特征身份和值嵌入，限制了捕捉值依赖特征交互的能力。

Method: 提出MedFuse框架，核心是MuFuse（乘法嵌入融合）模块，通过乘法调制融合值和特征嵌入，在保留特征特定信息的同时建模跨特征的高阶依赖关系。

Result: 在涵盖重症和慢性护理的三个真实世界数据集上的实验表明，MedFuse在关键预测任务上始终优于最先进的基线方法。学习表示的分析进一步证明乘法融合增强了表达能力并支持跨数据集预训练。

Conclusion: MedFuse为建模不规则临床时间序列提供了一个可推广的方法，通过乘法融合机制有效解决了传统加性嵌入的局限性。

Abstract: Clinical time series derived from electronic health records (EHRs) are inherently irregular, with asynchronous sampling, missing values, and heterogeneous feature dynamics. While numerical laboratory measurements are highly informative, existing embedding strategies usually combine feature identity and value embeddings through additive operations, which constrains their ability to capture value-dependent feature interactions. We propose MedFuse, a framework for irregular clinical time series centered on the MuFuse (Multiplicative Embedding Fusion) module. MuFuse fuses value and feature embeddings through multiplicative modulation, preserving feature-specific information while modeling higher-order dependencies across features. Experiments on three real-world datasets covering both intensive and chronic care show that MedFuse consistently outperforms state-of-the-art baselines on key predictive tasks. Analysis of the learned representations further demonstrates that multiplicative fusion enhances expressiveness and supports cross-dataset pretraining. These results establish MedFuse as a generalizable approach for modeling irregular clinical time series.

</details>


### [25] [HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting](https://arxiv.org/abs/2511.09275)
*Minlan Shao,Zijian Zhang,Yili Wang,Yiwei Dai,Xu Shen,Xin Wang*

Main category: cs.AI

TL;DR: HyperD是一个用于交通预测的新框架，通过将交通数据解耦为周期性和残差分量来处理复杂的时空依赖性和多尺度模式。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临两个关键挑战：复杂的空间依赖性和多尺度周期模式与不规则波动的共存。现有方法难以有效处理这些复杂因素。

Method: 提出HyperD框架，将交通数据解耦为周期性和残差分量。周期性分量使用混合周期表示模块处理，残差分量使用频率感知残差表示模块在频域建模。引入双视图对齐损失确保语义分离。

Result: 在四个真实交通数据集上的实验表明，HyperD实现了最先进的预测精度，同时在干扰下具有更好的鲁棒性和更高的计算效率。

Conclusion: HyperD通过解耦周期性和非周期性模式，有效解决了交通预测中的复杂时空依赖问题，为智能交通系统提供了更准确的预测能力。

Abstract: Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.

</details>


### [26] [From Model Training to Model Raising - A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development](https://arxiv.org/abs/2511.09287)
*Roland Aydin,Christian Cyron,Steve Bachelor,Ashton Anderson,Robert West*

Main category: cs.AI

TL;DR: 本文提出从"模型训练"转向"模型培养"的新范式，将价值观对齐从模型开发初期就融入其中，通过重新设计训练语料库来实现深度价值系统构建。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法只在核心能力建立后才进行价值观对齐，导致模型容易被误导且缺乏深层次价值体系。在LLM能力开始超越人类的背景下，需要从根本上改变这种训练方式。

Method: 重新设计训练语料库：采用第一人称视角重构训练数据、将信息重新情境化为生活经验、模拟社会互动、以及搭建训练数据的顺序框架。

Result: 预期这种训练语料库的重新设计将导致从第一个训练token开始就形成对价值观的早期承诺，使知识、技能和价值观在本质上更难分离。

Conclusion: 在大型语言模型能力开始在许多任务中超越人类能力的生态系统中，这种从模型开发初期就融入价值观对齐的"模型培养"范式是至关重要的需求。

Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.

</details>


### [27] [Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI](https://arxiv.org/abs/2511.09325)
*Stine Beltoft,Lukas Galke*

Main category: cs.AI

TL;DR: 本文主张开发专门为定性研究设计的AI系统，以解决当前通用AI工具在定性研究中存在的偏见、不透明、不可复现和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要推动了定量方法的发展，但定性研究这一对意义建构和全面科学理解至关重要的维度却被忽视。定性研究者对采用AI持犹豫态度，只能依赖有诸多局限的通用工具。

Method: 主张从零开始构建专门用于定性研究的AI系统，这些系统必须具有透明性、可复现性和隐私友好性。通过回顾近期文献，探讨如何通过增强定性能力来改进现有的自动化科学发现流程。

Result: 识别了安全定性AI可以推动多学科和混合方法研究的关键机会，展示了如何将定性维度整合到自动化发现流程中。

Conclusion: 需要开发专门针对定性研究的AI系统，这些系统应该具备透明、可复现和隐私友好的特性，以弥补当前AI在定性研究领域的不足，推动多学科研究的全面发展。

Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.

</details>


### [28] [The 2025 Planning Performance of Frontier Large Language Models](https://arxiv.org/abs/2511.09378)
*Augusto B. Corrêa,André G. Pereira,Jendrik Seipp*

Main category: cs.AI

TL;DR: 评估了2025年三个前沿大语言模型（DeepSeek R1、Gemini 2.5 Pro、GPT-5）在PDDL规划和推理任务上的表现，发现GPT-5在标准PDDL领域与LAMA规划器表现相当，但在混淆测试中所有模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 评估前沿大语言模型在端到端规划任务中的推理能力，了解它们与专业规划器的性能差距。

Method: 使用国际规划竞赛学习赛道中的PDDL领域和任务子集，对三个LLM进行测试，包括标准PDDL和混淆版本，并与LAMA规划器对比。

Result: GPT-5在标准PDDL任务中解决率与LAMA竞争；所有LLM在混淆任务中性能下降，但降幅小于之前模型报告；相比前代LLM有显著改进。

Conclusion: 前沿LLM在规划任务上取得重大进展，与规划器的性能差距在具有挑战性的基准测试中缩小。

Abstract: The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.

</details>


### [29] [What We Don't C: Representations for scientific discovery beyond VAEs](https://arxiv.org/abs/2511.09433)
*Brian Rogers,Micah Bowles,Chris J. Lintott,Steve Croft*

Main category: cs.AI

TL;DR: 提出一种基于潜在流匹配和分类器自由引导的新方法，通过明确分离条件信息和残差表示来解耦潜在子空间，实现在高维数据中访问有意义的特征。


<details>
  <summary>Details</summary>
Motivation: 在科学发现中访问学习表示中的信息至关重要，特别是在高维领域。需要一种机制来分析、控制和重新利用潜在表示，以便进行科学探索。

Method: 基于潜在流匹配与分类器自由引导，明确分离条件信息与残差表示，实现潜在子空间的解耦。

Result: 在三个实验（合成2D高斯玩具问题、彩色MNIST和Galaxy10天文数据集）中验证了该方法能够访问高维数据的有意义特征。

Conclusion: 该方法为分析、控制和重新利用潜在表示提供了简单而强大的机制，为使用生成模型进行科学探索开辟了途径。

Abstract: Accessing information in learned representations is critical for scientific discovery in high-dimensional domains. We introduce a novel method based on latent flow matching with classifier-free guidance that disentangles latent subspaces by explicitly separating information included in conditioning from information that remains in the residual representation. Across three experiments -- a synthetic 2D Gaussian toy problem, colored MNIST, and the Galaxy10 astronomy dataset -- we show that our method enables access to meaningful features of high dimensional data. Our results highlight a simple yet powerful mechanism for analyzing, controlling, and repurposing latent representations, providing a pathway toward using generative models for scientific exploration of what we don't capture, consider, or catalog.

</details>


### [30] [CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?](https://arxiv.org/abs/2511.09483)
*Peiyu Li,Xiaobao Huang,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: CrochetBench是一个评估多模态大语言模型在钩针编织领域进行细粒度、低层次程序推理能力的基准测试，强调从描述转向实际操作，要求模型识别针法、选择结构适当的指令并生成可编译的钩针程序。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注高层次描述或视觉问答，缺乏对低层次程序推理能力的评估。CrochetBench旨在填补这一空白，通过钩针编织这一需要精确程序执行的创造性领域来评估模型的实际操作能力。

Method: 采用CrochetPARADE DSL作为中间表示，支持结构验证和通过执行进行功能评估。基准测试涵盖针法分类、指令基础、自然语言到DSL翻译和图像到DSL翻译等任务。

Result: 在所有任务中，当评估从表面相似性转向可执行正确性时，模型性能急剧下降，暴露了长距离符号推理和3D感知程序合成的局限性。

Conclusion: CrochetBench为评估多模态模型的程序能力提供了新视角，并揭示了在现实世界创造性领域中表面理解与可执行精度之间的差距。

Abstract: We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at https://github.com/Peiyu-Georgia-Li/crochetBench.

</details>


### [31] [Consensus Sampling for Safer Generative AI](https://arxiv.org/abs/2511.09493)
*Adam Tauman Kalai,Yael Tauman Kalai,Or Zamir*

Main category: cs.AI

TL;DR: 提出了一种基于多模型聚合的AI安全方法，通过共识采样算法从k个模型中选择最安全的s个子集，在模型间达成足够共识时输出结果，否则弃权，从而增强安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI安全方法主要依赖检查模型输出或激活，但某些风险无法仅通过检查检测。需要一种架构无关的补充方法来增强安全性。

Method: 使用共识采样算法，利用k个生成模型，通过计算输出概率，选择最安全的s个模型子集，在模型间达成足够共识时输出结果，否则弃权。

Result: 该方法能够实现与最安全s个模型平均风险相当的安全性，并在足够多模型安全且达成共识时限制弃权概率。

Conclusion: 该方法提供了一种新的模型无关的AI安全方法，通过放大集合中未知安全子集的安全保证来创建单个可靠模型。

Abstract: Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given $k$ models and a prompt, achieves risk competitive with the average risk of the safest $s$ of the $k$ models, where $s$ is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.

</details>


### [32] [Fundamentals of Physical AI](https://arxiv.org/abs/2511.09497)
*Vahid Salehi*

Main category: cs.AI

TL;DR: 本文从科学和系统角度阐述了物理人工智能的基本原理，旨在为智能系统的物理体现、感知、行动、学习和情境敏感性建立统一的理论框架。


<details>
  <summary>Details</summary>
Motivation: 传统AI依赖符号处理和数据驱动模型，而物理AI将智能视为身体、环境和经验真实交互中涌现的现象，需要建立新的理论基础。

Method: 提出六个基本原则：具身性、感官感知、运动行动、学习、自主性和情境敏感性，这些原则构成封闭控制循环，能量、信息、控制和情境持续交互。

Result: 建立了物理AI的理论模型，通过康复机器人案例展示了六个原则在实际系统中的相互作用，证明智能来自物理体验而非抽象计算。

Conclusion: 物理AI将智能理解为物理具身过程，学习是智能体与环境结构耦合的变化，这一范式转变使系统能够从物理经验而非数据库中生成意义。

Abstract: This work will elaborate the fundamental principles of physical artificial intelligence (Physical AI) from a scientific and systemic perspective. The aim is to create a theoretical foundation that describes the physical embodiment, sensory perception, ability to act, learning processes, and context sensitivity of intelligent systems within a coherent framework. While classical AI approaches rely on symbolic processing and data driven models, Physical AI understands intelligence as an emergent phenomenon of real interaction between body, environment, and experience. The six fundamentals presented here are embodiment, sensory perception, motor action, learning, autonomy, and context sensitivity, and form the conceptual basis for designing and evaluating physically intelligent systems. Theoretically, it is shown that these six principles do not represent loose functional modules but rather act as a closed control loop in which energy, information, control, and context are in constant interaction. This circular interaction enables a system to generate meaning not from databases, but from physical experience, a paradigm shift that understands intelligence as an physical embodied process. Physical AI understands learning not as parameter adjustment, but as a change in the structural coupling between agents and the environment. To illustrate this, the theoretical model is explained using a practical scenario: An adaptive assistant robot supports patients in a rehabilitation clinic. This example illustrates that physical intelligence does not arise from abstract calculation, but from immediate, embodied experience. It shows how the six fundamentals interact in a real system: embodiment as a prerequisite, perception as input, movement as expression, learning as adaptation, autonomy as regulation, and context as orientation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: 本文提出了首个通过MLIR中的OpenMP目标指令实现选择性代码卸载到FPGA的方法，结合OpenMP方言和HLS方言，提供可移植的FPGA编译流程。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律放缓，FPGA等异构计算平台在加速HPC工作负载方面日益受到关注，需要更灵活的FPGA加速方法。

Method: 将MLIR OpenMP方言与高级综合(HLS)方言结合，利用现有MLIR构建模块，支持任何MLIR兼容前端(如Flang)，通过OpenMP指令手动优化卸载内核。

Result: 成功实现了基于指令的FPGA加速，显著减少了开发工作量，展示了MLIR生态系统的可组合性优势。

Conclusion: 该方法为基于指令的FPGA加速建立了一个灵活且可扩展的路径，完全集成在MLIR生态系统中。

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [34] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: 数据中心通过能源资源解耦电力容量与电网负荷来创造灵活性，优化分布和管理可显著降低电网碳排放，经济上具有可行性但需要电网干预。


<details>
  <summary>Details</summary>
Motivation: AI和云数据中心的高能耗加剧了碳足迹问题，数据中心恒定电力需求与波动性可再生能源的矛盾阻碍电网脱碳，需要数据中心灵活性来改善可再生能源消纳。

Method: 定义和计算数据中心负荷解耦的功率和能量需求，评估设计的分布和管理方法，包括站点差异和电网合作考虑。

Result: 优化分布可实现98%的潜在电网碳减排，仅需70%的总解耦需求；电网合作管理比单向信息共享实现1.4倍的碳减排；经济上平均收益大于本地成本。

Conclusion: 数据中心解耦是降低碳足迹的有效方法，优化分布和电网合作能显著提升效果，经济可行但存在站点差异，需要电网干预确保公平性。

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [35] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: 本文评估了四种主要云提供商（AWS、Azure、GCP、OCI）在虚拟化云基础设施中的HPC风格CPU性能和成本，使用SPEC ACCEL套件的OpenMP工作负载，比较了Intel、AMD和ARM实例类型在按需和一年折扣定价下的表现。


<details>
  <summary>Details</summary>
Motivation: 评估云环境中不同CPU架构（Intel、AMD、ARM）的性能和成本效益，帮助用户根据工作负载优先级（原始速度或成本最小化）做出明智的实例类型选择决策。

Method: 使用SPEC ACCEL套件的OpenMP工作负载子集，在AWS、Azure、GCP和OCI四个主要云提供商的Intel、AMD和ARM通用实例类型上进行测试，比较按需和一年折扣定价模式下的性能和成本。

Result: AWS在所有三种实例类型中始终提供最短运行时间，但收费较高；OCI是所有CPU系列中最经济的选择，但运行工作负载较慢；Azure通常表现中等的性能和成本；GCP从Intel切换到AMD时性能显著提升，但其ARM实例比自己的AMD产品慢两倍以上且更昂贵；AWS的ARM实例在运行时间上比其Intel和AMD实例快达49%。

Conclusion: 实例选择和云提供商选择在运行时间和价格上都会产生显著差异，工作负载优先级（原始速度或成本最小化）应指导实例类型决策。

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [36] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: 本文提出了企业级隐私保护联邦学习框架APPFL的设计愿景，旨在解决从本地原型到分布式部署的扩展性问题，支持跨异构计算环境的无缝部署。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在科学领域具有重要应用价值，但现有框架在可扩展性、隐私保护和从原型到部署的无缝过渡方面存在挑战，需要构建企业级的解决方案。

Method: 基于APPFL框架开发经验，提出包含五个关键能力的框架设计：可扩展本地仿真、无缝过渡到部署、跨异构基础设施部署、多级抽象、全面隐私安全保护。

Result: 提出了一个能够桥接研究原型和企业级部署的联邦学习框架架构设计，支持从个人设备到云集群和HPC系统的分布式部署。

Conclusion: 该框架旨在实现可扩展、可靠且隐私保护的科学AI，为联邦学习从研究到实际应用的转化提供支持。

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [37] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG是一个软件框架，通过将MIG的一对一分配模型改为一对多分配模型，并支持跨MIG实例的主机共享内存集合操作，解决了GPU集群中的资源碎片化和利用率低下问题，无需硬件修改即可显著提升集群效率。


<details>
  <summary>Details</summary>
Motivation: 多租户GPU集群存在利用率低下的问题，虽然NVIDIA MIG提供了硬件级隔离，但其硬件刚性和传统一对一分配模型导致严重的资源碎片化和集群范围利用率低下。

Method: Flex-MIG采用纯软件方法，将MIG的一对一分配模型替换为一对多分配模型，并实现跨MIG实例的主机共享内存集合操作，无需硬件修改。

Result: Flex-MIG消除了需要排空的重配置操作，减少了碎片化，在不同跟踪数据下将makespan提高了高达17%。

Conclusion: 将MIG的操作模型重新设计为软件协调层可以显著提高集群效率，Flex-MIG展示了通过软件创新解决硬件限制的可行性。

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [38] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: 论文提出了一种新的用户空间同步调度方法CES，用于解决协程等用户空间任务调度中的性能瓶颈问题，相比现有方法能带来3-8倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言广泛支持协程用于高并行或异步应用，用户空间同步避免了重量级系统调用，但现有用户空间同步原语仍采用内核级调度视角，导致关键路径上不必要的延迟，限制了吞吐量。

Method: 开发了Combine-and-Exchange Scheduling (CES)方法，确保竞争临界区保持在同一个执行线程上，同时将可并行工作均匀分布到其他线程。

Result: 该方法可应用于多种现有语言和库，在应用基准测试中实现3倍性能提升，在微基准测试中实现8倍性能提升。

Conclusion: CES方法重新思考了用户空间任务调度，通过优化同步机制显著提升了协程等用户空间调度任务的性能。

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [39] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: 本文提出了循环内存保护(CMP)方法，这是一种无协调的队列实现，在保持严格FIFO语义、无界容量和锁自由进度的同时恢复了简单性。CMP通过有界保护窗口提供实用的回收保证，在高并发场景下性能优于现有锁自由队列1.72-4倍。


<details>
  <summary>Details</summary>
Motivation: 现有锁自由队列实现在并发环境下为了防范ABA、释放后使用等风险而引入复杂的协调机制，这些机制往往掩盖了队列本身的设计。在AI时代，训练和推理流水线涉及数百到数千个并发线程，保护协调开销变得主导且远超基本队列操作本身。

Method: 提出循环内存保护(CMP)方法，采用有界保护窗口提供实用的回收保证，避免无限保护带来的复杂性。该方法保持严格FIFO语义、无界容量和锁自由进度，同时无需协调机制。

Result: 通过线性化和有界回收分析证明了严格FIFO和安全性。实验显示CMP在高竞争场景下性能优于最先进的锁自由队列1.72-4倍，并能扩展到数百个线程。

Conclusion: 高度并发的队列可以回归其基本简单性而无需削弱队列语义，CMP方法在保持严格FIFO的同时显著提升了性能和可扩展性。

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [40] [SPADA: A Spatial Dataflow Architecture Programming Language](https://arxiv.org/abs/2511.09447)
*Lukas Gianinazzi,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.DC

TL;DR: SPADA是一个针对空间数据流架构的编程语言，通过抽象底层架构细节，提供对数据放置、数据流模式和异步操作的精确控制，显著简化了编程复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的FPGA和CGRA编程模型主要关注循环调度，但忽视了空间数据流架构的独特能力，特别是高效的数据流管理和复杂路由控制，导致编程这些架构仍然具有挑战性。

Method: 提出了SPADA编程语言，引入严格的数据流语义框架来定义路由正确性、数据竞争和死锁，并设计了针对Cerebras CSL的多级降低编译器。

Result: SPADA使开发者能够用比CSL少6-8倍的代码表达复杂的并行模式，在三个数量级上实现了接近理想的弱扩展性能。

Conclusion: SPADA通过统一空间数据流架构的编程模型，推动了这些新兴高性能计算平台的理论基础和实践可用性。

Abstract: Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.
  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.
  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.

</details>


### [41] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: 本文使用CSP进程代数对Python联邦学习测试平台中的第三个通用算法（TDM通信）进行了形式化验证，分为两个阶段：构建CSP模型和自动验证安全性与活性属性。


<details>
  <summary>Details</summary>
Motivation: 该Python联邦学习测试平台提供了三种通用算法，前两种已在先前论文中通过CSP形式化验证，本文旨在验证第三个算法（TDM通信）的正确性。

Method: 采用CSP进程代数方法，首先构建忠实反映实际Python代码的CSP模型，然后使用模型检查器PAT自动验证算法的死锁自由性（安全性）和成功终止性（活性）。

Result: 模型检查器PAT成功证明了第三个通用算法的正确性，验证了其安全性和活性属性。

Conclusion: 通过形式化验证方法，成功证明了Python联邦学习测试平台中TDM通信算法的正确性，为边缘系统的联邦学习算法提供了可靠的理论保证。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>
