<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.AI](#cs.AI) [Total: 27]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow是一个用于MoE推理的运行时系统，通过自适应专家预取和缓存感知路由来优化内存受限环境下的推理性能，将模型停滞时间降低到基线的0.1%以下。


<details>
  <summary>Details</summary>
Motivation: 现代GPU内存容量有限限制了大型语言模型的扩展，传统MoE推理方法由于每层独立选择活跃专家导致频繁的参数传输，引入显著延迟，现有跨层预测策略缺乏对不同硬件平台和工作负载的适应性。

Method: 结合自适应专家预取和缓存感知路由，利用传输带宽、参数维度和模型反馈信号等运行时统计信息持续调整专家激活预测范围，采用混合跨层预测方案融合预门控信息和中间计算状态来预测未来专家需求。

Result: 评估显示ExpertFlow将模型停滞时间降低到基线的0.1%以下，有效减少缓存未命中和专家交换引入的延迟。

Conclusion: ExpertFlow能够在严格内存约束下优化MoE推理，通过自适应预取决策与实际使用行为对齐，显著提升推理效率。

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [2] [Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies](https://arxiv.org/abs/2510.25878)
*Pavel Hubáček,Jan Václavek,Michelle Yeo*

Main category: cs.CR

TL;DR: 该论文研究了以加密货币（如比特币）为抵押的法币贷款安全协议，提出了基于可信仲裁的有限托管协议，并进行了博弈论分析。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币作为金融资产的重要性日益上升，其应用范围从投机对象扩展到更接近标准金融工具（如贷款）。本研究旨在开发支持以加密货币为抵押的法币贷款的安全协议。

Method: 提出了基于可信仲裁的有限托管协议，仅依赖可信仲裁来确保贷款安全，并对协议进行了博弈论分析。

Result: 开发了支持加密货币抵押贷款的安全协议框架，通过有限托管和可信仲裁机制确保交易安全。

Conclusion: 该研究为加密货币抵押贷款提供了可行的安全协议解决方案，并指出了未来研究的多个有趣方向。

Abstract: The rising importance of cryptocurrencies as financial assets pushed their
applicability from an object of speculation closer to standard financial
instruments such as loans. In this work, we initiate the study of secure
protocols that enable fiat-denominated loans collateralized by cryptocurrencies
such as Bitcoin. We provide limited-custodial protocols for such loans relying
only on trusted arbitration and provide their game-theoretical analysis. We
also highlight various interesting directions for future research.

</details>


### [3] [A Critical Roadmap to Driver Authentication via CAN Bus: Dataset Review, Introduction of the Kidmose CANid Dataset (KCID), and Proof of Concept](https://arxiv.org/abs/2510.25856)
*Brooke Elizabeth Kidmose,Andreas Brasen Kidmose,Cliff C. Zou*

Main category: cs.CR

TL;DR: 本文介绍了Kidmose CANid数据集（KCID），解决了现有驾驶员指纹识别数据集的局限性，提供了原始CAN总线数据、人口统计信息，并开发了基于CAN总线的驾驶员认证防盗系统原型。


<details>
  <summary>Details</summary>
Motivation: 现代车辆虽然配备了传统安全措施，但仍面临未经授权使用和盗窃的风险。犯罪分子利用CAN总线系统的漏洞绕过认证机制，社交媒体趋势也扩大了未成年人偷车兜风的问题。现有驾驶员指纹识别数据集存在严重局限性，需要更好的数据集来支持驾驶员认证防盗系统的开发。

Method: 引入Kidmose CANid数据集（KCID），包含16名驾驶员在4辆车上的原始CAN总线数据，提供人口统计信息和日常驾驶及固定路线数据。开发驾驶员认证防盗框架，并在单板计算机上实现概念验证原型，通过实际道路试验验证系统可行性。

Result: 通过未改装乘用车的实际道路试验，证明了基于CAN总线的驾驶员认证防盗系统的实际可行性。KCID数据集为研究人员提供了开发稳健、可部署的驾驶员认证系统所需的数据和方法基础。

Conclusion: 这项工作不仅提供了改进的数据集，还展示了基于CAN总线的驾驶员认证防盗系统的实际应用潜力，并为驾驶员分析、机械异常检测、年轻驾驶员监控和受损驾驶检测等多样化应用奠定了基础。

Abstract: Modern vehicles remain vulnerable to unauthorized use and theft despite
traditional security measures including immobilizers and keyless entry systems.
Criminals exploit vulnerabilities in Controller Area Network (CAN) bus systems
to bypass authentication mechanisms, while social media trends have expanded
auto theft to include recreational joyriding by underage drivers. Driver
authentication via CAN bus data offers a promising additional layer of
defense-in-depth protection, but existing open-access driver fingerprinting
datasets suffer from critical limitations including reliance on decoded
diagnostic data rather than raw CAN traffic, artificial fixed-route
experimental designs, insufficient sampling rates, and lack of demographic
information.
  This paper provides a comprehensive review of existing open-access driver
fingerprinting datasets, analyzing their strengths and limitations to guide
practitioners in dataset selection. We introduce the Kidmose CANid Dataset
(KCID), which addresses these fundamental shortcomings by providing raw CAN bus
data from 16 drivers across four vehicles, including essential demographic
information and both daily driving and controlled fixed-route data. Beyond
dataset contributions, we present a driver authentication anti-theft framework
and implement a proof-of-concept prototype on a single-board computer. Through
live road trials with an unaltered passenger vehicle, we demonstrate the
practical feasibility of CAN bus-based driver authentication anti-theft
systems. Finally, we explore diverse applications of KCID beyond driver
authentication, including driver profiling for insurance and safety
assessments, mechanical anomaly detection, young driver monitoring, and
impaired driving detection. This work provides researchers with both the data
and methodological foundation necessary to develop robust, deployable driver
authentication systems...

</details>


### [4] [FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X](https://arxiv.org/abs/2510.25932)
*Soufiane Essahli,Oussama Sarsar,Imane Fouad,Anas Motii,Ahmed Bentajer*

Main category: cs.CR

TL;DR: FakeZero是一个完全客户端、跨平台的浏览器扩展，能够在用户滚动时实时标记Facebook和X平台上的不可靠帖子。所有计算都在本地进行，不泄露个人数据。


<details>
  <summary>Details</summary>
Motivation: 社交媒体以空前速度传播信息，加速了错误信息的传播并威胁公共话语。需要一种保护隐私的解决方案来识别不可靠内容。

Method: 采用三阶段训练课程：基线微调、域自适应训练（增强焦点损失、对抗增强和后训练量化）。使用DistilBERT-Quant和TinyBERT-Quant模型在本地运行。

Result: 在239,000条帖子的数据集上，DistilBERT-Quant模型（67.6 MB）达到97.1%宏F1、97.4%准确率和0.996 AUROC，中位延迟约103毫秒。TinyBERT-Quant变体（14.7 MB）保持95.7%宏F1和96.1%准确率，延迟降至约40毫秒。

Conclusion: FakeZero证明在严格资源预算下实现高质量假新闻检测是可行的，可作为政策制定者遏制错误信息传播的有价值工具，并为研究人员收集大规模假新闻数据集提供可能。

Abstract: Social platforms distribute information at unprecedented speed, which in turn
accelerates the spread of misinformation and threatens public discourse. We
present FakeZero, a fully client-side, cross-platform browser extension that
flags unreliable posts on Facebook and X (formerly Twitter) while the user
scrolls. All computation, DOM scraping, tokenisation, Transformer inference,
and UI rendering run locally through the Chromium messaging API, so no personal
data leaves the device.FakeZero employs a three-stage training curriculum:
baseline fine-tuning and domain-adaptive training enhanced with focal loss,
adversarial augmentation, and post-training quantisation. Evaluated on a
dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%
macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of
approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant
variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to
14.7 MB and lowering latency to approximately 40 ms, showing that high-quality
fake-news detection is feasible under tight resource budgets with only modest
performance loss.By providing inline credibility cues, the extension can serve
as a valuable tool for policymakers seeking to curb the spread of
misinformation across social networks. With user consent, FakeZero also opens
the door for researchers to collect large-scale datasets of fake news in the
wild, enabling deeper analysis and the development of more robust detection
techniques.

</details>


### [5] [SoK: Honeypots & LLMs, More Than the Sum of Their Parts?](https://arxiv.org/abs/2510.25939)
*Robert A. Bridges,Thomas R. Mitchell,Mauricio Muñoz,Ted Henriksson*

Main category: cs.CR

TL;DR: 本文对基于大语言模型（LLM）的蜜罐研究进行了首次系统性综述，提出了蜜罐检测向量的分类法，总结了LLM蜜罐的典型架构和评估趋势，并展望了自主、自改进欺骗系统的未来发展路径。


<details>
  <summary>Details</summary>
Motivation: 解决蜜罐设计中长期存在的矛盾：实现高保真欺骗与低操作风险。尽管自2022年底以来相关研究激增，但进展有限，领域缺乏对新兴架构模式、核心挑战和评估范式的统一理解。

Method: 采用知识系统化（SoK）方法，系统调研三个关键研究领域：1）蜜罐检测向量分类法；2）LLM蜜罐文献综合，识别典型架构和评估趋势；3）蜜罐日志分析的演进路径分析。

Result: 建立了蜜罐检测向量的结构化分类，识别了LLM蜜罐的典型架构模式，总结了关键评估趋势，并绘制了从简单数据缩减到自动化情报生成的蜜罐日志分析演进路径。

Conclusion: 该技术的真正潜力在于创建自主、自改进的欺骗系统，以应对新兴的智能自动化攻击者威胁，为未来研究提供了前瞻性路线图。

Abstract: The advent of Large Language Models (LLMs) promised to resolve the
long-standing paradox in honeypot design: achieving high-fidelity deception
with low operational risk. However, despite a flurry of research since late
2022, progress has been incremental, and the field lacks a cohesive
understanding of the emerging architectural patterns, core challenges, and
evaluation paradigms. To fill this gap, this Systematization of Knowledge (SoK)
paper provides the first comprehensive overview of this new domain. We survey
and systematize three critical, intersecting research areas: first, we provide
a taxonomy of honeypot detection vectors, structuring the core problems that
LLM-based realism must solve; second, we synthesize the emerging literature on
LLM-honeypots, identifying a canonical architecture and key evaluation trends;
and third, we chart the evolutionary path of honeypot log analysis, from simple
data reduction to automated intelligence generation. We synthesize these
findings into a forward-looking research roadmap, arguing that the true
potential of this technology lies in creating autonomous, self-improving
deception systems to counter the emerging threat of intelligent, automated
attackers.

</details>


### [6] [WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows](https://arxiv.org/abs/2510.25960)
*Zeynep Yasemin Erdogan,Shishir Nagaraja,Chuadhry Mujeeb Ahmed,Ryan Shah*

Main category: cs.CR

TL;DR: 该论文提出了一个基于声学侧信道分析的框架，用于监测和验证机器人是否正确执行其预期命令。通过机器学习方法分析机器人运动产生的声学信号，能够以超过80%的准确率验证单个机器人运动，并能高置信度识别工作流程。


<details>
  <summary>Details</summary>
Motivation: 开发一种实时、低成本、被动的验证系统，用于敏感机器人环境中的行为验证，无需硬件修改。

Method: 开发并评估基于机器学习的声学侧信道分析工作流程验证系统，使用四种分类器（SVM、DNN、RNN、CNN）分析机器人运动产生的声学发射信号，考虑运动速度、方向和麦克风距离等因素。

Result: 在基准条件下，使用四种不同分类器验证单个机器人运动的准确率超过80%；能够以类似高置信度识别拾取放置和包装等工作流程。

Conclusion: 声学信号可以支持敏感机器人环境中的实时、低成本、被动验证，无需硬件修改。

Abstract: In this paper, we present a framework that uses acoustic side- channel
analysis (ASCA) to monitor and verify whether a robot correctly executes its
intended commands. We develop and evaluate a machine-learning-based workflow
verification system that uses acoustic emissions generated by robotic
movements. The system can determine whether real-time behavior is consistent
with expected commands. The evaluation takes into account movement speed,
direction, and microphone distance. The results show that individual robot
movements can be validated with over 80% accuracy under baseline conditions
using four different classifiers: Support Vector Machine (SVM), Deep Neural
Network (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network
(CNN). Additionally, workflows such as pick-and-place and packing could be
identified with similarly high confidence. Our findings demonstrate that
acoustic signals can support real-time, low-cost, passive verification in
sensitive robotic environments without requiring hardware modifications.

</details>


### [7] [Message Recovery Attack in NTRU via Knapsack](https://arxiv.org/abs/2510.26003)
*Eirini Poimenidou,K. A. Draziotis*

Main category: cs.CR

TL;DR: 本文提出了一种基于模背包问题的消息恢复攻击，适用于所有NTRU-HPS密码系统变体。当已知消息和非向量的部分系数时，可将消息解密简化为在格中寻找短向量问题。


<details>
  <summary>Details</summary>
Motivation: 研究在已知部分消息或非向量信息的情况下，需要多少信息才能实现可行的消息恢复攻击，探讨NTRU-HPS密码系统的安全性边界。

Method: 使用基于模背包问题的攻击方法，通过FLATTER约简技术将消息解密问题转化为格中的短向量寻找问题。

Result: 当已知约45%的系数时，攻击在实践中成功恢复消息，在普通台式机上几分钟内即可找到原始消息。

Conclusion: 该攻击方法有效揭示了NTRU-HPS密码系统在部分信息泄露情况下的脆弱性，为密码系统的安全性评估提供了重要参考。

Abstract: In the present paper, we introduce a message-recovery attack based on the
Modular Knapsack Problem, applicable to all variants of the NTRU-HPS
cryptosystem. Assuming that a fraction $\epsilon$ of the coefficients of the
message ${\bf{m}}\in\{-1,0,1\}^N$ and of the nonce vector ${\bf
r}\in\{-1,0,1\}^N$ are known in advance at random positions, we reduce message
decryption to finding a short vector in a lattice that encodes an instance of a
modular knapsack system. This allows us to address a key question: how much
information about ${\bf m}$, or about the pair $({\bf m},{\bf r})$, is required
before recovery becomes feasible? A FLATTER reduction successfully recovers the
message, in practice when $\epsilon\approx 0.45$. Our implementation finds
${\bf m}$ within a few minutes on a commodity desktop.

</details>


### [8] [PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy](https://arxiv.org/abs/2510.26102)
*Lisha Shuai,Jiuling Dong,Nan Zhang,Shaofeng Tan,Haokun Zhang,Zilong Song,Gaoya Dong,Xiaolong Yang*

Main category: cs.CR

TL;DR: PEEL是一个针对本地差分隐私(LDP)的投毒攻击防御框架，通过重新编码LDP扰动数据来暴露投毒攻击，同时保持统计准确性和降低客户端计算成本。


<details>
  <summary>Details</summary>
Motivation: LDP在物联网中广泛应用，但易受投毒攻击，现有防御方法要么资源开销过大，要么依赖领域特定先验知识，限制了实际部署。

Method: PEEL作为非侵入式后处理模块，通过稀疏化、归一化和低秩投影重新编码LDP扰动数据，放大隐蔽的投毒效应，在重构空间中通过结构不一致性揭示输出和规则投毒攻击。

Result: 理论分析证明PEEL与LDP集成保持无偏性和统计准确性，同时能有效暴露投毒攻击。评估结果显示PEEL在投毒暴露准确率上优于四种最先进防御方法，并显著降低客户端计算成本。

Conclusion: PEEL框架为LDP提供了一种高效、轻量级的投毒攻击防御方案，特别适合大规模物联网部署。

Abstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model
in the Internet of Things (IoT) due to its lightweight, decentralized, and
scalable nature. However, it is vulnerable to poisoning attacks, and existing
defenses either incur prohibitive resource overheads or rely on domain-specific
prior knowledge, limiting their practical deployment. To address these
limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical
framework for LDP, which departs from resource- or prior-dependent
countermeasures and instead leverages the inherent structural consistency of
LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies
stealthy poisoning effects by re-encoding LDP-perturbed data via
sparsification, normalization, and low-rank projection, thereby revealing both
output and rule poisoning attacks through structural inconsistencies in the
reconstructed space. Theoretical analysis proves that PEEL, integrated with
LDP, retains unbiasedness and statistical accuracy, while being robust to
expose both output and rule poisoning attacks. Moreover, evaluation results
show that LDP-integrated PEEL not only outperforms four state-of-the-art
defenses in terms of poisoning exposure accuracy but also significantly reduces
client-side computational costs, making it highly suitable for large-scale IoT
deployments.

</details>


### [9] [Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories](https://arxiv.org/abs/2510.26103)
*Maximilian Schreiber,Pascal Tippe*

Main category: cs.CR

TL;DR: 对GitHub上AI生成代码安全漏洞的实证分析，收集了7,703个文件，发现4,241个CWE漏洞实例，Python漏洞率最高，不同AI工具在安全性能上存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，需要了解这些工具生成代码的安全状况，为负责任地集成AI生成代码到软件开发工作流提供指导。

Method: 使用CodeQL静态分析工具对GitHub上明确标注为AI生成的7,703个文件进行分析，涵盖ChatGPT、GitHub Copilot、Amazon CodeWhisperer和Tabnine四种主要工具。

Result: 87.9%的AI生成代码没有可识别漏洞，但Python漏洞率(16.18%-18.50%)显著高于JavaScript(8.66%-8.99%)和TypeScript(2.50%-7.14%)；GitHub Copilot在Python和TypeScript上安全密度更好，ChatGPT在JavaScript上表现更好；39%的文件用于文档生成。

Conclusion: 研究提供了大规模数据集的分析结果，强调了开发语言特定和上下文感知安全实践的重要性，为负责任地集成AI生成代码提供了宝贵见解。

Abstract: This paper presents a comprehensive empirical analysis of security
vulnerabilities in AI-generated code across public GitHub repositories. We
collected and analyzed 7,703 files explicitly attributed to four major AI
tools: ChatGPT (91.52\%), GitHub Copilot (7.50\%), Amazon CodeWhisperer
(0.52\%), and Tabnine (0.46\%). Using CodeQL static analysis, we identified
4,241 Common Weakness Enumeration (CWE) instances across 77 distinct
vulnerability types. Our findings reveal that while 87.9\% of AI-generated code
does not contain identifiable CWE-mapped vulnerabilities, significant patterns
emerge regarding language-specific vulnerabilities and tool performance. Python
consistently exhibited higher vulnerability rates (16.18\%-18.50\%) compared to
JavaScript (8.66\%-8.99\%) and TypeScript (2.50\%-7.14\%) across all tools. We
observed notable differences in security performance, with GitHub Copilot
achieving better security density for Python (1,739 LOC per CWE) and
TypeScript, while ChatGPT performed better for JavaScript. Additionally, we
discovered widespread use of AI tools for documentation generation (39\% of
collected files), an understudied application with implications for software
maintainability. These findings extend previous work with a significantly
larger dataset and provide valuable insights for developing language-specific
and context-aware security practices for the responsible integration of
AI-generated code into software development workflows.

</details>


### [10] [Who Moved My Transaction? Uncovering Post-Transaction Auditability Vulnerabilities in Modern Super Apps](https://arxiv.org/abs/2510.26210)
*Junlin Liu,Zhaomeng Deng,Ziming Wang,Mengyu Yao,Yifeng Cai,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: 研究发现超级应用普遍存在交易记录删除漏洞，83%的应用缺乏强认证保护，允许用户轻易删除交易历史，存在严重安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前超级应用安全范式过度关注交易前认证，而忽视了交易后审计追踪的脆弱性，用户可永久删除交易记录来隐藏未授权或敏感活动。

Method: 通过6名志愿者对6个超级应用进行交叉评估的实证研究，量化交易记录删除的安全威胁。

Result: 所有6个应用都允许用户删除交易记录，但其中5个（83%）缺乏强认证保护，只有1个应用要求生物特征验证才能删除。

Conclusion: 这是首次提供该普遍漏洞的具体证据，表明当前移动安全存在关键差距，迫切需要向确保交易后审计完整性进行范式转变。

Abstract: Super apps are the cornerstones of modern digital life, embedding financial
transactions into nearly every aspect of daily routine. The prevailing security
paradigm for these platforms is overwhelmingly focused on pre-transaction
authentication, preventing unauthorized payments before they occur. We argue
that a critical vulnerability vector has been largely overlooked: the fragility
of post-transaction audit trails. We investigate the ease with which a user can
permanently erase their transaction history from an app's interface, thereby
concealing unauthorized or sensitive activities from the account owner. To
quantify this threat, we conducted an empirical study with 6 volunteers who
performed a cross-evaluation on six super apps. Our findings are alarming: all
six applications studied allow users to delete transaction records, yet a
staggering five out of six (83+\%) fail to protect these records with strong
authentication. Only one app in our study required biometric verification for
deletion. This study provides the first concrete evidence of this
near-ubiquitous vulnerability, demonstrating a critical gap in the current
mobile security landscape and underscoring the urgent need for a paradigm shift
towards ensuring post-transaction audit integrity.

</details>


### [11] [PVMark: Enabling Public Verifiability for LLM Watermarking Schemes](https://arxiv.org/abs/2510.26274)
*Haohua Duan,Liyao Xiang,Xin Zhang*

Main category: cs.CR

TL;DR: PVMark是一个基于零知识证明的插件，使LLM水印检测过程能够被第三方公开验证，而无需泄露任何密钥，解决了当前水印方案中的信任问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM水印方案存在信任问题：非公开的水印检测无法证明其忠实执行检测过程。秘密密钥既不能公开（否则攻击者可能发起移除攻击），也不能私有（否则检测过程对公众不透明）。

Method: 基于零知识证明构建水印检测的'正确执行'证明，包括映射、随机数生成、比较和求和等约束。实现了多个PVMark变体，涵盖三种水印方案、三种哈希函数和四种ZKP协议的组合。

Result: PVMark在多种环境下有效工作，能够高效地为最先进的LLM水印方案提供公开可验证性，且不影响水印性能。

Conclusion: PVMark成功解决了LLM水印检测的信任困境，有望在实际中部署应用。

Abstract: Watermarking schemes for large language models (LLMs) have been proposed to
identify the source of the generated text, mitigating the potential threats
emerged from model theft. However, current watermarking solutions hardly
resolve the trust issue: the non-public watermark detection cannot prove itself
faithfully conducting the detection. We observe that it is attributed to the
secret key mostly used in the watermark detection -- it cannot be public, or
the adversary may launch removal attacks provided the key; nor can it be
private, or the watermarking detection is opaque to the public. To resolve the
dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),
enabling the watermark detection process to be publicly verifiable by third
parties without disclosing any secret key. PVMark hinges upon the proof of
`correct execution' of watermark detection on which a set of ZKP constraints
are built, including mapping, random number generation, comparison, and
summation. We implement multiple variants of PVMark in Python, Rust and Circom,
covering combinations of three watermarking schemes, three hash functions, and
four ZKP protocols, to show our approach effectively works under a variety of
circumstances. By experimental results, PVMark efficiently enables public
verifiability on the state-of-the-art LLM watermarking schemes yet without
compromising the watermarking performance, promising to be deployed in
practice.

</details>


### [12] [SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification](https://arxiv.org/abs/2510.26420)
*Yingjia Wang,Ting Qiao,Xing Liu,Chongzuo Li,Sixing Wu,Jianbin Li*

Main category: cs.CR

TL;DR: 提出了一种样本特定的干净标签后门水印方法（SSCL-BW），通过训练U-Net水印生成器为每个样本生成独特水印，克服静态水印模式的脆弱性，有效保护数据集知识产权。


<details>
  <summary>Details</summary>
Motivation: 现有基于后门的数据集所有权验证方法存在固有局限：毒标签水印因标签不一致易被检测，干净标签水印技术复杂度高且在高质量图像上易失败，且两种方法都使用易被检测和移除的静态水印模式。

Method: 训练U-Net水印生成器为每个样本生成独特水印，设计包含目标样本损失、非目标样本损失和感知相似性损失的复合损失函数，通过黑盒测试验证可疑模型是否表现出预定义的后门行为。

Result: 在基准数据集上的广泛实验证明该方法有效，并对潜在的水印移除攻击具有鲁棒性。

Conclusion: SSCL-BW方法通过样本特定水印生成和复合损失函数设计，有效解决了现有数据集所有权验证方法的局限性，为数据集知识产权保护提供了可靠解决方案。

Abstract: The rapid advancement of deep neural networks (DNNs) heavily relies on
large-scale, high-quality datasets. However, unauthorized commercial use of
these datasets severely violates the intellectual property rights of dataset
owners. Existing backdoor-based dataset ownership verification methods suffer
from inherent limitations: poison-label watermarks are easily detectable due to
label inconsistencies, while clean-label watermarks face high technical
complexity and failure on high-resolution images. Moreover, both approaches
employ static watermark patterns that are vulnerable to detection and removal.
To address these issues, this paper proposes a sample-specific clean-label
backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked
sample generator, this method generates unique watermarks for each sample,
fundamentally overcoming the vulnerability of static watermark patterns. The
core innovation lies in designing a composite loss function with three
components: target sample loss ensures watermark effectiveness, non-target
sample loss guarantees trigger reliability, and perceptual similarity loss
maintains visual imperceptibility. During ownership verification, black-box
testing is employed to check whether suspicious models exhibit predefined
backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed method and its robustness against potential
watermark removal attacks.

</details>


### [13] [Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies](https://arxiv.org/abs/2510.26523)
*Shuaishuai Liu,Gergely Acs,Gergely Biczók*

Main category: cs.CR

TL;DR: 该论文分析了20款视频门铃和智能摄像头的隐私政策，重点关注旁观者隐私问题。研究发现虽然部分厂商承认旁观者存在，但仅通过免责声明将收集非用户数据的道德责任转移给设备所有者。


<details>
  <summary>Details</summary>
Motivation: 智能家居设备（如视频门铃和安全摄像头）日益普及，虽然提供便利和安全，但也引发新的隐私担忧：这些设备如何影响邻居、访客或路过者等他人的隐私。这个问题被称为相互依赖隐私，即一个人的行为可能影响他人的隐私。

Method: 对20款视频门铃和智能摄像头产品进行聚焦隐私政策分析，特别关注旁观者方面。同时识别和检查与旁观者隐私相关的真实案例，展示当前部署如何影响非用户。

Result: 研究发现，虽然部分厂商承认旁观者存在，但仅通过免责声明将收集非用户数据的道德责任转移给设备所有者。通过真实案例展示了当前部署对非用户的影响。

Conclusion: 基于研究结果，分析现有法律框架和技术能力下的厂商隐私政策，为政策语言和系统设计提供实用建议，以增强透明度并赋予旁观者和设备所有者更多权力。

Abstract: Smart home devices such as video doorbells and security cameras are becoming
increasingly common in everyday life. While these devices offer convenience and
safety, they also raise new privacy concerns: how these devices affect others,
like neighbors, visitors, or people passing by. This issue is generally known
as interdependent privacy, where one person's actions (or inaction) may impact
the privacy of others, and, specifically, bystander privacy in the context of
smart homes. Given lax data protection regulations in terms of shared physical
spaces and amateur joint data controllers, we expect that the privacy policies
of smart home products reflect the missing regulatory incentives. This paper
presents a focused privacy policy analysis of 20 video doorbell and smart
camera products, concentrating explicitly on the bystander aspect. We show that
although some of the vendors acknowledge bystanders, they address it only to
the extent of including disclaimers, shifting the ethical responsibility for
collecting the data of non-users to the device owner. In addition, we identify
and examine real-world cases related to bystander privacy, demonstrating how
current deployments can impact non-users. Based on our findings, we analyze
vendor privacy policies in light of existing legal frameworks and technical
capabilities, and we provide practical recommendations for both policy language
and system design to enhance transparency and empower both bystanders and
device owners.

</details>


### [14] [Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis](https://arxiv.org/abs/2510.26620)
*Nicholas Pecka,Lotfi Ben Othmane,Renee Bryce*

Main category: cs.CR

TL;DR: 本文研究了通过使用基于密度和社区检测算法对调用图进行聚类，然后分析识别出的集群相关威胁，实现软件威胁建模自动化的方法。该方法在Splunk Forwarder Operator案例研究中得到验证，展示了其可行性。


<details>
  <summary>Details</summary>
Motivation: 手动威胁建模方法通常劳动密集且容易出错，需要自动化方法来提高效率和准确性，特别是在现代云原生环境中。

Method: 使用基于密度和社区检测算法对软件调用图进行聚类，然后分析识别出的集群相关威胁。

Result: 在Splunk Forwarder Operator案例研究中，应用选定的聚类指标评估代码密度安全弱点，证明了该方法的可行性。

Conclusion: 该方法有助于推进为现代云原生环境量身定制的可扩展半自动化威胁建模框架的发展。

Abstract: Threat modeling plays a critical role in the identification and mitigation of
security risks; however, manual approaches are often labor intensive and prone
to error. This paper investigates the automation of software threat modeling
through the clustering of call graphs using density-based and community
detection algorithms, followed by an analysis of the threats associated with
the identified clusters. The proposed method was evaluated through a case study
of the Splunk Forwarder Operator (SFO), wherein selected clustering metrics
were applied to the software's call graph to assess pertinent code-density
security weaknesses. The results demonstrate the viability of the approach and
underscore its potential to facilitate systematic threat assessment. This work
contributes to the advancement of scalable, semi-automated threat modeling
frameworks tailored for modern cloud-native environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 本文探索将SHAP（SHapley Additive exPlanations）方法应用于国际象棋分析，旨在将象棋引擎的评估归因于棋盘上的特定棋子，从而提供可解释的棋子贡献分析。


<details>
  <summary>Details</summary>
Motivation: 当前象棋引擎提供精确但不透明的评估（通常以百分兵分数表示），这些输出掩盖了单个棋子或模式的潜在贡献。作者希望开发一种能够解释引擎评估的方法，使其对人类更易理解。

Method: 通过将棋子视为特征并系统地消融它们，计算加性的、每个棋子的贡献，以局部忠实且人类可解释的方式解释引擎的输出。该方法借鉴了古典象棋教学法（玩家通过心理移除棋子来评估局面），并将其与现代可解释AI技术相结合。

Result: 该方法为可视化、人类训练和引擎比较开辟了新的可能性。作者发布了配套代码和数据以促进可解释象棋AI的未来研究。

Conclusion: SHAP方法成功应用于国际象棋分析，能够提供对引擎评估的棋子级解释，将古典象棋分析方法与现代AI解释技术相结合，推动了可解释象棋AI的发展。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [16] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 该论文提出了一个两层次框架来解释为什么压缩过程会强制发现因果结构而非表面统计模式。信息论必要性(ITI)建立了生存压力与信息处理需求之间的联系，压缩效率原则(CEP)解释了高效压缩如何通过异常积累动态选择生成性因果模型。


<details>
  <summary>Details</summary>
Motivation: 现有框架虽然认同压缩对智能的核心作用，但未能具体说明为什么这个过程会强制发现因果结构而非表面统计模式。本文旨在填补这一理论空白。

Method: 引入两层次框架：信息论必要性(ITI)和压缩效率原则(CEP)。ITI从物理、信息论和进化约束角度建立从生存压力到预测必要性、压缩需求、效率优化的因果链；CEP通过异常积累动态解释高效压缩如何选择生成性因果模型。

Result: 该框架产生了可实证检验的预测：压缩效率与分布外泛化相关；异常积累率区分因果模型与相关模型；分层系统在抽象层间显示递增效率；生物系统代谢成本跟踪表征复杂性。

Conclusion: ITI和CEP为生物、人工和多尺度系统中的收敛提供了统一解释，解决了智能的认知和功能维度，无需引入关于意识或主观经验的假设。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [17] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出一个基于角色的偏好建模框架，通过聚合多个基于评分标准的评估者输出来对齐LLM评估者与人类偏好，解决校准困难、评分标准敏感性、偏见和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 对齐基于LLM的评估者与人类偏好是一个重大挑战，因为它们难以校准且经常存在评分标准敏感性、偏见和不稳定性。克服这一挑战可以推进关键应用，如为RLHF创建可靠的奖励模型和构建有效的路由系统。

Method: 提出一个基于角色的偏好建模框架，通过聚合多个基于评分标准的评估者输出来学习偏好。包括基于角色的方法大规模合成偏好标签，以及两种聚合器实现：广义加性模型(GAM)和多层感知器(MLP)。

Result: 评估了该方法相对于简单基线的性能，并通过案例研究评估了其对人类和LLM评估者偏见的鲁棒性。

Conclusion: 该框架能够有效建模多样化的基于角色的偏好，为对齐LLM评估者与人类偏好提供了可行解决方案。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [18] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估科学应用中大语言模型可信度的综合框架，涵盖真实性、对抗鲁棒性、科学安全和科学伦理四个维度。评估显示通用行业模型在各方面优于科学专用模型，后者在逻辑和伦理推理方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学研究中展现出变革潜力，但在高风险环境中的部署引发了可信度担忧，需要系统评估框架来确保其可靠性。

Method: 开发了包含新颖开放式真实性基准和科学伦理基准的综合框架，通过验证反思调优流程和专家验证构建，使用准确性、语义相似度和基于LLM的评分等多种指标评估7个主流LLM。

Result: 通用行业模型在所有可信度维度上均优于科学专用模型，GPT-4-mini在真实性和对抗鲁棒性方面表现最佳。科学专用模型在逻辑和伦理推理能力上存在显著不足，在生物安全和化学武器等高危领域的安全评估中表现出令人担忧的脆弱性。

Conclusion: 通过开源该框架，为开发更可信的AI系统提供了基础，并推进了科学背景下模型安全与伦理的研究。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [19] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 3.8B参数的Humans-Junior模型在FACTS Grounding基准测试中与GPT-4o性能相当（在±5pp等效范围内），云服务成本降低约19倍，自托管部署可接近零边际成本。


<details>
  <summary>Details</summary>
Motivation: 开发成本效益高的小型语言模型，在保持与大型模型相当性能的同时显著降低推理成本。

Method: 结合最小化定向"外骨骼推理"支架和行为微调，教导协议合规性而非领域知识，两者协同作用显著提升性能。

Result: 在Q1-Q500测试中，Humans-Junior得分72.7%，GPT-4o得分73.5%，差异仅0.8pp；定向推理在GPT-4o上提升11.8pp至85.3%，在Gemini-2.5-Pro上提升5.0pp至93.3%。

Conclusion: 小型模型通过适当的推理支架和微调方法可以达到与大型模型相当的准确率，同时大幅降低部署成本。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [20] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文提出了注意力感知逆规划问题，旨在从人类行为中推断认知偏见，特别是注意力偏见。通过结合深度强化学习和计算认知建模，在真实驾驶场景中展示了该方法可扩展性。


<details>
  <summary>Details</summary>
Motivation: 人类的目标导向行为受到认知偏见影响，自主系统需要理解这些偏见。例如，人们在日常任务（如驾驶）中的注意力分配存在系统性偏见，影响行为表现。

Method: 结合深度强化学习与计算认知建模，构建注意力感知逆规划方法。该方法与标准逆强化学习不同，专门用于从行为中推断注意力偏见。

Result: 在Waymo开放数据集中的真实驾驶场景中，成功推断出强化学习智能体的注意力策略，证明了注意力感知逆规划在估计认知偏见方面的可扩展性。

Conclusion: 注意力感知逆规划能够有效从行为数据中推断认知偏见，为理解人类注意力机制及其对行为的影响提供了新方法，在自主系统人机交互中具有重要应用价值。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [21] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2是一个自动化生成学术综述论文的多阶段流水线系统，通过检索增强合成和结构化评估，结合并行章节生成、迭代优化和实时文献检索，确保主题完整性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 随着研究文献（特别是大语言模型领域）的快速增长，撰写全面且最新的综述论文变得越来越困难，需要自动化解决方案来提高效率。

Method: 采用多阶段流水线方法，包括检索增强合成、并行章节生成、迭代优化和实时文献检索，并使用多LLM评估框架来评估覆盖度、结构和相关性。

Result: 实验结果显示autosurvey2在结构连贯性和主题相关性方面持续优于现有检索基线和自动化基线，同时保持强大的引文保真度。

Conclusion: autosurvey2通过将检索、推理和自动评估整合到统一框架中，为生成长篇学术综述提供了可扩展和可复现的解决方案，并为自动化学术写作的未来研究奠定了坚实基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [22] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver是一个基于大语言模型的自动驾驶车辆恢复框架，能够在车辆陷入困境时通过自主推理或乘客指导来解决问题，无需修改现有系统架构。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在某些交通场景中容易陷入困境，而现有的远程干预和人工接管方案存在成本高、效率低、可访问性差等问题。

Method: 设计了一个插件式模块，利用LLM处理标准传感器数据流，检测困境状态，理解环境上下文，并生成可由车辆原生规划器执行的高级恢复命令。

Result: 在Bench2Drive基准测试和自定义不确定性场景中，StuckSolver仅通过自主推理就达到接近最先进的性能，结合乘客指导后性能进一步提升。

Conclusion: StuckSolver提供了一种有效且实用的自动驾驶车辆困境恢复解决方案，能够在不修改现有系统架构的情况下显著提升车辆在复杂场景中的应对能力。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [23] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 本文探讨了AI问责制的重要性，分析了当前AI缺乏问责性的问题，并提出了改善AI问责性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的快速增强，确保AI对消费者、选民和决策者负责变得至关重要。当前许多AI系统缺乏问责性，无法被质疑、讨论或制裁。

Method: 将一般问责制定义应用于AI，阐述AI问责性的含义，探索提高AI问责性的方法。

Result: 明确了AI问责制的概念框架，指出了当前AI问责性不足的问题，提出了改善问责性的途径。

Conclusion: 需要建立一个所有AI都能对其影响对象负责的世界，确保AI系统可以被质疑、讨论和必要时受到制裁。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [24] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS是一个在Lean4中用于大学物理问题推理的框架，包含LeanPhysBench基准测试和PhysLib物理库，当前最佳模型性能仅为35%，证明了该基准的挑战性和PhysLib的有效性。


<details>
  <summary>Details</summary>
Motivation: 为大学物理问题提供正式的推理框架和基准测试，填补Lean4中物理推理基准的空白，推动物理问题的形式化验证。

Method: 构建LeanPhysBench包含200个手工制作和同行评审的物理问题陈述，开发PhysLib社区驱动的物理定理库，使用主流Lean4证明器和闭源模型进行基准测试。

Result: DeepSeek-Prover-V2-7B模型达到16%的性能，Claude-Sonnet-4达到35%的最佳性能，PhysLib平均提升模型性能11.75%。

Conclusion: LeanPhysBench具有挑战性，PhysLib能有效提升模型性能，这是首个在Lean4中提供的物理基准研究。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [25] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出了一个量化的大语言模型推理经济学框架，将LLM推理视为计算驱动的智能生产活动，分析了边际成本、规模经济和输出质量，并基于WiNEval-3.0数据构建了首个LLM推理生产前沿。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理成本已成为决定其商业可行性和广泛应用的关键因素，需要建立经济分析框架来指导模型部署决策和资源优化。

Method: 采用定量经济学框架，将LLM推理过程视为计算驱动的智能生产活动，基于WiNEval-3.0的实证数据构建LLM推理生产前沿。

Result: 揭示了三个原则：边际成本递减、规模收益递减和最优成本效益区域，为模型部署提供了经济依据。

Conclusion: 该研究不仅为模型部署决策提供了经济基础，还为未来基于市场的AI推理资源定价和优化奠定了实证基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [26] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出Reasoning Curriculum两阶段课程学习方法：第一阶段通过数学领域RL训练推理技能，第二阶段通过多领域联合RL迁移和巩固技能，在多个模型上实现一致的推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注数学和代码领域，缺乏在其他领域激发大型语言模型推理能力的通用方法。

Method: 两阶段课程学习：1）数学领域RL训练，使用可验证奖励开发推理技能；2）多领域联合RL，迁移和巩固推理技能。方法简单、与模型架构无关，无需专用奖励模型。

Result: 在Qwen3-4B和Llama-3.1-8B模型上的多领域评估显示一致的性能提升，消融实验证明两阶段都是必要的，数学优先训练增加了解决复杂问题所需的关键认知行为。

Conclusion: Reasoning Curriculum提供了一个紧凑、易于采用的通用推理训练方案，通过数学优先的推理技能激发和跨领域迁移，有效提升模型推理能力。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [27] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: QASU是一个用于评估大型语言模型处理问卷数据能力的基准测试，通过六种序列化格式和多种提示策略测试六种结构技能，结果显示选择合适的格式和提示组合可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前问卷数据规模庞大但结构复杂，现有工具主要面向人工操作，缺乏与LLM集成的有效方法，研究人员和用户缺乏如何最佳表示问卷数据供LLM使用的指导。

Method: 开发QASU基准测试，包含六种结构技能评估（如答案查找、受访者计数、多跳推理），采用六种序列化格式和多种提示策略进行实验，包括自增强提示方法。

Result: 实验表明，选择有效的格式和提示组合可将准确率提高高达8.8个百分点；对于特定任务，通过自增强提示添加轻量级结构提示可平均提升3-4个百分点。

Conclusion: QASU基准通过系统隔离格式和提示效应，为基于LLM的问卷分析研究和实际应用提供了简单而多功能的基础框架。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [28] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出了一种名为IPA-UCT的新技术，通过弱化状态抽象条件来增强MCTS的样本效率，在多种测试领域和迭代预算下优于现有的OGA-UCT方法。


<details>
  <summary>Details</summary>
Motivation: 现有的状态-动作对抽象方法（如OGA-UCT）在噪声或大动作空间设置中难以找到状态抽象，限制了MCTS的样本效率提升。

Method: 提出了IPA-UCT技术，使用较弱的理想剪枝抽象（IPA）条件，在精度轻微损失的情况下找到更多抽象。IPA和ASAP都是更通用框架p-ASAP的特殊情况。

Result: IPA-UCT在广泛的测试领域和迭代预算下实验验证优于OGA-UCT及其衍生方法。

Conclusion: 通过放松状态抽象条件，IPA-UCT能够找到更多有效的抽象，显著提升MCTS性能，为状态抽象问题提供了实用的解决方案。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [29] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS是一个用于LLM强化微调的贝叶斯在线任务选择框架，通过自适应维护任务难度后验估计，结合显式和隐式证据，使用Thompson采样平衡探索与利用，提高数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调方法在任务选择上存在效率低下、成本高、适应性差或证据不完整的问题，需要更智能的任务选择策略来优化训练过程。

Method: 基于贝叶斯推断框架，维护任务难度后验估计，结合直接评估的显式证据和通过插值推断的隐式证据，使用Thompson采样进行任务选择，无需额外rollout开销。

Result: 在多个领域和不同规模的LLM上，BOTS相比基线方法和消融实验，在数据效率和性能方面都有显著提升。

Conclusion: BOTS为RFT中的动态任务选择提供了一个实用且可扩展的解决方案，能够有效提高训练效率和模型性能。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [30] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 该研究探索了AI数学家系统作为研究伙伴而非单纯问题解决者的角色，通过人类与AI的协作推理，在均质化理论中完成了一个具有挑战性的问题证明。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在数学推理方面取得了显著进展，但在数学研究实践中的应用仍然有限。研究旨在探索AI如何作为研究伙伴与人类协作，而不仅仅是作为问题解决工具。

Method: 通过分析AI的自主推理轨迹，结合针对性的人类干预来结构化发现过程。采用迭代分解问题为可处理的子目标、选择适当的分析方法以及验证中间结果的方法。

Result: 成功完成了一个完整且可验证的证明，展示了人类直觉与机器计算如何相互补充，提高了证明的可靠性、透明度和可解释性。

Conclusion: 系统化的人机协作推理能够推进数学发现的前沿，这种协作范式在保持人类对形式严谨性和正确性监督的同时，增强了数学证明的质量。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [31] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出一种基于项目认知需求的数据选择方法Scales++，用于创建小型但具有代表性的基准测试子集，显著降低评估成本，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型性能的数据选择方法存在高初始成本、无法处理新基准测试（冷启动问题）以及依赖现有模型失败模式的脆弱假设等局限性。

Method: 采用项目中心的方法，基于任务项目的内在属性而非模型特定失败模式进行数据选择，通过Scales++方法根据基准样本的认知需求选择数据。

Result: Scales++将初始选择成本降低18倍以上，在仅使用0.5%数据子集的情况下，在Open LLM排行榜上预测完整基准得分的平均绝对误差为2.9%。

Conclusion: 项目中心方法能够实现更高效的模型评估，同时提供更好的冷启动性能和更可解释的基准测试，且不会显著降低保真度。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [32] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+是一个AI驱动的编程作业自动评分系统，通过微调大语言模型生成教学对齐的反馈，并使用对比学习代码嵌入进行语义聚类可视化，将传统总结性评估转变为形成性学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统自动评分系统作为黑盒仅返回通过/失败结果，无法提供对学生思维和学习需求的深入洞察，编程教育的快速发展超出了传统评估工具的能力范围。

Method: 系统采用微调的大语言模型生成自动化反馈，使用对比学习训练代码嵌入进行语义聚类可视化，并支持提示池让教师指导反馈风格。

Result: 在600份学生提交的评估中，系统生成的反馈与教师评论具有强语义对齐，基于1000份标注提交训练的代码嵌入能够按功能和方法的相似性将解决方案分组为有意义的聚类。

Conclusion: 通过整合AI驱动反馈、语义聚类和交互式可视化，Autograder+在减少教师工作量的同时支持针对性教学，促进更强的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [33] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 该论文提出医学稀疏自编码器(MedSAEs)应用于MedCLIP模型的潜在空间，通过相关性指标、熵分析和自动神经元命名来量化可解释性，在CheXpert数据集上证明MedSAE神经元比原始MedCLIP特征具有更高的单义性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要既准确又可解释的模型，推进医学视觉中的机制可解释性研究。

Method: 将医学稀疏自编码器(MedSAEs)应用于MedCLIP的潜在空间，提出结合相关性指标、熵分析和通过MedGEMMA基础模型进行自动神经元命名的评估框架。

Result: 在CheXpert数据集上的实验显示，MedSAE神经元比原始MedCLIP特征实现了更高的单义性和可解释性。

Conclusion: 该研究连接了高性能医疗AI与透明度，为实现临床可靠表示提供了可扩展的步骤。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [34] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Thought Hijacking的越狱攻击方法，通过在有害请求前添加无害的推理序列来绕过大型推理模型的安全防护，在多个主流模型上达到极高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为推理模型的扩展推理能力可能增强安全性，但作者发现同样的推理能力也可被用于绕过安全防护，因此研究推理模型的安全漏洞。

Method: 使用Chain-of-Thought Hijacking攻击方法，将有害请求与长序列的无害谜题推理相结合，通过稀释安全检查信号来绕过模型的安全机制。

Result: 在HarmBench上，该方法在Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet上的攻击成功率分别达到99%、94%、100%和94%，远超现有越狱方法。

Conclusion: 研究表明最可解释的推理形式——显式链式思维，在与最终答案线索结合时，本身可能成为越狱攻击的载体，揭示了推理模型的安全脆弱性。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [35] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文系统性地定义了情境工程，追溯其从1990年代至今的发展历程，探讨了从人机交互到人-智能体交互的演变，并展望了未来人类级或超人类智能的情境工程前景。


<details>
  <summary>Details</summary>
Motivation: 受马克思关于"人的本质是社会关系总和"的启发，随着计算机和人工智能的发展，情境不再局限于纯粹的人际互动，人机互动也被包含其中。核心问题是如何让机器更好地理解人类的情境和目的。

Method: 通过历史分析的方法，将情境工程的发展划分为不同历史阶段，每个阶段由机器的智能水平塑造：从围绕原始计算机构建的早期人机交互框架，到今天由智能体驱动的人-智能体交互范式，以及未来可能的人类级或超人类智能。

Result: 提出了情境工程的系统性定义，勾勒了其历史和概念图景，并考察了实践中的关键设计考虑因素。

Conclusion: 本文为情境工程提供了概念基础，并描绘了其有前景的未来，是推动AI系统中系统性情境工程更广泛社区努力的垫脚石。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [36] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 本文探讨了如何利用AI提高人类监督质量，重点关注AI输出的事实验证问题。研究发现，结合AI评级和基于AI评分者置信度的人类评级比单独使用任一方法更好。AI事实验证助手能进一步提高人类准确性，但辅助方式很重要。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升和处理更复杂任务，验证质量和安全性变得日益困难。需要探索如何利用AI改进人类监督质量，特别是针对人类已难以胜任的事实验证问题。

Method: 研究AI评级与人类评级的结合方式，测试不同类型的AI辅助（如显示AI解释、置信度、标签，或仅显示搜索结果和证据）对人类事实验证准确性的影响。

Result: 结合AI评级和基于AI置信度的人类评级优于单独使用任一方法。AI助手能提高人类准确性，但显示AI解释、置信度和标签会导致过度依赖，而仅显示搜索结果和证据能培养更适当的信任。

Conclusion: 这些发现对放大监督（结合人类和AI来监督超越人类专家性能的AI系统）具有重要意义，强调了AI辅助方式对建立适当人机信任关系的关键作用。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [37] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型在规范推理领域的能力，发现虽然LLMs总体上遵循有效推理模式，但在特定类型的规范推理中存在不一致性，并表现出类似人类推理中的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在各种推理任务中表现出色，但它们在处理涉及义务和许可等规范模态的推理能力尚未得到充分探索。

Method: 引入了一个新数据集，涵盖规范和认知领域的广泛形式推理模式，同时纳入影响人类推理的非形式认知因素，比较LLMs在规范模态和认知模态推理中的表现。

Result: LLMs总体上遵循有效推理模式，但在特定类型的规范推理中存在显著不一致性，并表现出类似人类推理中的认知偏差。

Conclusion: 这些发现突显了在LLMs规范推理中实现逻辑一致性的挑战，为增强其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [38] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [39] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型存在文本偏好问题，其根源在于模型内部注意力机制中视觉键向量与文本键向量分布不匹配，导致视觉信息在注意力计算中被低估。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理视觉语言数据时表现出明显的文本偏好，限制了其基于视觉证据进行有效推理的能力。现有研究将此归因于外部因素如数据不平衡或指令调优，但本文认为偏见源于模型内部架构。

Method: 从LLaVA和Qwen2.5-VL模型中提取键向量，使用t-SNE可视化和Jensen-Shannon散度等定性和定量方法分析其分布结构。

Result: 视觉键向量和文本键向量在注意力空间中占据明显不同的子空间，模态间差异在统计上显著，比模态内变异高出几个数量级。

Conclusion: 文本偏见源于注意力键空间的内在错位，而不仅仅是外部数据因素。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [40] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 本文提出了一个跨平台的基础模型推理能力评估框架，在三种计算范式（HPC超级计算、云平台、大学集群）上评估了15个基础模型在8个学术领域的79个问题上的表现，挑战了传统的规模扩展假设，发现训练数据质量比模型规模更重要。


<details>
  <summary>Details</summary>
Motivation: 建立基础设施无关的基准测试，全面评估当代基础模型在不同计算平台上的推理能力，为教育、生产和研究场景提供模型选择指导。

Method: 采用三阶段实验设计：1) 在MareNostrum 5上建立6个模型的19问题基准；2) 在大学集群和Nebius AI Studio上验证基础设施无关的可复现性；3) 在两个平台上进行完整的79问题评估，探索不同架构的泛化能力。

Result: 研究结果挑战了传统的规模扩展假设，确立了训练数据质量比模型规模更关键的重要性，并提供了跨教育、生产和研究场景的模型选择实用指南。

Conclusion: 提出的三基础设施方法和79问题基准能够纵向跟踪基础模型推理能力的发展，为模型评估提供了标准化框架。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [41] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 研究发现LLMs通过少量注意力头编码通用的过滤操作表示，这种表示可以跨不同格式和任务移植，类似于函数式编程中的filter函数。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在列表处理任务中的工作机制，理解它们如何实现通用的计算操作。

Method: 使用因果中介分析在多种列表处理任务中识别编码过滤谓词的注意力头（称为filter heads），并测试这些表示的通用性和可移植性。

Result: 发现filter heads在特定token处编码紧凑的过滤谓词表示，这种表示可以提取并重新应用于不同集合、格式和语言。同时识别出另一种策略：急切评估项目是否满足谓词并在项目表示中存储标志。

Conclusion: Transformer LMs能够发展出人类可解释的抽象计算操作实现，其泛化方式与传统函数式编程模式惊人相似。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>
