<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Proceedings of the 1st International Workshop on Low Carbon Computing (LOCO 2024)](https://arxiv.org/abs/2601.02898)
*Wim Vanderbauwhede,Lauritz Thamsen,José Cano*

Main category: cs.DC

TL;DR: LOCO 2024是第一届低碳计算国际研讨会论文集，聚焦计算领域的低碳技术研究


<details>
  <summary>Details</summary>
Motivation: 随着计算需求增长和能源消耗增加，需要探索低碳计算技术以减少计算系统的碳足迹

Method: 通过国际研讨会汇集研究人员，分享低碳计算领域的最新研究成果、方法和实践经验

Result: 论文集收录了低碳计算相关的研究论文，为学术界和工业界提供了技术交流和合作平台

Conclusion: LOCO 2024成功建立了低碳计算研究社区，推动了该领域的发展和技术创新

Abstract: This is the proceedings of the 1st International Workshop on Low Carbon Computing (LOCO 2024).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [2] [APoW: Auditable Proof-of-Work Against Block Withholding Attacks](https://arxiv.org/abs/2601.02496)
*Sergio Demian Lerner*

Main category: cs.CR

TL;DR: APoW是一种新型工作量证明构造，通过可审计的非空间重扫描机制，使矿工能够验证其他矿工的工作，从而检测区块扣留攻击并支持去中心化矿池设计。


<details>
  <summary>Details</summary>
Motivation: 传统工作量证明机制中，矿池内的矿工无法验证其他矿工是否真实完成了所声称的工作量，这导致了区块扣留攻击等恶意行为。现有解决方案需要可信硬件或第三方，限制了去中心化矿池的发展。

Method: APoW基于Hashcash式随机数搜索，允许矿工通过概率性证明来验证其是否在早期挖矿轮次中搜索了指定的随机数空间区域。矿工在为新区块或矿池份额执行生产性工作的同时，能够提供可审计的工作证明。

Result: 该方案实现了矿池内矿工工作的可验证性，能够概率性检测区块扣留攻击，无需可信硬件或第三方。支持去中心化矿池设计，显著降低了扣留攻击的激励，同时保持了传统工作量证明的基本特性。

Conclusion: APoW为矿池挖矿提供了一个正交的可审计层，既可作为共识规则变更后的完整部署方案，也可在现有共识机制下作为矿池级审计机制使用，通过矿池储备实现可验证的按审计付费模式。

Abstract: We introduce APoW, a novel proof-of-work (PoW) construction inspired by Hashcash-style nonce searching, which enables the auditing of other miners' work through accountable re-scanning of the nonce space. The proposed scheme allows a miner to probabilistically attest to having searched specified regions of the nonce space in earlier mining rounds, while concurrently earning rewards for performing productive work for a new block or pool share. This capability enables miners belonging to a mining pools to audit another miner's claimed effort retroactively, thereby allowing the probabilistic detection of block withholding attacks (BWAs) without requiring trusted hardware or trusted third parties. As a consequence, the construction supports the design of decentralized mining pools in which work attribution is verifiable and withholding incentives are substantially reduced. The scheme preserves the fundamental properties of conventional PoW, including public verifiability and difficulty adjustment, while adding an orthogonal auditability layer tailored to pool-based mining. Finally, while a full deployment of APoW in Bitcoin would require a consensus rule change and minor modifications to mining ASICs, the construction remains practically useful even without consensus changes, for instance, as a pool-level auditing mechanism that enables verifiable pay-for-auditing using existing pool reserves.

</details>


### [3] [SWaRL: Safeguard Code Watermarking via Reinforcement Learning](https://arxiv.org/abs/2601.02602)
*Neusha Javidnia,Ruisi Zhang,Ashish Kundu,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: SWaRL是一个基于强化学习的代码水印框架，通过编译器反馈确保功能正确性，使用LoRA技术实现水印信息跨模型更新的可移植性，在保持代码功能完整的同时提高水印检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有代码水印方法存在缺陷：基于手动转换规则的方法容易导致编译错误，基于推理时令牌概率操纵的方法也不可靠。需要一种既能保护代码LLM知识产权，又能确保生成代码功能正确性的鲁棒水印方案。

Method: 采用基于强化学习的协同训练框架：1) 使用编译器反馈作为功能正确性奖励；2) 联合训练机密验证器作为水印可检测性奖励；3) 在微调阶段使用低秩适配(LoRA)技术，使水印信息可跨模型更新移植。

Result: 相比现有方法，SWaRL实现了更高的水印检测准确率，同时完全保持水印代码的功能性。LoRA签名嵌入使基础模型能够以水印特定方式生成和解决代码，且计算开销小。对重构和对抗转换攻击表现出强韧性。

Conclusion: SWaRL提供了一个鲁棒且保真的水印框架，有效保护代码LLM的知识产权，通过强化学习和编译器反馈确保功能正确性，LoRA技术实现水印可移植性，对攻击具有强抵抗力。

Abstract: We present SWaRL, a robust and fidelity-preserving watermarking framework designed to protect the intellectual property of code LLM owners by embedding unique and verifiable signatures in the generated output. Existing approaches rely on manually crafted transformation rules to preserve watermarked code functionality or manipulate token-generation probabilities at inference time, which are prone to compilation errors. To address these challenges, SWaRL employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward signal to maintain watermark detectability. Furthermore, SWaRL employs low-rank adaptation (LoRA) during fine-tuning, allowing the learned watermark information to be transferable across model updates. Extensive experiments show that SWaRL achieves higher watermark detection accuracy compared to prior methods while fully maintaining watermarked code functionality. The LoRA-based signature embedding steers the base model to generate and solve code in a watermark-specific manner without significant computational overhead. Moreover, SWaRL exhibits strong resilience against refactoring and adversarial transformation attacks.

</details>


### [4] [LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification](https://arxiv.org/abs/2601.02624)
*Md Ajoad Hasan,Dipayan Saha,Khan Thamid Hasan,Nashmin Alam,Azim Uddin,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Main category: cs.CR

TL;DR: LAsset是一个利用大语言模型自动从硬件设计规范和RTL描述中识别安全资产的框架，显著减少人工工作量，在SoC设计中达到90%召回率，IP设计中达到93%召回率。


<details>
  <summary>Details</summary>
Motivation: 现代SoC和IP设计日益复杂，使得安全验证变得困难。传统上安全资产需要安全专家手动识别，耗时耗力且需要专业知识，因此需要自动化解决方案来降低人工成本并提高可扩展性。

Method: LAsset框架利用大语言模型，通过结构和语义分析识别模块内主要和次要资产，并推导模块间关系来系统性地描述设计级别的安全依赖关系。该方法从硬件设计规范和RTL描述中自动提取安全资产。

Result: 实验结果显示，该框架在SoC设计中达到90%的召回率，在IP设计中达到93%的召回率，显著减少了手动开销，为安全硬件开发提供了可扩展的路径。

Conclusion: LAsset通过自动化安全资产识别，有效解决了硬件安全验证中的关键瓶颈问题，为下游安全验证任务（如威胁建模、安全属性生成和漏洞检测）提供了重要支持，推动了安全硬件开发的规模化发展。

Abstract: The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.

</details>


### [5] [Adversarial Contrastive Learning for LLM Quantization Attacks](https://arxiv.org/abs/2601.02680)
*Dinghong Song,Zhiwei Xu,Hai Wan,Xibin Zhao,Pengfei Su,Dong Li*

Main category: cs.CR

TL;DR: 本文提出了一种名为对抗对比学习（ACL）的新型梯度量化攻击方法，通过显式最大化良性响应和有害响应概率之间的差距，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 模型量化对于在资源受限硬件上部署大语言模型至关重要，但最近的研究发现，全精度下的良性LLM在量化后可能表现出恶意行为，存在严重的安全风险。

Method: ACL将攻击目标表述为基于三元组的对比损失，并结合投影梯度下降两阶段分布式微调策略，确保稳定高效的优化。

Result: 实验表明ACL具有显著效果，在过度拒绝、越狱和广告注入攻击上分别达到86.00%、97.69%和92.40%的成功率，比现有最优方法分别高出44.67%、18.84%和50.80%。

Conclusion: ACL方法在量化攻击方面表现出卓越的有效性，揭示了LLM量化过程中的安全漏洞，为防御此类攻击提供了重要参考。

Abstract: Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL's remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.

</details>


### [6] [Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System](https://arxiv.org/abs/2601.02720)
*Yuqiao Xu,Mina Namazi,Sahith Reddy Jalapally,Osama Zafar,Youngjin Yoo,Erman Ayday*

Main category: cs.CR

TL;DR: 提出一个基于隐私保护的AI驱动去中心化学习与就业记录系统，利用可信执行环境自动生成可验证技能凭证，支持选择性披露和隐私保护的职位匹配。


<details>
  <summary>Details</summary>
Motivation: 现有基于区块链的学习与就业记录系统虽然使用可验证凭证，但通常缺乏自动化的技能凭证生成能力，且无法整合非结构化的学习证据，需要更智能、隐私保护的解决方案。

Method: 采用去中心化架构，在可信执行环境内运行自然语言处理管道，分析正式记录（成绩单、教学大纲）和非正式学习成果，自动生成可验证的自发行技能凭证。所有验证和职位技能匹配都在安全飞地内进行，支持选择性披露。

Result: NLP组件在样本学习者数据上评估，技能映射遵循已验证的Syllabus-to-O*NET方法，重复运行的稳定性测试显示顶级技能排名方差小于5%。系统提供正式安全声明，证明派生凭证不可伪造且敏感信息保持机密。

Conclusion: 该系统在去中心化框架内支持安全的教育和就业凭证管理、强大的成绩单验证以及自动化的隐私保护技能提取，减少筛选偏见机会，同时确保原始凭证和私钥始终保持在安全飞地内。

Abstract: Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.

</details>


### [7] [Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges](https://arxiv.org/abs/2601.02949)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ellis Solaiman,Omer Rana,Rajiv Ranjan*

Main category: cs.CR

TL;DR: 该论文讨论了区块链互操作性解决方案，分析了几个支持异构区块链连接的平台，并通过案例研究说明了互操作性的重要性和优势，同时指出了该领域需要解决的问题。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的应用迁移到区块链，产生了大量数据，复杂应用需要在不同区块链之间共享信息，而早期区块链无法与其他区块链共享信息，这促使了互操作性解决方案的研究和开发。

Method: 论文讨论了几个提供互操作性解决方案的区块链平台，重点关注它们连接异构区块链的能力，并通过案例研究场景来说明互操作性的重要性和优势。

Result: 论文分析了多个区块链互操作性平台，展示了它们如何实现异构区块链的连接，并通过案例研究证明了互操作性解决方案在实际应用中的价值和益处。

Conclusion: 区块链互操作性对于现代区块链应用至关重要，虽然已有一些解决方案，但仍存在多个需要进一步研究和解决的问题领域。

Abstract: Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.

</details>


### [8] [Quality Degradation Attack in Synthetic Data](https://arxiv.org/abs/2601.02947)
*Qinyi Liu,Dong Liu,Farhad Vadiee,Mohammad Khalil,Pedro P. Vergara Barrios*

Main category: cs.CR

TL;DR: 该研究探讨了合成数据生成中的质量退化攻击，攻击者通过操纵真实数据或控制生成过程来降低合成数据质量，揭示了SDG流程中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注合成数据接收者发起的隐私攻击，而忽视了数据所有者、提供者或入侵者等能够访问真实数据或控制生成过程的攻击者发起的质量退化攻击。

Method: 形式化了相应的威胁模型，并通过实证评估了针对真实数据的定向操纵（如标签翻转和基于特征重要性的干预）对生成合成数据质量的影响。

Result: 结果显示，即使是微小的扰动也能显著降低下游预测性能并增加统计差异，暴露了SDG流程中的脆弱性。

Conclusion: 研究强调需要在隐私保护之外，整合完整性验证和鲁棒性机制，以确保合成数据共享框架的可靠性和可信度。

Abstract: Synthetic Data Generation (SDG) can be used to facilitate privacy-preserving data sharing. However, most existing research focuses on privacy attacks where the adversary is the recipient of the released synthetic data and attempts to infer sensitive information from it. This study investigates quality degradation attacks initiated by adversaries who possess access to the real dataset or control over the generation process, such as the data owner, the synthetic data provider, or potential intruders. We formalize a corresponding threat model and empirically evaluate the effectiveness of targeted manipulations of real data (e.g., label flipping and feature-importance-based interventions) on the quality of generated synthetic data. The results show that even small perturbations can substantially reduce downstream predictive performance and increase statistical divergence, exposing vulnerabilities within SDG pipelines. This study highlights the need to integrate integrity verification and robustness mechanisms, alongside privacy protection, to ensure the reliability and trustworthiness of synthetic data sharing frameworks.

</details>


### [9] [Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain](https://arxiv.org/abs/2601.02984)
*Martin Perešíni,Tomáš Hladký,Jakub Kubík,Ivan Homoliak*

Main category: cs.CR

TL;DR: 该研究通过提出随机模拟框架，分析多个自私矿工在不同共识协议中的攻击行为，验证已知阈值并发现新阈值。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个自私矿工在特定协议中的攻击，对多个攻击者的研究有限。需要深入理解自私挖矿攻击在不同共识协议中的表现，特别是那些有潜力缓解自私挖矿的协议。

Method: 提出随机模拟框架，建立PoW Nakamoto共识（作为基线）以及Fruitchain和Strongchain两种缓解自私挖矿的共识协议模型，分析多个自私矿工的攻击行为。

Result: 验证了文献中已知的自私挖矿阈值，并为2个及更多攻击者发现了多个新阈值。框架源代码已公开，可供研究人员评估新协议。

Conclusion: 该框架为分析多个自私矿工在不同共识协议中的攻击提供了有效工具，有助于更全面地评估区块链安全性，并为协议设计提供参考。

Abstract: The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.

</details>


### [10] [JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification](https://arxiv.org/abs/2601.03005)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Zhaoye Li,Bin Ji,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: 论文提出JPU方法，通过动态挖掘对抗样本来识别和修正动态越狱路径，显著提升大语言模型对动态攻击的抵抗能力，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的安全对齐，大语言模型仍容易受到越狱攻击。现有的机器遗忘防御方法虽然能擦除特定有害参数，但对多样化的越狱攻击仍然脆弱。研究发现失败机制源于越狱攻击主要激活中间层的未擦除参数，且存在动态的"越狱路径"。

Method: 提出Jailbreak Path Unlearning (JPU)方法：1）通过动态挖掘在线对抗样本来暴露漏洞；2）识别动态越狱路径；3）将这些路径修正指向安全锚点。这是首个能够修正动态越狱路径的方法。

Result: 大量实验表明，JPU显著增强了模型对动态越狱攻击的抵抗能力，同时保持了模型的实用性。

Conclusion: JPU通过识别和修正动态越狱路径，填补了现有遗忘防御方法的根本性缺陷，为大语言模型安全防御提供了有效解决方案。

Abstract: Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\textbf{J}$ailbreak $\textbf{P}$ath $\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.

</details>


### [11] [LLMs, You Can Evaluate It! Design of Multi-perspective Report Evaluation for Security Operation Centers](https://arxiv.org/abs/2601.03013)
*Hiroyuki Okada,Tatsumi Oba,Naoto Yanai*

Main category: cs.CR

TL;DR: 本文提出MESSALA框架，利用大语言模型评估安全运营中心的分析报告，通过分析师检查表和新技术使评估结果更接近资深分析师水平。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心经常生成安全事件分析报告，未来大语言模型可能用于此任务。理解资深分析师如何评估报告及其反馈，有助于改进SOC中的分析报告质量。

Method: 首先通过文献综述和SOC从业者用户研究构建分析师检查表；然后设计MESSALA框架，引入粒度化指南和多视角评估两项新技术，最大化报告评估并提供反馈。

Result: 实验表明，MESSALA的评估结果最接近资深SOC从业者的评估，优于现有基于LLM的方法。定性分析显示MESSALA能提供改进分析报告所需的具体行动项。

Conclusion: MESSALA框架能有效利用LLM评估安全分析报告，其评估结果接近资深分析师水平，并能提供有价值的改进反馈，有助于提升SOC分析报告质量。

Abstract: Security operation centers (SOCs) often produce analysis reports on security incidents, and large language models (LLMs) will likely be used for this task in the near future. We postulate that a better understanding of how veteran analysts evaluate reports, including their feedback, can help produce analysis reports in SOCs. In this paper, we aim to leverage LLMs for analysis reports. To this end, we first construct a Analyst-wise checklist to reflect SOC practitioners' opinions for analysis report evaluation through literature review and user study with SOC practitioners. Next, we design a novel LLM-based conceptual framework, named MESSALA, by further introducing two new techniques, granularization guideline and multi-perspective evaluation. MESSALA can maximize report evaluation and provide feedback on veteran SOC practitioners' perceptions. When we conduct extensive experiments with MESSALA, the evaluation results by MESSALA are the closest to those of veteran SOC practitioners compared with the existing LLM-based methods. We then show two key insights. We also conduct qualitative analysis with MESSALA, and then identify that MESSALA can provide actionable items that are necessary for improving analysis reports.

</details>


### [12] [FlexProofs: A Vector Commitment with Flexible Linear Time for Computing All Proofs](https://arxiv.org/abs/2601.03031)
*Jing Liu,Liang Feng Zhang*

Main category: cs.CR

TL;DR: FlexProofs是一种新型向量承诺方案，具有最优证明生成时间O(N)和灵活的批量参数b，兼容zkSNARKs，相比现有方案速度提升可达6倍。


<details>
  <summary>Details</summary>
Motivation: 现有向量承诺方案在生成所有个体开放证明时效率有限，特别是与zkSNARKs兼容的HydraProofs方案虽然能达到最优时间O(N)，但仍有提升空间。需要一种更灵活、高效的方案来支持实际应用如可验证秘密共享和可验证鲁棒聚合。

Method: 提出FlexProofs向量承诺方案，引入灵活的批量大小参数b来进一步减少证明生成时间。作为关键构建块，提出了首个支持批量开放的多指数功能承诺方案。该方案直接兼容使用多线性多项式编码输入的zkSNARKs家族。

Result: 当N=2^16且b=log²N时，FlexProofs比HydraProofs快6倍。方案与合适的zkSNARKs结合后，能够支持可验证秘密共享和可验证鲁棒聚合等实际应用。

Conclusion: FlexProofs通过引入灵活的批量参数b，在保持与zkSNARKs兼容的同时，显著提高了向量承诺证明的生成效率，为密码学应用提供了更实用的工具。

Abstract: In this paper, we introduce FlexProofs, a new vector commitment (VC) scheme that achieves two key properties: (1) the prover can generate all individual opening proofs for a vector of size $N$ in optimal time ${\cal O}(N)$, and there is a flexible batch size parameter $b$ that can be increased to further reduce the time to generate all proofs; and (2) the scheme is directly compatible with a family of zkSNARKs that encode their input as a multi-linear polynomial. As a critical building block, we propose the first functional commitment (FC) scheme for multi-exponentiations with batch opening. Compared with HydraProofs, the only existing VC scheme that computes all proofs in optimal time ${\cal O}(N)$ and is directly compatible with zkSNARKs, FlexProofs may speed up the process of generating all proofs, if the parameter $b$ is properly chosen. Our experiments show that for $N=2^{16}$ and $b=\log^2 N$, FlexProofs can be $6\times$ faster than HydraProofs. Moreover, when combined with suitable zkSNARKs, FlexProofs enable practical applications such as verifiable secret sharing and verifiable robust aggregation.

</details>


### [13] [SLIM: Stealthy Low-Coverage Black-Box Watermarking via Latent-Space Confusion Zones](https://arxiv.org/abs/2601.03242)
*Hengyu Wu,Yang Cao*

Main category: cs.CR

TL;DR: SLIM框架通过创建潜在空间混淆区实现低覆盖率数据水印，能在仅修改少量训练序列的情况下保护LLM训练数据的所有权验证。


<details>
  <summary>Details</summary>
Motivation: 训练数据是LLM开发中的关键资产，需要数据水印技术进行使用验证。现有方法在低覆盖率（单个数据所有者仅贡献极小部分训练数据）情况下无法同时保持隐蔽性、验证可行性和鲁棒性。

Method: SLIM框架利用LLM内在特性，通过训练模型将语义相似的前缀映射到不同的续写，创建潜在空间混淆区，表现为局部生成不稳定性，可通过假设检验可靠检测。

Result: 实验表明SLIM实现了超低覆盖率能力、强大的黑盒验证性能、良好的可扩展性，同时保持了隐蔽性和模型实用性。

Conclusion: SLIM为现代LLM流水线中的训练数据保护提供了鲁棒的解决方案，能够在严格黑盒访问条件下实现每个用户的数据来源验证。

Abstract: Training data is a critical and often proprietary asset in Large Language Model (LLM) development, motivating the use of data watermarking to embed model-transferable signals for usage verification. We identify low coverage as a vital yet largely overlooked requirement for practicality, as individual data owners typically contribute only a minute fraction of massive training corpora. Prior methods fail to maintain stealthiness, verification feasibility, or robustness when only one or a few sequences can be modified. To address these limitations, we introduce SLIM, a framework enabling per-user data provenance verification under strict black-box access. SLIM leverages intrinsic LLM properties to induce a Latent-Space Confusion Zone by training the model to map semantically similar prefixes to divergent continuations. This manifests as localized generation instability, which can be reliably detected via hypothesis testing. Experiments demonstrate that SLIM achieves ultra-low coverage capability, strong black-box verification performance, and great scalability while preserving both stealthiness and model utility, offering a robust solution for protecting training data in modern LLM pipelines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的可解释强化学习框架，通过LLM生成文本解释并转换为透明规则，结合专家知识和自动谓词生成器来提升解释质量，并在开源环境和电信用例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习策略的可解释性对于确保自主智能体行为符合人类期望至关重要。虽然文本解释易于人类理解，但确保其正确性仍然是一个挑战，现有评估方法有限。需要一种能够生成高质量文本解释并进行系统评估的框架。

Method: 提出一个XRL框架：1）使用大型语言模型生成文本解释；2）通过聚类技术识别频繁条件；3）将条件转换为透明规则；4）提出两种精炼技术改进解释质量；5）引入专家知识和自动谓词生成器确定状态语义信息；6）评估规则的保真度、性能和部署环境表现。

Result: 在三个开源环境中进行了可复现性实验，并在电信用例中评估了工业适用性。该框架解决了现有自主策略解释方法的局限性，生成的透明规则在某些任务上能够达到令人满意的性能。实现了对文本解释的系统化和定量评估。

Conclusion: 该框架为XRL领域提供了有价值的见解，能够生成高质量的可解释规则，结合专家知识提升解释质量，并通过系统评估方法解决了文本解释正确性的挑战，具有实际工业应用潜力。

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [15] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 该论文研究了设备端AI模型在实际部署中的关键问题，包括模型选择、资源消耗和领域适应能力，通过构建LiveChatBench基准测试在移动设备上进行实验。


<details>
  <summary>Details</summary>
Motivation: 尽管设备端AI模型效率高，但缺乏针对实际部署的研究，如设备CPU利用率和热条件等实际问题。需要解决模型选择、资源消耗和领域适应能力等关键问题。

Method: 构建LiveChatBench基准测试（包含1000个韩英平行句对），在5个移动设备上进行广泛实验，研究设备端模型的资源消耗和领域适应能力。

Result: 实验表明，虽然服务大规模异构用户需要考虑高度受限的部署环境和模型选择，但所提方法在针对性任务上达到了与GPT-5.1等商业模型相当的性能。

Conclusion: 设备端AI模型在实际部署中需要仔细考虑资源约束和模型选择，但通过适当方法可以达到商业级性能，为设备端AI社区提供有价值的见解。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [16] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: HAPO框架通过分层归因机制解决提示工程中的提示漂移问题，实现高效、可解释的提示优化


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法存在提示漂移问题（新提示修复先前失败但损害先前成功任务），且从零生成提示会损害可解释性，需要更有效的优化框架

Method: 提出分层归因提示优化（HAPO）框架，包含三个创新：动态归因机制针对训练数据和提示历史中的错误模式；语义单元优化编辑功能提示段；多模态友好进展支持端到端LLM和LLM-MLLM工作流

Result: 在单/多图像QA（如OCRV2）和复杂任务分析（如BBH）等场景中，HAPO表现出增强的优化效率，优于可比较的自动提示优化方法

Conclusion: HAPO建立了可扩展的提示工程范式，解决了提示漂移问题，同时保持可解释性，为大规模提示优化提供了有效框架

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [17] [Learning User Preferences Through Interaction for Long-Term Collaboration](https://arxiv.org/abs/2601.02702)
*Shuhaib Mehri,Priyanka Kargupta,Tal August,Dilek Hakkani-Tür*

Main category: cs.AI

TL;DR: MultiSessionCollab基准测试评估智能体在多轮会话中学习用户偏好并提升协作质量的能力，通过持久化记忆机制和用户模拟器训练信号来改进长期协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着对话智能体积累与用户的协作经验，适应用户偏好对于建立长期关系和提升协作质量至关重要，需要评估和改进智能体在多轮会话中的学习能力。

Method: 提出MultiSessionCollab基准测试，开发配备持久化记忆的长期协作智能体，记忆随交互经验积累而精炼；利用用户模拟器行为生成学习信号，训练智能体生成更全面的反思并更有效地更新记忆。

Result: 实验表明配备记忆的智能体显著改善长期协作，提高任务成功率、交互效率并减少用户努力；人类用户研究证实记忆在实际场景中提升用户体验。

Conclusion: 持久化记忆机制是提升对话智能体长期协作能力的关键，MultiSessionCollab基准为评估和改进智能体在多轮会话中的用户偏好学习提供了有效框架。

Abstract: As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.

</details>


### [18] [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714)
*Zhi Liu,Guangzhi Wang*

Main category: cs.AI

TL;DR: 论文提出"时间缩放"概念，即通过扩展和优化智能体在时间维度上的推理能力，实现更深层次的问题空间探索和动态策略调整，而不需要按比例增加静态模型参数。


<details>
  <summary>Details</summary>
Motivation: 早期人工智能范式存在认知功能分离问题，而当前大型语言模型虽然能生成流畅文本，但缺乏稳健的语义推理能力。现有的提示技术如思维链和思维树虽然扩展了推理路径，但在搜索完整性和效率方面存在局限，需要更系统的时间维度推理优化。

Method: 提出"时间缩放"架构设计，利用扩展的时间路径实现：1) 更深层次的问题空间探索；2) 动态策略调整；3) 增强的元认知控制。这种方法模拟人类在认知约束下的序列推理过程，将显式时间推理管理作为基础。

Result: 时间缩放代表了增强深度推理和问题解决能力的关键前沿，能够在不按比例增加静态模型参数的情况下，显著提升智能体的认知能力。这为构建具有闭环"感知-决策-行动"能力的认知智能体提供了新方向。

Conclusion: 推进智能体能力需要将时间缩放原则置于前沿，将显式时间推理管理定位为基础。这种方法能够实现Transformer大模型和世界模型中不同人工智能范式的融合，构建更接近人类认知过程的智能系统。

Abstract: Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for "Time-Scaling"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.

</details>


### [19] [The Path Ahead for Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2601.02749)
*Nadia Sibai,Yara Ahmed,Serry Sibaee,Sawsan AlHalawani,Adel Ammar,Wadii Boulila*

Main category: cs.AI

TL;DR: 本文探讨了LLM从被动文本生成器向自主智能体的演进，分析了实现自主行为所需的核心组件（感知、记忆、规划、工具执行），并指出了安全性、对齐性等关键挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究LLM从被动文本生成器向自主、目标驱动的智能体系统演进的根本性转变，探索如何将语言理解能力转化为自主行动能力，填补从语言模型到自主智能体之间的技术空白。

Method: 通过分析LLM架构从统计模型到基于Transformer系统的演进过程，识别实现智能体行为的关键能力（长程推理、上下文感知、自适应决策），提出集成框架描述感知、记忆、规划、工具执行等核心组件。

Result: 提出了三个主要贡献：1）LLM能力如何通过推理-行动-反思循环向自主性扩展的综合分析；2）连接LLM与自主行为的核心组件集成框架；3）对应用及安全性、对齐性、可靠性等持续挑战的批判性评估。

Conclusion: 实现LLM向自主智能体的转变需要解决可验证规划、可扩展多智能体协调、持久记忆架构和治理框架等关键技术挑战。负责任的发展需要在技术鲁棒性、可解释性和伦理保障方面同步推进，以在实现潜力的同时减轻错位和意外后果的风险。

Abstract: The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.

</details>


### [20] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: ChangeGPT：一个结合大语言模型与视觉基础模型的通用智能体框架，用于遥感变化检测，通过分层结构减少幻觉，在多样化真实场景查询中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法缺乏处理多样化真实世界查询的通用性和进行综合分析的能力，需要更智能、适应性更强的解决方案

Method: 提出ChangeGPT框架，集成大语言模型（LLM）与视觉基础模型，采用分层结构来缓解幻觉问题，通过工具选择和推理处理变化检测任务

Result: 在包含140个问题的数据集上评估，ChangeGPT（特别是GPT-4-turbo后端）达到90.71%的匹配率，在处理需要多步推理和工具选择的复杂变化查询方面表现突出，并通过深圳前海湾的实际案例验证了实用性

Conclusion: ChangeGPT通过提供智能性、适应性和多类型变化分析能力，为遥感应用中的决策制定提供了强大的解决方案

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [21] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: HAL框架通过从对比对话数据中提取可解释的对话特征，构建透明奖励信号，用于语言模型与人类对话相似度的对齐优化。


<details>
  <summary>Details</summary>
Motivation: 当前人类对话相似度难以定义、测量和优化，改进主要依赖规模或广泛监督训练，而非有针对性的对齐。需要一种能够将软性、定性语言属性转化为可测量、可对齐的方法。

Method: 从对比对话数据中提取明确的对话特征，将其组合成紧凑的标量分数，作为透明奖励信号，使用标准偏好优化方法进行对齐。

Result: 在不同规模模型上实现对齐而不影响整体性能，大规模人类评估显示HAL对齐的模型在对话中更频繁被感知为人类相似。

Conclusion: HAL展示了如何将先前超出对齐范围的软性、定性语言属性以可解释和可解释的方式变得可测量和对齐，实现了透明、可检查的对齐行为。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [22] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: 提出因果图增强的检索增强生成系统，用于医学系统综述，实现零幻觉和高准确率


<details>
  <summary>Details</summary>
Motivation: 传统医学系统综述面临每年150万+文献的手工审查不可行，现有AI方法存在幻觉问题（2-40%），这在影响患者护理时不可接受

Method: 因果图增强的检索增强生成系统，整合显式因果推理与双层知识图谱，采用证据优先协议，每个因果声明都追溯到检索文献，自动生成可视化干预-结果路径的有向无环图

Result: 在234篇痴呆运动研究摘要评估中，CausalAgent达到95%准确率、100%检索成功率和零幻觉，而基线AI只有34%准确率和10%幻觉。自动因果图支持显式机制建模、可视化合成和增强可解释性

Conclusion: 虽然概念验证评估仅针对痴呆运动研究的十个问题，但该架构方法展示了可信赖医学AI的可转移原则，以及因果推理在高风险医疗保健中的潜力

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [23] [Sample-Efficient Neurosymbolic Deep Reinforcement Learning](https://arxiv.org/abs/2601.02850)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 该论文提出了一种神经符号深度强化学习方法，通过整合符号知识来提升样本效率和泛化能力，在稀疏奖励环境和长规划任务中加速收敛。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习算法通常需要大量训练数据，且难以泛化到超出小规模训练场景的任务。为了解决样本效率低和泛化能力差的问题，研究者希望整合符号知识来改进DRL算法。

Method: 提出神经符号DRL方法，将简单域实例中定义的部分策略作为先验知识转移到更复杂设置中。部分策略表示为逻辑规则，通过在线推理指导训练过程：1）在探索阶段偏置动作分布；2）在利用阶段重新缩放Q值。

Result: 在网格世界环境（完全可观测和部分可观测设置）的挑战性变体上进行了实证验证，相比最先进的奖励机制基线方法，展示了性能改进。

Conclusion: 神经符号集成增强了可解释性和可信度，同时加速了收敛，特别是在稀疏奖励环境和长规划任务中。该方法通过整合符号知识有效提升了DRL的样本效率和泛化能力。

Abstract: Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.

</details>


### [24] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: 该论文提出了M3MAD-Bench基准测试，用于在多领域、多模态和多维度指标下统一评估多智能体辩论方法，解决了现有研究评估不一致和局限于单模态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论研究存在两个根本性局限：评估在分散和不一致的设置下进行，阻碍了公平比较；且主要局限于依赖纯文本输入的单模态场景。需要建立一个统一、可扩展的基准来标准化评估。

Method: 提出M3MAD-Bench基准，建立五个核心任务领域的标准化协议（知识、数学、医学、自然科学、复杂推理），系统覆盖纯文本和视觉语言数据集。评估了九种基础模型，涵盖不同架构、规模和模态能力，并纳入效率导向指标如令牌消耗和推理时间。

Result: 通过大量实验，系统揭示了多智能体辩论在纯文本和多模态场景下的有效性、鲁棒性和效率。提供了性能-成本权衡的整体视图，为标准化MAD评估提供了可靠基础。

Conclusion: M3MAD-Bench为未来标准化多智能体辩论评估研究提供了可靠基础，支持跨模态的受控比较，并考虑了性能与效率的平衡。

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [25] [ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: ReTreVal是一个结合树状思维探索、自我精炼、LLM批判评分和反思记忆的混合框架，用于解决LLM在多步推理中的挑战，在数学和创意写作任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ReAct、Reflexion和Self-Refine虽然通过迭代精炼和反思改进了推理，但缺乏对替代解决方案路径的结构化探索和跨问题的持续学习能力。

Method: ReTreVal构建基于问题复杂度的自适应深度推理树，每个节点进行迭代自我批判和精炼，采用双重验证机制评估推理质量，通过批判性剪枝保留最高分节点，并将成功路径和失败模式存储在反思记忆缓冲区中实现跨问题学习。

Result: 使用Qwen 2.5 7B作为基础LLM，在500个数学问题和创意写作任务上评估，ReTreVal在结构化探索、批判驱动精炼和跨问题记忆的结合下，持续优于ReAct、Reflexion和Self-Refine等方法。

Conclusion: ReTreVal通过结构化探索、批判驱动精炼和跨问题记忆的有效结合，特别适用于需要探索性推理、严格验证和知识迁移的任务，为LLM多步推理提供了有效的解决方案。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [26] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: LLMs在逻辑推理中存在"逻辑相变"现象：推理能力在特定复杂度范围内保持稳定，但超过临界深度后会突然崩溃，类似物理相变。研究提出神经符号课程调优框架来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 符号逻辑推理是LLMs的关键但未充分探索的能力，在数学推理和法律判断等高风险领域提供可靠且可验证的决策。当前研究缺乏对逻辑复杂度增加时推理能力变化的系统性分析。

Method: 提出神经符号课程调优框架：1) 自适应对齐自然语言与逻辑符号以建立共享表示；2) 围绕相变边界重塑训练动态，逐步增强在增加逻辑深度下的推理能力。

Result: 在五个基准测试上，该方法有效缓解了高复杂度下的逻辑推理崩溃，在简单提示中平均准确率提升+1.26，在思维链中提升+3.95，同时提高了对未见逻辑组合的泛化能力。

Conclusion: 发现了LLMs逻辑推理中的相变现象，并提出了有效的神经符号课程调优框架来增强模型在高逻辑复杂度下的推理能力，为可靠符号推理系统的发展提供了新方向。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [27] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: Batch-of-Thought (BoT) 是一种无需训练的方法，通过联合处理相关查询实现跨实例学习，利用共享推理模式和一致性约束来提高LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型推理系统独立处理查询，丢弃了有价值的跨实例信号，如共享推理模式和一致性约束，导致效率低下和信息浪费

Method: 提出Batch-of-Thought (BoT)方法，通过多智能体反思架构(BoT-R)实现联合处理，其中Reflector进行联合评估以获取孤立处理无法获得的互信息增益，包括识别高质量推理模板、通过一致性检查检测错误和分摊计算成本

Result: 在三个模型系列和六个基准测试上的实验表明，BoT-R持续提高准确性和置信度校准，同时将推理成本降低高达61%

Conclusion: 批量感知推理通过利用跨实例信号显著提升LLM系统性能，理论和实验分析揭示了批量感知推理何时以及为何对LLM系统有益

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [28] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: RationaleTS：一种基于原理的上下文学习方法，通过引入标签条件原理作为指导推理单元，改善多模态大语言模型在时间序列推理中的性能，避免模型依赖表面模式匹配。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在时间序列推理中表现不佳，主要原因是缺乏将时间观测与下游结果连接起来的原理先验，导致模型依赖表面模式匹配而非原则性推理。

Method: 提出RationaleTS方法：1) 诱导标签条件原理，构建从可观测证据到潜在结果的推理路径；2) 设计混合检索机制，平衡时间模式和语义上下文，为新的样本检索相关原理先验进行最终上下文推理。

Result: 在三个领域的时间序列推理任务上进行了广泛实验，证明了RationaleTS方法的有效性和效率。

Conclusion: 通过引入原理作为指导推理单元而非事后解释，RationaleTS方法显著改善了时间序列推理性能，代码将开源以供复现。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [29] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 提出一种更简单但同样有效的自动提示工程系统，无需调参和任务线索，在数据库表列名扩展任务上评估，支持英语和德语


<details>
  <summary>Details</summary>
Motivation: 现有自动提示工程方法复杂且需要调参，数据库表列名扩展任务对表格数据搜索、访问和理解至关重要但缺乏研究，且现有方法主要针对英语

Method: 设计简单无需调参的自动提示工程系统，不依赖任务明确线索，在英语和德语的数据库表列名扩展任务上进行评估

Result: 这是首个将自动提示工程应用于列名扩展任务的研究，也是首个在英语之外语言（德语）上应用自动提示工程的工作

Conclusion: 提出的自动提示工程系统设计简单、无需调参，在列名扩展任务上表现有效，并首次扩展到非英语语言应用

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>


### [30] [InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents](https://arxiv.org/abs/2601.03204)
*Chenglin Yu,Yuchen Wang,Songmiao Wang,Hongxia Yang,Ming Li*

Main category: cs.AI

TL;DR: InfiAgent框架通过文件中心状态抽象将智能体持久状态外部化，保持推理上下文严格有界，解决长时任务中上下文无限增长和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长时任务中经常因上下文无限增长和错误累积而失效，现有解决方案如上下文压缩或检索增强提示需要在信息保真度和推理稳定性之间权衡。

Method: 提出InfiAgent通用框架，通过文件中心状态抽象将智能体持久状态外部化，每一步从工作空间状态快照加上固定窗口的近期动作重建上下文，保持推理上下文严格有界。

Result: 在DeepResearch和80篇文献综述任务上的实验表明，无需任务特定微调，使用20B开源模型的InfiAgent与更大的专有系统竞争，并保持比上下文中心基线显著更高的长时覆盖率。

Conclusion: 显式状态外部化是稳定长时智能体的实用基础，InfiAgent框架为LLM智能体在长时任务中的稳定推理提供了有效解决方案。

Abstract: LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [31] [Sparsity-Aware Streaming SNN Accelerator with Output-Channel Dataflow for Automatic Modulation Classification](https://arxiv.org/abs/2601.02613)
*Kuilian Yang,Li Zhang,Ahmed M. Eltawil,Khaled Nabil Salama*

Main category: cs.AR

TL;DR: 提出一种面向自动调制分类任务的稀疏感知SNN流式加速器，通过GOAP算法优化稀疏计算，在FPGA上实现高吞吐量和低功耗


<details>
  <summary>Details</summary>
Motivation: 随着5G/6G和物联网的发展，频谱利用效率需求增加。自动调制分类在认知无线电中至关重要，但传统DNN计算和能耗高，SNN虽然能效高但难以同时实现高吞吐和低功耗

Method: 提出稀疏感知输出通道数据流加速器，利用推理期间核固定的特性，采用GOAP算法仅计算非零输入-权重交集，预计算额外或空迭代并嵌入推理数据流，实现全流水线控制

Result: 在FPGA上实现，在RadioML 2016数据集上达到23.5 MS/s吞吐量（约为基础方案的两倍），降低动态功耗同时保持相当的分类准确率

Conclusion: 该设计展示了在边缘认知无线电系统中实现实时低功耗部署的强大潜力

Abstract: The rapid advancement of wireless communication technologies, including 5G, emerging 6G networks, and the large-scale deployment of the Internet of Things (IoT), has intensified the need for efficient spectrum utilization. Automatic modulation classification (AMC) plays a vital role in cognitive radio systems by enabling real-time identification of modulation schemes for dynamic spectrum access and interference mitigation. While deep neural networks (DNNs) offer high classification accuracy, their computational and energy demands pose challenges for real-time edge deployment. Spiking neural networks (SNNs), with their event-driven nature, offer inherent energy efficiency, but achieving both high throughput and low power under constrained hardware resources remains challenging. This work proposes a sparsity-aware SNN streaming accelerator optimized for AMC tasks. Unlike traditional systolic arrays that exploit sparsity but suffer from low throughput, or streaming architectures that achieve high throughput but cannot fully utilize input and weight sparsity, our design integrates both advantages. By leveraging the fixed nature of kernels during inference, we apply the gated one-to-all product (GOAP) algorithm to compute only on non-zero input-weight intersections. Extra or empty iterations are precomputed and embedded into the inference dataflow, eliminating dynamic data fetches and enabling fully pipelined, control-free inter-layer execution. Implemented on an FPGA, our sparsity-aware output-channel dataflow streaming (SAOCDS) accelerator achieves 23.5 MS/s (approximately double the baseline throughput) on the RadioML 2016 dataset, while reducing dynamic power and maintaining comparable classification accuracy. These results demonstrate strong potential for real-time, low-power deployment in edge cognitive radio systems.

</details>
