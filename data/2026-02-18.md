<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Unforgeable Watermarks for Language Models via Robust Signatures](https://arxiv.org/abs/2602.15323)
*Huijia Lin,Kameron Shahabi,Min Jae Song*

Main category: cs.CR

TL;DR: 该论文提出了一种具有不可伪造性和可恢复性的新型水印方案，通过鲁棒数字签名技术增强语言模型输出的内容溯源能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型生成文本越来越难以与人类写作区分，需要更强大的工具来验证内容来源。现有水印方案在防止错误归属方面保护有限，需要更强的安全保障。

Method: 引入两种新保证：不可伪造性（防止伪造误报）和可恢复性（检测时识别源文本）。使用鲁棒数字签名作为关键技术，通过属性保持哈希函数将标准数字签名方案提升为鲁棒版本。

Result: 构建了第一个在替换扰动下具有鲁棒性、不可伪造性和可恢复性的不可检测水印方案，实现了安全归属和细粒度可追溯性。

Conclusion: 通过不可伪造性和可恢复性强化了水印的可靠性，将内容与其生成模型唯一链接，为语言模型输出提供了更强大的内容所有权保护机制。

Abstract: Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).

</details>


### [2] [A Unified Evaluation of Learning-Based Similarity Techniques for Malware Detection](https://arxiv.org/abs/2602.15376)
*Udbhav Prasad,Aniesh Chawla*

Main category: cs.CR

TL;DR: 该论文系统比较了基于学习的分类和相似性方法在安全应用中的表现，发现没有单一方法在所有维度都表现良好，需要结合互补技术。


<details>
  <summary>Details</summary>
Motivation: 传统加密摘要（如MD5、SHA-256）虽然能提供精确身份验证，但在威胁狩猎、恶意软件分析和数字取证等实际安全任务中存在局限性，因为攻击者经常进行微小变换。现有的相似性技术（如ssdeep、sdhash、TLSH）和机器学习方法缺乏系统性的比较评估。

Method: 采用统一的实验框架和行业接受度指标，对基于学习的分类和相似性方法进行系统比较。使用大型公开数据集，首次对这些不同的基于学习的相似性技术进行可重复的基准测试。

Result: 结果显示没有单一方法在所有维度都表现良好，每种方法都展现出不同的权衡。这表明有效的恶意软件分析和威胁狩猎平台必须结合互补的分类和相似性技术，而不是依赖单一方法。

Conclusion: 基于学习的相似性技术在安全应用中具有重要价值，但需要多种方法的组合使用。该研究为安全研究人员提供了首个可重复的基准测试框架，帮助选择合适的技术组合。

Abstract: Cryptographic digests (e.g., MD5, SHA-256) are designed to provide exact identity. Any single-bit change in the input produces a completely different hash, which is ideal for integrity verification but limits their usefulness in many real-world tasks like threat hunting, malware analysis and digital forensics, where adversaries routinely introduce minor transformations. Similarity-based techniques address this limitation by enabling approximate matching, allowing related byte sequences to produce measurably similar fingerprints. Modern enterprises manage tens of thousands of endpoints with billions of files, making the effectiveness and scalability of the proposed techniques more important than ever in security applications. Security researchers have proposed a range of approaches, including similarity digests and locality-sensitive hashes (e.g., ssdeep, sdhash, TLSH), as well as more recent machine-learning-based methods that generate embeddings from file features. However, these techniques have largely been evaluated in isolation, using disparate datasets and evaluation criteria. This paper presents a systematic comparison of learning-based classification and similarity methods using large, publicly available datasets. We evaluate each method under a unified experimental framework with industry-accepted metrics. To our knowledge, this is the first reproducible study to benchmark these diverse learning-based similarity techniques side by side for real-world security workloads. Our results show that no single approach performs well across all dimensions; instead, each exhibits distinct trade-offs, indicating that effective malware analysis and threat-hunting platforms must combine complementary classification and similarity techniques rather than rely on a single method.

</details>


### [3] [MEV in Binance Builder](https://arxiv.org/abs/2602.15395)
*Qin Wang,Ruiqiang Li,Guangsheng Yu,Vincent Gramoli,Shiping Chen*

Main category: cs.CR

TL;DR: 该研究分析了BNB智能链上建设者驱动的MEV套利活动，发现由于白名单PBS设计、短区块间隔和私人订单流绕过公共内存池，导致MEV利润高度集中在两个主导建设者手中（48Club和Blockrazor），形成垄断格局。


<details>
  <summary>Details</summary>
Motivation: 研究BNB智能链的提议者-建设者分离（PBS）机制，该机制采用更精简的设计：仅允许白名单建设者参与、区块生产间隔更短、私人订单流绕过公共内存池。这些特性长期引发社区对中心化的担忧，需要实证验证。

Method: 通过实证追踪2025年5月至11月期间两个主导建设者（48Club和Blockrazor）的套利活动，分析BNB智能链上的MEV套利模式、利润分布和区块建设集中度。

Result: 48Club和Blockrazor在几个月内生产了超过96%的区块，捕获了约92%的MEV利润。利润集中在短距离、低跳数的套利路径（主要涉及包装代币和稳定币），区块建设迅速向垄断收敛。短区块间隔和白名单PBS压缩了MEV竞争的可竞争窗口，放大了延迟优势，排除了较慢的建设者和搜索者。

Conclusion: BNB智能链上的MEV提取不仅比以太坊更加中心化，而且在结构上更容易受到审查和公平性削弱的影响。短区块间隔和白名单PBS设计加剧了中心化问题，限制了MEV竞争的公平性。

Abstract: We study the builder-driven MEV arbitrage on BNB Smart Chain (BSC). BSC's Proposer--Builder Separation (PBS) adopts a leaner design: only whitelisted builders can participate, blocks are produced at shorter intervals, and private order flow bypasses the public mempool. These features have long raised community concerns over centralization, which we empirically confirm by tracing arbitrage activity of the two dominant builders from May to November 2025. Within months, 48Club and Blockrazor produced over 96\% of blocks and captured about 92\% of MEV profits.
  We find that profits concentrate in short, low-hop arbitrage routes over wrapped tokens and stablecoins, and that block construction rapidly converges toward monopoly. Beyond concentration alone, our analysis reveals a structural source of inequality: BSC's short block interval and whitelisted PBS collapse the contestable window for MEV competition, amplifying latency advantages and excluding slower builders and searchers. MEV extraction on BSC is not only more centralized than on Ethereum, but also structurally more vulnerable to censorship and weakened fairness.

</details>


### [4] [SecCodeBench-V2 Technical Report](https://arxiv.org/abs/2602.15485)
*Longfei Chen,Ji Zhao,Lanxiao Cui,Tong Su,Xingbo Pan,Ziyang Li,Yongxing Wu,Qijiang Cao,Qiyao Cai,Jing Zhang,Yuandong Ni,Junyao He,Zeyu Zhang,Chao Ge,Xuhuai Lu,Zeyu Gao,Yuxin Cui,Weisen Chen,Yuxuan Peng,Shengping Wang,Qi Li,Yukai Huang,Yukun Liu,Tuo Zhou,Terry Yue Zhuo,Junyang Lin,Chao Zhang*

Main category: cs.CR

TL;DR: SecCodeBench-V2是一个用于评估大语言模型生成安全代码能力的公开基准测试，包含98个基于阿里巴巴工业生产的场景，涵盖5种编程语言和22个CWE安全漏洞类别。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程助手在软件开发中的广泛应用，评估其生成安全代码的能力变得至关重要。现有基准测试在覆盖范围、真实性和评估方法上存在不足，需要构建一个更全面、基于工业实践的安全代码生成评估框架。

Method: 1. 构建包含98个场景的基准测试，源自阿里巴巴工业生产，涵盖Java、C、Python、Go、Node.js五种语言和22个CWE类别；2. 采用函数级任务形式，提供完整项目脚手架；3. 提供可执行的PoC测试用例进行功能验证和安全验证；4. 建立统一评估流水线，主要通过动态执行评估模型；5. 使用LLM-as-a-judge辅助评估；6. 设计基于Pass@K的评分协议。

Result: SecCodeBench-V2提供了一个严格且可复现的评估框架，包含完整的基准测试集、评估流水线和评分协议。所有测试用例由安全专家编写和双重评审，确保高保真度、广泛覆盖和可靠的真实标签。

Conclusion: SecCodeBench-V2为评估AI编程助手的安全态势提供了坚实的基础，支持跨模型的全面可比评估，有助于推动更安全的AI辅助编程实践。

Abstract: We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.

</details>


### [5] [Onto-DP: Constructing Neighborhoods for Differential Privacy on Ontological Databases](https://arxiv.org/abs/2602.15614)
*Yasmine Hayder,Adrien Boiret,Cédric Eichler,Benjamin Nguyen*

Main category: cs.CR

TL;DR: 本文提出了一种新的差分隐私扩展模型——本体感知差分隐私（Onto-DP），通过增强语义意识来保护数据库免受基于推理规则的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私模型在保护数据库敏感信息方面存在不足，攻击者可以利用推理规则发现嵌入在数据库中的敏感信息，因此需要更强大的保护机制。

Method: 提出本体感知差分隐私（Onto-DP），这是一种在经典差分隐私模型基础上构建的新型扩展，通过增强语义意识来对抗知晓推理规则的攻击者。

Result: 研究表明，Onto-DP扩展是充分保护数据库免受推理规则攻击者侵害的充分条件，而现有差分隐私模型无法提供足够保护。

Conclusion: 需要引入语义感知的差分隐私扩展来有效防御基于推理规则的攻击，Onto-DP为此类攻击提供了充分保护。

Abstract: In this paper, we investigate how attackers can discover sensitive information embedded within databases by exploiting inference rules. We demonstrate the inadequacy of naively applied existing state of the art differential privacy (DP) models in safeguarding against such attacks. We introduce ontology aware differential privacy (Onto-DP), a novel extension of differential privacy paradigms built on top of any classical DP model by enriching it with semantic awareness. We show that this extension is a sufficient condition to adequately protect against attackers aware of inference rules.

</details>


### [6] [Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective](https://arxiv.org/abs/2602.15671)
*Haodong Zhao,Jinming Hu,Gongshen Liu*

Main category: cs.CR

TL;DR: 论文挑战联邦学习安全研究传统范式，揭示良性客户端数据集中分布的低浓度中毒数据造成的后门威胁，这种威胁比少数恶意客户端攻击更普遍且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习安全研究主要关注少数恶意客户端故意破坏模型更新的后门威胁，但忽视了更普遍且隐蔽的威胁：良性客户端数据集中分布的低浓度中毒数据造成的后门漏洞。这种场景在依赖未经验证的第三方和众包数据的联邦指令调优语言模型中越来越常见。

Method: 通过真实案例分析两种后门数据形式：1）自然触发器（固有特征作为隐式触发器）；2）攻击者注入的触发器。从信号聚合角度建模后门植入过程，提出后门信噪比来量化分布式后门信号的动态特性，并进行大量实验验证。

Result: 实验显示威胁严重性：仅需不到10%的训练数据中毒并分布在客户端之间，攻击成功率就超过85%，而主要任务性能基本不受影响。关键发现是，针对恶意客户端攻击设计的最先进后门防御机制对这种威胁基本无效。

Conclusion: 研究结果强调了针对现代去中心化数据生态系统现实情况开发新防御机制的紧迫需求，揭示了传统联邦学习安全范式的局限性。

Abstract: Federated learning security research has predominantly focused on backdoor threats from a minority of malicious clients that intentionally corrupt model updates. This paper challenges this paradigm by investigating a more pervasive and insidious threat: \textit{backdoor vulnerabilities from low-concentration poisoned data distributed across the datasets of benign clients.} This scenario is increasingly common in federated instruction tuning for language models, which often rely on unverified third-party and crowd-sourced data. We analyze two forms of backdoor data through real cases: 1) \textit{natural trigger (inherent features as implicit triggers)}; 2) \textit{adversary-injected trigger}. To analyze this threat, we model the backdoor implantation process from signal aggregation, proposing the Backdoor Signal-to-Noise Ratio to quantify the dynamics of the distributed backdoor signal. Extensive experiments reveal the severity of this threat: With just less than 10\% of training data poisoned and distributed across clients, the attack success rate exceeds 85\%, while the primary task performance remains largely intact. Critically, we demonstrate that state-of-the-art backdoor defenses, designed for attacks from malicious clients, are fundamentally ineffective against this threat. Our findings highlight an urgent need for new defense mechanisms tailored to the realities of modern, decentralized data ecosystems.

</details>


### [7] [Natural Privacy Filters Are Not Always Free: A Characterization of Free Natural Filters](https://arxiv.org/abs/2602.15815)
*Matthew Regehr,Bingshan Hu,Ethan Leeman,Pasin Manurangsi,Pierre Tholoniat,Mathias Lécuyer*

Main category: cs.CR

TL;DR: 本文研究了自然隐私过滤器，它能精确组合具有自适应选择隐私特性的差分隐私机制。研究发现，与其它形式的差分隐私不同，自然隐私过滤器通常不是"免费"的，只有那些在组合时具有良序性的隐私机制族才允许免费的过滤器。


<details>
  <summary>Details</summary>
Motivation: 现有隐私过滤器仅考虑简单的隐私参数（如Rényi-DP或高斯DP参数），而自然过滤器考虑每个查询的完整隐私配置文件，承诺在给定隐私预算下提供更大的效用。需要研究自然隐私过滤器的性质和限制。

Method: 研究自然隐私过滤器，分析其与差分隐私机制组合的特性。通过理论分析探讨哪些类型的隐私机制族在组合时允许免费的过滤器。

Result: 研究发现自然隐私过滤器通常不是免费的，这与其它形式的差分隐私不同。只有那些在组合时具有良序性的隐私机制族才允许免费的过滤器。

Conclusion: 自然隐私过滤器虽然能提供更好的效用，但通常需要付出代价。只有特定类型的隐私机制族才能实现免费的过滤器，这为设计高效的隐私保护系统提供了重要指导。

Abstract: We study natural privacy filters, which enable the exact composition of differentially private (DP) mechanisms with adaptively chosen privacy characteristics. Earlier privacy filters consider only simple privacy parameters such as Rényi-DP or Gaussian DP parameters. Natural filters account for the entire privacy profile of every query, promising greater utility for a given privacy budget. We show that, contrary to other forms of DP, natural privacy filters are not free in general. Indeed, we show that only families of privacy mechanisms that are well-ordered when composed admit free natural privacy filters.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Distributed Semi-Speculative Parallel Anisotropic Mesh Adaptation](https://arxiv.org/abs/2602.15204)
*Kevin Garner,Polykarpos Thomadakis,Nikos Chrisochoides*

Main category: cs.DC

TL;DR: 本文提出了一种避免使用集体通信和全局同步技术的分布式内存各向异性网格自适应方法，通过分离网格生成功能和性能方面，利用多核cc-NUMA网格生成软件和并行运行时系统，实现了高达约10亿元素的网格生成。


<details>
  <summary>Details</summary>
Motivation: 传统分布式内存网格自适应方法通常依赖集体通信和全局同步技术，这些技术在高性能计算架构中可能成为性能瓶颈。本文旨在开发一种避免这些限制的分布式内存方法，以更好地利用新兴HPC架构的并发性。

Method: 方法将网格功能与性能方面分离：使用多核cc-NUMA共享内存网格生成软件处理网格生成，并行运行时系统管理性能。首先在单个多核节点上分解初始网格并自适应接口元素，然后将子域分配到HPC集群节点，在保持接口元素冻结的情况下自适应内部元素，以维持网格一致性。

Result: 该方法能够生成高达约10亿元素的网格，其质量和性能与现有最先进的HPC网格生成软件相当。通过重新设计共享内存软件并利用其推测执行模型，实现了良好的性能表现。

Conclusion: 提出的分布式内存各向异性网格自适应方法通过避免集体通信和全局同步，有效利用了HPC架构的并发性，能够生成大规模高质量网格，性能与现有先进方法相当，为大规模科学计算提供了有效的网格生成解决方案。

Abstract: This paper presents a distributed memory method for anisotropic mesh adaptation that is designed to avoid the use of collective communication and global synchronization techniques. In the presented method, meshing functionality is separated from performance aspects by utilizing a separate entity for each - a multicore cc-NUMA-based (shared memory) mesh generation software and a parallel runtime system that is designed to help applications leverage the concurrency offered by emerging high-performance computing (HPC) architectures. First, an initial mesh is decomposed and its interface elements (subdomain boundaries) are adapted on a single multicore node (shared memory). Subdomains are then distributed among the nodes of an HPC cluster so that their interior elements are adapted while interface elements (already adapted) remain frozen to maintain mesh conformity. Lessons are presented regarding some re-designs of the shared memory software and how its speculative execution model is utilized by the distributed memory method to achieve good performance. The presented method is shown to generate meshes (of up to approximately 1 billion elements) with comparable quality and performance to existing state-of-the-art HPC meshing software.

</details>


### [9] [Service Orchestration in the Computing Continuum: Structural Challenges and Vision](https://arxiv.org/abs/2602.15794)
*Boris Sedlak,Víctor Casamayor Pujol,Ildefons Magrans de Abril,Praveen Kumar Donta,Adel N. Toosi,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文分析了计算连续体（CC）中服务编排的挑战，提出了自主服务编排的理想解决方案，并指出当前研究面临的结构性挑战，特别是需要标准化的仿真和评估环境。


<details>
  <summary>Details</summary>
Motivation: 计算连续体整合了从边缘到云的不同处理层，以优化服务质量，但异构和动态的基础设施增加了服务编排的复杂性。需要研究自主服务编排解决方案来应对这些挑战。

Method: 首先总结计算连续体的结构性问题，然后设想自主服务编排的理想解决方案。以主动推理（源自神经科学的概念）为例，展示其如何支持自组织服务持续解释环境以优化服务质量。

Result: 目前没有现有解决方案能够完全实现作者的愿景，服务编排研究面临多个结构性挑战，特别是缺乏标准化的仿真和评估环境来比较不同编排机制的性能。

Conclusion: 本文提出的挑战为计算连续体中弹性和可扩展服务编排的研究路线图奠定了基础，强调了标准化评估环境的重要性，以推动该领域的发展。

Abstract: The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该论文研究如何通过修改教师模型生成的推理轨迹来防止未经授权的知识蒸馏，实现反蒸馏和API水印两种目标。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏被广泛用于将大语言模型能力迁移到更小的学生模型，但未经授权的知识蒸馏不公平地利用了前沿模型开发的大量投入和成本，因此需要防止这种未经授权的使用。

Method: 引入多种动态重写教师模型推理输出的方法：包括两种利用LLM重写能力的方法，以及其他基于梯度的技术。这些方法在保持答案正确性和语义连贯性的同时修改推理轨迹。

Result: 实验表明，简单的基于指令的重写方法能实现强大的反蒸馏效果，同时保持甚至提升教师模型性能。此外，该方法还能实现高度可靠的水印检测，几乎没有误报。

Conclusion: 通过重写教师模型的推理输出可以有效防止未经授权的知识蒸馏，既能实现反蒸馏效果，又能嵌入可验证的水印，为保护模型知识产权提供了实用方法。

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [11] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: 本文提出了一种基于Carnapian-Goguenism的本体异质性新方法，命名为da Costian-Tarskianism，使用扩展后果系统和扩展发展图来关联本体。


<details>
  <summary>Details</summary>
Motivation: 为了解决本体论中的异质性问题，作者借鉴了Kutz、Mossakowski和Lücke提出的Carnapian-Goguenism思想，旨在建立一种更灵活的本体关联框架。

Method: 基于da Costa的数学容忍原则和Tarski的后果算子概念，使用Carnielli等人发展的后果系统机制，引入扩展后果系统（包含本体公理）和扩展发展图（通过扩展后果系统的态射以及纤维化和分裂等操作关联本体）。

Result: 提出了da Costian-Tarskianism方法，定义了扩展后果系统和扩展发展图的概念，为处理本体异质性提供了新的理论框架。

Conclusion: 该方法为应用本体论领域提供了新的理论工具，并指出了未来研究方向，包括进一步探索扩展发展图的应用和本体集成方法。

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [12] [Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173)
*Luise Ge,Yongyan Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该研究比较了20个前沿和开源大语言模型在风险决策中的表现，发现LLM可分为推理模型和对话模型两类，前者更理性，后者更接近人类但理性程度较低。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型作为决策支持系统或智能体工作流正在快速改变数字生态系统，但对其在不确定性下的决策机制理解仍然有限。研究旨在比较LLM在风险选择中的表现。

Method: 研究从两个维度比较LLM的风险决策：(1)前景表示方式（显式vs基于经验）和(2)决策理由（解释说明）。涉及20个前沿和开源LLM，并辅以匹配的人类被试实验作为参考点，同时以期望收益最大化的理性智能体模型作为另一个参考。

Result: 发现LLM可分为两类：推理模型（RMs）倾向于理性行为，对前景顺序、得失框架和解释不敏感，在显式或经验历史呈现前景时表现相似；对话模型（CMs）理性程度显著较低，更接近人类，对前景顺序、框架和解释敏感，且存在较大的描述-历史差距。开源LLM的配对比较表明，区分RMs和CMs的关键因素是数学推理训练。

Conclusion: LLM在风险决策中存在明显分化，推理模型表现出更强的理性特征，而对话模型更接近人类决策模式但理性程度不足。数学推理训练是区分两类模型的关键因素，这对LLM在决策支持系统中的应用具有重要意义。

Abstract: The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

</details>


### [13] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: 本文提出AI机器学习框架，补充确定性算法预测发票稀释风险，使用实时动态信用限额替代传统不可撤销支付承诺


<details>
  <summary>Details</summary>
Motivation: 发票或支付稀释（批准发票金额与实际收款之间的差距）是供应链金融中非信用风险和利润损失的重要来源。传统上通过买方不可撤销支付承诺管理此风险，但这会阻碍供应链金融的采用，特别是对于次级投资级买方。需要更灵活的数据驱动方法。

Method: 提出AI机器学习框架，补充确定性算法来预测发票稀释。使用实时动态信用限额方法，为每个买方-供应商对实时预测稀释风险。基于包含九个关键交易字段的广泛生产数据集进行评估。

Result: 论文评估了AI机器学习框架如何补充确定性算法来预测发票稀释风险。使用实时动态信用限额方法可以更灵活地管理稀释风险，特别是对于传统IPU方法难以覆盖的次级投资级买方。

Conclusion: AI机器学习框架可以有效补充传统确定性算法，为供应链金融中的发票稀释风险管理提供更灵活、数据驱动的解决方案，有助于扩大供应链金融的采用范围，特别是对于次级投资级买方。

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [14] [When Remembering and Planning are Worth it: Navigating under Change](https://arxiv.org/abs/2602.15274)
*Omid Madani,J. Brian Burns,Reza Eghbali,Thomas L. Dean*

Main category: cs.AI

TL;DR: 研究不同记忆类型和用途如何帮助智能体在变化的不确定环境中进行空间导航，发现结合多种策略的架构在处理探索、搜索和路径规划等不同性质子任务时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究在动态变化、感知受限的不确定环境中，如何通过记忆和学习机制提高空间导航效率。环境具有非平稳性（障碍物和食物位置每日变化）、感知不确定性（位置信息有限且不确定）等挑战。

Method: 采用从简单到复杂的多种策略，包括不同记忆使用和学习方式。重点研究能够整合多种策略的架构，特别是利用非平稳概率学习技术更新情景记忆，并使用这些记忆动态构建地图和规划路径（不完美地图，基于智能体经验且包含噪声）。

Result: 当任务难度（如目标距离）增加时，使用非平稳概率学习技术更新情景记忆并动态构建地图规划的智能体，相比简单（最小记忆）智能体效率显著提高，前提是定位和环境变化带来的不确定性不过大。

Conclusion: 在变化的不确定环境中进行空间导航需要能够整合多种策略的架构，特别是结合探索/搜索和路径规划的不同需求。利用非平稳概率学习更新记忆并动态构建地图的方法在适度不确定性条件下能显著提高导航效率。

Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.

</details>


### [15] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 提出自适应弃权系统，通过动态调整安全阈值和级联检测架构，解决LLM部署中的安全-效用权衡问题，在保持高性能的同时减少误报。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中面临安全与效用的根本权衡：严格过滤会阻止良性查询，宽松控制则可能生成不安全内容。传统基于静态规则或固定置信度阈值的护栏通常缺乏上下文敏感性且计算成本高，导致高延迟和用户体验下降。

Method: 引入自适应弃权系统，基于实时上下文信号（如领域和用户历史）动态调整安全阈值。采用多维检测架构，包含五个并行检测器，通过分层级联机制优化速度和精度。级联设计通过逐步过滤查询减少不必要的计算。

Result: 在混合和特定领域工作负载上的广泛评估显示，误报显著减少，特别是在医疗建议和创意写作等敏感领域。系统在严格操作模式下保持高安全精度和接近完美的召回率。与非级联模型和外部护栏系统相比，实现了显著的延迟改进。

Conclusion: 上下文感知的弃权框架有效平衡了安全性和效用，同时保持性能，为可靠的LLM部署提供了可扩展的解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [16] [Common Belief Revisited](https://arxiv.org/abs/2602.15403)
*Thomas Ågotnes*

Main category: cs.AI

TL;DR: 本文解决了关于共同信念逻辑的开放性问题，证明了在KD45个体信念下，共同信念的逻辑不仅需要KD4加上移位自反性公理，还需要一个额外的公理，且该公理依赖于智能体数量。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为共同信念具有KD4逻辑特性，但作者发现当个体信念为KD45时，共同信念会失去5属性而保留D和4属性，并具有移位自反性（C(Cφ→φ)）。这引发了一个开放性问题：KD4加上移位自反性公理是否足以完全刻画共同信念？

Method: 通过逻辑分析证明，作者展示了KD4加移位自反性公理不足以完全刻画共同信念，需要引入一个额外的公理，且该公理依赖于智能体数量。最终给出了共同信念的完整逻辑刻画。

Result: 证明了第一个问题的答案为"否"：KD4加移位自反性公理不足以完全刻画共同信念。存在一个额外的公理，且该公理依赖于智能体数量。最终得到了共同信念的完整逻辑刻画，解决了这个开放性问题。

Conclusion: 本文完全刻画了在KD45个体信念下的共同信念逻辑，证明了需要KD4、移位自反性公理以及一个依赖于智能体数量的额外公理，从而解决了该领域的开放性问题。

Abstract: Contrary to common belief, common belief is not KD4.
  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(Cφ\rightarrow φ)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:
  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.

</details>


### [17] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: EduEVAL-DB是一个基于教师角色的数据集，用于评估和训练自动教学评估器和AI导师，包含854个解释对应139个ScienceQA问题，涵盖K-12科学、语言和社会科学领域。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估和训练自动教学评估器和AI导师的数据集，特别是在教学解释质量评估方面。需要建立一个基于实际教育实践中观察到的教学风格和不足的数据集，以支持教育AI系统的开发和评估。

Method: 1. 基于ScienceQA基准的精选子集构建数据集，包含139个问题
2. 为每个问题提供1个人类教师解释和6个LLM模拟的教师角色解释
3. 通过提示工程实例化基于实际教育实践观察的教学风格和不足的教师角色
4. 提出与教育标准一致的教学风险评估框架，包含五个维度
5. 采用半自动流程加专家教师审查的方式进行二元风险标注

Result: 1. 创建了包含854个解释的EduEVAL-DB数据集
2. 建立了包含五个互补风险维度的教学风险评估框架
3. 初步验证实验表明数据集适用于评估目的
4. 基准测试显示教育导向模型(Gemini 2.5 Pro)与轻量级本地模型(Llama 3.1 8B)的性能对比
5. 监督微调实验表明EduEVAL-DB可用于支持教学风险检测，且可在消费级硬件上部署

Conclusion: EduEVAL-DB是一个有价值的资源，可用于评估和训练自动教学评估器和AI导师。数据集基于教师角色设计，结合了人类教师解释和LLM模拟的多样化教学风格，并提供了系统化的风险评估框架。初步验证表明该数据集在支持教学风险检测方面具有实用性，为教育AI系统的开发和评估提供了重要基础。

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [18] [Quantifying construct validity in large language model evaluations](https://arxiv.org/abs/2602.15532)
*Ryan Othniel Kearns*

Main category: cs.AI

TL;DR: 该论文提出结构化能力模型，首次从大量LLM基准测试结果中提取可解释且可泛化的能力，解决了现有方法在构建效度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社区常将基准测试结果等同于模型通用能力，但基准测试存在测试集污染、标注错误等问题。现有方法（潜在因子模型和缩放定律）都无法令人满意地解决构建效度问题，需要一种能分离模型规模与真实能力的方法。

Method: 提出结构化能力模型，结合了缩放定律和潜在因子模型的优点：模型规模影响能力（如缩放定律），能力影响观测结果并考虑测量误差（如潜在因子模型）。在OpenLLM排行榜的大规模结果样本上拟合该模型。

Result: 结构化能力模型在简约拟合指标上优于潜在因子模型，在分布外基准预测上优于缩放定律。该模型能更好地解释和预测LLM评估中的构建效度。

Conclusion: 结构化能力模型通过适当分离模型规模与能力，结合了缩放定律和潜在因子模型的洞见，为量化LLM评估的构建效度提供了更好的解释和预测能力。

Abstract: The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.
  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.
  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.

</details>


### [19] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: Ruva提出首个"透明盒"架构，用于人类参与的记忆管理，通过个人知识图谱取代向量匹配，实现可检查、可精确删除的AI记忆系统


<details>
  <summary>Details</summary>
Motivation: 当前个人AI领域被"黑盒"检索增强生成主导，向量数据库缺乏可追溯性：当AI产生幻觉或检索敏感数据时，用户无法检查原因或纠正错误。更严重的是，从向量空间中"删除"概念在数学上不精确，会留下违反真正隐私的概率"幽灵"

Method: 提出Ruva架构，将个人AI建立在个人知识图谱基础上，从向量匹配转向图谱推理，实现人类参与的记忆管理，让用户能够检查AI知道什么并执行特定事实的精确删除

Result: Ruva确保"被遗忘权"，用户成为自己生活的编辑者，系统提供透明、可追溯、可精确删除的AI记忆管理能力

Conclusion: 通过个人知识图谱替代向量数据库，Ruva解决了当前AI系统的可追溯性和隐私保护问题，实现了真正的人类中心化AI记忆管理

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [20] [How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning](https://arxiv.org/abs/2602.15580)
*Hongxuan Wu,Yukun Zhang,Xueqing Zhou*

Main category: cs.AI

TL;DR: 该研究提出PID Flow框架，通过信息分解分析多模态Transformer中视觉和语言信息的流动模式，发现在LLaVA模型中视觉信息早期达到峰值后衰减，语言信息在深层占主导（约82%），跨模态协同作用很低（<2%）。


<details>
  <summary>Details</summary>
Motivation: 研究多模态Transformer在回答视觉问题时，预测是受视觉证据、语言推理还是真正的跨模态计算驱动，以及这种结构如何在不同层间演化。旨在理解视觉信息如何转化为语言信息的信息流动机制。

Method: 提出基于部分信息分解（PID）的层间分析框架，引入PID Flow管道（结合降维、正态化流高斯化和闭式高斯PID估计），应用于LLaVA-1.5-7B和LLaVA-1.6-7B模型，并在六个GQA推理任务上进行评估。通过注意力敲除实验建立因果关系。

Result: 发现一致的模态转换模式：视觉独特信息早期达到峰值后衰减，语言独特信息在深层激增（约占最终预测的82%），跨模态协同作用保持在2%以下。这种模式在不同模型变体间高度稳定（层间相关性>0.96），但强烈依赖于任务。注意力敲除实验显示破坏主要转换路径会导致视觉信息滞留、补偿性协同作用增加和信息成本上升。

Conclusion: 研究为多模态Transformer中视觉如何转化为语言提供了信息论和因果解释，为识别模态特定信息丢失的架构瓶颈提供了量化指导。揭示了当前多模态模型主要依赖语言推理而非深度融合计算。

Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\% of the final prediction, and cross-modal synergy remains below 2\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.

</details>


### [21] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: 提出一种预处理方法，通过发现任务覆盖集、强化不等式并注入约束，改善累积约束的传播效果


<details>
  <summary>Details</summary>
Motivation: 传统约束规划中累积约束通常单独传播，忽略了多资源交互，导致在某些基准测试中性能严重下降

Method: 将累积约束解释为占用向量的线性不等式，通过发现不能并行运行的任务覆盖集，使用提升技术强化这些不等式，然后将生成的约束注入调度问题实例

Result: 在标准RCPSP和RCPSP/max测试套件上，推断出的约束改善了搜索性能并收紧目标界限，发现了25个新的下界和5个新的最优解

Conclusion: 该方法能够有效捕捉多资源交互，提升约束传播效果，在有利实例上显著改善性能，在不利实例上仅带来轻微性能下降

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [22] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: CARE Drive是一个模型无关的框架，用于评估自动驾驶中视觉语言模型的"原因响应性"，通过对比基准模型和原因增强模型在受控上下文变化下的决策，判断人类原因是否真正影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注结果性能（如安全性和轨迹精度），但无法确定模型决策是否反映人类相关考虑因素。这导致无法区分模型解释是真正的理性决策还是事后合理化，在安全关键领域可能产生虚假信心。

Method: CARE Drive采用两阶段评估过程：1) 提示校准确保稳定输出；2) 系统性的上下文扰动，测量决策对人类原因（如安全边际、社会压力、效率约束）的敏感性。通过对比基准模型和原因增强模型在受控上下文变化下的决策，评估原因响应性。

Result: 在自行车超车场景中，明确的人类原因显著影响模型决策，提高了与专家推荐行为的一致性。然而，响应性在不同上下文因素间存在差异，表明模型对不同类型原因的敏感性不均匀。

Conclusion: CARE Drive提供了无需修改模型参数即可系统评估基础模型原因响应性的实证证据，为自动驾驶中视觉语言模型的可靠评估提供了新方法。

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [23] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: PERSONA框架通过激活空间中的向量操作实现LLM人格控制，无需训练即可达到微调级别性能，揭示人格特质在表示空间中表现为可提取的近似正交方向。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型人格控制方法依赖静态提示或昂贵的微调，无法捕捉人类特质的动态性和组合性，需要更高效、可解释的控制框架。

Method: 提出三阶段框架：1) Persona-Base通过对比激活分析提取正交特质向量；2) Persona-Algebra通过向量算术实现精确控制（标量乘法调节强度，加法组合，减法抑制）；3) Persona-Flow在推理时动态组合向量实现上下文感知适应。

Result: 在PersonalityBench上平均得分9.60，接近监督微调上限9.61；在Persona-Evolve动态人格适应基准上，跨不同模型家族达到最高91%的胜率。

Conclusion: LLM的人格方面具有数学可处理性，为可解释和高效的行为控制开辟了新方向，证明了通过激活空间向量操作实现训练自由人格控制的可行性。

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [24] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: RCE框架让预训练语言模型能在推理时动态修改内部表征几何，通过生成低秩概念子空间来构建新抽象，显著提升组合推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩展token级搜索（如思维链、自一致性、强化学习）来改进推理，但保持模型的潜在表征空间固定。当所需抽象未编码在该空间中时，性能会急剧下降

Method: 提出递归概念演化(RCE)框架，在推理时检测表征不足时动态生成低秩概念子空间，通过最小描述长度准则选择，在协同时合并，并通过约束优化进行整合以保持稳定性

Result: 在Mistral-7B上集成RCE，在组合推理基准测试中：ARC-AGI-2提升12-18点，GPQA和BBH提升8-14点，在MATH和HLE上持续减少深度诱导误差

Conclusion: RCE使预训练语言模型能够在推理时修改内部表征几何，构建新抽象而非仅重组现有概念，显著提升组合推理性能

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [25] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: GlobeDiff算法通过多模态扩散过程从局部观测推断全局状态，解决了多智能体系统中部分可观测性的挑战


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的部分可观测性是协调和决策的关键障碍。现有方法如信念状态估计和智能体间通信存在局限：信念方法主要依赖过去经验而未能充分利用全局信息，通信方法缺乏有效利用辅助信息的鲁棒模型

Method: 提出Global State Diffusion Algorithm (GlobeDiff)，将状态推断过程建模为多模态扩散过程，基于局部观测推断全局状态。该方法通过扩散过程克服状态估计中的模糊性，同时以高保真度推断全局状态

Result: 证明了GlobeDiff在单模态和多模态分布下的估计误差都有界。大量实验结果表明，GlobeDiff实现了优越性能，能够准确推断全局状态

Conclusion: GlobeDiff通过多模态扩散过程有效解决了多智能体系统中的部分可观测性问题，相比现有方法在全局状态推断方面表现更优

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [26] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: 论文探讨了在社会科学实验中使用LLM作为合成参与者的有效性，对比了启发式方法和统计校准两种策略，分析了它们在探索性与验证性研究中的适用性。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多研究使用大型语言模型作为合成参与者来生成经济高效且即时的响应，但缺乏关于何时这种模拟能够有效推断人类行为的指导。需要明确在什么条件下LLM模拟能支持对人类行为的有效推断。

Method: 对比两种策略：1) 启发式方法：通过提示工程、模型微调等修复策略使模拟与观察的人类行为可互换；2) 统计校准：结合辅助人类数据与统计调整，在明确假设下保持有效性。分析两种方法在探索性与验证性研究中的适用性。

Result: 启发式方法适用于许多探索性任务，但缺乏验证性研究所需的正式统计保证。统计校准在明确假设下能保持有效性，并以比仅依赖人类参与者的实验更低的成本提供更精确的因果效应估计。两种方法的潜力都取决于LLM对相关人群的近似程度。

Conclusion: 研究者不应仅仅狭隘地关注用LLM替代研究中的参与者，而应考虑被忽视的机会。需要明确不同方法的适用条件和假设，以支持有效的因果推断。

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [27] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: 该研究提出使用大语言模型（LLM）嵌入作为编码方法，以更好地捕捉建筑语义中的细微差别，相比传统one-hot编码在建筑对象子类型分类任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 在AECO行业中，准确表示建筑语义（包括通用对象类型和特定子类型）对AI模型训练至关重要。传统的编码方法（如one-hot）无法有效传达密切相关的子类型之间的细微关系，限制了AI的语义理解能力。

Method: 提出一种新颖的训练方法，使用LLM嵌入（OpenAI GPT和Meta LLaMA）作为编码来保留建筑语义的精细区别。评估方法包括训练GraphSAGE模型对5个高层住宅BIM中的42个建筑对象子类型进行分类，测试了不同嵌入维度，包括原始高维LLM嵌入和通过Matryoshka表示模型生成的1024维压缩嵌入。

Result: 实验结果表明，LLM编码优于传统的one-hot基线，其中llama-3（压缩）嵌入的加权平均F1分数达到0.8766，而one-hot编码为0.8475。LLM编码在捕捉建筑语义的细微区别方面表现更好。

Conclusion: LLM编码在增强AI解释复杂、领域特定的建筑语义方面具有潜力。随着LLM和降维技术的不断发展，这种方法在AECO行业的语义细化任务中具有广泛的应用前景。

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [28] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本章介绍基于仿真的合成数据生成技术，用于解决AI训练中数据不足和质量问题，并提出了数字孪生AI仿真解决方案的参考框架。


<details>
  <summary>Details</summary>
Motivation: 现代符号AI的采用面临数据量不足和数据质量差的关键障碍，因此对合成数据生成技术有很高的需求。

Method: 采用仿真方法进行系统化的合成数据生成，并提出了描述、设计和分析数字孪生AI仿真解决方案的参考框架。

Result: 本章介绍了基于仿真的合成数据生成的关键概念、优势和挑战，为AI训练提供了系统化的解决方案。

Conclusion: 仿真为AI训练提供了有效的合成数据生成方法，数字孪生框架为设计和分析AI仿真解决方案提供了系统化的参考。

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [29] [Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15166)
*Tanner Andrulis,Michael Gilbert,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: FFM是首个能快速在张量代数工作负载的融合映射空间中寻找最优映射的映射器，通过剪枝非最优子映射并组合部分映射来构建最优融合映射，相比现有方法速度提升超过1000倍。


<details>
  <summary>Details</summary>
Motivation: 现有映射器无法在可行时间内找到融合的最优映射，因为融合映射的搜索空间随计算步骤数量呈指数增长。需要一种能快速找到最优融合映射的方法来充分发挥张量代数加速器的潜力。

Method: FFM通过剪枝永远不会成为最优映射一部分的部分映射（子映射）来缩小搜索空间，然后组合部分映射来构建最优融合映射。该方法能有效处理指数增长的映射空间。

Result: FFM的运行时间随计算步骤数量近似线性增长，而映射空间大小呈指数增长。对于Transformer工作负载，FFM比现有最优方法快1000倍以上。

Conclusion: FFM是首个能快速在融合映射空间中寻找最优映射的映射器，解决了现有方法无法在可行时间内找到最优融合映射的问题，为张量代数加速器的性能评估提供了有效工具。

Abstract: The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.
  In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers.

</details>


### [30] [The Turbo-Charged Mapper: Fast and Optimal Mapping for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15172)
*Michael Gilbert,Tanner Andrulis,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: TCM是一种能保证找到最优映射的快速映射器，通过数据放置概念大幅减少搜索空间，相比现有方法能更快找到最优解


<details>
  <summary>Details</summary>
Motivation: 现有加速器映射优化方法使用启发式或元启发式算法，无法保证找到最优映射，这阻碍了硬件评估的准确性，因为无法区分性能差异是源于硬件改进还是次优映射

Method: 提出Turbo-Charged Mapper (TCM)，引入数据放置新概念，通过分析识别冗余和次优映射机会，将搜索空间减少多达32个数量级

Result: TCM能在不到1分钟内找到最优映射，而现有方法即使给予1000倍运行时间（>10小时）也无法找到最优映射，其能量延迟乘积比最优解高21%

Conclusion: TCM是首个能在可行运行时间内执行完整映射空间搜索的映射器，保证了最优映射的发现，为硬件评估提供了可靠工具

Abstract: The energy and latency of an accelerator running a deep neural network (DNN) depend on how the computation and data movement are scheduled in the accelerator (i.e., mapping). Optimizing mappings is essential to evaluating and designing accelerators. However, the space of mappings is large, and prior works can not guarantee finding optimal mappings because they use heuristics or metaheuristics to narrow down the space. These limitations preclude proper hardware evaluation, since designers can not tell whether performance differences are due to changes in hardware or suboptimal mapping.
  To address this challenge, we propose the Turbo-Charged Mapper (TCM), a fast mapper that is guaranteed to find optimal mappings. The key to our approach is that we define a new concept in mapping, called dataplacement, which, like the prior concept of dataflow, allows for clear analysis and comparison of mappings. Through it, we identify multiple opportunities to prune redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude.
  Leveraging these insights, TCM can perform full mapspace searches, making it the first mapper that can find optimal mappings in feasible runtime. Compared to prior mappers, we show that TCM can find optimal mappings quickly (less than a minute), while prior works can not find optimal mappings (energy-delay-product $21\%$ higher than optimal) even when given $1000\times$ the runtime ($>10$ hours).

</details>


### [31] [Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis](https://arxiv.org/abs/2602.15336)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: 研究评估了GPT、Gemini和Claude三种主流大语言模型在数字逻辑问题上的表现，发现模型在形式正确性上表现不佳，但学生仍认为其有帮助，揭示了感知有用性与实际准确性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地被本科生用作按需导师，但其在电路和图表类数字逻辑问题上的可靠性尚不清楚。需要评估这些模型在核心数字逻辑主题上的表现，以了解其在本科教学中的适用性。

Method: 进行了人机交互研究，评估三种主流LLM在10个本科水平数字逻辑问题上的表现。24名学生进行配对模型比较，提供每个问题的偏好、感知正确性、一致性、冗长度和信心等评价。同时采用独立法官评估，对照官方解决方案使用严格正确性标准进行技术有效性基准测试。

Result: 结果显示感知有用性与形式正确性之间存在一致差距：对于最需要顺序推理的问题（Q1-Q7），所有评估的LLM都无法匹配官方答案，尽管它们产生了自信、结构良好的解释，学生通常给予积极评价。错误分析表明模型经常默认使用标准教科书模板，难以将电路结构转化为精确的状态演化和时序行为。

Conclusion: 研究发现，在没有验证框架的情况下，大语言模型在核心数字逻辑主题上可能不可靠，并可能在本科教学中无意中强化错误概念。这强调了在数字逻辑教育中使用LLM时需要验证机制的重要性。

Abstract: Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction.

</details>


### [32] [Iterative LLM-Based Assertion Generation Using Syntax-Semantic Representations for Functional Coverage-Guided Verification](https://arxiv.org/abs/2602.15388)
*Yonghao Wang,Jiaxin Zhou,Yang Yin,Hongqin Lyu,Zhiteng Chao,Wenchao Ding,Jing Ye,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: CoverAssert是一个基于LLM的迭代框架，通过功能覆盖率反馈机制优化SystemVerilog断言生成质量，解决了现有方法因LLM对IC设计理解不足导致的断言质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有利用LLM从自然语言规范自动生成SystemVerilog断言的技术面临关键挑战：LLM通常缺乏足够的IC设计理解，导致单次生成的断言质量较差。验证生成的断言是否有效覆盖功能规范，并基于此覆盖率设计反馈机制仍然是重大障碍。

Method: CoverAssert引入轻量级机制，将生成的断言与规范中的具体功能描述进行匹配。通过聚类LLM生成断言的语义特征和从抽象语法树提取的断言相关信号结构特征的联合表示，然后将其映射回规范以分析功能覆盖率质量。基于功能覆盖率构建反馈循环，指导LLM优先处理未覆盖的功能点，从而迭代改进断言质量。

Result: 在四个开源设计上的实验评估表明，将CoverAssert与最先进的生成器AssertLLM和Spec2Assertion集成后，平均改进：分支覆盖率9.57%，语句覆盖率9.64%，翻转覆盖率15.69%。

Conclusion: CoverAssert通过功能覆盖率驱动的迭代优化框架，显著提升了LLM生成SystemVerilog断言的质量和覆盖率，为解决现有技术中的关键挑战提供了有效方案。

Abstract: While leveraging LLMs to automatically generate SystemVerilog assertions (SVAs) from natural language specifications holds great potential, existing techniques face a key challenge: LLMs often lack sufficient understanding of IC design, leading to poor assertion quality in a single pass. Therefore, verifying whether the generated assertions effectively cover the functional specifications and designing feedback mechanisms based on this coverage remain significant hurdles. To address these limitations, this paper introduces CoverAssert, a novel iterative framework for optimizing SVA generation with LLMs. The core contribution is a lightweight mechanism for matching generated assertions with specific functional descriptions in the specifications. CoverAssert achieves this by clustering the joint representations of semantic features of LLM-generated assertions and structural features extracted from abstract syntax trees (ASTs) about signals related to assertions, and then mapping them back to the specifications to analyze functional coverage quality. Leveraging this capability, CoverAssert constructs a feedback loop based on functional coverage to guide LLMs in prioritizing uncovered functional points, thereby iteratively improving assertion quality. Experimental evaluations on four open-source designs demonstrate that integrating CoverAssert with state-of-the-art generators, AssertLLM and Spec2Assertion, achieves average improvements of 9.57 % in branch coverage, 9.64 % in statement coverage, and 15.69 % in toggle coverage.

</details>
