<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 提出了一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，超越表面语义相似性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略以优化LLM性能仍是一个重要挑战。

Method: 采用粗到细的知识检索方法：1）识别知识库中与上下文相关的子区域；2）在该子区域内提取与推理过程相关的知识。两阶段都使用蒙特卡洛树搜索启发的方法，通过关键词在知识句子中导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更贴近人类对话的底层推理逻辑，还显著提升了检索知识的多样性，从而产生更具信息量和创造性的响应。

Conclusion: 提出的推理感知知识检索方法能够有效整合检索和推理策略，通过逻辑对齐的知识检索增强LLMs在多轮对话中的表现。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [2] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 该研究开发了一种基于大型语言模型的自动化抑郁症筛查工具，专门针对尼日利亚皮钦语使用者，解决了传统筛查工具在低收入国家存在的语言和文化障碍问题。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，主要原因是临床医生资源有限、社会污名化和语言障碍。传统的PHQ-9筛查工具在高收入国家验证，但对使用尼日利亚皮钦语和520多种当地语言的社区存在语言和文化上的不适应性。

Method: 收集了432份尼日利亚年轻人（18-40岁）的皮钦语音频回答，内容基于PHQ-9项目的心理体验评估。进行转录、严格预处理和标注，包括语义标注、俚语和习语解释、PHQ-9严重程度评分。对Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1三种大型语言模型进行微调，并定量（准确性、精确度、语义对齐）和定性（清晰度、相关性、文化适宜性）评估其性能。

Result: GPT-4.1在PHQ-9严重程度评分预测中达到94.5%的准确率，表现最佳，超过了Gemma-3-4B-it和Phi-3-mini-4k-instruct。在定性评估中，GPT-4.1也产生了最具文化适宜性、最清晰且上下文最相关的回答。

Conclusion: 该研究为在语言多样、资源有限的环境中部署对话式心理健康工具奠定了基础，展示了AI介导的抑郁症筛查在服务不足的尼日利亚社区中的潜力。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [3] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 本文提出一种多算法方法来解决最后一公里包裹配送中的人力资源工作量平衡问题，通过结合距离和工作量考虑来优化包裹分配，确保每位配送员每天完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统的基于地理邻近性的包裹分配方法效率低下，容易导致配送员之间工作量分布不均衡。在最后一公里城市包裹配送系统中，需要优化工作量分配，确保所有员工完成相似的工作量，纠正区域内配送员之间的显著工作量不平衡。

Method: 提出多算法方法，包括不同版本的k-means、进化方法、基于k-means初始化的递归分配（采用不同问题编码）以及混合进化集成算法。该方法以配送点集合和工人数量为输入，结合距离和工作量考虑来优化包裹分配。

Result: 在西班牙Azuqueca de Henares城市最后一公里包裹配送系统的实际案例中验证了所提方法的性能，展示了其在平衡配送员工作量方面的有效性。

Conclusion: 通过多算法方法优化最后一公里包裹配送系统的工作量分配，能够有效平衡配送员之间的工作量，提高系统效率，纠正工作量不平衡问题。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [4] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的战略性13张牌印度拉米纸牌游戏框架，使用新的手牌评估指标MinDist来量化手牌与最近有效配置之间的编辑距离，显著提高了胜率。


<details>
  <summary>Details</summary>
Motivation: 13张牌印度拉米是一种不完全信息的顺序游戏，需要概率推理和组合决策。传统启发式方法在战略游戏表现上有限，需要更正式和可解释的算法策略设计。

Method: 提出MinDist手牌评估指标，通过量化手牌与最近有效配置之间的编辑距离来捕捉结构接近度；设计计算高效的算法，利用动态剪枝和模式缓存；在双人零和模拟框架中整合对手手牌建模；使用统计假设检验评估策略。

Result: 实证结果显示，基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法性拉米策略设计提供了正式且可解释的进展。

Conclusion: MinDist指标和相应算法框架为不完全信息顺序游戏提供了有效的战略决策方法，在印度拉米游戏中展现出优越性能，为算法策略设计开辟了新途径。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [5] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探索生成式AI如何理解乡土建筑中的建筑智慧，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，评估AI对建筑类型、材料、环境、真实性和文化特性的重建能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索生成式AI系统如何解释乡土建筑形式中蕴含的建筑智慧，了解AI在重建传统设计智能时的能力与局限，为分析AI如何感知、扭曲和重新想象传统设计提供框架。

Method: 以伊朗鸽塔为案例研究，测试Midjourney v6、DALL-E 3和基于Stable Diffusion XL的DreamStudio三种扩散模型，采用三个提示阶段（参考性、适应性、推测性），通过五标准评估框架（类型学、材料性、环境、真实性、文化特异性）评估AI表现。

Result: AI能可靠地再现几何图案，但误解材料和气候推理；参考图像提高了真实性但限制了创造性，而无参考的自由生成则产生创新但文化模糊的结果；研究界定了视觉相似性与建筑推理之间的边界。

Conclusion: 研究提出了计算乡土推理框架，用于分析AI如何感知、扭曲和重新想象传统设计智能，揭示了AI在理解建筑深层逻辑方面的局限性，为未来AI在建筑领域的应用提供了重要参考。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [6] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈机制启发的智能自愈框架，用于分布式计算连续体系统，通过语言模型驱动的智能体实现自主故障隔离、诊断、自适应恢复和知识积累。


<details>
  <summary>Details</summary>
Motivation: 现代分布式计算连续体系统（DCCS）集成了从物联网设备到云基础设施的异构计算资源，其固有的复杂性、移动性和动态运行条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个层次（遏制、诊断、元认知、知识），通过语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上评估显示，ReCiSt能在数十秒内实现自愈，智能体CPU使用率最低为10%，同时展示了克服不确定性的分析深度和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功将生物自愈机制转化为计算系统的弹性策略，通过语言模型驱动的智能体实现了分布式计算连续体系统的自主故障恢复和知识积累。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [7] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 本文设计了一个基于大语言模型（LLM）的智能体，能够从原始文本中提取因果反馈模糊认知图（FCM），并通过FCM动力系统的平衡状态驱动LLM智能体获取和处理因果文本，形成双向交互的自主学习系统。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一个能够从文本中自动提取因果关系的智能系统，通过LLM智能体与FCM动力系统的双向交互，实现具有一定自主性的因果学习过程，同时保持可控性。

Method: 方法包括：1）设计LLM智能体通过三步精细调优的系统指令从文本中提取关键名词和名词短语；2）从这些名词和短语中提取FCM概念节点；3）推断FCM节点之间的部分或模糊因果边。最后混合不同LLM智能体生成的FCM。

Result: 测试结果显示：1）三步过程生成的FCM动力系统收敛到与人工生成的FCM相同的平衡极限环，尽管节点和边的数量不同；2）混合不同LLM智能体生成的FCM不仅吸收了主要成分的平衡状态，还创造了新的平衡状态，更好地近似了底层因果动力系统。

Conclusion: 研究表明，通过精细调优的LLM智能体能够有效从文本中提取FCM，实现具有一定自主性的因果学习系统。混合不同LLM生成的FCM能够产生新的平衡状态，增强对底层因果系统的近似能力。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [8] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统结合质量多样性算法和大语言模型，自动演化游戏机制，通过合成完整游戏评估机制质量，基于技能排序得分优化机制设计


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计是耗时且依赖专家经验的过程，需要自动化方法来探索多样化的游戏机制，提高游戏设计的效率和质量

Method: 结合质量多样性算法和大语言模型探索多样化机制，通过树搜索程序合成完整游戏进行评估，基于技能排序（强玩家始终优于弱玩家）作为评估标准

Result: Mortar能够生成多样化且可玩的游戏，产生的机制在游戏中贡献更高的技能排序得分，通过消融研究和用户研究验证系统有效性

Conclusion: Mortar系统成功实现了游戏机制的自主演化，为自动游戏设计提供了有效方法，结合算法评估和人类反馈验证了生成机制的质量

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [9] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: LLMs作为端到端库存优化求解器存在"幻觉税"性能差距，提出混合智能体框架分离语义推理与数学计算，通过数字孪生测试显示成本降低32.1%


<details>
  <summary>Details</summary>
Motivation: 中小企业在库存管理中缺乏部署高级优化方法的专业知识，需要探索LLMs是否能帮助弥合这一差距，但发现直接使用LLMs作为端到端求解器存在性能问题

Method: 提出混合智能体框架，严格分离语义推理与数学计算：LLM作为智能接口从自然语言提取参数并解释结果，同时自动调用严格算法构建优化引擎；引入"人类模仿者"数字孪生进行可扩展的压力测试

Result: 混合框架相比GPT-4o端到端求解器减少总库存成本32.1%；提供完美真实信息无法改善GPT-4o性能，确认瓶颈是计算而非信息问题

Conclusion: LLMs不应替代运筹学，而应作为自然语言接口，使基于严格求解器的策略对非专家可访问；混合框架能有效降低库存成本并解决LLMs的幻觉问题

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [10] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 该研究探讨了在视频问答任务中，基于置信度的弃权机制是否能为视觉语言模型提供可靠的错误率控制，特别是在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测，即在不确定时弃权以避免代价高昂的错误。研究旨在验证置信度阈值方法是否能提供可靠的错误率控制，以及这种控制在分布偏移下是否保持稳健。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过调整置信度阈值epsilon来建立风险-覆盖权衡曲线，评估在分布内和分布偏移条件下的性能。

Result: 研究发现：1）在分布内，置信度阈值提供了机制性控制，通过调整阈值可以平滑地降低错误率；2）研究还探讨了在分布偏移下的表现（摘要未完整展示）。

Conclusion: 置信度阈值方法在视频问答中能够为视觉语言模型提供错误率控制，但需要进一步验证其在分布偏移下的鲁棒性。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [11] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM驱动的智能体不仅存在人口统计学偏见，还会在最小群体线索下表现出群体间偏见。当这种群体边界与智能体-人类划分一致时，人类可能被智能体视为外群体。研究通过多智能体社会模拟发现了这种偏见，并提出了信念中毒攻击来抑制有利于人类的社会规范脚本。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM驱动的智能体是否会在最小群体线索下表现出群体间偏见，特别是当这种群体边界与智能体-人类划分一致时，人类可能被智能体视为外群体。这涉及到从人口统计学偏见向更基本的群体层面不对称性的风险转变。

Method: 研究方法包括：1）构建受控的多智能体社会模拟，基于明确收益权衡下的分配决策；2）设计信念中毒攻击，包括初始化时的档案中毒和通过优化信念精炼后缀注入存储反思中的记忆中毒；3）通过实验验证智能体群体间偏见的存在和BPA攻击的严重性。

Result: 实验结果表明：1）智能体在最小群体线索下表现出一致的群体间偏见；2）当部分对应方被框架为人类时，这种偏见会减弱，但这种减弱依赖于智能体相信真实人类存在的信念；3）信念中毒攻击能够有效抑制有利于人类的社会规范脚本，重新激活对人类的群体间偏见。

Conclusion: 研究结论是LLM驱动的智能体确实存在群体间偏见，且这种偏见可能针对人类群体。信念中毒攻击揭示了新的攻击面，需要在档案和记忆边界实施实际缓解策略来强化当前的智能体框架。研究的目的是识别这些漏洞以指导更安全的智能体设计，而非实现现实世界的利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [12] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: ACCD框架通过三阶段渐进式架构，利用记忆引导自适应机制动态学习最优检测配置，显著提升社交媒体协调虚假行为检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体协调虚假行为检测方法存在依赖表面相关性分析、使用静态参数设置、需要大量人工标注等问题，需要更系统化的解决方案。

Method: 提出自适应因果协调检测（ACCD）框架，采用三阶段渐进式架构：1）自适应收敛交叉映射技术识别账户间真实因果关系；2）半监督分类结合主动学习和不确定性采样减少人工标注；3）基于历史检测经验的自动化验证模块进行结果自验证和优化。

Result: 在真实数据集（Twitter IRA、Reddit协调痕迹等）上评估，ACCD在协调攻击检测中达到87.3%的F1分数，比最强基线提升15.2%；减少68%人工标注需求；通过层次聚类优化实现2.8倍处理加速。

Conclusion: ACCD为社交媒体平台协调行为识别提供了更准确、高效、高度自动化的端到端解决方案，具有重要实践价值和广泛应用潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [13] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 该研究将语义空间推理从计算语言学扩展到团队运动的战术决策，将球员视为"词汇"，团队配置视为"语义结构"，通过向量空间建模评估战术适配度并生成动态策略建议。


<details>
  <summary>Details</summary>
Motivation: 传统计算语言学中的语义空间推理方法在团队运动战术决策中尚未得到充分应用。研究者希望将文本与团队的类比关系（球员如词汇，集体配合如语义）扩展到战术分析领域，为团队运动提供更系统化的决策框架。

Method: 将每位球员表示为整合技术、身体和心理属性的多维向量，通过上下文加权聚合成团队语义表示。在共享向量空间中编码战术模板（如高位压迫、反击、控球组织），使用向量距离度量评估战术"适配度"和对手利用潜力。开发Python原型系统生成可解释的动态策略建议。

Result: 提出的方法能够生成可解释的、动态适应的策略建议，并提供属性层面的细粒度诊断洞察。该方法不仅适用于足球，还可推广到篮球、曲棍球等团队运动，以及协作机器人和人机协调系统等更广泛的集体决策领域。

Conclusion: 语义空间推理为团队运动的战术决策提供了通用框架，未来研究方向包括实际数据整合、预测性模拟以及混合人机战术智能系统的开发。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [14] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 该研究分析了推理模型是否具有"顿悟"时刻，发现中程推理转变很罕见，不会随训练变得更频繁，且很少提高准确性，表明这些转变并非模型内在的自我修正机制，而是不稳定推理行为的症状。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的中程转变，导致准确输出，暗示模型具有内在的自我修正能力。但尚不清楚这种推理策略的内在转变是否真正提高了性能。

Method: 研究中程推理转变，通过检测训练运行来识别这些转变。分析涵盖超过100万个推理轨迹、数百个训练检查点、三个推理领域、多个解码温度和模型架构。

Result: 发现推理转变很罕见，不会随训练变得更频繁，且很少提高准确性，表明它们并不对应先前的模型洞察认知。然而，其效果随模型不确定性而变化。基于此发现，研究显示在高熵条件下人为触发外部转变可以可靠地提高准确性。

Conclusion: 中程推理转变是不稳定推理行为的症状，而非内在的自我修正机制。虽然这些转变本身很少提高性能，但在高不确定性条件下人为触发转变可以改善模型准确性。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [15] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO提出了一种难度感知的直接偏好优化框架，通过估计偏好数据的难度并重新加权训练样本，解决多模态大语言模型中现有DPO方法因难度不平衡导致的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态直接偏好优化方法由于偏好数据中的难度不平衡问题容易过拟合，模型倾向于过度关注容易区分的偏好对，这阻碍了细粒度的幻觉抑制并降低了整体性能。

Method: DA-DPO包含两个主要组件：1) 难度估计：利用预训练的视觉-语言模型，结合生成性和对比性目标，通过分布感知投票策略产生稳健的难度分数；2) 难度感知训练：根据估计的难度重新加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合。

Result: 大量实验表明DA-DPO持续改进多模态偏好优化，在标准基准测试中表现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO通过难度感知的偏好优化框架有效解决了多模态DPO中的过拟合问题，无需新数据或额外微调阶段，实现了更有效的幻觉抑制和性能提升。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [16] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: 该研究提出PedX-LLM框架，通过视觉特征提取和交通领域知识增强，将行人过街行为推断从站点特定模式识别转变为可泛化的行为推理，显著提升了模型性能和在未见场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习方法）泛化能力有限，在新场景中表现不佳。现有大语言模型应用缺乏领域特定适应性和视觉上下文，需要开发能够结合视觉特征和领域知识的通用推理框架。

Method: 提出PedX-LLM框架：1）使用LLaVA提取视觉特征；2）结合文本数据和交通领域知识；3）通过LoRA对LLaMA-2-7B基础模型进行微调；4）采用零样本和少样本学习策略评估泛化能力。

Result: PedX-LLM达到82.0%的平衡准确率，优于最佳统计和监督学习方法。视觉增强模块贡献2.9%性能提升，领域知识集成带来额外4.1%改进。在五个未见测试站点上，零样本配置达到66.9%平衡准确率，少样本学习（仅5个验证样本）进一步提升至72.2%。

Conclusion: PedX-LLM通过视觉和知识增强的推理机制，能够模拟人类决策逻辑，克服纯数据驱动方法的局限性，在未见场景中表现出强大的泛化能力，为行人过街行为推断提供了新的范式。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [17] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 是一个通过智能体工作流将自由形式任务描述自动转换为完整 DomiKnowS 程序的系统，显著减少了神经符号程序开发时间


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高模型的鲁棒性、可解释性和数据效率，但这一过程耗时且具有挑战性。现有框架如 DomiKnowS 虽然提供了高级声明式编程接口，但仍要求用户精通其特定语法

Method: 提出 AgenticDomiKnowS (ADS) 系统，通过智能体工作流将自由形式任务描述翻译为完整的 DomiKnowS 程序。该工作流单独创建和测试每个 DomiKnowS 组件，并支持可选的人工干预环节，允许熟悉 DomiKnowS 的用户精炼中间输出

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟

Conclusion: AgenticDomiKnowS 通过消除对特定库语法的依赖，显著降低了神经符号编程的门槛，使更多研究人员能够高效地将符号约束集成到深度学习模型中

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation](https://arxiv.org/abs/2601.00450)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: 本文提出REAP-cache方案，解决STT-MRAM缓存中并行读取导致的读干扰错误累积问题，显著提升缓存可靠性


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为SRAM的替代方案具有高密度、低功耗等优势，但其可靠性受到高读干扰错误率的威胁。传统ECC方案在并行读取缓存块时，由于只检查请求块的ECC，导致其他并行读取块的读干扰错误累积，严重降低缓存可靠性。

Method: 首先引入并形式化读干扰累积现象，揭示传统并行访问导致的错误累积问题。然后提出REAP-cache方案，通过简单有效的机制完全消除读干扰累积，同时不损害缓存性能。

Result: 评估显示REAP-cache将缓存平均故障时间（MTTF）提升171倍，同时缓存面积增加小于1%，能耗仅增加2.7%。

Conclusion: REAP-cache方案能有效解决STT-MRAM缓存中的读干扰累积问题，显著提升可靠性，同时硬件开销和能耗增加极小，具有实际应用价值。

Abstract: Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.

</details>


### [19] [ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches](https://arxiv.org/abs/2601.00456)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: STT-MRAM作为片上缓存有前景但错误率高，传统ECC因数据依赖错误模式失效，提出ROBIN ECC配置显著提升纠错能力


<details>
  <summary>Details</summary>
Motivation: STT-MRAM是片上缓存的有前景替代方案，但其高错误率是主要限制因素。传统错误校正码因数据依赖错误模式而效率降低，需要更有效的纠错方案。

Method: 首先对传统ECC效率进行综合分析，揭示数据依赖错误模式问题，然后提出名为ROBIN的高效ECC配置来提升纠错能力。

Result: 评估显示传统ECC的低效使缓存错误率平均增加151.7%，而ROBIN将此值降低超过28.6倍。

Conclusion: ROBIN ECC配置能有效解决STT-MRAM中数据依赖错误模式问题，显著提升纠错能力，为STT-MRAM在片上缓存应用提供可行解决方案。

Abstract: Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Word Frequency Counting Based on Serverless MapReduce](https://arxiv.org/abs/2601.00380)
*Hanzhe Li,Bingchen Lin,Mengyuan Xu*

Main category: cs.DC

TL;DR: 本文结合无服务器计算和MapReduce模型，通过优化Map和Reduce函数数量来提升词频统计任务的执行效率。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算需求的增长，无服务器计算成为研究热点，而MapReduce作为流行的大数据处理模型已在各领域广泛应用。本文旨在结合无服务器计算的函数即服务框架和MapReduce的高并发与鲁棒性，减少词频统计任务的时间跨度并提高效率。

Method: 采用基于无服务器计算平台的MapReduce编程模型，针对特定任务寻找最优的Map函数和Reduce函数数量配置。

Result: 大量实验表明，对于相同的工作负载，随着Map函数和Reduce函数数量的增加，执行时间减少，程序整体效率以不同速率提升。

Conclusion: 发现最优的Map和Reduce函数数量可以帮助企业和程序员找到最优的解决方案，提高大数据处理任务的效率。

Abstract: With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.

</details>


### [21] [Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving](https://arxiv.org/abs/2601.00397)
*Amey Agrawal,Mayank Yadav,Sukrit Kumar,Anirudha Agrawal,Garv Ghai,Souradeep Bera,Elton Pinto,Sirish Gambhira,Mohammad Adain,Kasra Sohrab,Chus Antonanzas,Alexey Tumanov*

Main category: cs.DC

TL;DR: Revati是一个时间扭曲仿真器，通过直接执行真实服务系统代码实现性能建模，无需物理GPU，速度比真实GPU执行快5-17倍


<details>
  <summary>Details</summary>
Motivation: LLM部署需要测试数百种服务配置，但在GPU集群上评估每个配置需要数小时和数千美元成本。离散事件仿真器虽然更快更便宜，但需要重新实现服务系统的控制逻辑，随着框架演进负担加重

Method: Revati是一个时间扭曲仿真器，通过拦截CUDA API调用来虚拟化设备管理，允许服务框架在没有物理GPU的情况下运行。系统不执行GPU内核，而是执行时间跳跃——通过预测的内核持续时间快速推进虚拟时间。提出了一个协调协议，在分布式进程中同步这些时间跳跃，同时保持因果关系

Result: 在vLLM和SGLang上，Revati在多个模型和并行配置下实现了小于5%的预测误差，同时运行速度比真实GPU执行快5-17倍

Conclusion: Revati通过直接执行真实服务系统代码实现了快速准确的性能建模，解决了传统仿真器需要重新实现控制逻辑的问题，为LLM服务配置优化提供了高效工具

Abstract: Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.
  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.

</details>


### [22] [Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure](https://arxiv.org/abs/2601.00530)
*Ravi Teja Pagidoju*

Main category: cs.DC

TL;DR: 本研究对零售POS系统在GCP和Azure云平台上的性能进行了系统比较，使用免费层资源和开源基准测试代码，发现GCP响应时间快23%，Azure成本效率高71.9%。


<details>
  <summary>Details</summary>
Motivation: 零售业数字化转型加速了云端POS系统的采用，但缺乏针对零售工作负载的平台特定性能实证研究，特别是小型零售商和研究人员缺乏透明的评估方法。

Method: 使用免费层云资源，通过实时API端点和开源基准测试代码，建立系统可重复的POS工作负载部署比较方法，测量响应延迟、吞吐量、可扩展性等关键性能指标，并根据实际资源使用和当前公有云定价估算运营成本。

Result: GCP在基准负载下实现23.0%更快的响应时间，而Azure在稳态操作中显示71.9%更高的成本效率。所有表格和图表直接从代码输出生成，确保实验数据与报告结果一致。

Conclusion: 本研究建立了强大的零售云应用开放基准测试方法，首次提供了针对POS系统独特工作负载的全面代码驱动比较，为考虑云POS实施的商家提供了有用的架构分析框架。

Abstract: Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.

</details>


### [23] [FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding](https://arxiv.org/abs/2601.00644)
*Yuchen Li,Rui Kong,Zhonghao Lyu,Qiyang Li,Xinran Chen,Hengyi Cai,Lingyong Yan,Shuaiqiang Wang,Jiashu Zhao,Guangxu Zhu,Linghe Kong,Guihai Chen,Haoyi Xiong,Dawei Yin*

Main category: cs.DC

TL;DR: FlexSpec是一个面向边缘-云协作推理的高效通信框架，通过共享主干架构和自适应推测机制解决传统推测解码中的模型耦合与通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 在移动和边缘计算环境中部署大型语言模型面临资源限制、无线带宽稀缺和模型频繁更新的挑战。现有的边缘-云协作推理框架采用推测解码技术，但存在模型紧密耦合的问题，导致重复的模型同步带来过高的通信开销，增加了端到端延迟，限制了推测解码在边缘环境中的可扩展性。

Method: FlexSpec采用共享主干架构，使单个静态的边缘侧草稿模型能够与一系列演化的云端目标模型保持兼容，从而解耦边缘部署与云端模型更新。此外，开发了信道感知自适应推测机制，根据实时信道状态信息和设备能量预算动态调整推测草稿长度。

Result: 大量实验表明，FlexSpec在推理效率方面优于传统的推测解码方法，显著减少了通信和维护成本，无需边缘侧重新训练或重复模型下载。

Conclusion: FlexSpec通过解耦边缘与云端模型的紧密耦合，并引入自适应推测机制，为演化中的边缘-云系统提供了一个通信高效的协作推理框架，有效解决了传统推测解码方法的可扩展性和通信开销问题。

Abstract: Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Evolution of Android's Permission-based Security Model and Challenges](https://arxiv.org/abs/2601.00252)
*Rajendra Kumar Solanki,Vijay Laxmi,Manoj Singh Gaur*

Main category: cs.CR

TL;DR: 本文对2010-2022年间Android权限模型及相关研究进行了系统性文献综述，分析了API调用与权限映射、权限演进机制、权限检查方式等核心问题，识别了现有研究空白并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自2008年Android发布以来，其权限模型和应用分析一直是研究重点。尽管权限模型从"全有或全无"发展到"用户选择危险资源访问"，但15年后仍存在未解决的挑战。本研究旨在通过全面文献调研和比较分析，系统梳理该领域的研究进展。

Method: 采用系统性文献综述方法，对2010-2022年间Android权限模型及相关研究进行综合分析。研究聚焦三个核心方面：Android API调用与权限映射关系、Android权限演进历程、权限检查机制。同时识别权限相关问题及过去十年的相关研究。

Result: 系统化整理了Android权限模型的知识体系，包括API调用与权限映射关系、权限演进历史、权限检查机制。识别了权限相关的研究空白和未解决问题，引用了该领域的开创性工作。

Conclusion: 总结了Android权限模型研究的现状和挑战，为早期和资深研究人员提供了未来研究方向。尽管权限模型已有显著演进，但仍存在需要进一步解决的安全和隐私问题。

Abstract: Android Permission Model and Application (app) analysis has consistently remained the focus of the investigation of research groups and stakeholders of the Android ecosystem since it was launched in 2008. Even though the Android smartphone operating system (OS) permission model has evolved significantly from `all-or-none access' to `user-chosen dangerous resource access', specific challenges and issues remain unresolved even after 15 years after the smartphone OS launch. This study addresses the issues and documents the research work in this arena through a comprehensive literature survey and comparative analysis.
  The survey's focal point is the Android permission model and relevant research between 2010-2022. We systematize the knowledge on (i) Android API Calls to permissions mapping, (ii) Android Permissions evolution, and (iii) how permissions are checked. Furthermore, the survey identifies the permission-related issues and relevant research addressed during the last decade. We reference seminal work in these areas. We summarize the identified research gaps and present future directions for early and experienced researchers.

</details>


### [25] [Rectifying Adversarial Examples Using Their Vulnerabilities](https://arxiv.org/abs/2601.00270)
*Fumiya Morimoto,Ryuto Morita,Satoshi Ono*

Main category: cs.CR

TL;DR: 该研究提出了一种通过重新攻击对抗样本来纠正其标签的方法，旨在将对抗样本移出决策边界以预测正确标签，无需参数调整或预训练，在各种攻击方法下表现稳定。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要关注基于输入特征检测对抗样本，但未解决攻击前正确分类的问题。某些应用（如自动驾驶中的交通标志识别）不仅需要检测对抗样本，还需要识别其原始正确类别。因此需要一种能够纠正对抗样本以估计其原始正确标签的方法。

Method: 采用重新攻击对抗样本的策略，将对抗样本移出决策边界以实现准确标签预测。该方法仅以对抗样本作为输入，无需参数调整或预训练，能够处理各种攻击类型，包括白盒攻击、黑盒攻击和定向攻击。

Result: 该方法在纠正各种攻击方法生成的对抗样本方面表现一致，包括定向攻击和黑盒攻击。与传统纠正方法和输入变换方法相比，在各种攻击下表现出更好的稳定性。

Conclusion: 提出的重新攻击方法能够有效纠正对抗样本，估计其原始正确标签，解决了现有方法仅关注检测而忽视正确分类的问题。该方法简单有效，无需复杂调整，在各种攻击场景下具有稳定性能。

Abstract: Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.

</details>


### [26] [From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm](https://arxiv.org/abs/2601.00273)
*Tamer Afifi,Abdelfatah Hegazy,Ehab Abousaif*

Main category: cs.CR

TL;DR: 本文对RAFT分布式共识算法进行系统性安全分析，发现其易受消息重放和伪造攻击，导致数据不一致性，并提出基于密码学、消息认证和新鲜度检查的安全增强方案。


<details>
  <summary>Details</summary>
Motivation: RAFT算法虽然以简单、可靠和高效著称，但其安全属性未得到充分认识，实现中存在多种攻击漏洞，可能导致共识机制失效和数据不一致性。

Method: 对RAFT协议进行系统性安全分析，重点关注消息重放和伪造攻击的脆弱性；通过模拟场景验证攻击的实际可行性；识别RAFT设计中的关键弱点；提出基于密码学、认证消息验证和新鲜度检查的安全增强方案。

Result: 研究发现RAFT协议的消息传递机制易被恶意攻击者利用，通过重放旧消息破坏共识过程；识别出RAFT设计中存在的关键安全弱点；提出的安全增强方案为RAFT实现提供了安全框架。

Conclusion: RAFT协议存在可被利用的安全漏洞，需要增强安全机制；提出的密码学方案能有效提升RAFT实现的安全性，为构建更具弹性的分布式系统提供指导。

Abstract: In recent decades, the RAFT distributed consensus algorithm has become a main pillar of the distributed systems ecosystem, ensuring data consistency and fault tolerance across multiple nodes. Although the fact that RAFT is well known for its simplicity, reliability, and efficiency, its security properties are not fully recognized, leaving implementations vulnerable to different kinds of attacks and threats, which can transform the RAFT harmony of consensus into a chaos of data inconsistency. This paper presents a systematic security analysis of the RAFT protocol, with a specific focus on its susceptibility to security threats such as message replay attacks and message forgery attacks. Examined how a malicious actor can exploit the protocol's message-passing mechanism to reintroduce old messages, disrupting the consensus process and leading to data inconsistency. The practical feasibility of these attacks is examined through simulated scenarios, and the key weaknesses in RAFT's design that enable them are identified. To address these vulnerabilities, a novel approach based on cryptography, authenticated message verification, and freshness check is proposed. This proposed solution provides a framework for enhancing the security of the RAFT implementations and guiding the development of more resilient distributed systems.

</details>


### [27] [Making Theft Useless: Adulteration-Based Protection of Proprietary Knowledge Graphs in GraphRAG Systems](https://arxiv.org/abs/2601.00274)
*Weijie Wang,Peizhuo Lv,Yan Wang,Rujie Dai,Guokun Xu,Qiujian Lv,Hangcheng Liu,Weiqing Huang,Wei Dong,Jiaheng Zhang*

Main category: cs.CR

TL;DR: AURA框架通过数据掺假技术保护知识图谱，防止被盗用：向KG中注入看似合理但虚假的掺假数据，使未经授权的用户获得错误结果，而授权用户可通过密钥过滤掺假数据保持准确性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱作为组织的宝贵知识产权，面临被盗用于私人用途的风险。传统水印技术需要访问输出才能检测，而强加密方案又因延迟过高不适用于GraphRAG场景，因此需要一种既能保护KG又能保持低延迟的防御机制。

Method: 提出AURA框架，基于数据掺假技术：预先向知识图谱中注入看似合理但虚假的掺假数据。对于攻击者，这些掺假数据会污染检索上下文导致错误响应；对于授权用户，通过加密元数据标签和密钥可高效过滤所有掺假数据，确保查询准确性。

Result: 评估显示AURA将未经授权系统的准确率降至仅5.3%，同时为授权用户保持100%的保真度且开销可忽略。此外，AURA对各种净化尝试表现出鲁棒性，能保留80.2%的掺假数据。

Conclusion: AURA提供了一种有效的知识图谱保护方案，通过数据掺假技术在不显著增加延迟的情况下，既能防止KG被盗用，又能确保授权用户的查询准确性，解决了GraphRAG场景中的知识产权保护难题。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has emerged as a key technique for enhancing Large Language Models (LLMs) with proprietary Knowledge Graphs (KGs) in knowledge-intensive applications. As these KGs often represent an organization's highly valuable intellectual property (IP), they face a significant risk of theft for private use. In this scenario, attackers operate in isolated environments. This private-use threat renders passive defenses like watermarking ineffective, as they require output access for detection. Simultaneously, the low-latency demands of GraphRAG make strong encryption which incurs prohibitive overhead impractical. To address these challenges, we propose AURA, a novel framework based on Data Adulteration designed to make any stolen KG unusable to an adversary. Our framework pre-emptively injects plausible but false adulterants into the KG. For an attacker, these adulterants deteriorate the retrieved context and lead to factually incorrect responses. Conversely, for authorized users, a secret key enables the efficient filtering of all adulterants via encrypted metadata tags before they are passed to the LLM, ensuring query results remain completely accurate. Our evaluation demonstrates the effectiveness of this approach: AURA degrades the performance of unauthorized systems to an accuracy of just 5.3%, while maintaining 100% fidelity for authorized users with negligible overhead. Furthermore, AURA proves robust against various sanitization attempts, retaining 80.2% of its adulterants.

</details>


### [28] [Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution](https://arxiv.org/abs/2601.00418)
*Prajwal Panth,Sahaj Raj Malla*

Main category: cs.CR

TL;DR: CPPDD是一个轻量级、后设置自主的安全多客户端数据聚合协议，通过双层保护机制实现一致释放保密性，支持标量、向量和矩阵负载，具有线性可扩展性和亚毫秒级计算时间。


<details>
  <summary>Details</summary>
Motivation: 解决受监管和资源受限环境中安全多方计算的可扩展性、信任最小化和可验证性关键问题，为安全投票、联盟联邦学习、区块链托管和地理信息能力建设等应用提供原子协作支持。

Method: 采用基于共识的隐私保护数据分发框架，结合每客户端仿射掩码和优先级驱动的顺序共识锁定的双层保护机制，通过步骤校验和（sigma_S）和数据校验和（sigma_D）实现去中心化完整性验证。

Result: 在MNIST衍生向量上的实证评估显示线性可扩展性达N=500，每客户端计算时间亚毫秒，实现100%恶意偏差检测和精确数据恢复，相比MPC和HE基线降低3-4个数量级的FLOPs。

Conclusion: CPPDD框架在O(N*D)计算和通信复杂度下提供正确性、共识依赖完整性和公平性，在N-1腐败假设下抵抗共谋，为资源受限环境中的安全多方计算提供了高效解决方案。

Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.

</details>


### [29] [PQC standards alternatives -- reliable semantically secure key encapsulation mechanism and digital signature protocols using the rank-deficient matrix power function](https://arxiv.org/abs/2601.00332)
*Juan Pedro Hecht,Hugo Daniel Scolnik*

Main category: cs.CR

TL;DR: 该研究提出新型后量子密码协议，包括密钥封装机制和数字签名方案，旨在为TLS 1.3协议提供紧凑、快速、安全的量子安全替代方案，以应对"现在收集、以后解密"的威胁。


<details>
  <summary>Details</summary>
Motivation: 开发能够抵御经典和量子计算攻击的公钥密码原语，为TLS 1.3协议提供可靠的量子安全替代方案，保护当前数据免受"现在收集、以后解密"的威胁，实现平稳的后量子过渡。

Method: 提出新型后量子密码协议，包括密钥封装机制和数字签名方案，特别针对线性攻击提供专门保护，设计紧凑、快速且安全的方案来替代TLS 1.3中的密钥交换和数字签名功能。

Result: 开发了能够抵御量子攻击的密码协议，为互联网流量保护提供量子安全的密钥交换和数字签名方案，支持从当前密码标准到后量子密码的平稳过渡。

Conclusion: 该研究成功设计了针对量子计算威胁的后量子密码解决方案，为TLS 1.3协议提供了可行的量子安全替代方案，有助于保护当前互联网通信免受未来量子攻击的威胁。

Abstract: Post-quantum cryptography-PQC- aims to develop public-key primitives that are secure against adversaries using classical and quantum computing technologies. This study introduces novel protocols, a key encapsulation mechanism, a digital signature scheme, and special protection against linear attacks. Our purpose is to create reliable alternatives to current standards, seeking compact, fast, and secure replacements of the key interchange and digital signature in the TLS 1_3 protocol, which safeguards Internet traffic, allowing an easy post-quantum transition to protect current data from the harvest now, decrypt later threat.

</details>


### [30] [Diamond: Design and Implementation of Breach-Resilient Authenticated Encryption Framework For Internet of Things](https://arxiv.org/abs/2601.00353)
*Saif E. Nouma,Gokhan Mumcu,Attila A. Yavuz*

Main category: cs.CR

TL;DR: Diamond是首个可证明安全的向前安全聚合认证加密框架，针对资源受限的IoT设备设计，通过轻量级密钥演化机制和离线-在线优化，显著降低延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级认证加密标准缺乏向前安全性保证、紧凑标签聚合和离线-在线优化，无法满足现代高吞吐量IoT管道的需求。IoT设备需要在对抗性无线信道中传输敏感遥测数据，同时受限于严格的计算和能源预算。

Method: 提出Diamond框架，包含：1) 轻量级密钥演化机制；2) 离线-在线优化的计算管道；3) 针对异构IoT平台的性能分层实例化。框架扩展并泛化了先前的FAAE构造。

Result: Diamond显著降低摊销离线预处理（高达47%），在大批量遥测数据中端到端延迟降低达一个数量级。在64位ARM Cortex-A72、32位ARM Cortex-M4和8位AVR架构上的评估显示，Diamond在认证加密吞吐量和端到端验证延迟方面持续优于基线FAAE变体和NIST轻量级AE候选方案。

Conclusion: Diamond是首个可证明安全的向前安全聚合认证加密框架，通过形式化安全证明和两个针对合规性与高效率优化的具体实例化，为资源受限的IoT设备提供了强大的安全保证和性能优势。

Abstract: Resource-constrained Internet of Things (IoT) devices, from medical implants to small drones, must transmit sensitive telemetry under adversarial wireless channels while operating under stringent computing and energy budgets. Authenticated Encryption (AE) is essential for ensuring confidentiality, integrity, and authenticity. However, existing lightweight AE standards lack forward-security guarantees, compact tag aggregation, and offline-online (OO) optimizations required for modern high-throughput IoT pipelines.
  We introduce Diamond, the first provable secure Forward-secure and Aggregate Authenticated Encryption (FAAE) framework that extends and generalizes prior FAAE constructions through a lightweight key evolution mechanism, an OO-optimized computation pipeline, and a set of performance-tiered instantiations tailored to heterogeneous IoT platforms. Diamond substantially reduces amortized offline preprocessing (up to 47%) and achieves up to an order-ofmagnitude reduction in end-to-end latency for large telemetry batches. Our comprehensive evaluation across 64-bit ARM Cortex-A72, 32-bit ARM Cortex-M4, and 8-bit AVR architectures confirms that Diamond consistently outperforms baseline FAAE variants and NIST lightweight AE candidates across authenticated encryption throughput and end-to-end verification latency while maintaining compact tag aggregation and strong breach resilience. We formally prove the security of Diamond and provide two concrete instantiations optimized for compliance and high efficiency. Our open-source release enables reproducibility and seamless integration into IoT platforms.

</details>


### [31] [LLM-Powered Analysis of IoT User Reviews: Tracking and Ranking Security and Privacy Concerns](https://arxiv.org/abs/2601.00372)
*Taufiq Islam Protick,Sai Teja Peddinti,Nina Taft,Anupam Das*

Main category: cs.CR

TL;DR: 使用GPT-3.5-Turbo构建自动化管道分析亚马逊IoT产品评论中的安全隐私问题，发现平均5%评论涉及S&P问题，智能摄像头最高达10%，检测效果远超传统方法


<details>
  <summary>Details</summary>
Motivation: 了解物联网用户的安全隐私关切对开发者和用户都有益处，通过分析亚马逊IoT评论这一最大市场之一来获取用户观点

Method: 开发自动化管道，通过微调GPT-3.5-Turbo构建两个模型：分类器-合理化器-分类器模型和主题映射器，利用动态少样本提示和大上下文窗口

Result: 管道达到超过97%的精确率和召回率，显著优于基于关键词和传统机器学习方法；分析91K条亚马逊评论发现平均5%包含S&P问题，智能摄像头最高达10%；检测到的相关评论比先前研究多15倍（健身追踪器）、29%（智能音箱）和70%（摄像头）

Conclusion: 纵向分析显示监控和数据控制等关切持续多年，表明行业进展有限；用户普遍要求更精确的数据收集和共享控制；发现多用户多设备交互中的新问题，为开发者提供改进用户满意度和信任的可操作见解

Abstract: Being able to understand the security and privacy (S&P) concerns of IoT users brings benefits to both developers and users. To learn about users' views, we examine Amazon IoT reviews - one of the biggest IoT markets. This work presents a state-of-the-art methodology to identify and categorize reviews in which users express S&P concerns. We developed an automated pipeline by fine-tuning GPT-3.5-Turbo to build two models: the Classifier-Rationalizer-Categorizer and the Thematic Mapper. By leveraging dynamic few-shot prompting and the model's large context size, our pipeline achieved over 97% precision and recall, significantly outperforming keyword-based and classical ML methods. We applied our pipeline to 91K Amazon reviews about fitness trackers, smart speakers and cameras, over multiple years. We found that on average 5% contained S&P concerns, while security camera exhibited the highest prevalence at 10%. Our method detected significantly more S&P-relevant reviews than prior works: 15x more for fitness trackers, 29% more for smart speakers, and 70% more for cameras. Our longitudinal analysis reveals that concerns like surveillance and data control have persisted for years, suggesting limited industry progress. We demonstrate that across all device types, users consistently demand more precise control over what data is collected and shared. We uncover challenges in multi-user and multi-device interactions, identifying two previously unreported themes concerning inadequate controls for account separation and data access. These findings, ranging from broad persistent trends to specific instances of customer loss, offer actionable insights for developers to improve user satisfaction and trust.

</details>


### [32] [Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback](https://arxiv.org/abs/2601.00509)
*Vidyut Sriram,Sawan Pandita,Achintya Lakshmanan,Aneesh Shamraj,Suman Saha*

Main category: cs.CR

TL;DR: 论文提出了一种检索增强的多工具修复工作流，通过编译器诊断、安全扫描和符号执行等工具，结合语义检索技术，显著降低了LLM生成代码中的安全漏洞和缺陷。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码常存在安全漏洞、逻辑不一致和编译错误，需要有效的自动化修复机制来提升代码质量。

Method: 采用检索增强的多工具修复工作流：单个代码生成LLM迭代优化输出，使用编译器诊断、CodeQL安全扫描和KLEE符号执行，结合轻量级嵌入模型进行语义检索，获取安全修复示例指导生成。

Result: 在3,242个程序的数据集上评估显示：DeepSeek-Coder-1.3B的安全漏洞减少96%；CodeLlama-7B的关键安全缺陷率从58.55%降至22.19%，证明工具辅助自修复对"顽固"模型也有效。

Conclusion: 工具辅助的自修复工作流能显著提升LLM生成代码的鲁棒性，特别是对安全漏洞的修复效果明显，为代码生成质量保障提供了有效解决方案。

Abstract: Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.

</details>


### [33] [Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?](https://arxiv.org/abs/2601.00559)
*Jason Quantrill,Noura Khajehnouri,Zihan Guo,Manar H. Alalfi*

Main category: cs.CR

TL;DR: 本文首次全面评估大语言模型在智能家居IoT平台规则交互威胁检测中的表现，发现LLMs在语义理解方面有潜力，但在需要跨规则结构推理的威胁检测上准确性显著下降，特别是面对规则变体时表现不稳定，而符号推理基线方法则保持稳定检测能力。


<details>
  <summary>Details</summary>
Motivation: 智能家居IoT平台（如openHAB）依赖触发-动作-条件规则实现设备自动化，但这些规则之间的交互可能产生交互威胁，如隐式依赖、冲突触发或重叠条件导致意外或不安全行为。传统上识别这些威胁需要语义理解和结构推理，依赖于符号化、约束驱动的静态分析。本研究旨在评估大语言模型在多类别交互威胁检测中的表现。

Method: 本研究首次全面评估大语言模型在智能家居规则交互威胁检测中的表现。使用多类别交互威胁分类法，在原始openHAB数据集和结构挑战性的Mutation数据集上进行测试。Mutation数据集专门设计用于测试规则变换下的鲁棒性。评估了Llama 3.1 8B、Llama 70B、GPT-4o、Gemini-2.5-Pro和DeepSeek-R1等模型在零样本、单样本和双样本设置下的表现，并与oHIT手动验证的基准真值进行比较。

Result: 研究发现：1）大语言模型展现出有前景的语义理解能力，特别是在动作和条件相关威胁方面；2）在需要跨规则结构推理的威胁检测上，准确性显著下降，尤其是在突变规则形式下；3）模型性能在不同威胁类别和提示设置下差异很大，没有模型能提供一致的可靠性；4）相比之下，符号推理基线方法在两个数据集上都保持稳定的检测能力，不受规则重写或结构扰动的影响。

Conclusion: 大语言模型单独使用尚不足以可靠地检测IoT环境中的安全关键交互威胁。研究强调了工具设计的意义，并指出将符号分析与基于LLM的语义解释相结合的混合架构具有潜力，可以在保持结构严谨性的同时减少误报。

Abstract: Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.

</details>


### [34] [Low Rank Comes with Low Security: Gradient Assembly Poisoning Attacks against Distributed LoRA-based LLM Systems](https://arxiv.org/abs/2601.00566)
*Yueyan Dong,Minghui Xu,Qin Hu,Yinhao Xiao,Qi Luo,Yechao Zhang,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 论文提出了一种名为梯度组装投毒（GAP）的新型攻击方法，该攻击针对联邦学习中基于LoRA的微调系统，通过分别构造看似无害的A和B矩阵，使其乘积产生恶意模型更新，从而绕过现有检测机制。


<details>
  <summary>Details</summary>
Motivation: LoRA在联邦学习中被广泛用于降低大语言模型微调成本，但存在安全漏洞：客户端分别提交A和B矩阵，而只有它们的乘积AB决定模型更新，但这个复合结果从未被直接验证，这为攻击者创造了可乘之机。

Method: 提出梯度组装投毒（GAP）攻击方法，攻击者无需访问训练数据或跨客户端协调，通过精心构造看似无害的A和B矩阵，使其乘积产生恶意更新。该方法利用了LoRA联邦系统中的四个系统性漏洞，并能在标准异常检测器下保持隐蔽。

Result: 在LLaMA、ChatGLM和GPT-2等模型上验证了GAP攻击的有效性。攻击能持续诱导模型产生质量下降或有偏见的输出，同时保持表面流畅性：BLEU分数下降高达14.5%，事实和语法错误增加超过800%，长文本回复长度保持92.6%。

Conclusion: GAP攻击揭示了分布式LoRA微调中存在一类新型的隐蔽、持续性威胁。研究结果表明，当前LoRA联邦学习系统存在严重安全漏洞，需要开发新的验证机制来防御此类攻击。

Abstract: Low-Rank Adaptation (LoRA) has become a popular solution for fine-tuning large language models (LLMs) in federated settings, dramatically reducing update costs by introducing trainable low-rank matrices. However, when integrated with frameworks like FedIT, LoRA introduces a critical vulnerability: clients submit $A$ and $B$ matrices separately, while only their product $AB$ determines the model update, yet this composite is never directly verified. We propose Gradient Assembly Poisoning (GAP), a novel attack that exploits this blind spot by crafting individually benign $A$ and $B$ matrices whose product yields malicious updates. GAP operates without access to training data or inter-client coordination and remains undetected by standard anomaly detectors. We identify four systemic vulnerabilities in LoRA-based federated systems and validate GAP across LLaMA, ChatGLM, and GPT-2. GAP consistently induces degraded or biased outputs while preserving surface fluency, reducing BLEU by up to 14.5\%, increasing factual and grammatical errors by over 800\%, and maintaining 92.6\% long-form response length. These results reveal a new class of stealthy, persistent threats in distributed LoRA fine-tuning.

</details>


### [35] [Towards Understanding and Characterizing Vulnerabilities in Intelligent Connected Vehicles through Real-World Exploits](https://arxiv.org/abs/2601.00627)
*Yuelin Wang,Yuqiao Ning,Yanbang Sun,Xiaofei Xie,Zhihua Xie,Yang Chen,Zhen Guo,Shihao Xue,Junjie Wang,Sen Chen*

Main category: cs.CR

TL;DR: 该研究对智能网联汽车漏洞进行了首次大规模实证分析，收集了649个可利用漏洞，评估了现有分类法的覆盖范围，发现了新的漏洞位置和类型，并提供了数据驱动的安全见解。


<details>
  <summary>Details</summary>
Motivation: 智能网联汽车安全至关重要，但现有研究大多只关注特定子组件，缺乏系统性理解，且依赖主观分析，理论发现与实际攻击之间存在显著差距。

Method: 通过分析现有ICV安全文献总结漏洞分类法，收集649个可利用漏洞（592个来自8次ICV漏洞发现竞赛，57个来自研究人员日常提交），评估分类法覆盖范围，识别差距，并对漏洞进行分类分析。

Result: 发现现有分类法存在覆盖不足，识别出1个新的漏洞位置和13个新的漏洞类型；将漏洞分类为6种威胁类型和4个风险等级；分析了竞赛参与者的技能和涉及的ICV类型。

Conclusion: 该研究提供了对ICV漏洞的全面数据驱动分析，为研究人员、行业从业者和政策制定者提供了可操作的见解，并公开了漏洞数据集以支持未来研究。

Abstract: Intelligent Connected Vehicles (ICVs) are a core component of modern transportation systems, and their security is crucial as it directly relates to user safety. Despite prior research, most existing studies focus only on specific sub-components of ICVs due to their inherent complexity. As a result, there is a lack of systematic understanding of ICV vulnerabilities. Moreover, much of the current literature relies on human subjective analysis, such as surveys and interviews, which tends to be high-level and unvalidated, leaving a significant gap between theoretical findings and real-world attacks. To address this issue, we conducted the first large-scale empirical study on ICV vulnerabilities. We began by analyzing existing ICV security literature and summarizing the prevailing taxonomies in terms of vulnerability locations and types. To evaluate their real-world relevance, we collected a total of 649 exploitable vulnerabilities, including 592 from eight ICV vulnerability discovery competitions, Anonymous Cup, between January 2023 and April 2024, covering 48 different vehicles. The remaining 57 vulnerabilities were submitted daily by researchers. Based on this dataset, we assessed the coverage of existing taxonomies and identified several gaps, discovering one new vulnerability location and 13 new vulnerability types. We further categorized these vulnerabilities into 6 threat types (e.g., privacy data breach) and 4 risk levels (ranging from low to critical) and analyzed participants' skills and the types of ICVs involved in the competitions. This study provides a comprehensive and data-driven analysis of ICV vulnerabilities, offering actionable insights for researchers, industry practitioners, and policymakers. To support future research, we have made our vulnerability dataset publicly available.

</details>
