{"id": "2602.06252", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.06252", "abs": "https://arxiv.org/abs/2602.06252", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs", "comment": null, "summary": "The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\\times$ lower latency, up to 3.8$\\times$ higher memory savings, and up to 3$\\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\\times$ lower total latency, up to 2.3$\\times$ higher total throughput, and up to 2.7$\\times$ higher total memory savings.", "AI": {"tldr": "D-Legion\u662f\u4e00\u79cd\u65b0\u578b\u53ef\u6269\u5c55\u591a\u6838\u67b6\u6784\uff0c\u4e13\u4e3a\u52a0\u901f\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\u800c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u6838\u5fc3\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ba1\u7b97\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u91cf\u5316\u6a21\u578b\u867d\u80fd\u51cf\u5c11\u9700\u6c42\u4f46\u9700\u8981\u4e13\u95e8\u67b6\u6784\u6765\u52a0\u901f\u3002\u73b0\u6709\u67b6\u6784\u5728\u5904\u7406\u91cf\u5316LLM\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u786c\u4ef6\u8bbe\u8ba1\u6765\u4f18\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51faD-Legion\u67b6\u6784\uff0c\u5305\u542b\u591a\u4e2aLegion\u5355\u5143\uff0c\u6bcf\u4e2aLegion\u5305\u542b\u4e00\u7ec4\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u3002\u652f\u6301\u91cf\u5316\u7a00\u758f\u548c\u7a20\u5bc6\u77e9\u9635\u4e58\u6cd5\uff0c\u5229\u7528\u5757\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u5e76\u884c\u7d2f\u52a0\u5668\u51cf\u5c11\u90e8\u5206\u548c\u7684\u5185\u5b58\u8bbf\u95ee\uff0c\u901a\u8fc7\u591a\u64ad\u4f18\u5316\u6570\u636e\u91cd\u7528\u3002", "result": "\u5728BitNet\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cD-Legion\u76f8\u6bd4\u6700\u5148\u8fdb\u5de5\u4f5c\u5b9e\u73b0\u4e868.2\u500d\u5ef6\u8fdf\u964d\u4f4e\u30013.8\u500d\u5185\u5b58\u8282\u7701\u548c3\u500d\u90e8\u5206\u548c\u5185\u5b58\u8282\u7701\u30028\u4e2aLegion\u7248\u672c\u8fbe\u5230135.68 TOPS\u5cf0\u503c\u541e\u5410\u91cf\uff0c32\u4e2aLegion\u7248\u672c\u76f8\u6bd4Google TPUv4i\u5b9e\u73b02.5\u500d\u5ef6\u8fdf\u964d\u4f4e\u30012.3\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c2.7\u500d\u5185\u5b58\u8282\u7701\u3002", "conclusion": "D-Legion\u67b6\u6784\u901a\u8fc7\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u548c\u4f18\u5316\u7684\u8c03\u5ea6\u6280\u672f\uff0c\u4e3a\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u786c\u4ef6\u52a0\u901f\u65b9\u6848\uff0c\u5728\u6027\u80fd\u3001\u5185\u5b58\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.06433", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.06433", "abs": "https://arxiv.org/abs/2602.06433", "authors": ["Anubhav Bhatla", "Navneet Navneet", "Moinuddin Qureshi", "Biswabandan Panda"], "title": "The Avatar Cache: Enabling On-Demand Security with Morphable Cache Architecture", "comment": null, "summary": "The sharing of the last-level cache (LLC) among multiple cores makes it vulnerable to cross-core conflict- and occupancy-based attacks. Despite extensive prior work, modern processors still employ non-secure set-associative LLCs. Existing secure LLC designs broadly fall into two categories: (i) randomized and (ii) partitioned. The state-of-the-art randomized design, Mirage, mitigates conflict-based attacks but incurs significant area overhead (20% additional storage) and design complexity. Partitioned LLCs mitigate both conflict- and occupancy-based attacks, but often suffer from large performance overheads (on average over 5% and up to 49%), require OS support in set-based schemes, or face scalability issues in way-based schemes. These factors pose major obstacles to the industrial adoption of secure LLCs. This paper asks whether strong LLC security can be achieved with minimal changes to a conventional set-associative LLC, enabling security only when needed while preserving low performance, power, and area overheads. We propose Avatar, a secure and morphable LLC that supports three modes: non-secure (Avatar-N), randomized secure (Avatar-R), and partitioned secure (Avatar-P), and can switch dynamically between them. Avatar closely resembles a conventional set-associative LLC, facilitating industrial adoption. Avatar-R introduces extra invalid entries and leverages high associativity to provide a strong security guarantee with little capacity loss, achieving only one set-associative eviction per $10^{30}$ years, while incurring 1.5% storage overhead, a 2.7% increase in static power, and a 0.2% slowdown over a 16~MB baseline. Avatar-P mitigates both conflict- and occupancy-based attacks with only a 3% performance overhead, substantially outperforming prior way-based partitioned LLCs. When security is unnecessary, Avatar switches to Avatar-N to maximize performance and energy efficiency.", "AI": {"tldr": "\u63d0\u51faAvatar\u7f13\u5b58\u8bbe\u8ba1\uff0c\u652f\u6301\u4e09\u79cd\u6a21\u5f0f\u52a8\u6001\u5207\u6362\uff1a\u975e\u5b89\u5168\u6a21\u5f0f\u3001\u968f\u673a\u5316\u5b89\u5168\u6a21\u5f0f\u548c\u5206\u533a\u5b89\u5168\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u4f20\u7edf\u7f13\u5b58\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u5f3a\u5b89\u5168\u6027", "motivation": "\u73b0\u4ee3\u5904\u7406\u5668\u4ecd\u4f7f\u7528\u4e0d\u5b89\u5168\u7684\u7ec4\u76f8\u8054LLC\uff0c\u73b0\u6709\u5b89\u5168\u8bbe\u8ba1\u8981\u4e48\u5b58\u50a8\u5f00\u9500\u5927\uff08\u968f\u673a\u5316\u65b9\u6848\uff09\uff0c\u8981\u4e48\u6027\u80fd\u635f\u5931\u4e25\u91cd\uff08\u5206\u533a\u65b9\u6848\uff09\uff0c\u963b\u788d\u5de5\u4e1a\u91c7\u7528\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u4f9b\u5f3a\u5b89\u5168\u6027\u53c8\u5bf9\u4f20\u7edf\u7f13\u5b58\u6539\u52a8\u6700\u5c0f\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51faAvatar\u53ef\u53d8\u5f62LLC\uff0c\u652f\u6301\u4e09\u79cd\u6a21\u5f0f\u52a8\u6001\u5207\u6362\uff1aAvatar-N\uff08\u975e\u5b89\u5168\uff09\u3001Avatar-R\uff08\u968f\u673a\u5316\u5b89\u5168\uff09\u548cAvatar-P\uff08\u5206\u533a\u5b89\u5168\uff09\u3002Avatar-R\u901a\u8fc7\u589e\u52a0\u65e0\u6548\u6761\u76ee\u548c\u9ad8\u76f8\u8054\u5ea6\u63d0\u4f9b\u5b89\u5168\u6027\uff1bAvatar-P\u91c7\u7528\u5206\u533a\u8bbe\u8ba1\u9632\u5fa1\u4e24\u79cd\u653b\u51fb\u3002", "result": "Avatar-R\u4ec5\u97001.5%\u5b58\u50a8\u5f00\u9500\uff0c\u9759\u6001\u529f\u8017\u589e\u52a02.7%\uff0c\u6027\u80fd\u4ec5\u964d\u4f4e0.2%\uff0c\u5b89\u5168\u6027\u6781\u9ad8\uff08\u6bcf10^30\u5e74\u624d\u53d1\u751f\u4e00\u6b21\u7ec4\u76f8\u8054\u9a71\u9010\uff09\u3002Avatar-P\u6027\u80fd\u5f00\u9500\u4ec53%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5206\u533a\u65b9\u6848\u3002\u6574\u4f53\u8bbe\u8ba1\u63a5\u8fd1\u4f20\u7edf\u7f13\u5b58\uff0c\u4fbf\u4e8e\u5de5\u4e1a\u91c7\u7528\u3002", "conclusion": "Avatar\u5728\u4fdd\u6301\u4f20\u7edf\u7f13\u5b58\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3aLLC\u5b89\u5168\u6027\uff0c\u652f\u6301\u6309\u9700\u5207\u6362\u5b89\u5168\u6a21\u5f0f\uff0c\u5e73\u8861\u4e86\u5b89\u5168\u3001\u6027\u80fd\u548c\u5de5\u4e1a\u53ef\u884c\u6027\uff0c\u4e3a\u5b89\u5168\u7f13\u5b58\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06057", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06057", "abs": "https://arxiv.org/abs/2602.06057", "authors": ["Satyam Kumar", "Saurabh Jha"], "title": "Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing", "comment": null, "summary": "Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.", "AI": {"tldr": "QEIL\u6846\u67b6\u901a\u8fc7\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\u548c\u5f02\u6784\u786c\u4ef6\u7f16\u6392\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u8986\u76d6\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5ef6\u8fdf\u6311\u6218\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8fc7\u5ea6\u4f9d\u8d56\u4e91\u6216\u6570\u636e\u4e2d\u5fc3\u57fa\u7840\u8bbe\u65bd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u672c\u5730\u9ad8\u6548\u6267\u884cLLM\u63a8\u7406\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faQEIL\u6846\u67b6\uff1a1) \u63a8\u5bfc\u4e94\u4e2a\u67b6\u6784\u65e0\u5173\u7684\u5b9a\u7406\uff0c\u63cf\u8ff0\u63a8\u7406\u6548\u7387\u5982\u4f55\u968f\u6a21\u578b\u5927\u5c0f\u3001\u6837\u672c\u9884\u7b97\u548c\u8bbe\u5907\u7ea6\u675f\u7f29\u653e\uff1b2) \u96c6\u6210\u4e09\u4e2a\u4f18\u5316\u7ef4\u5ea6\uff1a\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u3001\u786c\u4ef6\u611f\u77e5\u8def\u7531\u3001\u6027\u80fd-\u80fd\u8017\u6743\u8861\u91cf\u5316\uff1b3) \u901a\u8fc7\u6e10\u8fdb\u6837\u672c\u590d\u7528\u7edf\u4e00\u7f16\u6392\u5668\u7ec4\u5408\u8fd9\u4e9b\u7ec4\u4ef6\u3002", "result": "\u5728125M\u52302.6B\u53c2\u6570\u7684\u4e94\u4e2a\u6a21\u578b\u7cfb\u5217\u4e0a\u8bc4\u4f30\u663e\u793a\uff1ak\u8986\u76d6\u7387\u63d0\u53477-10.5\u4e2a\u767e\u5206\u70b9\uff0c\u80fd\u8017\u964d\u4f4e35.6-78.2%\uff0c\u5e73\u5747\u529f\u8017\u964d\u4f4e68%\u6ee1\u8db3\u8fb9\u7f18\u70ed\u9884\u7b97\uff0c\u5ef6\u8fdf\u6539\u558415.8%\uff0c\u96f6\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\u5177\u6709\u666e\u9002\u6027\u548c\u67b6\u6784\u65e0\u5173\u6027\uff0c\u5f02\u6784\u8fb9\u7f18\u7f16\u6392\u662f\u80fd\u6e90\u53d7\u9650\u667a\u80fd\u7cfb\u7edf\u7684\u6700\u4f18\u7b56\u7565\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2602.06107", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06107", "abs": "https://arxiv.org/abs/2602.06107", "authors": ["Zhuoming Chen", "Hongyi Liu", "Yang Zhou", "Haizhong Zheng", "Beidi Chen"], "title": "Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning", "comment": "ICLR 2026", "summary": "Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \\sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.", "AI": {"tldr": "Jackpot\u6846\u67b6\u901a\u8fc7\u6700\u4f18\u9884\u7b97\u62d2\u7edd\u91c7\u6837\u51cf\u5c11rollout\u6a21\u578b\u4e0e\u7b56\u7565\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5b9e\u73b0LLM\u5f3a\u5316\u5b66\u4e60\u4e2drollout\u751f\u6210\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u89e3\u8026\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6210\u672c\u9ad8\u6602\uff0c\u4e3b\u8981\u56e0\u4e3arollout\u8fc7\u7a0b\u5f00\u9500\u5927\u3002\u5c06rollout\u751f\u6210\u4e0e\u7b56\u7565\u4f18\u5316\u89e3\u8026\uff08\u5982\u4f7f\u7528\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u8fdb\u884crollout\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4f46\u8fd9\u4f1a\u5f15\u5165\u4e25\u91cd\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faJackpot\u6846\u67b6\uff0c\u91c7\u7528\u6700\u4f18\u9884\u7b97\u62d2\u7edd\u91c7\u6837\u76f4\u63a5\u51cf\u5c11rollout\u6a21\u578b\u4e0e\u6f14\u5316\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u3002\u6846\u67b6\u5305\u542b\uff1a1\uff09\u539f\u5219\u6027\u7684OBRS\u8fc7\u7a0b\uff1b2\uff09\u8054\u5408\u66f4\u65b0\u7b56\u7565\u548crollout\u6a21\u578b\u7684\u7edf\u4e00\u8bad\u7ec3\u76ee\u6807\uff1b3\uff09\u901a\u8fc7top-k\u6982\u7387\u4f30\u8ba1\u548c\u6279\u6b21\u7ea7\u504f\u5dee\u6821\u6b63\u5b9e\u73b0\u7684\u9ad8\u6548\u7cfb\u7edf\u5b9e\u73b0\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eOBRS\u5728\u53ef\u63a7\u63a5\u53d7\u9884\u7b97\u4e0b\u6301\u7eed\u5c06rollout\u5206\u5e03\u5411\u76ee\u6807\u5206\u5e03\u79fb\u52a8\u3002\u5b9e\u8bc1\u663e\u793a\uff0c\u76f8\u6bd4\u91cd\u8981\u6027\u91c7\u6837\u57fa\u7ebf\uff0cJackpot\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5728Qwen3-8B-Base\u6a21\u578b\u4e0a\u8bad\u7ec3300\u4e2a\u66f4\u65b0\u6b65\u9aa4\uff08\u6279\u6b21\u5927\u5c0f64\uff09\u65f6\uff0c\u6027\u80fd\u4e0eon-policy RL\u76f8\u5f53\u3002", "conclusion": "\u57fa\u4e8eOBRS\u7684\u5bf9\u9f50\u65b9\u6cd5\u4f7fLLM\u5f3a\u5316\u5b66\u4e60\u4e2drollout\u751f\u6210\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u89e3\u8026\u66f4\u63a5\u8fd1\u5b9e\u7528\u548c\u6709\u6548\uff0c\u4e3a\u89e3\u51b3RL\u8bad\u7ec3\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.06172", "categories": ["cs.CR", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06172", "abs": "https://arxiv.org/abs/2602.06172", "authors": ["Jonathan Feldman", "Tal Feldman", "Annie I Anton"], "title": "Know Your Scientist: KYC as Biosecurity Infrastructure", "comment": null, "summary": "Biological AI tools for protein design and structure prediction are advancing rapidly, creating dual-use risks that existing safeguards cannot adequately address. Current model-level restrictions, including keyword filtering, output screening, and content-based access denials, are fundamentally ill-suited to biology, where reliable function prediction remains beyond reach and novel threats evade detection by design. We propose a three-tier Know Your Customer (KYC) framework, inspired by anti-money laundering (AML) practices in the financial sector, that shifts governance from content inspection to user verification and monitoring. Tier I leverages research institutions as trust anchors to vouch for affiliated researchers and assume responsibility for vetting. Tier II applies output screening through sequence homology searches and functional annotation. Tier III monitors behavioral patterns to detect anomalies inconsistent with declared research purposes. This layered approach preserves access for legitimate researchers while raising the cost of misuse through institutional accountability and traceability. The framework can be implemented immediately using existing institutional infrastructure, requiring no new legislation or regulatory mandates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u91d1\u878d\u53cd\u6d17\u94b1KYC\u539f\u5219\u7684\u4e09\u5c42\u751f\u7269AI\u5b89\u5168\u6846\u67b6\uff0c\u4ece\u5185\u5bb9\u5ba1\u67e5\u8f6c\u5411\u7528\u6237\u9a8c\u8bc1\u4e0e\u76d1\u63a7\uff0c\u4ee5\u5e94\u5bf9\u86cb\u767d\u8d28\u8bbe\u8ba1AI\u7684\u53cc\u91cd\u7528\u9014\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5173\u952e\u8bcd\u8fc7\u6ee4\u3001\u8f93\u51fa\u7b5b\u67e5\u548c\u5185\u5bb9\u8bbf\u95ee\u9650\u5236\u7684\u751f\u7269AI\u5b89\u5168\u63aa\u65bd\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u65e0\u6cd5\u53ef\u9760\u9884\u6d4b\u86cb\u767d\u8d28\u529f\u80fd\uff0c\u4e14\u65b0\u578b\u5a01\u80c1\u4f1a\u523b\u610f\u89c4\u907f\u68c0\u6d4b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6cbb\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e09\u5c42KYC\u6846\u67b6\uff1a\u7b2c\u4e00\u5c42\u5229\u7528\u7814\u7a76\u673a\u6784\u4f5c\u4e3a\u4fe1\u4efb\u951a\u70b9\uff0c\u9a8c\u8bc1\u7814\u7a76\u4eba\u5458\u8eab\u4efd\u5e76\u627f\u62c5\u8d23\u4efb\uff1b\u7b2c\u4e8c\u5c42\u901a\u8fc7\u5e8f\u5217\u540c\u6e90\u6027\u641c\u7d22\u548c\u529f\u80fd\u6ce8\u91ca\u8fdb\u884c\u8f93\u51fa\u7b5b\u67e5\uff1b\u7b2c\u4e09\u5c42\u76d1\u63a7\u884c\u4e3a\u6a21\u5f0f\u4ee5\u68c0\u6d4b\u4e0e\u58f0\u660e\u7814\u7a76\u76ee\u7684\u4e0d\u7b26\u7684\u5f02\u5e38\u6d3b\u52a8\u3002", "result": "\u8be5\u5206\u5c42\u65b9\u6cd5\u5728\u4fdd\u6301\u5408\u6cd5\u7814\u7a76\u4eba\u5458\u8bbf\u95ee\u6743\u9650\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u673a\u6784\u95ee\u8d23\u548c\u53ef\u8ffd\u6eaf\u6027\u63d0\u9ad8\u4e86\u6ee5\u7528\u6210\u672c\uff0c\u53ef\u7acb\u5373\u5229\u7528\u73b0\u6709\u673a\u6784\u57fa\u7840\u8bbe\u65bd\u5b9e\u65bd\uff0c\u65e0\u9700\u65b0\u7acb\u6cd5\u6216\u76d1\u7ba1\u6388\u6743\u3002", "conclusion": "\u57fa\u4e8eKYC\u539f\u5219\u7684\u751f\u7269AI\u6cbb\u7406\u6846\u67b6\u6bd4\u5f53\u524d\u5185\u5bb9\u5ba1\u67e5\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u80fd\u591f\u5e94\u5bf9\u86cb\u767d\u8d28\u8bbe\u8ba1AI\u7684\u53cc\u91cd\u7528\u9014\u98ce\u9669\uff0c\u5b9e\u73b0\u5b89\u5168\u4e0e\u521b\u65b0\u7684\u5e73\u8861\u3002"}}
{"id": "2602.06063", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06063", "abs": "https://arxiv.org/abs/2602.06063", "authors": ["Shouyu Du", "Miaoxiang Yu", "Zhiheng Ni", "Jillian Cai", "Qing Yang", "Tao Wei", "Zhenyu Xu"], "title": "Mapping Gemma3 onto an Edge Dataflow Architecture", "comment": "Original Version, data shall be updated", "summary": "We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\\times$ faster prefill and $4.8\\times$ faster decoding versus the iGPU, and $33.5\\times$ and $2.2\\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\\times$ and $222.9\\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728AMD Ryzen AI NPU\u4e0a\u7aef\u5230\u7aef\u90e8\u7f72Gemma3\u7cfb\u5217\u5927\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u80fd\u6548\u6539\u8fdb\u3002", "motivation": "\u73b0\u4ee3NPU\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u6a21\u578b\u65f6\u9762\u4e34\u6027\u80fd\u74f6\u9888\u548c\u80fd\u6548\u95ee\u9898\uff0c\u9700\u8981\u9488\u5bf9tiled\u6570\u636e\u6d41\u67b6\u6784\u8fdb\u884c\u4e13\u95e8\u7684\u4f18\u5316\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6280\u672f\uff1a\u9884\u586b\u5145\u9636\u6bb5\u5305\u62ec\u9ad8\u6548\u53cd\u91cf\u5316\u5f15\u64ce\u3001\u4f18\u5316\u7684tiled\u77e9\u9635\u4e58\u6cd5\u5185\u6838\u3001\u4ee5\u53caFlowQKV\uff08\u5206\u5757\u6d41\u6c34\u7ebf\u6ce8\u610f\u529b\u673a\u5236\uff09\uff1b\u89e3\u7801\u9636\u6bb5\u5305\u62ecFusedDQP\uff08\u878d\u5408\u53cd\u91cf\u5316\u548c\u6295\u5f71\u7684\u5355\u4e00\u5185\u6838\uff09\u548cFlowKV\uff08\u91cd\u6784\u6ce8\u610f\u529b\u4ee5\u7ef4\u6301\u9ad8\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\uff09\uff0c\u914d\u5408Q4NX 4\u4f4d\u91cf\u5316\u683c\u5f0f\u3002", "result": "\u76f8\u6bd4iGPU\uff0c\u9884\u586b\u5145\u901f\u5ea6\u63d0\u53475.2\u500d\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u53474.8\u500d\uff1b\u76f8\u6bd4CPU\uff0c\u5206\u522b\u63d0\u534733.5\u500d\u548c2.2\u500d\u3002\u80fd\u6548\u76f8\u6bd4iGPU\u63d0\u534767.2\u500d\uff0c\u76f8\u6bd4CPU\u63d0\u5347222.9\u500d\u3002", "conclusion": "\u73b0\u4ee3NPU\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u7528\u3001\u4f4e\u529f\u8017\u7684LLM\u548cVLM\u63a8\u7406\uff0c\u4e3a\u5c06\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u6620\u5c04\u5230tiled\u6570\u636e\u6d41\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u84dd\u56fe\u3002"}}
{"id": "2602.06176", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06176", "abs": "https://arxiv.org/abs/2602.06176", "authors": ["Peiyang Song", "Pengrui Han", "Noah Goodman"], "title": "Large Language Model Reasoning Failures", "comment": "Repository: https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures. Published at TMLR 2026 with Survey Certification", "summary": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9LLM\u63a8\u7406\u5931\u8d25\u8fdb\u884c\u5168\u9762\u8c03\u67e5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u5206\u4e3a\u5177\u8eab\u4e0e\u975e\u5177\u8eab\u7c7b\u578b\uff0c\u5e76\u5c06\u63a8\u7406\u5931\u8d25\u5206\u4e3a\u57fa\u7840\u67b6\u6784\u5931\u8d25\u3001\u5e94\u7528\u7279\u5b9a\u9650\u5236\u548c\u9c81\u68d2\u6027\u95ee\u9898\u4e09\u7c7b\uff0c\u4e3a\u7406\u89e3LLM\u7cfb\u7edf\u6027\u5f31\u70b9\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\u3002", "motivation": "\u5c3d\u7ba1LLM\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u770b\u4f3c\u7b80\u5355\u7684\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u7684\u63a8\u7406\u5931\u8d25\u3002\u4e3a\u4e86\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u548c\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u9677\uff0c\u9700\u8981\u5bf9\u8fd9\u4e9b\u5931\u8d25\u8fdb\u884c\u5168\u9762\u7684\u8c03\u67e5\u548c\u5206\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u7c7b\u6846\u67b6\uff1a\u5c06\u63a8\u7406\u5206\u4e3a\u5177\u8eab\u63a8\u7406\u548c\u975e\u5177\u8eab\u63a8\u7406\uff0c\u540e\u8005\u8fdb\u4e00\u6b65\u7ec6\u5206\u4e3a\u975e\u6b63\u5f0f\uff08\u76f4\u89c9\uff09\u63a8\u7406\u548c\u6b63\u5f0f\uff08\u903b\u8f91\uff09\u63a8\u7406\u3002\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u5931\u8d25\u5206\u4e3a\u4e09\u7c7b\uff1a\u57fa\u7840\u67b6\u6784\u5931\u8d25\u3001\u5e94\u7528\u7279\u5b9a\u9650\u5236\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002\u5bf9\u6bcf\u79cd\u5931\u8d25\u63d0\u4f9b\u660e\u786e\u5b9a\u4e49\u3001\u5206\u6790\u73b0\u6709\u7814\u7a76\u3001\u63a2\u7d22\u6839\u672c\u539f\u56e0\u5e76\u63d0\u51fa\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5168\u9762\u7684LLM\u63a8\u7406\u5931\u8d25\u8c03\u67e5\uff0c\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\u6765\u7406\u89e3LLM\u7684\u7cfb\u7edf\u6027\u5f31\u70b9\u3002\u540c\u65f6\u53d1\u5e03\u4e86GitHub\u8d44\u6e90\u5e93\uff08Awesome-LLM-Reasoning-Failures\uff09\uff0c\u6536\u96c6\u4e86\u76f8\u5173\u7814\u7a76\u5de5\u4f5c\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u5165\u95e8\u70b9\u3002", "conclusion": "\u8be5\u8c03\u67e5\u7edf\u4e00\u4e86\u5206\u6563\u7684\u7814\u7a76\u5de5\u4f5c\uff0c\u4e3a\u7406\u89e3LLM\u63a8\u7406\u7684\u7cfb\u7edf\u6027\u5f31\u70b9\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\u671d\u7740\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u66f4\u53ef\u9760\u548c\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u80fd\u529b\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.06064", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06064", "abs": "https://arxiv.org/abs/2602.06064", "authors": ["Yi-Xiang Hu", "Yuke Wang", "Feng Wu", "Zirui Huang", "Shuli Zeng", "Xiang-Yang Li"], "title": "iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems", "comment": "13 pages, 7 figures,", "summary": "Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\\times$ against strong commercial baselines.", "AI": {"tldr": "iScheduler\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fed\u4ee3\u8c03\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8d44\u6e90\u6295\u8d44\u95ee\u9898(RIP)\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u5e76\u987a\u5e8f\u9009\u62e9\u8fdb\u7a0b\u6765\u6784\u5efa\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u8457\u52a0\u901f\u4f18\u5316\u5e76\u652f\u6301\u52a8\u6001\u91cd\u914d\u7f6e\u3002", "motivation": "\u5728\u5171\u4eab\u53ef\u518d\u751f\u8d44\u6e90\u4e0b\u8c03\u5ea6\u5177\u6709\u4f18\u5148\u7ea6\u675f\u7684\u4efb\u52a1\u662f\u73b0\u4ee3\u8ba1\u7b97\u5e73\u53f0\u7684\u6838\u5fc3\u95ee\u9898\u3002\u8d44\u6e90\u6295\u8d44\u95ee\u9898(RIP)\u65e8\u5728\u6700\u5c0f\u5316\u8d44\u6e90\u914d\u7f6e\u6210\u672c\uff0c\u4f46\u4f20\u7edf\u6df7\u5408\u6574\u6570\u89c4\u5212\u548c\u7ea6\u675f\u89c4\u5212\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u901f\u5ea6\u8fc7\u6162\uff0c\u4e14\u52a8\u6001\u66f4\u65b0\u9700\u8981\u5728\u4e25\u683c\u5ef6\u8fdf\u9884\u7b97\u4e0b\u91cd\u65b0\u8c03\u5ea6\u3002", "method": "iScheduler\u5c06RIP\u6c42\u89e3\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5206\u89e3\u5b50\u95ee\u9898\u5e76\u987a\u5e8f\u9009\u62e9\u8fdb\u7a0b\u6765\u6784\u5efa\u8c03\u5ea6\u65b9\u6848\u3002\u8be5\u6846\u67b6\u652f\u6301\u901a\u8fc7\u91cd\u7528\u672a\u66f4\u6539\u7684\u8fdb\u7a0b\u8c03\u5ea6\u548c\u4ec5\u91cd\u65b0\u8c03\u5ea6\u53d7\u5f71\u54cd\u8fdb\u7a0b\u6765\u5b9e\u73b0\u5feb\u901f\u91cd\u914d\u7f6e\u3002", "result": "iScheduler\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u8d44\u6e90\u6210\u672c\u7684\u540c\u65f6\uff0c\u5c06\u53ef\u884c\u89e3\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe43\u500d\uff08\u76f8\u6bd4\u5f3a\u5927\u7684\u5546\u4e1a\u57fa\u7ebf\uff09\u3002\u4f5c\u8005\u8fd8\u53d1\u5e03\u4e86L-RIPLIB\u5de5\u4e1a\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b1000\u4e2a\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u67092500-10000\u4e2a\u4efb\u52a1\u3002", "conclusion": "iScheduler\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u8fed\u4ee3\u8c03\u5ea6\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8d44\u6e90\u6295\u8d44\u95ee\u9898\u7684\u4f18\u5316\u6548\u7387\u95ee\u9898\uff0c\u652f\u6301\u5feb\u901f\u91cd\u914d\u7f6e\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u4efb\u52a1\u8c03\u5ea6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06227", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.06227", "abs": "https://arxiv.org/abs/2602.06227", "authors": ["Pierriccardo Olivieri", "Fausto Lasca", "Alessandro Gianola", "Matteo Papini"], "title": "Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)", "comment": "This is the extended version of a paper accepted at AAAI 2026", "summary": "In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLTLfMT\u903b\u8f91\u6846\u67b6\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u5956\u52b1\u89c4\u8303\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u7684MDP\uff0c\u901a\u8fc7\u5956\u52b1\u673a\u5668\u548cHER\u89e3\u51b3\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898", "motivation": "\u4f20\u7edf\u903b\u8f91\u89c4\u8303\u5728\u5904\u7406\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u65f6\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u624b\u52a8\u7f16\u7801\u8c13\u8bcd\uff0c\u7f3a\u4e4f\u7edf\u4e00\u53ef\u91cd\u7528\u6846\u67b6", "method": "\u4f7f\u7528LTLfMT\uff08\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u6a21\u7406\u8bba\uff09\u6269\u5c55\u7ecf\u5178\u65f6\u5e8f\u903b\u8f91\uff0c\u63d0\u51fa\u53ef\u5904\u7406\u7247\u6bb5\uff0c\u7ed3\u5408\u5956\u52b1\u673a\u5668\u548cHindsight Experience Replay\uff08HER\uff09\u65b9\u6cd5", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u4f7f\u7528\u975e\u7ebf\u6027\u7b97\u672f\u7406\u8bba\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5b9a\u5236\u7684HER\u5b9e\u73b0\u5bf9\u4e8e\u89e3\u51b3\u590d\u6742\u76ee\u6807\u4efb\u52a1\u81f3\u5173\u91cd\u8981", "conclusion": "LTLfMT\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4MDP\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u81ea\u7136\u89c4\u8303\u80fd\u529b\uff0c\u7ed3\u5408HER\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898"}}
{"id": "2602.06336", "categories": ["cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06336", "abs": "https://arxiv.org/abs/2602.06336", "authors": ["Ahmad Alemari", "Pritam Sen", "Cristian Borcea"], "title": "AdFL: In-Browser Federated Learning for Online Advertisement", "comment": null, "summary": "Since most countries are coming up with online privacy regulations, such as GDPR in the EU, online publishers need to find a balance between revenue from targeted advertisement and user privacy. One way to be able to still show targeted ads, based on user personal and behavioral information, is to employ Federated Learning (FL), which performs distributed learning across users without sharing user raw data with other stakeholders in the publishing ecosystem. This paper presents AdFL, an FL framework that works in the browsers to learn user ad preferences. These preferences are aggregated in a global FL model, which is then used in the browsers to show more relevant ads to users. AdFL can work with any model that uses features available in the browser such as ad viewability, ad click-through, user dwell time on pages, and page content. The AdFL server runs at the publisher and coordinates the learning process for the users who browse pages on the publisher's website. The AdFL prototype does not require the client to install any software, as it is built utilizing standard APIs available on most modern browsers. We built a proof-of-concept model for ad viewability prediction that runs on top of AdFL. We tested AdFL and the model with two non-overlapping datasets from a website with 40K visitors per day. The experiments demonstrate AdFL's feasibility to capture the training information in the browser in a few milliseconds, show that the ad viewability prediction achieves up to 92.59% AUC, and indicate that utilizing differential privacy (DP) to safeguard local model parameters yields adequate performance, with only modest declines in comparison to the non-DP variant.", "AI": {"tldr": "AdFL\u662f\u4e00\u4e2a\u5728\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u7528\u6237\u5e7f\u544a\u504f\u597d\uff0c\u5e73\u8861\u5e7f\u544a\u6536\u5165\u4e0e\u7528\u6237\u9690\u79c1\uff0c\u652f\u6301\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\uff0c\u5e7f\u544a\u53ef\u89c1\u6027\u9884\u6d4bAUC\u8fbe92.59%", "motivation": "\u968f\u7740GDPR\u7b49\u5728\u7ebf\u9690\u79c1\u6cd5\u89c4\u7684\u51fa\u53f0\uff0c\u5728\u7ebf\u51fa\u7248\u5546\u9700\u8981\u5728\u5b9a\u5411\u5e7f\u544a\u6536\u5165\u548c\u7528\u6237\u9690\u79c1\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5728\u4e0d\u5171\u4eab\u7528\u6237\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5206\u5e03\u5f0f\u5b66\u4e60\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "method": "\u63d0\u51faAdFL\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\uff0c\u5229\u7528\u5e7f\u544a\u53ef\u89c1\u6027\u3001\u70b9\u51fb\u7387\u3001\u9875\u9762\u505c\u7559\u65f6\u95f4\u3001\u9875\u9762\u5185\u5bb9\u7b49\u7279\u5f81\u5b66\u4e60\u7528\u6237\u5e7f\u544a\u504f\u597d\u3002\u672c\u5730\u6a21\u578b\u53c2\u6570\u5728\u670d\u52a1\u5668\u805a\u5408\u5f62\u6210\u5168\u5c40\u6a21\u578b\uff0c\u652f\u6301\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u672c\u5730\u53c2\u6570\u3002", "result": "AdFL\u539f\u578b\u5728\u6d4f\u89c8\u5668\u4e2d\u51e0\u6beb\u79d2\u5185\u6355\u83b7\u8bad\u7ec3\u4fe1\u606f\uff0c\u5e7f\u544a\u53ef\u89c1\u6027\u9884\u6d4b\u6a21\u578bAUC\u8fbe\u523092.59%\u3002\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u672c\u5730\u6a21\u578b\u53c2\u6570\u65f6\u6027\u80fd\u9002\u5ea6\u4e0b\u964d\uff0c\u4f46\u4ecd\u4fdd\u6301\u8db3\u591f\u6027\u80fd\u3002", "conclusion": "AdFL\u8bc1\u660e\u4e86\u5728\u6d4f\u89c8\u5668\u4e2d\u5b9e\u65bd\u8054\u90a6\u5b66\u4e60\u8fdb\u884c\u5e7f\u544a\u504f\u597d\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u80fd\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u6548\u7684\u5b9a\u5411\u5e7f\u544a\uff0c\u4e3a\u5728\u7ebf\u51fa\u7248\u5546\u63d0\u4f9b\u4e86GDPR\u5408\u89c4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06069", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06069", "abs": "https://arxiv.org/abs/2602.06069", "authors": ["Dinesh Gopalan", "Ratul Ali"], "title": "HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference", "comment": "7 pages, 3 figures, 2 tables", "summary": "The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.", "AI": {"tldr": "HQP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u526a\u679d\u548c\u91cf\u5316\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fdAI\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e1.5%\u7684\u540c\u65f6\uff0c\u8fbe\u52303.12\u500d\u52a0\u901f\u548c55%\u6a21\u578b\u538b\u7f29\u3002", "motivation": "\u8fb9\u7f18-\u4e91\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u5bf9\u9ad8\u4fdd\u771f\u5b9e\u65f6\u63a8\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u9650\u5236\uff0c\u9700\u8981\u6fc0\u8fdb\u7684\u6a21\u578b\u4f18\u5316\u6280\u672f\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u91cf\u5316\u4e0e\u526a\u679d\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8eFisher\u4fe1\u606f\u77e9\u9635\u8fd1\u4f3c\u8ba1\u7b97\u7684\u52a8\u6001\u6743\u91cd\u654f\u611f\u5ea6\u6307\u6807\u6307\u5bfc\u7ed3\u6784\u5316\u526a\u679d\uff0c\u5728\u6ee1\u8db3\u6700\u5927\u5141\u8bb8\u7cbe\u5ea6\u4e0b\u964d\u7ea6\u675f\u540e\uff0c\u518d\u8fdb\u884c8\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\u3002", "result": "\u5728NVIDIA Jetson\u8fb9\u7f18\u5e73\u53f0\u4e0a\uff0c\u4f7f\u7528MobileNetV3\u548cResNet-18\u67b6\u6784\uff0cHQP\u6846\u67b6\u5b9e\u73b0\u4e863.12\u500d\u63a8\u7406\u52a0\u901f\u548c55%\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\uff0c\u540c\u65f6\u7cbe\u5ea6\u635f\u5931\u4e25\u683c\u63a7\u5236\u57281.5%\u4ee5\u5185\u3002", "conclusion": "HQP\u6846\u67b6\u76f8\u6bd4\u4f20\u7edf\u5355\u76ee\u6807\u538b\u7f29\u6280\u672f\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u662f\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u57fa\u7840\u8bbe\u65bd\u4e2d\u90e8\u7f72\u8d85\u4f4e\u5ef6\u8fdfAI\u7684\u786c\u4ef6\u65e0\u5173\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06319", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06319", "abs": "https://arxiv.org/abs/2602.06319", "authors": ["Qifan Zhang", "Jianhao Ruan", "Aochuan Chen", "Kang Zeng", "Nuo Chen", "Jing Tang", "Jia Li"], "title": "Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems", "comment": null, "summary": "Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.", "AI": {"tldr": "GrAlgoBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u56fe\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc79\u4e2a\u4efb\u52a1\u63ed\u793aLRMs\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u4e24\u5927\u5f31\u70b9\uff1a\u51c6\u786e\u7387\u968f\u8282\u70b9\u6570\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\u5bfc\u81f4\u7684\u65e0\u6548\u81ea\u6211\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u3001\u4ee3\u7801\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7f3a\u4e4f\u957f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u3001\u6311\u6218\u6027\u4e0d\u8db3\u3001\u7b54\u6848\u96be\u4ee5\u7a0b\u5e8f\u5316\u9a8c\u8bc1\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faGrAlgoBench\u57fa\u51c6\uff0c\u4f7f\u7528\u56fe\u7b97\u6cd5\u95ee\u9898\u8bc4\u4f30LRMs\u3002\u56fe\u7b97\u6cd5\u95ee\u9898\u7279\u522b\u9002\u5408\u6d4b\u8bd5\u63a8\u7406\u80fd\u529b\uff1a\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u53ef\u7cbe\u7ec6\u63a7\u5236\u96be\u5ea6\u3001\u652f\u6301\u6807\u51c6\u5316\u7a0b\u5e8f\u5316\u8bc4\u4f30\u3002\u5305\u542b9\u4e2a\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u5f53\u524dLRMs\u7684\u4e24\u5927\u4e3b\u8981\u5f31\u70b9\uff1a1) \u51c6\u786e\u7387\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u5f53\u56fe\u8d85\u8fc7120\u4e2a\u8282\u70b9\u65f6\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff0c\u4e3b\u8981\u7531\u9891\u7e41\u6267\u884c\u9519\u8bef\u3001\u5f31\u8bb0\u5fc6\u548c\u5197\u4f59\u63a8\u7406\u5bfc\u81f4\uff1b2) \u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u4e3b\u8981\u7531\u5e7f\u6cdb\u4f46\u4f4e\u6548\u7684\u81ea\u6211\u9a8c\u8bc1\u5f15\u8d77\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u75d5\u8ff9\u4f46\u6ca1\u6709\u63d0\u9ad8\u6b63\u786e\u6027\u3002", "conclusion": "GrAlgoBench\u901a\u8fc7\u66b4\u9732\u8fd9\u4e9b\u9650\u5236\uff0c\u786e\u7acb\u4e86\u56fe\u7b97\u6cd5\u95ee\u9898\u4f5c\u4e3a\u63a8\u8fdbLRMs\u63a8\u7406\u7814\u7a76\u7684\u4e25\u8c28\u3001\u591a\u7ef4\u4e14\u5b9e\u9645\u76f8\u5173\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.06071", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06071", "abs": "https://arxiv.org/abs/2602.06071", "authors": ["Rajat Vadiraj Dwaraknath", "Sungyoon Kim", "Mert Pilanci"], "title": "FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs", "comment": null, "summary": "Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBlockPerm-SJLT\u7a00\u758f\u8349\u56fe\u65b9\u6cd5\u53ca\u5176\u9ad8\u6548GPU\u5b9e\u73b0FlashSketch\uff0c\u901a\u8fc7\u5757\u6392\u5217\u7ed3\u6784\u5e73\u8861GPU\u6548\u7387\u4e0e\u8349\u56fe\u8d28\u91cf\uff0c\u5728RandNLA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b01.7\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u8349\u56fe\u867d\u7136\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5176\u968f\u673a\u7a00\u758f\u6027\u5bfc\u81f4GPU\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4e0d\u89c4\u5219\uff0c\u4e25\u91cd\u5f71\u54cd\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\u3002\u4e3a\u89e3\u51b3GPU\u6548\u7387\u4e0e\u8349\u56fe\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u9700\u8981\u8bbe\u8ba1\u65b0\u7684\u7a00\u758f\u8349\u56fe\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u8349\u56fe-\u5185\u6838\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff1a1) \u8bbe\u8ba1BlockPerm-SJLT\u7a00\u758f\u8349\u56fe\u65cf\uff0c\u5f15\u5165\u53ef\u8c03\u53c2\u6570\u5e73\u8861GPU\u6548\u7387\u4e0e\u8349\u56fe\u9c81\u68d2\u6027\uff1b2) \u5f00\u53d1FlashSketch\u4f18\u5316CUDA\u5185\u6838\uff0c\u4e13\u95e8\u5b9e\u73b0\u8fd9\u4e9b\u8349\u56fe\u7684\u9ad8\u6548GPU\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eBlockPerm-SJLT\u6ee1\u8db3\u65e0\u610f\u8bc6\u5b50\u7a7a\u95f4\u5d4c\u5165(OSE)\u4fdd\u8bc1\uff1b\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aFlashSketch\u5728RandNLA\u57fa\u51c6\u6d4b\u8bd5\u548cGraSS\u6570\u636e\u5f52\u56e0\u7ba1\u9053\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u73b0\u6709GPU\u8349\u56fe\u5b9e\u73b0\u7ea61.7\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u8349\u56fe-\u5185\u6838\u534f\u540c\u8bbe\u8ba1\uff0cBlockPerm-SJLT\u548cFlashSketch\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u8349\u56fe\u5728GPU\u4e0a\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u8349\u56fe\u8d28\u91cf\u4e0e\u901f\u5ea6\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002"}}
{"id": "2602.06409", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06409", "abs": "https://arxiv.org/abs/2602.06409", "authors": ["Guowei Guan", "Yurong Hao", "Jiaming Zhang", "Tiantong Wu", "Fuyao Zhang", "Tianxiang Chen", "Longtao Huang", "Cyril Leung", "Wei Yang Bryan Lim"], "title": "VENOMREC: Cross-Modal Interactive Poisoning for Targeted Promotion in Multimodal LLM Recommender Systems", "comment": null, "summary": "Multimodal large language models (MLLMs) are pushing recommender systems (RecSys) toward content-grounded retrieval and ranking via cross-modal fusion. We find that while cross-modal consensus often mitigates conventional poisoning that manipulates interaction logs or perturbs a single modality, it also introduces a new attack surface where synchronised multimodal poisoning can reliably steer fused representations along stable semantic directions during fine-tuning. To characterise this threat, we formalise cross-modal interactive poisoning and propose VENOMREC, which performs Exposure Alignment to identify high-exposure regions in the joint embedding space and Cross-modal Interactive Perturbation to craft attention-guided coupled token-patch edits. Experiments on three real-world multimodal datasets demonstrate that VENOMREC consistently outperforms strong baselines, achieving 0.73 mean ER@20 and improving over the strongest baseline by +0.52 absolute ER points on average, while maintaining comparable recommendation utility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5VENOMREC\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u534f\u540c\u6295\u6bd2\u6765\u64cd\u7eb5\u878d\u5408\u8868\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u6a21\u6001\u653b\u51fb\u66f4\u6709\u6548", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u52a8\u63a8\u8350\u7cfb\u7edf\u5411\u57fa\u4e8e\u5185\u5bb9\u7684\u68c0\u7d22\u548c\u6392\u5e8f\u53d1\u5c55\uff0c\u8de8\u6a21\u6001\u5171\u8bc6\u867d\u7136\u80fd\u7f13\u89e3\u4f20\u7edf\u6295\u6bd2\u653b\u51fb\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\u2014\u2014\u540c\u6b65\u591a\u6a21\u6001\u6295\u6bd2\u53ef\u4ee5\u7a33\u5b9a\u5730\u64cd\u7eb5\u878d\u5408\u8868\u793a", "method": "\u63d0\u51faVENOMREC\u653b\u51fb\u6846\u67b6\uff1a1) \u66dd\u5149\u5bf9\u9f50\u8bc6\u522b\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u9ad8\u66dd\u5149\u533a\u57df\uff1b2) \u8de8\u6a21\u6001\u4ea4\u4e92\u6270\u52a8\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u8026\u5408token-patch\u7f16\u8f91\u6765\u5236\u4f5c\u534f\u540c\u653b\u51fb", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVENOMREC\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5e73\u5747ER@20\u8fbe\u52300.73\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8+0.52\u7edd\u5bf9ER\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u63a8\u8350\u6548\u7528", "conclusion": "\u8de8\u6a21\u6001\u4ea4\u4e92\u6295\u6bd2\u662f\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u65b0\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u6765\u5e94\u5bf9\u8fd9\u79cd\u534f\u540c\u653b\u51fb"}}
{"id": "2602.06072", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06072", "abs": "https://arxiv.org/abs/2602.06072", "authors": ["Rui Ning", "Wei Zhang", "Fan Lai"], "title": "PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference", "comment": null, "summary": "Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.", "AI": {"tldr": "PackInfer\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f02\u6784\u6279\u5904\u7406\u8bf7\u6c42\u6253\u5305\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6267\u884c\u7ec4\uff0c\u4f18\u5316\u8ba1\u7b97\u548cI/O\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u751f\u4ea7\u73af\u5883\u4e2dLLM\u63a8\u7406\u9700\u8981\u6279\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u7684\u8bf7\u6c42\u4ee5\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u73b0\u6709\u6ce8\u610f\u529b\u4f18\u5316\u6280\u672f\uff08\u5982FlashAttention\uff09\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u8bf7\u6c42\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548cI/O\u4e0d\u5747\u8861\u3001\u62d6\u5c3e\u6548\u5e94\u4e25\u91cd\u3001GPU\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3002", "method": "PackInfer\u901a\u8fc7\u5c06\u6279\u5904\u7406\u8bf7\u6c42\u7ec4\u7ec7\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6267\u884c\u7ec4\uff0c\u5c06\u591a\u4e2a\u8bf7\u6c42\u6253\u5305\u5230\u7edf\u4e00\u7684\u5185\u6838\u542f\u52a8\u4e2d\uff1b\u6784\u5efa\u76f4\u63a5\u4f5c\u7528\u4e8e\u6253\u5305\u67e5\u8be2\u952e\u533a\u57df\u7684\u6ce8\u610f\u529b\u5185\u6838\uff0c\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\u5e76\u5e73\u8861\u7ebf\u7a0b\u5757\u6267\u884c\uff1b\u91c7\u7528I/O\u611f\u77e5\u5206\u7ec4\uff0c\u5c06\u5171\u4eab\u524d\u7f00\u8bf7\u6c42\u5171\u7f6e\uff0c\u5e76\u5c06KV\u7f13\u5b58\u91cd\u7ec4\u4e3a\u7ec4\u8fde\u7eed\u5e03\u5c40\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0cPackInfer\u76f8\u6bd4\u6700\u5148\u8fdb\u7684FlashAttention\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e13.0-20.1%\uff0c\u541e\u5410\u91cf\u63d0\u534720%\u3002", "conclusion": "PackInfer\u901a\u8fc7\u8ba1\u7b97\u548cI/O\u611f\u77e5\u7684\u5f02\u6784\u6279\u5904\u7406\u63a8\u7406\u6267\u884c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u4ea7\u73af\u5883\u4e2dLLM\u63a8\u7406\u7684\u6ce8\u610f\u529b\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86GPU\u5229\u7528\u7387\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2602.06375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06375", "abs": "https://arxiv.org/abs/2602.06375", "authors": ["Yu Zhao", "Fan Jiang", "Tianle Liu", "Bo Zeng", "Yu Liu", "Longyue Wang", "Weihua Luo"], "title": "Difficulty-Estimated Policy Optimization", "comment": null, "summary": "Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDEPO\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u96be\u5ea6\u4f30\u8ba1\u7b5b\u9009\u8bad\u7ec3\u6570\u636e\uff0c\u51cf\u5c11\u4f4e\u6548\u6837\u672c\u7684rollout\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5c06rollout\u6210\u672c\u964d\u4f4e2\u500d\u3002", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u5728\u9047\u5230\u8fc7\u4e8e\u7b80\u5355\u6216\u590d\u6742\u7684\u95ee\u9898\u65f6\uff0c\u68af\u5ea6\u4fe1\u53f7\u4f1a\u8870\u51cf\u751a\u81f3\u6d88\u5931\uff0c\u5bfc\u81f4\u6536\u655b\u4e0d\u7a33\u5b9a\u3002\u867d\u7136DAPO\u7b49\u53d8\u4f53\u5c1d\u8bd5\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u4f46\u65e0\u6cd5\u7f13\u89e3\u5728\u4f4e\u6548\u7528\u6837\u672c\u4e0a\u8fdb\u884c\u5927\u91cfrollout\u5e26\u6765\u7684\u5de8\u5927\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faDifficulty-Estimated Policy Optimization (DEPO)\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u5728\u7ebf\u96be\u5ea6\u4f30\u8ba1\u5668\uff0c\u5728rollout\u9636\u6bb5\u524d\u52a8\u6001\u8bc4\u4f30\u548c\u7b5b\u9009\u8bad\u7ec3\u6570\u636e\uff0c\u4f18\u5148\u5c06\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7ed9\u5177\u6709\u9ad8\u5b66\u4e60\u6f5c\u529b\u7684\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDEPO\u5728\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06rollout\u6210\u672c\u964d\u4f4e\u4e86\u6700\u591a2\u500d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u9ad8\u6027\u80fd\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u95e8\u69db\u3002", "conclusion": "DEPO\u4e3a\u63a8\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u8d44\u6e90\u5206\u914d\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u89c4\u6a21\u5316\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u53ef\u6301\u7eed\u7684\u8def\u5f84\u3002"}}
{"id": "2602.06074", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06074", "abs": "https://arxiv.org/abs/2602.06074", "authors": ["Mohammad Umar", "Bharat Tripathi"], "title": "Experimental Analysis of Server-Side Caching for Web Performance", "comment": "4 pages, experimental study, server-side caching", "summary": "Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u65e0\u7f13\u5b58\u4e0e\u4f7f\u7528\u5185\u5b58\u7f13\u5b58\uff08\u56fa\u5b9aTTL\uff09\u7684\u4e24\u79cd\u670d\u52a1\u5668\u7aefWeb\u5e94\u7528\u914d\u7f6e\uff0c\u53d1\u73b0\u7f13\u5b58\u80fd\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u65f6\u95f4\uff0c\u4e3a\u5c0f\u578bWeb\u5e94\u7528\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u6027\u80fd\u4f18\u5316\u65b9\u6848\u3002", "motivation": "\u867d\u7136\u7f13\u5b58\u6280\u672f\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\u7528\u4e8eWeb\u6027\u80fd\u4f18\u5316\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5c0f\u578bWeb\u5e94\u7528\u4e2d\u7b80\u5355\u5185\u5b58\u7f13\u5b58\u6548\u679c\u7684\u5b9e\u9a8c\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u63a2\u7d22\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u5728\u5c0f\u578bWeb\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e24\u79cd\u670d\u52a1\u5668\u7aefWeb\u5e94\u7528\u914d\u7f6e\uff1a\u4e00\u79cd\u65e0\u7f13\u5b58\uff0c\u53e6\u4e00\u79cd\u4f7f\u7528\u5185\u5b58\u7f13\u5b58\u5e76\u8bbe\u7f6e\u56fa\u5b9a\u751f\u5b58\u65f6\u95f4\uff08TTL\uff09\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7Web\u670d\u52a1\u5668\u6846\u67b6\uff0c\u5728\u76f8\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u901a\u8fc7\u91cd\u590dHTTP\u8bf7\u6c42\u6d4b\u91cf\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u7ed3\u679c\u663e\u793a\u7f13\u5b58\u8bf7\u6c42\u7684\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u5728\u63d0\u9ad8Web\u5e94\u7528\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u80fd\u6709\u6548\u63d0\u9ad8Web\u5e94\u7528\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6559\u80b2\u73af\u5883\u548c\u9700\u8981\u7b80\u5355\u6027\u3001\u53ef\u91cd\u73b0\u6027\u7684\u5c0f\u578bWeb\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.06394", "categories": ["cs.AI", "cs.CE", "q-bio.GN", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2602.06394", "abs": "https://arxiv.org/abs/2602.06394", "authors": ["Arvid E. Gollwitzer", "Paridhi Latawa", "David de Gruijl", "Deepak A. Subramanian", "Adri\u00e1n Noriega de la Colina"], "title": "Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization", "comment": null, "summary": "Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.", "AI": {"tldr": "QA-Token\u662f\u4e00\u79cd\u8d28\u91cf\u611f\u77e5\u7684\u5206\u8bcd\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u53c2\u6570\u5b66\u4e60\uff0c\u5c06\u6570\u636e\u53ef\u9760\u6027\u76f4\u63a5\u7eb3\u5165\u8bcd\u6c47\u8868\u6784\u5efa\uff0c\u5728\u57fa\u56e0\u7ec4\u5b66\u548c\u91d1\u878d\u9886\u57df\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u5f53\u524d\u7684\u5206\u8bcd\u65b9\u6cd5\u5904\u7406\u5e8f\u5217\u6570\u636e\u65f6\u672a\u8003\u8651\u4fe1\u53f7\u8d28\u91cf\uff0c\u9650\u5236\u4e86\u5176\u5728\u566a\u58f0\u73b0\u5b9e\u4e16\u754c\u8bed\u6599\u5e93\u4e2d\u7684\u6709\u6548\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u7eb3\u5165\u6570\u636e\u53ef\u9760\u6027\u7684\u5206\u8bcd\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e86QA-Token\uff08\u8d28\u91cf\u611f\u77e5\u5206\u8bcd\uff09\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a1\uff09\u53cc\u5c42\u4f18\u5316\u516c\u5f0f\uff0c\u8054\u5408\u4f18\u5316\u8bcd\u6c47\u8868\u6784\u5efa\u548c\u4e0b\u6e38\u6027\u80fd\uff1b2\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d28\u91cf\u611f\u77e5\u5956\u52b1\u5b66\u4e60\u5408\u5e76\u7b56\u7565\uff0c\u5177\u6709\u6536\u655b\u4fdd\u8bc1\uff1b3\uff09\u901a\u8fc7Gumbel-Softmax\u677e\u5f1b\u7684\u81ea\u9002\u5e94\u53c2\u6570\u5b66\u4e60\u673a\u5236\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u4e00\u81f4\u6539\u8fdb\uff1a\u57fa\u56e0\u7ec4\u5b66\uff08\u53d8\u5f02\u68c0\u6d4bF1\u5206\u6570\u6bd4BPE\u63d0\u9ad86.7\u4e2a\u767e\u5206\u70b9\uff09\u3001\u91d1\u878d\uff08\u590f\u666e\u6bd4\u7387\u63d0\u9ad830%\uff09\u3002\u5728\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u4e0a\uff0c\u5bf91.7\u4e07\u4ebf\u78b1\u57fa\u5bf9\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u8fdb\u884c\u5206\u8bcd\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u75c5\u539f\u4f53\u68c0\u6d4b\uff0894.53 MCC\uff09\uff0c\u540c\u65f6\u51cf\u5c1115%\u7684token\u6570\u91cf", "conclusion": "QA-Token\u89e3\u9501\u4e86\u566a\u58f0\u73b0\u5b9e\u4e16\u754c\u8bed\u6599\u5e93\uff08\u5305\u62ecpetabases\u7ea7\u522b\u7684\u57fa\u56e0\u7ec4\u5e8f\u5217\u548cterabytes\u7ea7\u522b\u7684\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\uff09\uff0c\u7528\u4e8e\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\uff0c\u4e14\u63a8\u7406\u5f00\u9500\u4e3a\u96f6"}}
{"id": "2602.06413", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06413", "abs": "https://arxiv.org/abs/2602.06413", "authors": ["Hsien-Jyh Liao"], "title": "Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution", "comment": "16 Pages, 7 figures, Keyworda: Autoregressive Reasoning, Long-Horizon Stability, Chain-of-Thought Reasoning, Information-Theoretic Analysis, Structured Reasoning, Inference Dynamics", "summary": "Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.\n  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).\n  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u81ea\u56de\u5f52\u751f\u6210\u5b58\u5728\u5185\u5728\u7a33\u5b9a\u6027\u6781\u9650\uff0c\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u8fd9\u6e90\u4e8e\u8fc7\u7a0b\u7ea7\u4e0d\u7a33\u5b9a\u6027\u800c\u975e\u5355\u7eaf\u7684\u4efb\u52a1\u590d\u6742\u5ea6\uff0c\u9700\u8981\u79bb\u6563\u5206\u6bb5\u548c\u56fe\u72b6\u6267\u884c\u7ed3\u6784\u6765\u7ef4\u6301\u7a33\u5b9a\u63a8\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u4f20\u7edf\u89e3\u91ca\u4e3b\u8981\u5f52\u56e0\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u89e3\u91ca\u4e0d\u5b8c\u6574\uff0c\u5373\u4f7f\u5728\u65e0\u5206\u652f\u7684\u7ebf\u6027\u4efb\u52a1\u4e2d\uff0c\u81ea\u56de\u5f52\u6267\u884c\u4e5f\u5b58\u5728\u5185\u5728\u7a33\u5b9a\u6027\u6781\u9650\u3002", "method": "\u63d0\u51fa\u81ea\u56de\u5f52\u751f\u6210\u7684\u8fc7\u7a0b\u7ea7\u4e0d\u7a33\u5b9a\u6027\u662f\u957f\u65f6\u7a0b\u63a8\u7406\u7684\u6839\u672c\u7ea6\u675f\uff0c\u63a8\u5bfc\u5b9a\u7406A\u663e\u793a\u5355\u8def\u5f84\u81ea\u56de\u5f52\u63a8\u7406\u4e2d\u7684\u51b3\u7b56\u4f18\u52bf\u968f\u6267\u884c\u957f\u5ea6\u5448\u6307\u6570\u8870\u51cf\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u73af\u5883\u548c\u771f\u5b9eTextWorld\u4efb\u52a1\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u663e\u793a\u53ef\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u60ac\u5d16\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u5931\u8d25\u7684\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\uff0c\u5e76\u8868\u660e\u77ed\u65f6\u7a0b\u8bc4\u4f30\u534f\u8bae\u53ef\u80fd\u63a9\u76d6\u7ed3\u6784\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u957f\u65f6\u7a0b\u63a8\u7406\u9700\u8981\u79bb\u6563\u5206\u6bb5\uff0c\u81ea\u7136\u8bf1\u5bfc\u51fa\u6709\u5411\u65e0\u73af\u56fe\u7b49\u56fe\u72b6\u6267\u884c\u7ed3\u6784\uff0c\u672a\u6765\u63a8\u7406\u7cfb\u7edf\u53ef\u80fd\u9700\u8981\u4ece\u5355\u7eaf\u6269\u5c55\u8f6c\u5411\u7ed3\u6784\u5316\u6cbb\u7406\u3002"}}
{"id": "2602.06495", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06495", "abs": "https://arxiv.org/abs/2602.06495", "authors": ["Minkyoo Song", "Jaehan Kim", "Myungchul Kang", "Hanna Kim", "Seungwon Shin", "Sooel Son"], "title": "Subgraph Reconstruction Attacks on Graph RAG Deployments with Practical Defenses", "comment": null, "summary": "Graph-based retrieval-augmented generation (Graph RAG) is increasingly deployed to support LLM applications by augmenting user queries with structured knowledge retrieved from a knowledge graph. While Graph RAG improves relational reasoning, it introduces a largely understudied threat: adversaries can reconstruct subgraphs from a target RAG system's knowledge graph, enabling privacy inference and replication of curated knowledge assets. We show that existing attacks are largely ineffective against Graph RAG even with simple prompt-based safeguards, because these attacks expose explicit exfiltration intent and are therefore easily suppressed by lightweight safe prompts. We identify three technical challenges for practical Graph RAG extraction under realistic safeguards and introduce GRASP, a closed-box, multi-turn subgraph reconstruction attack. GRASP (i) reframes extraction as a context-processing task, (ii) enforces format-compliant, instance-grounded outputs via per-record identifiers to reduce hallucinations and preserve relational details, and (iii) diversifies goal-driven attack queries using a momentum-aware scheduler to operate within strict query budgets. Across two real-world knowledge graphs, four safety-aligned LLMs, and multiple Graph RAG frameworks, GRASP attains the strongest type-faithful reconstruction where prior methods fail, reaching up to 82.9 F1. We further evaluate defenses and propose two lightweight mitigations that substantially reduce reconstruction fidelity without utility loss.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRASP\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u4eceGraph RAG\u7cfb\u7edf\u4e2d\u63d0\u53d6\u77e5\u8bc6\u56fe\u8c31\u5b50\u56fe\uff0c\u514b\u670d\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u8fbe\u523082.9 F1\u5206\u6570\u7684\u9ad8\u6548\u91cd\u5efa\u3002", "motivation": "Graph RAG\u7cfb\u7edf\u867d\u7136\u589e\u5f3a\u4e86LLM\u7684\u5173\u7cfb\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5e26\u6765\u4e86\u65b0\u7684\u9690\u79c1\u5a01\u80c1\uff1a\u653b\u51fb\u8005\u53ef\u80fd\u4ece\u76ee\u6807RAG\u7cfb\u7edf\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u91cd\u5efa\u5b50\u56fe\uff0c\u4ece\u800c\u7a83\u53d6\u9690\u79c1\u4fe1\u606f\u548c\u590d\u73b0\u7cbe\u5fc3\u6784\u5efa\u7684\u77e5\u8bc6\u8d44\u4ea7\u3002\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5728Graph RAG\u73af\u5883\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u65b0\u7684\u653b\u51fb\u6280\u672f\u3002", "method": "\u63d0\u51faGRASP\u653b\u51fb\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a(1) \u5c06\u63d0\u53d6\u4efb\u52a1\u91cd\u6784\u4e3a\u4e0a\u4e0b\u6587\u5904\u7406\u4efb\u52a1\uff1b(2) \u901a\u8fc7\u6bcf\u8bb0\u5f55\u6807\u8bc6\u7b26\u5f3a\u5236\u683c\u5f0f\u5408\u89c4\u3001\u5b9e\u4f8b\u63a5\u5730\u7684\u8f93\u51fa\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u4fdd\u7559\u5173\u7cfb\u7ec6\u8282\uff1b(3) \u4f7f\u7528\u52a8\u91cf\u611f\u77e5\u8c03\u5ea6\u5668\u591a\u6837\u5316\u76ee\u6807\u9a71\u52a8\u7684\u653b\u51fb\u67e5\u8be2\uff0c\u5728\u4e25\u683c\u67e5\u8be2\u9884\u7b97\u5185\u64cd\u4f5c\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u3001\u56db\u4e2a\u5b89\u5168\u5bf9\u9f50\u7684LLM\u548c\u591a\u4e2aGraph RAG\u6846\u67b6\u4e0a\uff0cGRASP\u5b9e\u73b0\u4e86\u6700\u5f3a\u7684\u7c7b\u578b\u5fe0\u5b9e\u91cd\u5efa\uff0c\u8fbe\u5230\u6700\u9ad882.9 F1\u5206\u6570\uff0c\u800c\u5148\u524d\u65b9\u6cd5\u5b8c\u5168\u5931\u8d25\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u9632\u5fa1\u63aa\u65bd\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u8f7b\u91cf\u7ea7\u7f13\u89e3\u65b9\u6848\u3002", "conclusion": "Graph RAG\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0cGRASP\u653b\u51fb\u80fd\u591f\u6709\u6548\u63d0\u53d6\u77e5\u8bc6\u56fe\u8c31\u5b50\u56fe\u3002\u9700\u8981\u52a0\u5f3aGraph RAG\u7cfb\u7edf\u7684\u5b89\u5168\u9632\u62a4\uff0c\u8bba\u6587\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u7f13\u89e3\u65b9\u6848\u53ef\u4ee5\u5728\u4e0d\u635f\u5931\u5b9e\u7528\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2602.06079", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06079", "abs": "https://arxiv.org/abs/2602.06079", "authors": ["Liangyu Wang", "Siqi Zhang", "Junjie Wang", "Yiming Dong", "Bo Zheng", "Zihan Qiu", "Shengkun Tang", "Di Wang", "Rui Men", "Dayiheng Liu"], "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers", "comment": null, "summary": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.", "AI": {"tldr": "Canzona\u6846\u67b6\u89e3\u51b3\u4e86\u77e9\u9635\u4f18\u5316\u5668\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u903b\u8f91\u4f18\u5316\u5668\u5206\u914d\u4e0e\u7269\u7406\u53c2\u6570\u5206\u5e03\uff0c\u5728256 GPU\u4e0a\u5b9e\u73b01.57\u500d\u52a0\u901f\u548c5.8\u500d\u4f18\u5316\u5668\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\u63a8\u52a8\u4e86\u5bf9\u77e9\u9635\u4f18\u5316\u5668\u7684\u9700\u6c42\uff0c\u4f46\u8fd9\u4e9b\u4f18\u5316\u5668\u9700\u8981\u6574\u4f53\u66f4\u65b0\uff0c\u4e0e\u5206\u5e03\u5f0f\u6846\u67b6\u4e2d\u7684\u5f20\u91cf\u5206\u7247\u5b58\u5728\u51b2\u7a81\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u6216\u8fdd\u53cd\u9ad8\u6548\u901a\u4fe1\u539f\u8bed\u51e0\u4f55\u7ea6\u675f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCanzona\u7edf\u4e00\u5f02\u6b65\u8d1f\u8f7d\u5747\u8861\u6846\u67b6\uff1a1\uff09\u6570\u636e\u5e76\u884c\u91c7\u7528alpha\u5e73\u8861\u9759\u6001\u5206\u533a\u7b56\u7565\uff0c\u4fdd\u6301\u539f\u5b50\u6027\u540c\u65f6\u5e73\u8861\u8d1f\u8f7d\uff1b2\uff09\u5f20\u91cf\u5e76\u884c\u91c7\u7528\u5f02\u6b65\u8ba1\u7b97\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u5fae\u7ec4\u8c03\u5ea6\u6279\u91cf\u5904\u7406\u5206\u7247\u66f4\u65b0\u5e76\u9690\u85cf\u91cd\u6784\u5f00\u9500\u3002", "result": "\u5728Qwen3\u6a21\u578b\u5bb6\u65cf\uff08\u6700\u591a320\u4ebf\u53c2\u6570\uff09\u548c256\u4e2aGPU\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u73b0\u6709\u5e76\u884c\u67b6\u6784\u7684\u6548\u7387\uff0c\u7aef\u5230\u7aef\u8fed\u4ee3\u65f6\u95f4\u52a0\u901f1.57\u500d\uff0c\u4f18\u5316\u5668\u6b65\u9aa4\u5ef6\u8fdf\u964d\u4f4e5.8\u500d\u3002", "conclusion": "Canzona\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u77e9\u9635\u4f18\u5316\u5668\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u89e3\u8026\u8bbe\u8ba1\u548c\u5f02\u6b65\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06485", "abs": "https://arxiv.org/abs/2602.06485", "authors": ["Haotian Chen", "Xin Cong", "Shengda Fan", "Yuyang Fu", "Ziqin Gong", "Yaxi Lu", "Yishan Li", "Boye Niu", "Chengjun Pan", "Zijun Song", "Huadong Wang", "Yesai Wu", "Yueying Wu", "Zihao Xie", "Yukun Yan", "Zhong Zhang", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "title": "AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents", "comment": null, "summary": "While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e864B\u53c2\u6570\u89c4\u6a21\u7684\u8fb9\u7f18\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u51fa\u4e86AgentCPM-Explore\u6a21\u578b\uff0c\u901a\u8fc7\u53c2\u6570\u878d\u5408\u3001\u5956\u52b1\u53bb\u566a\u548c\u4e0a\u4e0b\u6587\u4f18\u5316\u89e3\u51b3\u4e86\u8fb9\u7f18\u6a21\u578b\u7684\u4e09\u4e2a\u74f6\u9888\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u800c\u8fb9\u7f18\u89c4\u6a21\u6a21\u578b\uff084B\u53c2\u6570\u7ea7\u522b\uff09\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u8fb9\u7f18\u89c4\u6a21\u667a\u80fd\u4f53\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u89e3\u51b3\u5176\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faAgentCPM-Explore\u6a21\u578b\uff0c\u91c7\u7528\u6574\u4f53\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u53c2\u6570\u7a7a\u95f4\u6a21\u578b\u878d\u5408\u89e3\u51b3SFT\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\uff1b2\uff09\u5956\u52b1\u4fe1\u53f7\u53bb\u566a\u89e3\u51b3RL\u4e2d\u7684\u566a\u58f0\u654f\u611f\u95ee\u9898\uff1b3\uff09\u4e0a\u4e0b\u6587\u4fe1\u606f\u7cbe\u70bc\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u63a8\u7406\u9000\u5316\u95ee\u9898\u3002", "result": "AgentCPM-Explore\u57284B\u7ea7\u522b\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u4e868B\u7ea7\u522bSOTA\u6a21\u578b\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u751a\u81f3\u8d85\u8d8a\u4e86Claude-4.5-Sonnet\u6216DeepSeek-v3.2\u7b49\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002\u5728GAIA\u6587\u672c\u4efb\u52a1\u4e2d\u8fbe\u523097.09%\u51c6\u786e\u7387\uff08pass@64\uff09\u3002", "conclusion": "\u8fb9\u7f18\u89c4\u6a21\u6a21\u578b\u7684\u74f6\u9888\u5e76\u975e\u5176\u5185\u5728\u80fd\u529b\u4e0a\u9650\uff0c\u800c\u662f\u63a8\u7406\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5efa\u7acb\u7684\u8bad\u7ec3\u6846\u67b6\uff0cAgentCPM-Explore\u6709\u6548\u91ca\u653e\u4e86\u8fb9\u7f18\u89c4\u6a21\u6a21\u578b\u88ab\u4f4e\u4f30\u7684\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\u3002"}}
{"id": "2602.06518", "categories": ["cs.CR", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.06518", "abs": "https://arxiv.org/abs/2602.06518", "authors": ["Tim Kutta", "Martin Dunsche", "Yu Wei", "Vassilis Zikas"], "title": "Sequential Auditing for f-Differential Privacy", "comment": "19 pages, 19 figures", "summary": "We present new auditors to assess Differential Privacy (DP) of an algorithm based on output samples. Such empirical auditors are common to check for algorithmic correctness and implementation bugs. Most existing auditors are batch-based or targeted toward the traditional notion of $(\\varepsilon,\u03b4)$-DP; typically both. In this work, we shift the focus to the highly expressive privacy concept of $f$-DP, in which the entire privacy behavior is captured by a single tradeoff curve. Our auditors detect violations across the full privacy spectrum with statistical significance guarantees, which are supported by theory and simulations. Most importantly, and in contrast to prior work, our auditors do not require a user-specified sample size as an input. Rather, they adaptively determine a near-optimal number of samples needed to reach a decision, thereby avoiding the excessively large sample sizes common in many auditing studies. This reduction in sampling cost becomes especially beneficial for expensive training procedures such as DP-SGD. Our method supports both whitebox and blackbox settings and can also be executed in single-run frameworks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8f93\u51fa\u6837\u672c\u8bc4\u4f30\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u7684\u65b0\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8ef-DP\u9690\u79c1\u6982\u5ff5\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6700\u4f18\u6837\u672c\u91cf\uff0c\u652f\u6301\u767d\u76d2\u548c\u9ed1\u76d2\u8bbe\u7f6e", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u5ba1\u8ba1\u65b9\u6cd5\u591a\u4e3a\u6279\u91cf\u5904\u7406\u6216\u9488\u5bf9\u4f20\u7edf(\u03b5,\u03b4)-DP\uff0c\u4e14\u9700\u8981\u7528\u6237\u6307\u5b9a\u6837\u672c\u91cf\uff0c\u5bfc\u81f4\u6837\u672c\u91cf\u8fc7\u5927\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6602\u8d35\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5982DP-SGD", "method": "\u5f00\u53d1\u57fa\u4e8ef-DP\u6982\u5ff5\u7684\u65b0\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u68c0\u6d4b\u5b8c\u6574\u9690\u79c1\u8c31\u7cfb\u4e2d\u7684\u8fdd\u89c4\uff0c\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u4fdd\u8bc1\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6700\u4f18\u6837\u672c\u91cf\uff0c\u65e0\u9700\u7528\u6237\u6307\u5b9a\u6837\u672c\u5927\u5c0f", "result": "\u5ba1\u8ba1\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u548c\u6a21\u62df\u9a8c\u8bc1\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6240\u9700\u6837\u672c\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6602\u8d35\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u652f\u6301\u767d\u76d2\u3001\u9ed1\u76d2\u8bbe\u7f6e\u548c\u5355\u6b21\u8fd0\u884c\u6846\u67b6", "conclusion": "\u63d0\u51fa\u7684f-DP\u5ba1\u8ba1\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6837\u672c\u91cf\uff0c\u51cf\u5c11\u91c7\u6837\u6210\u672c\uff0c\u4e3a\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177"}}
{"id": "2602.06085", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06085", "abs": "https://arxiv.org/abs/2602.06085", "authors": ["Maxim Moraru", "Kamalavasan Kamalakkannan", "Jered Dominguez-Trujillo", "Patrick Diehl", "Atanu Barai", "Julien Loiseau", "Zachary Kent Baker", "Howard Pritchard", "Galen M Shipman"], "title": "LAAFD: LLM-based Agents for Accelerated FPGA Design", "comment": null, "summary": "FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.", "AI": {"tldr": "LAAFD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u80fd\u591f\u5c06\u901a\u7528C++\u4ee3\u7801\u8f6c\u6362\u4e3a\u4f18\u5316\u7684Vitis HLS\u5185\u6838\uff0c\u5b9e\u73b0\u63a5\u8fd1\u624b\u5de5\u8c03\u4f18\u7684\u6027\u80fd\uff0899.9%\u51e0\u4f55\u5e73\u5747\u6027\u80fd\uff09\uff0c\u663e\u8457\u964d\u4f4eFPGA\u52a0\u901f\u7684\u4e13\u4e1a\u95e8\u69db\u3002", "motivation": "FPGA\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5177\u6709\u9ad8\u6027\u80fd\u3001\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u4e13\u95e8\u7684\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5176\u5e94\u7528\u53d7\u5230\u9650\u5236\u3002\u867d\u7136\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u63d0\u9ad8\u4e86\u751f\u4ea7\u529b\uff0c\u4f46\u7ade\u4e89\u6027\u8bbe\u8ba1\u4ecd\u9700\u8981\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u548c\u7cbe\u7ec6\u7684\u6570\u636e\u6d41\u8bbe\u8ba1\u3002", "method": "LAAFD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u80fd\u591f\u5c06\u901a\u7528C++\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u4e3a\u4f18\u5316\u7684Vitis HLS\u5185\u6838\u3002\u8be5\u65b9\u6cd5\u81ea\u52a8\u5316\u5173\u952e\u8f6c\u6362\uff1a\u6df1\u5ea6\u6d41\u6c34\u7ebf\u5316\u3001\u5411\u91cf\u5316\u548c\u6570\u636e\u6d41\u5206\u533a\uff0c\u5e76\u901a\u8fc7HLS\u534f\u540c\u4eff\u771f\u548c\u7efc\u5408\u53cd\u9988\u5f62\u6210\u95ed\u73af\uff0c\u9a8c\u8bc1\u6b63\u786e\u6027\u7684\u540c\u65f6\u8fed\u4ee3\u6539\u8fdb\u5468\u671f\u6267\u884c\u65f6\u95f4\u3002", "result": "\u5728\u4ee3\u8868HPC\u5e38\u89c1\u8ba1\u7b97\u6a21\u5f0f\u768415\u4e2a\u5185\u6838\u6d4b\u8bd5\u5957\u4ef6\u4e2d\uff0cLAAFD\u76f8\u6bd4\u624b\u5de5\u8c03\u4f18\u7684Vitis HLS\u57fa\u51c6\u5b9e\u73b0\u4e8699.9%\u7684\u51e0\u4f55\u5e73\u5747\u6027\u80fd\u3002\u5bf9\u4e8e\u6a21\u677f\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\uff0cLAAFD\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eDSL\u7684HLS\u4ee3\u7801\u751f\u6210\u5668SODA\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u751f\u6210\u66f4\u6613\u8bfb\u7684\u5185\u6838\u3002", "conclusion": "LAAFD\u663e\u8457\u964d\u4f4e\u4e86FPGA\u52a0\u901f\u7684\u4e13\u4e1a\u95e8\u69db\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6548\u7387\uff0c\u4e3aFPGA\u5728\u79d1\u5b66\u548c\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06532", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06532", "abs": "https://arxiv.org/abs/2602.06532", "authors": ["Hema Karnam Surendrababu", "Nithin Nagaraj"], "title": "Dependable Artificial Intelligence with Reliability and Security (DAIReS): A Unified Syndrome Decoding Approach for Hallucination and Backdoor Trigger Detection", "comment": null, "summary": "Machine Learning (ML) models, including Large Language Models (LLMs), are characterized by a range of system-level attributes such as security and reliability. Recent studies have demonstrated that ML models are vulnerable to multiple forms of security violations, among which backdoor data-poisoning attacks represent a particularly insidious threat, enabling unauthorized model behavior and systematic misclassification. In parallel, deficiencies in model reliability can manifest as hallucinations in LLMs, leading to unpredictable outputs and substantial risks for end users. In this work on Dependable Artificial Intelligence with Reliability and Security (DAIReS), we propose a novel unified approach based on Syndrome Decoding for the detection of both security and reliability violations in learning-based systems. Specifically, we adapt the syndrome decoding approach to the NLP sentence-embedding space, enabling the discrimination of poisoned and non-poisoned samples within ML training datasets. Additionally, the same methodology can effectively detect hallucinated content due to self referential meta explanation tasks in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSyndrome Decoding\u7684\u7edf\u4e00\u65b9\u6cd5DAIReS\uff0c\u7528\u4e8e\u68c0\u6d4b\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u8fdd\u89c4\uff0c\u5305\u62ec\u68c0\u6d4b\u6570\u636e\u6295\u6bd2\u653b\u51fb\u548cLLM\u5e7b\u89c9\u5185\u5bb9", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5b58\u5728\u591a\u79cd\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff1a\u5b89\u5168\u65b9\u9762\u6613\u53d7\u540e\u95e8\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u5bfc\u81f4\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u884c\u4e3a\u548c\u7cfb\u7edf\u6027\u8bef\u5206\u7c7b\uff1b\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u8f93\u51fa\uff0c\u7ed9\u7ec8\u7aef\u7528\u6237\u5e26\u6765\u91cd\u5927\u98ce\u9669", "method": "\u63d0\u51faDAIReS\u65b9\u6cd5\uff0c\u57fa\u4e8eSyndrome Decoding\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u5c06\u7efc\u5408\u5f81\u89e3\u7801\u65b9\u6cd5\u9002\u914d\u5230NLP\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\uff0c\u80fd\u591f\u68c0\u6d4bML\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u6295\u6bd2\u548c\u975e\u6295\u6bd2\u6837\u672c\uff0c\u540c\u65f6\u4e5f\u80fd\u6709\u6548\u68c0\u6d4bLLM\u4e2d\u7531\u4e8e\u81ea\u5f15\u7528\u5143\u89e3\u91ca\u4efb\u52a1\u4ea7\u751f\u7684\u5e7b\u89c9\u5185\u5bb9", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7edf\u4e00\u68c0\u6d4b\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u8fdd\u89c4\uff0c\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u5728NLP\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u4e2d\u533a\u5206\u6295\u6bd2\u548c\u975e\u6295\u6bd2\u6837\u672c\uff1b2\uff09\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9", "conclusion": "Syndrome Decoding\u65b9\u6cd5\u4e3a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5e94\u5bf9\u6570\u636e\u6295\u6bd2\u653b\u51fb\u548cLLM\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u8d56\u6027"}}
{"id": "2602.06498", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06498", "abs": "https://arxiv.org/abs/2602.06498", "authors": ["Arno Geimer"], "title": "BouquetFL: Emulating diverse participant hardware in Federated Learning", "comment": null, "summary": "In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.", "AI": {"tldr": "BouquetFL\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5355\u53f0\u7269\u7406\u673a\u4e0a\u6a21\u62df\u5f02\u6784\u5ba2\u6237\u7aef\u786c\u4ef6\u6765\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u786c\u4ef6\u5f02\u6784\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u5728\u4e2d\u592e\u673a\u5668\u4e0a\u8fdb\u884c\u6a21\u62df\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u53c2\u4e0e\u65b9\u4e4b\u95f4\u6f5c\u5728\u7684\u786c\u4ef6\u5f02\u6784\u6027\uff0c\u8fd9\u5bfc\u81f4\u4e86\u7814\u7a76\u65b9\u6cd5\u4e0e\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u8d44\u6e90\u9650\u5236\u7f16\u7a0b\u6a21\u62df\u4e0d\u540c\u7684\u786c\u4ef6\u914d\u7f6e\uff0c\u5728\u5355\u53f0\u7269\u7406\u673a\u4e0a\u6a21\u62df\u5f02\u6784\u5ba2\u6237\u7aef\u786c\u4ef6\uff0c\u63d0\u4f9b\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u786c\u4ef6\u6d41\u884c\u5ea6\u7684\u81ea\u5b9a\u4e49\u786c\u4ef6\u91c7\u6837\u5668\uff0c\u4ee5\u53ca\u4ece\u5e38\u89c1\u6d88\u8d39\u7ea7\u548c\u5c0f\u578b\u5b9e\u9a8c\u5ba4\u8bbe\u5907\u6d3e\u751f\u7684\u591a\u79cd\u786c\u4ef6\u914d\u7f6e\u6587\u4ef6\u3002", "result": "\u5f00\u53d1\u4e86BouquetFL\u6846\u67b6\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5728\u4e0d\u9700\u8981\u591a\u53f0\u7269\u7406\u8bbe\u5907\u7684\u60c5\u51b5\u4e0b\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7cfb\u7edf\u5f02\u6784\u6027\uff0c\u5c06\u5b9e\u9a8c\u5b9e\u8df5\u66f4\u8d34\u8fd1\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u3002", "conclusion": "BouquetFL\u4e3a\u7814\u7a76\u9ad8\u5ea6\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u65b9\u6cd5\u4e0e\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u652f\u6301\u66f4\u771f\u5b9e\u7684\u8054\u90a6\u5b66\u4e60\u5b9e\u9a8c\u3002"}}
{"id": "2602.06499", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06499", "abs": "https://arxiv.org/abs/2602.06499", "authors": ["Gyeongseo Park", "Eungyeong Lee", "Song-woo Sok", "Myung-Hoon Cha", "Kwangwon Koh", "Baik-Song An", "Hongyeon Kim", "Ki-Dong Kang"], "title": "FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training", "comment": "14 pages,10 figures", "summary": "Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.", "AI": {"tldr": "FCDP\u662f\u4e00\u79cd\u9488\u5bf9\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u524d\u5411\u4f20\u64ad\u53c2\u6570\u7f13\u5b58\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u590d\u7528\uff0c\u51cf\u5c1150%\u7684\u8282\u70b9\u95f4\u901a\u4fe1\uff0c\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u573a\u666f\u4e0b\u53ef\u51cf\u5c1199%\u4ee5\u4e0a\u7684\u8282\u70b9\u95f4\u6d41\u91cf\u3002", "motivation": "\u73b0\u6709ZeRO-3\u4f18\u5316\u65b9\u6848\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u5b58\u5728\u74f6\u9888\uff1aGPU\u5185\u5b58\u7f13\u5b58\u65b9\u6848\uff08\u5982MiCS\u3001ZeRO++\uff09\u7528\u5185\u5b58\u5bb9\u91cf\u6362\u53d6\u901a\u4fe1\u51cf\u5c11\uff0c\u4f46\u4f1a\u5bfc\u81f4\u5927\u6a21\u578b\u5185\u5b58\u6ea2\u51fa\uff1b\u4e3b\u673a\u5185\u5b58\u5378\u8f7d\u65b9\u6848\uff08\u5982ZeRO-Offload\u3001ZeRO-Infinity\uff09\u6269\u5c55\u4e86\u5bb9\u91cf\u4f46PCIe\u5f00\u9500\u964d\u4f4e\u4e86\u541e\u5410\u91cf\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u5728\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u4e2d\uff0c\u4e3b\u673a\u5185\u5b58\u53ef\u4ee5\u4f5c\u4e3a\u5feb\u901f\u7f13\u5b58\u5c42\uff0c\u6027\u80fd\u4f18\u4e8e\u8282\u70b9\u95f4\u901a\u4fe1\u3002", "method": "FCDP\u5c06\u524d\u5411\u4f20\u64ad\u53c2\u6570\u7f13\u5b58\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u901a\u8fc7\u5feb\u901f\u7684\u8282\u70b9\u5185all-gather\u590d\u7528\u8fd9\u4e9b\u53c2\u6570\uff0c\u4ece\u800c\u6d88\u9664\u5197\u4f59\u7684\u8282\u70b9\u95f4\u901a\u4fe1\u3002\u5bf9\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0cFCDP\u9009\u62e9\u6027\u5730\u53ea\u901a\u4fe1\u53ef\u8bad\u7ec3\u53c2\u6570\u4ee5\u6700\u5927\u5316\u7f13\u5b58\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86ZeRO-3\u7684\u6700\u5c0fGPU\u5185\u5b58\u5360\u7528\u3002", "result": "\u5728\u5546\u7528\u96c6\u7fa4\u8bbe\u7f6e\u4e2d\uff0cFCDP\u76f8\u6bd4ZeRO-3\u5b9e\u73b0\u4e86\u9ad8\u8fbe100\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u76f8\u6bd4ZeRO++\u5b9e\u73b0\u4e8651\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86ZeRO-3\u7684\u6700\u5927\u6279\u5904\u7406\u5927\u5c0f\u3002", "conclusion": "FCDP\u901a\u8fc7\u5c06\u4e3b\u673a\u5185\u5b58\u4f5c\u4e3a\u5feb\u901f\u7f13\u5b58\u5c42\u800c\u975e\u6ea2\u51fa\u5c42\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u4e2d\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u573a\u666f\u3002"}}
{"id": "2602.06527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06527", "abs": "https://arxiv.org/abs/2602.06527", "authors": ["Shengxuan Qiu", "Haochen Huang", "Shuzhang Zhong", "Pengfei Zuo", "Meng Li"], "title": "HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction", "comment": null, "summary": "Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.", "AI": {"tldr": "HyPER\u662f\u4e00\u79cd\u7528\u4e8e\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u591a\u8def\u5f84\u89e3\u7801\u7684\u8bad\u7ec3\u514d\u8d39\u5728\u7ebf\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u6269\u5c55-\u7f29\u51cf\u63a7\u5236\u91cd\u65b0\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22-\u5229\u7528\u6743\u8861\u4e0a\u5b58\u5728\u5c40\u9650\uff1a\u6811\u7ed3\u6784\u641c\u7d22\u901a\u8fc7\u8106\u5f31\u7684\u6269\u5c55\u89c4\u5219\u786c\u7f16\u7801\u63a2\u7d22\uff0c\u5e72\u6270\u540e\u8bad\u7ec3\u63a8\u7406\uff1b\u5e76\u884c\u63a8\u7406\u5219\u8fc7\u5ea6\u63a2\u7d22\u5197\u4f59\u5047\u8bbe\u8def\u5f84\u4e14\u4f9d\u8d56\u5f31\u7b54\u6848\u9009\u62e9\u3002\u7814\u7a76\u53d1\u73b0\u6700\u4f18\u5e73\u8861\u662f\u9636\u6bb5\u4f9d\u8d56\u7684\uff0c\u6b63\u786e\u548c\u9519\u8bef\u63a8\u7406\u8def\u5f84\u5f80\u5f80\u5728\u540e\u671f\u624d\u5206\u53c9\u3002", "method": "\u5c06\u6d4b\u8bd5\u65f6\u6269\u5c55\u91cd\u65b0\u8868\u8ff0\u4e3a\u5047\u8bbe\u6c60\u4e0a\u7684\u52a8\u6001\u6269\u5c55-\u7f29\u51cf\u63a7\u5236\u95ee\u9898\u3002\u63d0\u51faHyPER\uff1a\u5305\u542b\u5728\u7ebf\u63a7\u5236\u5668\uff08\u968f\u5047\u8bbe\u6c60\u6f14\u5316\u4ece\u63a2\u7d22\u8f6c\u5411\u5229\u7528\uff09\u3001\u4ee4\u724c\u7ea7\u7cbe\u70bc\u673a\u5236\uff08\u65e0\u9700\u5b8c\u6574\u8def\u5f84\u91cd\u91c7\u6837\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\u65f6\u5229\u7528\uff09\u3001\u957f\u5ea6\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u805a\u5408\u7b56\u7565\uff08\u53ef\u9760\u7b54\u6848\u65f6\u5229\u7528\uff09\u3002", "result": "\u5728\u56db\u4e2a\u4e13\u5bb6\u6df7\u5408\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHyPER\u59cb\u7ec8\u5b9e\u73b0\u66f4\u4f18\u7684\u51c6\u786e\u6027-\u8ba1\u7b97\u6743\u8861\uff0c\u51c6\u786e\u6027\u63d0\u9ad88-10%\uff0c\u540c\u65f6\u4ee4\u724c\u4f7f\u7528\u91cf\u51cf\u5c1125-40%\u3002", "conclusion": "HyPER\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8def\u5f84\u63a8\u7406\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06502", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06502", "abs": "https://arxiv.org/abs/2602.06502", "authors": ["Ying Yuan", "Pengfei Zuo", "Bo Wang", "Zhangyu Chen", "Zhipeng Tan", "Zhou Yu"], "title": "DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving", "comment": "23 pages, 15 figures", "summary": "In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\\times$ under the same TTFT SLO constraints compared with SOTA work.", "AI": {"tldr": "DualMap\u662f\u4e00\u79cd\u7528\u4e8e\u5206\u5e03\u5f0fLLM\u670d\u52a1\u7684\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u54c8\u5e0c\u51fd\u6570\u5c06\u8bf7\u6c42\u6620\u5c04\u5230\u5019\u9009\u5b9e\u4f8b\uff0c\u667a\u80fd\u9009\u62e9\u6700\u4f73\u5b9e\u4f8b\uff0c\u540c\u65f6\u5b9e\u73b0\u7f13\u5b58\u4eb2\u548c\u6027\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u5728LLM\u670d\u52a1\u4e2d\uff0c\u8de8\u8bf7\u6c42\u91cd\u7528\u63d0\u793a\u7684KV\u7f13\u5b58\u5bf9\u964d\u4f4eTTFT\u548c\u670d\u52a1\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u8c03\u5ea6\u5668\u65e0\u6cd5\u534f\u8c03\u7f13\u5b58\u4eb2\u548c\u6027\u8c03\u5ea6\uff08\u5c06\u76f8\u540c\u63d0\u793a\u524d\u7f00\u7684\u8bf7\u6c42\u653e\u5728\u4e00\u8d77\uff09\u548c\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\uff08\u5c06\u8bf7\u6c42\u5747\u5300\u5206\u5e03\u5230\u8ba1\u7b97\u5b9e\u4f8b\uff09\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u5728\u5355\u4e00\u6620\u5c04\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u3002", "method": "\u63d0\u51faDualMap\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u57fa\u4e8e\u8bf7\u6c42\u63d0\u793a\u7684\u54c8\u5e0c\u51fd\u6570\u5c06\u6bcf\u4e2a\u8bf7\u6c42\u6620\u5c04\u5230\u4e24\u4e2a\u5019\u9009\u5b9e\u4f8b\uff1b2\uff09\u57fa\u4e8e\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u667a\u80fd\u9009\u62e9\u66f4\u597d\u7684\u5019\u9009\u5b9e\u4f8b\uff1b3\uff09\u5f15\u5165\u4e09\u79cd\u6280\u672f\uff1aSLO\u611f\u77e5\u8bf7\u6c42\u8def\u7531\u3001\u70ed\u70b9\u611f\u77e5\u518d\u5e73\u8861\u3001\u8f7b\u91cf\u7ea7\u53cc\u54c8\u5e0c\u73af\u6269\u5c55\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u7684TTFT SLO\u7ea6\u675f\u4e0b\uff0cDualMap\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5de5\u4f5c\u5c06\u6709\u6548\u8bf7\u6c42\u5bb9\u91cf\u63d0\u9ad8\u4e86\u6700\u591a2.25\u500d\u3002", "conclusion": "DualMap\u901a\u8fc7\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u7f13\u5b58\u4eb2\u548c\u6027\u4e0e\u8d1f\u8f7d\u5747\u8861\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u63d0\u9ad8\u4e86\u5206\u5e03\u5f0fLLM\u670d\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u548c\u503e\u659c\u7684\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.06555", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06555", "abs": "https://arxiv.org/abs/2602.06555", "authors": ["Lanpei Li", "Massimo Coppola", "Malio Li", "Valerio Besozzi", "Jack Bell", "Vincenzo Lomonaco"], "title": "Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms", "comment": "Accepted at AHPC3 workshop, PDP 2026", "summary": "We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u52a8\u6001\u7ba1\u7406\u7ed3\u6784\u5316\u5e76\u884c\u5904\u7406\u9aa8\u67b6\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8eFarm\u6a21\u5f0f\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u6765\u7ba1\u7406\u5e76\u884c\u5ea6\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6a21\u578b\u7684\u6027\u80fd\u5bfc\u5411\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u9002\u5e94\u5e73\u53f0\u7279\u5b9a\u9650\u5236\u3002", "motivation": "\u5c06\u7c7b\u4f3cHPC\u7684\u6027\u80fd\u548c\u5f39\u6027\u5f15\u5165\u65e0\u670d\u52a1\u5668\u548c\u8fde\u7eed\u8ba1\u7b97\u73af\u5883\uff0c\u540c\u65f6\u4fdd\u6301\u9aa8\u67b6\u7f16\u7a0b\u7684\u53ef\u7f16\u7a0b\u6027\u4f18\u52bf\u3002\u89e3\u51b3\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u7ed3\u6784\u5316\u5e76\u884c\u5904\u7406\u9aa8\u67b6\u7684\u52a8\u6001\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u57fa\u4e8eOpenFaaS\u5e73\u53f0\u5b9e\u73b0Farm\u6a21\u5f0f\uff0c\u5c06\u5de5\u4f5c\u6c60\u81ea\u52a8\u6269\u5c55\u89c6\u4e3aQoS\u611f\u77e5\u7684\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u3002\u6846\u67b6\u5c06\u53ef\u590d\u7528\u7684farm\u6a21\u677f\u4e0e\u57fa\u4e8eGymnasium\u7684\u76d1\u63a7\u63a7\u5236\u5c42\u7ed3\u5408\uff0c\u66b4\u9732\u961f\u5217\u3001\u65f6\u5e8f\u548cQoS\u6307\u6807\u7ed9\u53cd\u5e94\u5f0f\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u5668\u3002\u7814\u7a76AI\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u6765\u7ba1\u7406farm\u7684\u5e76\u884c\u5ea6\u3002", "result": "AI\u9a71\u52a8\u7684\u7ba1\u7406\u76f8\u6bd4\u7eaf\u57fa\u4e8e\u6a21\u578b\u7684\u6027\u80fd\u5bfc\u5411\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u9002\u5e94\u5e73\u53f0\u7279\u5b9a\u9650\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u8d44\u6e90\u4f7f\u7528\u548c\u7a33\u5b9a\u6269\u5c55\u884c\u4e3a\u7684\u540c\u65f6\u6539\u5584QoS\u3002\u8bc4\u4f30\u4e86\u4e24\u79cd\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e0e\u57fa\u4e8e\u7b80\u5355farm\u6027\u80fd\u6a21\u578b\u7684\u53cd\u5e94\u5f0f\u7ba1\u7406\u57fa\u7ebf\u3002", "conclusion": "\u57fa\u4e8eAI\u7684\u7ba1\u7406\u6846\u67b6\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u5bf9\u7ed3\u6784\u5316\u5e76\u884c\u5904\u7406\u9aa8\u67b6\u7684\u52a8\u6001\u7ba1\u7406\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u5e73\u8861\u6027\u80fd\u3001\u8d44\u6e90\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4e3aHPC-like\u6027\u80fd\u5728\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.06554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06554", "abs": "https://arxiv.org/abs/2602.06554", "authors": ["Tianyi Hu", "Qingxu Fu", "Yanxi Chen", "Zhaoyang Liu", "Bolin Ding"], "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees", "comment": null, "summary": "Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.\n  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.\n  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.\n  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u4e3b\u6d41RL\u7b97\u6cd5\u5728\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e2d\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\uff0c\u63d0\u51fa\u4e86\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u7684\u65e0\u8bc4\u8bba\u5bb6\u65b9\u6cd5SeeUPO\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u5728\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e2d\u7f3a\u4e4f\u9a8c\u8bc1\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u65e0\u6cd5\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\uff0c\u9700\u8981\u8bbe\u8ba1\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u7684RL\u7b97\u6cd5\u3002", "method": "\u63d0\u51faSeeUPO\uff08\u5e8f\u5217\u7ea7\u987a\u5e8f\u66f4\u65b0\u7b56\u7565\u4f18\u5316\uff09\uff0c\u5c06\u591a\u8f6e\u4ea4\u4e92\u5efa\u6a21\u4e3a\u987a\u5e8f\u6267\u884c\u7684\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u901a\u8fc7\u53cd\u5411\u6267\u884c\u987a\u5e8f\u7684\u9010\u8f6e\u987a\u5e8f\u7b56\u7565\u66f4\u65b0\uff0c\u786e\u4fdd\u5355\u8c03\u6539\u8fdb\u5e76\u901a\u8fc7\u53cd\u5411\u5f52\u7eb3\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u89e3\u3002", "result": "\u5728AppWorld\u548cBFCL v4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSeeUPO\u76f8\u6bd4\u73b0\u6709\u9aa8\u5e72\u7b97\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1a\u5728Qwen3-14B\u4e0a\u76f8\u5bf9\u589e\u76ca43.3%-54.6%\uff0c\u5728Qwen2.5-14B\u4e0a\u76f8\u5bf9\u589e\u76ca24.1%-41.9%\uff08\u8de8\u57fa\u51c6\u5e73\u5747\uff09\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "SeeUPO\u89e3\u51b3\u4e86\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e2dRL\u7b97\u6cd5\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u7684\u65e0\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u6548\u679c\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.06616", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06616", "abs": "https://arxiv.org/abs/2602.06616", "authors": ["Haoyang Hu", "Zhejun Jiang", "Yueming Lyu", "Junyuan Zhang", "Yi Liu", "Ka-Ho Chow"], "title": "Confundo: Learning to Generate Robust Poison for Practical RAG Systems", "comment": null, "summary": "Retrieval-augmented generation (RAG) is increasingly deployed in real-world applications, where its reference-grounded design makes outputs appear trustworthy. This trust has spurred research on poisoning attacks that craft malicious content, inject it into knowledge sources, and manipulate RAG responses. However, when evaluated in practical RAG systems, existing attacks suffer from severely degraded effectiveness. This gap stems from two overlooked realities: (i) content is often processed before use, which can fragment the poison and weaken its effect, and (ii) users often do not issue the exact queries anticipated during attack design. These factors can lead practitioners to underestimate risks and develop a false sense of security. To better characterize the threat to practical systems, we present Confundo, a learning-to-poison framework that fine-tunes a large language model as a poison generator to achieve high effectiveness, robustness, and stealthiness. Confundo provides a unified framework supporting multiple attack objectives, demonstrated by manipulating factual correctness, inducing biased opinions, and triggering hallucinations. By addressing these overlooked challenges, Confundo consistently outperforms a wide range of purpose-built attacks across datasets and RAG configurations by large margins, even in the presence of defenses. Beyond exposing vulnerabilities, we also present a defensive use case that protects web content from unauthorized incorporation into RAG systems via scraping, with no impact on user experience.", "AI": {"tldr": "Confundo\u662f\u4e00\u4e2a\u9488\u5bf9\u5b9e\u9645RAG\u7cfb\u7edf\u7684\u5b66\u4e60\u578b\u6295\u6bd2\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6295\u6bd2\u751f\u6210\u5668\uff0c\u5728\u591a\u79cd\u653b\u51fb\u76ee\u6807\u4e0b\u5b9e\u73b0\u9ad8\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709RAG\u6295\u6bd2\u653b\u51fb\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u6548\u679c\u4e25\u91cd\u4e0b\u964d\uff0c\u539f\u56e0\u5728\u4e8e\u4e24\u4e2a\u88ab\u5ffd\u89c6\u7684\u73b0\u5b9e\uff1a1) \u5185\u5bb9\u5728\u4f7f\u7528\u524d\u5e38\u88ab\u5904\u7406\uff0c\u53ef\u80fd\u7834\u574f\u6295\u6bd2\u5185\u5bb9\uff1b2) \u7528\u6237\u67e5\u8be2\u5f80\u5f80\u4e0e\u653b\u51fb\u8bbe\u8ba1\u65f6\u9884\u671f\u7684\u4e0d\u540c\u3002\u8fd9\u5bfc\u81f4\u5b9e\u8df5\u8005\u4f4e\u4f30\u98ce\u9669\u5e76\u4ea7\u751f\u865a\u5047\u7684\u5b89\u5168\u611f\u3002", "method": "Confundo\u662f\u4e00\u4e2a\u5b66\u4e60\u578b\u6295\u6bd2\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6295\u6bd2\u751f\u6210\u5668\uff0c\u652f\u6301\u591a\u79cd\u653b\u51fb\u76ee\u6807\uff08\u64cd\u7eb5\u4e8b\u5b9e\u6b63\u786e\u6027\u3001\u8bf1\u5bfc\u504f\u89c1\u89c2\u70b9\u3001\u89e6\u53d1\u5e7b\u89c9\uff09\uff0c\u5e76\u8003\u8651\u5b9e\u9645RAG\u7cfb\u7edf\u7684\u5904\u7406\u6d41\u7a0b\u548c\u7528\u6237\u67e5\u8be2\u6a21\u5f0f\u3002", "result": "Confundo\u5728\u591a\u79cd\u6570\u636e\u96c6\u548cRAG\u914d\u7f6e\u4e0b\uff0c\u5927\u5e45\u4f18\u4e8e\u5404\u79cd\u4e13\u95e8\u8bbe\u8ba1\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u9632\u5fa1\u63aa\u65bd\u7684\u60c5\u51b5\u4e0b\u4e5f\u4fdd\u6301\u9ad8\u6709\u6548\u6027\u3002\u540c\u65f6\u5c55\u793a\u4e86\u9632\u5fa1\u6027\u5e94\u7528\uff0c\u4fdd\u62a4\u7f51\u9875\u5185\u5bb9\u514d\u906d\u672a\u7ecf\u6388\u6743\u7684RAG\u7cfb\u7edf\u6293\u53d6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7Confundo\u6846\u67b6\u66f4\u597d\u5730\u63cf\u8ff0\u4e86\u5b9e\u9645RAG\u7cfb\u7edf\u9762\u4e34\u7684\u5a01\u80c1\uff0c\u66b4\u9732\u4e86\u73b0\u6709\u653b\u51fb\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u65e2\u80fd\u653b\u51fb\u53c8\u80fd\u9632\u5fa1\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2602.06655", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06655", "abs": "https://arxiv.org/abs/2602.06655", "authors": ["Zeta Avarikioti", "Ray Neiheiser", "Krzysztof Pietrzak", "Michelle X. Yeo"], "title": "Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus", "comment": null, "summary": "Over the last years, Ethereum has evolved into a public platform that safeguards the savings of hundreds of millions of people and secures more than $650 billion in assets, placing it among the top 25 stock exchanges worldwide in market capitalization, ahead of Singapore, Mexico, and Thailand. As such, the performance and security of the Ethereum blockchain are not only of theoretical interest, but also carry significant global economic implications. At the time of writing, the Ethereum platform is collectively secured by almost one million validators highlighting its decentralized nature and underlining its economic security guarantees. However, due to this large validator set, the protocol takes around 15 minutes to finalize a block which is prohibitively slow for many real world applications. This delay is largely driven by the cost of aggregating and disseminating signatures across a validator set of this scale. Furthermore, as we show in this paper, the existing protocol that is used to aggregate and disseminate the signatures has several shortcomings that can be exploited by adversaries to shift stake proportion from honest to adversarial nodes. In this paper, we introduce Wonderboom, the first million scale aggregation protocol that can efficiently aggregate the signatures of millions of validators in a single Ethereum slot (x32 faster) while offering higher security guarantees than the state of the art protocol used in Ethereum. Furthermore, to evaluate Wonderboom, we implement the first simulation tool that can simulate such a protocol on the million scale and show that even in the worst case Wonderboom can aggregate and verify more than 2 million signatures within a single Ethereum slot.", "AI": {"tldr": "Wonderboom\u662f\u4e00\u4e2a\u767e\u4e07\u7ea7\u89c4\u6a21\u7684\u7b7e\u540d\u805a\u5408\u534f\u8bae\uff0c\u80fd\u591f\u5728\u5355\u4e2a\u4ee5\u592a\u574a\u65f6\u9699\u5185\u9ad8\u6548\u805a\u5408\u6570\u767e\u4e07\u9a8c\u8bc1\u8005\u7684\u7b7e\u540d\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u534f\u8bae\u5feb32\u500d\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u4fdd\u969c\u3002", "motivation": "\u4ee5\u592a\u574a\u4f5c\u4e3a\u5168\u7403\u91cd\u8981\u91d1\u878d\u5e73\u53f0\uff0c\u62e5\u6709\u8fd1\u767e\u4e07\u9a8c\u8bc1\u8005\uff0c\u4f46\u5176\u73b0\u6709\u534f\u8bae\u9700\u8981\u7ea615\u5206\u949f\u624d\u80fd\u6700\u7ec8\u786e\u5b9a\u533a\u5757\uff0c\u8fd9\u5bf9\u4e8e\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u6765\u8bf4\u8fc7\u6162\u3002\u73b0\u6709\u7b7e\u540d\u805a\u5408\u534f\u8bae\u5b58\u5728\u7f3a\u9677\uff0c\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u5229\u7528\u6765\u8f6c\u79fb\u6743\u76ca\u6bd4\u4f8b\u3002", "method": "\u63d0\u51fa\u4e86Wonderboom\u534f\u8bae\uff0c\u8fd9\u662f\u9996\u4e2a\u767e\u4e07\u7ea7\u89c4\u6a21\u7684\u805a\u5408\u534f\u8bae\u3002\u4e3a\u4e86\u8bc4\u4f30\u8be5\u534f\u8bae\uff0c\u4f5c\u8005\u5b9e\u73b0\u4e86\u9996\u4e2a\u80fd\u591f\u6a21\u62df\u767e\u4e07\u7ea7\u89c4\u6a21\u534f\u8bae\u7684\u4eff\u771f\u5de5\u5177\u3002", "result": "\u5373\u4f7f\u5728\u6700\u574f\u60c5\u51b5\u4e0b\uff0cWonderboom\u4e5f\u80fd\u5728\u5355\u4e2a\u4ee5\u592a\u574a\u65f6\u9699\u5185\u805a\u5408\u548c\u9a8c\u8bc1\u8d85\u8fc7200\u4e07\u4e2a\u7b7e\u540d\uff0c\u6bd4\u73b0\u6709\u534f\u8bae\u5feb32\u500d\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "conclusion": "Wonderboom\u534f\u8bae\u89e3\u51b3\u4e86\u4ee5\u592a\u574a\u5927\u89c4\u6a21\u9a8c\u8bc1\u8005\u96c6\u5408\u5e26\u6765\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533a\u5757\u6700\u7ec8\u786e\u5b9a\u901f\u5ea6\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5bf9\u4ee5\u592a\u574a\u7684\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.06652", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06652", "abs": "https://arxiv.org/abs/2602.06652", "authors": ["Farooq Ahmad Wani", "Alessandro Suglia", "Rohit Saxena", "Aryo Pradipta Gema", "Wai-Chung Kwan", "Fazl Barez", "Maria Sofia Bucarelli", "Fabrizio Silvestri", "Pasquale Minervini"], "title": "Same Answer, Different Representations: Hidden instability in VLMs", "comment": null, "summary": "The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u5b58\u5728\u7f3a\u9677\uff0c\u4ec5\u5173\u6ce8\u8f93\u51fa\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002\u901a\u8fc7\u63d0\u51fa\u8868\u793a\u611f\u77e5\u548c\u9891\u7387\u611f\u77e5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e09\u79cd\u5931\u6548\u6a21\u5f0f\uff1a\u5185\u90e8\u8868\u793a\u6f02\u79fb\u3001\u89c4\u6a21\u4e0d\u6539\u5584\u9c81\u68d2\u6027\u3001\u4ee5\u53ca\u6270\u52a8\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u5dee\u5f02\u5316\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u8f93\u51fa\u5c42\u9762\u7684\u4e0d\u53d8\u6027\uff0c\u9690\u542b\u5047\u8bbe\u7a33\u5b9a\u9884\u6d4b\u53cd\u6620\u7a33\u5b9a\u7684\u591a\u6a21\u6001\u5904\u7406\u8fc7\u7a0b\u3002\u7136\u800c\u8fd9\u79cd\u5047\u8bbe\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6df1\u5165\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u8868\u793a\u611f\u77e5\u548c\u9891\u7387\u611f\u77e5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u91cf\u5185\u90e8\u5d4c\u5165\u6f02\u79fb\u3001\u9891\u8c31\u654f\u611f\u6027\u548c\u7ed3\u6784\u5e73\u6ed1\u6027\uff08\u89c6\u89c9token\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff09\uff0c\u5e76\u7ed3\u5408\u6807\u51c6\u57fa\u4e8e\u6807\u7b7e\u7684\u6307\u6807\u3002\u5728SEEDBench\u3001MMMU\u548cPOPE\u6570\u636e\u96c6\u4e0a\u5bf9\u73b0\u4ee3VLMs\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63ed\u793a\u4e86\u4e09\u79cd\u5931\u6548\u6a21\u5f0f\uff1a1\uff09\u6a21\u578b\u7ecf\u5e38\u4fdd\u6301\u9884\u6d4b\u7b54\u6848\u4e0d\u53d8\uff0c\u4f46\u5185\u90e8\u8868\u793a\u53d1\u751f\u663e\u8457\u6f02\u79fb\uff0c\u67d0\u4e9b\u6270\u52a8\u4e0b\u6f02\u79fb\u63a5\u8fd1\u56fe\u50cf\u95f4\u53d8\u5f02\u7684\u5e45\u5ea6\uff1b2\uff09\u9c81\u68d2\u6027\u4e0d\u968f\u6a21\u578b\u89c4\u6a21\u63d0\u5347\uff0c\u66f4\u5927\u6a21\u578b\u51c6\u786e\u7387\u66f4\u9ad8\u4f46\u654f\u611f\u6027\u76f8\u540c\u6216\u66f4\u5927\uff1b3\uff09\u6270\u52a8\u5bf9\u4e0d\u540c\u4efb\u52a1\u5f71\u54cd\u4e0d\u540c\uff1a\u7834\u574f\u63a8\u7406\u4efb\u52a1\u4e2d\u7c97\u7ec6\u89c6\u89c9\u7ebf\u7d22\u7684\u7ec4\u5408\uff0c\u4f46\u5728\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53ef\u80fd\u51cf\u5c11\u8bef\u62a5\u3002", "conclusion": "\u4ec5\u4f9d\u8d56\u8f93\u51fa\u4e0d\u53d8\u6027\u8bc4\u4f30VLMs\u9c81\u68d2\u6027\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8868\u793a\u5c42\u9762\u8bc4\u4f30\u3002\u6a21\u578b\u5185\u90e8\u8868\u793a\u53ef\u80fd\u5728\u6270\u52a8\u4e0b\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u800c\u66f4\u5927\u6a21\u578b\u867d\u7136\u51c6\u786e\u7387\u66f4\u9ad8\u4f46\u9c81\u68d2\u6027\u5e76\u672a\u6539\u5584\uff0c\u8868\u660e\u9700\u8981\u65b0\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.06630", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06630", "abs": "https://arxiv.org/abs/2602.06630", "authors": ["Mengyao Du", "Han Fang", "Haokai Ma", "Gang Yang", "Quanjun Yin", "Shouling Ji", "Ee-Chien Chang"], "title": "TrapSuffix: Proactive Defense Against Adversarial Suffixes in Jailbreaking", "comment": "23 pages, 11 figures", "summary": "Suffix-based jailbreak attacks append an adversarial suffix, i.e., a short token sequence, to steer aligned LLMs into unsafe outputs. Since suffixes are free-form text, they admit endlessly many surface forms, making jailbreak mitigation difficult. Most existing defenses depend on passive detection of suspicious suffixes, without leveraging the defender's inherent asymmetric ability to inject secrets and proactively conceal gaps. Motivated by this, we take a controllability-oriented perspective and develop a proactive defense that nudges attackers into a no-win dilemma: either they fall into defender-designed optimization traps and fail to produce an effective adversarial suffix, or they can succeed only by generating adversarial suffixes that carry distinctive, traceable fingerprints. We propose TrapSuffix, a lightweight fine-tuning approach that injects trap-aligned behaviors into the base model without changing the inference pipeline. TrapSuffix channels jailbreak attempts into these two outcomes by reshaping the model's response landscape to adversarial suffixes. Across diverse suffix-based jailbreak settings, TrapSuffix reduces the average attack success rate to below 0.01 percent and achieves an average tracing success rate of 87.9 percent, providing both strong defense and reliable traceability. It introduces no inference-time overhead and incurs negligible memory cost, requiring only 15.87 MB of additional memory on average, whereas state-of-the-art LLM-based detection defenses typically incur memory overheads at the 1e4 MB level, while composing naturally with existing filtering-based defenses for complementary protection.", "AI": {"tldr": "TrapSuffix\u662f\u4e00\u79cd\u4e3b\u52a8\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u5728LLM\u4e2d\u690d\u5165\u9677\u9631\u884c\u4e3a\uff0c\u4f7f\u653b\u51fb\u8005\u8981\u4e48\u9677\u5165\u4f18\u5316\u9677\u9631\u65e0\u6cd5\u751f\u6210\u6709\u6548\u5bf9\u6297\u540e\u7f00\uff0c\u8981\u4e48\u751f\u6210\u5e26\u6709\u53ef\u8ffd\u8e2a\u6307\u7eb9\u7684\u540e\u7f00\uff0c\u5b9e\u73b0\u5f3a\u9632\u5fa1\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u540e\u7f00\u7684\u8d8a\u72f1\u653b\u51fb\u9632\u5fa1\u5927\u591a\u662f\u88ab\u52a8\u68c0\u6d4b\u53ef\u7591\u540e\u7f00\uff0c\u6ca1\u6709\u5145\u5206\u5229\u7528\u9632\u5fa1\u8005\u53ef\u4ee5\u4e3b\u52a8\u690d\u5165\u79d8\u5bc6\u548c\u9690\u85cf\u6f0f\u6d1e\u7684\u4e0d\u5bf9\u79f0\u4f18\u52bf\u3002\u540e\u7f00\u653b\u51fb\u6709\u65e0\u9650\u591a\u7684\u8868\u9762\u5f62\u5f0f\uff0c\u4f7f\u5f97\u9632\u5fa1\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51faTrapSuffix\u65b9\u6cd5\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u5c06\u9677\u9631\u5bf9\u9f50\u884c\u4e3a\u6ce8\u5165\u57fa\u7840\u6a21\u578b\uff0c\u4e0d\u6539\u53d8\u63a8\u7406\u6d41\u7a0b\u3002\u901a\u8fc7\u91cd\u5851\u6a21\u578b\u5bf9\u5bf9\u6297\u540e\u7f00\u7684\u54cd\u5e94\u666f\u89c2\uff0c\u5f15\u5bfc\u8d8a\u72f1\u5c1d\u8bd5\u8fdb\u5165\u4e24\u79cd\u7ed3\u679c\uff1a\u8981\u4e48\u9677\u5165\u9632\u5fa1\u8005\u8bbe\u8ba1\u7684\u4f18\u5316\u9677\u9631\u5931\u8d25\uff0c\u8981\u4e48\u751f\u6210\u5e26\u6709\u72ec\u7279\u53ef\u8ffd\u8e2a\u6307\u7eb9\u7684\u5bf9\u6297\u540e\u7f00\u3002", "result": "\u5728\u5404\u79cd\u57fa\u4e8e\u540e\u7f00\u7684\u8d8a\u72f1\u653b\u51fb\u8bbe\u7f6e\u4e2d\uff0cTrapSuffix\u5c06\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u52300.01%\u4ee5\u4e0b\uff0c\u5e73\u5747\u8ffd\u8e2a\u6210\u529f\u7387\u8fbe\u523087.9%\u3002\u4e0d\u5f15\u5165\u63a8\u7406\u65f6\u5f00\u9500\uff0c\u5e73\u5747\u4ec5\u970015.87MB\u989d\u5916\u5185\u5b58\uff0c\u800c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u68c0\u6d4b\u9632\u5fa1\u901a\u5e38\u9700\u89811e4MB\u7ea7\u522b\u7684\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "TrapSuffix\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e3b\u52a8\u9632\u5fa1\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u63a7\u6027\u5bfc\u5411\u7684\u65b9\u6cd5\u6709\u6548\u5e94\u5bf9\u540e\u7f00\u8d8a\u72f1\u653b\u51fb\uff0c\u5b9e\u73b0\u4e86\u5f3a\u9632\u5fa1\u548c\u53ef\u9760\u7684\u53ef\u8ffd\u6eaf\u6027\uff0c\u4e0e\u73b0\u6709\u57fa\u4e8e\u8fc7\u6ee4\u7684\u9632\u5fa1\u81ea\u7136\u4e92\u8865\u3002"}}
{"id": "2602.06707", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06707", "abs": "https://arxiv.org/abs/2602.06707", "authors": ["Thiviyan Thanapalasingam", "Antonis Vozikis", "Peter Bloem", "Paul Groth"], "title": "Autoregressive Models for Knowledge Graph Generation", "comment": null, "summary": "Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.", "AI": {"tldr": "ARK\u662f\u4e00\u79cd\u81ea\u56de\u5f52\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u6a21\u578b\uff0c\u5c06\u56fe\u8c31\u89c6\u4e3a\u4e09\u5143\u7ec4\u5e8f\u5217\u8fdb\u884c\u751f\u6210\uff0c\u65e0\u9700\u663e\u5f0f\u89c4\u5219\u76d1\u7763\u5373\u53ef\u5b66\u4e60\u8bed\u4e49\u7ea6\u675f\uff0c\u5728IntelliGraphs\u57fa\u51c6\u4e0a\u8fbe\u523089.2%-100%\u7684\u8bed\u4e49\u6709\u6548\u6027\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u9700\u8981\u6a21\u578b\u5b66\u4e60\u4e09\u5143\u7ec4\u95f4\u7684\u590d\u6742\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u4fdd\u6301\u9886\u57df\u6709\u6548\u6027\u7ea6\u675f\u3002\u4e0e\u72ec\u7acb\u8bc4\u5206\u4e09\u5143\u7ec4\u7684\u94fe\u63a5\u9884\u6d4b\u4e0d\u540c\uff0c\u751f\u6210\u6a21\u578b\u5fc5\u987b\u6355\u6349\u6574\u4e2a\u5b50\u56fe\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u4ee5\u4ea7\u751f\u8bed\u4e49\u4e00\u81f4\u7684\u7ed3\u6784\u3002", "method": "\u63d0\u51faARK\uff08\u81ea\u56de\u5f52\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\uff09\u6a21\u578b\u5bb6\u65cf\uff0c\u5c06\u56fe\u8c31\u89c6\u4e3a(head, relation, tail)\u4e09\u5143\u7ec4\u5e8f\u5217\u8fdb\u884c\u81ea\u56de\u5f52\u751f\u6210\u3002\u6a21\u578b\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u9690\u5f0f\u8bed\u4e49\u7ea6\u675f\uff08\u7c7b\u578b\u4e00\u81f4\u6027\u3001\u65f6\u95f4\u6709\u6548\u6027\u3001\u5173\u7cfb\u6a21\u5f0f\uff09\uff0c\u65e0\u9700\u663e\u5f0f\u89c4\u5219\u76d1\u7763\u3002\u8fd8\u63d0\u51faSAIL\uff0cARK\u7684\u53d8\u5206\u6269\u5c55\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u6f5c\u5728\u8868\u793a\u652f\u6301\u53d7\u63a7\u751f\u6210\u3002", "result": "\u5728IntelliGraphs\u57fa\u51c6\u4e0a\uff0c\u6a21\u578b\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.2%\u5230100.0%\u7684\u8bed\u4e49\u6709\u6548\u6027\uff0c\u540c\u65f6\u751f\u6210\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u65b0\u56fe\u8c31\u3002\u5206\u6790\u663e\u793a\u6a21\u578b\u5bb9\u91cf\uff08\u9690\u85cf\u7ef4\u5ea6\u226564\uff09\u6bd4\u67b6\u6784\u6df1\u5ea6\u5bf9KG\u751f\u6210\u66f4\u91cd\u8981\uff0c\u5faa\u73af\u67b6\u6784\u5728\u8fbe\u5230\u4e0e\u57fa\u4e8eTransformer\u7684\u66ff\u4ee3\u65b9\u6848\u76f8\u5f53\u6709\u6548\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u81ea\u56de\u5f52\u6a21\u578b\u4e3a\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5728\u77e5\u8bc6\u5e93\u8865\u5168\u548c\u67e5\u8be2\u56de\u7b54\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u6a21\u578b\u5bb9\u91cf\u6bd4\u67b6\u6784\u6df1\u5ea6\u66f4\u5173\u952e\uff0c\u5faa\u73af\u67b6\u6784\u5728\u4fdd\u6301\u6709\u6548\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\u3002"}}
{"id": "2602.06746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06746", "abs": "https://arxiv.org/abs/2602.06746", "authors": ["Alessandro Abate", "Giuseppe De Giacomo", "Mathias Jackermeier", "Jan Kret\u00ednsk\u00fd", "Maximilian Prokop", "Christoph Weinhuber"], "title": "Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions", "comment": null, "summary": "We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49LTL\u5230\u81ea\u52a8\u673a\u8f6c\u6362\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u6807\u8bb0\u7684\u81ea\u52a8\u673a\u63d0\u53d6\u4efb\u52a1\u5d4c\u5165\u6765\u8c03\u8282\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742LTL\u89c4\u8303\u7684\u9ad8\u6548\u5904\u7406\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u89c4\u8303\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5b8c\u6574LTL\u89c4\u8303\u5e76\u63d0\u53d6\u4e30\u5bcc\u4efb\u52a1\u8868\u793a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u5d4c\u5165\u6280\u672f\uff0c\u5229\u7528\u65b0\u4e00\u4ee3\u8bed\u4e49LTL\u5230\u81ea\u52a8\u673a\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u751f\u6210\u8bed\u4e49\u6807\u8bb0\u7684\u81ea\u52a8\u673a\uff0c\u4ece\u4e2d\u63d0\u53d6\u8868\u8fbe\u6027\u5f3a\u7684\u4efb\u52a1\u5d4c\u5165\u6765\u8c03\u8282\u7b56\u7565\uff0c\u652f\u6301\u5b8c\u6574\u7684LTL\u89c4\u8303\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6269\u5c55\u5230\u590d\u6742\u89c4\u8303\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u590d\u6742\u89c4\u8303\u4e0b\u4f1a\u5931\u8d25\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u4e49LTL\u5230\u81ea\u52a8\u673a\u8f6c\u6362\u7684\u4efb\u52a1\u5d4c\u5165\u6280\u672f\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684LTL\u89c4\u8303\u5e76\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.06687", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06687", "abs": "https://arxiv.org/abs/2602.06687", "authors": ["Li Lu", "Yanjie Zhao", "Hongzhou Rao", "Kechi Zhang", "Haoyu Wang"], "title": "Evaluating and Enhancing the Vulnerability Reasoning Capabilities of Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in vulnerability detection. However, a critical reliability gap persists: models frequently yield correct detection verdicts based on hallucinated logic or superficial patterns that deviate from the actual root cause. This misalignment remains largely obscured because contemporary benchmarks predominantly prioritize coarse-grained classification metrics, lacking the granular ground truth required to evaluate the underlying reasoning process. To bridge this gap, we first construct a benchmark consisting of two datasets: (1) real-world vulnerabilities with expert-curated causal reasoning as ground truth, and (2) semantically equivalent code perturbations for assessing reasoning robustness. Our large-scale empirical study reveals that even state-of-the-art models struggle to maintain logical consistency during semantic code comprehension, exhibiting 12 systematic failure patterns. Addressing these limitations, we propose DAGVul, a novel framework that models vulnerability reasoning as a Directed Acyclic Graph (DAG) generation task. Unlike linear chain-of-thought (CoT), our approach explicitly maps causal dependencies to enforce structural consistency. By further introducing Reinforcement Learning with Verifiable Rewards (RLVR), we align model reasoning trace with program-intrinsic logic. Experimental results demonstrate that our framework improves the reasoning F1-score by an average of 18.9% over all the baselines. Remarkably, our 8B-parameter implementation not only outperforms existing models of comparable scale but also surpasses specialized large-scale reasoning models, including Qwen3-30B-Reasoning and GPT-OSS-20B-High. It is even competitive with state-of-the-art models like Claude-Sonnet-4.5 (75.47% vs. 76.11%), establishing new efficiency in vulnerability reasoning across model scales.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDAGVul\u6846\u67b6\uff0c\u5c06\u6f0f\u6d1e\u63a8\u7406\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u57288B\u53c2\u6570\u89c4\u6a21\u4e0b\u8fbe\u5230\u4e0eClaude-Sonnet-4.5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff1a\u6a21\u578b\u7ecf\u5e38\u57fa\u4e8e\u5e7b\u89c9\u903b\u8f91\u6216\u8868\u9762\u6a21\u5f0f\u7ed9\u51fa\u6b63\u786e\u68c0\u6d4b\u7ed3\u679c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6839\u672c\u539f\u56e0\u7684\u51c6\u786e\u63a8\u7406\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7c97\u7c92\u5ea6\u5206\u7c7b\u6307\u6807\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u5e95\u5c42\u63a8\u7406\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea6\u771f\u5b9e\u6807\u7b7e\u3002", "method": "1) \u6784\u5efa\u5305\u542b\u771f\u5b9e\u6f0f\u6d1e\u4e13\u5bb6\u6807\u6ce8\u56e0\u679c\u63a8\u7406\u548c\u8bed\u4e49\u7b49\u4ef7\u4ee3\u7801\u6270\u52a8\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff1b2) \u63d0\u51faDAGVul\u6846\u67b6\uff0c\u5c06\u6f0f\u6d1e\u63a8\u7406\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\u751f\u6210\u4efb\u52a1\uff0c\u663e\u5f0f\u6620\u5c04\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff1b3) \u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u5bf9\u9f50\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e0e\u7a0b\u5e8f\u5185\u5728\u903b\u8f91\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1) \u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u5728\u8bed\u4e49\u4ee3\u7801\u7406\u89e3\u4e2d\u96be\u4ee5\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u8868\u73b0\u51fa12\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff1b2) DAGVul\u6846\u67b6\u76f8\u6bd4\u6240\u6709\u57fa\u7ebf\u5e73\u5747\u63d0\u5347\u63a8\u7406F1\u5206\u657018.9%\uff1b3) 8B\u53c2\u6570\u5b9e\u73b0\u4e0d\u4ec5\u8d85\u8d8a\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u8fd8\u8d85\u8d8a\u4e13\u95e8\u7684\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\uff0c\u4e0eClaude-Sonnet-4.5\u7ade\u4e89\uff0875.47% vs. 76.11%\uff09\u3002", "conclusion": "DAGVul\u6846\u67b6\u901a\u8fc7\u5c06\u6f0f\u6d1e\u63a8\u7406\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u63a8\u7406\u53ef\u9760\u6027\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6548\u7387\u6807\u51c6\u3002"}}
{"id": "2602.06818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06818", "abs": "https://arxiv.org/abs/2602.06818", "authors": ["Anirudh Chari", "Neil Pattanaik"], "title": "Wild Guesses and Mild Guesses in Active Concept Learning", "comment": null, "summary": "Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting \"safe\" queries, leading to faster convergence on simple rules. Our results suggest that \"confirmation bias\" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u7406\u6027\u4e3b\u52a8\u5b66\u4e60\u5668\uff08\u6700\u5927\u5316\u671f\u671b\u4fe1\u606f\u589e\u76ca\uff09\u548c\u4eba\u7c7b\u5f0f\u79ef\u6781\u6d4b\u8bd5\u7b56\u7565\u5728\u6982\u5ff5\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0EIG\u5728\u590d\u6742\u89c4\u5219\u5b66\u4e60\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u7b80\u5355\u6982\u5ff5\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cPTS\u867d\u4fe1\u606f\u6b21\u4f18\u4f46\u80fd\u4fdd\u6301\u5047\u8bbe\u6709\u6548\u6027\uff0c\u66f4\u5feb\u6536\u655b\u4e8e\u7b80\u5355\u89c4\u5219\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u6982\u5ff5\u5b66\u4e60\u4e2d\u7684\u4e3b\u52a8\u5b66\u4e60\u5e73\u8861\u95ee\u9898\uff1a\u5982\u4f55\u5728\u67e5\u8be2\u7684\u4fe1\u606f\u91cf\u548c\u5b66\u4e60\u8005\u751f\u6210\u5047\u8bbe\u7684\u7a33\u5b9a\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u63a2\u8ba8\"\u786e\u8ba4\u504f\u8bef\"\u662f\u5426\u53ef\u80fd\u662f\u4e00\u79cd\u8ba4\u77e5\u9519\u8bef\uff0c\u8fd8\u662f\u7ef4\u6301\u53ef\u5904\u7406\u63a8\u7406\u7684\u7406\u6027\u9002\u5e94\u7b56\u7565\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u8d1d\u53f6\u65af\u5b66\u4e60\u5668\uff0c\u5047\u8bbe\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\u7ec4\u6210\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u66f4\u65b0\u91cd\u65b0\u52a0\u6743\u3002\u6bd4\u8f83\u4e24\u79cd\u7b56\u7565\uff1a\u7406\u6027\u4e3b\u52a8\u5b66\u4e60\u5668\uff08\u6700\u5927\u5316\u8fd1\u4f3c\u671f\u671b\u4fe1\u606f\u589e\u76caEIG\uff09\u548c\u4eba\u7c7b\u5f0f\u79ef\u6781\u6d4b\u8bd5\u7b56\u7565\uff08\u67e5\u8be2\u5f53\u524d\u6700\u4f73\u5047\u8bbe\u9884\u6d4b\u4e3a\u6b63\u7684\u5b9e\u4f8b\uff09\u3002\u5728\u7ecf\u5178\u6570\u5b57\u6e38\u620f\u4e2d\u8fdb\u884c\u6982\u5ff5\u5b66\u4e60\u4efb\u52a1\u6d4b\u8bd5\u3002", "result": "EIG\u5728\u9700\u8981\u8bc1\u4f2a\u7684\u590d\u6742\u89c4\u5219\uff08\u5982\u590d\u5408\u89c4\u5219\u6216\u5305\u542b\u4f8b\u5916\u7684\u89c4\u5219\uff09\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u7b80\u5355\u6982\u5ff5\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u5931\u8d25\u539f\u56e0\u662fEIG\u7b56\u7565\u4e0eLLM\u63d0\u8bae\u5206\u5e03\u4e4b\u95f4\u7684\u652f\u6301\u4e0d\u5339\u914d\uff1a\u9ad8\u5ea6\u8bca\u65ad\u6027\u7684\u8fb9\u754c\u67e5\u8be2\u5c06\u540e\u9a8c\u63a8\u5411\u751f\u6210\u5668\u4ea7\u751f\u65e0\u6548\u6216\u8fc7\u4e8e\u5177\u4f53\u7a0b\u5e8f\u7684\u533a\u57df\uff0c\u5bfc\u81f4\u7c92\u5b50\u8fd1\u4f3c\u4e2d\u7684\u652f\u6301\u4e0d\u5339\u914d\u9677\u9631\u3002PTS\u867d\u4fe1\u606f\u6b21\u4f18\u4f46\u503e\u5411\u4e8e\u901a\u8fc7\u9009\u62e9\"\u5b89\u5168\"\u67e5\u8be2\u6765\u4fdd\u6301\u63d0\u8bae\u6709\u6548\u6027\uff0c\u5728\u7b80\u5355\u89c4\u5219\u4e0a\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\"\u786e\u8ba4\u504f\u8bef\"\u53ef\u80fd\u4e0d\u662f\u8ba4\u77e5\u9519\u8bef\uff0c\u800c\u662f\u5728\u4eba\u7c7b\u601d\u7ef4\u7279\u6709\u7684\u7a00\u758f\u3001\u5f00\u653e\u5f0f\u5047\u8bbe\u7a7a\u95f4\u4e2d\u7ef4\u6301\u53ef\u5904\u7406\u63a8\u7406\u7684\u7406\u6027\u9002\u5e94\u7b56\u7565\u3002\u79ef\u6781\u6d4b\u8bd5\u7b56\u7565\u901a\u8fc7\u9009\u62e9\u5b89\u5168\u67e5\u8be2\u6765\u4fdd\u6301\u5047\u8bbe\u6709\u6548\u6027\uff0c\u5728\u7b80\u5355\u6982\u5ff5\u5b66\u4e60\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2602.06700", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06700", "abs": "https://arxiv.org/abs/2602.06700", "authors": ["Ying Song", "Balaji Palanisamy"], "title": "Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs", "comment": null, "summary": "Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \\textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \\textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \\emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \\emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.", "AI": {"tldr": "\u63d0\u51fa\u4e86Taipan\uff0c\u9996\u4e2a\u65e0\u9700\u67e5\u8be2\u3001\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u56fe\u591a\u654f\u611f\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4ec5\u4ece\u516c\u5f00\u56fe\u6570\u636e\u4e2d\u5b58\u5728\u7684\u56fa\u6709\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u91cd\u590d\u6a21\u578b\u67e5\u8be2\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u4ec5\u4ece\u516c\u5f00\u53d1\u5e03\u7684\u56fe\u6570\u636e\u4e2d\u5c31\u5b58\u5728\u7684\u56fa\u6709\u591a\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faTaipan\u6846\u67b6\uff0c\u5305\u542b\u5206\u5c42\u653b\u51fb\u77e5\u8bc6\u8def\u7531\u4ee5\u6355\u6349\u5c5e\u6027\u95f4\u590d\u6742\u5173\u8054\uff0c\u4ee5\u53ca\u63d0\u793a\u5f15\u5bfc\u7684\u653b\u51fb\u539f\u578b\u7cbe\u70bc\u4ee5\u7f13\u89e3\u8d1f\u8fc1\u79fb\u548c\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTaipan\u5728\u76f8\u540c\u5206\u5e03\u3001\u5f02\u6784\u76f8\u4f3c\u5206\u5e03\u548c\u5f02\u5206\u5e03\u8bbe\u7f6e\u4e0b\u5747\u80fd\u4fdd\u6301\u5f3a\u653b\u51fb\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u4e25\u683c\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u4e0b\u4ecd\u7136\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u591a\u5c5e\u6027\u9690\u79c1\u4fdd\u62a4\u56fe\u53d1\u5e03\u65b9\u6cd5\u548c\u6570\u636e\u5171\u4eab\u5b9e\u8df5\u7684\u7d27\u8feb\u9700\u6c42\u3002"}}
{"id": "2602.06820", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06820", "abs": "https://arxiv.org/abs/2602.06820", "authors": ["Dunwei Tu", "Hongyan Hao", "Hansi Yang", "Yihao Chen", "Yi-Kai Zhang", "Zhikang Xia", "Yu Yang", "Yueqing Sun", "Xingchen Liu", "Furao Shen", "Qi Gu", "Hui Su", "Xunliang Cai"], "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training", "comment": null, "summary": "Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $\u03c4^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.", "AI": {"tldr": "ScaleEnv\u6846\u67b6\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u5b8c\u5168\u4ea4\u4e92\u5f0f\u73af\u5883\u548c\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u6d4b\u8bd5\u786e\u4fdd\u73af\u5883\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5de5\u5177\u4f9d\u8d56\u56fe\u6269\u5c55\u548c\u53ef\u6267\u884c\u52a8\u4f5c\u9a8c\u8bc1\u4fdd\u8bc1\u4efb\u52a1\u5b8c\u6574\u6027\u548c\u53ef\u89e3\u6027\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u672a\u89c1\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u573a\u666f\u7684\u901a\u7528\u667a\u80fd\u4f53\u9700\u8981\u4ea4\u4e92\u5f0f\u73af\u5883\u8fdb\u884c\u81ea\u6211\u63a2\u7d22\uff0c\u4f46\u5f53\u524d\u4ea4\u4e92\u73af\u5883\u4e25\u91cd\u7a00\u7f3a\uff0c\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u5728\u73af\u5883\u591a\u6837\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u9650\u5236\u3002", "method": "\u63d0\u51faScaleEnv\u6846\u67b6\uff0c\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u5b8c\u5168\u4ea4\u4e92\u5f0f\u73af\u5883\u548c\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff1b\u901a\u8fc7\u7a0b\u5e8f\u5316\u6d4b\u8bd5\u786e\u4fdd\u73af\u5883\u53ef\u9760\u6027\uff1b\u901a\u8fc7\u5de5\u5177\u4f9d\u8d56\u56fe\u6269\u5c55\u548c\u53ef\u6267\u884c\u52a8\u4f5c\u9a8c\u8bc1\u4fdd\u8bc1\u4efb\u52a1\u5b8c\u6574\u6027\u548c\u53ef\u89e3\u6027\u3002", "result": "\u5728\u672a\u89c1\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\uff08\u5982\u03c4\u00b2-Bench\u548cVitaBench\uff09\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u589e\u52a0\u9886\u57df\u6570\u91cf\u5bf9\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u6269\u5c55\u73af\u5883\u591a\u6837\u6027\u5bf9\u4e8e\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0cScaleEnv\u4e3a\u89e3\u51b3\u4ea4\u4e92\u73af\u5883\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.06718", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06718", "abs": "https://arxiv.org/abs/2602.06718", "authors": ["Zuyao Xu", "Yuqi Qiu", "Lu Sun", "FaSheng Miao", "Fubin Wu", "Xinyi Wang", "Xiang Li", "Haozhe Lu", "ZhengZe Zhang", "Yuxin Hu", "Jialu Li", "Jin Luo", "Feng Zhang", "Rui Luo", "Xinran Liu", "Yingxian Li", "Jiaji Liu"], "title": "GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models", "comment": null, "summary": "Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.\n  To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\\% to 94.93\\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\\% of researchers copy-paste BibTeX without checking and 44.4\\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\\% of reviewers do not thoroughly check references and 80.0\\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u865a\u5047\u5f15\u7528\u7684\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0cCiteVerifier\u6846\u67b6\u9996\u6b21\u7cfb\u7edf\u7814\u7a76LLM\u65f6\u4ee3\u7684\u5f15\u7528\u6709\u6548\u6027\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u5f15\u7528\u5e7b\u89c9\uff0c\u4e14\u5b66\u672f\u8bba\u6587\u4e2d\u865a\u5047\u5f15\u7528\u6bd4\u4f8b\u4e0a\u5347\uff0c\u7814\u7a76\u4eba\u5458\u548c\u5ba1\u7a3f\u4eba\u5b58\u5728\u9a8c\u8bc1\u7f3a\u53e3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u751f\u6210\u865a\u5047\u5f15\u7528\uff08\"\u5e7d\u7075\u5f15\u7528\"\uff09\u7684\u8d8b\u52bf\u5bf9\u5f15\u7528\u6709\u6548\u6027\u6784\u6210\u7cfb\u7edf\u6027\u5a01\u80c1\uff0c\u9700\u8981\u91cf\u5316\u8fd9\u79cd\u5a01\u80c1\u5e76\u5236\u5b9a\u7f13\u89e3\u63aa\u65bd\u3002", "method": "\u5f00\u53d1CiteVerifier\u5f00\u6e90\u6846\u67b6\u8fdb\u884c\u5927\u89c4\u6a21\u5f15\u7528\u9a8c\u8bc1\uff0c\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\uff1a1) \u572840\u4e2a\u7814\u7a76\u9886\u57df\u57fa\u51c6\u6d4b\u8bd513\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff1b2) \u5206\u67902020-2025\u5e74\u9876\u7ea7AI/ML\u548c\u5b89\u5168\u4f1a\u8bae56,381\u7bc7\u8bba\u6587\u4e2d\u7684220\u4e07\u6761\u5f15\u7528\uff1b3) \u8c03\u67e597\u540d\u7814\u7a76\u4eba\u5458\u5e76\u5206\u679094\u4efd\u6709\u6548\u56de\u590d\u3002", "result": "\u6240\u6709LLM\u90fd\u5b58\u5728\u5f15\u7528\u5e7b\u89c9\uff0c\u6bd4\u4f8b\u4ece14.23%\u523094.93%\uff1b1.07%\u7684\u8bba\u6587\u5305\u542b\u65e0\u6548\u6216\u4f2a\u9020\u5f15\u7528\uff08604\u7bc7\uff09\uff0c2025\u5e74\u5355\u5e74\u589e\u957f80.9%\uff1b41.5%\u7814\u7a76\u4eba\u5458\u590d\u5236\u7c98\u8d34BibTeX\u800c\u4e0d\u68c0\u67e5\uff0c44.4%\u9047\u5230\u53ef\u7591\u5f15\u7528\u65f6\u4e0d\u91c7\u53d6\u884c\u52a8\uff1b76.7%\u5ba1\u7a3f\u4eba\u4e0d\u5f7b\u5e95\u68c0\u67e5\u5f15\u7528\uff0c80%\u4ece\u672a\u6000\u7591\u865a\u5047\u5f15\u7528\u3002", "conclusion": "\u4e0d\u53ef\u9760\u7684AI\u5de5\u5177\u4e0e\u7814\u7a76\u4eba\u5458\u4e0d\u8db3\u7684\u4eba\u5de5\u9a8c\u8bc1\u4ee5\u53ca\u540c\u884c\u8bc4\u5ba1\u5ba1\u67e5\u4e0d\u8db3\u76f8\u7ed3\u5408\uff0c\u5bfc\u81f4\u865a\u5047\u5f15\u7528\u6c61\u67d3\u79d1\u5b66\u8bb0\u5f55\uff0c\u5f62\u6210\u52a0\u901f\u5371\u673a\u3002\u9700\u8981\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u4f1a\u8bae\u548c\u5de5\u5177\u5f00\u53d1\u8005\u63d0\u51fa\u5e72\u9884\u63aa\u65bd\u4ee5\u4fdd\u62a4\u5f15\u7528\u5b8c\u6574\u6027\u3002"}}
{"id": "2602.06822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06822", "abs": "https://arxiv.org/abs/2602.06822", "authors": ["Yi Chen", "Wonjin Shin", "Shuhong Liu", "Tho Mai", "Jeongmo Lee", "Chuanbo Hua", "Kun Wang", "Jun Liu", "Joo-Young Kim"], "title": "POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models", "comment": null, "summary": "Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.", "AI": {"tldr": "POP\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5728\u7ebf\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u80fd\u591f\u5728\u81ea\u56de\u5f52token\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e0a\u4e0b\u6587\u6761\u4ef6\u5316\u7684\u52a8\u6001\u526a\u679d\uff0c\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6269\u5c55\u83b7\u5f97\u5f3a\u5927\u6027\u80fd\uff0c\u4f46\u5f53\u524d\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u91c7\u7528\u56fa\u5b9a\u7684\u526a\u679d\u51b3\u7b56\uff0c\u5ffd\u7565\u4e86\u81ea\u56de\u5f52token\u751f\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u7a00\u758f\u6a21\u5f0f", "method": "POP\u5c06\u6a21\u578b\u901a\u9053\u5212\u5206\u4e3a\u4fdd\u7559\u533a\u3001\u5019\u9009\u533a\u548c\u526a\u679d\u533a\uff0c\u9884\u586b\u5145\u9636\u6bb5\u5b9a\u4e49\u7c97\u7c92\u5ea6\u526a\u679d\u5206\u533a\uff0c\u89e3\u7801\u9636\u6bb5\u5728\u5019\u9009\u533a\u5185\u751f\u6210\u7ec6\u7c92\u5ea6\u63a9\u7801\uff0c\u907f\u514d\u5168\u901a\u9053\u91cd\u65b0\u8bc4\u4f30", "result": "\u5728\u591a\u79cd\u5927\u578b\u57fa\u7840\u6a21\u578b\uff08\u5305\u62ecLLMs\u3001MoEs\u548cVLMs\uff09\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cPOP\u6bd4\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u63d0\u4f9b\u66f4\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\u5e76\u6700\u5c0f\u5316\u63a8\u7406\u5ef6\u8fdf", "conclusion": "POP\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u5904\u7406\uff08\u5305\u62ec\u79bb\u7ebf\u6821\u51c6\u3001\u91cd\u65b0\u8bad\u7ec3\u6216\u5b66\u4e60\u9884\u6d4b\u5668\uff09\uff0c\u80fd\u591f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u6761\u4ef6\u5316\u7684\u52a8\u6001\u526a\u679d"}}
{"id": "2602.06751", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06751", "abs": "https://arxiv.org/abs/2602.06751", "authors": ["Yikun Li", "Ting Zhang", "Jieke Shi", "Chengran Yang", "Junda He", "Xin Zhou", "Jinfeng Jiang", "Huihui Huang", "Wen Bin Leow", "Yide Yin", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection", "comment": null, "summary": "Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.\n  We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.\n  We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.", "AI": {"tldr": "CPRVul\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6f0f\u6d1e\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u6790\u548c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5728\u4e09\u4e2a\u9ad8\u8d28\u91cf\u6f0f\u6d1e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u51fd\u6570\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u5728\u51fd\u6570\u7ea7\u522b\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u8fc7\u7a0b\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u5b9e\u9645\u6f0f\u6d1e\u7684\u5b58\u5728\u548c\u6839\u672c\u539f\u56e0\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f46\u7b80\u5355\u5730\u9644\u52a0\u4e0a\u4e0b\u6587\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u4e0a\u4e0b\u6587\u5197\u957f\u3001\u5197\u4f59\u4e14\u5608\u6742\u3002", "method": "CPRVul\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4e0a\u4e0b\u6587\u5206\u6790\u548c\u9009\u62e9\uff1a\u6784\u5efa\u4ee3\u7801\u5c5e\u6027\u56fe\u63d0\u53d6\u5019\u9009\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528LLM\u751f\u6210\u5b89\u5168\u5bfc\u5411\u7684\u914d\u7f6e\u6587\u4ef6\u5e76\u5206\u914d\u76f8\u5173\u6027\u5206\u6570\uff0c\u9009\u62e9\u9ad8\u5f71\u54cd\u529b\u7684\u4e0a\u4e0b\u6587\u5143\u7d20\uff1b2) \u7ed3\u6784\u5316\u63a8\u7406\uff1a\u6574\u5408\u76ee\u6807\u51fd\u6570\u3001\u9009\u5b9a\u4e0a\u4e0b\u6587\u548c\u6f0f\u6d1e\u5143\u6570\u636e\u751f\u6210\u63a8\u7406\u8f68\u8ff9\uff0c\u7528\u4e8e\u5fae\u8c03LLM\u8fdb\u884c\u57fa\u4e8e\u63a8\u7406\u7684\u6f0f\u6d1e\u68c0\u6d4b\u3002", "result": "\u5728PrimeVul\u3001TitanVul\u548cCleanVul\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cCPRVul\u51c6\u786e\u7387\u4ece64.94%\u523073.76%\uff0c\u663e\u8457\u4f18\u4e8eUniXcoder\u768456.65%\u523063.68%\u3002\u5728PrimeVul\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523067.78%\u51c6\u786e\u7387\uff0c\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u534722.9%\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u53ea\u6709\u5904\u7406\u540e\u7684\u4e0a\u4e0b\u6587\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u7ed3\u5408\u624d\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CPRVul\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u6790\u548c\u7ed3\u6784\u5316\u63a8\u7406\u7684\u6709\u6548\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5904\u7406\u540e\u7684\u4e0a\u4e0b\u6587\u4e0e\u63a8\u7406\u673a\u5236\u534f\u540c\u5de5\u4f5c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.06838", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06838", "abs": "https://arxiv.org/abs/2602.06838", "authors": ["Jin Wang", "Hui Ma", "Fei Xing", "Ming Yan"], "title": "An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization", "comment": "submited to a conference", "summary": "Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u8f7b\u91cf\u538b\u7f29\u6a21\u5757\u3001\u670d\u52a1\u5668\u81ea\u9002\u5e94\u68af\u5ea6\u88c1\u526a\u548c\u7ea6\u675f\u611f\u77e5\u805a\u5408\u673a\u5236\uff0c\u89e3\u51b3\u5f02\u6784\u6570\u636e\u548c\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u8bbe\u5907\u5f02\u6784\u6027\u3001\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u68af\u5ea6\u66f4\u65b0\u4e0d\u7a33\u5b9a\u548c\u504f\u5dee\u95ee\u9898\uff0c\u5dee\u5206\u9690\u79c1\u7684\u56fa\u5b9a\u68af\u5ea6\u88c1\u526a\u548c\u9ad8\u65af\u566a\u58f0\u6ce8\u5165\u4f1a\u8fdb\u4e00\u6b65\u653e\u5927\u68af\u5ea6\u6270\u52a8\uff0c\u5bfc\u81f4\u8bad\u7ec3\u9707\u8361\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "1. \u5ba2\u6237\u7aef\u5f15\u5165\u8f7b\u91cf\u7ea7\u672c\u5730\u538b\u7f29\u6a21\u5757\uff0c\u6b63\u5219\u5316\u4e2d\u95f4\u8868\u793a\u5e76\u7ea6\u675f\u68af\u5ea6\u53d8\u5f02\u6027\uff1b2. \u670d\u52a1\u5668\u91c7\u7528\u81ea\u9002\u5e94\u68af\u5ea6\u88c1\u526a\u7b56\u7565\uff0c\u57fa\u4e8e\u5386\u53f2\u66f4\u65b0\u7edf\u8ba1\u52a8\u6001\u8c03\u6574\u88c1\u526a\u9608\u503c\uff1b3. \u8bbe\u8ba1\u7ea6\u675f\u611f\u77e5\u805a\u5408\u673a\u5236\uff0c\u6291\u5236\u4e0d\u53ef\u9760\u6216\u566a\u58f0\u4e3b\u5bfc\u7684\u5ba2\u6237\u7aef\u66f4\u65b0\u3002", "result": "\u5728CIFAR-10\u548cSVHN\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u6536\u655b\u7a33\u5b9a\u6027\u548c\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u5f02\u6784\u548c\u9690\u79c1\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2602.06756", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06756", "abs": "https://arxiv.org/abs/2602.06756", "authors": ["Long Tran", "Antti Koskela", "Ossi R\u00e4is\u00e4", "Antti Honkela"], "title": "$f$-Differential Privacy Filters: Validity and Approximate Solutions", "comment": "40 pages, 9 figures", "summary": "Accounting for privacy loss under fully adaptive composition -- where both the choice of mechanisms and their privacy parameters may depend on the entire history of prior outputs -- is a central challenge in differential privacy (DP). In this setting, privacy filters are stopping rules for compositions that ensure a prescribed global privacy budget is not exceeded. It remains unclear whether optimal trade-off-function-based notions, such as $f$-DP, admit valid privacy filters under fully adaptive interaction. We show that the natural approach to defining an $f$-DP filter -- composing individual trade-off curves and stopping when the prescribed $f$-DP curve is crossed -- is fundamentally invalid. We characterise when and why this failure occurs, and establish necessary and sufficient conditions under which the natural filter is valid. Furthermore, we prove a fully adaptive central limit theorem for $f$-DP and construct an approximate Gaussian DP filter for subsampled Gaussian mechanisms at small sampling rates $q<0.2$ and large sampling rates $q>0.8$, yielding tighter privacy guarantees than filters based on R\u00e9nyi DP in the same setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5b8c\u5168\u81ea\u9002\u5e94\u7ec4\u5408\u4e0b\u7684\u9690\u79c1\u635f\u5931\u8ba1\u7b97\u95ee\u9898\uff0c\u53d1\u73b0\u57fa\u4e8ef-DP\u7684\u81ea\u7136\u9690\u79c1\u8fc7\u6ee4\u5668\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5e76\u5efa\u7acb\u4e86\u5176\u6709\u6548\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5b8c\u5168\u81ea\u9002\u5e94\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u5e76\u6784\u5efa\u4e86\u8fd1\u4f3c\u9ad8\u65afDP\u8fc7\u6ee4\u5668\u3002", "motivation": "\u5728\u5dee\u5206\u9690\u79c1\u4e2d\uff0c\u5b8c\u5168\u81ea\u9002\u5e94\u7ec4\u5408\u4e0b\u7684\u9690\u79c1\u635f\u5931\u8ba1\u7b97\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5176\u4e2d\u673a\u5236\u9009\u62e9\u548c\u9690\u79c1\u53c2\u6570\u90fd\u53ef\u80fd\u4f9d\u8d56\u4e8e\u5148\u524d\u8f93\u51fa\u7684\u5b8c\u6574\u5386\u53f2\u3002\u9690\u79c1\u8fc7\u6ee4\u5668\u4f5c\u4e3a\u7ec4\u5408\u7684\u505c\u6b62\u89c4\u5219\uff0c\u786e\u4fdd\u89c4\u5b9a\u7684\u5168\u5c40\u9690\u79c1\u9884\u7b97\u4e0d\u88ab\u8d85\u8fc7\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u6700\u4f18\u7684\u57fa\u4e8e\u6743\u8861\u51fd\u6570\u7684f-DP\u6982\u5ff5\u662f\u5426\u5728\u5b8c\u5168\u81ea\u9002\u5e94\u4ea4\u4e92\u4e0b\u5141\u8bb8\u6709\u6548\u7684\u9690\u79c1\u8fc7\u6ee4\u5668\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u57fa\u4e8ef-DP\u7684\u81ea\u7136\u9690\u79c1\u8fc7\u6ee4\u5668\u65b9\u6cd5\uff08\u5373\u7ec4\u5408\u4e2a\u4f53\u6743\u8861\u66f2\u7ebf\u5e76\u5728\u89c4\u5b9a\u7684f-DP\u66f2\u7ebf\u88ab\u8de8\u8d8a\u65f6\u505c\u6b62\uff09\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u523b\u753b\u4e86\u8be5\u65b9\u6cd5\u5931\u8d25\u7684\u60c5\u51b5\u548c\u539f\u56e0\uff0c\u5e76\u5efa\u7acb\u4e86\u81ea\u7136\u8fc7\u6ee4\u5668\u6709\u6548\u7684\u5145\u8981\u6761\u4ef6\u3002\u540c\u65f6\u8bc1\u660e\u4e86\u5b8c\u5168\u81ea\u9002\u5e94\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff0c\u5e76\u4e3a\u5b50\u91c7\u6837\u9ad8\u65af\u673a\u5236\u5728\u7279\u5b9a\u91c7\u6837\u7387\u4e0b\u6784\u5efa\u4e86\u8fd1\u4f3c\u9ad8\u65afDP\u8fc7\u6ee4\u5668\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8ef-DP\u7684\u81ea\u7136\u9690\u79c1\u8fc7\u6ee4\u5668\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5e76\u975e\u603b\u662f\u6709\u6548\u3002\u5efa\u7acb\u4e86\u8be5\u8fc7\u6ee4\u5668\u6709\u6548\u6027\u7684\u5145\u8981\u6761\u4ef6\u3002\u8bc1\u660e\u4e86\u5b8c\u5168\u81ea\u9002\u5e94\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff0c\u5e76\u4e3a\u5b50\u91c7\u6837\u9ad8\u65af\u673a\u5236\u5728\u91c7\u6837\u7387q<0.2\u548cq>0.8\u65f6\u6784\u5efa\u4e86\u8fd1\u4f3c\u9ad8\u65afDP\u8fc7\u6ee4\u5668\uff0c\u8be5\u8fc7\u6ee4\u5668\u5728\u76f8\u540c\u8bbe\u7f6e\u4e0b\u6bd4\u57fa\u4e8eR\u00e9nyi DP\u7684\u8fc7\u6ee4\u5668\u63d0\u4f9b\u66f4\u4e25\u683c\u7684\u9690\u79c1\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u57fa\u4e8ef-DP\u7684\u81ea\u7136\u9690\u79c1\u8fc7\u6ee4\u5668\u5728\u5b8c\u5168\u81ea\u9002\u5e94\u7ec4\u5408\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\u6765\u7406\u89e3\u5176\u6709\u6548\u6027\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u8fd1\u4f3c\u9ad8\u65afDP\u8fc7\u6ee4\u5668\uff0c\u4e3a\u5b8c\u5168\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u7ec4\u5408\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u9690\u79c1\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2602.06841", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06841", "abs": "https://arxiv.org/abs/2602.06841", "authors": ["Sindhuja Chaduvula", "Jessee Ho", "Kina Kim", "Aravind Narayanan", "Mahshid Alinoori", "Muskan Garg", "Dhanesh Ramachandram", "Shaina Raza"], "title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems", "comment": null, "summary": "Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $\u03c1= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\\times$ more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.\n  Resources:\n  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u9759\u6001\u9884\u6d4b\u89e3\u91ca\u65b9\u6cd5\u4e0e\u667a\u80fd\u4f53\u7cfb\u7edf\u8f68\u8ff9\u8bca\u65ad\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f20\u7edf\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u5728\u9759\u6001\u5206\u7c7b\u4e2d\u6709\u6548\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u8bca\u65ad\u667a\u80fd\u4f53\u6267\u884c\u5931\u8d25\uff0c\u800c\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bca\u65ad\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u5b9a\u4f4d\u884c\u4e3a\u6545\u969c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\uff0cAI\u7cfb\u7edf\u4ece\u9759\u6001\u9884\u6d4b\u8f6c\u5411\u591a\u6b65\u9aa4\u51b3\u7b56\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u4f20\u7edf\u53ef\u89e3\u91caAI\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u6a21\u578b\u9884\u6d4b\uff0c\u800c\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6210\u529f\u4e0e\u5931\u8d25\u7531\u51b3\u7b56\u5e8f\u5217\u51b3\u5b9a\uff0c\u9700\u8981\u7814\u7a76\u4f20\u7edf\u89e3\u91ca\u65b9\u6cd5\u5982\u4f55\u9002\u5e94\u667a\u80fd\u4f53\u73af\u5883\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u6bd4\u8f83\u9759\u6001\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5f52\u56e0\u89e3\u91ca\u65b9\u6cd5\u4e0e\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\uff08TAU-bench Airline\u548cAssistantBench\uff09\u4e2d\u7684\u8f68\u8ff9\u8bca\u65ad\u65b9\u6cd5\uff0c\u533a\u5206\u4e24\u79cd\u89e3\u91ca\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "result": "\u5f52\u56e0\u65b9\u6cd5\u5728\u9759\u6001\u8bbe\u7f6e\u4e2d\u7279\u5f81\u6392\u5e8f\u7a33\u5b9a\uff08Spearman \u03c1=0.86\uff09\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u8bca\u65ad\u667a\u80fd\u4f53\u8f68\u8ff9\u4e2d\u7684\u6267\u884c\u7ea7\u6545\u969c\u3002\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bc4\u4f30\u65b9\u6cd5\u80fd\u4e00\u81f4\u5b9a\u4f4d\u884c\u4e3a\u6545\u969c\uff0c\u53d1\u73b0\u72b6\u6001\u8ddf\u8e2a\u4e0d\u4e00\u81f4\u6027\u5728\u5931\u8d25\u8fd0\u884c\u4e2d\u9ad82.7\u500d\uff0c\u5e76\u4f7f\u6210\u529f\u6982\u7387\u964d\u4f4e49%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u4ece\u9759\u6001\u89e3\u91ca\u8f6c\u5411\u8f68\u8ff9\u7ea7\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u8bca\u65ad\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2602.06759", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.06759", "abs": "https://arxiv.org/abs/2602.06759", "authors": ["Yunlong Lyu", "Yixuan Tang", "Peng Chen", "Tian Dong", "Xinyu Wang", "Zhiqiang Dong", "Hao Chen"], "title": "\"Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs", "comment": null, "summary": "Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood.\n  In this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86AI\u96c6\u6210IDE\u4e2dNext Edit Suggestions\uff08NES\uff09\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63ed\u793a\u4e86NES\u56e0\u6269\u5c55\u4e0a\u4e0b\u6587\u68c0\u7d22\u673a\u5236\u800c\u5f15\u5165\u7684\u65b0\u653b\u51fb\u9762\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u6295\u6bd2\u3001\u4e8b\u52a1\u7f16\u8f91\u548c\u4eba\u7c7b-IDE\u4ea4\u4e92\u654f\u611f\u6027\u7b49\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u4ee3AI\u96c6\u6210IDE\u6b63\u4ece\u88ab\u52a8\u4ee3\u7801\u8865\u5168\u8f6c\u5411\u4e3b\u52a8\u7684Next Edit Suggestions\uff08NES\uff09\uff0c\u8fd9\u79cd\u6f14\u53d8\u5f15\u5165\u4e86\u66f4\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u673a\u5236\u548c\u4ea4\u4e92\u6a21\u5f0f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u51e0\u4e4e\u5b8c\u5168\u4e13\u6ce8\u4e8e\u72ec\u7acbLLM\u4ee3\u7801\u751f\u6210\u7684\u5b89\u5168\u5f71\u54cd\uff0c\u5ffd\u89c6\u4e86NES\u5728\u73b0\u4ee3AI\u96c6\u6210IDE\u4e2d\u53ef\u80fd\u5e26\u6765\u7684\u653b\u51fb\u5411\u91cf\u3002NES\u7684\u5e95\u5c42\u673a\u5236\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5176\u5b89\u5168\u5f71\u54cd\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002", "method": "1. \u6df1\u5165\u5256\u6790NES\u673a\u5236\u4ee5\u7406\u89e3\u65b0\u5f15\u5165\u7684\u5a01\u80c1\u5411\u91cf\uff1b2. \u8fdb\u884c\u5168\u9762\u7684\u5b9e\u9a8c\u5ba4\u7814\u7a76\u8bc4\u4f30NES\u7684\u5b89\u5168\u5f71\u54cd\uff1b3. \u5f00\u5c55\u6d89\u53ca200\u591a\u540d\u4e13\u4e1a\u5f00\u53d1\u8005\u7684\u5927\u89c4\u6a21\u5728\u7ebf\u8c03\u67e5\uff0c\u8bc4\u4f30\u5b9e\u9645\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2dNES\u5b89\u5168\u98ce\u9669\u7684\u8ba4\u77e5\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u53d1\u73b0NES\u68c0\u7d22\u4e86\u663e\u8457\u6269\u5c55\u7684\u4e0a\u4e0b\u6587\uff08\u5305\u62ec\u4e0d\u53ef\u5bdf\u89c9\u7684\u7528\u6237\u64cd\u4f5c\u8f93\u5165\u548c\u5168\u5c40\u4ee3\u7801\u5e93\u68c0\u7d22\uff09\uff0c\u589e\u52a0\u4e86\u653b\u51fb\u9762\u3002\u8bc4\u4f30\u663e\u793aNES\u6613\u53d7\u4e0a\u4e0b\u6587\u6295\u6bd2\u653b\u51fb\uff0c\u5e76\u5bf9\u4e8b\u52a1\u7f16\u8f91\u548c\u4eba\u7c7b-IDE\u4ea4\u4e92\u654f\u611f\u3002\u8c03\u67e5\u7ed3\u679c\u8868\u660e\u5f00\u53d1\u8005\u666e\u904d\u7f3a\u4e4f\u5bf9NES\u76f8\u5173\u5b89\u5168\u98ce\u9669\u7684\u8ba4\u77e5\uff0c\u7a81\u663e\u4e86\u5728AI\u96c6\u6210IDE\u4e2d\u52a0\u5f3a\u6559\u80b2\u548c\u6539\u8fdb\u5b89\u5168\u5bf9\u7b56\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "NES\u7cfb\u7edf\u5728\u63d0\u9ad8\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5b89\u5168\u7814\u7a76\u548c\u76f8\u5e94\u7684\u9632\u62a4\u63aa\u65bd\u3002\u7814\u7a76\u63ed\u793a\u4e86NES\u673a\u5236\u7684\u6f0f\u6d1e\u548c\u5f00\u53d1\u8005\u8ba4\u77e5\u5dee\u8ddd\uff0c\u4e3aAI\u96c6\u6210IDE\u7684\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.06855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06855", "abs": "https://arxiv.org/abs/2602.06855", "authors": ["Alisia Lupidi", "Bhavul Gauri", "Thomas Simon Foster", "Bassel Al Omari", "Despoina Magka", "Alberto Pepe", "Alexis Audran-Reiss", "Muna Aghamelu", "Nicolas Baldwin", "Lucia Cipolina-Kun", "Jean-Christophe Gagnon-Audet", "Chee Hau Leow", "Sandra Lefdal", "Hossam Mossalam", "Abhinav Moudgil", "Saba Nazir", "Emanuel Tewolde", "Isabel Urrego", "Jordi Armengol Estape", "Amar Budhiraja", "Gaurav Chaurasia", "Abhishek Charnalia", "Derek Dunfield", "Karen Hambardzumyan", "Daniel Izcovich", "Martin Josifoski", "Ishita Mediratta", "Kelvin Niu", "Parth Pathak", "Michael Shvartsman", "Edan Toledo", "Anton Protopopov", "Roberta Raileanu", "Alexander Miller", "Tatiana Shavrina", "Jakob Foerster", "Yoram Bachrach"], "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents", "comment": "49 pages, 14 figures, 10 tables", "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.", "AI": {"tldr": "AIRS-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u79d1\u5b66\u7814\u7a76\u5168\u751f\u547d\u5468\u671f\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b20\u4e2a\u6765\u81ea\u524d\u6cbf\u673a\u5668\u5b66\u4e60\u8bba\u6587\u7684\u4efb\u52a1\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\uff0c\u65e8\u5728\u52a0\u901f\u81ea\u4e3b\u79d1\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u5176\u5728\u5b8c\u6574\u7814\u7a76\u751f\u547d\u5468\u671f\u4e2d\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u4e3a\u4e86\u52a0\u901f\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u667a\u80fd\u4f53\u4ece\u60f3\u6cd5\u751f\u6210\u5230\u5b9e\u9a8c\u5206\u6790\u548c\u8fed\u4ee3\u6539\u8fdb\u5168\u8fc7\u7a0b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u4ece\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u8bba\u6587\u4e2d\u9009\u53d620\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u8bed\u8a00\u5efa\u6a21\u3001\u6570\u5b66\u3001\u751f\u7269\u4fe1\u606f\u5b66\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b49\u591a\u4e2a\u9886\u57df\u3002\u4efb\u52a1\u683c\u5f0f\u7075\u6d3b\uff0c\u4fbf\u4e8e\u96c6\u6210\u65b0\u4efb\u52a1\u548c\u6bd4\u8f83\u4e0d\u540c\u667a\u80fd\u4f53\u6846\u67b6\u3002\u4f7f\u7528\u524d\u6cbf\u6a21\u578b\u914d\u5408\u987a\u5e8f\u548c\u5e76\u884c\u67b6\u6784\u5efa\u7acb\u57fa\u7ebf\u3002", "result": "\u667a\u80fd\u4f53\u57284\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4e86\u4eba\u7c7bSOTA\u6c34\u5e73\uff0c\u4f46\u572816\u4e2a\u4efb\u52a1\u4e2d\u672a\u80fd\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002\u5373\u4f7f\u667a\u80fd\u4f53\u8d85\u8d8a\u4e86\u4eba\u7c7b\u57fa\u51c6\uff0c\u4e5f\u672a\u80fd\u8fbe\u5230\u5e95\u5c42\u4efb\u52a1\u7684\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u3002\u8fd9\u8868\u660eAIRS-Bench\u8fdc\u672a\u9971\u548c\uff0c\u4ecd\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "AIRS-Bench\u662f\u4e00\u4e2a\u8fdc\u672a\u9971\u548c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u81ea\u4e3b\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u5f00\u6e90\u4efb\u52a1\u5b9a\u4e49\u548c\u8bc4\u4f30\u4ee3\u7801\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u667a\u80fd\u4f53\u5728\u79d1\u5b66\u7814\u7a76\u65b9\u9762\u4ecd\u6709\u5de8\u5927\u7684\u63d0\u5347\u6f5c\u529b\u3002"}}
{"id": "2602.06777", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06777", "abs": "https://arxiv.org/abs/2602.06777", "authors": ["Yassine Chagna", "Antal Goldschmidt"], "title": "Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs", "comment": null, "summary": "This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f02\u6784\u65e5\u5fd7\u6e90\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u9ad8\u8bef\u62a5\u7387\u3001\u8bed\u4e49\u76f2\u70b9\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u3001\u8bed\u4e49\u76f2\u70b9\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u56e0\u4e3a\u65e5\u5fd7\u6570\u636e\u5177\u6709\u654f\u611f\u6027\uff0c\u5e72\u51c0\u7684\u6807\u6ce8\u6570\u636e\u96c6\u5f88\u5c11\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u65e5\u5fd7\u8bed\u4e49\u5e76\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a1) LogAtlas-Foundation-Sessions\u548cLogAtlas-Defense-Set\u4e24\u4e2a\u5e73\u8861\u4e14\u5f02\u6784\u7684\u65e5\u5fd7\u6570\u636e\u96c6\uff0c\u5305\u542b\u660e\u786e\u7684\u653b\u51fb\u6807\u6ce8\u548c\u9690\u79c1\u4fdd\u62a4\uff1b2) \u5b9e\u8bc1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u4e3a\u4ec0\u4e48F1\u548c\u51c6\u786e\u7387\u7b49\u6807\u51c6\u6307\u6807\u5728\u5b89\u5168\u5e94\u7528\u4e2d\u5177\u6709\u8bef\u5bfc\u6027\uff1b3) \u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u65e5\u5fd7\u7406\u89e3(Base-AMAN, 3B\u53c2\u6570)\u548c\u5b9e\u65f6\u68c0\u6d4b(AMAN, 0.5B\u53c2\u6570\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f)\u3002", "result": "\u7ed3\u679c\u8bc1\u660e\u4e86\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u6bcf\u4e2a\u4f1a\u8bdd\u7684\u63a8\u7406\u65f6\u95f4\u4e3a0.3-0.5\u79d2\uff0c\u6bcf\u65e5\u8fd0\u8425\u6210\u672c\u4f4e\u4e8e50\u7f8e\u5143\u3002\u8be5\u65b9\u6cd5\u5728\u5f02\u6784\u65e5\u5fd7\u6e90\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u65e5\u5fd7\u5206\u6790\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6a21\u578b\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.06887", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06887", "abs": "https://arxiv.org/abs/2602.06887", "authors": ["Chen Chen", "Yuchen Sun", "Jiaxin Gao", "Yanwen Jia", "Xueluan Gong", "Qian Wang", "Kwok-Yan Lam"], "title": "Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in security-sensitive applications, yet remain vulnerable to backdoor attacks. However, existing backdoor defenses are difficult to operationalize for Backdoor Defense-as-a-Service (BDaaS), as they require unrealistic side information (e.g., downstream clean data, known triggers/targets, or task domain specifics), and lack reusable, scalable purification across diverse backdoored models. In this paper, we present PROTOPURIFY, a backdoor purification framework via parameter edits under minimal assumptions. PROTOPURIFY first builds a backdoor vector pool from clean and backdoored model pairs, aggregates vectors into candidate prototypes, and selects the most aligned candidate for the target model via similarity matching. PROTOPURIFY then identifies a boundary layer through layer-wise prototype alignment and performs targeted purification by suppressing prototype-aligned components in the affected layers, achieving fine-grained mitigation with minimal impact on benign utility. Designed as a BDaaS-ready primitive, PROTOPURIFY supports reusability, customizability, interpretability, and runtime efficiency. Experiments across various LLMs on both classification and generation tasks show that PROTOPURIFY consistently outperforms 6 representative defenses against 6 diverse attacks, including single-trigger, multi-trigger, and triggerless backdoor settings. PROTOPURIFY reduces ASR to below 10%, and even as low as 1.6% in some cases, while incurring less than a 3% drop in clean utility. PROTOPURIFY further demonstrates robustness against adaptive backdoor variants and stability on non-backdoored models.", "AI": {"tldr": "PROTOPURIFY\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u7f16\u8f91\u5b9e\u73b0\u540e\u95e8\u51c0\u5316\uff0c\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\u5de5\u4f5c\uff0c\u652f\u6301\u540e\u95e8\u9632\u5fa1\u5373\u670d\u52a1", "motivation": "\u73b0\u6709\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u540e\u95e8\u9632\u5fa1\u5373\u670d\u52a1\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u4e0d\u73b0\u5b9e\u7684\u8f85\u52a9\u4fe1\u606f\uff08\u5982\u4e0b\u6e38\u5e72\u51c0\u6570\u636e\u3001\u5df2\u77e5\u89e6\u53d1\u5668/\u76ee\u6807\u6216\u4efb\u52a1\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff09\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8de8\u4e0d\u540c\u540e\u95e8\u6a21\u578b\u7684\u53ef\u91cd\u7528\u3001\u53ef\u6269\u5c55\u7684\u51c0\u5316\u65b9\u6cd5", "method": "PROTOPURIFY\u9996\u5148\u4ece\u5e72\u51c0\u6a21\u578b\u548c\u540e\u95e8\u6a21\u578b\u5bf9\u6784\u5efa\u540e\u95e8\u5411\u91cf\u6c60\uff0c\u5c06\u5411\u91cf\u805a\u5408\u6210\u5019\u9009\u539f\u578b\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u5339\u914d\u9009\u62e9\u4e0e\u76ee\u6807\u6a21\u578b\u6700\u5bf9\u9f50\u7684\u5019\u9009\u539f\u578b\uff1b\u7136\u540e\u901a\u8fc7\u5c42\u95f4\u539f\u578b\u5bf9\u9f50\u8bc6\u522b\u8fb9\u754c\u5c42\uff0c\u5728\u53d7\u5f71\u54cd\u5c42\u4e2d\u6291\u5236\u539f\u578b\u5bf9\u9f50\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7f13\u89e3", "result": "\u5728\u5404\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cPROTOPURIFY\u57286\u79cd\u4ee3\u8868\u6027\u9632\u5fa1\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5bf9\u62976\u79cd\u591a\u6837\u5316\u653b\u51fb\uff08\u5305\u62ec\u5355\u89e6\u53d1\u5668\u3001\u591a\u89e6\u53d1\u5668\u548c\u65e0\u89e6\u53d1\u5668\u540e\u95e8\u8bbe\u7f6e\uff09\uff1b\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u81f310%\u4ee5\u4e0b\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f4e\u81f31.6%\uff0c\u540c\u65f6\u5e72\u51c0\u6548\u7528\u4e0b\u964d\u4e0d\u52303%", "conclusion": "PROTOPURIFY\u662f\u4e00\u4e2aBDaaS\u5c31\u7eea\u7684\u539f\u59cb\u6846\u67b6\uff0c\u652f\u6301\u53ef\u91cd\u7528\u6027\u3001\u53ef\u5b9a\u5236\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8fd0\u884c\u65f6\u6548\u7387\uff0c\u5bf9\u81ea\u9002\u5e94\u540e\u95e8\u53d8\u4f53\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u975e\u540e\u95e8\u6a21\u578b\u4e0a\u4fdd\u6301\u7a33\u5b9a"}}
{"id": "2602.06911", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06911", "abs": "https://arxiv.org/abs/2602.06911", "authors": ["Saad Hossain", "Tom Tseng", "Punya Syon Pandey", "Samanvay Vajpayee", "Matthew Kowal", "Nayeema Nonta", "Samuel Simko", "Stephen Casper", "Zhijing Jin", "Kellin Pelrine", "Sirisha Rambhatla"], "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering", "comment": "28 pages, 13 figures", "summary": "As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench", "AI": {"tldr": "TamperBench\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9632\u7be1\u6539\u80fd\u529b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u653b\u51fb\u65b9\u6cd5\u5e93\u3001\u8d85\u53c2\u6570\u626b\u63cf\u3001\u5b89\u5168\u4e0e\u5b9e\u7528\u6027\u8bc4\u4f30\uff0c\u7528\u4e8e\u8bc4\u4f3021\u4e2a\u5f00\u6e90LLM\u7684\u9632\u7be1\u6539\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u589e\u591a\uff0c\u63d0\u9ad8\u5176\u9632\u7be1\u6539\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e0d\u540c\u6570\u636e\u96c6\u3001\u6307\u6807\u548c\u7be1\u6539\u914d\u7f6e\u4f7f\u5f97\u6a21\u578b\u95f4\u7684\u5b89\u5168\u6027\u3001\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u96be\u4ee5\u6bd4\u8f83\u3002", "method": "TamperBench\u6846\u67b6\u5305\u542b\uff1a(1) \u6536\u96c6\u6700\u5148\u8fdb\u7684\u6743\u91cd\u7a7a\u95f4\u5fae\u8c03\u653b\u51fb\u548c\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u653b\u51fb\u65b9\u6cd5\u5e93\uff1b(2) \u901a\u8fc7\u7cfb\u7edf\u5316\u8d85\u53c2\u6570\u626b\u63cf\u5b9e\u73b0\u771f\u5b9e\u5bf9\u6297\u8bc4\u4f30\uff1b(3) \u63d0\u4f9b\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u8bc4\u4f30\u3002\u6846\u67b6\u53ea\u9700\u6700\u5c11\u989d\u5916\u4ee3\u7801\u5373\u53ef\u6307\u5b9a\u4efb\u4f55\u5fae\u8c03\u914d\u7f6e\u3001\u5bf9\u9f50\u9636\u6bb5\u9632\u5fa1\u65b9\u6cd5\u548c\u6307\u6807\u5957\u4ef6\uff0c\u786e\u4fdd\u7aef\u5230\u7aef\u53ef\u590d\u73b0\u6027\u3002", "result": "\u4f7f\u7528TamperBench\u8bc4\u4f30\u4e8621\u4e2a\u5f00\u6e90LLM\uff08\u5305\u62ec\u9632\u5fa1\u589e\u5f3a\u53d8\u4f53\uff09\uff0c\u57289\u79cd\u7be1\u6539\u5a01\u80c1\u4e0b\u4f7f\u7528\u6807\u51c6\u5316\u5b89\u5168\u548c\u80fd\u529b\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u83b7\u5f97\u65b0\u53d1\u73b0\uff1a\u8bad\u7ec3\u540e\u5904\u7406\u5bf9\u9632\u7be1\u6539\u80fd\u529b\u7684\u5f71\u54cd\u3001\u8d8a\u72f1\u8c03\u4f18\u901a\u5e38\u662f\u6700\u4e25\u91cd\u7684\u653b\u51fb\u3001Triplet\u6210\u4e3a\u9886\u5148\u7684\u5bf9\u9f50\u9636\u6bb5\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "TamperBench\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u9632\u7be1\u6539\u80fd\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u63ed\u793a\u4e86\u91cd\u8981\u53d1\u73b0\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5f00\u6e90LLM\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
